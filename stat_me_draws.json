{"http://arxiv.org/abs/2310.03114": {"title": "Bayesian Parameter Inference for Partially Observed Stochastic Volterra Equations", "link": "http://arxiv.org/abs/2310.03114", "description": "In this article we consider Bayesian parameter inference for a type of\npartially observed stochastic Volterra equation (SVE). SVEs are found in many\nareas such as physics and mathematical finance. In the latter field they can be\nused to represent long memory in unobserved volatility processes. In many cases\nof practical interest, SVEs must be time-discretized and then parameter\ninference is based upon the posterior associated to this time-discretized\nprocess. Based upon recent studies on time-discretization of SVEs (e.g. Richard\net al. 2021), we use Euler-Maruyama methods for the afore-mentioned\ndiscretization. We then show how multilevel Markov chain Monte Carlo (MCMC)\nmethods (Jasra et al. 2018) can be applied in this context. In the examples we\nstudy, we give a proof that shows that the cost to achieve a mean square error\n(MSE) of $\\mathcal{O}(\\epsilon^2)$, $\\epsilon&gt;0$, is\n$\\mathcal{O}(\\epsilon^{-20/9})$. If one uses a single level MCMC method then\nthe cost is $\\mathcal{O}(\\epsilon^{-38/9})$ to achieve the same MSE. We\nillustrate these results in the context of state-space and stochastic\nvolatility models, with the latter applied to real data."}, "http://arxiv.org/abs/2310.03164": {"title": "A Hierarchical Random Effects State-space Model for Modeling Brain Activities from Electroencephalogram Data", "link": "http://arxiv.org/abs/2310.03164", "description": "Mental disorders present challenges in diagnosis and treatment due to their\ncomplex and heterogeneous nature. Electroencephalogram (EEG) has shown promise\nas a potential biomarker for these disorders. However, existing methods for\nanalyzing EEG signals have limitations in addressing heterogeneity and\ncapturing complex brain activity patterns between regions. This paper proposes\na novel random effects state-space model (RESSM) for analyzing large-scale\nmulti-channel resting-state EEG signals, accounting for the heterogeneity of\nbrain connectivities between groups and individual subjects. We incorporate\nmulti-level random effects for temporal dynamical and spatial mapping matrices\nand address nonstationarity so that the brain connectivity patterns can vary\nover time. The model is fitted under a Bayesian hierarchical model framework\ncoupled with a Gibbs sampler. Compared to previous mixed-effects state-space\nmodels, we directly model high-dimensional random effects matrices without\nstructural constraints and tackle the challenge of identifiability. Through\nextensive simulation studies, we demonstrate that our approach yields valid\nestimation and inference. We apply RESSM to a multi-site clinical trial of\nMajor Depressive Disorder (MDD). Our analysis uncovers significant differences\nin resting-state brain temporal dynamics among MDD patients compared to healthy\nindividuals. In addition, we show the subject-level EEG features derived from\nRESSM exhibit a superior predictive value for the heterogeneous treatment\neffect compared to the EEG frequency band power, suggesting the potential of\nEEG as a valuable biomarker for MDD."}, "http://arxiv.org/abs/2310.03258": {"title": "Detecting Electricity Service Equity Issues with Transfer Counterfactual Learning on Large-Scale Outage Datasets", "link": "http://arxiv.org/abs/2310.03258", "description": "Energy justice is a growing area of interest in interdisciplinary energy\nresearch. However, identifying systematic biases in the energy sector remains\nchallenging due to confounding variables, intricate heterogeneity in treatment\neffects, and limited data availability. To address these challenges, we\nintroduce a novel approach for counterfactual causal analysis centered on\nenergy justice. We use subgroup analysis to manage diverse factors and leverage\nthe idea of transfer learning to mitigate data scarcity in each subgroup. In\nour numerical analysis, we apply our method to a large-scale customer-level\npower outage data set and investigate the counterfactual effect of demographic\nfactors, such as income and age of the population, on power outage durations.\nOur results indicate that low-income and elderly-populated areas consistently\nexperience longer power outages, regardless of weather conditions. This points\nto existing biases in the power system and highlights the need for focused\nimprovements in areas with economic challenges."}, "http://arxiv.org/abs/2310.03351": {"title": "Efficiently analyzing large patient registries with Bayesian joint models for longitudinal and time-to-event data", "link": "http://arxiv.org/abs/2310.03351", "description": "The joint modeling of longitudinal and time-to-event outcomes has become a\npopular tool in follow-up studies. However, fitting Bayesian joint models to\nlarge datasets, such as patient registries, can require extended computing\ntimes. To speed up sampling, we divided a patient registry dataset into\nsubsamples, analyzed them in parallel, and combined the resulting Markov chain\nMonte Carlo draws into a consensus distribution. We used a simulation study to\ninvestigate how different consensus strategies perform with joint models. In\nparticular, we compared grouping all draws together with using equal- and\nprecision-weighted averages. We considered scenarios reflecting different\nsample sizes, numbers of data splits, and processor characteristics.\nParallelization of the sampling process substantially decreased the time\nrequired to run the model. We found that the weighted-average consensus\ndistributions for large sample sizes were nearly identical to the target\nposterior distribution. The proposed algorithm has been made available in an R\npackage for joint models, JMbayes2. This work was motivated by the clinical\ninterest in investigating the association between ppFEV1, a commonly measured\nmarker of lung function, and the risk of lung transplant or death, using data\nfrom the US Cystic Fibrosis Foundation Patient Registry (35,153 individuals\nwith 372,366 years of cumulative follow-up). Splitting the registry into five\nsubsamples resulted in an 85\\% decrease in computing time, from 9.22 to 1.39\nhours. Splitting the data and finding a consensus distribution by\nprecision-weighted averaging proved to be a computationally efficient and\nrobust approach to handling large datasets under the joint modeling framework."}, "http://arxiv.org/abs/2310.03521": {"title": "Cutting Feedback in Misspecified Copula Models", "link": "http://arxiv.org/abs/2310.03521", "description": "In copula models the marginal distributions and copula function are specified\nseparately. We treat these as two modules in a modular Bayesian inference\nframework, and propose conducting modified Bayesian inference by ``cutting\nfeedback''. Cutting feedback limits the influence of potentially misspecified\nmodules in posterior inference. We consider two types of cuts. The first limits\nthe influence of a misspecified copula on inference for the marginals, which is\na Bayesian analogue of the popular Inference for Margins (IFM) estimator. The\nsecond limits the influence of misspecified marginals on inference for the\ncopula parameters by using a rank likelihood to define the cut model. We\nestablish that if only one of the modules is misspecified, then the appropriate\ncut posterior gives accurate uncertainty quantification asymptotically for the\nparameters in the other module. Computation of the cut posteriors is difficult,\nand new variational inference methods to do so are proposed. The efficacy of\nthe new methodology is demonstrated using both simulated data and a substantive\nmultivariate time series application from macroeconomic forecasting. In the\nlatter, cutting feedback from misspecified marginals to a 1096 dimension copula\nimproves posterior inference and predictive accuracy greatly, compared to\nconventional Bayesian inference."}, "http://arxiv.org/abs/2310.03630": {"title": "Model-based Clustering for Network Data via a Latent Shrinkage Position Cluster Model", "link": "http://arxiv.org/abs/2310.03630", "description": "Low-dimensional representation and clustering of network data are tasks of\ngreat interest across various fields. Latent position models are routinely used\nfor this purpose by assuming that each node has a location in a low-dimensional\nlatent space, and enabling node clustering. However, these models fall short in\nsimultaneously determining the optimal latent space dimension and the number of\nclusters. Here we introduce the latent shrinkage position cluster model\n(LSPCM), which addresses this limitation. The LSPCM posits a Bayesian\nnonparametric shrinkage prior on the latent positions' variance parameters\nresulting in higher dimensions having increasingly smaller variances, aiding in\nthe identification of dimensions with non-negligible variance. Further, the\nLSPCM assumes the latent positions follow a sparse finite Gaussian mixture\nmodel, allowing for automatic inference on the number of clusters related to\nnon-empty mixture components. As a result, the LSPCM simultaneously infers the\nlatent space dimensionality and the number of clusters, eliminating the need to\nfit and compare multiple models. The performance of the LSPCM is assessed via\nsimulation studies and demonstrated through application to two real Twitter\nnetwork datasets from sporting and political contexts. Open source software is\navailable to promote widespread use of the LSPCM."}, "http://arxiv.org/abs/2310.03722": {"title": "Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance", "link": "http://arxiv.org/abs/2310.03722", "description": "In 1976, Lai constructed a nontrivial confidence sequence for the mean $\\mu$\nof a Gaussian distribution with unknown variance $\\sigma$. Curiously, he\nemployed both an improper (right Haar) mixture over $\\sigma$ and an improper\n(flat) mixture over $\\mu$. Here, we elaborate carefully on the details of his\nconstruction, which use generalized nonintegrable martingales and an extended\nVille's inequality. While this does yield a sequential t-test, it does not\nyield an ``e-process'' (due to the nonintegrability of his martingale). In this\npaper, we develop two new e-processes and confidence sequences for the same\nsetting: one is a test martingale in a reduced filtration, while the other is\nan e-process in the canonical data filtration. These are respectively obtained\nby swapping Lai's flat mixture for a Gaussian mixture, and swapping the right\nHaar mixture over $\\sigma$ with the maximum likelihood estimate under the null,\nas done in universal inference. We also analyze the width of resulting\nconfidence sequences, which have a curious dependence on the error probability\n$\\alpha$. Numerical experiments are provided along the way to compare and\ncontrast the various approaches."}, "http://arxiv.org/abs/2103.10875": {"title": "Scalable Bayesian computation for crossed and nested hierarchical models", "link": "http://arxiv.org/abs/2103.10875", "description": "We develop sampling algorithms to fit Bayesian hierarchical models, the\ncomputational complexity of which scales linearly with the number of\nobservations and the number of parameters in the model. We focus on crossed\nrandom effect and nested multilevel models, which are used ubiquitously in\napplied sciences. The posterior dependence in both classes is sparse: in\ncrossed random effects models it resembles a random graph, whereas in nested\nmultilevel models it is tree-structured. For each class we identify a framework\nfor scalable computation, building on previous work. Methods for crossed models\nare based on extensions of appropriately designed collapsed Gibbs samplers,\nwhere we introduce the idea of local centering; while methods for nested models\nare based on sparse linear algebra and data augmentation. We provide a\ntheoretical analysis of the proposed algorithms in some simplified settings,\nincluding a comparison with previously proposed methodologies and an\naverage-case analysis based on random graph theory. Numerical experiments,\nincluding two challenging real data analyses on predicting electoral results\nand real estate prices, compare with off-the-shelf Hamiltonian Monte Carlo,\ndisplaying drastic improvement in performance."}, "http://arxiv.org/abs/2106.04106": {"title": "A Regression-based Approach to Robust Estimation and Inference for Genetic Covariance", "link": "http://arxiv.org/abs/2106.04106", "description": "Genome-wide association studies (GWAS) have identified thousands of genetic\nvariants associated with complex traits, and some variants are shown to be\nassociated with multiple complex traits. Genetic covariance between two traits\nis defined as the underlying covariance of genetic effects and can be used to\nmeasure the shared genetic architecture. The data used to estimate such a\ngenetic covariance can be from the same group or different groups of\nindividuals, and the traits can be of different types or collected based on\ndifferent study designs. This paper proposes a unified regression-based\napproach to robust estimation and inference for genetic covariance of general\ntraits that may be associated with genetic variants nonlinearly. The asymptotic\nproperties of the proposed estimator are provided and are shown to be robust\nunder certain model mis-specification. Our method under linear working models\nprovides a robust inference for the narrow-sense genetic covariance, even when\nboth linear models are mis-specified. Numerical experiments are performed to\nsupport the theoretical results. Our method is applied to an outbred mice GWAS\ndata set to study the overlapping genetic effects between the behavioral and\nphysiological phenotypes. The real data results reveal interesting genetic\ncovariance among different mice developmental traits."}, "http://arxiv.org/abs/2112.08417": {"title": "Characterization of causal ancestral graphs for time series with latent confounders", "link": "http://arxiv.org/abs/2112.08417", "description": "In this paper, we introduce a novel class of graphical models for\nrepresenting time lag specific causal relationships and independencies of\nmultivariate time series with unobserved confounders. We completely\ncharacterize these graphs and show that they constitute proper subsets of the\ncurrently employed model classes. As we show, from the novel graphs one can\nthus draw stronger causal inferences -- without additional assumptions. We\nfurther introduce a graphical representation of Markov equivalence classes of\nthe novel graphs. This graphical representation contains more causal knowledge\nthan what current state-of-the-art causal discovery algorithms learn."}, "http://arxiv.org/abs/2112.09313": {"title": "Federated Adaptive Causal Estimation (FACE) of Target Treatment Effects", "link": "http://arxiv.org/abs/2112.09313", "description": "Federated learning of causal estimands may greatly improve estimation\nefficiency by leveraging data from multiple study sites, but robustness to\nheterogeneity and model misspecifications is vital for ensuring validity. We\ndevelop a Federated Adaptive Causal Estimation (FACE) framework to incorporate\nheterogeneous data from multiple sites to provide treatment effect estimation\nand inference for a flexibly specified target population of interest. FACE\naccounts for site-level heterogeneity in the distribution of covariates through\ndensity ratio weighting. To safely incorporate source sites and avoid negative\ntransfer, we introduce an adaptive weighting procedure via a penalized\nregression, which achieves both consistency and optimal efficiency. Our\nstrategy is communication-efficient and privacy-preserving, allowing\nparticipating sites to share summary statistics only once with other sites. We\nconduct both theoretical and numerical evaluations of FACE and apply it to\nconduct a comparative effectiveness study of BNT162b2 (Pfizer) and mRNA-1273\n(Moderna) vaccines on COVID-19 outcomes in U.S. veterans using electronic\nhealth records from five VA regional sites. We show that compared to\ntraditional methods, FACE meaningfully increases the precision of treatment\neffect estimates, with reductions in standard errors ranging from $26\\%$ to\n$67\\%$."}, "http://arxiv.org/abs/2208.03246": {"title": "Non-Asymptotic Analysis of Ensemble Kalman Updates: Effective Dimension and Localization", "link": "http://arxiv.org/abs/2208.03246", "description": "Many modern algorithms for inverse problems and data assimilation rely on\nensemble Kalman updates to blend prior predictions with observed data. Ensemble\nKalman methods often perform well with a small ensemble size, which is\nessential in applications where generating each particle is costly. This paper\ndevelops a non-asymptotic analysis of ensemble Kalman updates that rigorously\nexplains why a small ensemble size suffices if the prior covariance has\nmoderate effective dimension due to fast spectrum decay or approximate\nsparsity. We present our theory in a unified framework, comparing several\nimplementations of ensemble Kalman updates that use perturbed observations,\nsquare root filtering, and localization. As part of our analysis, we develop\nnew dimension-free covariance estimation bounds for approximately sparse\nmatrices that may be of independent interest."}, "http://arxiv.org/abs/2307.10972": {"title": "Adaptively Weighted Audits of Instant-Runoff Voting Elections: AWAIRE", "link": "http://arxiv.org/abs/2307.10972", "description": "An election audit is risk-limiting if the audit limits (to a pre-specified\nthreshold) the chance that an erroneous electoral outcome will be certified.\nExtant methods for auditing instant-runoff voting (IRV) elections are either\nnot risk-limiting or require cast vote records (CVRs), the voting system's\nelectronic record of the votes on each ballot. CVRs are not always available,\nfor instance, in jurisdictions that tabulate IRV contests manually.\n\nWe develop an RLA method (AWAIRE) that uses adaptively weighted averages of\ntest supermartingales to efficiently audit IRV elections when CVRs are not\navailable. The adaptive weighting 'learns' an efficient set of hypotheses to\ntest to confirm the election outcome. When accurate CVRs are available, AWAIRE\ncan use them to increase the efficiency to match the performance of existing\nmethods that require CVRs.\n\nWe provide an open-source prototype implementation that can handle elections\nwith up to six candidates. Simulations using data from real elections show that\nAWAIRE is likely to be efficient in practice. We discuss how to extend the\ncomputational approach to handle elections with more candidates.\n\nAdaptively weighted averages of test supermartingales are a general tool,\nuseful beyond election audits to test collections of hypotheses sequentially\nwhile rigorously controlling the familywise error rate."}, "http://arxiv.org/abs/2309.10514": {"title": "Partially Specified Causal Simulations", "link": "http://arxiv.org/abs/2309.10514", "description": "Simulation studies play a key role in the validation of causal inference\nmethods. The simulation results are reliable only if the study is designed\naccording to the promised operational conditions of the method-in-test. Still,\nmany causal inference literature tend to design over-restricted or misspecified\nstudies. In this paper, we elaborate on the problem of improper simulation\ndesign for causal methods and compile a list of desiderata for an effective\nsimulation framework. We then introduce partially randomized causal simulation\n(PARCS), a simulation framework that meets those desiderata. PARCS synthesizes\ndata based on graphical causal models and a wide range of adjustable\nparameters. There is a legible mapping from usual causal assumptions to the\nparameters, thus, users can identify and specify the subset of related\nparameters and randomize the remaining ones to generate a range of complying\ndata-generating processes for their causal method. The result is a more\ncomprehensive and inclusive empirical investigation for causal claims. Using\nPARCS, we reproduce and extend the simulation studies of two well-known causal\ndiscovery and missing data analysis papers to emphasize the necessity of a\nproper simulation design. Our results show that those papers would have\nimproved and extended the findings, had they used PARCS for simulation. The\nframework is implemented as a Python package, too. By discussing the\ncomprehensiveness and transparency of PARCS, we encourage causal inference\nresearchers to utilize it as a standard tool for future works."}}