{"http://arxiv.org/abs/2310.03114": {"title": "Bayesian Parameter Inference for Partially Observed Stochastic Volterra Equations", "link": "http://arxiv.org/abs/2310.03114", "description": "In this article we consider Bayesian parameter inference for a type of\npartially observed stochastic Volterra equation (SVE). SVEs are found in many\nareas such as physics and mathematical finance. In the latter field they can be\nused to represent long memory in unobserved volatility processes. In many cases\nof practical interest, SVEs must be time-discretized and then parameter\ninference is based upon the posterior associated to this time-discretized\nprocess. Based upon recent studies on time-discretization of SVEs (e.g. Richard\net al. 2021), we use Euler-Maruyama methods for the afore-mentioned\ndiscretization. We then show how multilevel Markov chain Monte Carlo (MCMC)\nmethods (Jasra et al. 2018) can be applied in this context. In the examples we\nstudy, we give a proof that shows that the cost to achieve a mean square error\n(MSE) of $\\mathcal{O}(\\epsilon^2)$, $\\epsilon&gt;0$, is\n$\\mathcal{O}(\\epsilon^{-20/9})$. If one uses a single level MCMC method then\nthe cost is $\\mathcal{O}(\\epsilon^{-38/9})$ to achieve the same MSE. We\nillustrate these results in the context of state-space and stochastic\nvolatility models, with the latter applied to real data."}, "http://arxiv.org/abs/2310.03164": {"title": "A Hierarchical Random Effects State-space Model for Modeling Brain Activities from Electroencephalogram Data", "link": "http://arxiv.org/abs/2310.03164", "description": "Mental disorders present challenges in diagnosis and treatment due to their\ncomplex and heterogeneous nature. Electroencephalogram (EEG) has shown promise\nas a potential biomarker for these disorders. However, existing methods for\nanalyzing EEG signals have limitations in addressing heterogeneity and\ncapturing complex brain activity patterns between regions. This paper proposes\na novel random effects state-space model (RESSM) for analyzing large-scale\nmulti-channel resting-state EEG signals, accounting for the heterogeneity of\nbrain connectivities between groups and individual subjects. We incorporate\nmulti-level random effects for temporal dynamical and spatial mapping matrices\nand address nonstationarity so that the brain connectivity patterns can vary\nover time. The model is fitted under a Bayesian hierarchical model framework\ncoupled with a Gibbs sampler. Compared to previous mixed-effects state-space\nmodels, we directly model high-dimensional random effects matrices without\nstructural constraints and tackle the challenge of identifiability. Through\nextensive simulation studies, we demonstrate that our approach yields valid\nestimation and inference. We apply RESSM to a multi-site clinical trial of\nMajor Depressive Disorder (MDD). Our analysis uncovers significant differences\nin resting-state brain temporal dynamics among MDD patients compared to healthy\nindividuals. In addition, we show the subject-level EEG features derived from\nRESSM exhibit a superior predictive value for the heterogeneous treatment\neffect compared to the EEG frequency band power, suggesting the potential of\nEEG as a valuable biomarker for MDD."}, "http://arxiv.org/abs/2310.03258": {"title": "Detecting Electricity Service Equity Issues with Transfer Counterfactual Learning on Large-Scale Outage Datasets", "link": "http://arxiv.org/abs/2310.03258", "description": "Energy justice is a growing area of interest in interdisciplinary energy\nresearch. However, identifying systematic biases in the energy sector remains\nchallenging due to confounding variables, intricate heterogeneity in treatment\neffects, and limited data availability. To address these challenges, we\nintroduce a novel approach for counterfactual causal analysis centered on\nenergy justice. We use subgroup analysis to manage diverse factors and leverage\nthe idea of transfer learning to mitigate data scarcity in each subgroup. In\nour numerical analysis, we apply our method to a large-scale customer-level\npower outage data set and investigate the counterfactual effect of demographic\nfactors, such as income and age of the population, on power outage durations.\nOur results indicate that low-income and elderly-populated areas consistently\nexperience longer power outages, regardless of weather conditions. This points\nto existing biases in the power system and highlights the need for focused\nimprovements in areas with economic challenges."}, "http://arxiv.org/abs/2310.03351": {"title": "Efficiently analyzing large patient registries with Bayesian joint models for longitudinal and time-to-event data", "link": "http://arxiv.org/abs/2310.03351", "description": "The joint modeling of longitudinal and time-to-event outcomes has become a\npopular tool in follow-up studies. However, fitting Bayesian joint models to\nlarge datasets, such as patient registries, can require extended computing\ntimes. To speed up sampling, we divided a patient registry dataset into\nsubsamples, analyzed them in parallel, and combined the resulting Markov chain\nMonte Carlo draws into a consensus distribution. We used a simulation study to\ninvestigate how different consensus strategies perform with joint models. In\nparticular, we compared grouping all draws together with using equal- and\nprecision-weighted averages. We considered scenarios reflecting different\nsample sizes, numbers of data splits, and processor characteristics.\nParallelization of the sampling process substantially decreased the time\nrequired to run the model. We found that the weighted-average consensus\ndistributions for large sample sizes were nearly identical to the target\nposterior distribution. The proposed algorithm has been made available in an R\npackage for joint models, JMbayes2. This work was motivated by the clinical\ninterest in investigating the association between ppFEV1, a commonly measured\nmarker of lung function, and the risk of lung transplant or death, using data\nfrom the US Cystic Fibrosis Foundation Patient Registry (35,153 individuals\nwith 372,366 years of cumulative follow-up). Splitting the registry into five\nsubsamples resulted in an 85\\% decrease in computing time, from 9.22 to 1.39\nhours. Splitting the data and finding a consensus distribution by\nprecision-weighted averaging proved to be a computationally efficient and\nrobust approach to handling large datasets under the joint modeling framework."}, "http://arxiv.org/abs/2310.03521": {"title": "Cutting Feedback in Misspecified Copula Models", "link": "http://arxiv.org/abs/2310.03521", "description": "In copula models the marginal distributions and copula function are specified\nseparately. We treat these as two modules in a modular Bayesian inference\nframework, and propose conducting modified Bayesian inference by ``cutting\nfeedback''. Cutting feedback limits the influence of potentially misspecified\nmodules in posterior inference. We consider two types of cuts. The first limits\nthe influence of a misspecified copula on inference for the marginals, which is\na Bayesian analogue of the popular Inference for Margins (IFM) estimator. The\nsecond limits the influence of misspecified marginals on inference for the\ncopula parameters by using a rank likelihood to define the cut model. We\nestablish that if only one of the modules is misspecified, then the appropriate\ncut posterior gives accurate uncertainty quantification asymptotically for the\nparameters in the other module. Computation of the cut posteriors is difficult,\nand new variational inference methods to do so are proposed. The efficacy of\nthe new methodology is demonstrated using both simulated data and a substantive\nmultivariate time series application from macroeconomic forecasting. In the\nlatter, cutting feedback from misspecified marginals to a 1096 dimension copula\nimproves posterior inference and predictive accuracy greatly, compared to\nconventional Bayesian inference."}, "http://arxiv.org/abs/2310.03630": {"title": "Model-based Clustering for Network Data via a Latent Shrinkage Position Cluster Model", "link": "http://arxiv.org/abs/2310.03630", "description": "Low-dimensional representation and clustering of network data are tasks of\ngreat interest across various fields. Latent position models are routinely used\nfor this purpose by assuming that each node has a location in a low-dimensional\nlatent space, and enabling node clustering. However, these models fall short in\nsimultaneously determining the optimal latent space dimension and the number of\nclusters. Here we introduce the latent shrinkage position cluster model\n(LSPCM), which addresses this limitation. The LSPCM posits a Bayesian\nnonparametric shrinkage prior on the latent positions' variance parameters\nresulting in higher dimensions having increasingly smaller variances, aiding in\nthe identification of dimensions with non-negligible variance. Further, the\nLSPCM assumes the latent positions follow a sparse finite Gaussian mixture\nmodel, allowing for automatic inference on the number of clusters related to\nnon-empty mixture components. As a result, the LSPCM simultaneously infers the\nlatent space dimensionality and the number of clusters, eliminating the need to\nfit and compare multiple models. The performance of the LSPCM is assessed via\nsimulation studies and demonstrated through application to two real Twitter\nnetwork datasets from sporting and political contexts. Open source software is\navailable to promote widespread use of the LSPCM."}, "http://arxiv.org/abs/2310.03722": {"title": "Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance", "link": "http://arxiv.org/abs/2310.03722", "description": "In 1976, Lai constructed a nontrivial confidence sequence for the mean $\\mu$\nof a Gaussian distribution with unknown variance $\\sigma$. Curiously, he\nemployed both an improper (right Haar) mixture over $\\sigma$ and an improper\n(flat) mixture over $\\mu$. Here, we elaborate carefully on the details of his\nconstruction, which use generalized nonintegrable martingales and an extended\nVille's inequality. While this does yield a sequential t-test, it does not\nyield an ``e-process'' (due to the nonintegrability of his martingale). In this\npaper, we develop two new e-processes and confidence sequences for the same\nsetting: one is a test martingale in a reduced filtration, while the other is\nan e-process in the canonical data filtration. These are respectively obtained\nby swapping Lai's flat mixture for a Gaussian mixture, and swapping the right\nHaar mixture over $\\sigma$ with the maximum likelihood estimate under the null,\nas done in universal inference. We also analyze the width of resulting\nconfidence sequences, which have a curious dependence on the error probability\n$\\alpha$. Numerical experiments are provided along the way to compare and\ncontrast the various approaches."}, "http://arxiv.org/abs/2103.10875": {"title": "Scalable Bayesian computation for crossed and nested hierarchical models", "link": "http://arxiv.org/abs/2103.10875", "description": "We develop sampling algorithms to fit Bayesian hierarchical models, the\ncomputational complexity of which scales linearly with the number of\nobservations and the number of parameters in the model. We focus on crossed\nrandom effect and nested multilevel models, which are used ubiquitously in\napplied sciences. The posterior dependence in both classes is sparse: in\ncrossed random effects models it resembles a random graph, whereas in nested\nmultilevel models it is tree-structured. For each class we identify a framework\nfor scalable computation, building on previous work. Methods for crossed models\nare based on extensions of appropriately designed collapsed Gibbs samplers,\nwhere we introduce the idea of local centering; while methods for nested models\nare based on sparse linear algebra and data augmentation. We provide a\ntheoretical analysis of the proposed algorithms in some simplified settings,\nincluding a comparison with previously proposed methodologies and an\naverage-case analysis based on random graph theory. Numerical experiments,\nincluding two challenging real data analyses on predicting electoral results\nand real estate prices, compare with off-the-shelf Hamiltonian Monte Carlo,\ndisplaying drastic improvement in performance."}, "http://arxiv.org/abs/2106.04106": {"title": "A Regression-based Approach to Robust Estimation and Inference for Genetic Covariance", "link": "http://arxiv.org/abs/2106.04106", "description": "Genome-wide association studies (GWAS) have identified thousands of genetic\nvariants associated with complex traits, and some variants are shown to be\nassociated with multiple complex traits. Genetic covariance between two traits\nis defined as the underlying covariance of genetic effects and can be used to\nmeasure the shared genetic architecture. The data used to estimate such a\ngenetic covariance can be from the same group or different groups of\nindividuals, and the traits can be of different types or collected based on\ndifferent study designs. This paper proposes a unified regression-based\napproach to robust estimation and inference for genetic covariance of general\ntraits that may be associated with genetic variants nonlinearly. The asymptotic\nproperties of the proposed estimator are provided and are shown to be robust\nunder certain model mis-specification. Our method under linear working models\nprovides a robust inference for the narrow-sense genetic covariance, even when\nboth linear models are mis-specified. Numerical experiments are performed to\nsupport the theoretical results. Our method is applied to an outbred mice GWAS\ndata set to study the overlapping genetic effects between the behavioral and\nphysiological phenotypes. The real data results reveal interesting genetic\ncovariance among different mice developmental traits."}, "http://arxiv.org/abs/2112.08417": {"title": "Characterization of causal ancestral graphs for time series with latent confounders", "link": "http://arxiv.org/abs/2112.08417", "description": "In this paper, we introduce a novel class of graphical models for\nrepresenting time lag specific causal relationships and independencies of\nmultivariate time series with unobserved confounders. We completely\ncharacterize these graphs and show that they constitute proper subsets of the\ncurrently employed model classes. As we show, from the novel graphs one can\nthus draw stronger causal inferences -- without additional assumptions. We\nfurther introduce a graphical representation of Markov equivalence classes of\nthe novel graphs. This graphical representation contains more causal knowledge\nthan what current state-of-the-art causal discovery algorithms learn."}, "http://arxiv.org/abs/2112.09313": {"title": "Federated Adaptive Causal Estimation (FACE) of Target Treatment Effects", "link": "http://arxiv.org/abs/2112.09313", "description": "Federated learning of causal estimands may greatly improve estimation\nefficiency by leveraging data from multiple study sites, but robustness to\nheterogeneity and model misspecifications is vital for ensuring validity. We\ndevelop a Federated Adaptive Causal Estimation (FACE) framework to incorporate\nheterogeneous data from multiple sites to provide treatment effect estimation\nand inference for a flexibly specified target population of interest. FACE\naccounts for site-level heterogeneity in the distribution of covariates through\ndensity ratio weighting. To safely incorporate source sites and avoid negative\ntransfer, we introduce an adaptive weighting procedure via a penalized\nregression, which achieves both consistency and optimal efficiency. Our\nstrategy is communication-efficient and privacy-preserving, allowing\nparticipating sites to share summary statistics only once with other sites. We\nconduct both theoretical and numerical evaluations of FACE and apply it to\nconduct a comparative effectiveness study of BNT162b2 (Pfizer) and mRNA-1273\n(Moderna) vaccines on COVID-19 outcomes in U.S. veterans using electronic\nhealth records from five VA regional sites. We show that compared to\ntraditional methods, FACE meaningfully increases the precision of treatment\neffect estimates, with reductions in standard errors ranging from $26\\%$ to\n$67\\%$."}, "http://arxiv.org/abs/2208.03246": {"title": "Non-Asymptotic Analysis of Ensemble Kalman Updates: Effective Dimension and Localization", "link": "http://arxiv.org/abs/2208.03246", "description": "Many modern algorithms for inverse problems and data assimilation rely on\nensemble Kalman updates to blend prior predictions with observed data. Ensemble\nKalman methods often perform well with a small ensemble size, which is\nessential in applications where generating each particle is costly. This paper\ndevelops a non-asymptotic analysis of ensemble Kalman updates that rigorously\nexplains why a small ensemble size suffices if the prior covariance has\nmoderate effective dimension due to fast spectrum decay or approximate\nsparsity. We present our theory in a unified framework, comparing several\nimplementations of ensemble Kalman updates that use perturbed observations,\nsquare root filtering, and localization. As part of our analysis, we develop\nnew dimension-free covariance estimation bounds for approximately sparse\nmatrices that may be of independent interest."}, "http://arxiv.org/abs/2307.10972": {"title": "Adaptively Weighted Audits of Instant-Runoff Voting Elections: AWAIRE", "link": "http://arxiv.org/abs/2307.10972", "description": "An election audit is risk-limiting if the audit limits (to a pre-specified\nthreshold) the chance that an erroneous electoral outcome will be certified.\nExtant methods for auditing instant-runoff voting (IRV) elections are either\nnot risk-limiting or require cast vote records (CVRs), the voting system's\nelectronic record of the votes on each ballot. CVRs are not always available,\nfor instance, in jurisdictions that tabulate IRV contests manually.\n\nWe develop an RLA method (AWAIRE) that uses adaptively weighted averages of\ntest supermartingales to efficiently audit IRV elections when CVRs are not\navailable. The adaptive weighting 'learns' an efficient set of hypotheses to\ntest to confirm the election outcome. When accurate CVRs are available, AWAIRE\ncan use them to increase the efficiency to match the performance of existing\nmethods that require CVRs.\n\nWe provide an open-source prototype implementation that can handle elections\nwith up to six candidates. Simulations using data from real elections show that\nAWAIRE is likely to be efficient in practice. We discuss how to extend the\ncomputational approach to handle elections with more candidates.\n\nAdaptively weighted averages of test supermartingales are a general tool,\nuseful beyond election audits to test collections of hypotheses sequentially\nwhile rigorously controlling the familywise error rate."}, "http://arxiv.org/abs/2309.10514": {"title": "Partially Specified Causal Simulations", "link": "http://arxiv.org/abs/2309.10514", "description": "Simulation studies play a key role in the validation of causal inference\nmethods. The simulation results are reliable only if the study is designed\naccording to the promised operational conditions of the method-in-test. Still,\nmany causal inference literature tend to design over-restricted or misspecified\nstudies. In this paper, we elaborate on the problem of improper simulation\ndesign for causal methods and compile a list of desiderata for an effective\nsimulation framework. We then introduce partially randomized causal simulation\n(PARCS), a simulation framework that meets those desiderata. PARCS synthesizes\ndata based on graphical causal models and a wide range of adjustable\nparameters. There is a legible mapping from usual causal assumptions to the\nparameters, thus, users can identify and specify the subset of related\nparameters and randomize the remaining ones to generate a range of complying\ndata-generating processes for their causal method. The result is a more\ncomprehensive and inclusive empirical investigation for causal claims. Using\nPARCS, we reproduce and extend the simulation studies of two well-known causal\ndiscovery and missing data analysis papers to emphasize the necessity of a\nproper simulation design. Our results show that those papers would have\nimproved and extended the findings, had they used PARCS for simulation. The\nframework is implemented as a Python package, too. By discussing the\ncomprehensiveness and transparency of PARCS, we encourage causal inference\nresearchers to utilize it as a standard tool for future works."}, "http://arxiv.org/abs/2310.03776": {"title": "Significance of the negative binomial distribution in multiplicity phenomena", "link": "http://arxiv.org/abs/2310.03776", "description": "The negative binomial distribution (NBD) has been theorized to express a\nscale-invariant property of many-body systems and has been consistently shown\nto outperform other statistical models in both describing the multiplicity of\nquantum-scale events in particle collision experiments and predicting the\nprevalence of cosmological observables, such as the number of galaxies in a\nregion of space. Despite its widespread applicability and empirical success in\nthese contexts, a theoretical justification for the NBD from first principles\nhas remained elusive for fifty years. The accuracy of the NBD in modeling\nhadronic, leptonic, and semileptonic processes is suggestive of a highly\ngeneral principle, which is yet to be understood. This study demonstrates that\na statistical event of the NBD can in fact be derived in a general context via\nthe dynamical equations of a canonical ensemble of particles in Minkowski\nspace. These results describe a fundamental feature of many-body systems that\nis consistent with data from the ALICE and ATLAS experiments and provides an\nexplanation for the emergence of the NBD in these multiplicity observations.\nTwo methods are used to derive this correspondence: the Feynman path integral\nand a hypersurface parametrization of a propagating ensemble."}, "http://arxiv.org/abs/2310.04030": {"title": "Robust inference with GhostKnockoffs in genome-wide association studies", "link": "http://arxiv.org/abs/2310.04030", "description": "Genome-wide association studies (GWASs) have been extensively adopted to\ndepict the underlying genetic architecture of complex diseases. Motivated by\nGWASs' limitations in identifying small effect loci to understand complex\ntraits' polygenicity and fine-mapping putative causal variants from proxy ones,\nwe propose a knockoff-based method which only requires summary statistics from\nGWASs and demonstrate its validity in the presence of relatedness. We show that\nGhostKnockoffs inference is robust to its input Z-scores as long as they are\nfrom valid marginal association tests and their correlations are consistent\nwith the correlations among the corresponding genetic variants. The property\ngeneralizes GhostKnockoffs to other GWASs settings, such as the meta-analysis\nof multiple overlapping studies and studies based on association test\nstatistics deviated from score tests. We demonstrate GhostKnockoffs'\nperformance using empirical simulation and a meta-analysis of nine European\nancestral genome-wide association studies and whole exome/genome sequencing\nstudies. Both results demonstrate that GhostKnockoffs identify more putative\ncausal variants with weak genotype-phenotype associations that are missed by\nconventional GWASs."}, "http://arxiv.org/abs/2310.04082": {"title": "An energy-based model approach to rare event probability estimation", "link": "http://arxiv.org/abs/2310.04082", "description": "The estimation of rare event probabilities plays a pivotal role in diverse\nfields. Our aim is to determine the probability of a hazard or system failure\noccurring when a quantity of interest exceeds a critical value. In our\napproach, the distribution of the quantity of interest is represented by an\nenergy density, characterized by a free energy function. To efficiently\nestimate the free energy, a bias potential is introduced. Using concepts from\nenergy-based models (EBM), this bias potential is optimized such that the\ncorresponding probability density function approximates a pre-defined\ndistribution targeting the failure region of interest. Given the optimal bias\npotential, the free energy function and the rare event probability of interest\ncan be determined. The approach is applicable not just in traditional rare\nevent settings where the variable upon which the quantity of interest relies\nhas a known distribution, but also in inversion settings where the variable\nfollows a posterior distribution. By combining the EBM approach with a Stein\ndiscrepancy-based stopping criterion, we aim for a balanced accuracy-efficiency\ntrade-off. Furthermore, we explore both parametric and non-parametric\napproaches for the bias potential, with the latter eliminating the need for\nchoosing a particular parameterization, but depending strongly on the accuracy\nof the kernel density estimate used in the optimization process. Through three\nillustrative test cases encompassing both traditional and inversion settings,\nwe show that the proposed EBM approach, when properly configured, (i) allows\nstable and efficient estimation of rare event probabilities and (ii) compares\nfavorably against subset sampling approaches."}, "http://arxiv.org/abs/2310.04165": {"title": "When Composite Likelihood Meets Stochastic Approximation", "link": "http://arxiv.org/abs/2310.04165", "description": "A composite likelihood is an inference function derived by multiplying a set\nof likelihood components. This approach provides a flexible framework for\ndrawing inference when the likelihood function of a statistical model is\ncomputationally intractable. While composite likelihood has computational\nadvantages, it can still be demanding when dealing with numerous likelihood\ncomponents and a large sample size. This paper tackles this challenge by\nemploying an approximation of the conventional composite likelihood estimator,\nwhich is derived from an optimization procedure relying on stochastic\ngradients. This novel estimator is shown to be asymptotically normally\ndistributed around the true parameter. In particular, based on the relative\ndivergent rate of the sample size and the number of iterations of the\noptimization, the variance of the limiting distribution is shown to compound\nfor two sources of uncertainty: the sampling variability of the data and the\noptimization noise, with the latter depending on the sampling distribution used\nto construct the stochastic gradients. The advantages of the proposed framework\nare illustrated through simulation studies on two working examples: an Ising\nmodel for binary data and a gamma frailty model for count data. Finally, a\nreal-data application is presented, showing its effectiveness in a large-scale\nmental health survey."}, "http://arxiv.org/abs/1904.06340": {"title": "A Composite Likelihood-based Approach for Change-point Detection in Spatio-temporal Processes", "link": "http://arxiv.org/abs/1904.06340", "description": "This paper develops a unified and computationally efficient method for\nchange-point estimation along the time dimension in a non-stationary\nspatio-temporal process. By modeling a non-stationary spatio-temporal process\nas a piecewise stationary spatio-temporal process, we consider simultaneous\nestimation of the number and locations of change-points, and model parameters\nin each segment. A composite likelihood-based criterion is developed for\nchange-point and parameters estimation. Under the framework of increasing\ndomain asymptotics, theoretical results including consistency and distribution\nof the estimators are derived under mild conditions. In contrast to classical\nresults in fixed dimensional time series that the localization error of\nchange-point estimator is $O_{p}(1)$, exact recovery of true change-points can\nbe achieved in the spatio-temporal setting. More surprisingly, the consistency\nof change-point estimation can be achieved without any penalty term in the\ncriterion function. In addition, we further establish consistency of the number\nand locations of the change-point estimator under the infill asymptotics\nframework where the time domain is increasing while the spatial sampling domain\nis fixed. A computationally efficient pruned dynamic programming algorithm is\ndeveloped for the challenging criterion optimization problem. Extensive\nsimulation studies and an application to U.S. precipitation data are provided\nto demonstrate the effectiveness and practicality of the proposed method."}, "http://arxiv.org/abs/2201.12936": {"title": "Pigeonhole Design: Balancing Sequential Experiments from an Online Matching Perspective", "link": "http://arxiv.org/abs/2201.12936", "description": "Practitioners and academics have long appreciated the benefits of covariate\nbalancing when they conduct randomized experiments. For web-facing firms\nrunning online A/B tests, however, it still remains challenging in balancing\ncovariate information when experimental subjects arrive sequentially. In this\npaper, we study an online experimental design problem, which we refer to as the\n\"Online Blocking Problem.\" In this problem, experimental subjects with\nheterogeneous covariate information arrive sequentially and must be immediately\nassigned into either the control or the treated group. The objective is to\nminimize the total discrepancy, which is defined as the minimum weight perfect\nmatching between the two groups. To solve this problem, we propose a randomized\ndesign of experiment, which we refer to as the \"Pigeonhole Design.\" The\npigeonhole design first partitions the covariate space into smaller spaces,\nwhich we refer to as pigeonholes, and then, when the experimental subjects\narrive at each pigeonhole, balances the number of control and treated subjects\nfor each pigeonhole. We analyze the theoretical performance of the pigeonhole\ndesign and show its effectiveness by comparing against two well-known benchmark\ndesigns: the match-pair design and the completely randomized design. We\nidentify scenarios when the pigeonhole design demonstrates more benefits over\nthe benchmark design. To conclude, we conduct extensive simulations using\nYahoo! data to show a 10.2% reduction in variance if we use the pigeonhole\ndesign to estimate the average treatment effect."}, "http://arxiv.org/abs/2208.00137": {"title": "Efficient estimation and inference for the signed $\\beta$-model in directed signed networks", "link": "http://arxiv.org/abs/2208.00137", "description": "This paper proposes a novel signed $\\beta$-model for directed signed network,\nwhich is frequently encountered in application domains but largely neglected in\nliterature. The proposed signed $\\beta$-model decomposes a directed signed\nnetwork as the difference of two unsigned networks and embeds each node with\ntwo latent factors for in-status and out-status. The presence of negative edges\nleads to a non-concave log-likelihood, and a one-step estimation algorithm is\ndeveloped to facilitate parameter estimation, which is efficient both\ntheoretically and computationally. We also develop an inferential procedure for\npairwise and multiple node comparisons under the signed $\\beta$-model, which\nfills the void of lacking uncertainty quantification for node ranking.\nTheoretical results are established for the coverage probability of confidence\ninterval, as well as the false discovery rate (FDR) control for multiple node\ncomparison. The finite sample performance of the signed $\\beta$-model is also\nexamined through extensive numerical experiments on both synthetic and\nreal-life networks."}, "http://arxiv.org/abs/2208.08401": {"title": "Conformal Inference for Online Prediction with Arbitrary Distribution Shifts", "link": "http://arxiv.org/abs/2208.08401", "description": "We consider the problem of forming prediction sets in an online setting where\nthe distribution generating the data is allowed to vary over time. Previous\napproaches to this problem suffer from over-weighting historical data and thus\nmay fail to quickly react to the underlying dynamics. Here we correct this\nissue and develop a novel procedure with provably small regret over all local\ntime intervals of a given width. We achieve this by modifying the adaptive\nconformal inference (ACI) algorithm of Gibbs and Cand\\`{e}s (2021) to contain\nan additional step in which the step-size parameter of ACI's gradient descent\nupdate is tuned over time. Crucially, this means that unlike ACI, which\nrequires knowledge of the rate of change of the data-generating mechanism, our\nnew procedure is adaptive to both the size and type of the distribution shift.\nOur methods are highly flexible and can be used in combination with any\nbaseline predictive algorithm that produces point estimates or estimated\nquantiles of the target without the need for distributional assumptions. We\ntest our techniques on two real-world datasets aimed at predicting stock market\nvolatility and COVID-19 case counts and find that they are robust and adaptive\nto real-world distribution shifts."}, "http://arxiv.org/abs/2303.01031": {"title": "Identifiability and Consistent Estimation of the Gaussian Chain Graph Model", "link": "http://arxiv.org/abs/2303.01031", "description": "The chain graph model admits both undirected and directed edges in one graph,\nwhere symmetric conditional dependencies are encoded via undirected edges and\nasymmetric causal relations are encoded via directed edges. Though frequently\nencountered in practice, the chain graph model has been largely under\ninvestigated in literature, possibly due to the lack of identifiability\nconditions between undirected and directed edges. In this paper, we first\nestablish a set of novel identifiability conditions for the Gaussian chain\ngraph model, exploiting a low rank plus sparse decomposition of the precision\nmatrix. Further, an efficient learning algorithm is built upon the\nidentifiability conditions to fully recover the chain graph structure.\nTheoretical analysis on the proposed method is conducted, assuring its\nasymptotic consistency in recovering the exact chain graph structure. The\nadvantage of the proposed method is also supported by numerical experiments on\nboth simulated examples and a real application on the Standard &amp; Poor 500 index\ndata."}, "http://arxiv.org/abs/2305.10817": {"title": "Robust inference of causality in high-dimensional dynamical processes from the Information Imbalance of distance ranks", "link": "http://arxiv.org/abs/2305.10817", "description": "We introduce an approach which allows detecting causal relationships between\nvariables for which the time evolution is available. Causality is assessed by a\nvariational scheme based on the Information Imbalance of distance ranks, a\nstatistical test capable of inferring the relative information content of\ndifferent distance measures. We test whether the predictability of a putative\ndriven system Y can be improved by incorporating information from a potential\ndriver system X, without making assumptions on the underlying dynamics and\nwithout the need to compute probability densities of the dynamic variables.\nThis framework makes causality detection possible even for high-dimensional\nsystems where only few of the variables are known or measured. Benchmark tests\non coupled chaotic dynamical systems demonstrate that our approach outperforms\nother model-free causality detection methods, successfully handling both\nunidirectional and bidirectional couplings. We also show that the method can be\nused to robustly detect causality in human electroencephalography data."}, "http://arxiv.org/abs/2309.06264": {"title": "Spectral clustering algorithm for the allometric extension model", "link": "http://arxiv.org/abs/2309.06264", "description": "The spectral clustering algorithm is often used as a binary clustering method\nfor unclassified data by applying the principal component analysis. To study\ntheoretical properties of the algorithm, the assumption of conditional\nhomoscedasticity is often supposed in existing studies. However, this\nassumption is restrictive and often unrealistic in practice. Therefore, in this\npaper, we consider the allometric extension model, that is, the directions of\nthe first eigenvectors of two covariance matrices and the direction of the\ndifference of two mean vectors coincide, and we provide a non-asymptotic bound\nof the error probability of the spectral clustering algorithm for the\nallometric extension model. As a byproduct of the result, we obtain the\nconsistency of the clustering method in high-dimensional settings."}, "http://arxiv.org/abs/2309.12833": {"title": "Model-based causal feature selection for general response types", "link": "http://arxiv.org/abs/2309.12833", "description": "Discovering causal relationships from observational data is a fundamental yet\nchallenging task. Invariant causal prediction (ICP, Peters et al., 2016) is a\nmethod for causal feature selection which requires data from heterogeneous\nsettings and exploits that causal models are invariant. ICP has been extended\nto general additive noise models and to nonparametric settings using\nconditional independence tests. However, the latter often suffer from low power\n(or poor type I error control) and additive noise models are not suitable for\napplications in which the response is not measured on a continuous scale, but\nreflects categories or counts. Here, we develop transformation-model (TRAM)\nbased ICP, allowing for continuous, categorical, count-type, and\nuninformatively censored responses (these model classes, generally, do not\nallow for identifiability when there is no exogenous heterogeneity). As an\ninvariance test, we propose TRAM-GCM based on the expected conditional\ncovariance between environments and score residuals with uniform asymptotic\nlevel guarantees. For the special case of linear shift TRAMs, we also consider\nTRAM-Wald, which tests invariance based on the Wald statistic. We provide an\nopen-source R package 'tramicp' and evaluate our approach on simulated data and\nin a case study investigating causal features of survival in critically ill\npatients."}, "http://arxiv.org/abs/2310.04452": {"title": "Short text classification with machine learning in the social sciences: The case of climate change on Twitter", "link": "http://arxiv.org/abs/2310.04452", "description": "To analyse large numbers of texts, social science researchers are\nincreasingly confronting the challenge of text classification. When manual\nlabeling is not possible and researchers have to find automatized ways to\nclassify texts, computer science provides a useful toolbox of machine-learning\nmethods whose performance remains understudied in the social sciences. In this\narticle, we compare the performance of the most widely used text classifiers by\napplying them to a typical research scenario in social science research: a\nrelatively small labeled dataset with infrequent occurrence of categories of\ninterest, which is a part of a large unlabeled dataset. As an example case, we\nlook at Twitter communication regarding climate change, a topic of increasing\nscholarly interest in interdisciplinary social science research. Using a novel\ndataset including 5,750 tweets from various international organizations\nregarding the highly ambiguous concept of climate change, we evaluate the\nperformance of methods in automatically classifying tweets based on whether\nthey are about climate change or not. In this context, we highlight two main\nfindings. First, supervised machine-learning methods perform better than\nstate-of-the-art lexicons, in particular as class balance increases. Second,\ntraditional machine-learning methods, such as logistic regression and random\nforest, perform similarly to sophisticated deep-learning methods, whilst\nrequiring much less training time and computational resources. The results have\nimportant implications for the analysis of short texts in social science\nresearch."}, "http://arxiv.org/abs/2310.04563": {"title": "Modeling the Risk of In-Person Instruction during the COVID-19 Pandemic", "link": "http://arxiv.org/abs/2310.04563", "description": "During the COVID-19 pandemic, implementing in-person indoor instruction in a\nsafe manner was a high priority for universities nationwide. To support this\neffort at the University, we developed a mathematical model for estimating the\nrisk of SARS-CoV-2 transmission in university classrooms. This model was used\nto design a safe classroom environment at the University during the COVID-19\npandemic that supported the higher occupancy levels needed to match\npre-pandemic numbers of in-person courses, despite a limited number of large\nclassrooms. A retrospective analysis at the end of the semester confirmed the\nmodel's assessment that the proposed classroom configuration would be safe. Our\nframework is generalizable and was also used to support reopening decisions at\nStanford University. In addition, our methods are flexible; our modeling\nframework was repurposed to plan for large university events and gatherings. We\nfound that our approach and methods work in a wide range of indoor settings and\ncould be used to support reopening planning across various industries, from\nsecondary schools to movie theaters and restaurants."}, "http://arxiv.org/abs/2310.04578": {"title": "TNDDR: Efficient and doubly robust estimation of COVID-19 vaccine effectiveness under the test-negative design", "link": "http://arxiv.org/abs/2310.04578", "description": "While the test-negative design (TND), which is routinely used for monitoring\nseasonal flu vaccine effectiveness (VE), has recently become integral to\nCOVID-19 vaccine surveillance, it is susceptible to selection bias due to\noutcome-dependent sampling. Some studies have addressed the identifiability and\nestimation of causal parameters under the TND, but efficiency bounds for\nnonparametric estimators of the target parameter under the unconfoundedness\nassumption have not yet been investigated. We propose a one-step doubly robust\nand locally efficient estimator called TNDDR (TND doubly robust), which\nutilizes sample splitting and can incorporate machine learning techniques to\nestimate the nuisance functions. We derive the efficient influence function\n(EIF) for the marginal expectation of the outcome under a vaccination\nintervention, explore the von Mises expansion, and establish the conditions for\n$\\sqrt{n}-$consistency, asymptotic normality and double robustness of TNDDR.\nThe proposed TNDDR is supported by both theoretical and empirical\njustifications, and we apply it to estimate COVID-19 VE in an administrative\ndataset of community-dwelling older people (aged $\\geq 60$y) in the province of\nQu\\'ebec, Canada."}, "http://arxiv.org/abs/2310.04660": {"title": "Balancing Weights for Causal Inference in Observational Factorial Studies", "link": "http://arxiv.org/abs/2310.04660", "description": "Many scientific questions in biomedical, environmental, and psychological\nresearch involve understanding the impact of multiple factors on outcomes.\nWhile randomized factorial experiments are ideal for this purpose,\nrandomization is infeasible in many empirical studies. Therefore, investigators\noften rely on observational data, where drawing reliable causal inferences for\nmultiple factors remains challenging. As the number of treatment combinations\ngrows exponentially with the number of factors, some treatment combinations can\nbe rare or even missing by chance in observed data, further complicating\nfactorial effects estimation. To address these challenges, we propose a novel\nweighting method tailored to observational studies with multiple factors. Our\napproach uses weighted observational data to emulate a randomized factorial\nexperiment, enabling simultaneous estimation of the effects of multiple factors\nand their interactions. Our investigations reveal a crucial nuance: achieving\nbalance among covariates, as in single-factor scenarios, is necessary but\ninsufficient for unbiasedly estimating factorial effects. Our findings suggest\nthat balancing the factors is also essential in multi-factor settings.\nMoreover, we extend our weighting method to handle missing treatment\ncombinations in observed data. Finally, we study the asymptotic behavior of the\nnew weighting estimators and propose a consistent variance estimator, providing\nreliable inferences on factorial effects in observational studies."}, "http://arxiv.org/abs/2310.04709": {"title": "Time-dependent mediators in survival analysis: Graphical representation of causal assumptions", "link": "http://arxiv.org/abs/2310.04709", "description": "We study time-dependent mediators in survival analysis using a treatment\nseparation approach due to Didelez [2019] and based on earlier work by Robins\nand Richardson [2011]. This approach avoids nested counterfactuals and\ncrossworld assumptions which are otherwise common in mediation analysis. The\ncausal model of treatment, mediators, covariates, confounders and outcome is\nrepresented by causal directed acyclic graphs (DAGs). However, the DAGs tend to\nbe very complex when we have measurements at a large number of time points. We\ntherefore suggest using so-called rolled graphs in which a node represents an\nentire coordinate process instead of a single random variable, leading us to\nfar simpler graphical representations. The rolled graphs are not necessarily\nacyclic; they can be analyzed by $\\delta$-separation which is the appropriate\ngraphical separation criterion in this class of graphs and analogous to\n$d$-separation. In particular, $\\delta$-separation is a graphical tool for\nevaluating if the conditions of the mediation analysis are met or if unmeasured\nconfounders influence the estimated effects. We also state a mediational\ng-formula. This is similar to the approach in Vansteelandt et al. [2019]\nalthough that paper has a different conceptual basis. Finally, we apply this\nframework to a statistical model based on a Cox model with an added treatment\neffect.survival analysis; mediation; causal inference; graphical models; local\nindependence graphs"}, "http://arxiv.org/abs/2310.04853": {"title": "On changepoint detection in functional data using empirical energy distance", "link": "http://arxiv.org/abs/2310.04853", "description": "We propose a novel family of test statistics to detect the presence of\nchangepoints in a sequence of dependent, possibly multivariate,\nfunctional-valued observations. Our approach allows to test for a very general\nclass of changepoints, including the \"classical\" case of changes in the mean,\nand even changes in the whole distribution. Our statistics are based on a\ngeneralisation of the empirical energy distance; we propose weighted\nfunctionals of the energy distance process, which are designed in order to\nenhance the ability to detect breaks occurring at sample endpoints. The\nlimiting distribution of the maximally selected version of our statistics\nrequires only the computation of the eigenvalues of the covariance function,\nthus being readily implementable in the most commonly employed packages, e.g.\nR. We show that, under the alternative, our statistics are able to detect\nchangepoints occurring even very close to the beginning/end of the sample. In\nthe presence of multiple changepoints, we propose a binary segmentation\nalgorithm to estimate the number of breaks and the locations thereof.\nSimulations show that our procedures work very well in finite samples. We\ncomplement our theory with applications to financial and temperature data."}, "http://arxiv.org/abs/2310.04919": {"title": "The Conditional Prediction Function: A Novel Technique to Control False Discovery Rate for Complex Models", "link": "http://arxiv.org/abs/2310.04919", "description": "In modern scientific research, the objective is often to identify which\nvariables are associated with an outcome among a large class of potential\npredictors. This goal can be achieved by selecting variables in a manner that\ncontrols the the false discovery rate (FDR), the proportion of irrelevant\npredictors among the selections. Knockoff filtering is a cutting-edge approach\nto variable selection that provides FDR control. Existing knockoff statistics\nfrequently employ linear models to assess relationships between features and\nthe response, but the linearity assumption is often violated in real world\napplications. This may result in poor power to detect truly prognostic\nvariables. We introduce a knockoff statistic based on the conditional\nprediction function (CPF), which can pair with state-of-art machine learning\npredictive models, such as deep neural networks. The CPF statistics can capture\nthe nonlinear relationships between predictors and outcomes while also\naccounting for correlation between features. We illustrate the capability of\nthe CPF statistics to provide superior power over common knockoff statistics\nwith continuous, categorical, and survival outcomes using repeated simulations.\nKnockoff filtering with the CPF statistics is demonstrated using (1) a\nresidential building dataset to select predictors for the actual sales prices\nand (2) the TCGA dataset to select genes that are correlated with disease\nstaging in lung cancer patients."}, "http://arxiv.org/abs/2310.04924": {"title": "Markov Chain Monte Carlo Significance Tests", "link": "http://arxiv.org/abs/2310.04924", "description": "Markov chain Monte Carlo significance tests were first introduced by Besag\nand Clifford in [4]. These methods produce statistical valid p-values in\nproblems where sampling from the null hypotheses is intractable. We give an\noverview of the methods of Besag and Clifford and some recent developments. A\nrange of examples and applications are discussed."}, "http://arxiv.org/abs/2310.04934": {"title": "UBSea: A Unified Community Detection Framework", "link": "http://arxiv.org/abs/2310.04934", "description": "Detecting communities in networks and graphs is an important task across many\ndisciplines such as statistics, social science and engineering. There are\ngenerally three different kinds of mixing patterns for the case of two\ncommunities: assortative mixing, disassortative mixing and core-periphery\nstructure. Modularity optimization is a classical way for fitting network\nmodels with communities. However, it can only deal with assortative mixing and\ndisassortative mixing when the mixing pattern is known and fails to discover\nthe core-periphery structure. In this paper, we extend modularity in a\nstrategic way and propose a new framework based on Unified Bigroups Standadized\nEdge-count Analysis (UBSea). It can address all the formerly mentioned\ncommunity mixing structures. In addition, this new framework is able to\nautomatically choose the mixing type to fit the networks. Simulation studies\nshow that the new framework has superb performance in a wide range of settings\nunder the stochastic block model and the degree-corrected stochastic block\nmodel. We show that the new approach produces consistent estimate of the\ncommunities under a suitable signal-to-noise-ratio condition, for the case of a\nblock model with two communities, for both undirected and directed networks.\nThe new method is illustrated through applications to several real-world\ndatasets."}, "http://arxiv.org/abs/2310.05049": {"title": "On Estimation of Optimal Dynamic Treatment Regimes with Multiple Treatments for Survival Data-With Application to Colorectal Cancer Study", "link": "http://arxiv.org/abs/2310.05049", "description": "Dynamic treatment regimes (DTR) are sequential decision rules corresponding\nto several stages of intervention. Each rule maps patients' covariates to\noptional treatments. The optimal dynamic treatment regime is the one that\nmaximizes the mean outcome of interest if followed by the overall population.\nMotivated by a clinical study on advanced colorectal cancer with traditional\nChinese medicine, we propose a censored C-learning (CC-learning) method to\nestimate the dynamic treatment regime with multiple treatments using survival\ndata. To address the challenges of multiple stages with right censoring, we\nmodify the backward recursion algorithm in order to adapt to the flexible\nnumber and timing of treatments. For handling the problem of multiple\ntreatments, we propose a framework from the classification perspective by\ntransferring the problem of optimization with multiple treatment comparisons\ninto an example-dependent cost-sensitive classification problem. With\nclassification and regression tree (CART) as the classifier, the CC-learning\nmethod can produce an estimated optimal DTR with good interpretability. We\ntheoretically prove the optimality of our method and numerically evaluate its\nfinite sample performances through simulation. With the proposed method, we\nidentify the interpretable tree treatment regimes at each stage for the\nadvanced colorectal cancer treatment data from Xiyuan Hospital."}, "http://arxiv.org/abs/2310.05151": {"title": "Sequential linear regression for conditional mean imputation of longitudinal continuous outcomes under reference-based assumptions", "link": "http://arxiv.org/abs/2310.05151", "description": "In clinical trials of longitudinal continuous outcomes, reference based\nimputation (RBI) has commonly been applied to handle missing outcome data in\nsettings where the estimand incorporates the effects of intercurrent events,\ne.g. treatment discontinuation. RBI was originally developed in the multiple\nimputation framework, however recently conditional mean imputation (CMI)\ncombined with the jackknife estimator of the standard error was proposed as a\nway to obtain deterministic treatment effect estimates and correct frequentist\ninference. For both multiple and CMI, a mixed model for repeated measures\n(MMRM) is often used for the imputation model, but this can be computationally\nintensive to fit to multiple data sets (e.g. the jackknife samples) and lead to\nconvergence issues with complex MMRM models with many parameters. Therefore, a\nstep-wise approach based on sequential linear regression (SLR) of the outcomes\nat each visit was developed for the imputation model in the multiple imputation\nframework, but similar developments in the CMI framework are lacking. In this\narticle, we fill this gap in the literature by proposing a SLR approach to\nimplement RBI in the CMI framework, and justify its validity using theoretical\nresults and simulations. We also illustrate our proposal on a real data\napplication."}, "http://arxiv.org/abs/2310.05398": {"title": "Statistical Inference for Modulation Index in Phase-Amplitude Coupling", "link": "http://arxiv.org/abs/2310.05398", "description": "Phase-amplitude coupling is a phenomenon observed in several neurological\nprocesses, where the phase of one signal modulates the amplitude of another\nsignal with a distinct frequency. The modulation index (MI) is a common\ntechnique used to quantify this interaction by assessing the Kullback-Leibler\ndivergence between a uniform distribution and the empirical conditional\ndistribution of amplitudes with respect to the phases of the observed signals.\nThe uniform distribution is an ideal representation that is expected to appear\nunder the absence of coupling. However, it does not reflect the statistical\nproperties of coupling values caused by random chance. In this paper, we\npropose a statistical framework for evaluating the significance of an observed\nMI value based on a null hypothesis that a MI value can be entirely explained\nby chance. Significance is obtained by comparing the value with a reference\ndistribution derived under the null hypothesis of independence (i.e., no\ncoupling) between signals. We derived a closed-form distribution of this null\nmodel, resulting in a scaled beta distribution. To validate the efficacy of our\nproposed framework, we conducted comprehensive Monte Carlo simulations,\nassessing the significance of MI values under various experimental scenarios,\nincluding amplitude modulation, trains of spikes, and sequences of\nhigh-frequency oscillations. Furthermore, we corroborated the reliability of\nour model by comparing its statistical significance thresholds with reported\nvalues from other research studies conducted under different experimental\nsettings. Our method offers several advantages such as meta-analysis\nreliability, simplicity and computational efficiency, as it provides p-values\nand significance levels without resorting to generating surrogate data through\nsampling procedures."}, "http://arxiv.org/abs/2310.05526": {"title": "Projecting infinite time series graphs to finite marginal graphs using number theory", "link": "http://arxiv.org/abs/2310.05526", "description": "In recent years, a growing number of method and application works have\nadapted and applied the causal-graphical-model framework to time series data.\nMany of these works employ time-resolved causal graphs that extend infinitely\ninto the past and future and whose edges are repetitive in time, thereby\nreflecting the assumption of stationary causal relationships. However, most\nresults and algorithms from the causal-graphical-model framework are not\ndesigned for infinite graphs. In this work, we develop a method for projecting\ninfinite time series graphs with repetitive edges to marginal graphical models\non a finite time window. These finite marginal graphs provide the answers to\n$m$-separation queries with respect to the infinite graph, a task that was\npreviously unresolved. Moreover, we argue that these marginal graphs are useful\nfor causal discovery and causal effect estimation in time series, effectively\nenabling to apply results developed for finite graphs to the infinite graphs.\nThe projection procedure relies on finding common ancestors in the\nto-be-projected graph and is, by itself, not new. However, the projection\nprocedure has not yet been algorithmically implemented for time series graphs\nsince in these infinite graphs there can be infinite sets of paths that might\ngive rise to common ancestors. We solve the search over these possibly infinite\nsets of paths by an intriguing combination of path-finding techniques for\nfinite directed graphs and solution theory for linear Diophantine equations. By\nproviding an algorithm that carries out the projection, our paper makes an\nimportant step towards a theoretically-grounded and method-agnostic\ngeneralization of a range of causal inference methods and results to time\nseries."}, "http://arxiv.org/abs/2310.05539": {"title": "Testing High-Dimensional Mediation Effect with Arbitrary Exposure-Mediator Coefficients", "link": "http://arxiv.org/abs/2310.05539", "description": "In response to the unique challenge created by high-dimensional mediators in\nmediation analysis, this paper presents a novel procedure for testing the\nnullity of the mediation effect in the presence of high-dimensional mediators.\nThe procedure incorporates two distinct features. Firstly, the test remains\nvalid under all cases of the composite null hypothesis, including the\nchallenging scenario where both exposure-mediator and mediator-outcome\ncoefficients are zero. Secondly, it does not impose structural assumptions on\nthe exposure-mediator coefficients, thereby allowing for an arbitrarily strong\nexposure-mediator relationship. To the best of our knowledge, the proposed test\nis the first of its kind to provably possess these two features in\nhigh-dimensional mediation analysis. The validity and consistency of the\nproposed test are established, and its numerical performance is showcased\nthrough simulation studies. The application of the proposed test is\ndemonstrated by examining the mediation effect of DNA methylation between\nsmoking status and lung cancer development."}, "http://arxiv.org/abs/2310.05548": {"title": "Cokrig-and-Regress for Spatially Misaligned Environmental Data", "link": "http://arxiv.org/abs/2310.05548", "description": "Spatially misaligned data, where the response and covariates are observed at\ndifferent spatial locations, commonly arise in many environmental studies. Much\nof the statistical literature on handling spatially misaligned data has been\ndevoted to the case of a single covariate and a linear relationship between the\nresponse and this covariate. Motivated by spatially misaligned data collected\non air pollution and weather in China, we propose a cokrig-and-regress (CNR)\nmethod to estimate spatial regression models involving multiple covariates and\npotentially non-linear associations. The CNR estimator is constructed by\nreplacing the unobserved covariates (at the response locations) by their\ncokriging predictor derived from the observed but misaligned covariates under a\nmultivariate Gaussian assumption, where a generalized Kronecker product\ncovariance is used to account for spatial correlations within and between\ncovariates. A parametric bootstrap approach is employed to bias-correct the CNR\nestimates of the spatial covariance parameters and for uncertainty\nquantification. Simulation studies demonstrate that CNR outperforms several\nexisting methods for handling spatially misaligned data, such as\nnearest-neighbor interpolation. Applying CNR to the spatially misaligned air\npollution and weather data in China reveals a number of non-linear\nrelationships between PM$_{2.5}$ concentration and several meteorological\ncovariates."}, "http://arxiv.org/abs/2310.05622": {"title": "A neutral comparison of statistical methods for time-to-event analyses under non-proportional hazards", "link": "http://arxiv.org/abs/2310.05622", "description": "While well-established methods for time-to-event data are available when the\nproportional hazards assumption holds, there is no consensus on the best\ninferential approach under non-proportional hazards (NPH). However, a wide\nrange of parametric and non-parametric methods for testing and estimation in\nthis scenario have been proposed. To provide recommendations on the statistical\nanalysis of clinical trials where non proportional hazards are expected, we\nconducted a comprehensive simulation study under different scenarios of\nnon-proportional hazards, including delayed onset of treatment effect, crossing\nhazard curves, subgroups with different treatment effect and changing hazards\nafter disease progression. We assessed type I error rate control, power and\nconfidence interval coverage, where applicable, for a wide range of methods\nincluding weighted log-rank tests, the MaxCombo test, summary measures such as\nthe restricted mean survival time (RMST), average hazard ratios, and milestone\nsurvival probabilities as well as accelerated failure time regression models.\nWe found a trade-off between interpretability and power when choosing an\nanalysis strategy under NPH scenarios. While analysis methods based on weighted\nlogrank tests typically were favorable in terms of power, they do not provide\nan easily interpretable treatment effect estimate. Also, depending on the\nweight function, they test a narrow null hypothesis of equal hazard functions\nand rejection of this null hypothesis may not allow for a direct conclusion of\ntreatment benefit in terms of the survival function. In contrast,\nnon-parametric procedures based on well interpretable measures as the RMST\ndifference had lower power in most scenarios. Model based methods based on\nspecific survival distributions had larger power, however often gave biased\nestimates and lower than nominal confidence interval coverage."}, "http://arxiv.org/abs/2310.05646": {"title": "Transfer learning for piecewise-constant mean estimation: Optimality, $\\ell_1$- and $\\ell_0$-penalisation", "link": "http://arxiv.org/abs/2310.05646", "description": "We study transfer learning in the context of estimating piecewise-constant\nsignals when source data, which may be relevant but disparate, are available in\naddition to the target data. We initially investigate transfer learning\nestimators that respectively employ $\\ell_1$- and $\\ell_0$-penalties for\nunisource data scenarios and then generalise these estimators to accommodate\nmultisource data. To further reduce estimation errors, especially in scenarios\nwhere some sources significantly differ from the target, we introduce an\ninformative source selection algorithm. We then examine these estimators with\nmultisource selection and establish their minimax optimality under specific\nregularity conditions. It is worth emphasising that, unlike the prevalent\nnarrative in the transfer learning literature that the performance is enhanced\nthrough large source sample sizes, our approaches leverage higher observation\nfrequencies and accommodate diverse frequencies across multiple sources. Our\ntheoretical findings are empirically validated through extensive numerical\nexperiments, with the code available online, see\nhttps://github.com/chrisfanwang/transferlearning"}, "http://arxiv.org/abs/2310.05685": {"title": "Post-Selection Inference for Sparse Estimation", "link": "http://arxiv.org/abs/2310.05685", "description": "When the model is not known and parameter testing or interval estimation is\nconducted after model selection, it is necessary to consider selective\ninference. This paper discusses this issue in the context of sparse estimation.\nFirstly, we describe selective inference related to Lasso as per \\cite{lee},\nand then present polyhedra and truncated distributions when applying it to\nmethods such as Forward Stepwise and LARS. Lastly, we discuss the Significance\nTest for Lasso by \\cite{significant} and the Spacing Test for LARS by\n\\cite{ryan_exact}. This paper serves as a review article.\n\nKeywords: post-selective inference, polyhedron, LARS, lasso, forward\nstepwise, significance test, spacing test."}, "http://arxiv.org/abs/2310.05921": {"title": "Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions", "link": "http://arxiv.org/abs/2310.05921", "description": "We introduce Conformal Decision Theory, a framework for producing safe\nautonomous decisions despite imperfect machine learning predictions. Examples\nof such decisions are ubiquitous, from robot planning algorithms that rely on\npedestrian predictions, to calibrating autonomous manufacturing to exhibit high\nthroughput and low error, to the choice of trusting a nominal policy versus\nswitching to a safe backup policy at run-time. The decisions produced by our\nalgorithms are safe in the sense that they come with provable statistical\nguarantees of having low risk without any assumptions on the world model\nwhatsoever; the observations need not be I.I.D. and can even be adversarial.\nThe theory extends results from conformal prediction to calibrate decisions\ndirectly, without requiring the construction of prediction sets. Experiments\ndemonstrate the utility of our approach in robot motion planning around humans,\nautomated stock trading, and robot manufacturin"}, "http://arxiv.org/abs/2101.06950": {"title": "Learning and scoring Gaussian latent variable causal models with unknown additive interventions", "link": "http://arxiv.org/abs/2101.06950", "description": "With observational data alone, causal structure learning is a challenging\nproblem. The task becomes easier when having access to data collected from\nperturbations of the underlying system, even when the nature of these is\nunknown. Existing methods either do not allow for the presence of latent\nvariables or assume that these remain unperturbed. However, these assumptions\nare hard to justify if the nature of the perturbations is unknown. We provide\nresults that enable scoring causal structures in the setting with additive, but\nunknown interventions. Specifically, we propose a maximum-likelihood estimator\nin a structural equation model that exploits system-wide invariances to output\nan equivalence class of causal structures from perturbation data. Furthermore,\nunder certain structural assumptions on the population model, we provide a\nsimple graphical characterization of all the DAGs in the interventional\nequivalence class. We illustrate the utility of our framework on synthetic data\nas well as real data involving California reservoirs and protein expressions.\nThe software implementation is available as the Python package \\emph{utlvce}."}, "http://arxiv.org/abs/2107.14151": {"title": "Modern Non-Linear Function-on-Function Regression", "link": "http://arxiv.org/abs/2107.14151", "description": "We introduce a new class of non-linear function-on-function regression models\nfor functional data using neural networks. We propose a framework using a\nhidden layer consisting of continuous neurons, called a continuous hidden\nlayer, for functional response modeling and give two model fitting strategies,\nFunctional Direct Neural Network (FDNN) and Functional Basis Neural Network\n(FBNN). Both are designed explicitly to exploit the structure inherent in\nfunctional data and capture the complex relations existing between the\nfunctional predictors and the functional response. We fit these models by\nderiving functional gradients and implement regularization techniques for more\nparsimonious results. We demonstrate the power and flexibility of our proposed\nmethod in handling complex functional models through extensive simulation\nstudies as well as real data examples."}, "http://arxiv.org/abs/2112.00832": {"title": "On the mixed-model analysis of covariance in cluster-randomized trials", "link": "http://arxiv.org/abs/2112.00832", "description": "In the analyses of cluster-randomized trials, mixed-model analysis of\ncovariance (ANCOVA) is a standard approach for covariate adjustment and\nhandling within-cluster correlations. However, when the normality, linearity,\nor the random-intercept assumption is violated, the validity and efficiency of\nthe mixed-model ANCOVA estimators for estimating the average treatment effect\nremain unclear. Under the potential outcomes framework, we prove that the\nmixed-model ANCOVA estimators for the average treatment effect are consistent\nand asymptotically normal under arbitrary misspecification of its working\nmodel. If the probability of receiving treatment is 0.5 for each cluster, we\nfurther show that the model-based variance estimator under mixed-model ANCOVA1\n(ANCOVA without treatment-covariate interactions) remains consistent,\nclarifying that the confidence interval given by standard software is\nasymptotically valid even under model misspecification. Beyond robustness, we\ndiscuss several insights on precision among classical methods for analyzing\ncluster-randomized trials, including the mixed-model ANCOVA, individual-level\nANCOVA, and cluster-level ANCOVA estimators. These insights may inform the\nchoice of methods in practice. Our analytical results and insights are\nillustrated via simulation studies and analyses of three cluster-randomized\ntrials."}, "http://arxiv.org/abs/2201.10770": {"title": "Confidence intervals for the Cox model test error from cross-validation", "link": "http://arxiv.org/abs/2201.10770", "description": "Cross-validation (CV) is one of the most widely used techniques in\nstatistical learning for estimating the test error of a model, but its behavior\nis not yet fully understood. It has been shown that standard confidence\nintervals for test error using estimates from CV may have coverage below\nnominal levels. This phenomenon occurs because each sample is used in both the\ntraining and testing procedures during CV and as a result, the CV estimates of\nthe errors become correlated. Without accounting for this correlation, the\nestimate of the variance is smaller than it should be. One way to mitigate this\nissue is by estimating the mean squared error of the prediction error instead\nusing nested CV. This approach has been shown to achieve superior coverage\ncompared to intervals derived from standard CV. In this work, we generalize the\nnested CV idea to the Cox proportional hazards model and explore various\nchoices of test error for this setting."}, "http://arxiv.org/abs/2202.08419": {"title": "High-Dimensional Time-Varying Coefficient Estimation", "link": "http://arxiv.org/abs/2202.08419", "description": "In this paper, we develop a novel high-dimensional time-varying coefficient\nestimation method, based on high-dimensional Ito diffusion processes. To\naccount for high-dimensional time-varying coefficients, we first estimate local\n(or instantaneous) coefficients using a time-localized Dantzig selection scheme\nunder a sparsity condition, which results in biased local coefficient\nestimators due to the regularization. To handle the bias, we propose a\ndebiasing scheme, which provides well-performing unbiased local coefficient\nestimators. With the unbiased local coefficient estimators, we estimate the\nintegrated coefficient, and to further account for the sparsity of the\ncoefficient process, we apply thresholding schemes. We call this Thresholding\ndEbiased Dantzig (TED). We establish asymptotic properties of the proposed TED\nestimator. In the empirical analysis, we apply the TED procedure to analyzing\nhigh-dimensional factor models using high-frequency data."}, "http://arxiv.org/abs/2206.12525": {"title": "Causality of Functional Longitudinal Data", "link": "http://arxiv.org/abs/2206.12525", "description": "\"Treatment-confounder feedback\" is the central complication to resolve in\nlongitudinal studies, to infer causality. The existing frameworks for\nidentifying causal effects for longitudinal studies with discrete repeated\nmeasures hinge heavily on assuming that time advances in discrete time steps or\ntreatment changes as a jumping process, rendering the number of \"feedbacks\"\nfinite. However, medical studies nowadays with real-time monitoring involve\nfunctional time-varying outcomes, treatment, and confounders, which leads to an\nuncountably infinite number of feedbacks between treatment and confounders.\nTherefore more general and advanced theory is needed. We generalize the\ndefinition of causal effects under user-specified stochastic treatment regimes\nto longitudinal studies with continuous monitoring and develop an\nidentification framework, allowing right censoring and truncation by death. We\nprovide sufficient identification assumptions including a generalized\nconsistency assumption, a sequential randomization assumption, a positivity\nassumption, and a novel \"intervenable\" assumption designed for the\ncontinuous-time case. Under these assumptions, we propose a g-computation\nprocess and an inverse probability weighting process, which suggest a\ng-computation formula and an inverse probability weighting formula for\nidentification. For practical purposes, we also construct two classes of\npopulation estimating equations to identify these two processes, respectively,\nwhich further suggest a doubly robust identification formula with extra\nrobustness against process misspecification. We prove that our framework fully\ngeneralize the existing frameworks and is nonparametric."}, "http://arxiv.org/abs/2209.08139": {"title": "Sparse high-dimensional linear regression with a partitioned empirical Bayes ECM algorithm", "link": "http://arxiv.org/abs/2209.08139", "description": "Bayesian variable selection methods are powerful techniques for fitting and\ninferring on sparse high-dimensional linear regression models. However, many\nare computationally intensive or require restrictive prior distributions on\nmodel parameters. In this paper, we proposed a computationally efficient and\npowerful Bayesian approach for sparse high-dimensional linear regression.\nMinimal prior assumptions on the parameters are required through the use of\nplug-in empirical Bayes estimates of hyperparameters. Efficient maximum a\nposteriori (MAP) estimation is completed through a Parameter-Expanded\nExpectation-Conditional-Maximization (PX-ECM) algorithm. The PX-ECM results in\na robust computationally efficient coordinate-wise optimization which -- when\nupdating the coefficient for a particular predictor -- adjusts for the impact\nof other predictor variables. The completion of the E-step uses an approach\nmotivated by the popular two-group approach to multiple testing. The result is\na PaRtitiOned empirical Bayes Ecm (PROBE) algorithm applied to sparse\nhigh-dimensional linear regression, which can be completed using one-at-a-time\nor all-at-once type optimization. We compare the empirical properties of PROBE\nto comparable approaches with numerous simulation studies and analyses of\ncancer cell drug responses. The proposed approach is implemented in the R\npackage probe."}, "http://arxiv.org/abs/2212.02709": {"title": "SURE-tuned Bridge Regression", "link": "http://arxiv.org/abs/2212.02709", "description": "Consider the {$\\ell_{\\alpha}$} regularized linear regression, also termed\nBridge regression. For $\\alpha\\in (0,1)$, Bridge regression enjoys several\nstatistical properties of interest such as sparsity and near-unbiasedness of\nthe estimates (Fan and Li, 2001). However, the main difficulty lies in the\nnon-convex nature of the penalty for these values of $\\alpha$, which makes an\noptimization procedure challenging and usually it is only possible to find a\nlocal optimum. To address this issue, Polson et al. (2013) took a sampling\nbased fully Bayesian approach to this problem, using the correspondence between\nthe Bridge penalty and a power exponential prior on the regression\ncoefficients. However, their sampling procedure relies on Markov chain Monte\nCarlo (MCMC) techniques, which are inherently sequential and not scalable to\nlarge problem dimensions. Cross validation approaches are similarly\ncomputation-intensive. To this end, our contribution is a novel\n\\emph{non-iterative} method to fit a Bridge regression model. The main\ncontribution lies in an explicit formula for Stein's unbiased risk estimate for\nthe out of sample prediction risk of Bridge regression, which can then be\noptimized to select the desired tuning parameters, allowing us to completely\nbypass MCMC as well as computation-intensive cross validation approaches. Our\nprocedure yields results in a fraction of computational times compared to\niterative schemes, without any appreciable loss in statistical performance. An\nR implementation is publicly available online at:\nhttps://github.com/loriaJ/Sure-tuned_BridgeRegression ."}, "http://arxiv.org/abs/2212.03122": {"title": "Robust convex biclustering with a tuning-free method", "link": "http://arxiv.org/abs/2212.03122", "description": "Biclustering is widely used in different kinds of fields including gene\ninformation analysis, text mining, and recommendation system by effectively\ndiscovering the local correlation between samples and features. However, many\nbiclustering algorithms will collapse when facing heavy-tailed data. In this\npaper, we propose a robust version of convex biclustering algorithm with Huber\nloss. Yet, the newly introduced robustification parameter brings an extra\nburden to selecting the optimal parameters. Therefore, we propose a tuning-free\nmethod for automatically selecting the optimal robustification parameter with\nhigh efficiency. The simulation study demonstrates the more fabulous\nperformance of our proposed method than traditional biclustering methods when\nencountering heavy-tailed noise. A real-life biomedical application is also\npresented. The R package RcvxBiclustr is available at\nhttps://github.com/YifanChen3/RcvxBiclustr."}, "http://arxiv.org/abs/2301.09661": {"title": "Estimating marginal treatment effects from observational studies and indirect treatment comparisons: When are standardization-based methods preferable to those based on propensity score weighting?", "link": "http://arxiv.org/abs/2301.09661", "description": "In light of newly developed standardization methods, we evaluate, via\nsimulation study, how propensity score weighting and standardization -based\napproaches compare for obtaining estimates of the marginal odds ratio and the\nmarginal hazard ratio. Specifically, we consider how the two approaches compare\nin two different scenarios: (1) in a single observational study, and (2) in an\nanchored indirect treatment comparison (ITC) of randomized controlled trials.\nWe present the material in such a way so that the matching-adjusted indirect\ncomparison (MAIC) and the (novel) simulated treatment comparison (STC) methods\nin the ITC setting may be viewed as analogous to the propensity score weighting\nand standardization methods in the single observational study setting. Our\nresults suggest that current recommendations for conducting ITCs can be\nimproved and underscore the importance of adjusting for purely prognostic\nfactors."}, "http://arxiv.org/abs/2302.11746": {"title": "Logistic Regression and Classification with non-Euclidean Covariates", "link": "http://arxiv.org/abs/2302.11746", "description": "We introduce a logistic regression model for data pairs consisting of a\nbinary response and a covariate residing in a non-Euclidean metric space\nwithout vector structures. Based on the proposed model we also develop a binary\nclassifier for non-Euclidean objects. We propose a maximum likelihood estimator\nfor the non-Euclidean regression coefficient in the model, and provide upper\nbounds on the estimation error under various metric entropy conditions that\nquantify complexity of the underlying metric space. Matching lower bounds are\nderived for the important metric spaces commonly seen in statistics,\nestablishing optimality of the proposed estimator in such spaces. Similarly, an\nupper bound on the excess risk of the developed classifier is provided for\ngeneral metric spaces. A finer upper bound and a matching lower bound, and thus\noptimality of the proposed classifier, are established for Riemannian\nmanifolds. We investigate the numerical performance of the proposed estimator\nand classifier via simulation studies, and illustrate their practical merits\nvia an application to task-related fMRI data."}, "http://arxiv.org/abs/2302.13658": {"title": "Robust High-Dimensional Time-Varying Coefficient Estimation", "link": "http://arxiv.org/abs/2302.13658", "description": "In this paper, we develop a novel high-dimensional coefficient estimation\nprocedure based on high-frequency data. Unlike usual high-dimensional\nregression procedure such as LASSO, we additionally handle the heavy-tailedness\nof high-frequency observations as well as time variations of coefficient\nprocesses. Specifically, we employ Huber loss and truncation scheme to handle\nheavy-tailed observations, while $\\ell_{1}$-regularization is adopted to\novercome the curse of dimensionality. To account for the time-varying\ncoefficient, we estimate local coefficients which are biased due to the\n$\\ell_{1}$-regularization. Thus, when estimating integrated coefficients, we\npropose a debiasing scheme to enjoy the law of large number property and employ\na thresholding scheme to further accommodate the sparsity of the coefficients.\nWe call this Robust thrEsholding Debiased LASSO (RED-LASSO) estimator. We show\nthat the RED-LASSO estimator can achieve a near-optimal convergence rate. In\nthe empirical study, we apply the RED-LASSO procedure to the high-dimensional\nintegrated coefficient estimation using high-frequency trading data."}, "http://arxiv.org/abs/2307.04754": {"title": "Action-State Dependent Dynamic Model Selection", "link": "http://arxiv.org/abs/2307.04754", "description": "A model among many may only be best under certain states of the world.\nSwitching from a model to another can also be costly. Finding a procedure to\ndynamically choose a model in these circumstances requires to solve a complex\nestimation procedure and a dynamic programming problem. A Reinforcement\nlearning algorithm is used to approximate and estimate from the data the\noptimal solution to this dynamic programming problem. The algorithm is shown to\nconsistently estimate the optimal policy that may choose different models based\non a set of covariates. A typical example is the one of switching between\ndifferent portfolio models under rebalancing costs, using macroeconomic\ninformation. Using a set of macroeconomic variables and price data, an\nempirical application to the aforementioned portfolio problem shows superior\nperformance to choosing the best portfolio model with hindsight."}, "http://arxiv.org/abs/2307.14828": {"title": "Identifying regime switches through Bayesian wavelet estimation: evidence from flood detection in the Taquari River Valley", "link": "http://arxiv.org/abs/2307.14828", "description": "Two-component mixture models have proved to be a powerful tool for modeling\nheterogeneity in several cluster analysis contexts. However, most methods based\non these models assume a constant behavior for the mixture weights, which can\nbe restrictive and unsuitable for some applications. In this paper, we relax\nthis assumption and allow the mixture weights to vary according to the index\n(e.g., time) to make the model more adaptive to a broader range of data sets.\nWe propose an efficient MCMC algorithm to jointly estimate both component\nparameters and dynamic weights from their posterior samples. We evaluate the\nmethod's performance by running Monte Carlo simulation studies under different\nscenarios for the dynamic weights. In addition, we apply the algorithm to a\ntime series that records the level reached by a river in southern Brazil. The\nTaquari River is a water body whose frequent flood inundations have caused\nvarious damage to riverside communities. Implementing a dynamic mixture model\nallows us to properly describe the flood regimes for the areas most affected by\nthese phenomena."}, "http://arxiv.org/abs/2310.06130": {"title": "Statistical inference for radially-stable generalized Pareto distributions and return level-sets in geometric extremes", "link": "http://arxiv.org/abs/2310.06130", "description": "We obtain a functional analogue of the quantile function for probability\nmeasures admitting a continuous Lebesgue density on $\\mathbb{R}^d$, and use it\nto characterize the class of non-trivial limit distributions of radially\nrecentered and rescaled multivariate exceedances in geometric extremes. A new\nclass of multivariate distributions is identified, termed radially stable\ngeneralized Pareto distributions, and is shown to admit certain stability\nproperties that permit extrapolation to extremal sets along any direction in\n$\\mathbb{R}^d$. Based on the limit Poisson point process likelihood of the\nradially renormalized point process of exceedances, we develop parsimonious\nstatistical models that exploit theoretical links between structural\nstar-bodies and are amenable to Bayesian inference. The star-bodies determine\nthe mean measure of the limit Poisson process through a hierarchical structure.\nOur framework sharpens statistical inference by suitably including additional\ninformation from the angular directions of the geometric exceedances and\nfacilitates efficient computations in dimensions $d=2$ and $d=3$. Additionally,\nit naturally leads to the notion of the return level-set, which is a canonical\nquantile set expressed in terms of its average recurrence interval, and a\ngeometric analogue of the uni-dimensional return level. We illustrate our\nmethods with a simulation study showing superior predictive performance of\nprobabilities of rare events, and with two case studies, one associated with\nriver flow extremes, and the other with oceanographic extremes."}, "http://arxiv.org/abs/2310.06242": {"title": "Treatment Choice, Mean Square Regret and Partial Identification", "link": "http://arxiv.org/abs/2310.06242", "description": "We consider a decision maker who faces a binary treatment choice when their\nwelfare is only partially identified from data. We contribute to the literature\nby anchoring our finite-sample analysis on mean square regret, a decision\ncriterion advocated by Kitagawa, Lee, and Qiu (2022). We find that optimal\nrules are always fractional, irrespective of the width of the identified set\nand precision of its estimate. The optimal treatment fraction is a simple\nlogistic transformation of the commonly used t-statistic multiplied by a factor\ncalculated by a simple constrained optimization. This treatment fraction gets\ncloser to 0.5 as the width of the identified set becomes wider, implying the\ndecision maker becomes more cautious against the adversarial Nature."}, "http://arxiv.org/abs/2310.06252": {"title": "Power and sample size calculation of two-sample projection-based testing for sparsely observed functional data", "link": "http://arxiv.org/abs/2310.06252", "description": "Projection-based testing for mean trajectory differences in two groups of\nirregularly and sparsely observed functional data has garnered significant\nattention in the literature because it accommodates a wide spectrum of group\ndifferences and (non-stationary) covariance structures. This article presents\nthe derivation of the theoretical power function and the introduction of a\ncomprehensive power and sample size (PASS) calculation toolkit tailored to the\nprojection-based testing method developed by Wang (2021). Our approach\naccommodates a wide spectrum of group difference scenarios and a broad class of\ncovariance structures governing the underlying processes. Through extensive\nnumerical simulation, we demonstrate the robustness of this testing method by\nshowcasing that its statistical power remains nearly unaffected even when a\ncertain percentage of observations are missing, rendering it 'missing-immune'.\nFurthermore, we illustrate the practical utility of this test through analysis\nof two randomized controlled trials of Parkinson's disease. To facilitate\nimplementation, we provide a user-friendly R package fPASS, complete with a\ndetailed vignette to guide users through its practical application. We\nanticipate that this article will significantly enhance the usability of this\npotent statistical tool across a range of biostatistical applications, with a\nparticular focus on its relevance in the design of clinical trials."}, "http://arxiv.org/abs/2310.06315": {"title": "Ultra-high dimensional confounder selection algorithms comparison with application to radiomics data", "link": "http://arxiv.org/abs/2310.06315", "description": "Radiomics is an emerging area of medical imaging data analysis particularly\nfor cancer. It involves the conversion of digital medical images into mineable\nultra-high dimensional data. Machine learning algorithms are widely used in\nradiomics data analysis to develop powerful decision support model to improve\nprecision in diagnosis, assessment of prognosis and prediction of therapy\nresponse. However, machine learning algorithms for causal inference have not\nbeen previously employed in radiomics analysis. In this paper, we evaluate the\nvalue of machine learning algorithms for causal inference in radiomics. We\nselect three recent competitive variable selection algorithms for causal\ninference: outcome-adaptive lasso (OAL), generalized outcome-adaptive lasso\n(GOAL) and causal ball screening (CBS). We used a sure independence screening\nprocedure to propose an extension of GOAL and OAL for ultra-high dimensional\ndata, SIS + GOAL and SIS + OAL. We compared SIS + GOAL, SIS + OAL and CBS using\nsimulation study and two radiomics datasets in cancer, osteosarcoma and\ngliosarcoma. The two radiomics studies and the simulation study identified SIS\n+ GOAL as the optimal variable selection algorithm."}, "http://arxiv.org/abs/2310.06330": {"title": "Multivariate moment least-squares estimators for reversible Markov chains", "link": "http://arxiv.org/abs/2310.06330", "description": "Markov chain Monte Carlo (MCMC) is a commonly used method for approximating\nexpectations with respect to probability distributions. Uncertainty assessment\nfor MCMC estimators is essential in practical applications. Moreover, for\nmultivariate functions of a Markov chain, it is important to estimate not only\nthe auto-correlation for each component but also to estimate\ncross-correlations, in order to better assess sample quality, improve estimates\nof effective sample size, and use more effective stopping rules. Berg and Song\n[2022] introduced the moment least squares (momentLS) estimator, a\nshape-constrained estimator for the autocovariance sequence from a reversible\nMarkov chain, for univariate functions of the Markov chain. Based on this\nsequence estimator, they proposed an estimator of the asymptotic variance of\nthe sample mean from MCMC samples. In this study, we propose novel\nautocovariance sequence and asymptotic variance estimators for Markov chain\nfunctions with multiple components, based on the univariate momentLS estimators\nfrom Berg and Song [2022]. We demonstrate strong consistency of the proposed\nauto(cross)-covariance sequence and asymptotic variance matrix estimators. We\nconduct empirical comparisons of our method with other state-of-the-art\napproaches on simulated and real-data examples, using popular samplers\nincluding the random-walk Metropolis sampler and the No-U-Turn sampler from\nSTAN."}, "http://arxiv.org/abs/2310.06357": {"title": "Adaptive Storey's null proportion estimator", "link": "http://arxiv.org/abs/2310.06357", "description": "False discovery rate (FDR) is a commonly used criterion in multiple testing\nand the Benjamini-Hochberg (BH) procedure is arguably the most popular approach\nwith FDR guarantee. To improve power, the adaptive BH procedure has been\nproposed by incorporating various null proportion estimators, among which\nStorey's estimator has gained substantial popularity. The performance of\nStorey's estimator hinges on a critical hyper-parameter, where a pre-fixed\nconfiguration lacks power and existing data-driven hyper-parameters compromise\nthe FDR control. In this work, we propose a novel class of adaptive\nhyper-parameters and establish the FDR control of the associated BH procedure\nusing a martingale argument. Within this class of data-driven hyper-parameters,\nwe present a specific configuration designed to maximize the number of\nrejections and characterize the convergence of this proposal to the optimal\nhyper-parameter under a commonly-used mixture model. We evaluate our adaptive\nStorey's null proportion estimator and the associated BH procedure on extensive\nsimulated data and a motivating protein dataset. Our proposal exhibits\nsignificant power gains when dealing with a considerable proportion of weak\nnon-nulls or a conservative null distribution."}, "http://arxiv.org/abs/2310.06467": {"title": "Advances in Kth nearest-neighbour clutter removal", "link": "http://arxiv.org/abs/2310.06467", "description": "We consider the problem of feature detection in the presence of clutter in\nspatial point processes. Classification methods have been developed in previous\nstudies. Among these, Byers and Raftery (1998) models the observed Kth nearest\nneighbour distances as a mixture distribution and classifies the clutter and\nfeature points consequently. In this paper, we enhance such approach in two\nmanners. First, we propose an automatic procedure for selecting the number of\nnearest neighbours to consider in the classification method by means of\nsegmented regression models. Secondly, with the aim of applying the procedure\nmultiple times to get a ``better\" end result, we propose a stopping criterion\nthat minimizes the overall entropy measure of cluster separation between\nclutter and feature points. The proposed procedures are suitable for a feature\nwith clutter as two superimposed Poisson processes on any space, including\nlinear networks. We present simulations and two case studies of environmental\ndata to illustrate the method."}, "http://arxiv.org/abs/2310.06533": {"title": "Multilevel Monte Carlo for a class of Partially Observed Processes in Neuroscience", "link": "http://arxiv.org/abs/2310.06533", "description": "In this paper we consider Bayesian parameter inference associated to a class\nof partially observed stochastic differential equations (SDE) driven by jump\nprocesses. Such type of models can be routinely found in applications, of which\nwe focus upon the case of neuroscience. The data are assumed to be observed\nregularly in time and driven by the SDE model with unknown parameters. In\npractice the SDE may not have an analytically tractable solution and this leads\nnaturally to a time-discretization. We adapt the multilevel Markov chain Monte\nCarlo method of [11], which works with a hierarchy of time discretizations and\nshow empirically and theoretically that this is preferable to using one single\ntime discretization. The improvement is in terms of the computational cost\nneeded to obtain a pre-specified numerical error. Our approach is illustrated\non models that are found in neuroscience."}, "http://arxiv.org/abs/2310.06653": {"title": "Evaluating causal effects on time-to-event outcomes in an RCT in Oncology with treatment discontinuation due to adverse events", "link": "http://arxiv.org/abs/2310.06653", "description": "In clinical trials, patients sometimes discontinue study treatments\nprematurely due to reasons such as adverse events. Treatment discontinuation\noccurs after the randomisation as an intercurrent event, making causal\ninference more challenging. The Intention-To-Treat (ITT) analysis provides\nvalid causal estimates of the effect of treatment assignment; still, it does\nnot take into account whether or not patients had to discontinue the treatment\nprematurely. We propose to deal with the problem of treatment discontinuation\nusing principal stratification, recognised in the ICH E9(R1) addendum as a\nstrategy for handling intercurrent events. Under this approach, we can\ndecompose the overall ITT effect into principal causal effects for groups of\npatients defined by their potential discontinuation behaviour in continuous\ntime. In this framework, we must consider that discontinuation happening in\ncontinuous time generates an infinite number of principal strata and that\ndiscontinuation time is not defined for patients who would never discontinue.\nAn additional complication is that discontinuation time and time-to-event\noutcomes are subject to administrative censoring. We employ a flexible\nmodel-based Bayesian approach to deal with such complications. We apply the\nBayesian principal stratification framework to analyse synthetic data based on\na recent RCT in Oncology, aiming to assess the causal effects of a new\ninvestigational drug combined with standard of care vs. standard of care alone\non progression-free survival. We simulate data under different assumptions that\nreflect real situations where patients' behaviour depends on critical baseline\ncovariates. Finally, we highlight how such an approach makes it straightforward\nto characterise patients' discontinuation behaviour with respect to the\navailable covariates with the help of a simulation study."}, "http://arxiv.org/abs/2310.06673": {"title": "Assurance Methods for designing a clinical trial with a delayed treatment effect", "link": "http://arxiv.org/abs/2310.06673", "description": "An assurance calculation is a Bayesian alternative to a power calculation.\nOne may be performed to aid the planning of a clinical trial, specifically\nsetting the sample size or to support decisions about whether or not to perform\na study. Immuno-oncology (IO) is a rapidly evolving area in the development of\nanticancer drugs. A common phenomenon that arises from IO trials is one of\ndelayed treatment effects, that is, there is a delay in the separation of the\nsurvival curves. To calculate assurance for a trial in which a delayed\ntreatment effect is likely to be present, uncertainty about key parameters\nneeds to be considered. If uncertainty is not considered, then the number of\npatients recruited may not be enough to ensure we have adequate statistical\npower to detect a clinically relevant treatment effect. We present a new\nelicitation technique for when a delayed treatment effect is likely to be\npresent and show how to compute assurance using these elicited prior\ndistributions. We provide an example to illustrate how this could be used in\npractice. Open-source software is provided for implementing our methods. Our\nmethodology makes the benefits of assurance methods available for the planning\nof IO trials (and others where a delayed treatment expect is likely to occur)."}, "http://arxiv.org/abs/2310.06696": {"title": "Variable selection with FDR control for noisy data -- an application to screening metabolites that are associated with breast and colorectal cancer", "link": "http://arxiv.org/abs/2310.06696", "description": "The rapidly expanding field of metabolomics presents an invaluable resource\nfor understanding the associations between metabolites and various diseases.\nHowever, the high dimensionality, presence of missing values, and measurement\nerrors associated with metabolomics data can present challenges in developing\nreliable and reproducible methodologies for disease association studies.\nTherefore, there is a compelling need to develop robust statistical methods\nthat can navigate these complexities to achieve reliable and reproducible\ndisease association studies. In this paper, we focus on developing such a\nmethodology with an emphasis on controlling the False Discovery Rate during the\nscreening of mutual metabolomic signals for multiple disease outcomes. We\nillustrate the versatility and performance of this procedure in a variety of\nscenarios, dealing with missing data and measurement errors. As a specific\napplication of this novel methodology, we target two of the most prevalent\ncancers among US women: breast cancer and colorectal cancer. By applying our\nmethod to the Wome's Health Initiative data, we successfully identify\nmetabolites that are associated with either or both of these cancers,\ndemonstrating the practical utility and potential of our method in identifying\nconsistent risk factors and understanding shared mechanisms between diseases."}, "http://arxiv.org/abs/2310.06708": {"title": "Adjustment with Three Continuous Variables", "link": "http://arxiv.org/abs/2310.06708", "description": "Spurious association between X and Y may be due to a confounding variable W.\nStatisticians may adjust for W using a variety of techniques. This paper\npresents the results of simulations conducted to assess the performance of\nthose techniques under various, elementary, data-generating processes. The\nresults indicate that no technique is best overall and that specific techniques\nshould be selected based on the particulars of the data-generating process.\nHere we show how causal graphs can guide the selection or design of techniques\nfor statistical adjustment. R programs are provided for researchers interested\nin generalization."}, "http://arxiv.org/abs/2310.06720": {"title": "Asymptotic theory for Bayesian inference and prediction: from the ordinary to a conditional Peaks-Over-Threshold method", "link": "http://arxiv.org/abs/2310.06720", "description": "The Peaks Over Threshold (POT) method is the most popular statistical method\nfor the analysis of univariate extremes. Even though there is a rich applied\nliterature on Bayesian inference for the POT method there is no asymptotic\ntheory for such proposals. Even more importantly, the ambitious and challenging\nproblem of predicting future extreme events according to a proper probabilistic\nforecasting approach has received no attention to date. In this paper we\ndevelop the asymptotic theory (consistency, contraction rates, asymptotic\nnormality and asymptotic coverage of credible intervals) for the Bayesian\ninference based on the POT method. We extend such an asymptotic theory to cover\nthe Bayesian inference on the tail properties of the conditional distribution\nof a response random variable conditionally to a vector of random covariates.\nWith the aim to make accurate predictions of severer extreme events than those\noccurred in the past, we specify the posterior predictive distribution of a\nfuture unobservable excess variable in the unconditional and conditional\napproach and we prove that is Wasserstein consistent and derive its contraction\nrates. Simulations show the good performances of the proposed Bayesian\ninferential methods. The analysis of the change in the frequency of financial\ncrises over time shows the utility of our methodology."}, "http://arxiv.org/abs/2310.06730": {"title": "Sparse topic modeling via spectral decomposition and thresholding", "link": "http://arxiv.org/abs/2310.06730", "description": "The probabilistic Latent Semantic Indexing model assumes that the expectation\nof the corpus matrix is low-rank and can be written as the product of a\ntopic-word matrix and a word-document matrix. In this paper, we study the\nestimation of the topic-word matrix under the additional assumption that the\nordered entries of its columns rapidly decay to zero. This sparsity assumption\nis motivated by the empirical observation that the word frequencies in a text\noften adhere to Zipf's law. We introduce a new spectral procedure for\nestimating the topic-word matrix that thresholds words based on their corpus\nfrequencies, and show that its $\\ell_1$-error rate under our sparsity\nassumption depends on the vocabulary size $p$ only via a logarithmic term. Our\nerror bound is valid for all parameter regimes and in particular for the\nsetting where $p$ is extremely large; this high-dimensional setting is commonly\nencountered but has not been adequately addressed in prior literature.\nFurthermore, our procedure also accommodates datasets that violate the\nseparability assumption, which is necessary for most prior approaches in topic\nmodeling. Experiments with synthetic data confirm that our procedure is\ncomputationally fast and allows for consistent estimation of the topic-word\nmatrix in a wide variety of parameter regimes. Our procedure also performs well\nrelative to well-established methods when applied to a large corpus of research\npaper abstracts, as well as the analysis of single-cell and microbiome data\nwhere the same statistical model is relevant but the parameter regimes are\nvastly different."}, "http://arxiv.org/abs/2310.06746": {"title": "Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules", "link": "http://arxiv.org/abs/2310.06746", "description": "Interpretability is a key concern in estimating heterogeneous treatment\neffects using machine learning methods, especially for healthcare applications\nwhere high-stake decisions are often made. Inspired by the Predictive,\nDescriptive, Relevant framework of interpretability, we propose causal rule\nlearning which finds a refined set of causal rules characterizing potential\nsubgroups to estimate and enhance our understanding of heterogeneous treatment\neffects. Causal rule learning involves three phases: rule discovery, rule\nselection, and rule analysis. In the rule discovery phase, we utilize a causal\nforest to generate a pool of causal rules with corresponding subgroup average\ntreatment effects. The selection phase then employs a D-learning method to\nselect a subset of these rules to deconstruct individual-level treatment\neffects as a linear combination of the subgroup-level effects. This helps to\nanswer an ignored question by previous literature: what if an individual\nsimultaneously belongs to multiple groups with different average treatment\neffects? The rule analysis phase outlines a detailed procedure to further\nanalyze each rule in the subset from multiple perspectives, revealing the most\npromising rules for further validation. The rules themselves, their\ncorresponding subgroup treatment effects, and their weights in the linear\ncombination give us more insights into heterogeneous treatment effects.\nSimulation and real-world data analysis demonstrate the superior performance of\ncausal rule learning on the interpretable estimation of heterogeneous treatment\neffect when the ground truth is complex and the sample size is sufficient."}, "http://arxiv.org/abs/2310.06808": {"title": "Odds are the sign is right", "link": "http://arxiv.org/abs/2310.06808", "description": "This article introduces a new condition based on odds ratios for sensitivity\nanalysis. The analysis involves the average effect of a treatment or exposure\non a response or outcome with estimates adjusted for and conditional on a\nsingle, unmeasured, dichotomous covariate. Results of statistical simulations\nare displayed to show that the odds ratio condition is as reliable as other\ncommonly used conditions for sensitivity analysis. Other conditions utilize\nquantities reflective of a mediating covariate. The odds ratio condition can be\napplied when the covariate is a confounding variable. As an example application\nwe use the odds ratio condition to analyze and interpret a positive association\nobserved between Zika virus infection and birth defects."}, "http://arxiv.org/abs/2009.07551": {"title": "Manipulation-Robust Regression Discontinuity Designs", "link": "http://arxiv.org/abs/2009.07551", "description": "We present a new identification condition for regression discontinuity\ndesigns. We replace the local randomization of Lee (2008) with two restrictions\non its threat, namely, the manipulation of the running variable. Furthermore,\nwe provide the first auxiliary assumption of McCrary's (2008) diagnostic test\nto detect manipulation. Based on our auxiliary assumption, we derive a novel\nexpression of moments that immediately implies the worst-case bounds of Gerard,\nRokkanen, and Rothe (2020) and an enhanced interpretation of their target\nparameters. We highlight two issues: an overlooked source of identification\nfailure, and a missing auxiliary assumption to detect manipulation. In the case\nstudies, we illustrate our solution to these issues using institutional details\nand economic theories."}, "http://arxiv.org/abs/2204.06030": {"title": "Variable importance measures for heterogeneous causal effects", "link": "http://arxiv.org/abs/2204.06030", "description": "The recognition that personalised treatment decisions lead to better clinical\noutcomes has sparked recent research activity in the following two domains.\nPolicy learning focuses on finding optimal treatment rules (OTRs), which\nexpress whether an individual would be better off with or without treatment,\ngiven their measured characteristics. OTRs optimize a pre-set population\ncriterion, but do not provide insight into the extent to which treatment\nbenefits or harms individual subjects. Estimates of conditional average\ntreatment effects (CATEs) do offer such insights, but valid inference is\ncurrently difficult to obtain when data-adaptive methods are used. Moreover,\nclinicians are (rightly) hesitant to blindly adopt OTR or CATE estimates, not\nleast since both may represent complicated functions of patient characteristics\nthat provide little insight into the key drivers of heterogeneity. To address\nthese limitations, we introduce novel nonparametric treatment effect variable\nimportance measures (TE-VIMs). TE-VIMs extend recent regression-VIMs, viewed as\nnonparametric analogues to ANOVA statistics. By not being tied to a particular\nmodel, they are amenable to data-adaptive (machine learning) estimation of the\nCATE, itself an active area of research. Estimators for the proposed statistics\nare derived from their efficient influence curves and these are illustrated\nthrough a simulation study and an applied example."}, "http://arxiv.org/abs/2204.07907": {"title": "Just Identified Indirect Inference Estimator: Accurate Inference through Bias Correction", "link": "http://arxiv.org/abs/2204.07907", "description": "An important challenge in statistical analysis lies in controlling the\nestimation bias when handling the ever-increasing data size and model\ncomplexity of modern data settings. In this paper, we propose a reliable\nestimation and inference approach for parametric models based on the Just\nIdentified iNdirect Inference estimator (JINI). The key advantage of our\napproach is that it allows to construct a consistent estimator in a simple\nmanner, while providing strong bias correction guarantees that lead to accurate\ninference. Our approach is particularly useful for complex parametric models,\nas it allows to bypass the analytical and computational difficulties (e.g., due\nto intractable estimating equation) typically encountered in standard\nprocedures. The properties of JINI (including consistency, asymptotic\nnormality, and its bias correction property) are also studied when the\nparameter dimension is allowed to diverge, which provide the theoretical\nfoundation to explain the advantageous performance of JINI in increasing\ndimensional covariates settings. Our simulations and an alcohol consumption\ndata analysis highlight the practical usefulness and excellent performance of\nJINI when data present features (e.g., misclassification, rounding) as well as\nin robust estimation."}, "http://arxiv.org/abs/2209.05598": {"title": "Learning domain-specific causal discovery from time series", "link": "http://arxiv.org/abs/2209.05598", "description": "Causal discovery (CD) from time-varying data is important in neuroscience,\nmedicine, and machine learning. Techniques for CD encompass randomized\nexperiments, which are generally unbiased but expensive, and algorithms such as\nGranger causality, conditional-independence-based, structural-equation-based,\nand score-based methods that are only accurate under strong assumptions made by\nhuman designers. However, as demonstrated in other areas of machine learning,\nhuman expertise is often not entirely accurate and tends to be outperformed in\ndomains with abundant data. In this study, we examine whether we can enhance\ndomain-specific causal discovery for time series using a data-driven approach.\nOur findings indicate that this procedure significantly outperforms\nhuman-designed, domain-agnostic causal discovery methods, such as Mutual\nInformation, VAR-LiNGAM, and Granger Causality on the MOS 6502 microprocessor,\nthe NetSim fMRI dataset, and the Dream3 gene dataset. We argue that, when\nfeasible, the causality field should consider a supervised approach in which\ndomain-specific CD procedures are learned from extensive datasets with known\ncausal relationships, rather than being designed by human specialists. Our\nfindings promise a new approach toward improving CD in neural and medical data\nand for the broader machine learning community."}, "http://arxiv.org/abs/2209.05795": {"title": "Joint modelling of the body and tail of bivariate data", "link": "http://arxiv.org/abs/2209.05795", "description": "In situations where both extreme and non-extreme data are of interest,\nmodelling the whole data set accurately is important. In a univariate\nframework, modelling the bulk and tail of a distribution has been extensively\nstudied before. However, when more than one variable is of concern, models that\naim specifically at capturing both regions correctly are scarce in the\nliterature. A dependence model that blends two copulas with different\ncharacteristics over the whole range of the data support is proposed. One\ncopula is tailored to the bulk and the other to the tail, with a dynamic\nweighting function employed to transition smoothly between them. Tail\ndependence properties are investigated numerically and simulation is used to\nconfirm that the blended model is sufficiently flexible to capture a wide\nvariety of structures. The model is applied to study the dependence between\ntemperature and ozone concentration at two sites in the UK and compared with a\nsingle copula fit. The proposed model provides a better, more flexible, fit to\nthe data, and is also capable of capturing complex dependence structures."}, "http://arxiv.org/abs/2212.14650": {"title": "Two-step estimators of high dimensional correlation matrices", "link": "http://arxiv.org/abs/2212.14650", "description": "We investigate block diagonal and hierarchical nested stochastic multivariate\nGaussian models by studying their sample cross-correlation matrix on high\ndimensions. By performing numerical simulations, we compare a filtered sample\ncross-correlation with the population cross-correlation matrices by using\nseveral rotationally invariant estimators (RIE) and hierarchical clustering\nestimators (HCE) under several loss functions. We show that at large but finite\nsample size, sample cross-correlation filtered by RIE estimators are often\noutperformed by HCE estimators for several of the loss functions. We also show\nthat for block models and for hierarchically nested block models the best\ndetermination of the filtered sample cross-correlation is achieved by\nintroducing two-step estimators combining state-of-the-art non-linear shrinkage\nmodels with hierarchical clustering estimators."}, "http://arxiv.org/abs/2302.02457": {"title": "Scalable inference in functional linear regression with streaming data", "link": "http://arxiv.org/abs/2302.02457", "description": "Traditional static functional data analysis is facing new challenges due to\nstreaming data, where data constantly flow in. A major challenge is that\nstoring such an ever-increasing amount of data in memory is nearly impossible.\nIn addition, existing inferential tools in online learning are mainly developed\nfor finite-dimensional problems, while inference methods for functional data\nare focused on the batch learning setting. In this paper, we tackle these\nissues by developing functional stochastic gradient descent algorithms and\nproposing an online bootstrap resampling procedure to systematically study the\ninference problem for functional linear regression. In particular, the proposed\nestimation and inference procedures use only one pass over the data; thus they\nare easy to implement and suitable to the situation where data arrive in a\nstreaming manner. Furthermore, we establish the convergence rate as well as the\nasymptotic distribution of the proposed estimator. Meanwhile, the proposed\nperturbed estimator from the bootstrap procedure is shown to enjoy the same\ntheoretical properties, which provide the theoretical justification for our\nonline inference tool. As far as we know, this is the first inference result on\nthe functional linear regression model with streaming data. Simulation studies\nare conducted to investigate the finite-sample performance of the proposed\nprocedure. An application is illustrated with the Beijing multi-site\nair-quality data."}, "http://arxiv.org/abs/2303.09598": {"title": "Variational Bayesian analysis of survival data using a log-logistic accelerated failure time model", "link": "http://arxiv.org/abs/2303.09598", "description": "The log-logistic regression model is one of the most commonly used\naccelerated failure time (AFT) models in survival analysis, for which\nstatistical inference methods are mainly established under the frequentist\nframework. Recently, Bayesian inference for log-logistic AFT models using\nMarkov chain Monte Carlo (MCMC) techniques has also been widely developed. In\nthis work, we develop an alternative approach to MCMC methods and infer the\nparameters of the log-logistic AFT model via a mean-field variational Bayes\n(VB) algorithm. A piecewise approximation technique is embedded in deriving the\nVB algorithm to achieve conjugacy. The proposed VB algorithm is evaluated and\ncompared with typical frequentist inferences and MCMC inference using simulated\ndata under various scenarios. A publicly available dataset is employed for\nillustration. We demonstrate that the proposed VB algorithm can achieve good\nestimation accuracy and has a lower computational cost compared with MCMC\nmethods."}, "http://arxiv.org/abs/2304.03853": {"title": "StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables", "link": "http://arxiv.org/abs/2304.03853", "description": "StepMix is an open-source Python package for the pseudo-likelihood estimation\n(one-, two- and three-step approaches) of generalized finite mixture models\n(latent profile and latent class analysis) with external variables (covariates\nand distal outcomes). In many applications in social sciences, the main\nobjective is not only to cluster individuals into latent classes, but also to\nuse these classes to develop more complex statistical models. These models\ngenerally divide into a measurement model that relates the latent classes to\nobserved indicators, and a structural model that relates covariates and outcome\nvariables to the latent classes. The measurement and structural models can be\nestimated jointly using the so-called one-step approach or sequentially using\nstepwise methods, which present significant advantages for practitioners\nregarding the interpretability of the estimated latent classes. In addition to\nthe one-step approach, StepMix implements the most important stepwise\nestimation methods from the literature, including the bias-adjusted three-step\nmethods with Bolk-Croon-Hagenaars and maximum likelihood corrections and the\nmore recent two-step approach. These pseudo-likelihood estimators are presented\nin this paper under a unified framework as specific expectation-maximization\nsubroutines. To facilitate and promote their adoption among the data science\ncommunity, StepMix follows the object-oriented design of the scikit-learn\nlibrary and provides an additional R wrapper."}, "http://arxiv.org/abs/2310.06926": {"title": "Bayesian inference and cure rate modeling for event history data", "link": "http://arxiv.org/abs/2310.06926", "description": "Estimating model parameters of a general family of cure models is always a\nchallenging task mainly due to flatness and multimodality of the likelihood\nfunction. In this work, we propose a fully Bayesian approach in order to\novercome these issues. Posterior inference is carried out by constructing a\nMetropolis-coupled Markov chain Monte Carlo (MCMC) sampler, which combines\nGibbs sampling for the latent cure indicators and Metropolis-Hastings steps\nwith Langevin diffusion dynamics for parameter updates. The main MCMC algorithm\nis embedded within a parallel tempering scheme by considering heated versions\nof the target posterior distribution. It is demonstrated via simulations that\nthe proposed algorithm freely explores the multimodal posterior distribution\nand produces robust point estimates, while it outperforms maximum likelihood\nestimation via the Expectation-Maximization algorithm. A by-product of our\nBayesian implementation is to control the False Discovery Rate when classifying\nitems as cured or not. Finally, the proposed method is illustrated in a real\ndataset which refers to recidivism for offenders released from prison; the\nevent of interest is whether the offender was re-incarcerated after probation\nor not."}, "http://arxiv.org/abs/2310.06969": {"title": "Positivity-free Policy Learning with Observational Data", "link": "http://arxiv.org/abs/2310.06969", "description": "Policy learning utilizing observational data is pivotal across various\ndomains, with the objective of learning the optimal treatment assignment policy\nwhile adhering to specific constraints such as fairness, budget, and\nsimplicity. This study introduces a novel positivity-free (stochastic) policy\nlearning framework designed to address the challenges posed by the\nimpracticality of the positivity assumption in real-world scenarios. This\nframework leverages incremental propensity score policies to adjust propensity\nscore values instead of assigning fixed values to treatments. We characterize\nthese incremental propensity score policies and establish identification\nconditions, employing semiparametric efficiency theory to propose efficient\nestimators capable of achieving rapid convergence rates, even when integrated\nwith advanced machine learning algorithms. This paper provides a thorough\nexploration of the theoretical guarantees associated with policy learning and\nvalidates the proposed framework's finite-sample performance through\ncomprehensive numerical experiments, ensuring the identification of causal\neffects from observational data is both robust and reliable."}, "http://arxiv.org/abs/2310.07002": {"title": "Bayesian cross-validation by parallel Markov Chain Monte Carlo", "link": "http://arxiv.org/abs/2310.07002", "description": "Brute force cross-validation (CV) is a method for predictive assessment and\nmodel selection that is general and applicable to a wide range of Bayesian\nmodels. However, in many cases brute force CV is too computationally burdensome\nto form part of interactive modeling workflows, especially when inference\nrelies on Markov chain Monte Carlo (MCMC). In this paper we present a method\nfor conducting fast Bayesian CV by massively parallel MCMC. On suitable\naccelerator hardware, for many applications our approach is about as fast (in\nwall clock time) as a single full-data model fit.\n\nParallel CV is more flexible than existing fast CV approximation methods\nbecause it can easily exploit a wide range of scoring rules and data\npartitioning schemes. This is particularly useful for CV methods designed for\nnon-exchangeable data. Our approach also delivers accurate estimates of Monte\nCarlo and CV uncertainty. In addition to parallelizing computations, parallel\nCV speeds up inference by reusing information from earlier MCMC adaptation and\ninference obtained during initial model fitting and checking of the full-data\nmodel.\n\nWe propose MCMC diagnostics for parallel CV applications, including a summary\nof MCMC mixing based on the popular potential scale reduction factor\n($\\hat{R}$) and MCMC effective sample size ($\\widehat{ESS}$) measures.\nFurthermore, we describe a method for determining whether an $\\hat{R}$\ndiagnostic indicates approximate stationarity of the chains, that may be of\nmore general interest for applications beyond parallel CV.\n\nFor parallel CV to work on memory-constrained computing accelerators, we show\nthat parallel CV and associated diagnostics can be implemented using online\n(streaming) algorithms ideal for parallel computing environments with limited\nmemory. Constant memory algorithms allow parallel CV to scale up to very large\nblocking designs."}, "http://arxiv.org/abs/2310.07016": {"title": "Discovering the Unknowns: A First Step", "link": "http://arxiv.org/abs/2310.07016", "description": "This article aims at discovering the unknown variables in the system through\ndata analysis. The main idea is to use the time of data collection as a\nsurrogate variable and try to identify the unknown variables by modeling\ngradual and sudden changes in the data. We use Gaussian process modeling and a\nsparse representation of the sudden changes to efficiently estimate the large\nnumber of parameters in the proposed statistical model. The method is tested on\na realistic dataset generated using a one-dimensional implementation of a\nMagnetized Liner Inertial Fusion (MagLIF) simulation model and encouraging\nresults are obtained."}, "http://arxiv.org/abs/2310.07107": {"title": "Root n consistent extremile regression and its supervised and semi-supervised learning", "link": "http://arxiv.org/abs/2310.07107", "description": "Extremile (Daouia, Gijbels and Stupfler,2019) is a novel and coherent measure\nof risk, determined by weighted expectations rather than tail probabilities. It\nfinds application in risk management, and, in contrast to quantiles, it\nfulfills the axioms of consistency, taking into account the severity of tail\nlosses. However, existing studies (Daouia, Gijbels and Stupfler,2019,2022) on\nextremile involve unknown distribution functions, making it challenging to\nobtain a root n-consistent estimator for unknown parameters in linear extremile\nregression. This article introduces a new definition of linear extremile\nregression and its estimation method, where the estimator is root n-consistent.\nAdditionally, while the analysis of unlabeled data for extremes presents a\nsignificant challenge and is currently a topic of great interest in machine\nlearning for various classification problems, we have developed a\nsemi-supervised framework for the proposed extremile regression using unlabeled\ndata. This framework can also enhance estimation accuracy under model\nmisspecification. Both simulations and real data analyses have been conducted\nto illustrate the finite sample performance of the proposed methods."}, "http://arxiv.org/abs/2310.07124": {"title": "Systematic simulation of age-period-cohort analysis: Demonstrating bias of Bayesian regularization", "link": "http://arxiv.org/abs/2310.07124", "description": "Age-period-cohort (APC) analysis is one of the fundamental time-series\nanalyses used in the social sciences. This paper evaluates APC analysis via\nsystematic simulation in term of how well the artificial parameters are\nrecovered. We consider three models of Bayesian regularization using normal\nprior distributions: the random effects model with reference to multilevel\nanalysis, the ridge regression model equivalent to the intrinsic estimator, and\nthe random walk model referred to as the Bayesian cohort model. The proposed\nsimulation generates artificial data through combinations of the linear\ncomponents, focusing on the fact that the identification problem affects the\nlinear components of the three effects. Among the 13 cases of artificial data,\nthe random walk model recovered the artificial parameters well in 10 cases,\nwhile the random effects model and the ridge regression model did so in 4\ncases. The cases in which the models failed to recover the artificial\nparameters show the estimated linear component of the cohort effects as close\nto zero. In conclusion, the models of Bayesian regularization in APC analysis\nhave a bias: the index weights have a large influence on the cohort effects and\nthese constraints drive the linear component of the cohort effects close to\nzero. However, the random walk model mitigates underestimating the linear\ncomponent of the cohort effects."}, "http://arxiv.org/abs/2310.07330": {"title": "Functional Generalized Canonical Correlation Analysis for studying multiple longitudinal variables", "link": "http://arxiv.org/abs/2310.07330", "description": "In this paper, we introduce Functional Generalized Canonical Correlation\nAnalysis (FGCCA), a new framework for exploring associations between multiple\nrandom processes observed jointly. The framework is based on the multiblock\nRegularized Generalized Canonical Correlation Analysis (RGCCA) framework. It is\nrobust to sparsely and irregularly observed data, making it applicable in many\nsettings. We establish the monotonic property of the solving procedure and\nintroduce a Bayesian approach for estimating canonical components. We propose\nan extension of the framework that allows the integration of a univariate or\nmultivariate response into the analysis, paving the way for predictive\napplications. We evaluate the method's efficiency in simulation studies and\npresent a use case on a longitudinal dataset."}, "http://arxiv.org/abs/2310.07364": {"title": "Statistical inference of high-dimensional vector autoregressive time series with non-i", "link": "http://arxiv.org/abs/2310.07364", "description": "Independent or i.i.d. innovations is an essential assumption in the\nliterature for analyzing a vector time series. However, this assumption is\neither too restrictive for a real-life time series to satisfy or is hard to\nverify through a hypothesis test. This paper performs statistical inference on\na sparse high-dimensional vector autoregressive time series, allowing its white\nnoise innovations to be dependent, even non-stationary. To achieve this goal,\nit adopts a post-selection estimator to fit the vector autoregressive model and\nderives the asymptotic distribution of the post-selection estimator. The\ninnovations in the autoregressive time series are not assumed to be\nindependent, thus making the covariance matrices of the autoregressive\ncoefficient estimators complex and difficult to estimate. Our work develops a\nbootstrap algorithm to facilitate practitioners in performing statistical\ninference without having to engage in sophisticated calculations. Simulations\nand real-life data experiments reveal the validity of the proposed methods and\ntheoretical results.\n\nReal-life data is rarely considered to exactly satisfy an autoregressive\nmodel with independent or i.i.d. innovations, so our work should better reflect\nthe reality compared to the literature that assumes i.i.d. innovations."}, "http://arxiv.org/abs/2310.07399": {"title": "Randomized Runge-Kutta-Nystr\\\"om", "link": "http://arxiv.org/abs/2310.07399", "description": "We present 5/2- and 7/2-order $L^2$-accurate randomized Runge-Kutta-Nystr\\\"om\nmethods to approximate the Hamiltonian flow underlying various non-reversible\nMarkov chain Monte Carlo chains including unadjusted Hamiltonian Monte Carlo\nand unadjusted kinetic Langevin chains. Quantitative 5/2-order $L^2$-accuracy\nupper bounds are provided under gradient and Hessian Lipschitz assumptions on\nthe potential energy function. The superior complexity of the corresponding\nMarkov chains is numerically demonstrated for a selection of `well-behaved',\nhigh-dimensional target distributions."}, "http://arxiv.org/abs/2310.07456": {"title": "Hierarchical Bayesian Claim Count modeling with Overdispersed Outcome and Mismeasured Covariates in Actuarial Practice", "link": "http://arxiv.org/abs/2310.07456", "description": "The problem of overdispersed claim counts and mismeasured covariates is\ncommon in insurance. On the one hand, the presence of overdispersion in the\ncount data violates the homogeneity assumption, and on the other hand,\nmeasurement errors in covariates highlight the model risk issue in actuarial\npractice. The consequence can be inaccurate premium pricing which would\nnegatively affect business competitiveness. Our goal is to address these two\nmodelling problems simultaneously by capturing the unobservable correlations\nbetween observations that arise from overdispersed outcome and mismeasured\ncovariate in actuarial process. To this end, we establish novel connections\nbetween the count-based generalized linear mixed model (GLMM) and a popular\nerror-correction tool for non-linear modelling - Simulation Extrapolation\n(SIMEX). We consider a modelling framework based on the hierarchical Bayesian\nparadigm. To our knowledge, the approach of combining a hierarchical Bayes with\nSIMEX has not previously been discussed in the literature. We demonstrate the\napplicability of our approach on the workplace absenteeism data. Our results\nindicate that the hierarchical Bayesian GLMM incorporated with the SIMEX\noutperforms naive GLMM / SIMEX in terms of goodness of fit."}, "http://arxiv.org/abs/2310.07567": {"title": "Comparing the effectiveness of k-different treatments through the area under the ROC curve", "link": "http://arxiv.org/abs/2310.07567", "description": "The area under the receiver-operating characteristic curve (AUC) has become a\npopular index not only for measuring the overall prediction capacity of a\nmarker but also the association strength between continuous and binary\nvariables. In the current study, it has been used for comparing the association\nsize of four different interventions involving impulsive decision making,\nstudied through an animal model, in which each animal provides several negative\n(pre-treatment) and positive (post-treatment) measures. The problem of the full\ncomparison of the average AUCs arises therefore in a natural way. We construct\nan analysis of variance (ANOVA) type test for testing the equality of the\nimpact of these treatments measured through the respective AUCs, and\nconsidering the random-effect represented by the animal. The use (and\ndevelopment) of a post-hoc Tukey's HSD type test is also considered. We explore\nthe finite-sample behavior of our proposal via Monte Carlo simulations, and\nanalyze the data generated from the original problem. An R package implementing\nthe procedures is provided as supplementary material."}, "http://arxiv.org/abs/2310.07605": {"title": "Split Knockoffs for Multiple Comparisons: Controlling the Directional False Discovery Rate", "link": "http://arxiv.org/abs/2310.07605", "description": "Multiple comparisons in hypothesis testing often encounter structural\nconstraints in various applications. For instance, in structural Magnetic\nResonance Imaging for Alzheimer's Disease, the focus extends beyond examining\natrophic brain regions to include comparisons of anatomically adjacent regions.\nThese constraints can be modeled as linear transformations of parameters, where\nthe sign patterns play a crucial role in estimating directional effects. This\nclass of problems, encompassing total variations, wavelet transforms, fused\nLASSO, trend filtering, and more, presents an open challenge in effectively\ncontrolling the directional false discovery rate. In this paper, we propose an\nextended Split Knockoff method specifically designed to address the control of\ndirectional false discovery rate under linear transformations. Our proposed\napproach relaxes the stringent linear manifold constraint to its neighborhood,\nemploying a variable splitting technique commonly used in optimization. This\nmethodology yields an orthogonal design that benefits both power and\ndirectional false discovery rate control. By incorporating a sample splitting\nscheme, we achieve effective control of the directional false discovery rate,\nwith a notable reduction to zero as the relaxed neighborhood expands. To\ndemonstrate the efficacy of our method, we conduct simulation experiments and\napply it to two real-world scenarios: Alzheimer's Disease analysis and human\nage comparisons."}, "http://arxiv.org/abs/2310.07680": {"title": "Hamiltonian Dynamics of Bayesian Inference Formalised by Arc Hamiltonian Systems", "link": "http://arxiv.org/abs/2310.07680", "description": "This paper makes two theoretical contributions. First, we establish a novel\nclass of Hamiltonian systems, called arc Hamiltonian systems, for saddle\nHamiltonian functions over infinite-dimensional metric spaces. Arc Hamiltonian\nsystems generate a flow that satisfies the law of conservation of energy\neverywhere in a metric space. They are governed by an extension of Hamilton's\nequation formulated based on (i) the framework of arc fields and (ii) an\ninfinite-dimensional gradient, termed the arc gradient, of a Hamiltonian\nfunction. We derive conditions for the existence of a flow generated by an arc\nHamiltonian system, showing that they reduce to local Lipschitz continuity of\nthe arc gradient under sufficient regularity. Second, we present two\nHamiltonian functions, called the cumulant generating functional and the\ncentred cumulant generating functional, over a metric space of log-likelihoods\nand measures. The former characterises the posterior of Bayesian inference as a\npart of the arc gradient that induces a flow of log-likelihoods and\nnon-negative measures. The latter characterises the difference of the posterior\nand the prior as a part of the arc gradient that induces a flow of\nlog-likelihoods and probability measures. Our results reveal an implication of\nthe belief updating mechanism from the prior to the posterior as an\ninfinitesimal change of a measure in the infinite-dimensional Hamiltonian\nflows."}, "http://arxiv.org/abs/2009.12217": {"title": "Latent Causal Socioeconomic Health Index", "link": "http://arxiv.org/abs/2009.12217", "description": "This research develops a model-based LAtent Causal Socioeconomic Health\n(LACSH) index at the national level. Motivated by the need for a holistic\nnational well-being index, we build upon the latent health factor index (LHFI)\napproach that has been used to assess the unobservable ecological/ecosystem\nhealth. LHFI integratively models the relationship between metrics, latent\nhealth, and covariates that drive the notion of health. In this paper, the LHFI\nstructure is integrated with spatial modeling and statistical causal modeling.\nOur efforts are focused on developing the integrated framework to facilitate\nthe understanding of how an observational continuous variable might have\ncausally affected a latent trait that exhibits spatial correlation. A novel\nvisualization technique to evaluate covariate balance is also introduced for\nthe case of a continuous policy (treatment) variable. Our resulting LACSH\nframework and visualization tool are illustrated through two global case\nstudies on national socioeconomic health (latent trait), each with various\nmetrics and covariates pertaining to different aspects of societal health, and\nthe treatment variable being mandatory maternity leave days and government\nexpenditure on healthcare, respectively. We validate our model by two\nsimulation studies. All approaches are structured in a Bayesian hierarchical\nframework and results are obtained by Markov chain Monte Carlo techniques."}, "http://arxiv.org/abs/2201.02958": {"title": "Smooth Nested Simulation: Bridging Cubic and Square Root Convergence Rates in High Dimensions", "link": "http://arxiv.org/abs/2201.02958", "description": "Nested simulation concerns estimating functionals of a conditional\nexpectation via simulation. In this paper, we propose a new method based on\nkernel ridge regression to exploit the smoothness of the conditional\nexpectation as a function of the multidimensional conditioning variable.\nAsymptotic analysis shows that the proposed method can effectively alleviate\nthe curse of dimensionality on the convergence rate as the simulation budget\nincreases, provided that the conditional expectation is sufficiently smooth.\nThe smoothness bridges the gap between the cubic root convergence rate (that\nis, the optimal rate for the standard nested simulation) and the square root\nconvergence rate (that is, the canonical rate for the standard Monte Carlo\nsimulation). We demonstrate the performance of the proposed method via\nnumerical examples from portfolio risk management and input uncertainty\nquantification."}, "http://arxiv.org/abs/2204.12635": {"title": "Multivariate and regression models for directional data based on projected P\\'olya trees", "link": "http://arxiv.org/abs/2204.12635", "description": "Projected distributions have proved to be useful in the study of circular and\ndirectional data. Although any multivariate distribution can be used to produce\na projected model, these distributions are typically parametric. In this\narticle we consider a multivariate P\\'olya tree on $R^k$ and project it to the\nunit hypersphere $S^k$ to define a new Bayesian nonparametric model for\ndirectional data. We study the properties of the proposed model and in\nparticular, concentrate on the implied conditional distributions of some\ndirections given the others to define a directional-directional regression\nmodel. We also define a multivariate linear regression model with P\\'olya tree\nerror and project it to define a linear-directional regression model. We obtain\nthe posterior characterisation of all models and show their performance with\nsimulated and real datasets."}, "http://arxiv.org/abs/2207.13250": {"title": "Spatio-Temporal Wildfire Prediction using Multi-Modal Data", "link": "http://arxiv.org/abs/2207.13250", "description": "Due to severe societal and environmental impacts, wildfire prediction using\nmulti-modal sensing data has become a highly sought-after data-analytical tool\nby various stakeholders (such as state governments and power utility companies)\nto achieve a more informed understanding of wildfire activities and plan\npreventive measures. A desirable algorithm should precisely predict fire risk\nand magnitude for a location in real time. In this paper, we develop a flexible\nspatio-temporal wildfire prediction framework using multi-modal time series\ndata. We first predict the wildfire risk (the chance of a wildfire event) in\nreal-time, considering the historical events using discrete mutually exciting\npoint process models. Then we further develop a wildfire magnitude prediction\nset method based on the flexible distribution-free time-series conformal\nprediction (CP) approach. Theoretically, we prove a risk model parameter\nrecovery guarantee, as well as coverage and set size guarantees for the CP\nsets. Through extensive real-data experiments with wildfire data in California,\nwe demonstrate the effectiveness of our methods, as well as their flexibility\nand scalability in large regions."}, "http://arxiv.org/abs/2210.13550": {"title": "Regularized Nonlinear Regression with Dependent Errors and its Application to a Biomechanical Model", "link": "http://arxiv.org/abs/2210.13550", "description": "A biomechanical model often requires parameter estimation and selection in a\nknown but complicated nonlinear function. Motivated by observing that data from\na head-neck position tracking system, one of biomechanical models, show\nmultiplicative time dependent errors, we develop a modified penalized weighted\nleast squares estimator. The proposed method can be also applied to a model\nwith non-zero mean time dependent additive errors. Asymptotic properties of the\nproposed estimator are investigated under mild conditions on a weight matrix\nand the error process. A simulation study demonstrates that the proposed\nestimation works well in both parameter estimation and selection with time\ndependent error. The analysis and comparison with an existing method for\nhead-neck position tracking data show better performance of the proposed method\nin terms of the variance accounted for (VAF)."}, "http://arxiv.org/abs/2210.14965": {"title": "Topology-Driven Goodness-of-Fit Tests in Arbitrary Dimensions", "link": "http://arxiv.org/abs/2210.14965", "description": "This paper adopts a tool from computational topology, the Euler\ncharacteristic curve (ECC) of a sample, to perform one- and two-sample goodness\nof fit tests. We call our procedure TopoTests. The presented tests work for\nsamples of arbitrary dimension, having comparable power to the state-of-the-art\ntests in the one-dimensional case. It is demonstrated that the type I error of\nTopoTests can be controlled and their type II error vanishes exponentially with\nincreasing sample size. Extensive numerical simulations of TopoTests are\nconducted to demonstrate their power for samples of various sizes."}, "http://arxiv.org/abs/2211.03860": {"title": "Automatic Change-Point Detection in Time Series via Deep Learning", "link": "http://arxiv.org/abs/2211.03860", "description": "Detecting change-points in data is challenging because of the range of\npossible types of change and types of behaviour of data when there is no\nchange. Statistically efficient methods for detecting a change will depend on\nboth of these features, and it can be difficult for a practitioner to develop\nan appropriate detection method for their application of interest. We show how\nto automatically generate new offline detection methods based on training a\nneural network. Our approach is motivated by many existing tests for the\npresence of a change-point being representable by a simple neural network, and\nthus a neural network trained with sufficient data should have performance at\nleast as good as these methods. We present theory that quantifies the error\nrate for such an approach, and how it depends on the amount of training data.\nEmpirical results show that, even with limited training data, its performance\nis competitive with the standard CUSUM-based classifier for detecting a change\nin mean when the noise is independent and Gaussian, and can substantially\noutperform it in the presence of auto-correlated or heavy-tailed noise. Our\nmethod also shows strong results in detecting and localising changes in\nactivity based on accelerometer data."}, "http://arxiv.org/abs/2211.09099": {"title": "Selecting Subpopulations for Causal Inference in Regression Discontinuity Designs", "link": "http://arxiv.org/abs/2211.09099", "description": "The Brazil Bolsa Familia (BF) program is a conditional cash transfer program\naimed to reduce short-term poverty by direct cash transfers and to fight\nlong-term poverty by increasing human capital among poor Brazilian people.\nEligibility for Bolsa Familia benefits depends on a cutoff rule, which\nclassifies the BF study as a regression discontinuity (RD) design. Extracting\ncausal information from RD studies is challenging. Following Li et al (2015)\nand Branson and Mealli (2019), we formally describe the BF RD design as a local\nrandomized experiment within the potential outcome approach. Under this\nframework, causal effects can be identified and estimated on a subpopulation\nwhere a local overlap assumption, a local SUTVA and a local ignorability\nassumption hold. We first discuss the potential advantages of this framework\nover local regression methods based on continuity assumptions, which concern\nthe definition of the causal estimands, the design and the analysis of the\nstudy, and the interpretation and generalizability of the results. A critical\nissue of this local randomization approach is how to choose subpopulations for\nwhich we can draw valid causal inference. We propose a Bayesian model-based\nfinite mixture approach to clustering to classify observations into\nsubpopulations where the RD assumptions hold and do not hold. This approach has\nimportant advantages: a) it allows to account for the uncertainty in the\nsubpopulation membership, which is typically neglected; b) it does not impose\nany constraint on the shape of the subpopulation; c) it is scalable to\nhigh-dimensional settings; e) it allows to target alternative causal estimands\nthan the average treatment effect (ATE); and f) it is robust to a certain\ndegree of manipulation/selection of the running variable. We apply our proposed\napproach to assess causal effects of the Bolsa Familia program on leprosy\nincidence in 2009."}, "http://arxiv.org/abs/2301.08276": {"title": "Cross-validatory model selection for Bayesian autoregressions with exogenous regressors", "link": "http://arxiv.org/abs/2301.08276", "description": "Bayesian cross-validation (CV) is a popular method for predictive model\nassessment that is simple to implement and broadly applicable. A wide range of\nCV schemes is available for time series applications, including generic\nleave-one-out (LOO) and K-fold methods, as well as specialized approaches\nintended to deal with serial dependence such as leave-future-out (LFO),\nh-block, and hv-block.\n\nExisting large-sample results show that both specialized and generic methods\nare applicable to models of serially-dependent data. However, large sample\nconsistency results overlook the impact of sampling variability on accuracy in\nfinite samples. Moreover, the accuracy of a CV scheme depends on many aspects\nof the procedure. We show that poor design choices can lead to elevated rates\nof adverse selection.\n\nIn this paper, we consider the problem of identifying the regression\ncomponent of an important class of models of data with serial dependence,\nautoregressions of order p with q exogenous regressors (ARX(p,q)), under the\nlogarithmic scoring rule. We show that when serial dependence is present,\nscores computed using the joint (multivariate) density have lower variance and\nbetter model selection accuracy than the popular pointwise estimator. In\naddition, we present a detailed case study of the special case of ARX models\nwith fixed autoregressive structure and variance. For this class, we derive the\nfinite-sample distribution of the CV estimators and the model selection\nstatistic. We conclude with recommendations for practitioners."}, "http://arxiv.org/abs/2301.12026": {"title": "G-formula for causal inference via multiple imputation", "link": "http://arxiv.org/abs/2301.12026", "description": "G-formula is a popular approach for estimating treatment or exposure effects\nfrom longitudinal data that are subject to time-varying confounding. G-formula\nestimation is typically performed by Monte-Carlo simulation, with\nnon-parametric bootstrapping used for inference. We show that G-formula can be\nimplemented by exploiting existing methods for multiple imputation (MI) for\nsynthetic data. This involves using an existing modified version of Rubin's\nvariance estimator. In practice missing data is ubiquitous in longitudinal\ndatasets. We show that such missing data can be readily accommodated as part of\nthe MI procedure when using G-formula, and describe how MI software can be used\nto implement the approach. We explore its performance using a simulation study\nand an application from cystic fibrosis."}, "http://arxiv.org/abs/2306.01292": {"title": "Alternative Measures of Direct and Indirect Effects", "link": "http://arxiv.org/abs/2306.01292", "description": "There are a number of measures of direct and indirect effects in the\nliterature. They are suitable in some cases and unsuitable in others. We\ndescribe a case where the existing measures are unsuitable and propose new\nsuitable ones. We also show that the new measures can partially handle\nunmeasured treatment-outcome confounding, and bound long-term effects by\ncombining experimental and observational data."}, "http://arxiv.org/abs/2308.00913": {"title": "The Bayesian Context Trees State Space Model for time series modelling and forecasting", "link": "http://arxiv.org/abs/2308.00913", "description": "A hierarchical Bayesian framework is introduced for developing rich mixture\nmodels for real-valued time series, partly motivated by important applications\nin financial time series analysis. At the top level, meaningful discrete states\nare identified as appropriately quantised values of some of the most recent\nsamples. These observable states are described as a discrete context-tree\nmodel. At the bottom level, a different, arbitrary model for real-valued time\nseries -- a base model -- is associated with each state. This defines a very\ngeneral framework that can be used in conjunction with any existing model class\nto build flexible and interpretable mixture models. We call this the Bayesian\nContext Trees State Space Model, or the BCT-X framework. Efficient algorithms\nare introduced that allow for effective, exact Bayesian inference and learning\nin this setting; in particular, the maximum a posteriori probability (MAP)\ncontext-tree model can be identified. These algorithms can be updated\nsequentially, facilitating efficient online forecasting. The utility of the\ngeneral framework is illustrated in two particular instances: When\nautoregressive (AR) models are used as base models, resulting in a nonlinear AR\nmixture model, and when conditional heteroscedastic (ARCH) models are used,\nresulting in a mixture model that offers a powerful and systematic way of\nmodelling the well-known volatility asymmetries in financial data. In\nforecasting, the BCT-X methods are found to outperform state-of-the-art\ntechniques on simulated and real-world data, both in terms of accuracy and\ncomputational requirements. In modelling, the BCT-X structure finds natural\nstructure present in the data. In particular, the BCT-ARCH model reveals a\nnovel, important feature of stock market index data, in the form of an enhanced\nleverage effect."}, "http://arxiv.org/abs/2309.11942": {"title": "On the Probability of Immunity", "link": "http://arxiv.org/abs/2309.11942", "description": "This work is devoted to the study of the probability of immunity, i.e. the\neffect occurs whether exposed or not. We derive necessary and sufficient\nconditions for non-immunity and $\\epsilon$-bounded immunity, i.e. the\nprobability of immunity is zero and $\\epsilon$-bounded, respectively. The\nformer allows us to estimate the probability of benefit (i.e., the effect\noccurs if and only if exposed) from a randomized controlled trial, and the\nlatter allows us to produce bounds of the probability of benefit that are\ntighter than the existing ones. We also introduce the concept of indirect\nimmunity (i.e., through a mediator) and repeat our previous analysis for it.\nFinally, we propose a method for sensitivity analysis of the probability of\nimmunity under unmeasured confounding."}, "http://arxiv.org/abs/2309.13441": {"title": "Anytime valid and asymptotically optimal statistical inference driven by predictive recursion", "link": "http://arxiv.org/abs/2309.13441", "description": "Distinguishing two classes of candidate models is a fundamental and\npractically important problem in statistical inference. Error rate control is\ncrucial to the logic but, in complex nonparametric settings, such guarantees\ncan be difficult to achieve, especially when the stopping rule that determines\nthe data collection process is not available. In this paper we develop a novel\ne-process construction that leverages the so-called predictive recursion (PR)\nalgorithm designed to rapidly and recursively fit nonparametric mixture models.\nThe resulting PRe-process affords anytime valid inference uniformly over\nstopping rules and is shown to be efficient in the sense that it achieves the\nmaximal growth rate under the alternative relative to the mixture model being\nfit by PR. In the special case of testing for a log-concave density, the\nPRe-process test is computationally simpler and faster, more stable, and no\nless efficient compared to a recently proposed anytime valid test."}, "http://arxiv.org/abs/2309.16598": {"title": "Cross-Prediction-Powered Inference", "link": "http://arxiv.org/abs/2309.16598", "description": "While reliable data-driven decision-making hinges on high-quality labeled\ndata, the acquisition of quality labels often involves laborious human\nannotations or slow and expensive scientific measurements. Machine learning is\nbecoming an appealing alternative as sophisticated predictive techniques are\nbeing used to quickly and cheaply produce large amounts of predicted labels;\ne.g., predicted protein structures are used to supplement experimentally\nderived structures, predictions of socioeconomic indicators from satellite\nimagery are used to supplement accurate survey data, and so on. Since\npredictions are imperfect and potentially biased, this practice brings into\nquestion the validity of downstream inferences. We introduce cross-prediction:\na method for valid inference powered by machine learning. With a small labeled\ndataset and a large unlabeled dataset, cross-prediction imputes the missing\nlabels via machine learning and applies a form of debiasing to remedy the\nprediction inaccuracies. The resulting inferences achieve the desired error\nprobability and are more powerful than those that only leverage the labeled\ndata. Closely related is the recent proposal of prediction-powered inference,\nwhich assumes that a good pre-trained model is already available. We show that\ncross-prediction is consistently more powerful than an adaptation of\nprediction-powered inference in which a fraction of the labeled data is split\noff and used to train the model. Finally, we observe that cross-prediction\ngives more stable conclusions than its competitors; its confidence intervals\ntypically have significantly lower variability."}, "http://arxiv.org/abs/2310.07801": {"title": "Trajectory-aware Principal Manifold Framework for Data Augmentation and Image Generation", "link": "http://arxiv.org/abs/2310.07801", "description": "Data augmentation for deep learning benefits model training, image\ntransformation, medical imaging analysis and many other fields. Many existing\nmethods generate new samples from a parametric distribution, like the Gaussian,\nwith little attention to generate samples along the data manifold in either the\ninput or feature space. In this paper, we verify that there are theoretical and\npractical advantages of using the principal manifold hidden in the feature\nspace than the Gaussian distribution. We then propose a novel trajectory-aware\nprincipal manifold framework to restore the manifold backbone and generate\nsamples along a specific trajectory. On top of the autoencoder architecture, we\nfurther introduce an intrinsic dimension regularization term to make the\nmanifold more compact and enable few-shot image generation. Experimental\nresults show that the novel framework is able to extract more compact manifold\nrepresentation, improve classification accuracy and generate smooth\ntransformation among few samples."}, "http://arxiv.org/abs/2310.07817": {"title": "Nonlinear global Fr\\'echet regression for random objects via weak conditional expectation", "link": "http://arxiv.org/abs/2310.07817", "description": "Random objects are complex non-Euclidean data taking value in general metric\nspace, possibly devoid of any underlying vector space structure. Such data are\ngetting increasingly abundant with the rapid advancement in technology.\nExamples include probability distributions, positive semi-definite matrices,\nand data on Riemannian manifolds. However, except for regression for\nobject-valued response with Euclidean predictors and\ndistribution-on-distribution regression, there has been limited development of\na general framework for object-valued response with object-valued predictors in\nthe literature. To fill this gap, we introduce the notion of a weak conditional\nFr\\'echet mean based on Carleman operators and then propose a global nonlinear\nFr\\'echet regression model through the reproducing kernel Hilbert space (RKHS)\nembedding. Furthermore, we establish the relationships between the conditional\nFr\\'echet mean and the weak conditional Fr\\'echet mean for both Euclidean and\nobject-valued data. We also show that the state-of-the-art global Fr\\'echet\nregression developed by Petersen and Mueller, 2019 emerges as a special case of\nour method by choosing a linear kernel. We require that the metric space for\nthe predictor admits a reproducing kernel, while the intrinsic geometry of the\nmetric space for the response is utilized to study the asymptotic properties of\nthe proposed estimates. Numerical studies, including extensive simulations and\na real application, are conducted to investigate the performance of our\nestimator in a finite sample."}, "http://arxiv.org/abs/2310.07839": {"title": "Marital Sorting, Household Inequality and Selection", "link": "http://arxiv.org/abs/2310.07839", "description": "Using CPS data for 1976 to 2022 we explore how wage inequality has evolved\nfor married couples with both spouses working full time full year, and its\nimpact on household income inequality. We also investigate how marriage sorting\npatterns have changed over this period. To determine the factors driving income\ninequality we estimate a model explaining the joint distribution of wages which\naccounts for the spouses' employment decisions. We find that income inequality\nhas increased for these households and increased assortative matching of wages\nhas exacerbated the inequality resulting from individual wage growth. We find\nthat positive sorting partially reflects the correlation across unobservables\ninfluencing both members' of the marriage wages. We decompose the changes in\nsorting patterns over the 47 years comprising our sample into structural,\ncomposition and selection effects and find that the increase in positive\nsorting primarily reflects the increased skill premia for both observed and\nunobserved characteristics."}, "http://arxiv.org/abs/2310.07850": {"title": "Conformal prediction with local weights: randomization enables local guarantees", "link": "http://arxiv.org/abs/2310.07850", "description": "In this work, we consider the problem of building distribution-free\nprediction intervals with finite-sample conditional coverage guarantees.\nConformal prediction (CP) is an increasingly popular framework for building\nprediction intervals with distribution-free guarantees, but these guarantees\nonly ensure marginal coverage: the probability of coverage is averaged over a\nrandom draw of both the training and test data, meaning that there might be\nsubstantial undercoverage within certain subpopulations. Instead, ideally, we\nwould want to have local coverage guarantees that hold for each possible value\nof the test point's features. While the impossibility of achieving pointwise\nlocal coverage is well established in the literature, many variants of\nconformal prediction algorithm show favorable local coverage properties\nempirically. Relaxing the definition of local coverage can allow for a\ntheoretical understanding of this empirical phenomenon. We aim to bridge this\ngap between theoretical validation and empirical performance by proving\nachievable and interpretable guarantees for a relaxed notion of local coverage.\nBuilding on the localized CP method of Guan (2023) and the weighted CP\nframework of Tibshirani et al. (2019), we propose a new method,\nrandomly-localized conformal prediction (RLCP), which returns prediction\nintervals that are not only marginally valid but also achieve a relaxed local\ncoverage guarantee and guarantees under covariate shift. Through a series of\nsimulations and real data experiments, we validate these coverage guarantees of\nRLCP while comparing it with the other local conformal prediction methods."}, "http://arxiv.org/abs/2310.07852": {"title": "On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism", "link": "http://arxiv.org/abs/2310.07852", "description": "We consider the problem of model selection in a high-dimensional sparse\nlinear regression model under the differential privacy framework. In\nparticular, we consider the problem of differentially private best subset\nselection and study its utility guarantee. We adopt the well-known exponential\nmechanism for selecting the best model, and under a certain margin condition,\nwe establish its strong model recovery property. However, the exponential\nsearch space of the exponential mechanism poses a serious computational\nbottleneck. To overcome this challenge, we propose a Metropolis-Hastings\nalgorithm for the sampling step and establish its polynomial mixing time to its\nstationary distribution in the problem parameters $n,p$, and $s$. Furthermore,\nwe also establish approximate differential privacy for the final estimates of\nthe Metropolis-Hastings random walk using its mixing property. Finally, we also\nperform some illustrative simulations that echo the theoretical findings of our\nmain results."}, "http://arxiv.org/abs/2310.07935": {"title": "Estimating the Likelihood of Arrest from Police Records in Presence of Unreported Crimes", "link": "http://arxiv.org/abs/2310.07935", "description": "Many important policy decisions concerning policing hinge on our\nunderstanding of how likely various criminal offenses are to result in arrests.\nSince many crimes are never reported to law enforcement, estimates based on\npolice records alone must be adjusted to account for the likelihood that each\ncrime would have been reported to the police. In this paper, we present a\nmethodological framework for estimating the likelihood of arrest from police\ndata that incorporates estimates of crime reporting rates computed from a\nvictimization survey. We propose a parametric regression-based two-step\nestimator that (i) estimates the likelihood of crime reporting using logistic\nregression with survey weights; and then (ii) applies a second regression step\nto model the likelihood of arrest. Our empirical analysis focuses on racial\ndisparities in arrests for violent crimes (sex offenses, robbery, aggravated\nand simple assaults) from 2006--2015 police records from the National Incident\nBased Reporting System (NIBRS), with estimates of crime reporting obtained\nusing 2003--2020 data from the National Crime Victimization Survey (NCVS). We\nfind that, after adjusting for unreported crimes, the likelihood of arrest\ncomputed from police records decreases significantly. We also find that, while\nincidents with white offenders on average result in arrests more often than\nthose with black offenders, the disparities tend to be small after accounting\nfor crime characteristics and unreported crimes."}, "http://arxiv.org/abs/2310.07953": {"title": "Enhancing Sample Quality through Minimum Energy Importance Weights", "link": "http://arxiv.org/abs/2310.07953", "description": "Importance sampling is a powerful tool for correcting the distributional\nmismatch in many statistical and machine learning problems, but in practice its\nperformance is limited by the usage of simple proposals whose importance\nweights can be computed analytically. To address this limitation, Liu and Lee\n(2017) proposed a Black-Box Importance Sampling (BBIS) algorithm that computes\nthe importance weights for arbitrary simulated samples by minimizing the\nkernelized Stein discrepancy. However, this requires knowing the score function\nof the target distribution, which is not easy to compute for many Bayesian\nproblems. Hence, in this paper we propose another novel BBIS algorithm using\nminimum energy design, BBIS-MED, that requires only the unnormalized density\nfunction, which can be utilized as a post-processing step to improve the\nquality of Markov Chain Monte Carlo samples. We demonstrate the effectiveness\nand wide applicability of our proposed BBIS-MED algorithm on extensive\nsimulations and a real-world Bayesian model calibration problem where the score\nfunction cannot be derived analytically."}, "http://arxiv.org/abs/2310.07958": {"title": "Towards Causal Deep Learning for Vulnerability Detection", "link": "http://arxiv.org/abs/2310.07958", "description": "Deep learning vulnerability detection has shown promising results in recent\nyears. However, an important challenge that still blocks it from being very\nuseful in practice is that the model is not robust under perturbation and it\ncannot generalize well over the out-of-distribution (OOD) data, e.g., applying\na trained model to unseen projects in real world. We hypothesize that this is\nbecause the model learned non-robust features, e.g., variable names, that have\nspurious correlations with labels. When the perturbed and OOD datasets no\nlonger have the same spurious features, the model prediction fails. To address\nthe challenge, in this paper, we introduced causality into deep learning\nvulnerability detection. Our approach CausalVul consists of two phases. First,\nwe designed novel perturbations to discover spurious features that the model\nmay use to make predictions. Second, we applied the causal learning algorithms,\nspecifically, do-calculus, on top of existing deep learning models to\nsystematically remove the use of spurious features and thus promote causal\nbased prediction. Our results show that CausalVul consistently improved the\nmodel accuracy, robustness and OOD performance for all the state-of-the-art\nmodels and datasets we experimented. To the best of our knowledge, this is the\nfirst work that introduces do calculus based causal learning to software\nengineering models and shows it's indeed useful for improving the model\naccuracy, robustness and generalization. Our replication package is located at\nhttps://figshare.com/s/0ffda320dcb96c249ef2."}, "http://arxiv.org/abs/2310.07973": {"title": "Statistical Performance Guarantee for Selecting Those Predicted to Benefit Most from Treatment", "link": "http://arxiv.org/abs/2310.07973", "description": "Across a wide array of disciplines, many researchers use machine learning\n(ML) algorithms to identify a subgroup of individuals, called exceptional\nresponders, who are likely to be helped by a treatment the most. A common\napproach consists of two steps. One first estimates the conditional average\ntreatment effect or its proxy using an ML algorithm. They then determine the\ncutoff of the resulting treatment prioritization score to select those\npredicted to benefit most from the treatment. Unfortunately, these estimated\ntreatment prioritization scores are often biased and noisy. Furthermore,\nutilizing the same data to both choose a cutoff value and estimate the average\ntreatment effect among the selected individuals suffer from a multiple testing\nproblem. To address these challenges, we develop a uniform confidence band for\nexperimentally evaluating the sorted average treatment effect (GATES) among the\nindividuals whose treatment prioritization score is at least as high as any\ngiven quantile value, regardless of how the quantile is chosen. This provides a\nstatistical guarantee that the GATES for the selected subgroup exceeds a\ncertain threshold. The validity of the proposed methodology depends solely on\nrandomization of treatment and random sampling of units without requiring\nmodeling assumptions or resampling methods. This widens its applicability\nincluding a wide range of other causal quantities. A simulation study shows\nthat the empirical coverage of the proposed uniform confidence bands is close\nto the nominal coverage when the sample is as small as 100. We analyze a\nclinical trial of late-stage prostate cancer and find a relatively large\nproportion of exceptional responders with a statistical performance guarantee."}, "http://arxiv.org/abs/2310.08020": {"title": "Assessing Copula Models for Mixed Continuous-Ordinal Variables", "link": "http://arxiv.org/abs/2310.08020", "description": "Vine pair-copula constructions exist for a mix of continuous and ordinal\nvariables. In some steps, this can involve estimating a bivariate copula for a\npair of mixed continuous-ordinal variables. To assess the adequacy of copula\nfits for such a pair, diagnostic and visualization methods based on normal\nscore plots and conditional Q-Q plots are proposed. The former utilizes a\nlatent continuous variable for the ordinal variable. Using the Kullback-Leibler\ndivergence, existing probability models for mixed continuous-ordinal variable\npair are assessed for the adequacy of fit with simple parametric copula\nfamilies. The effectiveness of the proposed visualization and diagnostic\nmethods is illustrated on simulated and real datasets."}, "http://arxiv.org/abs/2310.08115": {"title": "Model-Agnostic Covariate-Assisted Inference on Partially Identified Causal Effects", "link": "http://arxiv.org/abs/2310.08115", "description": "Many causal estimands are only partially identifiable since they depend on\nthe unobservable joint distribution between potential outcomes. Stratification\non pretreatment covariates can yield sharper partial identification bounds;\nhowever, unless the covariates are discrete with relatively small support, this\napproach typically requires consistent estimation of the conditional\ndistributions of the potential outcomes given the covariates. Thus, existing\napproaches may fail under model misspecification or if consistency assumptions\nare violated. In this study, we propose a unified and model-agnostic\ninferential approach for a wide class of partially identified estimands, based\non duality theory for optimal transport problems. In randomized experiments,\nour approach can wrap around any estimates of the conditional distributions and\nprovide uniformly valid inference, even if the initial estimates are\narbitrarily inaccurate. Also, our approach is doubly robust in observational\nstudies. Notably, this property allows analysts to use the multiplier bootstrap\nto select covariates and models without sacrificing validity even if the true\nmodel is not included. Furthermore, if the conditional distributions are\nestimated at semiparametric rates, our approach matches the performance of an\noracle with perfect knowledge of the outcome model. Finally, we propose an\nefficient computational framework, enabling implementation on many practical\nproblems in causal inference."}, "http://arxiv.org/abs/2310.08193": {"title": "Are sanctions for losers? A network study of trade sanctions", "link": "http://arxiv.org/abs/2310.08193", "description": "Studies built on dependency and world-system theory using network approaches\nhave shown that international trade is structured into clusters of 'core' and\n'peripheral' countries performing distinct functions. However, few have used\nthese methods to investigate how sanctions affect the position of the countries\ninvolved in the capitalist world-economy. Yet, this topic has acquired pressing\nrelevance due to the emergence of economic warfare as a key geopolitical weapon\nsince the 1950s. And even more so in light of the preeminent role that\nsanctions have played in the US and their allies' response to the\nRussian-Ukrainian war. Applying several clustering techniques designed for\ncomplex and temporal networks, this paper shows that a shift in the pattern of\ncommerce away from sanctioning countries and towards neutral or friendly ones.\nAdditionally, there are suggestions that these shifts may lead to the creation\nof an alternative 'core' that interacts with the world-economy's periphery\nbypassing traditional 'core' countries such as EU member States and the US."}, "http://arxiv.org/abs/2310.08268": {"title": "Change point detection in dynamic heterogeneous networks via subspace tracking", "link": "http://arxiv.org/abs/2310.08268", "description": "Dynamic networks consist of a sequence of time-varying networks, and it is of\ngreat importance to detect the network change points. Most existing methods\nfocus on detecting abrupt change points, necessitating the assumption that the\nunderlying network probability matrix remains constant between adjacent change\npoints. This paper introduces a new model that allows the network probability\nmatrix to undergo continuous shifting, while the latent network structure,\nrepresented via the embedding subspace, only changes at certain time points.\nTwo novel statistics are proposed to jointly detect these network subspace\nchange points, followed by a carefully refined detection procedure.\nTheoretically, we show that the proposed method is asymptotically consistent in\nterms of change point detection, and also establish the impossibility region\nfor detecting these network subspace change points. The advantage of the\nproposed method is also supported by extensive numerical experiments on both\nsynthetic networks and a UK politician social network."}, "http://arxiv.org/abs/2310.08397": {"title": "Assessing Marine Mammal Abundance: A Novel Data Fusion", "link": "http://arxiv.org/abs/2310.08397", "description": "Marine mammals are increasingly vulnerable to human disturbance and climate\nchange. Their diving behavior leads to limited visual access during data\ncollection, making studying the abundance and distribution of marine mammals\nchallenging. In theory, using data from more than one observation modality\nshould lead to better informed predictions of abundance and distribution. With\nfocus on North Atlantic right whales, we consider the fusion of two data\nsources to inform about their abundance and distribution. The first source is\naerial distance sampling which provides the spatial locations of whales\ndetected in the region. The second source is passive acoustic monitoring (PAM),\nreturning calls received at hydrophones placed on the ocean floor. Due to\nlimited time on the surface and detection limitations arising from sampling\neffort, aerial distance sampling only provides a partial realization of\nlocations. With PAM, we never observe numbers or locations of individuals. To\naddress these challenges, we develop a novel thinned point pattern data fusion.\nOur approach leads to improved inference regarding abundance and distribution\nof North Atlantic right whales throughout Cape Cod Bay, Massachusetts in the\nUS. We demonstrate performance gains of our approach compared to that from a\nsingle source through both simulation and real data."}, "http://arxiv.org/abs/2310.08410": {"title": "Evaluation of ChatGPT-Generated Medical Responses: A Systematic Review and Meta-Analysis", "link": "http://arxiv.org/abs/2310.08410", "description": "Large language models such as ChatGPT are increasingly explored in medical\ndomains. However, the absence of standard guidelines for performance evaluation\nhas led to methodological inconsistencies. This study aims to summarize the\navailable evidence on evaluating ChatGPT's performance in medicine and provide\ndirection for future research. We searched ten medical literature databases on\nJune 15, 2023, using the keyword \"ChatGPT\". A total of 3520 articles were\nidentified, of which 60 were reviewed and summarized in this paper and 17 were\nincluded in the meta-analysis. The analysis showed that ChatGPT displayed an\noverall integrated accuracy of 56% (95% CI: 51%-60%, I2 = 87%) in addressing\nmedical queries. However, the studies varied in question resource,\nquestion-asking process, and evaluation metrics. Moreover, many studies failed\nto report methodological details, including the version of ChatGPT and whether\neach question was used independently or repeatedly. Our findings revealed that\nalthough ChatGPT demonstrated considerable potential for application in\nhealthcare, the heterogeneity of the studies and insufficient reporting may\naffect the reliability of these results. Further well-designed studies with\ncomprehensive and transparent reporting are needed to evaluate ChatGPT's\nperformance in medicine."}, "http://arxiv.org/abs/2310.08414": {"title": "Confidence bounds for the true discovery proportion based on the exact distribution of the number of rejections", "link": "http://arxiv.org/abs/2310.08414", "description": "In multiple hypotheses testing it has become widely popular to make inference\non the true discovery proportion (TDP) of a set $\\mathcal{M}$ of null\nhypotheses. This approach is useful for several application fields, such as\nneuroimaging and genomics. Several procedures to compute simultaneous lower\nconfidence bounds for the TDP have been suggested in prior literature.\nSimultaneity allows for post-hoc selection of $\\mathcal{M}$. If sets of\ninterest are specified a priori, it is possible to gain power by removing the\nsimultaneity requirement. We present an approach to compute lower confidence\nbounds for the TDP if the set of null hypotheses is defined a priori. The\nproposed method determines the bounds using the exact distribution of the\nnumber of rejections based on a step-up multiple testing procedure under\nindependence assumptions. We assess robustness properties of our procedure and\napply it to real data from the field of functional magnetic resonance imaging."}, "http://arxiv.org/abs/2310.08426": {"title": "Extensions of Heterogeneity in Integration and Prediction (HIP) with R Shiny Application", "link": "http://arxiv.org/abs/2310.08426", "description": "Multiple data views measured on the same set of participants is becoming more\ncommon and has the potential to deepen our understanding of many complex\ndiseases by analyzing these different views simultaneously. Equally important,\nmany of these complex diseases show evidence of subgroup heterogeneity (e.g.,\nby sex or race). HIP (Heterogeneity in Integration and Prediction) is among the\nfirst methods proposed to integrate multiple data views while also accounting\nfor subgroup heterogeneity to identify common and subgroup-specific markers of\na particular disease. However, HIP is applicable to continuous outcomes and\nrequires programming expertise by the user. Here we propose extensions to HIP\nthat accommodate multi-class, Poisson, and Zero-Inflated Poisson outcomes while\nretaining the benefits of HIP. Additionally, we introduce an R Shiny\napplication, accessible on shinyapps.io at\nhttps://multi-viewlearn.shinyapps.io/HIP_ShinyApp/, that provides an interface\nwith the Python implementation of HIP to allow more researchers to use the\nmethod anywhere and on any device. We applied HIP to identify genes and\nproteins common and specific to males and females that are associated with\nexacerbation frequency. Although some of the identified genes and proteins show\nevidence of a relationship with chronic obstructive pulmonary disease (COPD) in\nexisting literature, others may be candidates for future research investigating\ntheir relationship with COPD. We demonstrate the use of the Shiny application\nwith a publicly available data. An R-package for HIP would be made available at\nhttps://github.com/lasandrall/HIP."}, "http://arxiv.org/abs/2310.08479": {"title": "Personalised dynamic super learning: an application in predicting hemodiafiltration's convection volumes", "link": "http://arxiv.org/abs/2310.08479", "description": "Obtaining continuously updated predictions is a major challenge for\npersonalised medicine. Leveraging combinations of parametric regressions and\nmachine learning approaches, the personalised online super learner (POSL) can\nachieve such dynamic and personalised predictions. We adapt POSL to predict a\nrepeated continuous outcome dynamically and propose a new way to validate such\npersonalised or dynamic prediction models. We illustrate its performance by\npredicting the convection volume of patients undergoing hemodiafiltration. POSL\noutperformed its candidate learners with respect to median absolute error,\ncalibration-in-the-large, discrimination, and net benefit. We finally discuss\nthe choices and challenges underlying the use of POSL."}, "http://arxiv.org/abs/1903.00037": {"title": "Distance-Based Independence Screening for Canonical Analysis", "link": "http://arxiv.org/abs/1903.00037", "description": "This paper introduces a novel method called Distance-Based Independence\nScreening for Canonical Analysis (DISCA) that performs simultaneous dimension\nreduction for a pair of random variables by optimizing the distance covariance\n(dCov). dCov is a statistic first proposed by Sz\\'ekely et al. [2009] for\nindependence testing. Compared with sufficient dimension reduction (SDR) and\ncanonical correlation analysis (CCA)-based approaches, DISCA is a model-free\napproach that does not impose dimensional or distributional restrictions on\nvariables and is more sensitive to nonlinear relationships. Theoretically, we\nestablish a non-asymptotic error bound to provide a guarantee of our method's\nperformance. Numerically, DISCA performs comparable to or better than other\nstate-of-the-art algorithms and is computationally faster. All codes of our\nDISCA method can be found on GitHub https : //github.com/Yijin911/DISCA.git,\nincluding an R package named DISCA."}, "http://arxiv.org/abs/2105.13952": {"title": "Generalized Permutation Framework for Testing Model Variable Significance", "link": "http://arxiv.org/abs/2105.13952", "description": "A common problem in machine learning is determining if a variable\nsignificantly contributes to a model's prediction performance. This problem is\naggravated for datasets, such as gene expression datasets, that suffer the\nworst case of dimensionality: a low number of observations along with a high\nnumber of possible explanatory variables. In such scenarios, traditional\nmethods for testing variable statistical significance or constructing variable\nconfidence intervals do not apply. To address these problems, we developed a\nnovel permutation framework for testing the significance of variables in\nsupervised models. Our permutation framework has three main advantages. First,\nit is non-parametric and does not rely on distributional assumptions or\nasymptotic results. Second, it not only ranks model variables in terms of\nrelative importance, but also tests for statistical significance of each\nvariable. Third, it can test for the significance of the interaction between\nmodel variables. We applied this permutation framework to multi-class\nclassification of the Iris flower dataset and of brain regions in RNA\nexpression data, and using this framework showed variable-level statistical\nsignificance and interactions."}, "http://arxiv.org/abs/2210.02002": {"title": "Factor Augmented Sparse Throughput Deep ReLU Neural Networks for High Dimensional Regression", "link": "http://arxiv.org/abs/2210.02002", "description": "This paper introduces a Factor Augmented Sparse Throughput (FAST) model that\nutilizes both latent factors and sparse idiosyncratic components for\nnonparametric regression. The FAST model bridges factor models on one end and\nsparse nonparametric models on the other end. It encompasses structured\nnonparametric models such as factor augmented additive models and sparse\nlow-dimensional nonparametric interaction models and covers the cases where the\ncovariates do not admit factor structures. Via diversified projections as\nestimation of latent factor space, we employ truncated deep ReLU networks to\nnonparametric factor regression without regularization and to a more general\nFAST model using nonconvex regularization, resulting in factor augmented\nregression using neural network (FAR-NN) and FAST-NN estimators respectively.\nWe show that FAR-NN and FAST-NN estimators adapt to the unknown low-dimensional\nstructure using hierarchical composition models in nonasymptotic minimax rates.\nWe also study statistical learning for the factor augmented sparse additive\nmodel using a more specific neural network architecture. Our results are\napplicable to the weak dependent cases without factor structures. In proving\nthe main technical result for FAST-NN, we establish a new deep ReLU network\napproximation result that contributes to the foundation of neural network\ntheory. Our theory and methods are further supported by simulation studies and\nan application to macroeconomic data."}, "http://arxiv.org/abs/2210.04482": {"title": "Leave-group-out cross-validation for latent Gaussian models", "link": "http://arxiv.org/abs/2210.04482", "description": "Evaluating the predictive performance of a statistical model is commonly done\nusing cross-validation. Although the leave-one-out method is frequently\nemployed, its application is justified primarily for independent and\nidentically distributed observations. However, this method tends to mimic\ninterpolation rather than prediction when dealing with dependent observations.\nThis paper proposes a modified cross-validation for dependent observations.\nThis is achieved by excluding an automatically determined set of observations\nfrom the training set to mimic a more reasonable prediction scenario. Also,\nwithin the framework of latent Gaussian models, we illustrate a method to\nadjust the joint posterior for this modified cross-validation to avoid model\nrefitting. This new approach is accessible in the R-INLA package\n(www.r-inla.org)."}, "http://arxiv.org/abs/2210.11355": {"title": "Network Synthetic Interventions: A Causal Framework for Panel Data Under Network Interference", "link": "http://arxiv.org/abs/2210.11355", "description": "We propose a generalization of the synthetic controls and synthetic\ninterventions methodology to incorporate network interference. We consider the\nestimation of unit-specific potential outcomes from panel data in the presence\nof spillover across units and unobserved confounding. Key to our approach is a\nnovel latent factor model that takes into account network interference and\ngeneralizes the factor models typically used in panel data settings. We propose\nan estimator, Network Synthetic Interventions (NSI), and show that it\nconsistently estimates the mean outcomes for a unit under an arbitrary set of\ncounterfactual treatments for the network. We further establish that the\nestimator is asymptotically normal. We furnish two validity tests for whether\nthe NSI estimator reliably generalizes to produce accurate counterfactual\nestimates. We provide a novel graph-based experiment design that guarantees the\nNSI estimator produces accurate counterfactual estimates, and also analyze the\nsample complexity of the proposed design. We conclude with simulations that\ncorroborate our theoretical findings."}, "http://arxiv.org/abs/2212.01179": {"title": "Feasibility of using survey data and semi-variogram kriging to obtain bespoke indices of neighbourhood characteristics: a simulation and a case study", "link": "http://arxiv.org/abs/2212.01179", "description": "Data on neighbourhood characteristics are not typically collected in\nepidemiological studies. They are however useful in the study of small-area\nhealth inequalities. Neighbourhood characteristics are collected in some\nsurveys and could be linked to the data of other studies. We propose to use\nkriging based on semi-variogram models to predict values at non-observed\nlocations with the aim of constructing bespoke indices of neighbourhood\ncharacteristics to be linked to data from epidemiological studies. We perform a\nsimulation study to assess the feasibility of the method as well as a case\nstudy using data from the RECORD study. Apart from having enough observed data\nat small distances to the non-observed locations, a good fitting\nsemi-variogram, a larger range and the absence of nugget effects for the\nsemi-variogram models are factors leading to a higher reliability."}, "http://arxiv.org/abs/2303.17823": {"title": "An interpretable neural network-based non-proportional odds model for ordinal regression", "link": "http://arxiv.org/abs/2303.17823", "description": "This study proposes an interpretable neural network-based non-proportional\nodds model (N$^3$POM) for ordinal regression. N$^3$POM is different from\nconventional approaches to ordinal regression with non-proportional models in\nseveral ways: (1) N$^3$POM is designed to directly handle continuous responses,\nwhereas standard methods typically treat de facto ordered continuous variables\nas discrete, (2) instead of estimating response-dependent finite coefficients\nof linear models from discrete responses as is done in conventional approaches,\nwe train a non-linear neural network to serve as a coefficient function. Thanks\nto the neural network, N$^3$POM offers flexibility while preserving the\ninterpretability of conventional ordinal regression. We establish a sufficient\ncondition under which the predicted conditional cumulative probability locally\nsatisfies the monotonicity constraint over a user-specified region in the\ncovariate space. Additionally, we provide a monotonicity-preserving stochastic\n(MPS) algorithm for effectively training the neural network. We apply N$^3$POM\nto several real-world datasets."}, "http://arxiv.org/abs/2306.16335": {"title": "Emulating the dynamics of complex systems using autoregressive models on manifolds (mNARX)", "link": "http://arxiv.org/abs/2306.16335", "description": "We propose a novel surrogate modelling approach to efficiently and accurately\napproximate the response of complex dynamical systems driven by time-varying\nexogenous excitations over extended time periods. Our approach, namely manifold\nnonlinear autoregressive modelling with exogenous input (mNARX), involves\nconstructing a problem-specific exogenous input manifold that is optimal for\nconstructing autoregressive surrogates. The manifold, which forms the core of\nmNARX, is constructed incrementally by incorporating the physics of the system,\nas well as prior expert- and domain- knowledge. Because mNARX decomposes the\nfull problem into a series of smaller sub-problems, each with a lower\ncomplexity than the original, it scales well with the complexity of the\nproblem, both in terms of training and evaluation costs of the final surrogate.\nFurthermore, mNARX synergizes well with traditional dimensionality reduction\ntechniques, making it highly suitable for modelling dynamical systems with\nhigh-dimensional exogenous inputs, a class of problems that is typically\nchallenging to solve. Since domain knowledge is particularly abundant in\nphysical systems, such as those found in civil and mechanical engineering,\nmNARX is well suited for these applications. We demonstrate that mNARX\noutperforms traditional autoregressive surrogates in predicting the response of\na classical coupled spring-mass system excited by a one-dimensional random\nexcitation. Additionally, we show that mNARX is well suited for emulating very\nhigh-dimensional time- and state-dependent systems, even when affected by\nactive controllers, by surrogating the dynamics of a realistic\naero-servo-elastic onshore wind turbine simulator. In general, our results\ndemonstrate that mNARX offers promising prospects for modelling complex\ndynamical systems, in terms of accuracy and efficiency."}, "http://arxiv.org/abs/2307.02236": {"title": "D-optimal Subsampling Design for Massive Data Linear Regression", "link": "http://arxiv.org/abs/2307.02236", "description": "Data reduction is a fundamental challenge of modern technology, where\nclassical statistical methods are not applicable because of computational\nlimitations. We consider linear regression for an extraordinarily large number\nof observations, but only a few covariates. Subsampling aims at the selection\nof a given percentage of the existing original data. Under distributional\nassumptions on the covariates, we derive D-optimal subsampling designs and\nstudy their theoretical properties. We make use of fundamental concepts of\noptimal design theory and an equivalence theorem from constrained convex\noptimization. The thus obtained subsampling designs provide simple rules for\nwhether to accept or reject a data point, allowing for an easy algorithmic\nimplementation. In addition, we propose a simplified subsampling method that\ndiffers from the D-optimal design but requires lower computing time. We present\na simulation study, comparing both subsampling schemes with the IBOSS method."}, "http://arxiv.org/abs/2310.08672": {"title": "Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal", "link": "http://arxiv.org/abs/2310.08672", "description": "In many settings, interventions may be more effective for some individuals\nthan others, so that targeting interventions may be beneficial. We analyze the\nvalue of targeting in the context of a large-scale field experiment with over\n53,000 college students, where the goal was to use \"nudges\" to encourage\nstudents to renew their financial-aid applications before a non-binding\ndeadline. We begin with baseline approaches to targeting. First, we target\nbased on a causal forest that estimates heterogeneous treatment effects and\nthen assigns students to treatment according to those estimated to have the\nhighest treatment effects. Next, we evaluate two alternative targeting\npolicies, one targeting students with low predicted probability of renewing\nfinancial aid in the absence of the treatment, the other targeting those with\nhigh probability. The predicted baseline outcome is not the ideal criterion for\ntargeting, nor is it a priori clear whether to prioritize low, high, or\nintermediate predicted probability. Nonetheless, targeting on low baseline\noutcomes is common in practice, for example because the relationship between\nindividual characteristics and treatment effects is often difficult or\nimpossible to estimate with historical data. We propose hybrid approaches that\nincorporate the strengths of both predictive approaches (accurate estimation)\nand causal approaches (correct criterion); we show that targeting intermediate\nbaseline outcomes is most effective, while targeting based on low baseline\noutcomes is detrimental. In one year of the experiment, nudging all students\nimproved early filing by an average of 6.4 percentage points over a baseline\naverage of 37% filing, and we estimate that targeting half of the students\nusing our preferred policy attains around 75% of this benefit."}, "http://arxiv.org/abs/2310.08726": {"title": "Design-Based RCT Estimators and Central Limit Theorems for Baseline Subgroup and Related Analyses", "link": "http://arxiv.org/abs/2310.08726", "description": "There is a growing literature on design-based methods to estimate average\ntreatment effects (ATEs) for randomized controlled trials (RCTs) for full\nsample analyses. This article extends these methods to estimate ATEs for\ndiscrete subgroups defined by pre-treatment variables, with an application to\nan RCT testing subgroup effects for a school voucher experiment in New York\nCity. We consider ratio estimators for subgroup effects using regression\nmethods, allowing for model covariates to improve precision, and prove a finite\npopulation central limit theorem. We discuss extensions to blocked and\nclustered RCT designs, and to other common estimators with random\ntreatment-control sample sizes (or weights): post-stratification estimators,\nweighted estimators that adjust for data nonresponse, and estimators for\nBernoulli trials. We also develop simple variance estimators that share\nfeatures with robust estimators. Simulations show that the design-based\nsubgroup estimators yield confidence interval coverage near nominal levels,\neven for small subgroups."}, "http://arxiv.org/abs/2310.08798": {"title": "Alteration Detection of Tensor Dependence Structure via Sparsity-Exploited Reranking Algorithm", "link": "http://arxiv.org/abs/2310.08798", "description": "Tensor-valued data arise frequently from a wide variety of scientific\napplications, and many among them can be translated into an alteration\ndetection problem of tensor dependence structures. In this article, we\nformulate the problem under the popularly adopted tensor-normal distributions\nand aim at two-sample correlation/partial correlation comparisons of\ntensor-valued observations. Through decorrelation and centralization, a\nseparable covariance structure is employed to pool sample information from\ndifferent tensor modes to enhance the power of the test. Additionally, we\npropose a novel Sparsity-Exploited Reranking Algorithm (SERA) to further\nimprove the multiple testing efficiency. The algorithm is approached through\nreranking of the p-values derived from the primary test statistics, by\nincorporating a carefully constructed auxiliary tensor sequence. Besides the\ntensor framework, SERA is also generally applicable to a wide range of\ntwo-sample large-scale inference problems with sparsity structures, and is of\nindependent interest. The asymptotic properties of the proposed test are\nderived and the algorithm is shown to control the false discovery at the\npre-specified level. We demonstrate the efficacy of the proposed method through\nintensive simulations and two scientific applications."}, "http://arxiv.org/abs/2310.08812": {"title": "A Nonlinear Method for time series forecasting using VMD-GARCH-LSTM model", "link": "http://arxiv.org/abs/2310.08812", "description": "Time series forecasting represents a significant and challenging task across\nvarious fields. Recently, methods based on mode decomposition have dominated\nthe forecasting of complex time series because of the advantages of capturing\nlocal characteristics and extracting intrinsic modes from data. Unfortunately,\nmost models fail to capture the implied volatilities that contain significant\ninformation. To enhance the forecasting of current, rapidly evolving, and\nvolatile time series, we propose a novel decomposition-ensemble paradigm, the\nVMD-LSTM-GARCH model. The Variational Mode Decomposition algorithm is employed\nto decompose the time series into K sub-modes. Subsequently, the GARCH model\nextracts the volatility information from these sub-modes, which serve as the\ninput for the LSTM. The numerical and volatility information of each sub-mode\nis utilized to train a Long Short-Term Memory network. This network predicts\nthe sub-mode, and then we aggregate the predictions from all sub-modes to\nproduce the output. By integrating econometric and artificial intelligence\nmethods, and taking into account both the numerical and volatility information\nof the time series, our proposed model demonstrates superior performance in\ntime series forecasting, as evidenced by the significant decrease in MSE, RMSE,\nand MAPE in our comparative experimental results."}, "http://arxiv.org/abs/2310.08867": {"title": "A Survey of Methods for Handling Disk Data Imbalance", "link": "http://arxiv.org/abs/2310.08867", "description": "Class imbalance exists in many classification problems, and since the data is\ndesigned for accuracy, imbalance in data classes can lead to classification\nchallenges with a few classes having higher misclassification costs. The\nBackblaze dataset, a widely used dataset related to hard discs, has a small\namount of failure data and a large amount of health data, which exhibits a\nserious class imbalance. This paper provides a comprehensive overview of\nresearch in the field of imbalanced data classification. The discussion is\norganized into three main aspects: data-level methods, algorithmic-level\nmethods, and hybrid methods. For each type of method, we summarize and analyze\nthe existing problems, algorithmic ideas, strengths, and weaknesses.\nAdditionally, the challenges of unbalanced data classification are discussed,\nalong with strategies to address them. It is convenient for researchers to\nchoose the appropriate method according to their needs."}, "http://arxiv.org/abs/2310.08939": {"title": "Fast Screening Rules for Optimal Design via Quadratic Lasso Reformulation", "link": "http://arxiv.org/abs/2310.08939", "description": "The problems of Lasso regression and optimal design of experiments share a\ncritical property: their optimal solutions are typically \\emph{sparse}, i.e.,\nonly a small fraction of the optimal variables are non-zero. Therefore, the\nidentification of the support of an optimal solution reduces the dimensionality\nof the problem and can yield a substantial simplification of the calculations.\nIt has recently been shown that linear regression with a \\emph{squared}\n$\\ell_1$-norm sparsity-inducing penalty is equivalent to an optimal\nexperimental design problem. In this work, we use this equivalence to derive\nsafe screening rules that can be used to discard inessential samples. Compared\nto previously existing rules, the new tests are much faster to compute,\nespecially for problems involving a parameter space of high dimension, and can\nbe used dynamically within any iterative solver, with negligible computational\noverhead. Moreover, we show how an existing homotopy algorithm to compute the\nregularization path of the lasso method can be reparametrized with respect to\nthe squared $\\ell_1$-penalty. This allows the computation of a Bayes\n$c$-optimal design in a finite number of steps and can be several orders of\nmagnitude faster than standard first-order algorithms. The efficiency of the\nnew screening rules and of the homotopy algorithm are demonstrated on different\nexamples based on real data."}, "http://arxiv.org/abs/2310.09013": {"title": "Smoothed instrumental variables quantile regression", "link": "http://arxiv.org/abs/2310.09013", "description": "In this article, I introduce the sivqr command, which estimates the\ncoefficients of the instrumental variables (IV) quantile regression model\nintroduced by Chernozhukov and Hansen (2005). The sivqr command offers several\nadvantages over the existing ivqreg and ivqreg2 commands for estimating this IV\nquantile regression model, which complements the alternative \"triangular model\"\nbehind cqiv and the \"local quantile treatment effect\" model of ivqte.\nComputationally, sivqr implements the smoothed estimator of Kaplan and Sun\n(2017), who show that smoothing improves both computation time and statistical\naccuracy. Standard errors are computed analytically or by Bayesian bootstrap;\nfor non-iid sampling, sivqr is compatible with bootstrap. I discuss syntax and\nthe underlying methodology, and I compare sivqr with other commands in an\nexample."}, "http://arxiv.org/abs/2310.09100": {"title": "Time-Uniform Self-Normalized Concentration for Vector-Valued Processes", "link": "http://arxiv.org/abs/2310.09100", "description": "Self-normalized processes arise naturally in many statistical tasks. While\nself-normalized concentration has been extensively studied for scalar-valued\nprocesses, there is less work on multidimensional processes outside of the\nsub-Gaussian setting. In this work, we construct a general, self-normalized\ninequality for $\\mathbb{R}^d$-valued processes that satisfy a simple yet broad\n\"sub-$\\psi$\" tail condition, which generalizes assumptions based on cumulant\ngenerating functions. From this general inequality, we derive an upper law of\nthe iterated logarithm for sub-$\\psi$ vector-valued processes, which is tight\nup to small constants. We demonstrate applications in prototypical statistical\ntasks, such as parameter estimation in online linear regression and\nauto-regressive modeling, and bounded mean estimation via a new (multivariate)\nempirical Bernstein concentration inequality."}, "http://arxiv.org/abs/2310.09185": {"title": "Mediation Analysis using Semi-parametric Shape-Restricted Regression with Applications", "link": "http://arxiv.org/abs/2310.09185", "description": "Often linear regression is used to perform mediation analysis. However, in\nmany instances, the underlying relationships may not be linear, as in the case\nof placental-fetal hormones and fetal development. Although, the exact\nfunctional form of the relationship may be unknown, one may hypothesize the\ngeneral shape of the relationship. For these reasons, we develop a novel\nshape-restricted inference-based methodology for conducting mediation analysis.\nThis work is motivated by an application in fetal endocrinology where\nresearchers are interested in understanding the effects of pesticide\napplication on birth weight, with human chorionic gonadotropin (hCG) as the\nmediator. We assume a practically plausible set of nonlinear effects of hCG on\nthe birth weight and a linear relationship between pesticide exposure and hCG,\nwith both exposure-outcome and exposure-mediator models being linear in the\nconfounding factors. Using the proposed methodology on a population-level\nprenatal screening program data, with hCG as the mediator, we discovered that,\nwhile the natural direct effects suggest a positive association between\npesticide application and birth weight, the natural indirect effects were\nnegative."}, "http://arxiv.org/abs/2310.09214": {"title": "An Introduction to the Calibration of Computer Models", "link": "http://arxiv.org/abs/2310.09214", "description": "In the context of computer models, calibration is the process of estimating\nunknown simulator parameters from observational data. Calibration is variously\nreferred to as model fitting, parameter estimation/inference, an inverse\nproblem, and model tuning. The need for calibration occurs in most areas of\nscience and engineering, and has been used to estimate hard to measure\nparameters in climate, cardiology, drug therapy response, hydrology, and many\nother disciplines. Although the statistical method used for calibration can\nvary substantially, the underlying approach is essentially the same and can be\nconsidered abstractly. In this survey, we review the decisions that need to be\ntaken when calibrating a model, and discuss a range of computational methods\nthat can be used to compute Bayesian posterior distributions."}, "http://arxiv.org/abs/2310.09239": {"title": "Estimating weighted quantile treatment effects with missing outcome data by double sampling", "link": "http://arxiv.org/abs/2310.09239", "description": "Causal weighted quantile treatment effects (WQTE) are a useful compliment to\nstandard causal contrasts that focus on the mean when interest lies at the\ntails of the counterfactual distribution. To-date, however, methods for\nestimation and inference regarding causal WQTEs have assumed complete data on\nall relevant factors. Missing or incomplete data, however, is a widespread\nchallenge in practical settings, particularly when the data are not collected\nfor research purposes such as electronic health records and disease registries.\nFurthermore, in such settings may be particularly susceptible to the outcome\ndata being missing-not-at-random (MNAR). In this paper, we consider the use of\ndouble-sampling, through which the otherwise missing data is ascertained on a\nsub-sample of study units, as a strategy to mitigate bias due to MNAR data in\nthe estimation of causal WQTEs. With the additional data in-hand, we present\nidentifying conditions that do not require assumptions regarding missingness in\nthe original data. We then propose a novel inverse-probability weighted\nestimator and derive its' asymptotic properties, both pointwise at specific\nquantiles and uniform across a range of quantiles in (0,1), when the propensity\nscore and double-sampling probabilities are estimated. For practical inference,\nwe develop a bootstrap method that can be used for both pointwise and uniform\ninference. A simulation study is conducted to examine the finite sample\nperformance of the proposed estimators."}, "http://arxiv.org/abs/2310.09257": {"title": "A SIMPLE Approach to Provably Reconstruct Ising Model with Global Optimality", "link": "http://arxiv.org/abs/2310.09257", "description": "Reconstruction of interaction network between random events is a critical\nproblem arising from statistical physics and politics to sociology, biology,\nand psychology, and beyond. The Ising model lays the foundation for this\nreconstruction process, but finding the underlying Ising model from the least\namount of observed samples in a computationally efficient manner has been\nhistorically challenging for half a century. By using the idea of sparsity\nlearning, we present a approach named SIMPLE that has a dominant sample\ncomplexity from theoretical limit. Furthermore, a tuning-free algorithm is\ndeveloped to give a statistically consistent solution of SIMPLE in polynomial\ntime with high probability. On extensive benchmarked cases, the SIMPLE approach\nprovably reconstructs underlying Ising models with global optimality. The\napplication on the U.S. senators voting in the last six congresses reveals that\nboth the Republicans and Democrats noticeably assemble in each congresses;\ninterestingly, the assembling of Democrats is particularly pronounced in the\nlatest congress."}, "http://arxiv.org/abs/2208.09817": {"title": "High-Dimensional Composite Quantile Regression: Optimal Statistical Guarantees and Fast Algorithms", "link": "http://arxiv.org/abs/2208.09817", "description": "The composite quantile regression (CQR) was introduced by Zou and Yuan [Ann.\nStatist. 36 (2008) 1108--1126] as a robust regression method for linear models\nwith heavy-tailed errors while achieving high efficiency. Its penalized\ncounterpart for high-dimensional sparse models was recently studied in Gu and\nZou [IEEE Trans. Inf. Theory 66 (2020) 7132--7154], along with a specialized\noptimization algorithm based on the alternating direct method of multipliers\n(ADMM). Compared to the various first-order algorithms for penalized least\nsquares, ADMM-based algorithms are not well-adapted to large-scale problems. To\novercome this computational hardness, in this paper we employ a\nconvolution-smoothed technique to CQR, complemented with iteratively reweighted\n$\\ell_1$-regularization. The smoothed composite loss function is convex, twice\ncontinuously differentiable, and locally strong convex with high probability.\nWe propose a gradient-based algorithm for penalized smoothed CQR via a variant\nof the majorize-minimization principal, which gains substantial computational\nefficiency over ADMM. Theoretically, we show that the iteratively reweighted\n$\\ell_1$-penalized smoothed CQR estimator achieves near-minimax optimal\nconvergence rate under heavy-tailed errors without any moment constraint, and\nfurther achieves near-oracle convergence rate under a weaker minimum signal\nstrength condition than needed in Gu and Zou (2020). Numerical studies\ndemonstrate that the proposed method exhibits significant computational\nadvantages without compromising statistical performance compared to two\nstate-of-the-art methods that achieve robustness and high efficiency\nsimultaneously."}, "http://arxiv.org/abs/2210.14292": {"title": "Statistical Inference for H\\\"usler-Reiss Graphical Models Through Matrix Completions", "link": "http://arxiv.org/abs/2210.14292", "description": "The severity of multivariate extreme events is driven by the dependence\nbetween the largest marginal observations. The H\\\"usler-Reiss distribution is a\nversatile model for this extremal dependence, and it is usually parameterized\nby a variogram matrix. In order to represent conditional independence relations\nand obtain sparse parameterizations, we introduce the novel H\\\"usler-Reiss\nprecision matrix. Similarly to the Gaussian case, this matrix appears naturally\nin density representations of the H\\\"usler-Reiss Pareto distribution and\nencodes the extremal graphical structure through its zero pattern. For a given,\narbitrary graph we prove the existence and uniqueness of the completion of a\npartially specified H\\\"usler-Reiss variogram matrix so that its precision\nmatrix has zeros on non-edges in the graph. Using suitable estimators for the\nparameters on the edges, our theory provides the first consistent estimator of\ngraph structured H\\\"usler-Reiss distributions. If the graph is unknown, our\nmethod can be combined with recent structure learning algorithms to jointly\ninfer the graph and the corresponding parameter matrix. Based on our\nmethodology, we propose new tools for statistical inference of sparse\nH\\\"usler-Reiss models and illustrate them on large flight delay data in the\nU.S., as well as Danube river flow data."}, "http://arxiv.org/abs/2302.02288": {"title": "Efficient Adaptive Joint Significance Tests and Sobel-Type Confidence Intervals for Mediation Effects", "link": "http://arxiv.org/abs/2302.02288", "description": "Mediation analysis is an important statistical tool in many research fields.\nIts aim is to investigate the mechanism along the causal pathway between an\nexposure and an outcome. The joint significance test is widely utilized as a\nprominent statistical approach for examining mediation effects in practical\napplications. Nevertheless, the limitation of this mediation testing method\nstems from its conservative Type I error, which reduces its statistical power\nand imposes certain constraints on its popularity and utility. The proposed\nsolution to address this gap is the adaptive joint significance test for one\nmediator, a novel data-adaptive test for mediation effect that exhibits\nsignificant advancements compared to traditional joint significance test. The\nproposed method is designed to be user-friendly, eliminating the need for\ncomplicated procedures. We have derived explicit expressions for size and\npower, ensuring the theoretical validity of our approach. Furthermore, we\nextend the proposed adaptive joint significance tests for small-scale mediation\nhypotheses with family-wise error rate (FWER) control. Additionally, a novel\nadaptive Sobel-type approach is proposed for the estimation of confidence\nintervals for the mediation effects, demonstrating significant advancements\nover conventional Sobel's confidence intervals in terms of achieving desirable\ncoverage probabilities. Our mediation testing and confidence intervals\nprocedure is evaluated through comprehensive simulations, and compared with\nnumerous existing approaches. Finally, we illustrate the usefulness of our\nmethod by analysing three real-world datasets with continuous, binary and\ntime-to-event outcomes, respectively."}, "http://arxiv.org/abs/2302.02747": {"title": "Testing Quantile Forecast Optimality", "link": "http://arxiv.org/abs/2302.02747", "description": "Quantile forecasts made across multiple horizons have become an important\noutput of many financial institutions, central banks and international\norganisations. This paper proposes misspecification tests for such quantile\nforecasts that assess optimality over a set of multiple forecast horizons\nand/or quantiles. The tests build on multiple Mincer-Zarnowitz quantile\nregressions cast in a moment equality framework. Our main test is for the null\nhypothesis of autocalibration, a concept which assesses optimality with respect\nto the information contained in the forecasts themselves. We provide an\nextension that allows to test for optimality with respect to larger information\nsets and a multivariate extension. Importantly, our tests do not just inform\nabout general violations of optimality, but may also provide useful insights\ninto specific forms of sub-optimality. A simulation study investigates the\nfinite sample performance of our tests, and two empirical applications to\nfinancial returns and U.S. macroeconomic series illustrate that our tests can\nyield interesting insights into quantile forecast sub-optimality and its\ncauses."}, "http://arxiv.org/abs/2305.00700": {"title": "Double and Single Descent in Causal Inference with an Application to High-Dimensional Synthetic Control", "link": "http://arxiv.org/abs/2305.00700", "description": "Motivated by a recent literature on the double-descent phenomenon in machine\nlearning, we consider highly over-parameterized models in causal inference,\nincluding synthetic control with many control units. In such models, there may\nbe so many free parameters that the model fits the training data perfectly. We\nfirst investigate high-dimensional linear regression for imputing wage data and\nestimating average treatment effects, where we find that models with many more\ncovariates than sample size can outperform simple ones. We then document the\nperformance of high-dimensional synthetic control estimators with many control\nunits. We find that adding control units can help improve imputation\nperformance even beyond the point where the pre-treatment fit is perfect. We\nprovide a unified theoretical perspective on the performance of these\nhigh-dimensional models. Specifically, we show that more complex models can be\ninterpreted as model-averaging estimators over simpler ones, which we link to\nan improvement in average performance. This perspective yields concrete\ninsights into the use of synthetic control when control units are many relative\nto the number of pre-treatment periods."}, "http://arxiv.org/abs/2305.15742": {"title": "Counterfactual Generative Models for Time-Varying Treatments", "link": "http://arxiv.org/abs/2305.15742", "description": "Estimating the counterfactual outcome of treatment is essential for\ndecision-making in public health and clinical science, among others. Often,\ntreatments are administered in a sequential, time-varying manner, leading to an\nexponentially increased number of possible counterfactual outcomes.\nFurthermore, in modern applications, the outcomes are high-dimensional and\nconventional average treatment effect estimation fails to capture disparities\nin individuals. To tackle these challenges, we propose a novel conditional\ngenerative framework capable of producing counterfactual samples under\ntime-varying treatment, without the need for explicit density estimation. Our\nmethod carefully addresses the distribution mismatch between the observed and\ncounterfactual distributions via a loss function based on inverse probability\nweighting. We present a thorough evaluation of our method using both synthetic\nand real-world data. Our results demonstrate that our method is capable of\ngenerating high-quality counterfactual samples and outperforms the\nstate-of-the-art baselines."}, "http://arxiv.org/abs/2309.09115": {"title": "Fully Synthetic Data for Complex Surveys", "link": "http://arxiv.org/abs/2309.09115", "description": "When seeking to release public use files for confidential data, statistical\nagencies can generate fully synthetic data. We propose an approach for making\nfully synthetic data from surveys collected with complex sampling designs.\nSpecifically, we generate pseudo-populations by applying the weighted finite\npopulation Bayesian bootstrap to account for survey weights, take simple random\nsamples from those pseudo-populations, estimate synthesis models using these\nsimple random samples, and release simulated data drawn from the models as the\npublic use files. We use the framework of multiple imputation to enable\nvariance estimation using two data generation strategies. In the first, we\ngenerate multiple data sets from each simple random sample, whereas in the\nsecond, we generate a single synthetic data set from each simple random sample.\nWe present multiple imputation combining rules for each setting. We illustrate\neach approach and the repeated sampling properties of the combining rules using\nsimulation studies."}, "http://arxiv.org/abs/2309.09323": {"title": "Answering Layer 3 queries with DiscoSCMs", "link": "http://arxiv.org/abs/2309.09323", "description": "Addressing causal queries across the Pearl Causal Hierarchy (PCH) (i.e.,\nassociational, interventional and counterfactual), which is formalized as\n\\Layer{} Valuations, is a central task in contemporary causal inference\nresearch. Counterfactual questions, in particular, pose a significant challenge\nas they often necessitate a complete knowledge of structural equations. This\npaper identifies \\textbf{the degeneracy problem} caused by the consistency\nrule. To tackle this, the \\textit{Distribution-consistency Structural Causal\nModels} (DiscoSCMs) is introduced, which extends both the structural causal\nmodels (SCM) and the potential outcome framework. The correlation pattern of\npotential outcomes in personalized incentive scenarios, described by $P(y_x,\ny'_{x'})$, is used as a case study for elucidation. Although counterfactuals\nare no longer degenerate, they remain indeterminable. As a result, the\ncondition of independent potential noise is incorporated into DiscoSCM. It is\nfound that by adeptly using homogeneity, counterfactuals can be identified.\nFurthermore, more refined results are achieved in the unit problem scenario. In\nsimpler terms, when modeling counterfactuals, one should contemplate: \"Consider\na person with average ability who takes a test and, due to good luck, achieves\nan exceptionally high score. If this person were to retake the test under\nidentical external conditions, what score will he obtain? An exceptionally high\nscore or an average score?\" If your choose is predicting an average score, then\nyou are essentially choosing DiscoSCM over the traditional frameworks based on\nthe consistency rule."}, "http://arxiv.org/abs/2310.01748": {"title": "A generative approach to frame-level multi-competitor races", "link": "http://arxiv.org/abs/2310.01748", "description": "Multi-competitor races often feature complicated within-race strategies that\nare difficult to capture when training data on race outcome level data.\nFurther, models which do not account for such strategic effects may suffer from\nconfounded inferences and predictions. In this work we develop a general\ngenerative model for multi-competitor races which allows analysts to explicitly\nmodel certain strategic effects such as changing lanes or drafting and separate\nthese impacts from competitor ability. The generative model allows one to\nsimulate full races from any real or created starting position which opens new\navenues for attributing value to within-race actions and to perform\ncounter-factual analyses. This methodology is sufficiently general to apply to\nany track based multi-competitor races where both tracking data is available\nand competitor movement is well described by simultaneous forward and lateral\nmovements. We apply this methodology to one-mile horse races using data\nprovided by the New York Racing Association (NYRA) and the New York\nThoroughbred Horsemen's Association (NYTHA) for the Big Data Derby 2022 Kaggle\nCompetition. This data features granular tracking data for all horses at the\nframe-level (occurring at approximately 4hz). We demonstrate how this model can\nyield new inferences, such as the estimation of horse-specific speed profiles\nwhich vary over phases of the race, and examples of posterior predictive\ncounterfactual simulations to answer questions of interest such as starting\nlane impacts on race outcomes."}, "http://arxiv.org/abs/2310.09345": {"title": "A Unified Bayesian Framework for Modeling Measurement Error in Multinomial Data", "link": "http://arxiv.org/abs/2310.09345", "description": "Measurement error in multinomial data is a well-known and well-studied\ninferential problem that is encountered in many fields, including engineering,\nbiomedical and omics research, ecology, finance, and social sciences.\nSurprisingly, methods developed to accommodate measurement error in multinomial\ndata are typically equipped to handle false negatives or false positives, but\nnot both. We provide a unified framework for accommodating both forms of\nmeasurement error using a Bayesian hierarchical approach. We demonstrate the\nproposed method's performance on simulated data and apply it to acoustic bat\nmonitoring data."}, "http://arxiv.org/abs/2310.09384": {"title": "Modeling Missing at Random Neuropsychological Test Scores Using a Mixture of Binomial Product Experts", "link": "http://arxiv.org/abs/2310.09384", "description": "Multivariate bounded discrete data arises in many fields. In the setting of\nlongitudinal dementia studies, such data is collected when individuals complete\nneuropsychological tests. We outline a modeling and inference procedure that\ncan model the joint distribution conditional on baseline covariates, leveraging\nprevious work on mixtures of experts and latent class models. Furthermore, we\nillustrate how the work can be extended when the outcome data is missing at\nrandom using a nested EM algorithm. The proposed model can incorporate\ncovariate information, perform imputation and clustering, and infer latent\ntrajectories. We apply our model on simulated data and an Alzheimer's disease\ndata set."}, "http://arxiv.org/abs/2310.09398": {"title": "An In-Depth Examination of Requirements for Disclosure Risk Assessment", "link": "http://arxiv.org/abs/2310.09398", "description": "The use of formal privacy to protect the confidentiality of responses in the\n2020 Decennial Census of Population and Housing has triggered renewed interest\nand debate over how to measure the disclosure risks and societal benefits of\nthe published data products. Following long-established precedent in economics\nand statistics, we argue that any proposal for quantifying disclosure risk\nshould be based on pre-specified, objective criteria. Such criteria should be\nused to compare methodologies to identify those with the most desirable\nproperties. We illustrate this approach, using simple desiderata, to evaluate\nthe absolute disclosure risk framework, the counterfactual framework underlying\ndifferential privacy, and prior-to-posterior comparisons. We conclude that\nsatisfying all the desiderata is impossible, but counterfactual comparisons\nsatisfy the most while absolute disclosure risk satisfies the fewest.\nFurthermore, we explain that many of the criticisms levied against differential\nprivacy would be levied against any technology that is not equivalent to\ndirect, unrestricted access to confidential data. Thus, more research is\nneeded, but in the near-term, the counterfactual approach appears best-suited\nfor privacy-utility analysis."}, "http://arxiv.org/abs/2310.09428": {"title": "Sparse higher order partial least squares for simultaneous variable selection, dimension reduction, and tensor denoising", "link": "http://arxiv.org/abs/2310.09428", "description": "Partial Least Squares (PLS) regression emerged as an alternative to ordinary\nleast squares for addressing multicollinearity in a wide range of scientific\napplications. As multidimensional tensor data is becoming more widespread,\ntensor adaptations of PLS have been developed. Our investigations reveal that\nthe previously established asymptotic result of the PLS estimator for a tensor\nresponse breaks down as the tensor dimensions and the number of features\nincrease relative to the sample size. To address this, we propose Sparse Higher\nOrder Partial Least Squares (SHOPS) regression and an accompanying algorithm.\nSHOPS simultaneously accommodates variable selection, dimension reduction, and\ntensor association denoising. We establish the asymptotic accuracy of the SHOPS\nalgorithm under a high-dimensional regime and verify these results through\ncomprehensive simulation experiments, and applications to two contemporary\nhigh-dimensional biological data analysis."}, "http://arxiv.org/abs/2310.09493": {"title": "Summary Statistics Knockoffs Inference with Family-wise Error Rate Control", "link": "http://arxiv.org/abs/2310.09493", "description": "Testing multiple hypotheses of conditional independence with provable error\nrate control is a fundamental problem with various applications. To infer\nconditional independence with family-wise error rate (FWER) control when only\nsummary statistics of marginal dependence are accessible, we adopt\nGhostKnockoff to directly generate knockoff copies of summary statistics and\npropose a new filter to select features conditionally dependent to the response\nwith provable FWER control. In addition, we develop a computationally efficient\nalgorithm to greatly reduce the computational cost of knockoff copies\ngeneration without sacrificing power and FWER control. Experiments on simulated\ndata and a real dataset of Alzheimer's disease genetics demonstrate the\nadvantage of proposed method over the existing alternatives in both statistical\npower and computational efficiency."}, "http://arxiv.org/abs/2310.09545": {"title": "A Semiparametric Instrumented Difference-in-Differences Approach to Policy Learning", "link": "http://arxiv.org/abs/2310.09545", "description": "Recently, there has been a surge in methodological development for the\ndifference-in-differences (DiD) approach to evaluate causal effects. Standard\nmethods in the literature rely on the parallel trends assumption to identify\nthe average treatment effect on the treated. However, the parallel trends\nassumption may be violated in the presence of unmeasured confounding, and the\naverage treatment effect on the treated may not be useful in learning a\ntreatment assignment policy for the entire population. In this article, we\npropose a general instrumented DiD approach for learning the optimal treatment\npolicy. Specifically, we establish identification results using a binary\ninstrumental variable (IV) when the parallel trends assumption fails to hold.\nAdditionally, we construct a Wald estimator, novel inverse probability\nweighting (IPW) estimators, and a class of semiparametric efficient and\nmultiply robust estimators, with theoretical guarantees on consistency and\nasymptotic normality, even when relying on flexible machine learning algorithms\nfor nuisance parameters estimation. Furthermore, we extend the instrumented DiD\nto the panel data setting. We evaluate our methods in extensive simulations and\na real data application."}, "http://arxiv.org/abs/2310.09646": {"title": "Jackknife empirical likelihood confidence intervals for the categorical Gini correlation", "link": "http://arxiv.org/abs/2310.09646", "description": "The categorical Gini correlation, $\\rho_g$, was proposed by Dang et al. to\nmeasure the dependence between a categorical variable, $Y$ , and a numerical\nvariable, $X$. It has been shown that $\\rho_g$ has more appealing properties\nthan current existing dependence measurements. In this paper, we develop the\njackknife empirical likelihood (JEL) method for $\\rho_g$. Confidence intervals\nfor the Gini correlation are constructed without estimating the asymptotic\nvariance. Adjusted and weighted JEL are explored to improve the performance of\nthe standard JEL. Simulation studies show that our methods are competitive to\nexisting methods in terms of coverage accuracy and shortness of confidence\nintervals. The proposed methods are illustrated in an application on two real\ndatasets."}, "http://arxiv.org/abs/2310.09673": {"title": "Robust Quickest Change Detection in Non-Stationary Processes", "link": "http://arxiv.org/abs/2310.09673", "description": "Optimal algorithms are developed for robust detection of changes in\nnon-stationary processes. These are processes in which the distribution of the\ndata after change varies with time. The decision-maker does not have access to\nprecise information on the post-change distribution. It is shown that if the\npost-change non-stationary family has a distribution that is least favorable in\na well-defined sense, then the algorithms designed using the least favorable\ndistributions are robust and optimal. Non-stationary processes are encountered\nin public health monitoring and space and military applications. The robust\nalgorithms are applied to real and simulated data to show their effectiveness."}, "http://arxiv.org/abs/2310.09701": {"title": "A powerful empirical Bayes approach for high dimensional replicability analysis", "link": "http://arxiv.org/abs/2310.09701", "description": "Identifying replicable signals across different studies provides stronger\nscientific evidence and more powerful inference. Existing literature on high\ndimensional applicability analysis either imposes strong modeling assumptions\nor has low power. We develop a powerful and robust empirical Bayes approach for\nhigh dimensional replicability analysis. Our method effectively borrows\ninformation from different features and studies while accounting for\nheterogeneity. We show that the proposed method has better power than competing\nmethods while controlling the false discovery rate, both empirically and\ntheoretically. Analyzing datasets from the genome-wide association studies\nreveals new biological insights that otherwise cannot be obtained by using\nexisting methods."}, "http://arxiv.org/abs/2310.09702": {"title": "Inference with Mondrian Random Forests", "link": "http://arxiv.org/abs/2310.09702", "description": "Random forests are popular methods for classification and regression, and\nmany different variants have been proposed in recent years. One interesting\nexample is the Mondrian random forest, in which the underlying trees are\nconstructed according to a Mondrian process. In this paper we give a central\nlimit theorem for the estimates made by a Mondrian random forest in the\nregression setting. When combined with a bias characterization and a consistent\nvariance estimator, this allows one to perform asymptotically valid statistical\ninference, such as constructing confidence intervals, on the unknown regression\nfunction. We also provide a debiasing procedure for Mondrian random forests\nwhich allows them to achieve minimax-optimal estimation rates with\n$\\beta$-H\\\"older regression functions, for all $\\beta$ and in arbitrary\ndimension, assuming appropriate parameter tuning."}, "http://arxiv.org/abs/2310.09818": {"title": "MCMC for Bayesian nonparametric mixture modeling under differential privacy", "link": "http://arxiv.org/abs/2310.09818", "description": "Estimating the probability density of a population while preserving the\nprivacy of individuals in that population is an important and challenging\nproblem that has received considerable attention in recent years. While the\nprevious literature focused on frequentist approaches, in this paper, we\npropose a Bayesian nonparametric mixture model under differential privacy (DP)\nand present two Markov chain Monte Carlo (MCMC) algorithms for posterior\ninference. One is a marginal approach, resembling Neal's algorithm 5 with a\npseudo-marginal Metropolis-Hastings move, and the other is a conditional\napproach. Although our focus is primarily on local DP, we show that our MCMC\nalgorithms can be easily extended to deal with global differential privacy\nmechanisms. Moreover, for certain classes of mechanisms and mixture kernels, we\nshow how standard algorithms can be employed, resulting in substantial\nefficiency gains. Our approach is general and applicable to any mixture model\nand privacy mechanism. In several simulations and a real case study, we discuss\nthe performance of our algorithms and evaluate different privacy mechanisms\nproposed in the frequentist literature."}, "http://arxiv.org/abs/2310.09955": {"title": "On the Statistical Foundations of H-likelihood for Unobserved Random Variables", "link": "http://arxiv.org/abs/2310.09955", "description": "The maximum likelihood estimation is widely used for statistical inferences.\nIn this study, we reformulate the h-likelihood proposed by Lee and Nelder in\n1996, whose maximization yields maximum likelihood estimators for fixed\nparameters and asymptotically best unbiased predictors for random parameters.\nWe establish the statistical foundations for h-likelihood theories, which\nextend classical likelihood theories to embrace broad classes of statistical\nmodels with random parameters. The maximum h-likelihood estimators\nasymptotically achieve the generalized Cramer-Rao lower bound. Furthermore, we\nexplore asymptotic theory when the consistency of either fixed parameter\nestimation or random parameter prediction is violated. The introduction of this\nnew h-likelihood framework enables likelihood theories to cover inferences for\na much broader class of models, while also providing computationally efficient\nfitting algorithms to give asymptotically optimal estimators for fixed\nparameters and predictors for random parameters."}, "http://arxiv.org/abs/2310.09960": {"title": "Point Mass in the Confidence Distribution: Is it a Drawback or an Advantage?", "link": "http://arxiv.org/abs/2310.09960", "description": "Stein's (1959) problem highlights the phenomenon called the probability\ndilution in high dimensional cases, which is known as a fundamental deficiency\nin probabilistic inference. The satellite conjunction problem also suffers from\nprobability dilution that poor-quality data can lead to a dilution of collision\nprobability. Though various methods have been proposed, such as generalized\nfiducial distribution and the reference posterior, they could not maintain the\ncoverage probability of confidence intervals (CIs) in both problems. On the\nother hand, the confidence distribution (CD) has a point mass at zero, which\nhas been interpreted paradoxical. However, we show that this point mass is an\nadvantage rather than a drawback, because it gives a way to maintain the\ncoverage probability of CIs. More recently, `false confidence theorem' was\npresented as another deficiency in probabilistic inferences, called the false\nconfidence. It was further claimed that the use of consonant belief can\nmitigate this deficiency. However, we show that the false confidence theorem\ncannot be applied to the CD in both Stein's and satellite conjunction problems.\nIt is crucial that a confidence feature, not a consonant one, is the key to\novercome the deficiencies in probabilistic inferences. Our findings reveal that\nthe CD outperforms the other existing methods, including the consonant belief,\nin the context of Stein's and satellite conjunction problems. Additionally, we\ndemonstrate the ambiguity of coverage probability in an observed CI from the\nfrequentist CI procedure, and show that the CD provides valuable information\nregarding this ambiguity."}, "http://arxiv.org/abs/2310.09961": {"title": "Theoretical Evaluation of Asymmetric Shapley Values for Root-Cause Analysis", "link": "http://arxiv.org/abs/2310.09961", "description": "In this work, we examine Asymmetric Shapley Values (ASV), a variant of the\npopular SHAP additive local explanation method. ASV proposes a way to improve\nmodel explanations incorporating known causal relations between variables, and\nis also considered as a way to test for unfair discrimination in model\npredictions. Unexplored in previous literature, relaxing symmetry in Shapley\nvalues can have counter-intuitive consequences for model explanation. To better\nunderstand the method, we first show how local contributions correspond to\nglobal contributions of variance reduction. Using variance, we demonstrate\nmultiple cases where ASV yields counter-intuitive attributions, arguably\nproducing incorrect results for root-cause analysis. Second, we identify\ngeneralized additive models (GAM) as a restricted class for which ASV exhibits\ndesirable properties. We support our arguments by proving multiple theoretical\nresults about the method. Finally, we demonstrate the use of asymmetric\nattributions on multiple real-world datasets, comparing the results with and\nwithout restricted model families using gradient boosting and deep learning\nmodels."}, "http://arxiv.org/abs/2310.10003": {"title": "Conformal Contextual Robust Optimization", "link": "http://arxiv.org/abs/2310.10003", "description": "Data-driven approaches to predict-then-optimize decision-making problems seek\nto mitigate the risk of uncertainty region misspecification in safety-critical\nsettings. Current approaches, however, suffer from considering overly\nconservative uncertainty regions, often resulting in suboptimal decisionmaking.\nTo this end, we propose Conformal-Predict-Then-Optimize (CPO), a framework for\nleveraging highly informative, nonconvex conformal prediction regions over\nhigh-dimensional spaces based on conditional generative models, which have the\ndesired distribution-free coverage guarantees. Despite guaranteeing robustness,\nsuch black-box optimization procedures alone inspire little confidence owing to\nthe lack of explanation of why a particular decision was found to be optimal.\nWe, therefore, augment CPO to additionally provide semantically meaningful\nvisual summaries of the uncertainty regions to give qualitative intuition for\nthe optimal decision. We highlight the CPO framework by demonstrating results\non a suite of simulation-based inference benchmark tasks and a vehicle routing\ntask based on probabilistic weather prediction."}, "http://arxiv.org/abs/2310.10048": {"title": "Evaluation of transplant benefits with the U", "link": "http://arxiv.org/abs/2310.10048", "description": "Kidney transplantation is the most effective renal replacement therapy for\nend stage renal disease patients. With the severe shortage of kidney supplies\nand for the clinical effectiveness of transplantation, patient's life\nexpectancy post transplantation is used to prioritize patients for\ntransplantation; however, severe comorbidity conditions and old age are the\nmost dominant factors that negatively impact post-transplantation life\nexpectancy, effectively precluding sick or old patients from receiving\ntransplants. It would be crucial to design objective measures to quantify the\ntransplantation benefit by comparing the mean residual life with and without a\ntransplant, after adjusting for comorbidity and demographic conditions. To\naddress this urgent need, we propose a new class of semiparametric\ncovariate-dependent mean residual life models. Our method estimates covariate\neffects semiparametrically efficiently and the mean residual life function\nnonparametrically, enabling us to predict the residual life increment potential\nfor any given patient. Our method potentially leads to a more fair system that\nprioritizes patients who would have the largest residual life gains. Our\nanalysis of the kidney transplant data from the U.S. Scientific Registry of\nTransplant Recipients also suggests that a single index of covariates summarize\nwell the impacts of multiple covariates, which may facilitate interpretations\nof each covariate's effect. Our subgroup analysis further disclosed\ninequalities in survival gains across groups defined by race, gender and\ninsurance type (reflecting socioeconomic status)."}, "http://arxiv.org/abs/2310.10052": {"title": "Group-Orthogonal Subsampling for Hierarchical Data Based on Linear Mixed Models", "link": "http://arxiv.org/abs/2310.10052", "description": "Hierarchical data analysis is crucial in various fields for making\ndiscoveries. The linear mixed model is often used for training hierarchical\ndata, but its parameter estimation is computationally expensive, especially\nwith big data. Subsampling techniques have been developed to address this\nchallenge. However, most existing subsampling methods assume homogeneous data\nand do not consider the possible heterogeneity in hierarchical data. To address\nthis limitation, we develop a new approach called group-orthogonal subsampling\n(GOSS) for selecting informative subsets of hierarchical data that may exhibit\nheterogeneity. GOSS selects subdata with balanced data size among groups and\ncombinatorial orthogonality within each group, resulting in subdata that are\n$D$- and $A$-optimal for building linear mixed models. Estimators of parameters\ntrained on GOSS subdata are consistent and asymptotically normal. GOSS is shown\nto be numerically appealing via simulations and a real data application.\nTheoretical proofs, R codes, and supplementary numerical results are accessible\nonline as Supplementary Materials."}, "http://arxiv.org/abs/2310.10239": {"title": "Structural transfer learning of non-Gaussian DAG", "link": "http://arxiv.org/abs/2310.10239", "description": "Directed acyclic graph (DAG) has been widely employed to represent\ndirectional relationships among a set of collected nodes. Yet, the available\ndata in one single study is often limited for accurate DAG reconstruction,\nwhereas heterogeneous data may be collected from multiple relevant studies. It\nremains an open question how to pool the heterogeneous data together for better\nDAG structure reconstruction in the target study. In this paper, we first\nintroduce a novel set of structural similarity measures for DAG and then\npresent a transfer DAG learning framework by effectively leveraging information\nfrom auxiliary DAGs of different levels of similarities. Our theoretical\nanalysis shows substantial improvement in terms of DAG reconstruction in the\ntarget study, even when no auxiliary DAG is overall similar to the target DAG,\nwhich is in sharp contrast to most existing transfer learning methods. The\nadvantage of the proposed transfer DAG learning is also supported by extensive\nnumerical experiments on both synthetic data and multi-site brain functional\nconnectivity network data."}, "http://arxiv.org/abs/2310.10271": {"title": "A geometric power analysis for general log-linear models", "link": "http://arxiv.org/abs/2310.10271", "description": "General log-linear models are widely used to express the association in\nmultivariate frequency data on contingency tables. The paper focuses on the\npower analysis for testing the goodness-of-fit hypothesis for these models.\nConventionally, for the power-related sample size calculations a deviation from\nthe null hypothesis, aka effect size, is specified by means of the chi-square\ngoodness-of-fit index. It is argued that the odds ratio is a more natural\nmeasure of effect size, with the advantage of having a data-relevant\ninterpretation. Therefore, a class of log-affine models that are specified by\nodds ratios whose values deviate from those of the null by a small amount can\nbe chosen as an alternative. Being expressed as sets of constraints on odds\nratios, both hypotheses are represented by smooth surfaces in the probability\nsimplex, and thus, the power analysis can be given a geometric interpretation\nas well. A concept of geometric power is introduced and a Monte-Carlo algorithm\nfor its estimation is proposed. The framework is applied to the power analysis\nof goodness-of-fit in the context of multinomial sampling. An iterative scaling\nprocedure for generating distributions from a log-affine model is described and\nits convergence is proved. To illustrate, the geometric power analysis is\ncarried out for data from a clinical study."}, "http://arxiv.org/abs/2310.10324": {"title": "Assessing univariate and bivariate risks of late-frost and drought using vine copulas: A historical study for Bavaria", "link": "http://arxiv.org/abs/2310.10324", "description": "In light of climate change's impacts on forests, including extreme drought\nand late-frost, leading to vitality decline and regional forest die-back, we\nassess univariate drought and late-frost risks and perform a joint risk\nanalysis in Bavaria, Germany, from 1952 to 2020. Utilizing a vast dataset with\n26 bioclimatic and topographic variables, we employ vine copula models due to\nthe data's non-Gaussian and asymmetric dependencies. We use D-vine regression\nfor univariate and Y-vine regression for bivariate analysis, and propose\ncorresponding univariate and bivariate conditional probability risk measures.\nWe identify \"at-risk\" regions, emphasizing the need for forest adaptation due\nto climate change."}, "http://arxiv.org/abs/2310.10329": {"title": "Towards Data-Conditional Simulation for ABC Inference in Stochastic Differential Equations", "link": "http://arxiv.org/abs/2310.10329", "description": "We develop a Bayesian inference method for discretely-observed stochastic\ndifferential equations (SDEs). Inference is challenging for most SDEs, due to\nthe analytical intractability of the likelihood function. Nevertheless, forward\nsimulation via numerical methods is straightforward, motivating the use of\napproximate Bayesian computation (ABC). We propose a conditional simulation\nscheme for SDEs that is based on lookahead strategies for sequential Monte\nCarlo (SMC) and particle smoothing using backward simulation. This leads to the\nsimulation of trajectories that are consistent with the observed trajectory,\nthereby increasing the ABC acceptance rate. We additionally employ an invariant\nneural network, previously developed for Markov processes, to learn the summary\nstatistics function required in ABC. The neural network is incrementally\nretrained by exploiting an ABC-SMC sampler, which provides new training data at\neach round. Since the SDE simulation scheme differs from standard forward\nsimulation, we propose a suitable importance sampling correction, which has the\nadded advantage of guiding the parameters towards regions of high posterior\ndensity, especially in the first ABC-SMC round. Our approach achieves accurate\ninference and is about three times faster than standard (forward-only) ABC-SMC.\nWe illustrate our method in four simulation studies, including three examples\nfrom the Chan-Karaolyi-Longstaff-Sanders SDE family."}, "http://arxiv.org/abs/2310.10331": {"title": "Specifications tests for count time series models with covariates", "link": "http://arxiv.org/abs/2310.10331", "description": "We propose a goodness-of-fit test for a class of count time series models\nwith covariates which includes the Poisson autoregressive model with covariates\n(PARX) as a special case. The test criteria are derived from a specific\ncharacterization for the conditional probability generating function and the\ntest statistic is formulated as a $L_2$ weighting norm of the corresponding\nsample counterpart. The asymptotic properties of the proposed test statistic\nare provided under the null hypothesis as well as under specific alternatives.\nA bootstrap version of the test is explored in a Monte--Carlo study and\nillustrated on a real data set on road safety."}, "http://arxiv.org/abs/2310.10373": {"title": "False Discovery Proportion control for aggregated Knockoffs", "link": "http://arxiv.org/abs/2310.10373", "description": "Controlled variable selection is an important analytical step in various\nscientific fields, such as brain imaging or genomics. In these high-dimensional\ndata settings, considering too many variables leads to poor models and high\ncosts, hence the need for statistical guarantees on false positives. Knockoffs\nare a popular statistical tool for conditional variable selection in high\ndimension. However, they control for the expected proportion of false\ndiscoveries (FDR) and not their actual proportion (FDP). We present a new\nmethod, KOPI, that controls the proportion of false discoveries for\nKnockoff-based inference. The proposed method also relies on a new type of\naggregation to address the undesirable randomness associated with classical\nKnockoff inference. We demonstrate FDP control and substantial power gains over\nexisting Knockoff-based methods in various simulation settings and achieve good\nsensitivity/specificity tradeoffs on brain imaging and genomic data."}, "http://arxiv.org/abs/2310.10393": {"title": "Statistical and Causal Robustness for Causal Null Hypothesis Tests", "link": "http://arxiv.org/abs/2310.10393", "description": "Prior work applying semiparametric theory to causal inference has primarily\nfocused on deriving estimators that exhibit statistical robustness under a\nprespecified causal model that permits identification of a desired causal\nparameter. However, a fundamental challenge is correct specification of such a\nmodel, which usually involves making untestable assumptions. Evidence factors\nis an approach to combining hypothesis tests of a common causal null hypothesis\nunder two or more candidate causal models. Under certain conditions, this\nyields a test that is valid if at least one of the underlying models is\ncorrect, which is a form of causal robustness. We propose a method of combining\nsemiparametric theory with evidence factors. We develop a causal null\nhypothesis test based on joint asymptotic normality of K asymptotically linear\nsemiparametric estimators, where each estimator is based on a distinct\nidentifying functional derived from each of K candidate causal models. We show\nthat this test provides both statistical and causal robustness in the sense\nthat it is valid if at least one of the K proposed causal models is correct,\nwhile also allowing for slower than parametric rates of convergence in\nestimating nuisance functions. We demonstrate the efficacy of our method via\nsimulations and an application to the Framingham Heart Study."}, "http://arxiv.org/abs/2310.10407": {"title": "Ensemble methods for testing a global null", "link": "http://arxiv.org/abs/2310.10407", "description": "Testing a global null is a canonical problem in statistics and has a wide\nrange of applications. In view of the fact that no uniformly most powerful test\nexists, prior and/or domain knowledge are commonly used to focus on a certain\nclass of alternatives to improve the testing power. However, it is generally\nchallenging to develop tests that are particularly powerful against a certain\nclass of alternatives. In this paper, motivated by the success of ensemble\nlearning methods for prediction or classification, we propose an ensemble\nframework for testing that mimics the spirit of random forests to deal with the\nchallenges. Our ensemble testing framework aggregates a collection of weak base\ntests to form a final ensemble test that maintains strong and robust power for\nglobal nulls. We apply the framework to four problems about global testing in\ndifferent classes of alternatives arising from Whole Genome Sequencing (WGS)\nassociation studies. Specific ensemble tests are proposed for each of these\nproblems, and their theoretical optimality is established in terms of Bahadur\nefficiency. Extensive simulations and an analysis of a real WGS dataset are\nconducted to demonstrate the type I error control and/or power gain of the\nproposed ensemble tests."}, "http://arxiv.org/abs/2310.10422": {"title": "A Neural Network-Based Approach to Normality Testing for Dependent Data", "link": "http://arxiv.org/abs/2310.10422", "description": "There is a wide availability of methods for testing normality under the\nassumption of independent and identically distributed data. When data are\ndependent in space and/or time, however, assessing and testing the marginal\nbehavior is considerably more challenging, as the marginal behavior is impacted\nby the degree of dependence. We propose a new approach to assess normality for\ndependent data by non-linearly incorporating existing statistics from normality\ntests as well as sample moments such as skewness and kurtosis through a neural\nnetwork. We calibrate (deep) neural networks by simulated normal and non-normal\ndata with a wide range of dependence structures and we determine the\nprobability of rejecting the null hypothesis. We compare several approaches for\nnormality tests and demonstrate the superiority of our method in terms of\nstatistical power through an extensive simulation study. A real world\napplication to global temperature data further demonstrates how the degree of\nspatio-temporal aggregation affects the marginal normality in the data."}, "http://arxiv.org/abs/2310.10494": {"title": "Multivariate Scalar on Multidimensional Distribution Regression", "link": "http://arxiv.org/abs/2310.10494", "description": "We develop a new method for multivariate scalar on multidimensional\ndistribution regression. Traditional approaches typically analyze isolated\nunivariate scalar outcomes or consider unidimensional distributional\nrepresentations as predictors. However, these approaches are sub-optimal\nbecause: i) they fail to utilize the dependence between the distributional\npredictors: ii) neglect the correlation structure of the response. To overcome\nthese limitations, we propose a multivariate distributional analysis framework\nthat harnesses the power of multivariate density functions and multitask\nlearning. We develop a computationally efficient semiparametric estimation\nmethod for modelling the effect of the latent joint density on multivariate\nresponse of interest. Additionally, we introduce a new conformal algorithm for\nquantifying the uncertainty of regression models with multivariate responses\nand distributional predictors, providing valuable insights into the conditional\ndistribution of the response. We have validated the effectiveness of our\nproposed method through comprehensive numerical simulations, clearly\ndemonstrating its superior performance compared to traditional methods. The\napplication of the proposed method is demonstrated on tri-axial accelerometer\ndata from the National Health and Nutrition Examination Survey (NHANES)\n2011-2014 for modelling the association between cognitive scores across various\ndomains and distributional representation of physical activity among older\nadult population. Our results highlight the advantages of the proposed\napproach, emphasizing the significance of incorporating complete spatial\ninformation derived from the accelerometer device."}, "http://arxiv.org/abs/2310.10588": {"title": "Max-convolution processes with random shape indicator kernels", "link": "http://arxiv.org/abs/2310.10588", "description": "In this paper, we introduce a new class of models for spatial data obtained\nfrom max-convolution processes based on indicator kernels with random shape. We\nshow that this class of models have appealing dependence properties including\ntail dependence at short distances and independence at long distances. We\nfurther consider max-convolutions between such processes and processes with\ntail independence, in order to separately control the bulk and tail dependence\nbehaviors, and to increase flexibility of the model at longer distances, in\nparticular, to capture intermediate tail dependence. We show how parameters can\nbe estimated using a weighted pairwise likelihood approach, and we conduct an\nextensive simulation study to show that the proposed inference approach is\nfeasible in high dimensions and it yields accurate parameter estimates in most\ncases. We apply the proposed methodology to analyse daily temperature maxima\nmeasured at 100 monitoring stations in the state of Oklahoma, US. Our results\nindicate that our proposed model provides a good fit to the data, and that it\ncaptures both the bulk and the tail dependence structures accurately."}, "http://arxiv.org/abs/1805.07301": {"title": "Enhanced Pricing and Management of Bundled Insurance Risks with Dependence-aware Prediction using Pair Copula Construction", "link": "http://arxiv.org/abs/1805.07301", "description": "We propose a dependence-aware predictive modeling framework for multivariate\nrisks stemmed from an insurance contract with bundling features - an important\ntype of policy increasingly offered by major insurance companies. The bundling\nfeature naturally leads to longitudinal measurements of multiple insurance\nrisks, and correct pricing and management of such risks is of fundamental\ninterest to financial stability of the macroeconomy. We build a novel\npredictive model that fully captures the dependence among the multivariate\nrepeated risk measurements. Specifically, the longitudinal measurement of each\nindividual risk is first modeled using pair copula construction with a D-vine\nstructure, and the multiple D-vines are then integrated by a flexible copula.\nThe proposed model provides a unified modeling framework for multivariate\nlongitudinal data that can accommodate different scales of measurements,\nincluding continuous, discrete, and mixed observations, and thus can be\npotentially useful for various economic studies. A computationally efficient\nsequential method is proposed for model estimation and inference, and its\nperformance is investigated both theoretically and via simulation studies. In\nthe application, we examine multivariate bundled risks in multi-peril property\ninsurance using proprietary data from a commercial property insurance provider.\nThe proposed model is found to provide improved decision making for several key\ninsurance operations. For underwriting, we show that the experience rate priced\nby the proposed model leads to a 9% lift in the insurer's net revenue. For\nreinsurance, we show that the insurer underestimates the risk of the retained\ninsurance portfolio by 10% when ignoring the dependence among bundled insurance\nrisks."}, "http://arxiv.org/abs/2005.04721": {"title": "Decision Making in Drug Development via Inference on Power", "link": "http://arxiv.org/abs/2005.04721", "description": "A typical power calculation is performed by replacing unknown\npopulation-level quantities in the power function with what is observed in\nexternal studies. Many authors and practitioners view this as an assumed value\nof power and offer the Bayesian quantity probability of success or assurance as\nan alternative. The claim is by averaging over a prior or posterior\ndistribution, probability of success transcends power by capturing the\nuncertainty around the unknown true treatment effect and any other\npopulation-level parameters. We use p-value functions to frame both the\nprobability of success calculation and the typical power calculation as merely\nproducing two different point estimates of power. We demonstrate that Go/No-Go\ndecisions based on either point estimate of power do not adequately quantify\nand control the risk involved, and instead we argue for Go/No-Go decisions that\nutilize inference on power for better risk management and decision making."}, "http://arxiv.org/abs/2103.00674": {"title": "BEAUTY Powered BEAST", "link": "http://arxiv.org/abs/2103.00674", "description": "We study distribution-free goodness-of-fit tests with the proposed Binary\nExpansion Approximation of UniformiTY (BEAUTY) approach. This method\ngeneralizes the renowned Euler's formula, and approximates the characteristic\nfunction of any copula through a linear combination of expectations of binary\ninteractions from marginal binary expansions. This novel theory enables a\nunification of many important tests of independence via approximations from\nspecific quadratic forms of symmetry statistics, where the deterministic weight\nmatrix characterizes the power properties of each test. To achieve a robust\npower, we examine test statistics with data-adaptive weights, referred to as\nthe Binary Expansion Adaptive Symmetry Test (BEAST). Using properties of the\nbinary expansion filtration, we demonstrate that the Neyman-Pearson test of\nuniformity can be approximated by an oracle weighted sum of symmetry\nstatistics. The BEAST with this oracle provides a useful benchmark of feasible\npower. To approach this oracle power, we devise the BEAST through a regularized\nresampling approximation of the oracle test. The BEAST improves the empirical\npower of many existing tests against a wide spectrum of common alternatives and\ndelivers a clear interpretation of dependency forms when significant."}, "http://arxiv.org/abs/2103.16159": {"title": "Controlling the False Discovery Rate in Transformational Sparsity: Split Knockoffs", "link": "http://arxiv.org/abs/2103.16159", "description": "Controlling the False Discovery Rate (FDR) in a variable selection procedure\nis critical for reproducible discoveries, and it has been extensively studied\nin sparse linear models. However, it remains largely open in scenarios where\nthe sparsity constraint is not directly imposed on the parameters but on a\nlinear transformation of the parameters to be estimated. Examples of such\nscenarios include total variations, wavelet transforms, fused LASSO, and trend\nfiltering. In this paper, we propose a data-adaptive FDR control method, called\nthe Split Knockoff method, for this transformational sparsity setting. The\nproposed method exploits both variable and data splitting. The linear\ntransformation constraint is relaxed to its Euclidean proximity in a lifted\nparameter space, which yields an orthogonal design that enables the orthogonal\nSplit Knockoff construction. To overcome the challenge that exchangeability\nfails due to the heterogeneous noise brought by the transformation, new inverse\nsupermartingale structures are developed via data splitting for provable FDR\ncontrol without sacrificing power. Simulation experiments demonstrate that the\nproposed methodology achieves the desired FDR and power. We also provide an\napplication to Alzheimer's Disease study, where atrophy brain regions and their\nabnormal connections can be discovered based on a structural Magnetic Resonance\nImaging dataset (ADNI)."}, "http://arxiv.org/abs/2201.05967": {"title": "Uniform Inference for Kernel Density Estimators with Dyadic Data", "link": "http://arxiv.org/abs/2201.05967", "description": "Dyadic data is often encountered when quantities of interest are associated\nwith the edges of a network. As such it plays an important role in statistics,\neconometrics and many other data science disciplines. We consider the problem\nof uniformly estimating a dyadic Lebesgue density function, focusing on\nnonparametric kernel-based estimators taking the form of dyadic empirical\nprocesses. Our main contributions include the minimax-optimal uniform\nconvergence rate of the dyadic kernel density estimator, along with strong\napproximation results for the associated standardized and Studentized\n$t$-processes. A consistent variance estimator enables the construction of\nvalid and feasible uniform confidence bands for the unknown density function.\nWe showcase the broad applicability of our results by developing novel\ncounterfactual density estimation and inference methodology for dyadic data,\nwhich can be used for causal inference and program evaluation. A crucial\nfeature of dyadic distributions is that they may be \"degenerate\" at certain\npoints in the support of the data, a property making our analysis somewhat\ndelicate. Nonetheless our methods for uniform inference remain robust to the\npotential presence of such points. For implementation purposes, we discuss\ninference procedures based on positive semi-definite covariance estimators,\nmean squared error optimal bandwidth selectors and robust bias correction\ntechniques. We illustrate the empirical finite-sample performance of our\nmethods both in simulations and with real-world trade data, for which we make\ncomparisons between observed and counterfactual trade distributions in\ndifferent years. Our technical results concerning strong approximations and\nmaximal inequalities are of potential independent interest."}, "http://arxiv.org/abs/2206.01076": {"title": "Likelihood-based Inference for Random Networks with Changepoints", "link": "http://arxiv.org/abs/2206.01076", "description": "Generative, temporal network models play an important role in analyzing the\ndependence structure and evolution patterns of complex networks. Due to the\ncomplicated nature of real network data, it is often naive to assume that the\nunderlying data-generative mechanism itself is invariant with time. Such\nobservation leads to the study of changepoints or sudden shifts in the\ndistributional structure of the evolving network. In this paper, we propose a\nlikelihood-based methodology to detect changepoints in undirected, affine\npreferential attachment networks, and establish a hypothesis testing framework\nto detect a single changepoint, together with a consistent estimator for the\nchangepoint. Such results require establishing consistency and asymptotic\nnormality of the MLE under the changepoint regime, which suffers from long\nrange dependence. The methodology is then extended to the multiple changepoint\nsetting via both a sliding window method and a more computationally efficient\nscore statistic. We also compare the proposed methodology with previously\ndeveloped non-parametric estimators of the changepoint via simulation, and the\nmethods developed herein are applied to modeling the popularity of a topic in a\nTwitter network over time."}, "http://arxiv.org/abs/2301.01616": {"title": "Locally Private Causal Inference for Randomized Experiments", "link": "http://arxiv.org/abs/2301.01616", "description": "Local differential privacy is a differential privacy paradigm in which\nindividuals first apply a privacy mechanism to their data (often by adding\nnoise) before transmitting the result to a curator. The noise for privacy\nresults in additional bias and variance in their analyses. Thus it is of great\nimportance for analysts to incorporate the privacy noise into valid inference.\nIn this article, we develop methodologies to infer causal effects from locally\nprivatized data under randomized experiments. First, we present frequentist\nestimators under various privacy scenarios with their variance estimators and\nplug-in confidence intervals. We show a na\\\"ive debiased estimator results in\ninferior mean-squared error (MSE) compared to minimax lower bounds. In\ncontrast, we show that using a customized privacy mechanism, we can match the\nlower bound, giving minimax optimal inference. We also develop a Bayesian\nnonparametric methodology along with a blocked Gibbs sampling algorithm, which\ncan be applied to any of our proposed privacy mechanisms, and which performs\nespecially well in terms of MSE for tight privacy budgets. Finally, we present\nsimulation studies to evaluate the performance of our proposed frequentist and\nBayesian methodologies for various privacy budgets, resulting in useful\nsuggestions for performing causal inference for privatized data."}, "http://arxiv.org/abs/2303.03215": {"title": "Quantile-Quantile Methodology -- Detailed Results", "link": "http://arxiv.org/abs/2303.03215", "description": "The linear quantile-quantile relationship provides an easy-to-implement yet\neffective tool for transformation to and testing for normality. Its good\nperformance is verified in this report."}, "http://arxiv.org/abs/2305.06645": {"title": "Causal Inference for Continuous Multiple Time Point Interventions", "link": "http://arxiv.org/abs/2305.06645", "description": "There are limited options to estimate the treatment effects of variables\nwhich are continuous and measured at multiple time points, particularly if the\ntrue dose-response curve should be estimated as closely as possible. However,\nthese situations may be of relevance: in pharmacology, one may be interested in\nhow outcomes of people living with -- and treated for -- HIV, such as viral\nfailure, would vary for time-varying interventions such as different drug\nconcentration trajectories. A challenge for doing causal inference with\ncontinuous interventions is that the positivity assumption is typically\nviolated. To address positivity violations, we develop projection functions,\nwhich reweigh and redefine the estimand of interest based on functions of the\nconditional support for the respective interventions. With these functions, we\nobtain the desired dose-response curve in areas of enough support, and\notherwise a meaningful estimand that does not require the positivity\nassumption. We develop $g$-computation type plug-in estimators for this case.\nThose are contrasted with g-computation estimators which are applied to\ncontinuous interventions without specifically addressing positivity violations,\nwhich we propose to be presented with diagnostics. The ideas are illustrated\nwith longitudinal data from HIV positive children treated with an\nefavirenz-based regimen as part of the CHAPAS-3 trial, which enrolled children\n$&lt;13$ years in Zambia/Uganda. Simulations show in which situations a standard\n$g$-computation approach is appropriate, and in which it leads to bias and how\nthe proposed weighted estimation approach then recovers the alternative\nestimand of interest."}, "http://arxiv.org/abs/2305.14275": {"title": "Amortized Variational Inference with Coverage Guarantees", "link": "http://arxiv.org/abs/2305.14275", "description": "Amortized variational inference produces a posterior approximation that can\nbe rapidly computed given any new observation. Unfortunately, there are few\nguarantees about the quality of these approximate posteriors. We propose\nConformalized Amortized Neural Variational Inference (CANVI), a procedure that\nis scalable, easily implemented, and provides guaranteed marginal coverage.\nGiven a collection of candidate amortized posterior approximators, CANVI\nconstructs conformalized predictors based on each candidate, compares the\npredictors using a metric known as predictive efficiency, and returns the most\nefficient predictor. CANVI ensures that the resulting predictor constructs\nregions that contain the truth with a user-specified level of probability.\nCANVI is agnostic to design decisions in formulating the candidate\napproximators and only requires access to samples from the forward model,\npermitting its use in likelihood-free settings. We prove lower bounds on the\npredictive efficiency of the regions produced by CANVI and explore how the\nquality of a posterior approximation relates to the predictive efficiency of\nprediction regions based on that approximation. Finally, we demonstrate the\naccurate calibration and high predictive efficiency of CANVI on a suite of\nsimulation-based inference benchmark tasks and an important scientific task:\nanalyzing galaxy emission spectra."}, "http://arxiv.org/abs/2305.17187": {"title": "Clip-OGD: An Experimental Design for Adaptive Neyman Allocation in Sequential Experiments", "link": "http://arxiv.org/abs/2305.17187", "description": "From clinical development of cancer therapies to investigations into partisan\nbias, adaptive sequential designs have become increasingly popular method for\ncausal inference, as they offer the possibility of improved precision over\ntheir non-adaptive counterparts. However, even in simple settings (e.g. two\ntreatments) the extent to which adaptive designs can improve precision is not\nsufficiently well understood. In this work, we study the problem of Adaptive\nNeyman Allocation in a design-based potential outcomes framework, where the\nexperimenter seeks to construct an adaptive design which is nearly as efficient\nas the optimal (but infeasible) non-adaptive Neyman design, which has access to\nall potential outcomes. Motivated by connections to online optimization, we\npropose Neyman Ratio and Neyman Regret as two (equivalent) performance measures\nof adaptive designs for this problem. We present Clip-OGD, an adaptive design\nwhich achieves $\\widetilde{O}(\\sqrt{T})$ expected Neyman regret and thereby\nrecovers the optimal Neyman variance in large samples. Finally, we construct a\nconservative variance estimator which facilitates the development of\nasymptotically valid confidence intervals. To complement our theoretical\nresults, we conduct simulations using data from a microeconomic experiment."}, "http://arxiv.org/abs/2306.15622": {"title": "Biclustering random matrix partitions with an application to classification of forensic body fluids", "link": "http://arxiv.org/abs/2306.15622", "description": "Classification of unlabeled data is usually achieved by supervised learning\nfrom labeled samples. Although there exist many sophisticated supervised\nmachine learning methods that can predict the missing labels with a high level\nof accuracy, they often lack the required transparency in situations where it\nis important to provide interpretable results and meaningful measures of\nconfidence. Body fluid classification of forensic casework data is the case in\npoint. We develop a new Biclustering Dirichlet Process for Class-assignment\nwith Random Matrices (BDP-CaRMa), with a three-level hierarchy of clustering,\nand a model-based approach to classification that adapts to block structure in\nthe data matrix. As the class labels of some observations are missing, the\nnumber of rows in the data matrix for each class is unknown. BDP-CaRMa handles\nthis and extends existing biclustering methods by simultaneously biclustering\nmultiple matrices each having a randomly variable number of rows. We\ndemonstrate our method by applying it to the motivating problem, which is the\nclassification of body fluids based on mRNA profiles taken from crime scenes.\nThe analyses of casework-like data show that our method is interpretable and\nproduces well-calibrated posterior probabilities. Our model can be more\ngenerally applied to other types of data with a similar structure to the\nforensic data."}, "http://arxiv.org/abs/2307.05644": {"title": "Lambert W random variables and their applications in loss modelling", "link": "http://arxiv.org/abs/2307.05644", "description": "Several distributions and families of distributions are proposed to model\nskewed data, think, e.g., of skew-normal and related distributions. Lambert W\nrandom variables offer an alternative approach where, instead of constructing a\nnew distribution, a certain transform is proposed (Goerg, 2011). Such an\napproach allows the construction of a Lambert W skewed version from any\ndistribution. We choose Lambert W normal distribution as a natural starting\npoint and also include Lambert W exponential distribution due to the simplicity\nand shape of the exponential distribution, which, after skewing, may produce a\nreasonably heavy tail for loss models. In the theoretical part, we focus on the\nmathematical properties of obtained distributions, including the range of\nskewness. In the practical part, the suitability of corresponding Lambert W\ntransformed distributions is evaluated on real insurance data. The results are\ncompared with those obtained using common loss distributions."}, "http://arxiv.org/abs/2307.06840": {"title": "Ensemble learning for blending gridded satellite and gauge-measured precipitation data", "link": "http://arxiv.org/abs/2307.06840", "description": "Regression algorithms are regularly used for improving the accuracy of\nsatellite precipitation products. In this context, satellite precipitation and\ntopography data are the predictor variables, and gauged-measured precipitation\ndata are the dependent variables. Alongside this, it is increasingly recognised\nin many fields that combinations of algorithms through ensemble learning can\nlead to substantial predictive performance improvements. Still, a sufficient\nnumber of ensemble learners for improving the accuracy of satellite\nprecipitation products and their large-scale comparison are currently missing\nfrom the literature. In this study, we work towards filling in this specific\ngap by proposing 11 new ensemble learners in the field and by extensively\ncomparing them. We apply the ensemble learners to monthly data from the\nPERSIANN (Precipitation Estimation from Remotely Sensed Information using\nArtificial Neural Networks) and IMERG (Integrated Multi-satellitE Retrievals\nfor GPM) gridded datasets that span over a 15-year period and over the entire\nthe contiguous United States (CONUS). We also use gauge-measured precipitation\ndata from the Global Historical Climatology Network monthly database, version 2\n(GHCNm). The ensemble learners combine the predictions of six machine learning\nregression algorithms (base learners), namely the multivariate adaptive\nregression splines (MARS), multivariate adaptive polynomial splines\n(poly-MARS), random forests (RF), gradient boosting machines (GBM), extreme\ngradient boosting (XGBoost) and Bayesian regularized neural networks (BRNN),\nand each of them is based on a different combiner. The combiners include the\nequal-weight combiner, the median combiner, two best learners and seven\nvariants of a sophisticated stacking method. The latter stacks a regression\nalgorithm on top of the base learners to combine their independent\npredictions..."}, "http://arxiv.org/abs/2309.12819": {"title": "Doubly Robust Proximal Causal Learning for Continuous Treatments", "link": "http://arxiv.org/abs/2309.12819", "description": "Proximal causal learning is a promising framework for identifying the causal\neffect under the existence of unmeasured confounders. Within this framework,\nthe doubly robust (DR) estimator was derived and has shown its effectiveness in\nestimation, especially when the model assumption is violated. However, the\ncurrent form of the DR estimator is restricted to binary treatments, while the\ntreatment can be continuous in many real-world applications. The primary\nobstacle to continuous treatments resides in the delta function present in the\noriginal DR estimator, making it infeasible in causal effect estimation and\nintroducing a heavy computational burden in nuisance function estimation. To\naddress these challenges, we propose a kernel-based DR estimator that can well\nhandle continuous treatments. Equipped with its smoothness, we show that its\noracle form is a consistent approximation of the influence function. Further,\nwe propose a new approach to efficiently solve the nuisance functions. We then\nprovide a comprehensive convergence analysis in terms of the mean square error.\nWe demonstrate the utility of our estimator on synthetic datasets and\nreal-world applications."}, "http://arxiv.org/abs/2309.17283": {"title": "The Blessings of Multiple Treatments and Outcomes in Treatment Effect Estimation", "link": "http://arxiv.org/abs/2309.17283", "description": "Assessing causal effects in the presence of unobserved confounding is a\nchallenging problem. Existing studies leveraged proxy variables or multiple\ntreatments to adjust for the confounding bias. In particular, the latter\napproach attributes the impact on a single outcome to multiple treatments,\nallowing estimating latent variables for confounding control. Nevertheless,\nthese methods primarily focus on a single outcome, whereas in many real-world\nscenarios, there is greater interest in studying the effects on multiple\noutcomes. Besides, these outcomes are often coupled with multiple treatments.\nExamples include the intensive care unit (ICU), where health providers evaluate\nthe effectiveness of therapies on multiple health indicators. To accommodate\nthese scenarios, we consider a new setting dubbed as multiple treatments and\nmultiple outcomes. We then show that parallel studies of multiple outcomes\ninvolved in this setting can assist each other in causal identification, in the\nsense that we can exploit other treatments and outcomes as proxies for each\ntreatment effect under study. We proceed with a causal discovery method that\ncan effectively identify such proxies for causal estimation. The utility of our\nmethod is demonstrated in synthetic data and sepsis disease."}, "http://arxiv.org/abs/2310.10740": {"title": "Unbiased Estimation of Structured Prediction Error", "link": "http://arxiv.org/abs/2310.10740", "description": "Many modern datasets, such as those in ecology and geology, are composed of\nsamples with spatial structure and dependence. With such data violating the\nusual independent and identically distributed (IID) assumption in machine\nlearning and classical statistics, it is unclear a priori how one should\nmeasure the performance and generalization of models. Several authors have\nempirically investigated cross-validation (CV) methods in this setting,\nreaching mixed conclusions. We provide a class of unbiased estimation methods\nfor general quadratic errors, correlated Gaussian response, and arbitrary\nprediction function $g$, for a noise-elevated version of the error. Our\napproach generalizes the coupled bootstrap (CB) from the normal means problem\nto general normal data, allowing correlation both within and between the\ntraining and test sets. CB relies on creating bootstrap samples that are\nintelligently decoupled, in the sense of being statistically independent.\nSpecifically, the key to CB lies in generating two independent \"views\" of our\ndata and using them as stand-ins for the usual independent training and test\nsamples. Beginning with Mallows' $C_p$, we generalize the estimator to develop\nour generalized $C_p$ estimators (GC). We show at under only a moment condition\non $g$, this noise-elevated error estimate converges smoothly to the noiseless\nerror estimate. We show that when Stein's unbiased risk estimator (SURE)\napplies, GC converges to SURE as in the normal means problem. Further, we use\nthese same tools to analyze CV and provide some theoretical analysis to help\nunderstand when CV will provide good estimates of error. Simulations align with\nour theoretical results, demonstrating the effectiveness of GC and illustrating\nthe behavior of CV methods. Lastly, we apply our estimator to a model selection\ntask on geothermal data in Nevada."}, "http://arxiv.org/abs/2310.10761": {"title": "Simulation Based Composite Likelihood", "link": "http://arxiv.org/abs/2310.10761", "description": "Inference for high-dimensional hidden Markov models is challenging due to the\nexponential-in-dimension computational cost of the forward algorithm. To\naddress this issue, we introduce an innovative composite likelihood approach\ncalled \"Simulation Based Composite Likelihood\" (SimBa-CL). With SimBa-CL, we\napproximate the likelihood by the product of its marginals, which we estimate\nusing Monte Carlo sampling. In a similar vein to approximate Bayesian\ncomputation (ABC), SimBa-CL requires multiple simulations from the model, but,\nin contrast to ABC, it provides a likelihood approximation that guides the\noptimization of the parameters. Leveraging automatic differentiation libraries,\nit is simple to calculate gradients and Hessians to not only speed-up\noptimization, but also to build approximate confidence sets. We conclude with\nan extensive experimental section, where we empirically validate our\ntheoretical results, conduct a comparative analysis with SMC, and apply\nSimBa-CL to real-world Aphtovirus data."}, "http://arxiv.org/abs/2310.10798": {"title": "Poisson Count Time Series", "link": "http://arxiv.org/abs/2310.10798", "description": "This paper reviews and compares popular methods, some old and some very\nrecent, that produce time series having Poisson marginal distributions. The\npaper begins by narrating ways where time series with Poisson marginal\ndistributions can be produced. Modeling nonstationary series with covariates\nmotivates consideration of methods where the Poisson parameter depends on time.\nHere, estimation methods are developed for some of the more flexible methods.\nThe results are used in the analysis of 1) a count sequence of tropical\ncyclones occurring in the North Atlantic Basin since 1970, and 2) the number of\nno-hitter games pitched in major league baseball since 1893. Tests for whether\nthe Poisson marginal distribution is appropriate are included."}, "http://arxiv.org/abs/2310.10915": {"title": "Identifiability of the Multinomial Processing Tree-IRT model for the Philadelphia Naming Test", "link": "http://arxiv.org/abs/2310.10915", "description": "For persons with aphasia, naming tests are used to evaluate the severity of\nthe disease and observing progress toward recovery. The Philadelphia Naming\nTest (PNT) is a leading naming test composed of 175 items. The items are common\nnouns which are one to four syllables in length and with low, medium, and high\nfrequency. Since the target word is known to the administrator, the response\nfrom the patient can be classified as correct or an error. If the patient\ncommits an error, the PNT provides procedures for classifying the type of error\nin the response. Item response theory can be applied to PNT data to provide\nestimates of item difficulty and subject naming ability. Walker et al. (2018)\ndeveloped a IRT multinomial processing tree (IRT-MPT) model to attempt to\nunderstand the pathways through which the different errors are made by patients\nwhen responding to an item. The MPT model expands on existing models by\nconsidering items to be heterogeneous and estimating multiple latent parameters\nfor patients to more precisely determine at which step of word of production a\npatient's ability has been affected. These latent parameters represent the\ntheoretical cognitive steps taken in responding to an item. Given the\ncomplexity of the model proposed in Walker et al. (2018), here we investigate\nthe identifiability of the parameters included in the IRT-MPT model."}, "http://arxiv.org/abs/2310.10976": {"title": "Exact nonlinear state estimation", "link": "http://arxiv.org/abs/2310.10976", "description": "The majority of data assimilation (DA) methods in the geosciences are based\non Gaussian assumptions. While these assumptions facilitate efficient\nalgorithms, they cause analysis biases and subsequent forecast degradations.\nNon-parametric, particle-based DA algorithms have superior accuracy, but their\napplication to high-dimensional models still poses operational challenges.\nDrawing inspiration from recent advances in the field of generative artificial\nintelligence (AI), this article introduces a new nonlinear estimation theory\nwhich attempts to bridge the existing gap in DA methodology. Specifically, a\nConjugate Transform Filter (CTF) is derived and shown to generalize the\ncelebrated Kalman filter to arbitrarily non-Gaussian distributions. The new\nfilter has several desirable properties, such as its ability to preserve\nstatistical relationships in the prior state and convergence to highly accurate\nobservations. An ensemble approximation of the new theory (ECTF) is also\npresented and validated using idealized statistical experiments that feature\nbounded quantities with non-Gaussian distributions, a prevalent challenge in\nEarth system models. Results from these experiments indicate that the greatest\nbenefits from ECTF occur when observation errors are small relative to the\nforecast uncertainty and when state variables exhibit strong nonlinear\ndependencies. Ultimately, the new filtering theory offers exciting avenues for\nimproving conventional DA algorithms through their principled integration with\nAI techniques."}, "http://arxiv.org/abs/2310.11122": {"title": "Sensitivity-Aware Amortized Bayesian Inference", "link": "http://arxiv.org/abs/2310.11122", "description": "Bayesian inference is a powerful framework for making probabilistic\ninferences and decisions under uncertainty. Fundamental choices in modern\nBayesian workflows concern the specification of the likelihood function and\nprior distributions, the posterior approximator, and the data. Each choice can\nsignificantly influence model-based inference and subsequent decisions, thereby\nnecessitating sensitivity analysis. In this work, we propose a multifaceted\napproach to integrate sensitivity analyses into amortized Bayesian inference\n(ABI, i.e., simulation-based inference with neural networks). First, we utilize\nweight sharing to encode the structural similarities between alternative\nlikelihood and prior specifications in the training process with minimal\ncomputational overhead. Second, we leverage the rapid inference of neural\nnetworks to assess sensitivity to various data perturbations or pre-processing\nprocedures. In contrast to most other Bayesian approaches, both steps\ncircumvent the costly bottleneck of refitting the model(s) for each choice of\nlikelihood, prior, or dataset. Finally, we propose to use neural network\nensembles to evaluate variation in results induced by unreliable approximation\non unseen data. We demonstrate the effectiveness of our method in applied\nmodeling problems, ranging from the estimation of disease outbreak dynamics and\nglobal warming thresholds to the comparison of human decision-making models.\nOur experiments showcase how our approach enables practitioners to effectively\nunveil hidden relationships between modeling choices and inferential\nconclusions."}, "http://arxiv.org/abs/2310.11357": {"title": "A Pseudo-likelihood Approach to Under-5 Mortality Estimation", "link": "http://arxiv.org/abs/2310.11357", "description": "Accurate and precise estimates of under-5 mortality rates (U5MR) are an\nimportant health summary for countries. Full survival curves are additionally\nof interest to better understand the pattern of mortality in children under 5.\nModern demographic methods for estimating a full mortality schedule for\nchildren have been developed for countries with good vital registration and\nreliable census data, but perform poorly in many low- and middle-income\ncountries. In these countries, the need to utilize nationally representative\nsurveys to estimate U5MR requires additional statistical care to mitigate\npotential biases in survey data, acknowledge the survey design, and handle\naspects of survival data (i.e., censoring and truncation). In this paper, we\ndevelop parametric and non-parametric pseudo-likelihood approaches to\nestimating under-5 mortality across time from complex survey data. We argue\nthat the parametric approach is particularly useful in scenarios where data are\nsparse and estimation may require stronger assumptions. The nonparametric\napproach provides an aid to model validation. We compare a variety of\nparametric models to three existing methods for obtaining a full survival curve\nfor children under the age of 5, and argue that a parametric pseudo-likelihood\napproach is advantageous in low- and middle-income countries. We apply our\nproposed approaches to survey data from Burkina Faso, Malawi, Senegal, and\nNamibia. All code for fitting the models described in this paper is available\nin the R package pssst."}, "http://arxiv.org/abs/2006.00767": {"title": "Generative Multiple-purpose Sampler for Weighted M-estimation", "link": "http://arxiv.org/abs/2006.00767", "description": "To overcome the computational bottleneck of various data perturbation\nprocedures such as the bootstrap and cross validations, we propose the\nGenerative Multiple-purpose Sampler (GMS), which constructs a generator\nfunction to produce solutions of weighted M-estimators from a set of given\nweights and tuning parameters. The GMS is implemented by a single optimization\nwithout having to repeatedly evaluate the minimizers of weighted losses, and is\nthus capable of significantly reducing the computational time. We demonstrate\nthat the GMS framework enables the implementation of various statistical\nprocedures that would be unfeasible in a conventional framework, such as the\niterated bootstrap, bootstrapped cross-validation for penalized likelihood,\nbootstrapped empirical Bayes with nonparametric maximum likelihood, etc. To\nconstruct a computationally efficient generator function, we also propose a\nnovel form of neural network called the \\emph{weight multiplicative multilayer\nperceptron} to achieve fast convergence. Our numerical results demonstrate that\nthe new neural network structure enjoys a few orders of magnitude speed\nadvantage in comparison to the conventional one. An R package called GMS is\nprovided, which runs under Pytorch to implement the proposed methods and allows\nthe user to provide a customized loss function to tailor to their own models of\ninterest."}, "http://arxiv.org/abs/2012.03593": {"title": "Algebraic geometry of discrete interventional models", "link": "http://arxiv.org/abs/2012.03593", "description": "We investigate the algebra and geometry of general interventions in discrete\nDAG models. To this end, we introduce a theory for modeling soft interventions\nin the more general family of staged tree models and develop the formalism to\nstudy these models as parametrized subvarieties of a product of probability\nsimplices. We then consider the problem of finding their defining equations,\nand we derive a combinatorial criterion for identifying interventional staged\ntree models for which the defining ideal is toric. We apply these results to\nthe class of discrete interventional DAG models and establish a criteria to\ndetermine when these models are toric varieties."}, "http://arxiv.org/abs/2105.12720": {"title": "Marginal structural models with Latent Class Growth Modeling of Treatment Trajectories", "link": "http://arxiv.org/abs/2105.12720", "description": "In a real-life setting, little is known regarding the effectiveness of\nstatins for primary prevention among older adults, and analysis of\nobservational data can add crucial information on the benefits of actual\npatterns of use. Latent class growth models (LCGM) are increasingly proposed as\na solution to summarize the observed longitudinal treatment in a few distinct\ngroups. When combined with standard approaches like Cox proportional hazards\nmodels, LCGM can fail to control time-dependent confounding bias because of\ntime-varying covariates that have a double role of confounders and mediators.\nWe propose to use LCGM to classify individuals into a few latent classes based\non their medication adherence pattern, then choose a working marginal\nstructural model (MSM) that relates the outcome to these groups. The parameter\nof interest is nonparametrically defined as the projection of the true MSM onto\nthe chosen working model. The combination of LCGM with MSM is a convenient way\nto describe treatment adherence and can effectively control time-dependent\nconfounding. Simulation studies were used to illustrate our approach and\ncompare it with unadjusted, baseline covariates-adjusted, time-varying\ncovariates adjusted and inverse probability of trajectory groups weighting\nadjusted models. We found that our proposed approach yielded estimators with\nlittle or no bias."}, "http://arxiv.org/abs/2208.07610": {"title": "E-Statistics, Group Invariance and Anytime Valid Testing", "link": "http://arxiv.org/abs/2208.07610", "description": "We study worst-case-growth-rate-optimal (GROW) e-statistics for hypothesis\ntesting between two group models. It is known that under a mild condition on\nthe action of the underlying group G on the data, there exists a maximally\ninvariant statistic. We show that among all e-statistics, invariant or not, the\nlikelihood ratio of the maximally invariant statistic is GROW, both in the\nabsolute and in the relative sense, and that an anytime-valid test can be based\non it. The GROW e-statistic is equal to a Bayes factor with a right Haar prior\non G. Our treatment avoids nonuniqueness issues that sometimes arise for such\npriors in Bayesian contexts. A crucial assumption on the group G is its\namenability, a well-known group-theoretical condition, which holds, for\ninstance, in scale-location families. Our results also apply to\nfinite-dimensional linear regression."}, "http://arxiv.org/abs/2302.03246": {"title": "CDANs: Temporal Causal Discovery from Autocorrelated and Non-Stationary Time Series Data", "link": "http://arxiv.org/abs/2302.03246", "description": "Time series data are found in many areas of healthcare such as medical time\nseries, electronic health records (EHR), measurements of vitals, and wearable\ndevices. Causal discovery, which involves estimating causal relationships from\nobservational data, holds the potential to play a significant role in\nextracting actionable insights about human health. In this study, we present a\nnovel constraint-based causal discovery approach for autocorrelated and\nnon-stationary time series data (CDANs). Our proposed method addresses several\nlimitations of existing causal discovery methods for autocorrelated and\nnon-stationary time series data, such as high dimensionality, the inability to\nidentify lagged causal relationships, and overlooking changing modules. Our\napproach identifies lagged and instantaneous/contemporaneous causal\nrelationships along with changing modules that vary over time. The method\noptimizes the conditioning sets in a constraint-based search by considering\nlagged parents instead of conditioning on the entire past that addresses high\ndimensionality. The changing modules are detected by considering both\ncontemporaneous and lagged parents. The approach first detects the lagged\nadjacencies, then identifies the changing modules and contemporaneous\nadjacencies, and finally determines the causal direction. We extensively\nevaluated our proposed method on synthetic and real-world clinical datasets,\nand compared its performance with several baseline approaches. The experimental\nresults demonstrate the effectiveness of the proposed method in detecting\ncausal relationships and changing modules for autocorrelated and non-stationary\ntime series data."}, "http://arxiv.org/abs/2305.07089": {"title": "Hierarchically Coherent Multivariate Mixture Networks", "link": "http://arxiv.org/abs/2305.07089", "description": "Large collections of time series data are often organized into hierarchies\nwith different levels of aggregation; examples include product and geographical\ngroupings. Probabilistic coherent forecasting is tasked to produce forecasts\nconsistent across levels of aggregation. In this study, we propose to augment\nneural forecasting architectures with a coherent multivariate mixture output.\nWe optimize the networks with a composite likelihood objective, allowing us to\ncapture time series' relationships while maintaining high computational\nefficiency. Our approach demonstrates 13.2% average accuracy improvements on\nmost datasets compared to state-of-the-art baselines. We conduct ablation\nstudies of the framework components and provide theoretical foundations for\nthem. To assist related work, the code is available at this\nhttps://github.com/Nixtla/neuralforecast."}, "http://arxiv.org/abs/2307.16720": {"title": "The epigraph and the hypograph indexes as useful tools for clustering multivariate functional data", "link": "http://arxiv.org/abs/2307.16720", "description": "The proliferation of data generation has spurred advancements in functional\ndata analysis. With the ability to analyze multiple variables simultaneously,\nthe demand for working with multivariate functional data has increased. This\nstudy proposes a novel formulation of the epigraph and hypograph indexes, as\nwell as their generalized expressions, specifically tailored for the\nmultivariate functional context. These definitions take into account the\ninterrelations between components. Furthermore, the proposed indexes are\nemployed to cluster multivariate functional data. In the clustering process,\nthe indexes are applied to both the data and their first and second\nderivatives. This generates a reduced-dimension dataset from the original\nmultivariate functional data, enabling the application of well-established\nmultivariate clustering techniques which have been extensively studied in the\nliterature. This methodology has been tested through simulated and real\ndatasets, performing comparative analyses against state-of-the-art to assess\nits performance."}, "http://arxiv.org/abs/2309.07810": {"title": "Spectrum-Aware Adjustment: A New Debiasing Framework with Applications to Principal Component Regression", "link": "http://arxiv.org/abs/2309.07810", "description": "We introduce a new debiasing framework for high-dimensional linear regression\nthat bypasses the restrictions on covariate distributions imposed by modern\ndebiasing technology. We study the prevalent setting where the number of\nfeatures and samples are both large and comparable. In this context,\nstate-of-the-art debiasing technology uses a degrees-of-freedom correction to\nremove the shrinkage bias of regularized estimators and conduct inference.\nHowever, this method requires that the observed samples are i.i.d., the\ncovariates follow a mean zero Gaussian distribution, and reliable covariance\nmatrix estimates for observed features are available. This approach struggles\nwhen (i) covariates are non-Gaussian with heavy tails or asymmetric\ndistributions, (ii) rows of the design exhibit heterogeneity or dependencies,\nand (iii) reliable feature covariance estimates are lacking.\n\nTo address these, we develop a new strategy where the debiasing correction is\na rescaled gradient descent step (suitably initialized) with step size\ndetermined by the spectrum of the sample covariance matrix. Unlike prior work,\nwe assume that eigenvectors of this matrix are uniform draws from the\northogonal group. We show this assumption remains valid in diverse situations\nwhere traditional debiasing fails, including designs with complex row-column\ndependencies, heavy tails, asymmetric properties, and latent low-rank\nstructures. We establish asymptotic normality of our proposed estimator\n(centered and scaled) under various convergence notions. Moreover, we develop a\nconsistent estimator for its asymptotic variance. Lastly, we introduce a\ndebiased Principal Components Regression (PCR) technique using our\nSpectrum-Aware approach. In varied simulations and real data experiments, we\nobserve that our method outperforms degrees-of-freedom debiasing by a margin."}, "http://arxiv.org/abs/2310.11471": {"title": "Modeling lower-truncated and right-censored insurance claims with an extension of the MBBEFD class", "link": "http://arxiv.org/abs/2310.11471", "description": "In general insurance, claims are often lower-truncated and right-censored\nbecause insurance contracts may involve deductibles and maximal covers. Most\nclassical statistical models are not (directly) suited to model lower-truncated\nand right-censored claims. A surprisingly flexible family of distributions that\ncan cope with lower-truncated and right-censored claims is the class of MBBEFD\ndistributions that originally has been introduced by Bernegger (1997) for\nreinsurance pricing, but which has not gained much attention outside the\nreinsurance literature. We derive properties of the class of MBBEFD\ndistributions, and we extend it to a bigger family of distribution functions\nsuitable for modeling lower-truncated and right-censored claims. Interestingly,\nin general insurance, we mainly rely on unimodal skewed densities, whereas the\nreinsurance literature typically proposes monotonically decreasing densities\nwithin the MBBEFD class."}, "http://arxiv.org/abs/2310.11603": {"title": "Group sequential two-stage preference designs", "link": "http://arxiv.org/abs/2310.11603", "description": "The two-stage preference design (TSPD) enables the inference for treatment\nefficacy while allowing for incorporation of patient preference to treatment.\nIt can provide unbiased estimates for selection and preference effects, where a\nselection effect occurs when patients who prefer one treatment respond\ndifferently than those who prefer another, and a preference effect is the\ndifference in response caused by an interaction between the patient's\npreference and the actual treatment they receive. One potential barrier to\nadopting TSPD in practice, however, is the relatively large sample size\nrequired to estimate selection and preference effects with sufficient power. To\naddress this concern, we propose a group sequential two-stage preference design\n(GS-TSPD), which combines TSPD with sequential monitoring for early stopping.\nIn the GS-TSPD, pre-planned sequential monitoring allows investigators to\nconduct repeated hypothesis tests on accumulated data prior to full enrollment\nto assess study eligibility for early trial termination without inflating type\nI error rates. Thus, the procedure allows investigators to terminate the study\nwhen there is sufficient evidence of treatment, selection, or preference\neffects during an interim analysis, thereby reducing the design resource in\nexpectation. To formalize such a procedure, we verify the independent\nincrements assumption for testing the selection and preference effects and\napply group sequential stopping boundaries from the approximate sequential\ndensity functions. Simulations are then conducted to investigate the operating\ncharacteristics of our proposed GS-TSPD compared to the traditional TSPD. We\ndemonstrate the applicability of the design using a study of Hepatitis C\ntreatment modality."}, "http://arxiv.org/abs/2310.11620": {"title": "Enhancing modified treatment policy effect estimation with weighted energy distance", "link": "http://arxiv.org/abs/2310.11620", "description": "The effects of continuous treatments are often characterized through the\naverage dose response function, which is challenging to estimate from\nobservational data due to confounding and positivity violations. Modified\ntreatment policies (MTPs) are an alternative approach that aim to assess the\neffect of a modification to observed treatment values and work under relaxed\nassumptions. Estimators for MTPs generally focus on estimating the conditional\ndensity of treatment given covariates and using it to construct weights.\nHowever, weighting using conditional density models has well-documented\nchallenges. Further, MTPs with larger treatment modifications have stronger\nconfounding and no tools exist to help choose an appropriate modification\nmagnitude. This paper investigates the role of weights for MTPs showing that to\ncontrol confounding, weights should balance the weighted data to an unobserved\nhypothetical target population, that can be characterized with observed data.\nLeveraging this insight, we present a versatile set of tools to enhance\nestimation for MTPs. We introduce a distance that measures imbalance of\ncovariate distributions under the MTP and use it to develop new weighting\nmethods and tools to aid in the estimation of MTPs. We illustrate our methods\nthrough an example studying the effect of mechanical power of ventilation on\nin-hospital mortality."}, "http://arxiv.org/abs/2310.11630": {"title": "Adaptive Bootstrap Tests for Composite Null Hypotheses in the Mediation Pathway Analysis", "link": "http://arxiv.org/abs/2310.11630", "description": "Mediation analysis aims to assess if, and how, a certain exposure influences\nan outcome of interest through intermediate variables. This problem has\nrecently gained a surge of attention due to the tremendous need for such\nanalyses in scientific fields. Testing for the mediation effect is greatly\nchallenged by the fact that the underlying null hypothesis (i.e. the absence of\nmediation effects) is composite. Most existing mediation tests are overly\nconservative and thus underpowered. To overcome this significant methodological\nhurdle, we develop an adaptive bootstrap testing framework that can accommodate\ndifferent types of composite null hypotheses in the mediation pathway analysis.\nApplied to the product of coefficients (PoC) test and the joint significance\n(JS) test, our adaptive testing procedures provide type I error control under\nthe composite null, resulting in much improved statistical power compared to\nexisting tests. Both theoretical properties and numerical examples of the\nproposed methodology are discussed."}, "http://arxiv.org/abs/2310.11683": {"title": "Are we bootstrapping the right thing? A new approach to quantify uncertainty of Average Treatment Effect Estimate", "link": "http://arxiv.org/abs/2310.11683", "description": "Existing approaches of using the bootstrap method to derive standard error\nand confidence interval of average treatment effect estimate has one potential\nissue, which is that they are actually bootstrapping the wrong thing, resulting\nin unvalid statistical inference. In this paper, we discuss this important\nissue and propose a new non-parametric bootstrap method that can more precisely\nquantify the uncertainty associated with average treatment effect estimates. We\ndemonstrate the validity of this approach through a simulation study and a\nreal-world example, and highlight the importance of deriving standard error and\nconfidence interval of average treatment effect estimates that both remove\nextra undesired noise and are easy to interpret when applied in real world\nscenarios."}, "http://arxiv.org/abs/2310.11724": {"title": "Simultaneous Nonparametric Inference of M-regression under Complex Temporal Dynamics", "link": "http://arxiv.org/abs/2310.11724", "description": "The paper considers simultaneous nonparametric inference for a wide class of\nM-regression models with time-varying coefficients. The covariates and errors\nof the regression model are tackled as a general class of piece-wise locally\nstationary time series and are allowed to be cross-dependent. We introduce an\nintegration technique to study the M-estimators, whose limiting properties are\ndisclosed using Bahadur representation and Gaussian approximation theory.\nFacilitated by a self-convolved bootstrap proposed in this paper, we introduce\na unified framework to conduct general classes of Exact Function Tests,\nLack-of-fit Tests, and Qualitative Tests for the time-varying coefficient\nM-regression under complex temporal dynamics. As an application, our method is\napplied to studying the anthropogenic warming trend and time-varying structures\nof the ENSO effect using global climate data from 1882 to 2005."}, "http://arxiv.org/abs/2310.11741": {"title": "Graph Sphere: From Nodes to Supernodes in Graphical Models", "link": "http://arxiv.org/abs/2310.11741", "description": "High-dimensional data analysis typically focuses on low-dimensional\nstructure, often to aid interpretation and computational efficiency. Graphical\nmodels provide a powerful methodology for learning the conditional independence\nstructure in multivariate data by representing variables as nodes and\ndependencies as edges. Inference is often focused on individual edges in the\nlatent graph. Nonetheless, there is increasing interest in determining more\ncomplex structures, such as communities of nodes, for multiple reasons,\nincluding more effective information retrieval and better interpretability. In\nthis work, we propose a multilayer graphical model where we first cluster nodes\nand then, at the second layer, investigate the relationships among groups of\nnodes. Specifically, nodes are partitioned into \"supernodes\" with a\ndata-coherent size-biased tessellation prior which combines ideas from Bayesian\nnonparametrics and Voronoi tessellations. This construct allows accounting also\nfor dependence of nodes within supernodes. At the second layer, dependence\nstructure among supernodes is modelled through a Gaussian graphical model,\nwhere the focus of inference is on \"superedges\". We provide theoretical\njustification for our modelling choices. We design tailored Markov chain Monte\nCarlo schemes, which also enable parallel computations. We demonstrate the\neffectiveness of our approach for large-scale structure learning in simulations\nand a transcriptomics application."}, "http://arxiv.org/abs/2310.11779": {"title": "A Multivariate Skew-Normal-Tukey-h Distribution", "link": "http://arxiv.org/abs/2310.11779", "description": "We introduce a new family of multivariate distributions by taking the\ncomponent-wise Tukey-h transformation of a random vector following a\nskew-normal distribution. The proposed distribution is named the\nskew-normal-Tukey-h distribution and is an extension of the skew-normal\ndistribution for handling heavy-tailed data. We compare this proposed\ndistribution to the skew-t distribution, which is another extension of the\nskew-normal distribution for modeling tail-thickness, and demonstrate that when\nthere are substantial differences in marginal kurtosis, the proposed\ndistribution is more appropriate. Moreover, we derive many appealing stochastic\nproperties of the proposed distribution and provide a methodology for the\nestimation of the parameters in which the computational requirement increases\nlinearly with the dimension. Using simulations, as well as a wine and a wind\nspeed data application, we illustrate how to draw inferences based on the\nmultivariate skew-normal-Tukey-h distribution."}, "http://arxiv.org/abs/2310.11799": {"title": "Testing for patterns and structures in covariance and correlation matrices", "link": "http://arxiv.org/abs/2310.11799", "description": "Covariance matrices of random vectors contain information that is crucial for\nmodelling. Certain structures and patterns of the covariances (or correlations)\nmay be used to justify parametric models, e.g., autoregressive models. Until\nnow, there have been only few approaches for testing such covariance structures\nsystematically and in a unified way. In the present paper, we propose such a\nunified testing procedure, and we will exemplify the approach with a large\nvariety of covariance structure models. This includes common structures such as\ndiagonal matrices, Toeplitz matrices, and compound symmetry but also the more\ninvolved autoregressive matrices. We propose hypothesis tests for these\nstructures, and we use bootstrap techniques for better small-sample\napproximation. The structures of the proposed tests invite for adaptations to\nother covariance patterns by choosing the hypothesis matrix appropriately. We\nprove their correctness for large sample sizes. The proposed methods require\nonly weak assumptions.\n\nWith the help of a simulation study, we assess the small sample properties of\nthe tests.\n\nWe also analyze a real data set to illustrate the application of the\nprocedure."}, "http://arxiv.org/abs/2310.11822": {"title": "Post-clustering Inference under Dependency", "link": "http://arxiv.org/abs/2310.11822", "description": "Recent work by Gao et al. has laid the foundations for post-clustering\ninference. For the first time, the authors established a theoretical framework\nallowing to test for differences between means of estimated clusters.\nAdditionally, they studied the estimation of unknown parameters while\ncontrolling the selective type I error. However, their theory was developed for\nindependent observations identically distributed as $p$-dimensional Gaussian\nvariables with a spherical covariance matrix. Here, we aim at extending this\nframework to a more convenient scenario for practical applications, where\narbitrary dependence structures between observations and features are allowed.\nWe show that a $p$-value for post-clustering inference under general dependency\ncan be defined, and we assess the theoretical conditions allowing the\ncompatible estimation of a covariance matrix. The theory is developed for\nhierarchical agglomerative clustering algorithms with several types of\nlinkages, and for the $k$-means algorithm. We illustrate our method with\nsynthetic data and real data of protein structures."}, "http://arxiv.org/abs/2310.11969": {"title": "Survey calibration for causal inference: a simple method to balance covariate distributions", "link": "http://arxiv.org/abs/2310.11969", "description": "This paper proposes a simple method for balancing distributions of covariates\nfor causal inference based on observational studies. The method makes it\npossible to balance an arbitrary number of quantiles (e.g., medians, quartiles,\nor deciles) together with means if necessary. The proposed approach is based on\nthe theory of calibration estimators (Deville and S\\\"arndal 1992), in\nparticular, calibration estimators for quantiles, proposed by Harms and\nDuchesne (2006). By modifying the entropy balancing method and the covariate\nbalancing propensity score method, it is possible to balance the distributions\nof the treatment and control groups. The method does not require numerical\nintegration, kernel density estimation or assumptions about the distributions;\nvalid estimates can be obtained by drawing on existing asymptotic theory.\nResults of a simulation study indicate that the method efficiently estimates\naverage treatment effects on the treated (ATT), the average treatment effect\n(ATE), the quantile treatment effect on the treated (QTT) and the quantile\ntreatment effect (QTE), especially in the presence of non-linearity and\nmis-specification of the models. The proposed methods are implemented in an\nopen source R package jointCalib."}, "http://arxiv.org/abs/2310.12000": {"title": "Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models", "link": "http://arxiv.org/abs/2310.12000", "description": "Latent Gaussian process (GP) models are flexible probabilistic non-parametric\nfunction models. Vecchia approximations are accurate approximations for GPs to\novercome computational bottlenecks for large data, and the Laplace\napproximation is a fast method with asymptotic convergence guarantees to\napproximate marginal likelihoods and posterior predictive distributions for\nnon-Gaussian likelihoods. Unfortunately, the computational complexity of\ncombined Vecchia-Laplace approximations grows faster than linearly in the\nsample size when used in combination with direct solver methods such as the\nCholesky decomposition. Computations with Vecchia-Laplace approximations thus\nbecome prohibitively slow precisely when the approximations are usually the\nmost accurate, i.e., on large data sets. In this article, we present several\niterative methods for inference with Vecchia-Laplace approximations which make\ncomputations considerably faster compared to Cholesky-based calculations. We\nanalyze our proposed methods theoretically and in experiments with simulated\nand real-world data. In particular, we obtain a speed-up of an order of\nmagnitude compared to Cholesky-based inference and a threefold increase in\nprediction accuracy in terms of the continuous ranked probability score\ncompared to a state-of-the-art method on a large satellite data set. All\nmethods are implemented in a free C++ software library with high-level Python\nand R packages."}, "http://arxiv.org/abs/2310.12010": {"title": "A Note on Improving Variational Estimation for Multidimensional Item Response Theory", "link": "http://arxiv.org/abs/2310.12010", "description": "Survey instruments and assessments are frequently used in many domains of\nsocial science. When the constructs that these assessments try to measure\nbecome multifaceted, multidimensional item response theory (MIRT) provides a\nunified framework and convenient statistical tool for item analysis,\ncalibration, and scoring. However, the computational challenge of estimating\nMIRT models prohibits its wide use because many of the extant methods can\nhardly provide results in a realistic time frame when the number of dimensions,\nsample size, and test length are large. Instead, variational estimation\nmethods, such as Gaussian Variational Expectation Maximization (GVEM)\nalgorithm, have been recently proposed to solve the estimation challenge by\nproviding a fast and accurate solution. However, results have shown that\nvariational estimation methods may produce some bias on discrimination\nparameters during confirmatory model estimation, and this note proposes an\nimportance weighted version of GVEM (i.e., IW-GVEM) to correct for such bias\nunder MIRT models. We also use the adaptive moment estimation method to update\nthe learning rate for gradient descent automatically. Our simulations show that\nIW-GVEM can effectively correct bias with modest increase of computation time,\ncompared with GVEM. The proposed method may also shed light on improving the\nvariational estimation for other psychometrics models."}, "http://arxiv.org/abs/2310.12115": {"title": "MMD-based Variable Importance for Distributional Random Forest", "link": "http://arxiv.org/abs/2310.12115", "description": "Distributional Random Forest (DRF) is a flexible forest-based method to\nestimate the full conditional distribution of a multivariate output of interest\ngiven input variables. In this article, we introduce a variable importance\nalgorithm for DRFs, based on the well-established drop and relearn principle\nand MMD distance. While traditional importance measures only detect variables\nwith an influence on the output mean, our algorithm detects variables impacting\nthe output distribution more generally. We show that the introduced importance\nmeasure is consistent, exhibits high empirical performance on both real and\nsimulated data, and outperforms competitors. In particular, our algorithm is\nhighly efficient to select variables through recursive feature elimination, and\ncan therefore provide small sets of variables to build accurate estimates of\nconditional output distributions."}, "http://arxiv.org/abs/2310.12140": {"title": "Online Estimation with Rolling Validation: Adaptive Nonparametric Estimation with Stream Data", "link": "http://arxiv.org/abs/2310.12140", "description": "Online nonparametric estimators are gaining popularity due to their efficient\ncomputation and competitive generalization abilities. An important example\nincludes variants of stochastic gradient descent. These algorithms often take\none sample point at a time and instantly update the parameter estimate of\ninterest. In this work we consider model selection and hyperparameter tuning\nfor such online algorithms. We propose a weighted rolling-validation procedure,\nan online variant of leave-one-out cross-validation, that costs minimal extra\ncomputation for many typical stochastic gradient descent estimators. Similar to\nbatch cross-validation, it can boost base estimators to achieve a better,\nadaptive convergence rate. Our theoretical analysis is straightforward, relying\nmainly on some general statistical stability assumptions. The simulation study\nunderscores the significance of diverging weights in rolling validation in\npractice and demonstrates its sensitivity even when there is only a slim\ndifference between candidate estimators."}, "http://arxiv.org/abs/2010.02968": {"title": "Modelling of functional profiles and explainable shape shifts detection: An approach combining the notion of the Fr\\'echet mean with the shape invariant model", "link": "http://arxiv.org/abs/2010.02968", "description": "A modelling framework suitable for detecting shape shifts in functional\nprofiles combining the notion of Fr\\'echet mean and the concept of deformation\nmodels is developed and proposed. The generalized mean sense offered by the\nFr\\'echet mean notion is employed to capture the typical pattern of the\nprofiles under study, while the concept of deformation models, and in\nparticular of the shape invariant model, allows for interpretable\nparameterizations of profile's deviations from the typical shape. EWMA-type\ncontrol charts compatible with the functional nature of data and the employed\ndeformation model are built and proposed, exploiting certain shape\ncharacteristics of the profiles under study with respect to the generalized\nmean sense, allowing for the identification of potential shifts concerning the\nshape and/or the deformation process. Potential shifts in the shape deformation\nprocess, are further distinguished to significant shifts with respect to\namplitude and/or the phase of the profile under study. The proposed modelling\nand shift detection framework is implemented to a real world case study, where\ndaily concentration profiles concerning air pollutants from an area in the city\nof Athens are modelled, while profiles indicating hazardous concentration\nlevels are successfully identified in most of the cases."}, "http://arxiv.org/abs/2207.07218": {"title": "On the Selection of Tuning Parameters for Patch-Stitching Embedding Methods", "link": "http://arxiv.org/abs/2207.07218", "description": "While classical scaling, just like principal component analysis, is\nparameter-free, other methods for embedding multivariate data require the\nselection of one or several tuning parameters. This tuning can be difficult due\nto the unsupervised nature of the situation. We propose a simple, almost\nobvious, approach to supervise the choice of tuning parameter(s): minimize a\nnotion of stress. We apply this approach to the selection of the patch size in\na prototypical patch-stitching embedding method, both in the multidimensional\nscaling (aka network localization) setting and in the dimensionality reduction\n(aka manifold learning) setting. In our study, we uncover a new bias--variance\ntradeoff phenomenon."}, "http://arxiv.org/abs/2303.17856": {"title": "Bootstrapping multiple systems estimates to account for model selection", "link": "http://arxiv.org/abs/2303.17856", "description": "Multiple systems estimation using a Poisson loglinear model is a standard\napproach to quantifying hidden populations where data sources are based on\nlists of known cases. Information criteria are often used for selecting between\nthe large number of possible models. Confidence intervals are often reported\nconditional on the model selected, providing an over-optimistic impression of\nestimation accuracy. A bootstrap approach is a natural way to account for the\nmodel selection. However, because the model selection step has to be carried\nout for every bootstrap replication, there may be a high or even prohibitive\ncomputational burden. We explore the merit of modifying the model selection\nprocedure in the bootstrap to look only among a subset of models, chosen on the\nbasis of their information criterion score on the original data. This provides\nlarge computational gains with little apparent effect on inference. We also\nincorporate rigorous and economical ways of approaching issues of the existence\nof estimators when applying the method to sparse data tables."}, "http://arxiv.org/abs/2308.07319": {"title": "Partial identification for discrete data with nonignorable missing outcomes", "link": "http://arxiv.org/abs/2308.07319", "description": "Nonignorable missing outcomes are common in real world datasets and often\nrequire strong parametric assumptions to achieve identification. These\nassumptions can be implausible or untestable, and so we may forgo them in\nfavour of partially identified models that narrow the set of a priori possible\nvalues to an identification region. Here we propose a new nonparametric Bayes\nmethod that allows for the incorporation of multiple clinically relevant\nrestrictions of the parameter space simultaneously. We focus on two common\nrestrictions, instrumental variables and the direction of missing data bias,\nand investigate how these restrictions narrow the identification region for\nparameters of interest. Additionally, we propose a rejection sampling algorithm\nthat allows us to quantify the evidence for these assumptions in the data. We\ncompare our method to a standard Heckman selection model in both simulation\nstudies and in an applied problem examining the effectiveness of cash-transfers\nfor people experiencing homelessness."}, "http://arxiv.org/abs/2310.12285": {"title": "Sparse high-dimensional linear mixed modeling with a partitioned empirical Bayes ECM algorithm", "link": "http://arxiv.org/abs/2310.12285", "description": "High-dimensional longitudinal data is increasingly used in a wide range of\nscientific studies. However, there are few statistical methods for\nhigh-dimensional linear mixed models (LMMs), as most Bayesian variable\nselection or penalization methods are designed for independent observations.\nAdditionally, the few available software packages for high-dimensional LMMs\nsuffer from scalability issues. This work presents an efficient and accurate\nBayesian framework for high-dimensional LMMs. We use empirical Bayes estimators\nof hyperparameters for increased flexibility and an\nExpectation-Conditional-Minimization (ECM) algorithm for computationally\nefficient maximum a posteriori probability (MAP) estimation of parameters. The\nnovelty of the approach lies in its partitioning and parameter expansion as\nwell as its fast and scalable computation. We illustrate Linear Mixed Modeling\nwith PaRtitiOned empirical Bayes ECM (LMM-PROBE) in simulation studies\nevaluating fixed and random effects estimation along with computation time. A\nreal-world example is provided using data from a study of lupus in children,\nwhere we identify genes and clinical factors associated with a new lupus\nbiomarker and predict the biomarker over time."}, "http://arxiv.org/abs/2310.12348": {"title": "Goodness--of--Fit Tests Based on the Min--Characteristic Function", "link": "http://arxiv.org/abs/2310.12348", "description": "We propose tests of fit for classes of distributions that include the\nWeibull, the Pareto and the Fr\\'echet, distributions. The new tests employ the\nnovel tool of the min--characteristic function and are based on an L2--type\nweighted distance between this function and its empirical counterpart applied\non suitably standardized data. If data--standardization is performed using the\nMLE of the distributional parameters then the method reduces to testing for the\nstandard member of the family, with parameter values known and set equal to\none. We investigate asymptotic properties of the tests, while a Monte Carlo\nstudy is presented that includes the new procedure as well as competitors for\nthe purpose of specification testing with three extreme value distributions.\nThe new tests are also applied on a few real--data sets."}, "http://arxiv.org/abs/2310.12358": {"title": "causalBETA: An R Package for Bayesian Semiparametric Casual Inference with Event-Time Outcomes", "link": "http://arxiv.org/abs/2310.12358", "description": "Observational studies are often conducted to estimate causal effects of\ntreatments or exposures on event-time outcomes. Since treatments are not\nrandomized in observational studies, techniques from causal inference are\nrequired to adjust for confounding. Bayesian approaches to causal estimates are\ndesirable because they provide 1) prior smoothing provides useful\nregularization of causal effect estimates, 2) flexible models that are robust\nto misspecification, 3) full inference (i.e. both point and uncertainty\nestimates) for causal estimands. However, Bayesian causal inference is\ndifficult to implement manually and there is a lack of user-friendly software,\npresenting a significant barrier to wide-spread use. We address this gap by\ndeveloping causalBETA (Bayesian Event Time Analysis) - an open-source R package\nfor estimating causal effects on event-time outcomes using Bayesian\nsemiparametric models. The package provides a familiar front-end to users, with\nsyntax identical to existing survival analysis R packages such as survival. At\nthe same time, it back-ends to Stan - a popular platform for Bayesian modeling\nand high performance statistical computing - for efficient posterior\ncomputation. To improve user experience, the package is built using customized\nS3 class objects and methods to facilitate visualizations and summaries of\nresults using familiar generic functions like plot() and summary(). In this\npaper, we provide the methodological details of the package, a demonstration\nusing publicly-available data, and computational guidance."}, "http://arxiv.org/abs/2310.12391": {"title": "Real-time Semiparametric Regression via Sequential Monte Carlo", "link": "http://arxiv.org/abs/2310.12391", "description": "We develop and describe online algorithms for performing real-time\nsemiparametric regression analyses. Earlier work on this topic is in Luts,\nBroderick &amp; Wand (J. Comput. Graph. Statist., 2014) where online mean field\nvariational Bayes was employed. In this article we instead develop sequential\nMonte Carlo approaches to circumvent well-known inaccuracies inherent in\nvariational approaches. Even though sequential Monte Carlo is not as fast as\nonline mean field variational Bayes, it can be a viable alternative for\napplications where the data rate is not overly high. For Gaussian response\nsemiparametric regression models our new algorithms share the online mean field\nvariational Bayes property of only requiring updating and storage of sufficient\nstatistics quantities of streaming data. In the non-Gaussian case accurate\nreal-time semiparametric regression requires the full data to be kept in\nstorage. The new algorithms allow for new options concerning accuracy/speed\ntrade-offs for real-time semiparametric regression."}, "http://arxiv.org/abs/2310.12402": {"title": "Data visualization and dimension reduction for metric-valued response regression", "link": "http://arxiv.org/abs/2310.12402", "description": "As novel data collection becomes increasingly common, traditional dimension\nreduction and data visualization techniques are becoming inadequate to analyze\nthese complex data. A surrogate-assisted sufficient dimension reduction (SDR)\nmethod for regression with a general metric-valued response on Euclidean\npredictors is proposed. The response objects are mapped to a real-valued\ndistance matrix using an appropriate metric and then projected onto a large\nsample of random unit vectors to obtain scalar-valued surrogate responses. An\nensemble estimate of the subspaces for the regression of the surrogate\nresponses versus the predictor is used to estimate the original central space.\nUnder this framework, classical SDR methods such as ordinary least squares and\nsliced inverse regression are extended. The surrogate-assisted method applies\nto responses on compact metric spaces including but not limited to Euclidean,\ndistributional, and functional. An extensive simulation experiment demonstrates\nthe superior performance of the proposed surrogate-assisted method on synthetic\ndata compared to existing competing methods where applicable. The analysis of\nthe distributions and functional trajectories of county-level COVID-19\ntransmission rates in the U.S. as a function of demographic characteristics is\nalso provided. The theoretical justifications are included as well."}, "http://arxiv.org/abs/2310.12424": {"title": "Optimal heteroskedasticity testing in nonparametric regression", "link": "http://arxiv.org/abs/2310.12424", "description": "Heteroskedasticity testing in nonparametric regression is a classic\nstatistical problem with important practical applications, yet fundamental\nlimits are unknown. Adopting a minimax perspective, this article considers the\ntesting problem in the context of an $\\alpha$-H\\\"{o}lder mean and a\n$\\beta$-H\\\"{o}lder variance function. For $\\alpha &gt; 0$ and $\\beta \\in (0,\n\\frac{1}{2})$, the sharp minimax separation rate $n^{-4\\alpha} +\nn^{-\\frac{4\\beta}{4\\beta+1}} + n^{-2\\beta}$ is established. To achieve the\nminimax separation rate, a kernel-based statistic using first-order squared\ndifferences is developed. Notably, the statistic estimates a proxy rather than\na natural quadratic functional (the squared distance between the variance\nfunction and its best $L^2$ approximation by a constant) suggested in previous\nwork.\n\nThe setting where no smoothness is assumed on the variance function is also\nstudied; the variance profile across the design points can be arbitrary.\nDespite the lack of structure, consistent testing turns out to still be\npossible by using the Gaussian character of the noise, and the minimax rate is\nshown to be $n^{-4\\alpha} + n^{-1/2}$. Exploiting noise information happens to\nbe a fundamental necessity as consistent testing is impossible if nothing more\nthan zero mean and unit variance is known about the noise distribution.\nFurthermore, in the setting where $V$ is $\\beta$-H\\\"{o}lder but\nheteroskedasticity is measured only with respect to the design points, the\nminimax separation rate is shown to be $n^{-4\\alpha} + n^{-\\left(\\frac{1}{2}\n\\vee \\frac{4\\beta}{4\\beta+1}\\right)}$ when the noise is Gaussian and\n$n^{-4\\alpha} + n^{-\\frac{4\\beta}{4\\beta+1}} + n^{-2\\beta}$ when the noise\ndistribution is unknown."}, "http://arxiv.org/abs/2310.12427": {"title": "Fast Power Curve Approximation for Posterior Analyses", "link": "http://arxiv.org/abs/2310.12427", "description": "Bayesian hypothesis testing leverages posterior probabilities, Bayes factors,\nor credible intervals to assess characteristics that summarize data. We propose\na framework for power curve approximation with such hypothesis tests that\nassumes data are generated using statistical models with fixed parameters for\nthe purposes of sample size determination. We present a fast approach to\nexplore the sampling distribution of posterior probabilities when the\nconditions for the Bernstein-von Mises theorem are satisfied. We extend that\napproach to facilitate targeted sampling from the approximate sampling\ndistribution of posterior probabilities for each sample size explored. These\nsampling distributions are used to construct power curves for various types of\nposterior analyses. Our resulting method for power curve approximation is\norders of magnitude faster than conventional power curve estimation for\nBayesian hypothesis tests. We also prove the consistency of the corresponding\npower estimates and sample size recommendations under certain conditions."}, "http://arxiv.org/abs/2310.12428": {"title": "Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach", "link": "http://arxiv.org/abs/2310.12428", "description": "We initiate a novel approach to explain the out of sample performance of\nrandom forest (RF) models by exploiting the fact that any RF can be formulated\nas an adaptive weighted K nearest-neighbors model. Specifically, we use the\nproximity between points in the feature space learned by the RF to re-write\nrandom forest predictions exactly as a weighted average of the target labels of\ntraining data points. This linearity facilitates a local notion of\nexplainability of RF predictions that generates attributions for any model\nprediction across observations in the training set, and thereby complements\nestablished methods like SHAP, which instead generates attributions for a model\nprediction across dimensions of the feature space. We demonstrate this approach\nin the context of a bond pricing model trained on US corporate bond trades, and\ncompare our approach to various existing approaches to model explainability."}, "http://arxiv.org/abs/2310.12460": {"title": "Linear Source Apportionment using Generalized Least Squares", "link": "http://arxiv.org/abs/2310.12460", "description": "Motivated by applications to water quality monitoring using fluorescence\nspectroscopy, we develop the source apportionment model for high dimensional\nprofiles of dissolved organic matter (DOM). We describe simple methods to\nestimate the parameters of a linear source apportionment model, and show how\nthe estimates are related to those of ordinary and generalized least squares.\nUsing this least squares framework, we analyze the variability of the\nestimates, and we propose predictors for missing elements of a DOM profile. We\ndemonstrate the practical utility of our results on fluorescence spectroscopy\ndata collected from the Neuse River in North Carolina."}, "http://arxiv.org/abs/2310.12711": {"title": "Modelling multivariate extremes through angular-radial decomposition of the density function", "link": "http://arxiv.org/abs/2310.12711", "description": "We present a new framework for modelling multivariate extremes, based on an\nangular-radial representation of the probability density function. Under this\nrepresentation, the problem of modelling multivariate extremes is transformed\nto that of modelling an angular density and the tail of the radial variable,\nconditional on angle. Motivated by univariate theory, we assume that the tail\nof the conditional radial distribution converges to a generalised Pareto (GP)\ndistribution. To simplify inference, we also assume that the angular density is\ncontinuous and finite and the GP parameter functions are continuous with angle.\nWe refer to the resulting model as the semi-parametric angular-radial (SPAR)\nmodel for multivariate extremes. We consider the effect of the choice of polar\ncoordinate system and introduce generalised concepts of angular-radial\ncoordinate systems and generalised scalar angles in two dimensions. We show\nthat under certain conditions, the choice of polar coordinate system does not\naffect the validity of the SPAR assumptions. However, some choices of\ncoordinate system lead to simpler representations. In contrast, we show that\nthe choice of margin does affect whether the model assumptions are satisfied.\nIn particular, the use of Laplace margins results in a form of the density\nfunction for which the SPAR assumptions are satisfied for many common families\nof copula, with various dependence classes. We show that the SPAR model\nprovides a more versatile framework for characterising multivariate extremes\nthan provided by existing approaches, and that several commonly-used approaches\nare special cases of the SPAR model. Moreover, the SPAR framework provides a\nmeans of characterising all `extreme regions' of a joint distribution using a\nsingle inference. Applications in which this is useful are discussed."}, "http://arxiv.org/abs/2310.12757": {"title": "Conservative Inference for Counterfactuals", "link": "http://arxiv.org/abs/2310.12757", "description": "In causal inference, the joint law of a set of counterfactual random\nvariables is generally not identified. We show that a conservative version of\nthe joint law - corresponding to the smallest treatment effect - is identified.\nFinding this law uses recent results from optimal transport theory. Under this\nconservative law we can bound causal effects and we may construct inferences\nfor each individual's counterfactual dose-response curve. Intuitively, this is\nthe flattest counterfactual curve for each subject that is consistent with the\ndistribution of the observables. If the outcome is univariate then, under mild\nconditions, this curve is simply the quantile function of the counterfactual\ndistribution that passes through the observed point. This curve corresponds to\na nonparametric rank preserving structural model."}, "http://arxiv.org/abs/2310.12882": {"title": "Sequential Gibbs Posteriors with Applications to Principal Component Analysis", "link": "http://arxiv.org/abs/2310.12882", "description": "Gibbs posteriors are proportional to a prior distribution multiplied by an\nexponentiated loss function, with a key tuning parameter weighting information\nin the loss relative to the prior and providing a control of posterior\nuncertainty. Gibbs posteriors provide a principled framework for\nlikelihood-free Bayesian inference, but in many situations, including a single\ntuning parameter inevitably leads to poor uncertainty quantification. In\nparticular, regardless of the value of the parameter, credible regions have far\nfrom the nominal frequentist coverage even in large samples. We propose a\nsequential extension to Gibbs posteriors to address this problem. We prove the\nproposed sequential posterior exhibits concentration and a Bernstein-von Mises\ntheorem, which holds under easy to verify conditions in Euclidean space and on\nmanifolds. As a byproduct, we obtain the first Bernstein-von Mises theorem for\ntraditional likelihood-based Bayesian posteriors on manifolds. All methods are\nillustrated with an application to principal component analysis."}, "http://arxiv.org/abs/2207.06949": {"title": "Seeking the Truth Beyond the Data", "link": "http://arxiv.org/abs/2207.06949", "description": "Clustering is an unsupervised machine learning methodology where unlabeled\nelements/objects are grouped together aiming to the construction of\nwell-established clusters that their elements are classified according to their\nsimilarity. The goal of this process is to provide a useful aid to the\nresearcher that will help her/him to identify patterns among the data. Dealing\nwith large databases, such patterns may not be easily detectable without the\ncontribution of a clustering algorithm. This article provides a deep\ndescription of the most widely used clustering methodologies accompanied by\nuseful presentations concerning suitable parameter selection and\ninitializations. Simultaneously, this article not only represents a review\nhighlighting the major elements of examined clustering techniques but\nemphasizes the comparison of these algorithms' clustering efficiency based on 3\ndatasets, revealing their existing weaknesses and capabilities through accuracy\nand complexity, during the confrontation of discrete and continuous\nobservations. The produced results help us extract valuable conclusions about\nthe appropriateness of the examined clustering techniques in accordance with\nthe dataset's size."}, "http://arxiv.org/abs/2208.07831": {"title": "Structured prior distributions for the covariance matrix in latent factor models", "link": "http://arxiv.org/abs/2208.07831", "description": "Factor models are widely used for dimension reduction in the analysis of\nmultivariate data. This is achieved through decomposition of a p x p covariance\nmatrix into the sum of two components. Through a latent factor representation,\nthey can be interpreted as a diagonal matrix of idiosyncratic variances and a\nshared variation matrix, that is, the product of a p x k factor loadings matrix\nand its transpose. If k &lt;&lt; p, this defines a sparse factorisation of the\ncovariance matrix. Historically, little attention has been paid to\nincorporating prior information in Bayesian analyses using factor models where,\nat best, the prior for the factor loadings is order invariant. In this work, a\nclass of structured priors is developed that can encode ideas of dependence\nstructure about the shared variation matrix. The construction allows\ndata-informed shrinkage towards sensible parametric structures while also\nfacilitating inference over the number of factors. Using an unconstrained\nreparameterisation of stationary vector autoregressions, the methodology is\nextended to stationary dynamic factor models. For computational inference,\nparameter-expanded Markov chain Monte Carlo samplers are proposed, including an\nefficient adaptive Gibbs sampler. Two substantive applications showcase the\nscope of the methodology and its inferential benefits."}, "http://arxiv.org/abs/2209.11840": {"title": "Revisiting the Analysis of Matched-Pair and Stratified Experiments in the Presence of Attrition", "link": "http://arxiv.org/abs/2209.11840", "description": "In this paper we revisit some common recommendations regarding the analysis\nof matched-pair and stratified experimental designs in the presence of\nattrition. Our main objective is to clarify a number of well-known claims about\nthe practice of dropping pairs with an attrited unit when analyzing\nmatched-pair designs. Contradictory advice appears in the literature about\nwhether or not dropping pairs is beneficial or harmful, and stratifying into\nlarger groups has been recommended as a resolution to the issue. To address\nthese claims, we derive the estimands obtained from the difference-in-means\nestimator in a matched-pair design both when the observations from pairs with\nan attrited unit are retained and when they are dropped. We find limited\nevidence to support the claims that dropping pairs helps recover the average\ntreatment effect, but we find that it may potentially help in recovering a\nconvex weighted average of conditional average treatment effects. We report\nsimilar findings for stratified designs when studying the estimands obtained\nfrom a regression of outcomes on treatment with and without strata fixed\neffects."}, "http://arxiv.org/abs/2211.01746": {"title": "Log-density gradient covariance and automatic metric tensors for Riemann manifold Monte Carlo methods", "link": "http://arxiv.org/abs/2211.01746", "description": "A metric tensor for Riemann manifold Monte Carlo particularly suited for\nnon-linear Bayesian hierarchical models is proposed. The metric tensor is built\nfrom symmetric positive semidefinite log-density gradient covariance (LGC)\nmatrices, which are also proposed and further explored here. The LGCs\ngeneralize the Fisher information matrix by measuring the joint information\ncontent and dependence structure of both a random variable and the parameters\nof said variable. Consequently, positive definite Fisher/LGC-based metric\ntensors may be constructed not only from the observation likelihoods as is\ncurrent practice, but also from arbitrarily complicated non-linear prior/latent\nvariable structures, provided the LGC may be derived for each conditional\ndistribution used to construct said structures. The proposed methodology is\nhighly automatic and allows for exploitation of any sparsity associated with\nthe model in question. When implemented in conjunction with a Riemann manifold\nvariant of the recently proposed numerical generalized randomized Hamiltonian\nMonte Carlo processes, the proposed methodology is highly competitive, in\nparticular for the more challenging target distributions associated with\nBayesian hierarchical models."}, "http://arxiv.org/abs/2211.02383": {"title": "Simulation-Based Calibration Checking for Bayesian Computation: The Choice of Test Quantities Shapes Sensitivity", "link": "http://arxiv.org/abs/2211.02383", "description": "Simulation-based calibration checking (SBC) is a practical method to validate\ncomputationally-derived posterior distributions or their approximations. In\nthis paper, we introduce a new variant of SBC to alleviate several known\nproblems. Our variant allows the user to in principle detect any possible issue\nwith the posterior, while previously reported implementations could never\ndetect large classes of problems including when the posterior is equal to the\nprior. This is made possible by including additional data-dependent test\nquantities when running SBC. We argue and demonstrate that the joint likelihood\nof the data is an especially useful test quantity. Some other types of test\nquantities and their theoretical and practical benefits are also investigated.\nWe provide theoretical analysis of SBC, thereby providing a more complete\nunderstanding of the underlying statistical mechanisms. We also bring attention\nto a relatively common mistake in the literature and clarify the difference\nbetween SBC and checks based on the data-averaged posterior. We support our\nrecommendations with numerical case studies on a multivariate normal example\nand a case study in implementing an ordered simplex data type for use with\nHamiltonian Monte Carlo. The SBC variant introduced in this paper is\nimplemented in the $\\mathtt{SBC}$ R package."}, "http://arxiv.org/abs/2310.13081": {"title": "Metastable Hidden Markov Processes: a theory for modeling financial markets", "link": "http://arxiv.org/abs/2310.13081", "description": "The modeling of financial time series by hidden Markov models has been\nperformed successfully in the literature. In this paper, we propose a theory\nthat justifies such a modeling under the assumption that there exists a market\nformed by agents whose states evolve on time as an interacting Markov system\nthat has a metastable behavior described by the hidden Markov chain. This\ntheory is a rare application of metastability outside the modeling of physical\nsystems, and may inspire the development of new interacting Markov systems with\nfinancial constraints. In the context of financial economics and causal factor\ninvestment, the theory implies a new paradigm in which fluctuations in\ninvestment performance are primarily driven by the state of the market, rather\nthan being directly caused by other variables. Even though the usual approach\nto causal factor investment based on causal inference is not completely\ninconsistent with the proposed theory, the latter has the advantage of\naccounting for the non-stationary evolution of the time series through the\nchange between hidden market states. By accounting for this possibility, one\ncan more effectively assess risks and implement mitigation strategies."}, "http://arxiv.org/abs/2310.13162": {"title": "Network Meta-Analysis of Time-to-Event Endpoints with Individual Participant Data using Restricted Mean Survival Time Regression", "link": "http://arxiv.org/abs/2310.13162", "description": "Restricted mean survival time (RMST) models have gained popularity when\nanalyzing time-to-event outcomes because RMST models offer more straightforward\ninterpretations of treatment effects with fewer assumptions than hazard ratios\ncommonly estimated from Cox models. However, few network meta-analysis (NMA)\nmethods have been developed using RMST. In this paper, we propose advanced RMST\nNMA models when individual participant data are available. Our models allow us\nto study treatment effect moderation and provide comprehensive understanding\nabout comparative effectiveness of treatments and subgroup effects. An\nextensive simulation study and a real data example about treatments for\npatients with atrial fibrillation are presented."}, "http://arxiv.org/abs/2310.13178": {"title": "Exact Inference for Common Odds Ratio in Meta-Analysis with Zero-Total-Event Studies", "link": "http://arxiv.org/abs/2310.13178", "description": "Stemming from the high profile publication of Nissen and Wolski (2007) and\nsubsequent discussions with divergent views on how to handle observed\nzero-total-event studies, defined to be studies which observe zero events in\nboth treatment and control arms, the research topic concerning the common odds\nratio model with zero-total-event studies remains to be an unresolved problem\nin meta-analysis. In this article, we address this problem by proposing a novel\nrepro samples method to handle zero-total-event studies and make inference for\nthe parameter of common odds ratio. The development explicitly accounts for\nsampling scheme and does not rely on large sample approximation. It is\ntheoretically justified with a guaranteed finite sample performance. The\nempirical performance of the proposed method is demonstrated through simulation\nstudies. It shows that the proposed confidence set achieves the desired\nempirical coverage rate and also that the zero-total-event studies contains\ninformation and impacts the inference for the common odds ratio. The proposed\nmethod is applied to combine information in the Nissen and Wolski study."}, "http://arxiv.org/abs/2310.13232": {"title": "Interaction Screening and Pseudolikelihood Approaches for Tensor Learning in Ising Models", "link": "http://arxiv.org/abs/2310.13232", "description": "In this paper, we study two well known methods of Ising structure learning,\nnamely the pseudolikelihood approach and the interaction screening approach, in\nthe context of tensor recovery in $k$-spin Ising models. We show that both\nthese approaches, with proper regularization, retrieve the underlying\nhypernetwork structure using a sample size logarithmic in the number of network\nnodes, and exponential in the maximum interaction strength and maximum\nnode-degree. We also track down the exact dependence of the rate of tensor\nrecovery on the interaction order $k$, that is allowed to grow with the number\nof samples and nodes, for both the approaches. Finally, we provide a\ncomparative discussion of the performance of the two approaches based on\nsimulation studies, which also demonstrate the exponential dependence of the\ntensor recovery rate on the maximum coupling strength."}, "http://arxiv.org/abs/2310.13387": {"title": "Assumption violations in causal discovery and the robustness of score matching", "link": "http://arxiv.org/abs/2310.13387", "description": "When domain knowledge is limited and experimentation is restricted by\nethical, financial, or time constraints, practitioners turn to observational\ncausal discovery methods to recover the causal structure, exploiting the\nstatistical properties of their data. Because causal discovery without further\nassumptions is an ill-posed problem, each algorithm comes with its own set of\nusually untestable assumptions, some of which are hard to meet in real\ndatasets. Motivated by these considerations, this paper extensively benchmarks\nthe empirical performance of recent causal discovery methods on observational\ni.i.d. data generated under different background conditions, allowing for\nviolations of the critical assumptions required by each selected approach. Our\nexperimental findings show that score matching-based methods demonstrate\nsurprising performance in the false positive and false negative rate of the\ninferred graph in these challenging scenarios, and we provide theoretical\ninsights into their performance. This work is also the first effort to\nbenchmark the stability of causal discovery algorithms with respect to the\nvalues of their hyperparameters. Finally, we hope this paper will set a new\nstandard for the evaluation of causal discovery methods and can serve as an\naccessible entry point for practitioners interested in the field, highlighting\nthe empirical implications of different algorithm choices."}, "http://arxiv.org/abs/2310.13444": {"title": "Testing for the extent of instability in nearly unstable processes", "link": "http://arxiv.org/abs/2310.13444", "description": "This paper deals with unit root issues in time series analysis. It has been\nknown for a long time that unit root tests may be flawed when a series although\nstationary has a root close to unity. That motivated recent papers dedicated to\nautoregressive processes where the bridge between stability and instability is\nexpressed by means of time-varying coefficients. In this vein the process we\nconsider has a companion matrix $A_{n}$ with spectral radius $\\rho(A_{n}) &lt; 1$\nsatisfying $\\rho(A_{n}) \\rightarrow 1$, a situation that we describe as `nearly\nunstable'. The question we investigate is the following: given an observed path\nsupposed to come from a nearly-unstable process, is it possible to test for the\n`extent of instability', \\textit{i.e.} to test how close we are to the unit\nroot? In this regard, we develop a strategy to evaluate $\\alpha$ and to test\nfor $\\mathcal{H}_0 : \"\\alpha = \\alpha_0\"$ against $\\mathcal{H}_1 : \"\\alpha &gt;\n\\alpha_0\"$ when $\\rho(A_{n})$ lies in an inner $O(n^{-\\alpha})$-neighborhood of\nthe unity, for some $0 &lt; \\alpha &lt; 1$. Empirical evidence is given (on\nsimulations and real time series) about the advantages of the flexibility\ninduced by such a procedure compared to the usual unit root tests and their\nbinary responses. As a by-product, we also build a symmetric procedure for the\nusually left out situation where the dominant root lies around $-1$."}, "http://arxiv.org/abs/2310.13446": {"title": "Simple binning algorithm and SimDec visualization for comprehensive sensitivity analysis of complex computational models", "link": "http://arxiv.org/abs/2310.13446", "description": "Models of complex technological systems inherently contain interactions and\ndependencies among their input variables that affect their joint influence on\nthe output. Such models are often computationally expensive and few sensitivity\nanalysis methods can effectively process such complexities. Moreover, the\nsensitivity analysis field as a whole pays limited attention to the nature of\ninteraction effects, whose understanding can prove to be critical for the\ndesign of safe and reliable systems. In this paper, we introduce and\nextensively test a simple binning approach for computing sensitivity indices\nand demonstrate how complementing it with the smart visualization method,\nsimulation decomposition (SimDec), can permit important insights into the\nbehavior of complex engineering models. The simple binning approach computes\nfirst-, second-order effects, and a combined sensitivity index, and is\nconsiderably more computationally efficient than Sobol' indices. The totality\nof the sensitivity analysis framework provides an efficient and intuitive way\nto analyze the behavior of complex systems containing interactions and\ndependencies."}, "http://arxiv.org/abs/2310.13487": {"title": "Two-stage weighted least squares estimator of multivariate discrete-valued observation-driven models", "link": "http://arxiv.org/abs/2310.13487", "description": "In this work a general semi-parametric multivariate model where the first two\nconditional moments are assumed to be multivariate time series is introduced.\nThe focus of the estimation is the conditional mean parameter vector for\ndiscrete-valued distributions. Quasi-Maximum Likelihood Estimators (QMLEs)\nbased on the linear exponential family are typically employed for such\nestimation problems when the true multivariate conditional probability\ndistribution is unknown or too complex. Although QMLEs provide consistent\nestimates they may be inefficient. In this paper novel two-stage Multivariate\nWeighted Least Square Estimators (MWLSEs) are introduced which enjoy the same\nconsistency property as the QMLEs but can provide improved efficiency with\nsuitable choice of the covariance matrix of the observations. The proposed\nmethod allows for a more accurate estimation of model parameters in particular\nfor count and categorical data when maximum likelihood estimation is\nunfeasible. Moreover, consistency and asymptotic normality of MWLSEs are\nderived. The estimation performance of QMLEs and MWLSEs is compared through\nsimulation experiments and a real data application, showing superior accuracy\nof the proposed methodology."}, "http://arxiv.org/abs/2310.13511": {"title": "Dynamic Realized Minimum Variance Portfolio Models", "link": "http://arxiv.org/abs/2310.13511", "description": "This paper introduces a dynamic minimum variance portfolio (MVP) model using\nnonlinear volatility dynamic models, based on high-frequency financial data.\nSpecifically, we impose an autoregressive dynamic structure on MVP processes,\nwhich helps capture the MVP dynamics directly. To evaluate the dynamic MVP\nmodel, we estimate the inverse volatility matrix using the constrained\n$\\ell_1$-minimization for inverse matrix estimation (CLIME) and calculate daily\nrealized non-normalized MVP weights. Based on the realized non-normalized MVP\nweight estimator, we propose the dynamic MVP model, which we call the dynamic\nrealized minimum variance portfolio (DR-MVP) model. To estimate a large number\nof parameters, we employ the least absolute shrinkage and selection operator\n(LASSO) and predict the future MVP and establish its asymptotic properties.\nUsing high-frequency trading data, we apply the proposed method to MVP\nprediction."}, "http://arxiv.org/abs/2310.13580": {"title": "Bayesian Hierarchical Modeling for Bivariate Multiscale Spatial Data with Application to Blood Test Monitoring", "link": "http://arxiv.org/abs/2310.13580", "description": "In public health applications, spatial data collected are often recorded at\ndifferent spatial scales and over different correlated variables. Spatial\nchange of support is a key inferential problem in these applications and have\nbecome standard in univariate settings; however, it is less standard in\nmultivariate settings. There are several existing multivariate spatial models\nthat can be easily combined with multiscale spatial approach to analyze\nmultivariate multiscale spatial data. In this paper, we propose three new\nmodels from such combinations for bivariate multiscale spatial data in a\nBayesian context. In particular, we extend spatial random effects models,\nmultivariate conditional autoregressive models, and ordered hierarchical models\nthrough a multiscale spatial approach. We run simulation studies for the three\nmodels and compare them in terms of prediction performance and computational\nefficiency. We motivate our models through an analysis of 2015 Texas annual\naverage percentage receiving two blood tests from the Dartmouth Atlas Project."}, "http://arxiv.org/abs/2102.13209": {"title": "Wielding Occam's razor: Fast and frugal retail forecasting", "link": "http://arxiv.org/abs/2102.13209", "description": "The algorithms available for retail forecasting have increased in complexity.\nNewer methods, such as machine learning, are inherently complex. The more\ntraditional families of forecasting models, such as exponential smoothing and\nautoregressive integrated moving averages, have expanded to contain multiple\npossible forms and forecasting profiles. We question complexity in forecasting\nand the need to consider such large families of models. Our argument is that\nparsimoniously identifying suitable subsets of models will not decrease\nforecasting accuracy nor will it reduce the ability to estimate forecast\nuncertainty. We propose a framework that balances forecasting performance\nversus computational cost, resulting in the consideration of only a reduced set\nof models. We empirically demonstrate that a reduced set performs well.\nFinally, we translate computational benefits to monetary cost savings and\nenvironmental impact and discuss the implications of our results in the context\nof large retailers."}, "http://arxiv.org/abs/2211.04666": {"title": "Fast and Locally Adaptive Bayesian Quantile Smoothing using Calibrated Variational Approximations", "link": "http://arxiv.org/abs/2211.04666", "description": "Quantiles are useful characteristics of random variables that can provide\nsubstantial information on distributions compared with commonly used summary\nstatistics such as means. In this paper, we propose a Bayesian quantile trend\nfiltering method to estimate non-stationary trend of quantiles. We introduce\ngeneral shrinkage priors to induce locally adaptive Bayesian inference on\ntrends and mixture representation of the asymmetric Laplace likelihood. To\nquickly compute the posterior distribution, we develop calibrated mean-field\nvariational approximations to guarantee that the frequentist coverage of\ncredible intervals obtained from the approximated posterior is a specified\nnominal level. Simulation and empirical studies show that the proposed\nalgorithm is computationally much more efficient than the Gibbs sampler and\ntends to provide stable inference results, especially for high/low quantiles."}, "http://arxiv.org/abs/2305.17631": {"title": "A Bayesian Approach for Clustering Constant-wise Change-point Data", "link": "http://arxiv.org/abs/2305.17631", "description": "Change-point models deal with ordered data sequences. Their primary goal is\nto infer the locations where an aspect of the data sequence changes. In this\npaper, we propose and implement a nonparametric Bayesian model for clustering\nobservations based on their constant-wise change-point profiles via Gibbs\nsampler. Our model incorporates a Dirichlet Process on the constant-wise\nchange-point structures to cluster observations while simultaneously performing\nchange-point estimation. Additionally, our approach controls the number of\nclusters in the model, not requiring the specification of the number of\nclusters a priori. Our method's performance is evaluated on simulated data\nunder various scenarios and on a real dataset from single-cell genomic\nsequencing."}, "http://arxiv.org/abs/2306.06342": {"title": "Distribution-free inference with hierarchical data", "link": "http://arxiv.org/abs/2306.06342", "description": "This paper studies distribution-free inference in settings where the data set\nhas a hierarchical structure -- for example, groups of observations, or\nrepeated measurements. In such settings, standard notions of exchangeability\nmay not hold. To address this challenge, a hierarchical form of exchangeability\nis derived, facilitating extensions of distribution-free methods, including\nconformal prediction and jackknife+. While the standard theoretical guarantee\nobtained by the conformal prediction framework is a marginal predictive\ncoverage guarantee, in the special case of independent repeated measurements,\nit is possible to achieve a stronger form of coverage -- the \"second-moment\ncoverage\" property -- to provide better control of conditional miscoverage\nrates, and distribution-free prediction sets that achieve this property are\nconstructed. Simulations illustrate that this guarantee indeed leads to\nuniformly small conditional miscoverage rates. Empirically, this stronger\nguarantee comes at the cost of a larger width of the prediction set in\nscenarios where the fitted model is poorly calibrated, but this cost is very\nmild in cases where the fitted model is accurate."}, "http://arxiv.org/abs/2307.15205": {"title": "Robust graph-based methods for overcoming the curse of dimensionality", "link": "http://arxiv.org/abs/2307.15205", "description": "Graph-based two-sample tests and graph-based change-point detection that\nutilize a similarity graph provide a powerful tool for analyzing\nhigh-dimensional and non-Euclidean data as these methods do not impose\ndistributional assumptions on data and have good performance across various\nscenarios. Current graph-based tests that deliver efficacy across a broad\nspectrum of alternatives typically reply on the $K$-nearest neighbor graph or\nthe $K$-minimum spanning tree. However, these graphs can be vulnerable for\nhigh-dimensional data due to the curse of dimensionality. To mitigate this\nissue, we propose to use a robust graph that is considerably less influenced by\nthe curse of dimensionality. We also establish a theoretical foundation for\ngraph-based methods utilizing this proposed robust graph and demonstrate its\nconsistency under fixed alternatives for both low-dimensional and\nhigh-dimensional data."}, "http://arxiv.org/abs/2310.13764": {"title": "Random Flows of Covariance Operators and their Statistical Inference", "link": "http://arxiv.org/abs/2310.13764", "description": "We develop a statistical framework for conducting inference on collections of\ntime-varying covariance operators (covariance flows) over a general, possibly\ninfinite dimensional, Hilbert space. We model the intrinsically non-linear\nstructure of covariances by means of the Bures-Wasserstein metric geometry. We\nmake use of the Riemmanian-like structure induced by this metric to define a\nnotion of mean and covariance of a random flow, and develop an associated\nKarhunen-Lo\\`eve expansion. We then treat the problem of estimation and\nconstruction of functional principal components from a finite collection of\ncovariance flows. Our theoretical results are motivated by modern problems in\nfunctional data analysis, where one observes operator-valued random processes\n-- for instance when analysing dynamic functional connectivity and fMRI data,\nor when analysing multiple functional time series in the frequency domain.\n{Nevertheless, our framework is also novel in the finite-dimensions (matrix\ncase), and we demonstrate what simplifications can be afforded then}. We\nillustrate our methodology by means of simulations and a data analyses."}, "http://arxiv.org/abs/2310.13796": {"title": "Faithful graphical representations of local independence", "link": "http://arxiv.org/abs/2310.13796", "description": "Graphical models use graphs to represent conditional independence structure\nin the distribution of a random vector. In stochastic processes, graphs may\nrepresent so-called local independence or conditional Granger causality. Under\nsome regularity conditions, a local independence graph implies a set of\nindependences using a graphical criterion known as $\\delta$-separation, or\nusing its generalization, $\\mu$-separation. This is a stochastic process\nanalogue of $d$-separation in DAGs. However, there may be more independences\nthan implied by this graph and this is a violation of so-called faithfulness.\nWe characterize faithfulness in local independence graphs and give a method to\nconstruct a faithful graph from any local independence model such that the\noutput equals the true graph when Markov and faithfulness assumptions hold. We\ndiscuss various assumptions that are weaker than faithfulness, and we explore\ndifferent structure learning algorithms and their properties under varying\nassumptions."}, "http://arxiv.org/abs/2310.13826": {"title": "A p-value for Process Tracing and other N=1 Studies", "link": "http://arxiv.org/abs/2310.13826", "description": "The paper introduces a \\(p\\)-value that summarizes the evidence against a\nrival causal theory that explains an observed outcome in a single case. We show\nhow to represent the probability distribution characterizing a theorized rival\nhypothesis (the null) in the absence of randomization of treatment and when\ncounting on qualitative data, for instance when conducting process tracing. As\nin Fisher's \\autocite*{fisher1935design} original design, our \\(p\\)-value\nindicates how frequently one would find the same observations or even more\nfavorable observations under a theory that is compatible with our observations\nbut antagonistic to the working hypothesis. We also present an extension that\nallows researchers assess the sensitivity of their results to confirmation\nbias. Finally, we illustrate the application of our hypothesis test using the\nstudy by Snow \\autocite*{Snow1855} about the cause of Cholera in Soho, a\nclassic in Process Tracing, Epidemiology, and Microbiology. Our framework suits\nany type of case studies and evidence, such as data from interviews, archives,\nor participant observation."}, "http://arxiv.org/abs/2310.13858": {"title": "Likelihood-based surrogate dimension reduction", "link": "http://arxiv.org/abs/2310.13858", "description": "We consider the problem of surrogate sufficient dimension reduction, that is,\nestimating the central subspace of a regression model, when the covariates are\ncontaminated by measurement error. When no measurement error is present, a\nlikelihood-based dimension reduction method that relies on maximizing the\nlikelihood of a Gaussian inverse regression model on the Grassmann manifold is\nwell-known to have superior performance to traditional inverse moment methods.\nWe propose two likelihood-based estimators for the central subspace in\nmeasurement error settings, which make different adjustments to the observed\nsurrogates. Both estimators are computed based on maximizing objective\nfunctions on the Grassmann manifold and are shown to consistently recover the\ntrue central subspace. When the central subspace is assumed to depend on only a\nfew covariates, we further propose to augment the likelihood function with a\npenalty term that induces sparsity on the Grassmann manifold to obtain sparse\nestimators. The resulting objective function has a closed-form Riemann gradient\nwhich facilitates efficient computation of the penalized estimator. We leverage\nthe state-of-the-art trust region algorithm on the Grassmann manifold to\ncompute the proposed estimators efficiently. Simulation studies and a data\napplication demonstrate the proposed likelihood-based estimators perform better\nthan inverse moment-based estimators in terms of both estimation and variable\nselection accuracy."}, "http://arxiv.org/abs/2310.13874": {"title": "A Linear Errors-in-Variables Model with Unknown Heteroscedastic Measurement Errors", "link": "http://arxiv.org/abs/2310.13874", "description": "In the classic measurement error framework, covariates are contaminated by\nindependent additive noise. This paper considers parameter estimation in such a\nlinear errors-in-variables model where the unknown measurement error\ndistribution is heteroscedastic across observations. We propose a new\ngeneralized method of moment (GMM) estimator that combines a moment correction\napproach and a phase function-based approach. The former requires distributions\nto have four finite moments, while the latter relies on covariates having\nasymmetric distributions. The new estimator is shown to be consistent and\nasymptotically normal under appropriate regularity conditions. The asymptotic\ncovariance of the estimator is derived, and the estimated standard error is\ncomputed using a fast bootstrap procedure. The GMM estimator is demonstrated to\nhave strong finite sample performance in numerical studies, especially when the\nmeasurement errors follow non-Gaussian distributions."}, "http://arxiv.org/abs/2310.13911": {"title": "Multilevel Matrix Factor Model", "link": "http://arxiv.org/abs/2310.13911", "description": "Large-scale matrix data has been widely discovered and continuously studied\nin various fields recently. Considering the multi-level factor structure and\nutilizing the matrix structure, we propose a multilevel matrix factor model\nwith both global and local factors. The global factors can affect all matrix\ntimes series, whereas the local factors are only allow to affect within each\nspecific matrix time series. The estimation procedures can consistently\nestimate the factor loadings and determine the number of factors. We establish\nthe asymptotic properties of the estimators. The simulation is presented to\nillustrate the performance of the proposed estimation method. We utilize the\nmodel to analyze eight indicators across 200 stocks from ten distinct\nindustries, demonstrating the empirical utility of our proposed approach."}, "http://arxiv.org/abs/2310.13966": {"title": "Minimax Optimal Transfer Learning for Kernel-based Nonparametric Regression", "link": "http://arxiv.org/abs/2310.13966", "description": "In recent years, transfer learning has garnered significant attention in the\nmachine learning community. Its ability to leverage knowledge from related\nstudies to improve generalization performance in a target study has made it\nhighly appealing. This paper focuses on investigating the transfer learning\nproblem within the context of nonparametric regression over a reproducing\nkernel Hilbert space. The aim is to bridge the gap between practical\neffectiveness and theoretical guarantees. We specifically consider two\nscenarios: one where the transferable sources are known and another where they\nare unknown. For the known transferable source case, we propose a two-step\nkernel-based estimator by solely using kernel ridge regression. For the unknown\ncase, we develop a novel method based on an efficient aggregation algorithm,\nwhich can automatically detect and alleviate the effects of negative sources.\nThis paper provides the statistical properties of the desired estimators and\nestablishes the minimax optimal rate. Through extensive numerical experiments\non synthetic data and real examples, we validate our theoretical findings and\ndemonstrate the effectiveness of our proposed method."}, "http://arxiv.org/abs/2310.13973": {"title": "Estimation and convergence rates in the distributional single index model", "link": "http://arxiv.org/abs/2310.13973", "description": "The distributional single index model is a semiparametric regression model in\nwhich the conditional distribution functions $P(Y \\leq y | X = x) =\nF_0(\\theta_0(x), y)$ of a real-valued outcome variable $Y$ depend on\n$d$-dimensional covariates $X$ through a univariate, parametric index function\n$\\theta_0(x)$, and increase stochastically as $\\theta_0(x)$ increases. We\npropose least squares approaches for the joint estimation of $\\theta_0$ and\n$F_0$ in the important case where $\\theta_0(x) = \\alpha_0^{\\top}x$ and obtain\nconvergence rates of $n^{-1/3}$, thereby improving an existing result that\ngives a rate of $n^{-1/6}$. A simulation study indicates that the convergence\nrate for the estimation of $\\alpha_0$ might be faster. Furthermore, we\nillustrate our methods in a real data application that demonstrates the\nadvantages of shape restrictions in single index models."}, "http://arxiv.org/abs/2310.14068": {"title": "Unobserved Grouped Heteroskedasticity and Fixed Effects", "link": "http://arxiv.org/abs/2310.14068", "description": "This paper extends the linear grouped fixed effects (GFE) panel model to\nallow for heteroskedasticity from a discrete latent group variable. Key\nfeatures of GFE are preserved, such as individuals belonging to one of a finite\nnumber of groups and group membership is unrestricted and estimated. Ignoring\ngroup heteroskedasticity may lead to poor classification, which is detrimental\nto finite sample bias and standard errors of estimators. I introduce the\n\"weighted grouped fixed effects\" (WGFE) estimator that minimizes a weighted\naverage of group sum of squared residuals. I establish $\\sqrt{NT}$-consistency\nand normality under a concept of group separation based on second moments. A\ntest of group homoskedasticity is discussed. A fast computation procedure is\nprovided. Simulations show that WGFE outperforms alternatives that exclude\nsecond moment information. I demonstrate this approach by considering the link\nbetween income and democracy and the effect of unionization on earnings."}, "http://arxiv.org/abs/2310.14246": {"title": "Shortcuts for causal discovery of nonlinear models by score matching", "link": "http://arxiv.org/abs/2310.14246", "description": "The use of simulated data in the field of causal discovery is ubiquitous due\nto the scarcity of annotated real data. Recently, Reisach et al., 2021\nhighlighted the emergence of patterns in simulated linear data, which displays\nincreasing marginal variance in the casual direction. As an ablation in their\nexperiments, Montagna et al., 2023 found that similar patterns may emerge in\nnonlinear models for the variance of the score vector $\\nabla \\log\np_{\\mathbf{X}}$, and introduced the ScoreSort algorithm. In this work, we\nformally define and characterize this score-sortability pattern of nonlinear\nadditive noise models. We find that it defines a class of identifiable\n(bivariate) causal models overlapping with nonlinear additive noise models. We\ntheoretically demonstrate the advantages of ScoreSort in terms of statistical\nefficiency compared to prior state-of-the-art score matching-based methods and\nempirically show the score-sortability of the most common synthetic benchmarks\nin the literature. Our findings remark (1) the lack of diversity in the data as\nan important limitation in the evaluation of nonlinear causal discovery\napproaches, (2) the importance of thoroughly testing different settings within\na problem class, and (3) the importance of analyzing statistical properties in\ncausal discovery, where research is often limited to defining identifiability\nconditions of the model."}, "http://arxiv.org/abs/2310.14293": {"title": "Testing exchangeability by pairwise betting", "link": "http://arxiv.org/abs/2310.14293", "description": "In this paper, we address the problem of testing exchangeability of a\nsequence of random variables, $X_1, X_2,\\cdots$. This problem has been studied\nunder the recently popular framework of testing by betting. But the mapping of\ntesting problems to game is not one to one: many games can be designed for the\nsame test. Past work established that it is futile to play single game betting\non every observation: test martingales in the data filtration are powerless.\nTwo avenues have been explored to circumvent this impossibility: betting in a\nreduced filtration (wealth is a test martingale in a coarsened filtration), or\nplaying many games in parallel (wealth is an e-process in the data filtration).\nThe former has proved to be difficult to theoretically analyze, while the\nlatter only works for binary or discrete observation spaces. Here, we introduce\na different approach that circumvents both drawbacks. We design a new (yet\nsimple) game in which we observe the data sequence in pairs. Despite the fact\nthat betting on individual observations is futile, we show that betting on\npairs of observations is not. To elaborate, we prove that our game leads to a\nnontrivial test martingale, which is interesting because it has been obtained\nby shrinking the filtration very slightly. We show that our test controls\ntype-1 error despite continuous monitoring, and achieves power one for both\nbinary and continuous observations, under a broad class of alternatives. Due to\nthe shrunk filtration, optional stopping is only allowed at even stopping\ntimes, not at odd ones: a relatively minor price. We provide a wide array of\nsimulations that align with our theoretical findings."}, "http://arxiv.org/abs/2310.14399": {"title": "The role of randomization inference in unraveling individual treatment effects in clinical trials: Application to HIV vaccine trials", "link": "http://arxiv.org/abs/2310.14399", "description": "Randomization inference is a powerful tool in early phase vaccine trials to\nestimate the causal effect of a regimen against a placebo or another regimen.\nTraditionally, randomization-based inference often focuses on testing either\nFisher's sharp null hypothesis of no treatment effect for any unit or Neyman's\nweak null hypothesis of no sample average treatment effect. Many recent efforts\nhave explored conducting exact randomization-based inference for other\nsummaries of the treatment effect profile, for instance, quantiles of the\ntreatment effect distribution function. In this article, we systematically\nreview methods that conduct exact, randomization-based inference for quantiles\nof individual treatment effects (ITEs) and extend some results by incorporating\nauxiliary information often available in a vaccine trial. These methods are\nsuitable for four scenarios: (i) a randomized controlled trial (RCT) where the\npotential outcomes under one regimen are constant; (ii) an RCT with no\nrestriction on any potential outcomes; (iii) an RCT with some user-specified\nbounds on potential outcomes; and (iv) a matched study comparing two\nnon-randomized, possibly confounded treatment arms. We then conduct two\nextensive simulation studies, one comparing the performance of each method in\nmany practical clinical settings and the other evaluating the usefulness of the\nmethods in ranking and advancing experimental therapies. We apply these methods\nto an early-phase clinical trail, HIV Vaccine Trials Network Study 086 (HVTN\n086), to showcase the usefulness of the methods."}, "http://arxiv.org/abs/2310.14419": {"title": "An RKHS Approach for Variable Selection in High-dimensional Functional Linear Models", "link": "http://arxiv.org/abs/2310.14419", "description": "High-dimensional functional data has become increasingly prevalent in modern\napplications such as high-frequency financial data and neuroimaging data\nanalysis. We investigate a class of high-dimensional linear regression models,\nwhere each predictor is a random element in an infinite dimensional function\nspace, and the number of functional predictors p can potentially be much\ngreater than the sample size n. Assuming that each of the unknown coefficient\nfunctions belongs to some reproducing kernel Hilbert space (RKHS), we\nregularized the fitting of the model by imposing a group elastic-net type of\npenalty on the RKHS norms of the coefficient functions. We show that our loss\nfunction is Gateaux sub-differentiable, and our functional elastic-net\nestimator exists uniquely in the product RKHS. Under suitable sparsity\nassumptions and a functional version of the irrepresentible condition, we\nestablish the variable selection consistency property of our approach. The\nproposed method is illustrated through simulation studies and a real-data\napplication from the Human Connectome Project."}, "http://arxiv.org/abs/2310.14448": {"title": "Semiparametrically Efficient Score for the Survival Odds Ratio", "link": "http://arxiv.org/abs/2310.14448", "description": "We consider a general proportional odds model for survival data under binary\ntreatment, where the functional form of the covariates is left unspecified. We\nderive the efficient score for the conditional survival odds ratio given the\ncovariates using modern semiparametric theory. The efficient score may be\nuseful in the development of doubly robust estimators, although computational\nchallenges remain."}, "http://arxiv.org/abs/2310.14763": {"title": "Externally Valid Policy Evaluation Combining Trial and Observational Data", "link": "http://arxiv.org/abs/2310.14763", "description": "Randomized trials are widely considered as the gold standard for evaluating\nthe effects of decision policies. Trial data is, however, drawn from a\npopulation which may differ from the intended target population and this raises\na problem of external validity (aka. generalizability). In this paper we seek\nto use trial data to draw valid inferences about the outcome of a policy on the\ntarget population. Additional covariate data from the target population is used\nto model the sampling of individuals in the trial study. We develop a method\nthat yields certifiably valid trial-based policy evaluations under any\nspecified range of model miscalibrations. The method is nonparametric and the\nvalidity is assured even with finite samples. The certified policy evaluations\nare illustrated using both simulated and real data."}, "http://arxiv.org/abs/2310.14922": {"title": "The Complex Network Patterns of Human Migration at Different Geographical Scales: Network Science meets Regression Analysis", "link": "http://arxiv.org/abs/2310.14922", "description": "Migration's influence in shaping population dynamics in times of impending\nclimate and population crises exposes its crucial role in upholding societal\ncohesion. As migration impacts virtually all aspects of life, it continues to\nrequire attention across scientific disciplines. This study delves into two\ndistinctive substrates of Migration Studies: the \"why\" substrate, which deals\nwith identifying the factors driving migration relying primarily on regression\nmodeling, encompassing economic, demographic, geographic, cultural, political,\nand other variables; and the \"how\" substrate, which focuses on identifying\nmigration flows and patterns, drawing from Network Science tools and\nvisualization techniques to depict complex migration networks. Despite the\ngrowing percentage of Network Science studies in migration, the explanations of\nthe identified network traits remain very scarce, highlighting the detachment\nbetween the two research substrates. Our study includes real-world network\nanalyses of human migration across different geographical levels: city,\ncountry, and global. We examine inter-district migration in Vienna at the city\nlevel, review internal migration networks in Austria and Croatia at the country\nlevel, and analyze migration exchange between Croatia and the world at the\nglobal level. By comparing network structures, we demonstrate how distinct\nnetwork traits impact regression modeling. This work not only uncovers\nmigration network patterns in previously unexplored areas but also presents a\ncomprehensive overview of recent research, highlighting gaps in each field and\ntheir interconnectedness. Our contribution offers suggestions for integrating\nboth fields to enhance methodological rigor and support future research."}, "http://arxiv.org/abs/2310.14983": {"title": "Causal clustering: design of cluster experiments under network interference", "link": "http://arxiv.org/abs/2310.14983", "description": "This paper studies the design of cluster experiments to estimate the global\ntreatment effect in the presence of spillovers on a single network. We provide\nan econometric framework to choose the clustering that minimizes the worst-case\nmean-squared error of the estimated global treatment effect. We show that the\noptimal clustering can be approximated as the solution of a novel penalized\nmin-cut optimization problem computed via off-the-shelf semi-definite\nprogramming algorithms. Our analysis also characterizes easy-to-check\nconditions to choose between a cluster or individual-level randomization. We\nillustrate the method's properties using unique network data from the universe\nof Facebook's users and existing network data from a field experiment."}, "http://arxiv.org/abs/2310.15016": {"title": "Impact of Record-Linkage Errors in Covid-19 Vaccine-Safety Analyses using German Health-Care Data: A Simulation Study", "link": "http://arxiv.org/abs/2310.15016", "description": "With unprecedented speed, 192,248,678 doses of Covid-19 vaccines were\nadministered in Germany by July 11, 2023 to combat the pandemic. Limitations of\nclinical trials imply that the safety profile of these vaccines is not fully\nknown before marketing. However, routine health-care data can help address\nthese issues. Despite the high proportion of insured people, the analysis of\nvaccination-related data is challenging in Germany. Generally, the Covid-19\nvaccination status and other health-care data are stored in separate databases,\nwithout persistent and database-independent person identifiers. Error-prone\nrecord-linkage techniques must be used to merge these databases. Our aim was to\nquantify the impact of record-linkage errors on the power and bias of different\nanalysis methods designed to assess Covid-19 vaccine safety when using German\nhealth-care data with a Monte-Carlo simulation study. We used a discrete-time\nsimulation and empirical data to generate realistic data with varying amounts\nof record-linkage errors. Afterwards, we analysed this data using a Cox model\nand the self-controlled case series (SCCS) method. Realistic proportions of\nrandom linkage errors only had little effect on the power of either method. The\nSCCS method produced unbiased results even with a high percentage of linkage\nerrors, while the Cox model underestimated the true effect."}, "http://arxiv.org/abs/2310.15069": {"title": "Second-order group knockoffs with applications to GWAS", "link": "http://arxiv.org/abs/2310.15069", "description": "Conditional testing via the knockoff framework allows one to identify --\namong large number of possible explanatory variables -- those that carry unique\ninformation about an outcome of interest, and also provides a false discovery\nrate guarantee on the selection. This approach is particularly well suited to\nthe analysis of genome wide association studies (GWAS), which have the goal of\nidentifying genetic variants which influence traits of medical relevance.\n\nWhile conditional testing can be both more powerful and precise than\ntraditional GWAS analysis methods, its vanilla implementation encounters a\ndifficulty common to all multivariate analysis methods: it is challenging to\ndistinguish among multiple, highly correlated regressors. This impasse can be\novercome by shifting the object of inference from single variables to groups of\ncorrelated variables. To achieve this, it is necessary to construct \"group\nknockoffs.\" While successful examples are already documented in the literature,\nthis paper substantially expands the set of algorithms and software for group\nknockoffs. We focus in particular on second-order knockoffs, for which we\ndescribe correlation matrix approximations that are appropriate for GWAS data\nand that result in considerable computational savings. We illustrate the\neffectiveness of the proposed methods with simulations and with the analysis of\nalbuminuria data from the UK Biobank.\n\nThe described algorithms are implemented in an open-source Julia package\nKnockoffs.jl, for which both R and Python wrappers are available."}, "http://arxiv.org/abs/2310.15070": {"title": "Improving estimation efficiency of case-cohort study with interval-censored failure time data", "link": "http://arxiv.org/abs/2310.15070", "description": "The case-cohort design is a commonly used cost-effective sampling strategy\nfor large cohort studies, where some covariates are expensive to measure or\nobtain. In this paper, we consider regression analysis under a case-cohort\nstudy with interval-censored failure time data, where the failure time is only\nknown to fall within an interval instead of being exactly observed. A common\napproach to analyze data from a case-cohort study is the inverse probability\nweighting approach, where only subjects in the case-cohort sample are used in\nestimation, and the subjects are weighted based on the probability of inclusion\ninto the case-cohort sample. This approach, though consistent, is generally\ninefficient as it does not incorporate information outside the case-cohort\nsample. To improve efficiency, we first develop a sieve maximum weighted\nlikelihood estimator under the Cox model based on the case-cohort sample, and\nthen propose a procedure to update this estimator by using information in the\nfull cohort. We show that the update estimator is consistent, asymptotically\nnormal, and more efficient than the original estimator. The proposed method can\nflexibly incorporate auxiliary variables to further improve estimation\nefficiency. We employ a weighted bootstrap procedure for variance estimation.\nSimulation results indicate that the proposed method works well in practical\nsituations. A real study on diabetes is provided for illustration."}, "http://arxiv.org/abs/2310.15108": {"title": "Evaluating machine learning models in non-standard settings: An overview and new findings", "link": "http://arxiv.org/abs/2310.15108", "description": "Estimating the generalization error (GE) of machine learning models is\nfundamental, with resampling methods being the most common approach. However,\nin non-standard settings, particularly those where observations are not\nindependently and identically distributed, resampling using simple random data\ndivisions may lead to biased GE estimates. This paper strives to present\nwell-grounded guidelines for GE estimation in various such non-standard\nsettings: clustered data, spatial data, unequal sampling probabilities, concept\ndrift, and hierarchically structured outcomes. Our overview combines\nwell-established methodologies with other existing methods that, to our\nknowledge, have not been frequently considered in these particular settings. A\nunifying principle among these techniques is that the test data used in each\niteration of the resampling procedure should reflect the new observations to\nwhich the model will be applied, while the training data should be\nrepresentative of the entire data set used to obtain the final model. Beyond\nproviding an overview, we address literature gaps by conducting simulation\nstudies. These studies assess the necessity of using GE-estimation methods\ntailored to the respective setting. Our findings corroborate the concern that\nstandard resampling methods often yield biased GE estimates in non-standard\nsettings, underscoring the importance of tailored GE estimation."}, "http://arxiv.org/abs/2310.15124": {"title": "Mixed-Variable Global Sensitivity Analysis For Knowledge Discovery And Efficient Combinatorial Materials Design", "link": "http://arxiv.org/abs/2310.15124", "description": "Global Sensitivity Analysis (GSA) is the study of the influence of any given\ninputs on the outputs of a model. In the context of engineering design, GSA has\nbeen widely used to understand both individual and collective contributions of\ndesign variables on the design objectives. So far, global sensitivity studies\nhave often been limited to design spaces with only quantitative (numerical)\ndesign variables. However, many engineering systems also contain, if not only,\nqualitative (categorical) design variables in addition to quantitative design\nvariables. In this paper, we integrate Latent Variable Gaussian Process (LVGP)\nwith Sobol' analysis to develop the first metamodel-based mixed-variable GSA\nmethod. Through numerical case studies, we validate and demonstrate the\neffectiveness of our proposed method for mixed-variable problems. Furthermore,\nwhile the proposed GSA method is general enough to benefit various engineering\ndesign applications, we integrate it with multi-objective Bayesian optimization\n(BO) to create a sensitivity-aware design framework in accelerating the Pareto\nfront design exploration for metal-organic framework (MOF) materials with\nmany-level combinatorial design spaces. Although MOFs are constructed only from\nqualitative variables that are notoriously difficult to design, our method can\nutilize sensitivity analysis to navigate the optimization in the many-level\nlarge combinatorial design space, greatly expediting the exploration of novel\nMOF candidates."}, "http://arxiv.org/abs/2003.04433": {"title": "Least Squares Estimation of a Quasiconvex Regression Function", "link": "http://arxiv.org/abs/2003.04433", "description": "We develop a new approach for the estimation of a multivariate function based\non the economic axioms of quasiconvexity (and monotonicity). On the\ncomputational side, we prove the existence of the quasiconvex constrained least\nsquares estimator (LSE) and provide a characterization of the function space to\ncompute the LSE via a mixed integer quadratic programme. On the theoretical\nside, we provide finite sample risk bounds for the LSE via a sharp oracle\ninequality. Our results allow for errors to depend on the covariates and to\nhave only two finite moments. We illustrate the superior performance of the LSE\nagainst some competing estimators via simulation. Finally, we use the LSE to\nestimate the production function for the Japanese plywood industry and the cost\nfunction for hospitals across the US."}, "http://arxiv.org/abs/2004.08318": {"title": "Causal Inference under Outcome-Based Sampling with Monotonicity Assumptions", "link": "http://arxiv.org/abs/2004.08318", "description": "We study causal inference under case-control and case-population sampling.\nSpecifically, we focus on the binary-outcome and binary-treatment case, where\nthe parameters of interest are causal relative and attributable risks defined\nvia the potential outcome framework. It is shown that strong ignorability is\nnot always as powerful as it is under random sampling and that certain\nmonotonicity assumptions yield comparable results in terms of sharp identified\nintervals. Specifically, the usual odds ratio is shown to be a sharp identified\nupper bound on causal relative risk under the monotone treatment response and\nmonotone treatment selection assumptions. We offer algorithms for inference on\nthe causal parameters that are aggregated over the true population distribution\nof the covariates. We show the usefulness of our approach by studying three\nempirical examples: the benefit of attending private school for entering a\nprestigious university in Pakistan; the relationship between staying in school\nand getting involved with drug-trafficking gangs in Brazil; and the link\nbetween physicians' hours and size of the group practice in the United States."}, "http://arxiv.org/abs/2008.10296": {"title": "Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison", "link": "http://arxiv.org/abs/2008.10296", "description": "Leave-one-out cross-validation (LOO-CV) is a popular method for comparing\nBayesian models based on their estimated predictive performance on new, unseen,\ndata. As leave-one-out cross-validation is based on finite observed data, there\nis uncertainty about the expected predictive performance on new data. By\nmodeling this uncertainty when comparing two models, we can compute the\nprobability that one model has a better predictive performance than the other.\nModeling this uncertainty well is not trivial, and for example, it is known\nthat the commonly used standard error estimate is often too small. We study the\nproperties of the Bayesian LOO-CV estimator and the related uncertainty\nestimates when comparing two models. We provide new results of the properties\nboth theoretically in the linear regression case and empirically for multiple\ndifferent models and discuss the challenges of modeling the uncertainty. We\nshow that problematic cases include: comparing models with similar predictions,\nmisspecified models, and small data. In these cases, there is a weak connection\nin the skewness of the individual leave-one-out terms and the distribution of\nthe error of the Bayesian LOO-CV estimator. We show that it is possible that\nthe problematic skewness of the error distribution, which occurs when the\nmodels make similar predictions, does not fade away when the data size grows to\ninfinity in certain situations. Based on the results, we also provide practical\nrecommendations for the users of Bayesian LOO-CV for model comparison."}, "http://arxiv.org/abs/2105.04981": {"title": "Uncovering patterns for adverse pregnancy outcomes with a Bayesian spatial model: Evidence from Philadelphia", "link": "http://arxiv.org/abs/2105.04981", "description": "We introduce a Bayesian conditional autoregressive model for analyzing\npatient-specific and neighborhood risks of stillbirth and preterm birth within\na city. Our fully Bayesian approach automatically learns the amount of spatial\nheterogeneity and spatial dependence between neighborhoods. Our model provides\nmeaningful inferences and uncertainty quantification for both covariate effects\nand neighborhood risk probabilities through their posterior distributions. We\napply our methodology to data from the city of Philadelphia. Using electronic\nhealth records (45,919 deliveries at hospitals within the University of\nPennsylvania Health System) and United States Census Bureau data from 363\ncensus tracts in Philadelphia, we find that both patient-level characteristics\n(e.g. self-identified race/ethnicity) and neighborhood-level characteristics\n(e.g. violent crime) are highly associated with patients' odds of stillbirth or\npreterm birth. Our neighborhood risk analysis further reveals that census\ntracts in West Philadelphia and North Philadelphia are at highest risk of these\noutcomes. Specifically, neighborhoods with higher rates of women in poverty or\non public assistance have greater neighborhood risk for these outcomes, while\nneighborhoods with higher rates of college-educated women or women in the labor\nforce have lower risk. Our findings could be useful for targeted individual and\nneighborhood interventions."}, "http://arxiv.org/abs/2107.07317": {"title": "Nonparametric Statistical Inference via Metric Distribution Function in Metric Spaces", "link": "http://arxiv.org/abs/2107.07317", "description": "Distribution function is essential in statistical inference, and connected\nwith samples to form a directed closed loop by the correspondence theorem in\nmeasure theory and the Glivenko-Cantelli and Donsker properties. This\nconnection creates a paradigm for statistical inference. However, existing\ndistribution functions are defined in Euclidean spaces and no longer convenient\nto use in rapidly evolving data objects of complex nature. It is imperative to\ndevelop the concept of distribution function in a more general space to meet\nemerging needs. Note that the linearity allows us to use hypercubes to define\nthe distribution function in a Euclidean space, but without the linearity in a\nmetric space, we must work with the metric to investigate the probability\nmeasure. We introduce a class of metric distribution functions through the\nmetric between random objects and a fixed location in metric spaces. We\novercome this challenging step by proving the correspondence theorem and the\nGlivenko-Cantelli theorem for metric distribution functions in metric spaces\nthat lie the foundation for conducting rational statistical inference for\nmetric space-valued data. Then, we develop homogeneity test and mutual\nindependence test for non-Euclidean random objects, and present comprehensive\nempirical evidence to support the performance of our proposed methods."}, "http://arxiv.org/abs/2108.07455": {"title": "Causal Inference with Noncompliance and Unknown Interference", "link": "http://arxiv.org/abs/2108.07455", "description": "We consider a causal inference model in which individuals interact in a\nsocial network and they may not comply with the assigned treatments. In\nparticular, we suppose that the form of network interference is unknown to\nresearchers. To estimate meaningful causal parameters in this situation, we\nintroduce a new concept of exposure mapping, which summarizes potentially\ncomplicated spillover effects into a fixed dimensional statistic of\ninstrumental variables. We investigate identification conditions for the\nintention-to-treat effects and the average treatment effects for compliers,\nwhile explicitly considering the possibility of misspecification of exposure\nmapping. Based on our identification results, we develop nonparametric\nestimation procedures via inverse probability weighting. Their asymptotic\nproperties, including consistency and asymptotic normality, are investigated\nusing an approximate neighborhood interference framework. For an empirical\nillustration, we apply our method to experimental data on the anti-conflict\nintervention school program. The proposed methods are readily available with\nthe companion R package latenetwork."}, "http://arxiv.org/abs/2109.03694": {"title": "Parameterizing and Simulating from Causal Models", "link": "http://arxiv.org/abs/2109.03694", "description": "Many statistical problems in causal inference involve a probability\ndistribution other than the one from which data are actually observed; as an\nadditional complication, the object of interest is often a marginal quantity of\nthis other probability distribution. This creates many practical complications\nfor statistical inference, even where the problem is non-parametrically\nidentified. In particular, it is difficult to perform likelihood-based\ninference, or even to simulate from the model in a general way.\n\nWe introduce the `frugal parameterization', which places the causal effect of\ninterest at its centre, and then builds the rest of the model around it. We do\nthis in a way that provides a recipe for constructing a regular, non-redundant\nparameterization using causal quantities of interest. In the case of discrete\nvariables we can use odds ratios to complete the parameterization, while in the\ncontinuous case copulas are the natural choice; other possibilities are also\ndiscussed.\n\nOur methods allow us to construct and simulate from models with\nparametrically specified causal distributions, and fit them using\nlikelihood-based methods, including fully Bayesian approaches. Our proposal\nincludes parameterizations for the average causal effect and effect of\ntreatment on the treated, as well as other causal quantities of interest."}, "http://arxiv.org/abs/2112.03872": {"title": "Nonparametric Treatment Effect Identification in School Choice", "link": "http://arxiv.org/abs/2112.03872", "description": "This paper studies nonparametric identification and estimation of causal\neffects in centralized school assignment. In many centralized assignment\nsettings, students are subjected to both lottery-driven variation and\nregression discontinuity (RD) driven variation. We characterize the full set of\nidentified atomic treatment effects (aTEs), defined as the conditional average\ntreatment effect between a pair of schools, given student characteristics.\nAtomic treatment effects are the building blocks of more aggregated notions of\ntreatment contrasts, and common approaches estimating aggregations of aTEs can\nmask important heterogeneity. In particular, many aggregations of aTEs put zero\nweight on aTEs driven by RD variation, and estimators of such aggregations put\nasymptotically vanishing weight on the RD-driven aTEs. We develop a diagnostic\ntool for empirically assessing the weight put on aTEs driven by RD variation.\nLastly, we provide estimators and accompanying asymptotic results for inference\non aggregations of RD-driven aTEs."}, "http://arxiv.org/abs/2202.09534": {"title": "Locally Adaptive Spatial Quantile Smoothing: Application to Monitoring Crime Density in Tokyo", "link": "http://arxiv.org/abs/2202.09534", "description": "Spatial trend estimation under potential heterogeneity is an important\nproblem to extract spatial characteristics and hazards such as criminal\nactivity. By focusing on quantiles, which provide substantial information on\ndistributions compared with commonly used summary statistics such as means, it\nis often useful to estimate not only the average trend but also the high (low)\nrisk trend additionally. In this paper, we propose a Bayesian quantile trend\nfiltering method to estimate the non-stationary trend of quantiles on graphs\nand apply it to crime data in Tokyo between 2013 and 2017. By modeling multiple\nobservation cases, we can estimate the potential heterogeneity of spatial crime\ntrends over multiple years in the application. To induce locally adaptive\nBayesian inference on trends, we introduce general shrinkage priors for graph\ndifferences. Introducing so-called shadow priors with multivariate distribution\nfor local scale parameters and mixture representation of the asymmetric Laplace\ndistribution, we provide a simple Gibbs sampling algorithm to generate\nposterior samples. The numerical performance of the proposed method is\ndemonstrated through simulation studies."}, "http://arxiv.org/abs/2203.16710": {"title": "Detecting Treatment Interference under the K-Nearest-Neighbors Interference Model", "link": "http://arxiv.org/abs/2203.16710", "description": "We propose a model of treatment interference where the response of a unit\ndepends only on its treatment status and the statuses of units within its\nK-neighborhood. Current methods for detecting interference include carefully\ndesigned randomized experiments and conditional randomization tests on a set of\nfocal units. We give guidance on how to choose focal units under this model of\ninterference. We then conduct a simulation study to evaluate the efficacy of\nexisting methods for detecting network interference. We show that this choice\nof focal units leads to powerful tests of treatment interference which\noutperform current experimental methods."}, "http://arxiv.org/abs/2206.00646": {"title": "Importance sampling for stochastic reaction-diffusion equations in the moderate deviation regime", "link": "http://arxiv.org/abs/2206.00646", "description": "We develop a provably efficient importance sampling scheme that estimates\nexit probabilities of solutions to small-noise stochastic reaction-diffusion\nequations from scaled neighborhoods of a stable equilibrium. The moderate\ndeviation scaling allows for a local approximation of the nonlinear dynamics by\ntheir linearized version. In addition, we identify a finite-dimensional\nsubspace where exits take place with high probability. Using stochastic control\nand variational methods we show that our scheme performs well both in the zero\nnoise limit and pre-asymptotically. Simulation studies for stochastically\nperturbed bistable dynamics illustrate the theoretical results."}, "http://arxiv.org/abs/2206.12084": {"title": "Functional Mixed Membership Models", "link": "http://arxiv.org/abs/2206.12084", "description": "Mixed membership models, or partial membership models, are a flexible\nunsupervised learning method that allows each observation to belong to multiple\nclusters. In this paper, we propose a Bayesian mixed membership model for\nfunctional data. By using the multivariate Karhunen-Lo\\`eve theorem, we are\nable to derive a scalable representation of Gaussian processes that maintains\ndata-driven learning of the covariance structure. Within this framework, we\nestablish conditional posterior consistency given a known feature allocation\nmatrix. Compared to previous work on mixed membership models, our proposal\nallows for increased modeling flexibility, with the benefit of a directly\ninterpretable mean and covariance structure. Our work is motivated by studies\nin functional brain imaging through electroencephalography (EEG) of children\nwith autism spectrum disorder (ASD). In this context, our work formalizes the\nclinical notion of \"spectrum\" in terms of feature membership proportions."}, "http://arxiv.org/abs/2208.07614": {"title": "Reweighting the RCT for generalization: finite sample error and variable selection", "link": "http://arxiv.org/abs/2208.07614", "description": "Randomized Controlled Trials (RCTs) may suffer from limited scope. In\nparticular, samples may be unrepresentative: some RCTs over- or under- sample\nindividuals with certain characteristics compared to the target population, for\nwhich one wants conclusions on treatment effectiveness. Re-weighting trial\nindividuals to match the target population can improve the treatment effect\nestimation. In this work, we establish the exact expressions of the bias and\nvariance of such reweighting procedures -- also called Inverse Propensity of\nSampling Weighting (IPSW) -- in presence of categorical covariates for any\nsample size. Such results allow us to compare the theoretical performance of\ndifferent versions of IPSW estimates. Besides, our results show how the\nperformance (bias, variance, and quadratic risk) of IPSW estimates depends on\nthe two sample sizes (RCT and target population). A by-product of our work is\nthe proof of consistency of IPSW estimates. Results also reveal that IPSW\nperformances are improved when the trial probability to be treated is estimated\n(rather than using its oracle counterpart). In addition, we study choice of\nvariables: how including covariates that are not necessary for identifiability\nof the causal effect may impact the asymptotic variance. Including covariates\nthat are shifted between the two samples but not treatment effect modifiers\nincreases the variance while non-shifted but treatment effect modifiers do not.\nWe illustrate all the takeaways in a didactic example, and on a semi-synthetic\nsimulation inspired from critical care medicine."}, "http://arxiv.org/abs/2209.15448": {"title": "Blessing from Human-AI Interaction: Super Reinforcement Learning in Confounded Environments", "link": "http://arxiv.org/abs/2209.15448", "description": "As AI becomes more prevalent throughout society, effective methods of\nintegrating humans and AI systems that leverage their respective strengths and\nmitigate risk have become an important priority. In this paper, we introduce\nthe paradigm of super reinforcement learning that takes advantage of Human-AI\ninteraction for data driven sequential decision making. This approach utilizes\nthe observed action, either from AI or humans, as input for achieving a\nstronger oracle in policy learning for the decision maker (humans or AI). In\nthe decision process with unmeasured confounding, the actions taken by past\nagents can offer valuable insights into undisclosed information. By including\nthis information for the policy search in a novel and legitimate manner, the\nproposed super reinforcement learning will yield a super-policy that is\nguaranteed to outperform both the standard optimal policy and the behavior one\n(e.g., past agents' actions). We call this stronger oracle a blessing from\nhuman-AI interaction. Furthermore, to address the issue of unmeasured\nconfounding in finding super-policies using the batch data, a number of\nnonparametric and causal identifications are established. Building upon on\nthese novel identification results, we develop several super-policy learning\nalgorithms and systematically study their theoretical properties such as\nfinite-sample regret guarantee. Finally, we illustrate the effectiveness of our\nproposal through extensive simulations and real-world applications."}, "http://arxiv.org/abs/2212.06906": {"title": "Flexible Regularized Estimation in High-Dimensional Mixed Membership Models", "link": "http://arxiv.org/abs/2212.06906", "description": "Mixed membership models are an extension of finite mixture models, where each\nobservation can partially belong to more than one mixture component. A\nprobabilistic framework for mixed membership models of high-dimensional\ncontinuous data is proposed with a focus on scalability and interpretability.\nThe novel probabilistic representation of mixed membership is based on convex\ncombinations of dependent multivariate Gaussian random vectors. In this\nsetting, scalability is ensured through approximations of a tensor covariance\nstructure through multivariate eigen-approximations with adaptive\nregularization imposed through shrinkage priors. Conditional weak posterior\nconsistency is established on an unconstrained model, allowing for a simple\nposterior sampling scheme while keeping many of the desired theoretical\nproperties of our model. The model is motivated by two biomedical case studies:\na case study on functional brain imaging of children with autism spectrum\ndisorder (ASD) and a case study on gene expression data from breast cancer\ntissue. These applications highlight how the typical assumption made in cluster\nanalysis, that each observation comes from one homogeneous subgroup, may often\nbe restrictive in several applications, leading to unnatural interpretations of\ndata features."}, "http://arxiv.org/abs/2301.09020": {"title": "On the Role of Volterra Integral Equations in Self-Consistent, Product-Limit, Inverse Probability of Censoring Weighted, and Redistribution-to-the-Right Estimators for the Survival Function", "link": "http://arxiv.org/abs/2301.09020", "description": "This paper reconsiders several results of historical and current importance\nto nonparametric estimation of the survival distribution for failure in the\npresence of right-censored observation times, demonstrating in particular how\nVolterra integral equations of the first kind help inter-connect the resulting\nestimators. The paper begins by considering Efron's self-consistency equation,\nintroduced in a seminal 1967 Berkeley symposium paper. Novel insights provided\nin the current work include the observations that (i) the self-consistency\nequation leads directly to an anticipating Volterra integral equation of the\nfirst kind whose solution is given by a product-limit estimator for the\ncensoring survival function; (ii) a definition used in this argument\nimmediately establishes the familiar product-limit estimator for the failure\nsurvival function; (iii) the usual Volterra integral equation for the\nproduct-limit estimator of the failure survival function leads to an immediate\nand simple proof that it can be represented as an inverse probability of\ncensoring weighted estimator (i.e., under appropriate conditions). Finally, we\nshow that the resulting inverse probability of censoring weighted estimators,\nattributed to a highly influential 1992 paper of Robins and Rotnitzky, were\nimplicitly introduced in Efron's 1967 paper in its development of the\nredistribution-to-the-right algorithm. All results developed herein allow for\nties between failure and/or censored observations."}, "http://arxiv.org/abs/2302.01576": {"title": "ResMem: Learn what you can and memorize the rest", "link": "http://arxiv.org/abs/2302.01576", "description": "The impressive generalization performance of modern neural networks is\nattributed in part to their ability to implicitly memorize complex training\npatterns. Inspired by this, we explore a novel mechanism to improve model\ngeneralization via explicit memorization. Specifically, we propose the\nresidual-memorization (ResMem) algorithm, a new method that augments an\nexisting prediction model (e.g. a neural network) by fitting the model's\nresiduals with a $k$-nearest neighbor based regressor. The final prediction is\nthen the sum of the original model and the fitted residual regressor. By\nconstruction, ResMem can explicitly memorize the training labels. Empirically,\nwe show that ResMem consistently improves the test set generalization of the\noriginal prediction model across various standard vision and natural language\nprocessing benchmarks. Theoretically, we formulate a stylized linear regression\nproblem and rigorously show that ResMem results in a more favorable test risk\nover the base predictor."}, "http://arxiv.org/abs/2303.05032": {"title": "Sensitivity analysis for principal ignorability violation in estimating complier and noncomplier average causal effects", "link": "http://arxiv.org/abs/2303.05032", "description": "An important strategy for identifying principal causal effects, which are\noften used in settings with noncompliance, is to invoke the principal\nignorability (PI) assumption. As PI is untestable, it is important to gauge how\nsensitive effect estimates are to its violation. We focus on this task for the\ncommon one-sided noncompliance setting where there are two principal strata,\ncompliers and noncompliers. Under PI, compliers and noncompliers share the same\noutcome-mean-given-covariates function under the control condition. For\nsensitivity analysis, we allow this function to differ between compliers and\nnoncompliers in several ways, indexed by an odds ratio, a generalized odds\nratio, a mean ratio, or a standardized mean difference sensitivity parameter.\nWe tailor sensitivity analysis techniques (with any sensitivity parameter\nchoice) to several types of PI-based main analysis methods, including outcome\nregression, influence function (IF) based and weighting methods. We illustrate\nthe proposed sensitivity analyses using several outcome types from the JOBS II\nstudy. This application estimates nuisance functions parametrically -- for\nsimplicity and accessibility. In addition, we establish rate conditions on\nnonparametric nuisance estimation for IF-based estimators to be asymptotically\nnormal -- with a view to inform nonparametric inference."}, "http://arxiv.org/abs/2304.13307": {"title": "A Statistical Interpretation of the Maximum Subarray Problem", "link": "http://arxiv.org/abs/2304.13307", "description": "Maximum subarray is a classical problem in computer science that given an\narray of numbers aims to find a contiguous subarray with the largest sum. We\nfocus on its use for a noisy statistical problem of localizing an interval with\na mean different from background. While a naive application of maximum subarray\nfails at this task, both a penalized and a constrained version can succeed. We\nshow that the penalized version can be derived for common exponential family\ndistributions, in a manner similar to the change-point detection literature,\nand we interpret the resulting optimal penalty value. The failure of the naive\nformulation is then explained by an analysis of the estimated interval\nboundaries. Experiments further quantify the effect of deviating from the\noptimal penalty. We also relate the penalized and constrained formulations and\nshow that the solutions to the former lie on the convex hull of the solutions\nto the latter."}, "http://arxiv.org/abs/2305.10637": {"title": "Conformalized matrix completion", "link": "http://arxiv.org/abs/2305.10637", "description": "Matrix completion aims to estimate missing entries in a data matrix, using\nthe assumption of a low-complexity structure (e.g., low rank) so that\nimputation is possible. While many effective estimation algorithms exist in the\nliterature, uncertainty quantification for this problem has proved to be\nchallenging, and existing methods are extremely sensitive to model\nmisspecification. In this work, we propose a distribution-free method for\npredictive inference in the matrix completion problem. Our method adapts the\nframework of conformal prediction, which provides confidence intervals with\nguaranteed distribution-free validity in the setting of regression, to the\nproblem of matrix completion. Our resulting method, conformalized matrix\ncompletion (cmc), offers provable predictive coverage regardless of the\naccuracy of the low-rank model. Empirical results on simulated and real data\ndemonstrate that cmc is robust to model misspecification while matching the\nperformance of existing model-based methods when the model is correct."}, "http://arxiv.org/abs/2305.15027": {"title": "A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods", "link": "http://arxiv.org/abs/2305.15027", "description": "We establish the first mathematically rigorous link between Bayesian,\nvariational Bayesian, and ensemble methods. A key step towards this it to\nreformulate the non-convex optimisation problem typically encountered in deep\nlearning as a convex optimisation in the space of probability measures. On a\ntechnical level, our contribution amounts to studying generalised variational\ninference through the lense of Wasserstein gradient flows. The result is a\nunified theory of various seemingly disconnected approaches that are commonly\nused for uncertainty quantification in deep learning -- including deep\nensembles and (variational) Bayesian methods. This offers a fresh perspective\non the reasons behind the success of deep ensembles over procedures based on\nparameterised variational inference, and allows the derivation of new\nensembling schemes with convergence guarantees. We showcase this by proposing a\nfamily of interacting deep ensembles with direct parallels to the interactions\nof particle systems in thermodynamics, and use our theory to prove the\nconvergence of these algorithms to a well-defined global minimiser on the space\nof probability measures."}, "http://arxiv.org/abs/2306.02584": {"title": "Synthetic Regressing Control Method", "link": "http://arxiv.org/abs/2306.02584", "description": "Estimating weights in the synthetic control method, typically resulting in\nsparse weights where only a few control units have non-zero weights, involves\nan optimization procedure that simultaneously selects and aligns control units\nto closely match the treated unit. However, this simultaneous selection and\nalignment of control units may lead to a loss of efficiency. Another concern\narising from the aforementioned procedure is its susceptibility to\nunder-fitting due to imperfect pre-treatment fit. It is not uncommon for the\nlinear combination, using nonnegative weights, of pre-treatment period outcomes\nfor the control units to inadequately approximate the pre-treatment outcomes\nfor the treated unit. To address both of these issues, this paper proposes a\nsimple and effective method called Synthetic Regressing Control (SRC). The SRC\nmethod begins by performing the univariate linear regression to appropriately\nalign the pre-treatment periods of the control units with the treated unit.\nSubsequently, a SRC estimator is obtained by synthesizing (taking a weighted\naverage) the fitted controls. To determine the weights in the synthesis\nprocedure, we propose an approach that utilizes a criterion of unbiased risk\nestimator. Theoretically, we show that the synthesis way is asymptotically\noptimal in the sense of achieving the lowest possible squared error. Extensive\nnumerical experiments highlight the advantages of the SRC method."}, "http://arxiv.org/abs/2308.05858": {"title": "Inconsistency and Acausality of Model Selection in Bayesian Inverse Problems", "link": "http://arxiv.org/abs/2308.05858", "description": "Bayesian inference paradigms are regarded as powerful tools for solution of\ninverse problems. However, when applied to inverse problems in physical\nsciences, Bayesian formulations suffer from a number of inconsistencies that\nare often overlooked. A well known, but mostly neglected, difficulty is\nconnected to the notion of conditional probability densities. Borel, and later\nKolmogorov's (1933/1956), found that the traditional definition of conditional\ndensities is incomplete: In different parameterizations it leads to different\nresults. We will show an example where two apparently correct procedures\napplied to the same problem lead to two widely different results. Another type\nof inconsistency involves violation of causality. This problem is found in\nmodel selection strategies in Bayesian inversion, such as Hierarchical Bayes\nand Trans-Dimensional Inversion where so-called hyperparameters are included as\nvariables to control either the number (or type) of unknowns, or the prior\nuncertainties on data or model parameters. For Hierarchical Bayes we\ndemonstrate that the calculated 'prior' distributions of data or model\nparameters are not prior-, but posterior information. In fact, the calculated\n'standard deviations' of the data are a measure of the inability of the forward\nfunction to model the data, rather than uncertainties of the data. For\ntrans-dimensional inverse problems we show that the so-called evidence is, in\nfact, not a measure of the success of fitting the data for the given choice (or\nnumber) of parameters, as often claimed. We also find that the notion of\nNatural Parsimony is ill-defined, because of its dependence on the parameter\nprior. Based on this study, we find that careful rethinking of Bayesian\ninversion practices is required, with special emphasis on ways of avoiding the\nBorel-Kolmogorov inconsistency, and on the way we interpret model selection\nresults."}, "http://arxiv.org/abs/2308.12470": {"title": "Scalable Estimation of Multinomial Response Models with Uncertain Consideration Sets", "link": "http://arxiv.org/abs/2308.12470", "description": "A standard assumption in the fitting of unordered multinomial response models\nfor $J$ mutually exclusive nominal categories, on cross-sectional or\nlongitudinal data, is that the responses arise from the same set of $J$\ncategories between subjects. However, when responses measure a choice made by\nthe subject, it is more appropriate to assume that the distribution of\nmultinomial responses is conditioned on a subject-specific consideration set,\nwhere this consideration set is drawn from the power set of $\\{1,2,\\ldots,J\\}$.\nBecause the cardinality of this power set is exponential in $J$, estimation is\ninfeasible in general. In this paper, we provide an approach to overcoming this\nproblem. A key step in the approach is a probability model over consideration\nsets, based on a general representation of probability distributions on\ncontingency tables, which results in mixtures of independent consideration\nmodels. Although the support of this distribution is exponentially large, the\nposterior distribution over consideration sets given parameters is typically\nsparse, and is easily sampled in an MCMC scheme. We show posterior consistency\nof the parameters of the conditional response model and the distribution of\nconsideration sets. The effectiveness of the methodology is documented in\nsimulated longitudinal data sets with $J=100$ categories and real data from the\ncereal market with $J=68$ brands."}, "http://arxiv.org/abs/2310.15266": {"title": "Causal progress with imperfect placebo treatments and outcomes", "link": "http://arxiv.org/abs/2310.15266", "description": "In the quest to make defensible causal claims from observational data, it is\nsometimes possible to leverage information from \"placebo treatments\" and\n\"placebo outcomes\" (or \"negative outcome controls\"). Existing approaches\nemploying such information focus largely on point identification and assume (i)\n\"perfect placebos\", meaning placebo treatments have precisely zero effect on\nthe outcome and the real treatment has precisely zero effect on a placebo\noutcome; and (ii) \"equiconfounding\", meaning that the treatment-outcome\nrelationship where one is a placebo suffers the same amount of confounding as\ndoes the real treatment-outcome relationship, on some scale. We instead\nconsider an omitted variable bias framework, in which users can postulate\nnon-zero effects of placebo treatment on real outcomes or of real treatments on\nplacebo outcomes, and the relative strengths of confounding suffered by a\nplacebo treatment/outcome compared to the true treatment-outcome relationship.\nOnce postulated, these assumptions identify or bound the linear estimates of\ntreatment effects. While applicable in many settings, one ubiquitous use-case\nfor this approach is to employ pre-treatment outcomes as (perfect) placebo\noutcomes. In this setting, the parallel trends assumption of\ndifference-in-difference is in fact a strict equiconfounding assumption on a\nparticular scale, which can be relaxed in our framework. Finally, we\ndemonstrate the use of our framework with two applications, employing an R\npackage that implements these approaches."}, "http://arxiv.org/abs/2310.15333": {"title": "Estimating Trustworthy and Safe Optimal Treatment Regimes", "link": "http://arxiv.org/abs/2310.15333", "description": "Recent statistical and reinforcement learning methods have significantly\nadvanced patient care strategies. However, these approaches face substantial\nchallenges in high-stakes contexts, including missing data, inherent\nstochasticity, and the critical requirements for interpretability and patient\nsafety. Our work operationalizes a safe and interpretable framework to identify\noptimal treatment regimes. This approach involves matching patients with\nsimilar medical and pharmacological characteristics, allowing us to construct\nan optimal policy via interpolation. We perform a comprehensive simulation\nstudy to demonstrate the framework's ability to identify optimal policies even\nin complex settings. Ultimately, we operationalize our approach to study\nregimes for treating seizures in critically ill patients. Our findings strongly\nsupport personalized treatment strategies based on a patient's medical history\nand pharmacological features. Notably, we identify that reducing medication\ndoses for patients with mild and brief seizure episodes while adopting\naggressive treatment for patients in intensive care unit experiencing intense\nseizures leads to more favorable outcomes."}, "http://arxiv.org/abs/2310.15459": {"title": "Strategies to mitigate bias from time recording errors in pharmacokinetic studies", "link": "http://arxiv.org/abs/2310.15459", "description": "Opportunistic pharmacokinetic (PK) studies have sparse and imbalanced\nclinical measurement data, and the impact of sample time errors is an important\nconcern when seeking accurate estimates of treatment response. We evaluated an\napproximate Bayesian model for individualized pharmacokinetics in the presence\nof time recording errors (TREs), considering both a short and long infusion\ndosing pattern. We found that the long infusion schedule generally had lower\nbias in estimates of the pharmacodynamic (PD) endpoint relative to the short\ninfusion schedule. We investigated three different design strategies for their\nability to mitigate the impact of TREs: (i) shifting blood draws taken during\nan active infusion to the post-infusion period, (ii) identifying the best next\nsample time by minimizing bias in the presence of TREs, and (iii) collecting\nadditional information on a subset of patients based on estimate uncertainty or\nquadrature-estimated variance in the presence of TREs. Generally, the proposed\nstrategies led to a decrease in bias of the PD estimate for the short infusion\nschedule, but had a negligible impact for the long infusion schedule. Dosing\nregimens with periods of high non-linearity may benefit from design\nmodifications, while more stable concentration-time profiles are generally more\nrobust to TREs with no design modifications."}, "http://arxiv.org/abs/2310.15497": {"title": "Generalized Box-Cox method to estimate sample mean and standard deviation for Meta-analysis", "link": "http://arxiv.org/abs/2310.15497", "description": "Meta-analysis is the aggregation of data from multiple studies to find\npatterns across a broad range relating to a particular subject. It is becoming\nincreasingly useful to apply meta-analysis to summarize these studies being\ndone across various fields. In meta-analysis, it is common to use the mean and\nstandard deviation from each study to compare for analysis. While many studies\nreported mean and standard deviation for their summary statistics, some report\nother values including the minimum, maximum, median, and first and third\nquantiles. Often, the quantiles and median are reported when the data is skewed\nand does not follow a normal distribution. In order to correctly summarize the\ndata and draw conclusions from multiple studies, it is necessary to estimate\nthe mean and standard deviation from each study, considering variation and\nskewness within each study. In past literature, methods have been proposed to\nestimate the mean and standard deviation, but do not consider negative values.\nData that include negative values are common and would increase the accuracy\nand impact of the me-ta-analysis. We propose a method that implements a\ngeneralized Box-Cox transformation to estimate the mean and standard deviation\naccounting for such negative values while maintaining similar accuracy."}, "http://arxiv.org/abs/2310.15877": {"title": "Regression analysis of multiplicative hazards model with time-dependent coefficient for sparse longitudinal covariates", "link": "http://arxiv.org/abs/2310.15877", "description": "We study the multiplicative hazards model with intermittently observed\nlongitudinal covariates and time-varying coefficients. For such models, the\nexisting {\\it ad hoc} approach, such as the last value carried forward, is\nbiased. We propose a kernel weighting approach to get an unbiased estimation of\nthe non-parametric coefficient function and establish asymptotic normality for\nany fixed time point. Furthermore, we construct the simultaneous confidence\nband to examine the overall magnitude of the variation. Simulation studies\nsupport our theoretical predictions and show favorable performance of the\nproposed method. A data set from cerebral infarction is used to illustrate our\nmethodology."}, "http://arxiv.org/abs/2310.15956": {"title": "Likelihood-Based Inference for Semi-Parametric Transformation Cure Models with Interval Censored Data", "link": "http://arxiv.org/abs/2310.15956", "description": "A simple yet effective way of modeling survival data with cure fraction is by\nconsidering Box-Cox transformation cure model (BCTM) that unifies mixture and\npromotion time cure models. In this article, we numerically study the\nstatistical properties of the BCTM when applied to interval censored data.\nTime-to-events associated with susceptible subjects are modeled through\nproportional hazards structure that allows for non-homogeneity across subjects,\nwhere the baseline hazard function is estimated by distribution-free piecewise\nlinear function with varied degrees of non-parametricity. Due to missing cured\nstatuses for right censored subjects, maximum likelihood estimates of model\nparameters are obtained by developing an expectation-maximization (EM)\nalgorithm. Under the EM framework, the conditional expectation of the complete\ndata log-likelihood function is maximized by considering all parameters\n(including the Box-Cox transformation parameter $\\alpha$) simultaneously, in\ncontrast to conventional profile-likelihood technique of estimating $\\alpha$.\nThe robustness and accuracy of the model and estimation method are established\nthrough a detailed simulation study under various parameter settings, and an\nanalysis of real-life data obtained from a smoking cessation study."}, "http://arxiv.org/abs/1901.04916": {"title": "Pairwise accelerated failure time regression models for infectious disease transmission in close-contact groups with external sources of infection", "link": "http://arxiv.org/abs/1901.04916", "description": "Many important questions in infectious disease epidemiology involve the\neffects of covariates (e.g., age or vaccination status) on infectiousness and\nsusceptibility, which can be measured in studies of transmission in households\nor other close-contact groups. Because the transmission of disease produces\ndependent outcomes, these questions are difficult or impossible to address\nusing standard regression models from biostatistics. Pairwise survival analysis\nhandles dependent outcomes by calculating likelihoods in terms of contact\ninterval distributions in ordered pairs of individuals. The contact interval in\nthe ordered pair ij is the time from the onset of infectiousness in i to\ninfectious contact from i to j, where an infectious contact is sufficient to\ninfect j if they are susceptible. Here, we introduce a pairwise accelerated\nfailure time regression model for infectious disease transmission that allows\nthe rate parameter of the contact interval distribution to depend on\ninfectiousness covariates for i, susceptibility covariates for j, and pairwise\ncovariates. This model can simultaneously handle internal infections (caused by\ntransmission between individuals under observation) and external infections\n(caused by environmental or community sources of infection). In a simulation\nstudy, we show that these models produce valid point and interval estimates of\nparameters governing the contact interval distributions. We also explore the\nrole of epidemiologic study design and the consequences of model\nmisspecification. We use this regression model to analyze household data from\nLos Angeles County during the 2009 influenza A (H1N1) pandemic, where we find\nthat the ability to account for external sources of infection is critical to\nestimating the effect of antiviral prophylaxis."}, "http://arxiv.org/abs/2003.06416": {"title": "VCBART: Bayesian trees for varying coefficients", "link": "http://arxiv.org/abs/2003.06416", "description": "The linear varying coefficient models posits a linear relationship between an\noutcome and covariates in which the covariate effects are modeled as functions\nof additional effect modifiers. Despite a long history of study and use in\nstatistics and econometrics, state-of-the-art varying coefficient modeling\nmethods cannot accommodate multivariate effect modifiers without imposing\nrestrictive functional form assumptions or involving computationally intensive\nhyperparameter tuning. In response, we introduce VCBART, which flexibly\nestimates the covariate effect in a varying coefficient model using Bayesian\nAdditive Regression Trees. With simple default settings, VCBART outperforms\nexisting varying coefficient methods in terms of covariate effect estimation,\nuncertainty quantification, and outcome prediction. We illustrate the utility\nof VCBART with two case studies: one examining how the association between\nlater-life cognition and measures of socioeconomic position vary with respect\nto age and socio-demographics and another estimating how temporal trends in\nurban crime vary at the neighborhood level. An R package implementing VCBART is\navailable at https://github.com/skdeshpande91/VCBART"}, "http://arxiv.org/abs/2204.05870": {"title": "How much of the past matters? Using dynamic survival models for the monitoring of potassium in heart failure patients using electronic health records", "link": "http://arxiv.org/abs/2204.05870", "description": "Statistical methods to study the association between a longitudinal biomarker\nand the risk of death are very relevant for the long-term care of subjects\naffected by chronic illnesses, such as potassium in heart failure patients.\nParticularly in the presence of comorbidities or pharmacological treatments,\nsudden crises can cause potassium to undergo very abrupt yet transient changes.\nIn the context of the monitoring of potassium, there is a need for a dynamic\nmodel that can be used in clinical practice to assess the risk of death related\nto an observed patient's potassium trajectory. We considered different dynamic\nsurvival approaches, starting from the simple approach considering the most\nrecent measurement, to the joint model. We then propose a novel method based on\nwavelet filtering and landmarking to retrieve the prognostic role of past\nshort-term potassium shifts. We argue that while taking into account past\ninformation is important, not all past information is equally informative.\nState-of-the-art dynamic survival models are prone to give more importance to\nthe mean long-term value of potassium. However, our findings suggest that it is\nessential to take into account also recent potassium instability to capture all\nthe relevant prognostic information. The data used comes from over 2000\nsubjects, with a total of over 80 000 repeated potassium measurements collected\nthrough Administrative Health Records and Outpatient and Inpatient Clinic\nE-charts. A novel dynamic survival approach is proposed in this work for the\nmonitoring of potassium in heart failure. The proposed wavelet landmark method\nshows promising results revealing the prognostic role of past short-term\nchanges, according to their different duration, and achieving higher\nperformances in predicting the survival probability of individuals."}, "http://arxiv.org/abs/2212.09494": {"title": "Optimal Treatment Regimes for Proximal Causal Learning", "link": "http://arxiv.org/abs/2212.09494", "description": "A common concern when a policymaker draws causal inferences from and makes\ndecisions based on observational data is that the measured covariates are\ninsufficiently rich to account for all sources of confounding, i.e., the\nstandard no confoundedness assumption fails to hold. The recently proposed\nproximal causal inference framework shows that proxy variables that abound in\nreal-life scenarios can be leveraged to identify causal effects and therefore\nfacilitate decision-making. Building upon this line of work, we propose a novel\noptimal individualized treatment regime based on so-called outcome and\ntreatment confounding bridges. We then show that the value function of this new\noptimal treatment regime is superior to that of existing ones in the\nliterature. Theoretical guarantees, including identification, superiority,\nexcess value bound, and consistency of the estimated regime, are established.\nFurthermore, we demonstrate the proposed optimal regime via numerical\nexperiments and a real data application."}, "http://arxiv.org/abs/2301.09016": {"title": "Inference for Two-stage Experiments under Covariate-Adaptive Randomization", "link": "http://arxiv.org/abs/2301.09016", "description": "This paper studies inference in two-stage randomized experiments under\ncovariate-adaptive randomization. In the initial stage of this experimental\ndesign, clusters (e.g., households, schools, or graph partitions) are\nstratified and randomly assigned to control or treatment groups based on\ncluster-level covariates. Subsequently, an independent second-stage design is\ncarried out, wherein units within each treated cluster are further stratified\nand randomly assigned to either control or treatment groups, based on\nindividual-level covariates. Under the homogeneous partial interference\nassumption, I establish conditions under which the proposed\ndifference-in-\"average of averages\" estimators are consistent and\nasymptotically normal for the corresponding average primary and spillover\neffects and develop consistent estimators of their asymptotic variances.\nCombining these results establishes the asymptotic validity of tests based on\nthese estimators. My findings suggest that ignoring covariate information in\nthe design stage can result in efficiency loss, and commonly used inference\nmethods that ignore or improperly use covariate information can lead to either\nconservative or invalid inference. Finally, I apply these results to studying\noptimal use of covariate information under covariate-adaptive randomization in\nlarge samples, and demonstrate that a specific generalized matched-pair design\nachieves minimum asymptotic variance for each proposed estimator. The practical\nrelevance of the theoretical results is illustrated through a simulation study\nand an empirical application."}, "http://arxiv.org/abs/2302.07294": {"title": "Derandomized Novelty Detection with FDR Control via Conformal E-values", "link": "http://arxiv.org/abs/2302.07294", "description": "Conformal inference provides a general distribution-free method to rigorously\ncalibrate the output of any machine learning algorithm for novelty detection.\nWhile this approach has many strengths, it has the limitation of being\nrandomized, in the sense that it may lead to different results when analyzing\ntwice the same data, and this can hinder the interpretation of any findings. We\npropose to make conformal inferences more stable by leveraging suitable\nconformal e-values instead of p-values to quantify statistical significance.\nThis solution allows the evidence gathered from multiple analyses of the same\ndata to be aggregated effectively while provably controlling the false\ndiscovery rate. Further, we show that the proposed method can reduce randomness\nwithout much loss of power compared to standard conformal inference, partly\nthanks to an innovative way of weighting conformal e-values based on additional\nside information carefully extracted from the same data. Simulations with\nsynthetic and real data confirm this solution can be effective at eliminating\nrandom noise in the inferences obtained with state-of-the-art alternative\ntechniques, sometimes also leading to higher power."}, "http://arxiv.org/abs/2304.02127": {"title": "A Bayesian Collocation Integral Method for Parameter Estimation in Ordinary Differential Equations", "link": "http://arxiv.org/abs/2304.02127", "description": "Inferring the parameters of ordinary differential equations (ODEs) from noisy\nobservations is an important problem in many scientific fields. Currently, most\nparameter estimation methods that bypass numerical integration tend to rely on\nbasis functions or Gaussian processes to approximate the ODE solution and its\nderivatives. Due to the sensitivity of the ODE solution to its derivatives,\nthese methods can be hindered by estimation error, especially when only sparse\ntime-course observations are available. We present a Bayesian collocation\nframework that operates on the integrated form of the ODEs and also avoids the\nexpensive use of numerical solvers. Our methodology has the capability to\nhandle general nonlinear ODE systems. We demonstrate the accuracy of the\nproposed method through simulation studies, where the estimated parameters and\nrecovered system trajectories are compared with other recent methods. A real\ndata example is also provided."}, "http://arxiv.org/abs/2307.00127": {"title": "Large-scale Bayesian Structure Learning for Gaussian Graphical Models using Marginal Pseudo-likelihood", "link": "http://arxiv.org/abs/2307.00127", "description": "Bayesian methods for learning Gaussian graphical models offer a robust\nframework that addresses model uncertainty and incorporates prior knowledge.\nDespite their theoretical strengths, the applicability of Bayesian methods is\noften constrained by computational needs, especially in modern contexts\ninvolving thousands of variables. To overcome this issue, we introduce two\nnovel Markov chain Monte Carlo (MCMC) search algorithms that have a\nsignificantly lower computational cost than leading Bayesian approaches. Our\nproposed MCMC-based search algorithms use the marginal pseudo-likelihood\napproach to bypass the complexities of computing intractable normalizing\nconstants and iterative precision matrix sampling. These algorithms can deliver\nreliable results in mere minutes on standard computers, even for large-scale\nproblems with one thousand variables. Furthermore, our proposed method is\ncapable of addressing model uncertainty by efficiently exploring the full\nposterior graph space. Our simulation study indicates that the proposed\nalgorithms, particularly for large-scale sparse graphs, outperform the leading\nBayesian approaches in terms of computational efficiency and precision. The\nimplementation supporting the new approach is available through the R package\nBDgraph."}, "http://arxiv.org/abs/2307.09302": {"title": "Conformal prediction under ambiguous ground truth", "link": "http://arxiv.org/abs/2307.09302", "description": "Conformal Prediction (CP) allows to perform rigorous uncertainty\nquantification by constructing a prediction set $C(X)$ satisfying $\\mathbb{P}(Y\n\\in C(X))\\geq 1-\\alpha$ for a user-chosen $\\alpha \\in [0,1]$ by relying on\ncalibration data $(X_1,Y_1),...,(X_n,Y_n)$ from $\\mathbb{P}=\\mathbb{P}^{X}\n\\otimes \\mathbb{P}^{Y|X}$. It is typically implicitly assumed that\n$\\mathbb{P}^{Y|X}$ is the \"true\" posterior label distribution. However, in many\nreal-world scenarios, the labels $Y_1,...,Y_n$ are obtained by aggregating\nexpert opinions using a voting procedure, resulting in a one-hot distribution\n$\\mathbb{P}_{vote}^{Y|X}$. For such ``voted'' labels, CP guarantees are thus\nw.r.t. $\\mathbb{P}_{vote}=\\mathbb{P}^X \\otimes \\mathbb{P}_{vote}^{Y|X}$ rather\nthan the true distribution $\\mathbb{P}$. In cases with unambiguous ground truth\nlabels, the distinction between $\\mathbb{P}_{vote}$ and $\\mathbb{P}$ is\nirrelevant. However, when experts do not agree because of ambiguous labels,\napproximating $\\mathbb{P}^{Y|X}$ with a one-hot distribution\n$\\mathbb{P}_{vote}^{Y|X}$ ignores this uncertainty. In this paper, we propose\nto leverage expert opinions to approximate $\\mathbb{P}^{Y|X}$ using a\nnon-degenerate distribution $\\mathbb{P}_{agg}^{Y|X}$. We develop Monte Carlo CP\nprocedures which provide guarantees w.r.t. $\\mathbb{P}_{agg}=\\mathbb{P}^X\n\\otimes \\mathbb{P}_{agg}^{Y|X}$ by sampling multiple synthetic pseudo-labels\nfrom $\\mathbb{P}_{agg}^{Y|X}$ for each calibration example $X_1,...,X_n$. In a\ncase study of skin condition classification with significant disagreement among\nexpert annotators, we show that applying CP w.r.t. $\\mathbb{P}_{vote}$\nunder-covers expert annotations: calibrated for $72\\%$ coverage, it falls short\nby on average $10\\%$; our Monte Carlo CP closes this gap both empirically and\ntheoretically."}, "http://arxiv.org/abs/2310.16203": {"title": "Multivariate Dynamic Mediation Analysis under a Reinforcement Learning Framework", "link": "http://arxiv.org/abs/2310.16203", "description": "Mediation analysis is an important analytic tool commonly used in a broad\nrange of scientific applications. In this article, we study the problem of\nmediation analysis when there are multivariate and conditionally dependent\nmediators, and when the variables are observed over multiple time points. The\nproblem is challenging, because the effect of a mediator involves not only the\npath from the treatment to this mediator itself at the current time point, but\nalso all possible paths pointed to this mediator from its upstream mediators,\nas well as the carryover effects from all previous time points. We propose a\nnovel multivariate dynamic mediation analysis approach. Drawing inspiration\nfrom the Markov decision process model that is frequently employed in\nreinforcement learning, we introduce a Markov mediation process paired with a\nsystem of time-varying linear structural equation models to formulate the\nproblem. We then formally define the individual mediation effect, built upon\nthe idea of simultaneous interventions and intervention calculus. We next\nderive the closed-form expression and propose an iterative estimation procedure\nunder the Markov mediation process model. We study both the asymptotic property\nand the empirical performance of the proposed estimator, and further illustrate\nour method with a mobile health application."}, "http://arxiv.org/abs/2310.16207": {"title": "Propensity score weighting plus an adjusted proportional hazards model does not equal doubly robust away from the null", "link": "http://arxiv.org/abs/2310.16207", "description": "Recently it has become common for applied works to combine commonly used\nsurvival analysis modeling methods, such as the multivariable Cox model, and\npropensity score weighting with the intention of forming a doubly robust\nestimator that is unbiased in large samples when either the Cox model or the\npropensity score model is correctly specified. This combination does not, in\ngeneral, produce a doubly robust estimator, even after regression\nstandardization, when there is truly a causal effect. We demonstrate via\nsimulation this lack of double robustness for the semiparametric Cox model, the\nWeibull proportional hazards model, and a simple proportional hazards flexible\nparametric model, with both the latter models fit via maximum likelihood. We\nprovide a novel proof that the combination of propensity score weighting and a\nproportional hazards survival model, fit either via full or partial likelihood,\nis consistent under the null of no causal effect of the exposure on the outcome\nunder particular censoring mechanisms if either the propensity score or the\noutcome model is correctly specified and contains all confounders. Given our\nresults suggesting that double robustness only exists under the null, we\noutline two simple alternative estimators that are doubly robust for the\nsurvival difference at a given time point (in the above sense), provided the\ncensoring mechanism can be correctly modeled, and one doubly robust method of\nestimation for the full survival curve. We provide R code to use these\nestimators for estimation and inference in the supplementary materials."}, "http://arxiv.org/abs/2310.16213": {"title": "Bayes factor functions", "link": "http://arxiv.org/abs/2310.16213", "description": "We describe Bayes factors functions based on z, t, $\\chi^2$, and F statistics\nand the prior distributions used to define alternative hypotheses. The\nnon-local alternative prior distributions are centered on standardized effects,\nwhich index the Bayes factor function. The prior densities include a dispersion\nparameter that models the variation of effect sizes across replicated\nexperiments. We examine the convergence rates of Bayes factor functions under\ntrue null and true alternative hypotheses. Several examples illustrate the\napplication of the Bayes factor functions to replicated experimental designs\nand compare the conclusions from these analyses to other default Bayes factor\nmethods."}, "http://arxiv.org/abs/2310.16256": {"title": "A Causal Disentangled Multi-Granularity Graph Classification Method", "link": "http://arxiv.org/abs/2310.16256", "description": "Graph data widely exists in real life, with large amounts of data and complex\nstructures. It is necessary to map graph data to low-dimensional embedding.\nGraph classification, a critical graph task, mainly relies on identifying the\nimportant substructures within the graph. At present, some graph classification\nmethods do not combine the multi-granularity characteristics of graph data.\nThis lack of granularity distinction in modeling leads to a conflation of key\ninformation and false correlations within the model. So, achieving the desired\ngoal of a credible and interpretable model becomes challenging. This paper\nproposes a causal disentangled multi-granularity graph representation learning\nmethod (CDM-GNN) to solve this challenge. The CDM-GNN model disentangles the\nimportant substructures and bias parts within the graph from a\nmulti-granularity perspective. The disentanglement of the CDM-GNN model reveals\nimportant and bias parts, forming the foundation for its classification task,\nspecifically, model interpretations. The CDM-GNN model exhibits strong\nclassification performance and generates explanatory outcomes aligning with\nhuman cognitive patterns. In order to verify the effectiveness of the model,\nthis paper compares the three real-world datasets MUTAG, PTC, and IMDM-M. Six\nstate-of-the-art models, namely GCN, GAT, Top-k, ASAPool, SUGAR, and SAT are\nemployed for comparison purposes. Additionally, a qualitative analysis of the\ninterpretation results is conducted."}, "http://arxiv.org/abs/2310.16260": {"title": "Private Estimation and Inference in High-Dimensional Regression with FDR Control", "link": "http://arxiv.org/abs/2310.16260", "description": "This paper presents novel methodologies for conducting practical\ndifferentially private (DP) estimation and inference in high-dimensional linear\nregression. We start by proposing a differentially private Bayesian Information\nCriterion (BIC) for selecting the unknown sparsity parameter in DP-Lasso,\neliminating the need for prior knowledge of model sparsity, a requisite in the\nexisting literature. Then we propose a differentially private debiased LASSO\nalgorithm that enables privacy-preserving inference on regression parameters.\nOur proposed method enables accurate and private inference on the regression\nparameters by leveraging the inherent sparsity of high-dimensional linear\nregression models. Additionally, we address the issue of multiple testing in\nhigh-dimensional linear regression by introducing a differentially private\nmultiple testing procedure that controls the false discovery rate (FDR). This\nallows for accurate and privacy-preserving identification of significant\npredictors in the regression model. Through extensive simulations and real data\nanalysis, we demonstrate the efficacy of our proposed methods in conducting\ninference for high-dimensional linear models while safeguarding privacy and\ncontrolling the FDR."}, "http://arxiv.org/abs/2310.16284": {"title": "Bayesian Image Mediation Analysis", "link": "http://arxiv.org/abs/2310.16284", "description": "Mediation analysis aims to separate the indirect effect through mediators\nfrom the direct effect of the exposure on the outcome. It is challenging to\nperform mediation analysis with neuroimaging data which involves high\ndimensionality, complex spatial correlations, sparse activation patterns and\nrelatively low signal-to-noise ratio. To address these issues, we develop a new\nspatially varying coefficient structural equation model for Bayesian Image\nMediation Analysis (BIMA). We define spatially varying mediation effects within\nthe potential outcome framework, employing the soft-thresholded Gaussian\nprocess prior for functional parameters. We establish the posterior consistency\nfor spatially varying mediation effects along with selection consistency on\nimportant regions that contribute to the mediation effects. We develop an\nefficient posterior computation algorithm scalable to analysis of large-scale\nimaging data. Through extensive simulations, we show that BIMA can improve the\nestimation accuracy and computational efficiency for high-dimensional mediation\nanalysis over the existing methods. We apply BIMA to analyze the behavioral and\nfMRI data in the Adolescent Brain Cognitive Development (ABCD) study with a\nfocus on inferring the mediation effects of the parental education level on the\nchildren's general cognitive ability that are mediated through the working\nmemory brain activities."}, "http://arxiv.org/abs/2310.16290": {"title": "Fair Adaptive Experiments", "link": "http://arxiv.org/abs/2310.16290", "description": "Randomized experiments have been the gold standard for assessing the\neffectiveness of a treatment or policy. The classical complete randomization\napproach assigns treatments based on a prespecified probability and may lead to\ninefficient use of data. Adaptive experiments improve upon complete\nrandomization by sequentially learning and updating treatment assignment\nprobabilities. However, their application can also raise fairness and equity\nconcerns, as assignment probabilities may vary drastically across groups of\nparticipants. Furthermore, when treatment is expected to be extremely\nbeneficial to certain groups of participants, it is more appropriate to expose\nmany of these participants to favorable treatment. In response to these\nchallenges, we propose a fair adaptive experiment strategy that simultaneously\nenhances data use efficiency, achieves an envy-free treatment assignment\nguarantee, and improves the overall welfare of participants. An important\nfeature of our proposed strategy is that we do not impose parametric modeling\nassumptions on the outcome variables, making it more versatile and applicable\nto a wider array of applications. Through our theoretical investigation, we\ncharacterize the convergence rate of the estimated treatment effects and the\nassociated standard deviations at the group level and further prove that our\nadaptive treatment assignment algorithm, despite not having a closed-form\nexpression, approaches the optimal allocation rule asymptotically. Our proof\nstrategy takes into account the fact that the allocation decisions in our\ndesign depend on sequentially accumulated data, which poses a significant\nchallenge in characterizing the properties and conducting statistical inference\nof our method. We further provide simulation evidence to showcase the\nperformance of our fair adaptive experiment strategy."}, "http://arxiv.org/abs/2310.16294": {"title": "Producer-Side Experiments Based on Counterfactual Interleaving Designs for Online Recommender Systems", "link": "http://arxiv.org/abs/2310.16294", "description": "Recommender systems have become an integral part of online platforms,\nproviding personalized suggestions for purchasing items, consuming contents,\nand connecting with individuals. An online recommender system consists of two\nsides of components: the producer side comprises product sellers, content\ncreators, or service providers, etc., and the consumer side includes buyers,\nviewers, or guests, etc. To optimize an online recommender system, A/B tests\nserve as the golden standard for comparing different ranking models and\nevaluating their impact on both the consumers and producers. While\nconsumer-side experiments are relatively straightforward to design and commonly\nused to gauge the impact of ranking changes on the behavior of consumers\n(buyers, viewers, etc.), designing producer-side experiments presents a\nconsiderable challenge because producer items in the treatment and control\ngroups need to be ranked by different models and then merged into a single\nranking for the recommender to show to each consumer. In this paper, we review\nissues with the existing methods, propose new design principles for\nproducer-side experiments, and develop a rigorous solution based on\ncounterfactual interleaving designs for accurately measuring the effects of\nranking changes on the producers (sellers, creators, etc.)."}, "http://arxiv.org/abs/2310.16466": {"title": "Learning Continuous Network Emerging Dynamics from Scarce Observations via Data-Adaptive Stochastic Processes", "link": "http://arxiv.org/abs/2310.16466", "description": "Learning network dynamics from the empirical structure and spatio-temporal\nobservation data is crucial to revealing the interaction mechanisms of complex\nnetworks in a wide range of domains. However, most existing methods only aim at\nlearning network dynamic behaviors generated by a specific ordinary\ndifferential equation instance, resulting in ineffectiveness for new ones, and\ngenerally require dense observations. The observed data, especially from\nnetwork emerging dynamics, are usually difficult to obtain, which brings\ntrouble to model learning. Therefore, how to learn accurate network dynamics\nwith sparse, irregularly-sampled, partial, and noisy observations remains a\nfundamental challenge. We introduce Neural ODE Processes for Network Dynamics\n(NDP4ND), a new class of stochastic processes governed by stochastic\ndata-adaptive network dynamics, to overcome the challenge and learn continuous\nnetwork dynamics from scarce observations. Intensive experiments conducted on\nvarious network dynamics in ecological population evolution, phototaxis\nmovement, brain activity, epidemic spreading, and real-world empirical systems,\ndemonstrate that the proposed method has excellent data adaptability and\ncomputational efficiency, and can adapt to unseen network emerging dynamics,\nproducing accurate interpolation and extrapolation with reducing the ratio of\nrequired observation data to only about 6\\% and improving the learning speed\nfor new dynamics by three orders of magnitude."}, "http://arxiv.org/abs/2310.16489": {"title": "Latent event history models for quasi-reaction systems", "link": "http://arxiv.org/abs/2310.16489", "description": "Various processes can be modelled as quasi-reaction systems of stochastic\ndifferential equations, such as cell differentiation and disease spreading.\nSince the underlying data of particle interactions, such as reactions between\nproteins or contacts between people, are typically unobserved, statistical\ninference of the parameters driving these systems is developed from\nconcentration data measuring each unit in the system over time. While observing\nthe continuous time process at a time scale as fine as possible should in\ntheory help with parameter estimation, the existing Local Linear Approximation\n(LLA) methods fail in this case, due to numerical instability caused by small\nchanges of the system at successive time points. On the other hand, one may be\nable to reconstruct the underlying unobserved interactions from the observed\ncount data. Motivated by this, we first formalise the latent event history\nmodel underlying the observed count process. We then propose a computationally\nefficient Expectation-Maximation algorithm for parameter estimation, with an\nextended Kalman filtering procedure for the prediction of the latent states. A\nsimulation study shows the performance of the proposed method and highlights\nthe settings where it is particularly advantageous compared to the existing LLA\napproaches. Finally, we present an illustration of the methodology on the\nspreading of the COVID-19 pandemic in Italy."}, "http://arxiv.org/abs/2310.16502": {"title": "Assessing the overall and partial causal well-specification of nonlinear additive noise models", "link": "http://arxiv.org/abs/2310.16502", "description": "We propose a method to detect model misspecifications in nonlinear causal\nadditive and potentially heteroscedastic noise models. We aim to identify\npredictor variables for which we can infer the causal effect even in cases of\nsuch misspecification. We develop a general framework based on knowledge of the\nmultivariate observational data distribution and we then propose an algorithm\nfor finite sample data, discuss its asymptotic properties, and illustrate its\nperformance on simulated and real data."}, "http://arxiv.org/abs/2310.16600": {"title": "Balancing central and marginal rejection when combining independent significance tests", "link": "http://arxiv.org/abs/2310.16600", "description": "A common approach to evaluating the significance of a collection of\n$p$-values combines them with a pooling function, in particular when the\noriginal data are not available. These pooled $p$-values convert a sample of\n$p$-values into a single number which behaves like a univariate $p$-value. To\nclarify discussion of these functions, a telescoping series of alternative\nhypotheses are introduced that communicate the strength and prevalence of\nnon-null evidence in the $p$-values before general pooling formulae are\ndiscussed. A pattern noticed in the UMP pooled $p$-value for a particular\nalternative motivates the definition and discussion of central and marginal\nrejection levels at $\\alpha$. It is proven that central rejection is always\ngreater than or equal to marginal rejection, motivating a quotient to measure\nthe balance between the two for pooled $p$-values. A combining function based\non the $\\chi^2_{\\kappa}$ quantile transformation is proposed to control this\nquotient and shown to be robust to mis-specified parameters relative to the\nUMP. Different powers for different parameter settings motivate a map of\nplausible alternatives based on where this pooled $p$-value is minimized."}, "http://arxiv.org/abs/2310.16626": {"title": "Scalable Causal Structure Learning via Amortized Conditional Independence Testing", "link": "http://arxiv.org/abs/2310.16626", "description": "Controlling false positives (Type I errors) through statistical hypothesis\ntesting is a foundation of modern scientific data analysis. Existing causal\nstructure discovery algorithms either do not provide Type I error control or\ncannot scale to the size of modern scientific datasets. We consider a variant\nof the causal discovery problem with two sets of nodes, where the only edges of\ninterest form a bipartite causal subgraph between the sets. We develop Scalable\nCausal Structure Learning (SCSL), a method for causal structure discovery on\nbipartite subgraphs that provides Type I error control. SCSL recasts the\ndiscovery problem as a simultaneous hypothesis testing problem and uses\ndiscrete optimization over the set of possible confounders to obtain an upper\nbound on the test statistic for each edge. Semi-synthetic simulations\ndemonstrate that SCSL scales to handle graphs with hundreds of nodes while\nmaintaining error control and good power. We demonstrate the practical\napplicability of the method by applying it to a cancer dataset to reveal\nconnections between somatic gene mutations and metastases to different tissues."}, "http://arxiv.org/abs/2310.16638": {"title": "Covariate Shift Adaptation Robust to Density-Ratio Estimation", "link": "http://arxiv.org/abs/2310.16638", "description": "Consider a scenario where we have access to train data with both covariates\nand outcomes while test data only contains covariates. In this scenario, our\nprimary aim is to predict the missing outcomes of the test data. With this\nobjective in mind, we train parametric regression models under a covariate\nshift, where covariate distributions are different between the train and test\ndata. For this problem, existing studies have proposed covariate shift\nadaptation via importance weighting using the density ratio. This approach\naverages the train data losses, each weighted by an estimated ratio of the\ncovariate densities between the train and test data, to approximate the\ntest-data risk. Although it allows us to obtain a test-data risk minimizer, its\nperformance heavily relies on the accuracy of the density ratio estimation.\nMoreover, even if the density ratio can be consistently estimated, the\nestimation errors of the density ratio also yield bias in the estimators of the\nregression model's parameters of interest. To mitigate these challenges, we\nintroduce a doubly robust estimator for covariate shift adaptation via\nimportance weighting, which incorporates an additional estimator for the\nregression function. Leveraging double machine learning techniques, our\nestimator reduces the bias arising from the density ratio estimation errors. We\ndemonstrate the asymptotic distribution of the regression parameter estimator.\nNotably, our estimator remains consistent if either the density ratio estimator\nor the regression function is consistent, showcasing its robustness against\npotential errors in density ratio estimation. Finally, we confirm the soundness\nof our proposed method via simulation studies."}, "http://arxiv.org/abs/2310.16650": {"title": "Data-integration with pseudoweights and survey-calibration: application to developing US-representative lung cancer risk models for use in screening", "link": "http://arxiv.org/abs/2310.16650", "description": "Accurate cancer risk estimation is crucial to clinical decision-making, such\nas identifying high-risk people for screening. However, most existing cancer\nrisk models incorporate data from epidemiologic studies, which usually cannot\nrepresent the target population. While population-based health surveys are\nideal for making inference to the target population, they typically do not\ncollect time-to-cancer incidence data. Instead, time-to-cancer specific\nmortality is often readily available on surveys via linkage to vital\nstatistics. We develop calibrated pseudoweighting methods that integrate\nindividual-level data from a cohort and a survey, and summary statistics of\ncancer incidence from national cancer registries. By leveraging\nindividual-level cancer mortality data in the survey, the proposed methods\nimpute time-to-cancer incidence for survey sample individuals and use survey\ncalibration with auxiliary variables of influence functions generated from Cox\nregression to improve robustness and efficiency of the inverse-propensity\npseudoweighting method in estimating pure risks. We develop a lung cancer\nincidence pure risk model from the Prostate, Lung, Colorectal, and Ovarian\n(PLCO) Cancer Screening Trial using our proposed methods by integrating data\nfrom the National Health Interview Survey (NHIS) and cancer registries."}, "http://arxiv.org/abs/2310.16653": {"title": "Adaptive importance sampling for heavy-tailed distributions via $\\alpha$-divergence minimization", "link": "http://arxiv.org/abs/2310.16653", "description": "Adaptive importance sampling (AIS) algorithms are widely used to approximate\nexpectations with respect to complicated target probability distributions. When\nthe target has heavy tails, existing AIS algorithms can provide inconsistent\nestimators or exhibit slow convergence, as they often neglect the target's tail\nbehaviour. To avoid this pitfall, we propose an AIS algorithm that approximates\nthe target by Student-t proposal distributions. We adapt location and scale\nparameters by matching the escort moments - which are defined even for\nheavy-tailed distributions - of the target and the proposal. These updates\nminimize the $\\alpha$-divergence between the target and the proposal, thereby\nconnecting with variational inference. We then show that the\n$\\alpha$-divergence can be approximated by a generalized notion of effective\nsample size and leverage this new perspective to adapt the tail parameter with\nBayesian optimization. We demonstrate the efficacy of our approach through\napplications to synthetic targets and a Bayesian Student-t regression task on a\nreal example with clinical trial data."}, "http://arxiv.org/abs/2310.16690": {"title": "Dynamic treatment effect phenotyping through functional survival analysis", "link": "http://arxiv.org/abs/2310.16690", "description": "In recent years, research interest in personalised treatments has been\ngrowing. However, treatment effect heterogeneity and possibly time-varying\ntreatment effects are still often overlooked in clinical studies. Statistical\ntools are needed for the identification of treatment response patterns, taking\ninto account that treatment response is not constant over time. We aim to\nprovide an innovative method to obtain dynamic treatment effect phenotypes on a\ntime-to-event outcome, conditioned on a set of relevant effect modifiers. The\nproposed method does not require the assumption of proportional hazards for the\ntreatment effect, which is rarely realistic. We propose a spline-based survival\nneural network, inspired by the Royston-Parmar survival model, to estimate\ntime-varying conditional treatment effects. We then exploit the functional\nnature of the resulting estimates to apply a functional clustering of the\ntreatment effect curves in order to identify different patterns of treatment\neffects. The application that motivated this work is the discontinuation of\ntreatment with Mineralocorticoid receptor Antagonists (MRAs) in patients with\nheart failure, where there is no clear evidence as to which patients it is the\nsafest choice to discontinue treatment and, conversely, when it leads to a\nhigher risk of adverse events. The data come from an electronic health record\ndatabase. A simulation study was performed to assess the performance of the\nspline-based neural network and the stability of the treatment response\nphenotyping procedure. We provide a novel method to inform individualized\nmedical decisions by characterising subject-specific treatment responses over\ntime."}, "http://arxiv.org/abs/2310.16698": {"title": "Causal Discovery with Generalized Linear Models through Peeling Algorithms", "link": "http://arxiv.org/abs/2310.16698", "description": "This article presents a novel method for causal discovery with generalized\nstructural equation models suited for analyzing diverse types of outcomes,\nincluding discrete, continuous, and mixed data. Causal discovery often faces\nchallenges due to unmeasured confounders that hinder the identification of\ncausal relationships. The proposed approach addresses this issue by developing\ntwo peeling algorithms (bottom-up and top-down) to ascertain causal\nrelationships and valid instruments. This approach first reconstructs a\nsuper-graph to represent ancestral relationships between variables, using a\npeeling algorithm based on nodewise GLM regressions that exploit relationships\nbetween primary and instrumental variables. Then, it estimates parent-child\neffects from the ancestral relationships using another peeling algorithm while\ndeconfounding a child's model with information borrowed from its parents'\nmodels. The article offers a theoretical analysis of the proposed approach,\nwhich establishes conditions for model identifiability and provides statistical\nguarantees for accurately discovering parent-child relationships via the\npeeling algorithms. Furthermore, the article presents numerical experiments\nshowcasing the effectiveness of our approach in comparison to state-of-the-art\nstructure learning methods without confounders. Lastly, it demonstrates an\napplication to Alzheimer's disease (AD), highlighting the utility of the method\nin constructing gene-to-gene and gene-to-disease regulatory networks involving\nSingle Nucleotide Polymorphisms (SNPs) for healthy and AD subjects."}, "http://arxiv.org/abs/2310.16813": {"title": "Improving the Aggregation and Evaluation of NBA Mock Drafts", "link": "http://arxiv.org/abs/2310.16813", "description": "Many enthusiasts and experts publish forecasts of the order players are\ndrafted into professional sports leagues, known as mock drafts. Using a novel\ndataset of mock drafts for the National Basketball Association (NBA), we\nanalyze authors' mock draft accuracy over time and ask how we can reasonably\nuse information from multiple authors. To measure how accurate mock drafts are,\nwe assume that both mock drafts and the actual draft are ranked lists, and we\npropose that rank-biased distance (RBD) of Webber et al. (2010) is the\nappropriate error metric for mock draft accuracy. This is because RBD allows\nmock drafts to have a different length than the actual draft, accounts for\nplayers not appearing in both lists, and weights errors early in the draft more\nthan errors later on. We validate that mock drafts, as expected, improve in\naccuracy over the course of a season, and that accuracy of the mock drafts\nproduced right before their drafts is fairly stable across seasons. To be able\nto combine information from multiple mock drafts into a single consensus mock\ndraft, we also propose a ranked-list combination method based on the ideas of\nranked-choice voting. We show that our method provides improved forecasts over\nthe standard Borda count combination method used for most similar analyses in\nsports, and that either combination method provides a more accurate forecast\nover time than any single author."}, "http://arxiv.org/abs/2310.16819": {"title": "CATE Lasso: Conditional Average Treatment Effect Estimation with High-Dimensional Linear Regression", "link": "http://arxiv.org/abs/2310.16819", "description": "In causal inference about two treatments, Conditional Average Treatment\nEffects (CATEs) play an important role as a quantity representing an\nindividualized causal effect, defined as a difference between the expected\noutcomes of the two treatments conditioned on covariates. This study assumes\ntwo linear regression models between a potential outcome and covariates of the\ntwo treatments and defines CATEs as a difference between the linear regression\nmodels. Then, we propose a method for consistently estimating CATEs even under\nhigh-dimensional and non-sparse parameters. In our study, we demonstrate that\ndesirable theoretical properties, such as consistency, remain attainable even\nwithout assuming sparsity explicitly if we assume a weaker assumption called\nimplicit sparsity originating from the definition of CATEs. In this assumption,\nwe suppose that parameters of linear models in potential outcomes can be\ndivided into treatment-specific and common parameters, where the\ntreatment-specific parameters take difference values between each linear\nregression model, while the common parameters remain identical. Thus, in a\ndifference between two linear regression models, the common parameters\ndisappear, leaving only differences in the treatment-specific parameters.\nConsequently, the non-zero parameters in CATEs correspond to the differences in\nthe treatment-specific parameters. Leveraging this assumption, we develop a\nLasso regression method specialized for CATE estimation and present that the\nestimator is consistent. Finally, we confirm the soundness of the proposed\nmethod by simulation studies."}, "http://arxiv.org/abs/2310.16824": {"title": "Parametric model for post-processing visibility ensemble forecasts", "link": "http://arxiv.org/abs/2310.16824", "description": "Despite the continuous development of the different operational ensemble\nprediction systems over the past decades, ensemble forecasts still might suffer\nfrom lack of calibration and/or display systematic bias, thus require some\npost-processing to improve their forecast skill. Here we focus on visibility,\nwhich quantity plays a crucial role e.g. in aviation and road safety or in ship\nnavigation, and propose a parametric model where the predictive distribution is\na mixture of a gamma and a truncated normal distribution, both right censored\nat the maximal reported visibility value. The new model is evaluated in two\ncase studies based on visibility ensemble forecasts of the European Centre for\nMedium-Range Weather Forecasts covering two distinct domains in Central and\nWestern Europe and two different time periods. The results of the case studies\nindicate that climatology is substantially superior to the raw ensemble;\nnevertheless, the forecast skill can be further improved by post-processing, at\nleast for short lead times. Moreover, the proposed mixture model consistently\noutperforms the Bayesian model averaging approach used as reference\npost-processing technique."}, "http://arxiv.org/abs/2109.09339": {"title": "Improving the accuracy of estimating indexes in contingency tables using Bayesian estimators", "link": "http://arxiv.org/abs/2109.09339", "description": "In contingency table analysis, one is interested in testing whether a model\nof interest (e.g., the independent or symmetry model) holds using\ngoodness-of-fit tests. When the null hypothesis where the model is true is\nrejected, the interest turns to the degree to which the probability structure\nof the contingency table deviates from the model. Many indexes have been\nstudied to measure the degree of the departure, such as the Yule coefficient\nand Cram\\'er coefficient for the independence model, and Tomizawa's symmetry\nindex for the symmetry model. The inference of these indexes is performed using\nsample proportions, which are estimates of cell probabilities, but it is\nwell-known that the bias and mean square error (MSE) values become large\nwithout a sufficient number of samples. To address the problem, this study\nproposes a new estimator for indexes using Bayesian estimators of cell\nprobabilities. Assuming the Dirichlet distribution for the prior of cell\nprobabilities, we asymptotically evaluate the value of MSE when plugging the\nposterior means of cell probabilities into the index, and propose an estimator\nof the index using the Dirichlet hyperparameter that minimizes the value.\nNumerical experiments show that when the number of samples per cell is small,\nthe proposed method has smaller values of bias and MSE than other methods of\ncorrecting estimation accuracy. We also show that the values of bias and MSE\nare smaller than those obtained by using the uniform and Jeffreys priors."}, "http://arxiv.org/abs/2110.01031": {"title": "A general framework for formulating structured variable selection", "link": "http://arxiv.org/abs/2110.01031", "description": "In variable selection, a selection rule that prescribes the permissible sets\nof selected variables (called a \"selection dictionary\") is desirable due to the\ninherent structural constraints among the candidate variables. Such selection\nrules can be complex in real-world data analyses, and failing to incorporate\nsuch restrictions could not only compromise the interpretability of the model\nbut also lead to decreased prediction accuracy. However, no general framework\nhas been proposed to formalize selection rules and their applications, which\nposes a significant challenge for practitioners seeking to integrate these\nrules into their analyses. In this work, we establish a framework for\nstructured variable selection that can incorporate universal structural\nconstraints. We develop a mathematical language for constructing arbitrary\nselection rules, where the selection dictionary is formally defined. We\ndemonstrate that all selection rules can be expressed as combinations of\noperations on constructs, facilitating the identification of the corresponding\nselection dictionary. Once this selection dictionary is derived, practitioners\ncan apply their own user-defined criteria to select the optimal model.\nAdditionally, our framework enhances existing penalized regression methods for\nvariable selection by providing guidance on how to appropriately group\nvariables to achieve the desired selection rule. Furthermore, our innovative\nframework opens the door to establishing new l0 norm-based penalized regression\ntechniques that can be tailored to respect arbitrary selection rules, thereby\nexpanding the possibilities for more robust and tailored model development."}, "http://arxiv.org/abs/2203.14223": {"title": "Identifying Peer Influence in Therapeutic Communities", "link": "http://arxiv.org/abs/2203.14223", "description": "We investigate if there is a peer influence or role model effect on\nsuccessful graduation from Therapeutic Communities (TCs). We analyze anonymized\nindividual-level observational data from 3 TCs that kept records of written\nexchanges of affirmations and corrections among residents, and their precise\nentry and exit dates. The affirmations allow us to form peer networks, and the\nentry and exit dates allow us to define a causal effect of interest. We\nconceptualize the causal role model effect as measuring the difference in the\nexpected outcome of a resident (ego) who can observe one of their social\ncontacts (e.g., peers who gave affirmations), to be successful in graduating\nbefore the ego's exit vs not successfully graduating before the ego's exit.\nSince peer influence is usually confounded with unobserved homophily in\nobservational data, we model the network with a latent variable model to\nestimate homophily and include it in the outcome equation. We provide a\ntheoretical guarantee that the bias of our peer influence estimator decreases\nwith sample size. Our results indicate there is an effect of peers' graduation\non the graduation of residents. The magnitude of peer influence differs based\non gender, race, and the definition of the role model effect. A counterfactual\nexercise quantifies the potential benefits of intervention of assigning a buddy\nto \"at-risk\" individuals directly on the treated resident and indirectly on\ntheir peers through network propagation."}, "http://arxiv.org/abs/2207.03182": {"title": "Chilled Sampling for Uncertainty Quantification: A Motivation From A Meteorological Inverse Problem", "link": "http://arxiv.org/abs/2207.03182", "description": "Atmospheric motion vectors (AMVs) extracted from satellite imagery are the\nonly wind observations with good global coverage. They are important features\nfor feeding numerical weather prediction (NWP) models. Several Bayesian models\nhave been proposed to estimate AMVs. Although critical for correct assimilation\ninto NWP models, very few methods provide a thorough characterization of the\nestimation errors. The difficulty of estimating errors stems from the\nspecificity of the posterior distribution, which is both very high dimensional,\nand highly ill-conditioned due to a singular likelihood. Motivated by this\ndifficult inverse problem, this work studies the evaluation of the (expected)\nestimation errors using gradient-based Markov Chain Monte Carlo (MCMC)\nalgorithms. The main contribution is to propose a general strategy, called here\nchilling, which amounts to sampling a local approximation of the posterior\ndistribution in the neighborhood of a point estimate. From a theoretical point\nof view, we show that under regularity assumptions, the family of chilled\nposterior distributions converges in distribution as temperature decreases to\nan optimal Gaussian approximation at a point estimate given by the Maximum A\nPosteriori, also known as the Laplace approximation. Chilled sampling therefore\nprovides access to this approximation generally out of reach in such\nhigh-dimensional nonlinear contexts. From an empirical perspective, we evaluate\nthe proposed approach based on some quantitative Bayesian criteria. Our\nnumerical simulations are performed on synthetic and real meteorological data.\nThey reveal that not only the proposed chilling exhibits a significant gain in\nterms of accuracy of the point estimates and of their associated expected\nerrors, but also a substantial acceleration in the convergence speed of the\nMCMC algorithms."}, "http://arxiv.org/abs/2207.13612": {"title": "Robust Output Analysis with Monte-Carlo Methodology", "link": "http://arxiv.org/abs/2207.13612", "description": "In predictive modeling with simulation or machine learning, it is critical to\naccurately assess the quality of estimated values through output analysis. In\nrecent decades output analysis has become enriched with methods that quantify\nthe impact of input data uncertainty in the model outputs to increase\nrobustness. However, most developments are applicable assuming that the input\ndata adheres to a parametric family of distributions. We propose a unified\noutput analysis framework for simulation and machine learning outputs through\nthe lens of Monte Carlo sampling. This framework provides nonparametric\nquantification of the variance and bias induced in the outputs with\nhigher-order accuracy. Our new bias-corrected estimation from the model outputs\nleverages the extension of fast iterative bootstrap sampling and higher-order\ninfluence functions. For the scalability of the proposed estimation methods, we\ndevise budget-optimal rules and leverage control variates for variance\nreduction. Our theoretical and numerical results demonstrate a clear advantage\nin building more robust confidence intervals from the model outputs with higher\ncoverage probability."}, "http://arxiv.org/abs/2208.06685": {"title": "Adaptive novelty detection with false discovery rate guarantee", "link": "http://arxiv.org/abs/2208.06685", "description": "This paper studies the semi-supervised novelty detection problem where a set\nof \"typical\" measurements is available to the researcher. Motivated by recent\nadvances in multiple testing and conformal inference, we propose AdaDetect, a\nflexible method that is able to wrap around any probabilistic classification\nalgorithm and control the false discovery rate (FDR) on detected novelties in\nfinite samples without any distributional assumption other than\nexchangeability. In contrast to classical FDR-controlling procedures that are\noften committed to a pre-specified p-value function, AdaDetect learns the\ntransformation in a data-adaptive manner to focus the power on the directions\nthat distinguish between inliers and outliers. Inspired by the multiple testing\nliterature, we further propose variants of AdaDetect that are adaptive to the\nproportion of nulls while maintaining the finite-sample FDR control. The\nmethods are illustrated on synthetic datasets and real-world datasets,\nincluding an application in astrophysics."}, "http://arxiv.org/abs/2211.02582": {"title": "Inference for Network Count Time Series with the R Package PNAR", "link": "http://arxiv.org/abs/2211.02582", "description": "We introduce a new R package useful for inference about network count time\nseries. Such data are frequently encountered in statistics and they are usually\ntreated as multivariate time series. Their statistical analysis is based on\nlinear or log linear models. Nonlinear models, which have been applied\nsuccessfully in several research areas, have been neglected from such\napplications mainly because of their computational complexity. We provide R\nusers the flexibility to fit and study nonlinear network count time series\nmodels which include either a drift in the intercept or a regime switching\nmechanism. We develop several computational tools including estimation of\nvarious count Network Autoregressive models and fast computational algorithms\nfor testing linearity in standard cases and when non-identifiable parameters\nhamper the analysis. Finally, we introduce a copula Poisson algorithm for\nsimulating multivariate network count time series. We illustrate the\nmethodology by modeling weekly number of influenza cases in Germany."}, "http://arxiv.org/abs/2212.08642": {"title": "Estimating Higher-Order Mixed Memberships via the $\\ell_{2,\\infty}$ Tensor Perturbation Bound", "link": "http://arxiv.org/abs/2212.08642", "description": "Higher-order multiway data is ubiquitous in machine learning and statistics\nand often exhibits community-like structures, where each component (node) along\neach different mode has a community membership associated with it. In this\npaper we propose the tensor mixed-membership blockmodel, a generalization of\nthe tensor blockmodel positing that memberships need not be discrete, but\ninstead are convex combinations of latent communities. We establish the\nidentifiability of our model and propose a computationally efficient estimation\nprocedure based on the higher-order orthogonal iteration algorithm (HOOI) for\ntensor SVD composed with a simplex corner-finding algorithm. We then\ndemonstrate the consistency of our estimation procedure by providing a per-node\nerror bound, which showcases the effect of higher-order structures on\nestimation accuracy. To prove our consistency result, we develop the\n$\\ell_{2,\\infty}$ tensor perturbation bound for HOOI under independent,\npossibly heteroskedastic, subgaussian noise that may be of independent\ninterest. Our analysis uses a novel leave-one-out construction for the\niterates, and our bounds depend only on spectral properties of the underlying\nlow-rank tensor under nearly optimal signal-to-noise ratio conditions such that\ntensor SVD is computationally feasible. Whereas other leave-one-out analyses\ntypically focus on sequences constructed by analyzing the output of a given\nalgorithm with a small part of the noise removed, our leave-one-out analysis\nconstructions use both the previous iterates and the additional tensor\nstructure to eliminate a potential additional source of error. Finally, we\napply our methodology to real and simulated data, including applications to two\nflight datasets and a trade network dataset, demonstrating some effects not\nidentifiable from the model with discrete community memberships."}, "http://arxiv.org/abs/2304.10372": {"title": "Statistical inference for Gaussian Whittle-Mat\\'ern fields on metric graphs", "link": "http://arxiv.org/abs/2304.10372", "description": "Whittle-Mat\\'ern fields are a recently introduced class of Gaussian processes\non metric graphs, which are specified as solutions to a fractional-order\nstochastic differential equation. Unlike earlier covariance-based approaches\nfor specifying Gaussian fields on metric graphs, the Whittle-Mat\\'ern fields\nare well-defined for any compact metric graph and can provide Gaussian\nprocesses with differentiable sample paths. We derive the main statistical\nproperties of the model class, particularly the consistency and asymptotic\nnormality of maximum likelihood estimators of model parameters and the\nnecessary and sufficient conditions for asymptotic optimality properties of\nlinear prediction based on the model with misspecified parameters.\n\nThe covariance function of the Whittle-Mat\\'ern fields is generally\nunavailable in closed form, and they have therefore been challenging to use for\nstatistical inference. However, we show that for specific values of the\nfractional exponent, when the fields have Markov properties, likelihood-based\ninference and spatial prediction can be performed exactly and computationally\nefficiently. This facilitates using the Whittle-Mat\\'ern fields in statistical\napplications involving big datasets without the need for any approximations.\nThe methods are illustrated via an application to modeling of traffic data,\nwhere allowing for differentiable processes dramatically improves the results."}, "http://arxiv.org/abs/2305.09282": {"title": "Errors-in-variables Fr\\'echet Regression with Low-rank Covariate Approximation", "link": "http://arxiv.org/abs/2305.09282", "description": "Fr\\'echet regression has emerged as a promising approach for regression\nanalysis involving non-Euclidean response variables. However, its practical\napplicability has been hindered by its reliance on ideal scenarios with\nabundant and noiseless covariate data. In this paper, we present a novel\nestimation method that tackles these limitations by leveraging the low-rank\nstructure inherent in the covariate matrix. Our proposed framework combines the\nconcepts of global Fr\\'echet regression and principal component regression,\naiming to improve the efficiency and accuracy of the regression estimator. By\nincorporating the low-rank structure, our method enables more effective\nmodeling and estimation, particularly in high-dimensional and\nerrors-in-variables regression settings. We provide a theoretical analysis of\nthe proposed estimator's large-sample properties, including a comprehensive\nrate analysis of bias, variance, and additional variations due to measurement\nerrors. Furthermore, our numerical experiments provide empirical evidence that\nsupports the theoretical findings, demonstrating the superior performance of\nour approach. Overall, this work introduces a promising framework for\nregression analysis of non-Euclidean variables, effectively addressing the\nchallenges associated with limited and noisy covariate data, with potential\napplications in diverse fields."}, "http://arxiv.org/abs/2305.19417": {"title": "Model averaging approaches to data subset selection", "link": "http://arxiv.org/abs/2305.19417", "description": "Model averaging is a useful and robust method for dealing with model\nuncertainty in statistical analysis. Often, it is useful to consider data\nsubset selection at the same time, in which model selection criteria are used\nto compare models across different subsets of the data. Two different criteria\nhave been proposed in the literature for how the data subsets should be\nweighted. We compare the two criteria closely in a unified treatment based on\nthe Kullback-Leibler divergence, and conclude that one of them is subtly flawed\nand will tend to yield larger uncertainties due to loss of information.\nAnalytical and numerical examples are provided."}, "http://arxiv.org/abs/2309.06053": {"title": "Confounder selection via iterative graph expansion", "link": "http://arxiv.org/abs/2309.06053", "description": "Confounder selection, namely choosing a set of covariates to control for\nconfounding between a treatment and an outcome, is arguably the most important\nstep in the design of observational studies. Previous methods, such as Pearl's\ncelebrated back-door criterion, typically require pre-specifying a causal\ngraph, which can often be difficult in practice. We propose an interactive\nprocedure for confounder selection that does not require pre-specifying the\ngraph or the set of observed variables. This procedure iteratively expands the\ncausal graph by finding what we call \"primary adjustment sets\" for a pair of\npossibly confounded variables. This can be viewed as inverting a sequence of\nlatent projections of the underlying causal graph. Structural information in\nthe form of primary adjustment sets is elicited from the user, bit by bit,\nuntil either a set of covariates are found to control for confounding or it can\nbe determined that no such set exists. Other information, such as the causal\nrelations between confounders, is not required by the procedure. We show that\nif the user correctly specifies the primary adjustment sets in every step, our\nprocedure is both sound and complete."}, "http://arxiv.org/abs/2310.16989": {"title": "Randomization Inference When N Equals One", "link": "http://arxiv.org/abs/2310.16989", "description": "N-of-1 experiments, where a unit serves as its own control and treatment in\ndifferent time windows, have been used in certain medical contexts for decades.\nHowever, due to effects that accumulate over long time windows and\ninterventions that have complex evolution, a lack of robust inference tools has\nlimited the widespread applicability of such N-of-1 designs. This work combines\ntechniques from experiment design in causal inference and system identification\nfrom control theory to provide such an inference framework. We derive a model\nof the dynamic interference effect that arises in linear time-invariant\ndynamical systems. We show that a family of causal estimands analogous to those\nstudied in potential outcomes are estimable via a standard estimator derived\nfrom the method of moments. We derive formulae for higher moments of this\nestimator and describe conditions under which N-of-1 designs may provide faster\nways to estimate the effects of interventions in dynamical systems. We also\nprovide conditions under which our estimator is asymptotically normal and\nderive valid confidence intervals for this setting."}, "http://arxiv.org/abs/2310.17009": {"title": "Simulation based stacking", "link": "http://arxiv.org/abs/2310.17009", "description": "Simulation-based inference has been popular for amortized Bayesian\ncomputation. It is typical to have more than one posterior approximation, from\ndifferent inference algorithms, different architectures, or simply the\nrandomness of initialization and stochastic gradients. With a provable\nasymptotic guarantee, we present a general stacking framework to make use of\nall available posterior approximations. Our stacking method is able to combine\ndensities, simulation draws, confidence intervals, and moments, and address the\noverall precision, calibration, coverage, and bias at the same time. We\nillustrate our method on several benchmark simulations and a challenging\ncosmological inference task."}, "http://arxiv.org/abs/2310.17153": {"title": "Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration", "link": "http://arxiv.org/abs/2310.17153", "description": "Semi-implicit variational inference (SIVI) has been introduced to expand the\nanalytical variational families by defining expressive semi-implicit\ndistributions in a hierarchical manner. However, the single-layer architecture\ncommonly used in current SIVI methods can be insufficient when the target\nposterior has complicated structures. In this paper, we propose hierarchical\nsemi-implicit variational inference, called HSIVI, which generalizes SIVI to\nallow more expressive multi-layer construction of semi-implicit distributions.\nBy introducing auxiliary distributions that interpolate between a simple base\ndistribution and the target distribution, the conditional layers can be trained\nby progressively matching these auxiliary distributions one layer after\nanother. Moreover, given pre-trained score networks, HSIVI can be used to\naccelerate the sampling process of diffusion models with the score matching\nobjective. We show that HSIVI significantly enhances the expressiveness of SIVI\non several Bayesian inference problems with complicated target distributions.\nWhen used for diffusion model acceleration, we show that HSIVI can produce high\nquality samples comparable to or better than the existing fast diffusion model\nbased samplers with a small number of function evaluations on various datasets."}, "http://arxiv.org/abs/2310.17165": {"title": "Price Experimentation and Interference in Online Platforms", "link": "http://arxiv.org/abs/2310.17165", "description": "In this paper, we examine the biases arising in A/B tests where a firm\nmodifies a continuous parameter, such as price, to estimate the global\ntreatment effect associated to a given performance metric. Such biases emerge\nfrom canonical designs and estimators due to interference among market\nparticipants. We employ structural modeling and differential calculus to derive\nintuitive structural characterizations of this bias. We then specialize our\ngeneral model to a standard revenue management pricing problem. This setting\nhighlights a key potential pitfall in the use of pricing experiments to guide\nprofit maximization: notably, the canonical estimator for the change in profits\ncan have the {\\em wrong sign}. In other words, following the guidance of the\ncanonical estimator may lead the firm to move prices in the wrong direction,\nand thereby decrease profits relative to the status quo. We apply these results\nto a two-sided market model and show how this ``change of sign\" regime depends\non model parameters, and discuss structural and practical implications for\nplatform operators."}, "http://arxiv.org/abs/2310.17248": {"title": "The observed Fisher information attached to the EM algorithm, illustrated on Shepp and Vardi estimation procedure for positron emission tomography", "link": "http://arxiv.org/abs/2310.17248", "description": "The Shepp &amp; Vardi (1982) implementation of the EM algorithm for PET scan\ntumor estimation provides a point estimate of the tumor. The current study\npresents a closed-form formula of the observed Fisher information for Shepp &amp;\nVardi PET scan tumor estimation. Keywords: PET scan, EM algorithm, Fisher\ninformation matrix, standard errors."}, "http://arxiv.org/abs/2310.17308": {"title": "Wild Bootstrap for Counting Process-Based Statistics", "link": "http://arxiv.org/abs/2310.17308", "description": "The wild bootstrap is a popular resampling method in the context of\ntime-to-event data analyses. Previous works established the large sample\nproperties of it for applications to different estimators and test statistics.\nIt can be used to justify the accuracy of inference procedures such as\nhypothesis tests or time-simultaneous confidence bands. This paper consists of\ntwo parts: in Part~I, a general framework is developed in which the large\nsample properties are established in a unified way by using martingale\nstructures. The framework includes most of the well-known non- and\nsemiparametric statistical methods in time-to-event analysis and parametric\napproaches. In Part II, the Fine-Gray proportional sub-hazards model\nexemplifies the theory for inference on cumulative incidence functions given\nthe covariates. The model falls within the framework if the data are\ncensoring-complete. A simulation study demonstrates the reliability of the\nmethod and an application to a data set about hospital-acquired infections\nillustrates the statistical procedure."}, "http://arxiv.org/abs/2310.17334": {"title": "Bayesian Optimization for Personalized Dose-Finding Trials with Combination Therapies", "link": "http://arxiv.org/abs/2310.17334", "description": "Identification of optimal dose combinations in early phase dose-finding\ntrials is challenging, due to the trade-off between precisely estimating the\nmany parameters required to flexibly model the dose-response surface, and the\nsmall sample sizes in early phase trials. Existing methods often restrict the\nsearch to pre-defined dose combinations, which may fail to identify regions of\noptimality in the dose combination space. These difficulties are even more\npertinent in the context of personalized dose-finding, where patient\ncharacteristics are used to identify tailored optimal dose combinations. To\novercome these challenges, we propose the use of Bayesian optimization for\nfinding optimal dose combinations in standard (\"one size fits all\") and\npersonalized multi-agent dose-finding trials. Bayesian optimization is a method\nfor estimating the global optima of expensive-to-evaluate objective functions.\nThe objective function is approximated by a surrogate model, commonly a\nGaussian process, paired with a sequential design strategy to select the next\npoint via an acquisition function. This work is motivated by an\nindustry-sponsored problem, where focus is on optimizing a dual-agent therapy\nin a setting featuring minimal toxicity. To compare the performance of the\nstandard and personalized methods under this setting, simulation studies are\nperformed for a variety of scenarios. Our study concludes that taking a\npersonalized approach is highly beneficial in the presence of heterogeneity."}, "http://arxiv.org/abs/2310.17434": {"title": "The `Why' behind including `Y' in your imputation model", "link": "http://arxiv.org/abs/2310.17434", "description": "Missing data is a common challenge when analyzing epidemiological data, and\nimputation is often used to address this issue. Here, we investigate the\nscenario where a covariate used in an analysis has missingness and will be\nimputed. There are recommendations to include the outcome from the analysis\nmodel in the imputation model for missing covariates, but it is not necessarily\nclear if this recommmendation always holds and why this is sometimes true. We\nexamine deterministic imputation (i.e., single imputation where the imputed\nvalues are treated as fixed) and stochastic imputation (i.e., single imputation\nwith a random value or multiple imputation) methods and their implications for\nestimating the relationship between the imputed covariate and the outcome. We\nmathematically demonstrate that including the outcome variable in imputation\nmodels is not just a recommendation but a requirement to achieve unbiased\nresults when using stochastic imputation methods. Moreover, we dispel common\nmisconceptions about deterministic imputation models and demonstrate why the\noutcome should not be included in these models. This paper aims to bridge the\ngap between imputation in theory and in practice, providing mathematical\nderivations to explain common statistical recommendations. We offer a better\nunderstanding of the considerations involved in imputing missing covariates and\nemphasize when it is necessary to include the outcome variable in the\nimputation model."}, "http://arxiv.org/abs/2310.17440": {"title": "Gibbs optimal design of experiments", "link": "http://arxiv.org/abs/2310.17440", "description": "Bayesian optimal design of experiments is a well-established approach to\nplanning experiments. Briefly, a probability distribution, known as a\nstatistical model, for the responses is assumed which is dependent on a vector\nof unknown parameters. A utility function is then specified which gives the\ngain in information for estimating the true value of the parameters using the\nBayesian posterior distribution. A Bayesian optimal design is given by\nmaximising the expectation of the utility with respect to the joint\ndistribution given by the statistical model and prior distribution for the true\nparameter values. The approach takes account of the experimental aim via\nspecification of the utility and of all assumed sources of uncertainty via the\nexpected utility. However, it is predicated on the specification of the\nstatistical model. Recently, a new type of statistical inference, known as\nGibbs (or General Bayesian) inference, has been advanced. This is\nBayesian-like, in that uncertainty on unknown quantities is represented by a\nposterior distribution, but does not necessarily rely on specification of a\nstatistical model. Thus the resulting inference should be less sensitive to\nmisspecification of the statistical model. The purpose of this paper is to\npropose Gibbs optimal design: a framework for optimal design of experiments for\nGibbs inference. The concept behind the framework is introduced along with a\ncomputational approach to find Gibbs optimal designs in practice. The framework\nis demonstrated on exemplars including linear models, and experiments with\ncount and time-to-event responses."}, "http://arxiv.org/abs/2310.17496": {"title": "Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach", "link": "http://arxiv.org/abs/2310.17496", "description": "In modern recommendation systems, the standard pipeline involves training\nmachine learning models on historical data to predict user behaviors and\nimprove recommendations continuously. However, these data training loops can\nintroduce interference in A/B tests, where data generated by control and\ntreatment algorithms, potentially with different distributions, are combined.\nTo address these challenges, we introduce a novel approach called weighted\ntraining. This approach entails training a model to predict the probability of\neach data point appearing in either the treatment or control data and\nsubsequently applying weighted losses during model training. We demonstrate\nthat this approach achieves the least variance among all estimators without\ncausing shifts in the training distributions. Through simulation studies, we\ndemonstrate the lower bias and variance of our approach compared to other\nmethods."}, "http://arxiv.org/abs/2310.17546": {"title": "A changepoint approach to modelling non-stationary soil moisture dynamics", "link": "http://arxiv.org/abs/2310.17546", "description": "Soil moisture dynamics provide an indicator of soil health that scientists\nmodel via soil drydown curves. The typical modeling process requires the soil\nmoisture time series to be manually separated into drydown segments and then\nexponential decay models are fitted to them independently. Sensor development\nover recent years means that experiments that were previously conducted over a\nfew field campaigns can now be scaled to months or even years, often at a\nhigher sampling rate. Manual identification of drydown segments is no longer\npractical. To better meet the challenge of increasing data size, this paper\nproposes a novel changepoint-based approach to automatically identify\nstructural changes in the soil drying process, and estimate the parameters\ncharacterizing the drying processes simultaneously. A simulation study is\ncarried out to assess the performance of the method. The results demonstrate\nits ability to identify structural changes and retrieve key parameters of\ninterest to soil scientists. The method is applied to hourly soil moisture time\nseries from the NEON data portal to investigate the temporal dynamics of soil\nmoisture drydown. We recover known relationships previously identified\nmanually, alongside delivering new insights into the temporal variability\nacross soil types and locations."}, "http://arxiv.org/abs/2310.17629": {"title": "Approximate Leave-one-out Cross Validation for Regression with $\\ell_1$ Regularizers (extended version)", "link": "http://arxiv.org/abs/2310.17629", "description": "The out-of-sample error (OO) is the main quantity of interest in risk\nestimation and model selection. Leave-one-out cross validation (LO) offers a\n(nearly) distribution-free yet computationally demanding approach to estimate\nOO. Recent theoretical work showed that approximate leave-one-out cross\nvalidation (ALO) is a computationally efficient and statistically reliable\nestimate of LO (and OO) for generalized linear models with differentiable\nregularizers. For problems involving non-differentiable regularizers, despite\nsignificant empirical evidence, the theoretical understanding of ALO's error\nremains unknown. In this paper, we present a novel theory for a wide class of\nproblems in the generalized linear model family with non-differentiable\nregularizers. We bound the error |ALO - LO| in terms of intuitive metrics such\nas the size of leave-i-out perturbations in active sets, sample size n, number\nof features p and regularization parameters. As a consequence, for the\n$\\ell_1$-regularized problems, we show that |ALO - LO| goes to zero as p goes\nto infinity while n/p and SNR are fixed and bounded."}, "http://arxiv.org/abs/2108.04201": {"title": "Guaranteed Functional Tensor Singular Value Decomposition", "link": "http://arxiv.org/abs/2108.04201", "description": "This paper introduces the functional tensor singular value decomposition\n(FTSVD), a novel dimension reduction framework for tensors with one functional\nmode and several tabular modes. The problem is motivated by high-order\nlongitudinal data analysis. Our model assumes the observed data to be a random\nrealization of an approximate CP low-rank functional tensor measured on a\ndiscrete time grid. Incorporating tensor algebra and the theory of Reproducing\nKernel Hilbert Space (RKHS), we propose a novel RKHS-based constrained power\niteration with spectral initialization. Our method can successfully estimate\nboth singular vectors and functions of the low-rank structure in the observed\ndata. With mild assumptions, we establish the non-asymptotic contractive error\nbounds for the proposed algorithm. The superiority of the proposed framework is\ndemonstrated via extensive experiments on both simulated and real data."}, "http://arxiv.org/abs/2202.02146": {"title": "Elastic Gradient Descent, an Iterative Optimization Method Approximating the Solution Paths of the Elastic Net", "link": "http://arxiv.org/abs/2202.02146", "description": "The elastic net combines lasso and ridge regression to fuse the sparsity\nproperty of lasso with the grouping property of ridge regression. The\nconnections between ridge regression and gradient descent and between lasso and\nforward stagewise regression have previously been shown. Similar to how the\nelastic net generalizes lasso and ridge regression, we introduce elastic\ngradient descent, a generalization of gradient descent and forward stagewise\nregression. We theoretically analyze elastic gradient descent and compare it to\nthe elastic net and forward stagewise regression. Parts of the analysis are\nbased on elastic gradient flow, a piecewise analytical construction, obtained\nfor elastic gradient descent with infinitesimal step size. We also compare\nelastic gradient descent to the elastic net on real and simulated data and show\nthat it provides similar solution paths, but is several orders of magnitude\nfaster. Compared to forward stagewise regression, elastic gradient descent\nselects a model that, although still sparse, provides considerably lower\nprediction and estimation errors."}, "http://arxiv.org/abs/2202.03897": {"title": "Inference from Sampling with Response Probabilities Estimated via Calibration", "link": "http://arxiv.org/abs/2202.03897", "description": "A solution to control for nonresponse bias consists of multiplying the design\nweights of respondents by the inverse of estimated response probabilities to\ncompensate for the nonrespondents. Maximum likelihood and calibration are two\napproaches that can be applied to obtain estimated response probabilities. We\nconsider a common framework in which these approaches can be compared. We\ndevelop an asymptotic study of the behavior of the resulting estimator when\ncalibration is applied. A logistic regression model for the response\nprobabilities is postulated. Missing at random and unclustered data are\nsupposed. Three main contributions of this work are: 1) we show that the\nestimators with the response probabilities estimated via calibration are\nasymptotically equivalent to unbiased estimators and that a gain in efficiency\nis obtained when estimating the response probabilities via calibration as\ncompared to the estimator with the true response probabilities, 2) we show that\nthe estimators with the response probabilities estimated via calibration are\ndoubly robust to model misspecification and explain why double robustness is\nnot guaranteed when maximum likelihood is applied, and 3) we discuss and\nillustrate problems related to response probabilities estimation, namely\nexistence of a solution to the estimating equations, problems of convergence,\nand extreme weights. We explain and illustrate why the first aforementioned\nproblem is more likely with calibration than with maximum likelihood\nestimation. We present the results of a simulation study in order to illustrate\nthese elements."}, "http://arxiv.org/abs/2208.14951": {"title": "Statistical inference for multivariate extremes via a geometric approach", "link": "http://arxiv.org/abs/2208.14951", "description": "A geometric representation for multivariate extremes, based on the shapes of\nscaled sample clouds in light-tailed margins and their so-called limit sets,\nhas recently been shown to connect several existing extremal dependence\nconcepts. However, these results are purely probabilistic, and the geometric\napproach itself has not been fully exploited for statistical inference. We\noutline a method for parametric estimation of the limit set shape, which\nincludes a useful non/semi-parametric estimate as a pre-processing step. More\nfundamentally, our approach provides a new class of asymptotically-motivated\nstatistical models for the tails of multivariate distributions, and such models\ncan accommodate any combination of simultaneous or non-simultaneous extremes\nthrough appropriate parametric forms for the limit set shape. Extrapolation\nfurther into the tail of the distribution is possible via simulation from the\nfitted model. A simulation study confirms that our methodology is very\ncompetitive with existing approaches, and can successfully allow estimation of\nsmall probabilities in regions where other methods struggle. We apply the\nmethodology to two environmental datasets, with diagnostics demonstrating a\ngood fit."}, "http://arxiv.org/abs/2209.08889": {"title": "Inference of nonlinear causal effects with GWAS summary data", "link": "http://arxiv.org/abs/2209.08889", "description": "Large-scale genome-wide association studies (GWAS) have offered an exciting\nopportunity to discover putative causal genes or risk factors associated with\ndiseases by using SNPs as instrumental variables (IVs). However, conventional\napproaches assume linear causal relations partly for simplicity and partly for\nthe availability of GWAS summary data. In this work, we propose a novel model\n{for transcriptome-wide association studies (TWAS)} to incorporate nonlinear\nrelationships across IVs, an exposure/gene, and an outcome, which is robust\nagainst violations of the valid IV assumptions, permits the use of GWAS summary\ndata, and covers two-stage least squares as a special case. We decouple the\nestimation of a marginal causal effect and a nonlinear transformation, where\nthe former is estimated via sliced inverse regression and a sparse instrumental\nvariable regression, and the latter is estimated by a ratio-adjusted inverse\nregression. On this ground, we propose an inferential procedure. An application\nof the proposed method to the ADNI gene expression data and the IGAP GWAS\nsummary data identifies 18 causal genes associated with Alzheimer's disease,\nincluding APOE and TOMM40, in addition to 7 other genes missed by two-stage\nleast squares considering only linear relationships. Our findings suggest that\nnonlinear modeling is required to unleash the power of IV regression for\nidentifying potentially nonlinear gene-trait associations. Accompanying this\npaper is our Python library \\texttt{nl-causal}\n(\\url{https://nonlinear-causal.readthedocs.io/}) that implements the proposed\nmethod."}, "http://arxiv.org/abs/2301.03038": {"title": "Skewed Bernstein-von Mises theorem and skew-modal approximations", "link": "http://arxiv.org/abs/2301.03038", "description": "Gaussian approximations are routinely employed in Bayesian statistics to ease\ninference when the target posterior is intractable. Although these\napproximations are asymptotically justified by Bernstein-von Mises type\nresults, in practice the expected Gaussian behavior may poorly represent the\nshape of the posterior, thus affecting approximation accuracy. Motivated by\nthese considerations, we derive an improved class of closed-form approximations\nof posterior distributions which arise from a new treatment of a third-order\nversion of the Laplace method yielding approximations in a tractable family of\nskew-symmetric distributions. Under general assumptions which account for\nmisspecified models and non-i.i.d. settings, this family of approximations is\nshown to have a total variation distance from the target posterior whose rate\nof convergence improves by at least one order of magnitude the one established\nby the classical Bernstein-von Mises theorem. Specializing this result to the\ncase of regular parametric models shows that the same improvement in\napproximation accuracy can be also derived for polynomially bounded posterior\nfunctionals. Unlike other higher-order approximations, our results prove that\nit is possible to derive closed-form and valid densities which are expected to\nprovide, in practice, a more accurate, yet similarly-tractable, alternative to\nGaussian approximations of the target posterior, while inheriting its limiting\nfrequentist properties. We strengthen such arguments by developing a practical\nskew-modal approximation for both joint and marginal posteriors that achieves\nthe same theoretical guarantees of its theoretical counterpart by replacing the\nunknown model parameters with the corresponding MAP estimate. Empirical studies\nconfirm that our theoretical results closely match the remarkable performance\nobserved in practice, even in finite, possibly small, sample regimes."}, "http://arxiv.org/abs/2303.05878": {"title": "Identification and Estimation of Causal Effects with Confounders Missing Not at Random", "link": "http://arxiv.org/abs/2303.05878", "description": "Making causal inferences from observational studies can be challenging when\nconfounders are missing not at random. In such cases, identifying causal\neffects is often not guaranteed. Motivated by a real example, we consider a\ntreatment-independent missingness assumption under which we establish the\nidentification of causal effects when confounders are missing not at random. We\npropose a weighted estimating equation (WEE) approach for estimating model\nparameters and introduce three estimators for the average causal effect, based\non regression, propensity score weighting, and doubly robust estimation. We\nevaluate the performance of these estimators through simulations, and provide a\nreal data analysis to illustrate our proposed method."}, "http://arxiv.org/abs/2305.12283": {"title": "Distribution-Free Model-Agnostic Regression Calibration via Nonparametric Methods", "link": "http://arxiv.org/abs/2305.12283", "description": "In this paper, we consider the uncertainty quantification problem for\nregression models. Specifically, we consider an individual calibration\nobjective for characterizing the quantiles of the prediction model. While such\nan objective is well-motivated from downstream tasks such as newsvendor cost,\nthe existing methods have been largely heuristic and lack of statistical\nguarantee in terms of individual calibration. We show via simple examples that\nthe existing methods focusing on population-level calibration guarantees such\nas average calibration or sharpness can lead to harmful and unexpected results.\nWe propose simple nonparametric calibration methods that are agnostic of the\nunderlying prediction model and enjoy both computational efficiency and\nstatistical consistency. Our approach enables a better understanding of the\npossibility of individual calibration, and we establish matching upper and\nlower bounds for the calibration error of our proposed methods. Technically,\nour analysis combines the nonparametric analysis with a covering number\nargument for parametric analysis, which advances the existing theoretical\nanalyses in the literature of nonparametric density estimation and quantile\nbandit problems. Importantly, the nonparametric perspective sheds new\ntheoretical insights into regression calibration in terms of the curse of\ndimensionality and reconciles the existing results on the impossibility of\nindividual calibration. To our knowledge, we make the first effort to reach\nboth individual calibration and finite-sample guarantee with minimal\nassumptions in terms of conformal prediction. Numerical experiments show the\nadvantage of such a simple approach under various metrics, and also under\ncovariates shift. We hope our work provides a simple benchmark and a starting\npoint of theoretical ground for future research on regression calibration."}, "http://arxiv.org/abs/2305.14943": {"title": "Learning Rate Free Bayesian Inference in Constrained Domains", "link": "http://arxiv.org/abs/2305.14943", "description": "We introduce a suite of new particle-based algorithms for sampling on\nconstrained domains which are entirely learning rate free. Our approach\nleverages coin betting ideas from convex optimisation, and the viewpoint of\nconstrained sampling as a mirrored optimisation problem on the space of\nprobability measures. Based on this viewpoint, we also introduce a unifying\nframework for several existing constrained sampling algorithms, including\nmirrored Langevin dynamics and mirrored Stein variational gradient descent. We\ndemonstrate the performance of our algorithms on a range of numerical examples,\nincluding sampling from targets on the simplex, sampling with fairness\nconstraints, and constrained sampling problems in post-selection inference. Our\nresults indicate that our algorithms achieve competitive performance with\nexisting constrained sampling methods, without the need to tune any\nhyperparameters."}, "http://arxiv.org/abs/2308.07983": {"title": "Monte Carlo guided Diffusion for Bayesian linear inverse problems", "link": "http://arxiv.org/abs/2308.07983", "description": "Ill-posed linear inverse problems arise frequently in various applications,\nfrom computational photography to medical imaging. A recent line of research\nexploits Bayesian inference with informative priors to handle the ill-posedness\nof such problems. Amongst such priors, score-based generative models (SGM) have\nrecently been successfully applied to several different inverse problems. In\nthis study, we exploit the particular structure of the prior defined by the SGM\nto define a sequence of intermediate linear inverse problems. As the noise\nlevel decreases, the posteriors of these inverse problems get closer to the\ntarget posterior of the original inverse problem. To sample from this sequence\nof posteriors, we propose the use of Sequential Monte Carlo (SMC) methods. The\nproposed algorithm, MCGDiff, is shown to be theoretically grounded and we\nprovide numerical simulations showing that it outperforms competing baselines\nwhen dealing with ill-posed inverse problems in a Bayesian setting."}, "http://arxiv.org/abs/2308.12485": {"title": "Optimal Shrinkage Estimation of Fixed Effects in Linear Panel Data Models", "link": "http://arxiv.org/abs/2308.12485", "description": "Shrinkage methods are frequently used to estimate fixed effects to reduce the\nnoisiness of the least squares estimators. However, widely used shrinkage\nestimators guarantee such noise reduction only under strong distributional\nassumptions. I develop an estimator for the fixed effects that obtains the best\npossible mean squared error within a class of shrinkage estimators. This class\nincludes conventional shrinkage estimators and the optimality does not require\ndistributional assumptions. The estimator has an intuitive form and is easy to\nimplement. Moreover, the fixed effects are allowed to vary with time and to be\nserially correlated, and the shrinkage optimally incorporates the underlying\ncorrelation structure in this case. In such a context, I also provide a method\nto forecast fixed effects one period ahead."}, "http://arxiv.org/abs/2309.16843": {"title": "A Mean Field Approach to Empirical Bayes Estimation in High-dimensional Linear Regression", "link": "http://arxiv.org/abs/2309.16843", "description": "We study empirical Bayes estimation in high-dimensional linear regression. To\nfacilitate computationally efficient estimation of the underlying prior, we\nadopt a variational empirical Bayes approach, introduced originally in\nCarbonetto and Stephens (2012) and Kim et al. (2022). We establish asymptotic\nconsistency of the nonparametric maximum likelihood estimator (NPMLE) and its\n(computable) naive mean field variational surrogate under mild assumptions on\nthe design and the prior. Assuming, in addition, that the naive mean field\napproximation has a dominant optimizer, we develop a computationally efficient\napproximation to the oracle posterior distribution, and establish its accuracy\nunder the 1-Wasserstein metric. This enables computationally feasible Bayesian\ninference; e.g., construction of posterior credible intervals with an average\ncoverage guarantee, Bayes optimal estimation for the regression coefficients,\nestimation of the proportion of non-nulls, etc. Our analysis covers both\ndeterministic and random designs, and accommodates correlations among the\nfeatures. To the best of our knowledge, this provides the first rigorous\nnonparametric empirical Bayes method in a high-dimensional regression setting\nwithout sparsity."}, "http://arxiv.org/abs/2310.17679": {"title": "Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score Search and Grow-Shrink Trees", "link": "http://arxiv.org/abs/2310.17679", "description": "Learning graphical conditional independence structures is an important\nmachine learning problem and a cornerstone of causal discovery. However, the\naccuracy and execution time of learning algorithms generally struggle to scale\nto problems with hundreds of highly connected variables -- for instance,\nrecovering brain networks from fMRI data. We introduce the best order score\nsearch (BOSS) and grow-shrink trees (GSTs) for learning directed acyclic graphs\n(DAGs) in this paradigm. BOSS greedily searches over permutations of variables,\nusing GSTs to construct and score DAGs from permutations. GSTs efficiently\ncache scores to eliminate redundant calculations. BOSS achieves\nstate-of-the-art performance in accuracy and execution time, comparing\nfavorably to a variety of combinatorial and gradient-based learning algorithms\nunder a broad range of conditions. To demonstrate its practicality, we apply\nBOSS to two sets of resting-state fMRI data: simulated data with\npseudo-empirical noise distributions derived from randomized empirical fMRI\ncortical signals and clinical data from 3T fMRI scans processed into cortical\nparcels. BOSS is available for use within the TETRAD project which includes\nPython and R wrappers."}, "http://arxiv.org/abs/2310.17712": {"title": "Community Detection and Classification Guarantees Using Embeddings Learned by Node2Vec", "link": "http://arxiv.org/abs/2310.17712", "description": "Embedding the nodes of a large network into an Euclidean space is a common\nobjective in modern machine learning, with a variety of tools available. These\nembeddings can then be used as features for tasks such as community\ndetection/node clustering or link prediction, where they achieve state of the\nart performance. With the exception of spectral clustering methods, there is\nlittle theoretical understanding for other commonly used approaches to learning\nembeddings. In this work we examine the theoretical properties of the\nembeddings learned by node2vec. Our main result shows that the use of k-means\nclustering on the embedding vectors produced by node2vec gives weakly\nconsistent community recovery for the nodes in (degree corrected) stochastic\nblock models. We also discuss the use of these embeddings for node and link\nprediction tasks. We demonstrate this result empirically, and examine how this\nrelates to other embedding tools for network data."}, "http://arxiv.org/abs/2310.17760": {"title": "Novel Models for Multiple Dependent Heteroskedastic Time Series", "link": "http://arxiv.org/abs/2310.17760", "description": "Functional magnetic resonance imaging or functional MRI (fMRI) is a very\npopular tool used for differing brain regions by measuring brain activity. It\nis affected by physiological noise, such as head and brain movement in the\nscanner from breathing, heart beats, or the subject fidgeting. The purpose of\nthis paper is to propose a novel approach to handling fMRI data for infants\nwith high volatility caused by sudden head movements. Another purpose is to\nevaluate the volatility modelling performance of multiple dependent fMRI time\nseries data. The models examined in this paper are AR and GARCH and the\nmodelling performance is evaluated by several statistical performance measures.\nThe conclusions of this paper are that multiple dependent fMRI series data can\nbe fitted with AR + GARCH model if the multiple fMRI data have many sudden head\nmovements. The GARCH model can capture the shared volatility clustering caused\nby head movements across brain regions. However, the multiple fMRI data without\nmany head movements have fitted AR + GARCH model with different performance.\nThe conclusions are supported by statistical tests and measures. This paper\nhighlights the difference between the proposed approach from traditional\napproaches when estimating model parameters and modelling conditional variances\non multiple dependent time series. In the future, the proposed approach can be\napplied to other research fields, such as financial economics, and signal\nprocessing. Code is available at \\url{https://github.<a href=\"https://export.arxiv.org/abs/com/1320494\">com/1320494</a>2/STAT40710}."}, "http://arxiv.org/abs/2310.17766": {"title": "Minibatch Markov chain Monte Carlo Algorithms for Fitting Gaussian Processes", "link": "http://arxiv.org/abs/2310.17766", "description": "Gaussian processes (GPs) are a highly flexible, nonparametric statistical\nmodel that are commonly used to fit nonlinear relationships or account for\ncorrelation between observations. However, the computational load of fitting a\nGaussian process is $\\mathcal{O}(n^3)$ making them infeasible for use on large\ndatasets. To make GPs more feasible for large datasets, this research focuses\non the use of minibatching to estimate GP parameters. Specifically, we outline\nboth approximate and exact minibatch Markov chain Monte Carlo algorithms that\nsubstantially reduce the computation of fitting a GP by only considering small\nsubsets of the data at a time. We demonstrate and compare this methodology\nusing various simulations and real datasets."}, "http://arxiv.org/abs/2310.17806": {"title": "Transporting treatment effects from difference-in-differences studies", "link": "http://arxiv.org/abs/2310.17806", "description": "Difference-in-differences (DID) is a popular approach to identify the causal\neffects of treatments and policies in the presence of unmeasured confounding.\nDID identifies the sample average treatment effect in the treated (SATT).\nHowever, a goal of such research is often to inform decision-making in target\npopulations outside the treated sample. Transportability methods have been\ndeveloped to extend inferences from study samples to external target\npopulations; these methods have primarily been developed and applied in\nsettings where identification is based on conditional independence between the\ntreatment and potential outcomes, such as in a randomized trial. This paper\ndevelops identification and estimators for effects in a target population,\nbased on DID conducted in a study sample that differs from the target\npopulation. We present a range of assumptions under which one may identify\ncausal effects in the target population and employ causal diagrams to\nillustrate these assumptions. In most realistic settings, results depend\ncritically on the assumption that any unmeasured confounders are not effect\nmeasure modifiers on the scale of the effect of interest. We develop several\nestimators of transported effects, including a doubly robust estimator based on\nthe efficient influence function. Simulation results support theoretical\nproperties of the proposed estimators. We discuss the potential application of\nour approach to a study of the effects of a US federal smoke-free housing\npolicy, where the original study was conducted in New York City alone and the\ngoal is extend inferences to other US cities."}, "http://arxiv.org/abs/2310.17816": {"title": "Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs", "link": "http://arxiv.org/abs/2310.17816", "description": "This work addresses the problem of automated covariate selection under\nlimited prior knowledge. Given an exposure-outcome pair {X,Y} and a variable\nset Z of unknown causal structure, the Local Discovery by Partitioning (LDP)\nalgorithm partitions Z into subsets defined by their relation to {X,Y}. We\nenumerate eight exhaustive and mutually exclusive partitions of any arbitrary Z\nand leverage this taxonomy to differentiate confounders from other variable\ntypes. LDP is motivated by valid adjustment set identification, but avoids the\npretreatment assumption commonly made by automated covariate selection methods.\nWe provide theoretical guarantees that LDP returns a valid adjustment set for\nany Z that meets sufficient graphical conditions. Under stronger conditions, we\nprove that partition labels are asymptotically correct. Total independence\ntests is worst-case quadratic in |Z|, with sub-quadratic runtimes observed\nempirically. We numerically validate our theoretical guarantees on synthetic\nand semi-synthetic graphs. Adjustment sets from LDP yield less biased and more\nprecise average treatment effect estimates than baselines, with LDP\noutperforming on confounder recall, test count, and runtime for valid\nadjustment set discovery."}, "http://arxiv.org/abs/2310.17820": {"title": "Sparse Bayesian Multidimensional Item Response Theory", "link": "http://arxiv.org/abs/2310.17820", "description": "Multivariate Item Response Theory (MIRT) is sought-after widely by applied\nresearchers looking for interpretable (sparse) explanations underlying response\npatterns in questionnaire data. There is, however, an unmet demand for such\nsparsity discovery tools in practice. Our paper develops a Bayesian platform\nfor binary and ordinal item MIRT which requires minimal tuning and scales well\non relatively large datasets due to its parallelizable features. Bayesian\nmethodology for MIRT models has traditionally relied on MCMC simulation, which\ncannot only be slow in practice, but also often renders exact sparsity recovery\nimpossible without additional thresholding. In this work, we develop a scalable\nBayesian EM algorithm to estimate sparse factor loadings from binary and\nordinal item responses. We address the seemingly insurmountable problem of\nunknown latent factor dimensionality with tools from Bayesian nonparametrics\nwhich enable estimating the number of factors. Rotations to sparsity through\nparameter expansion further enhance convergence and interpretability without\nidentifiability constraints. In our simulation study, we show that our method\nreliably recovers both the factor dimensionality as well as the latent\nstructure on high-dimensional synthetic data even for small samples. We\ndemonstrate the practical usefulness of our approach on two datasets: an\neducational item response dataset and a quality-of-life measurement dataset.\nBoth demonstrations show that our tool yields interpretable estimates,\nfacilitating interesting discoveries that might otherwise go unnoticed under a\npure confirmatory factor analysis setting. We provide an easy-to-use software\nwhich is a useful new addition to the MIRT toolkit and which will hopefully\nserve as the go-to method for practitioners."}, "http://arxiv.org/abs/2310.17845": {"title": "A Unified and Optimal Multiple Testing Framework based on rho-values", "link": "http://arxiv.org/abs/2310.17845", "description": "Multiple testing is an important research direction that has gained major\nattention in recent years. Currently, most multiple testing procedures are\ndesigned with p-values or Local false discovery rate (Lfdr) statistics.\nHowever, p-values obtained by applying probability integral transform to some\nwell-known test statistics often do not incorporate information from the\nalternatives, resulting in suboptimal procedures. On the other hand, Lfdr based\nprocedures can be asymptotically optimal but their guarantee on false discovery\nrate (FDR) control relies on consistent estimation of Lfdr, which is often\ndifficult in practice especially when the incorporation of side information is\ndesirable. In this article, we propose a novel and flexibly constructed class\nof statistics, called rho-values, which combines the merits of both p-values\nand Lfdr while enjoys superiorities over methods based on these two types of\nstatistics. Specifically, it unifies these two frameworks and operates in two\nsteps, ranking and thresholding. The ranking produced by rho-values mimics that\nproduced by Lfdr statistics, and the strategy for choosing the threshold is\nsimilar to that of p-value based procedures. Therefore, the proposed framework\nguarantees FDR control under weak assumptions; it maintains the integrity of\nthe structural information encoded by the summary statistics and the auxiliary\ncovariates and hence can be asymptotically optimal. We demonstrate the efficacy\nof the new framework through extensive simulations and two data applications."}, "http://arxiv.org/abs/2310.17999": {"title": "Automated threshold selection and associated inference uncertainty for univariate extremes", "link": "http://arxiv.org/abs/2310.17999", "description": "Threshold selection is a fundamental problem in any threshold-based extreme\nvalue analysis. While models are asymptotically motivated, selecting an\nappropriate threshold for finite samples can be difficult through standard\nmethods. Inference can also be highly sensitive to the choice of threshold. Too\nlow a threshold choice leads to bias in the fit of the extreme value model,\nwhile too high a choice leads to unnecessary additional uncertainty in the\nestimation of model parameters. In this paper, we develop a novel methodology\nfor automated threshold selection that directly tackles this bias-variance\ntrade-off. We also develop a method to account for the uncertainty in this\nthreshold choice and propagate this uncertainty through to high quantile\ninference. Through a simulation study, we demonstrate the effectiveness of our\nmethod for threshold selection and subsequent extreme quantile estimation. We\napply our method to the well-known, troublesome example of the River Nidd\ndataset."}, "http://arxiv.org/abs/2310.18027": {"title": "Bayesian Prognostic Covariate Adjustment With Additive Mixture Priors", "link": "http://arxiv.org/abs/2310.18027", "description": "Effective and rapid decision-making from randomized controlled trials (RCTs)\nrequires unbiased and precise treatment effect inferences. Two strategies to\naddress this requirement are to adjust for covariates that are highly\ncorrelated with the outcome, and to leverage historical control information via\nBayes' theorem. We propose a new Bayesian prognostic covariate adjustment\nmethodology, referred to as Bayesian PROCOVA, that combines these two\nstrategies. Covariate adjustment is based on generative artificial intelligence\n(AI) algorithms that construct a digital twin generator (DTG) for RCT\nparticipants. The DTG is trained on historical control data and yields a\ndigital twin (DT) probability distribution for each participant's control\noutcome. The expectation of the DT distribution defines the single covariate\nfor adjustment. Historical control information are leveraged via an additive\nmixture prior with two components: an informative prior probability\ndistribution specified based on historical control data, and a non-informative\nprior distribution. The weight parameter in the mixture has a prior\ndistribution as well, so that the entire additive mixture prior distribution is\ncompletely pre-specifiable and does not involve any information from the RCT.\nWe establish an efficient Gibbs algorithm for sampling from the posterior\ndistribution, and derive closed-form expressions for the posterior mean and\nvariance of the treatment effect conditional on the weight parameter, of\nBayesian PROCOVA. We evaluate the bias control and variance reduction of\nBayesian PROCOVA compared to frequentist prognostic covariate adjustment\n(PROCOVA) via simulation studies that encompass different types of\ndiscrepancies between the historical control and RCT data. Ultimately, Bayesian\nPROCOVA can yield informative treatment effect inferences with fewer control\nparticipants, accelerating effective decision-making."}, "http://arxiv.org/abs/2310.18047": {"title": "Robust Bayesian Inference on Riemannian Submanifold", "link": "http://arxiv.org/abs/2310.18047", "description": "Non-Euclidean spaces routinely arise in modern statistical applications such\nas in medical imaging, robotics, and computer vision, to name a few. While\ntraditional Bayesian approaches are applicable to such settings by considering\nan ambient Euclidean space as the parameter space, we demonstrate the benefits\nof integrating manifold structure into the Bayesian framework, both\ntheoretically and computationally. Moreover, existing Bayesian approaches which\nare designed specifically for manifold-valued parameters are primarily\nmodel-based, which are typically subject to inaccurate uncertainty\nquantification under model misspecification. In this article, we propose a\nrobust model-free Bayesian inference for parameters defined on a Riemannian\nsubmanifold, which is shown to provide valid uncertainty quantification from a\nfrequentist perspective. Computationally, we propose a Markov chain Monte Carlo\nto sample from the posterior on the Riemannian submanifold, where the mixing\ntime, in the large sample regime, is shown to depend only on the intrinsic\ndimension of the parameter space instead of the potentially much larger ambient\ndimension. Our numerical results demonstrate the effectiveness of our approach\non a variety of problems, such as reduced-rank multiple quantile regression,\nprincipal component analysis, and Fr\\'{e}chet mean estimation."}, "http://arxiv.org/abs/2310.18108": {"title": "Transductive conformal inference with adaptive scores", "link": "http://arxiv.org/abs/2310.18108", "description": "Conformal inference is a fundamental and versatile tool that provides\ndistribution-free guarantees for many machine learning tasks. We consider the\ntransductive setting, where decisions are made on a test sample of $m$ new\npoints, giving rise to $m$ conformal $p$-values. {While classical results only\nconcern their marginal distribution, we show that their joint distribution\nfollows a P\\'olya urn model, and establish a concentration inequality for their\nempirical distribution function.} The results hold for arbitrary exchangeable\nscores, including {\\it adaptive} ones that can use the covariates of the\ntest+calibration samples at training stage for increased accuracy. We\ndemonstrate the usefulness of these theoretical results through uniform,\nin-probability guarantees for two machine learning tasks of current interest:\ninterval prediction for transductive transfer learning and novelty detection\nbased on two-class classification."}, "http://arxiv.org/abs/2310.18212": {"title": "Robustness of Algorithms for Causal Structure Learning to Hyperparameter Choice", "link": "http://arxiv.org/abs/2310.18212", "description": "Hyperparameters play a critical role in machine learning. Hyperparameter\ntuning can make the difference between state-of-the-art and poor prediction\nperformance for any algorithm, but it is particularly challenging for structure\nlearning due to its unsupervised nature. As a result, hyperparameter tuning is\noften neglected in favour of using the default values provided by a particular\nimplementation of an algorithm. While there have been numerous studies on\nperformance evaluation of causal discovery algorithms, how hyperparameters\naffect individual algorithms, as well as the choice of the best algorithm for a\nspecific problem, has not been studied in depth before. This work addresses\nthis gap by investigating the influence of hyperparameters on causal structure\nlearning tasks. Specifically, we perform an empirical evaluation of\nhyperparameter selection for some seminal learning algorithms on datasets of\nvarying levels of complexity. We find that, while the choice of algorithm\nremains crucial to obtaining state-of-the-art performance, hyperparameter\nselection in ensemble settings strongly influences the choice of algorithm, in\nthat a poor choice of hyperparameters can lead to analysts using algorithms\nwhich do not give state-of-the-art performance for their data."}, "http://arxiv.org/abs/2310.18261": {"title": "Label Shift Estimators for Non-Ignorable Missing Data", "link": "http://arxiv.org/abs/2310.18261", "description": "We consider the problem of estimating the mean of a random variable Y subject\nto non-ignorable missingness, i.e., where the missingness mechanism depends on\nY . We connect the auxiliary proxy variable framework for non-ignorable\nmissingness (West and Little, 2013) to the label shift setting (Saerens et al.,\n2002). Exploiting this connection, we construct an estimator for non-ignorable\nmissing data that uses high-dimensional covariates (or proxies) without the\nneed for a generative model. In synthetic and semi-synthetic experiments, we\nstudy the behavior of the proposed estimator, comparing it to commonly used\nignorable estimators in both well-specified and misspecified settings.\nAdditionally, we develop a score to assess how consistent the data are with the\nlabel shift assumption. We use our approach to estimate disease prevalence\nusing a large health survey, comparing ignorable and non-ignorable approaches.\nWe show that failing to account for non-ignorable missingness can have profound\nconsequences on conclusions drawn from non-representative samples."}, "http://arxiv.org/abs/2102.12698": {"title": "Improving the Hosmer-Lemeshow Goodness-of-Fit Test in Large Models with Replicated Trials", "link": "http://arxiv.org/abs/2102.12698", "description": "The Hosmer-Lemeshow (HL) test is a commonly used global goodness-of-fit (GOF)\ntest that assesses the quality of the overall fit of a logistic regression\nmodel. In this paper, we give results from simulations showing that the type 1\nerror rate (and hence power) of the HL test decreases as model complexity\ngrows, provided that the sample size remains fixed and binary replicates are\npresent in the data. We demonstrate that the generalized version of the HL test\nby Surjanovic et al. (2020) can offer some protection against this power loss.\nWe conclude with a brief discussion explaining the behaviour of the HL test,\nalong with some guidance on how to choose between the two tests."}, "http://arxiv.org/abs/2110.04852": {"title": "Mixture representations and Bayesian nonparametric inference for likelihood ratio ordered distributions", "link": "http://arxiv.org/abs/2110.04852", "description": "In this article, we introduce mixture representations for likelihood ratio\nordered distributions. Essentially, the ratio of two probability densities, or\nmass functions, is monotone if and only if one can be expressed as a mixture of\none-sided truncations of the other. To illustrate the practical value of the\nmixture representations, we address the problem of density estimation for\nlikelihood ratio ordered distributions. In particular, we propose a\nnonparametric Bayesian solution which takes advantage of the mixture\nrepresentations. The prior distribution is constructed from Dirichlet process\nmixtures and has large support on the space of pairs of densities satisfying\nthe monotone ratio constraint. Posterior consistency holds under reasonable\nconditions on the prior specification and the true unknown densities. To our\nknowledge, this is the first posterior consistency result in the literature on\norder constrained inference. With a simple modification to the prior\ndistribution, we can test the equality of two distributions against the\nalternative of likelihood ratio ordering. We develop a Markov chain Monte Carlo\nalgorithm for posterior inference and demonstrate the method in a biomedical\napplication."}, "http://arxiv.org/abs/2207.08911": {"title": "Deeply-Learned Generalized Linear Models with Missing Data", "link": "http://arxiv.org/abs/2207.08911", "description": "Deep Learning (DL) methods have dramatically increased in popularity in\nrecent years, with significant growth in their application to supervised\nlearning problems in the biomedical sciences. However, the greater prevalence\nand complexity of missing data in modern biomedical datasets present\nsignificant challenges for DL methods. Here, we provide a formal treatment of\nmissing data in the context of deeply learned generalized linear models, a\nsupervised DL architecture for regression and classification problems. We\npropose a new architecture, \\textit{dlglm}, that is one of the first to be able\nto flexibly account for both ignorable and non-ignorable patterns of\nmissingness in input features and response at training time. We demonstrate\nthrough statistical simulation that our method outperforms existing approaches\nfor supervised learning tasks in the presence of missing not at random (MNAR)\nmissingness. We conclude with a case study of a Bank Marketing dataset from the\nUCI Machine Learning Repository, in which we predict whether clients subscribed\nto a product based on phone survey data. Supplementary materials for this\narticle are available online."}, "http://arxiv.org/abs/2208.04627": {"title": "Causal Effect Identification in Uncertain Causal Networks", "link": "http://arxiv.org/abs/2208.04627", "description": "Causal identification is at the core of the causal inference literature,\nwhere complete algorithms have been proposed to identify causal queries of\ninterest. The validity of these algorithms hinges on the restrictive assumption\nof having access to a correctly specified causal structure. In this work, we\nstudy the setting where a probabilistic model of the causal structure is\navailable. Specifically, the edges in a causal graph exist with uncertainties\nwhich may, for example, represent degree of belief from domain experts.\nAlternatively, the uncertainty about an edge may reflect the confidence of a\nparticular statistical test. The question that naturally arises in this setting\nis: Given such a probabilistic graph and a specific causal effect of interest,\nwhat is the subgraph which has the highest plausibility and for which the\ncausal effect is identifiable? We show that answering this question reduces to\nsolving an NP-complete combinatorial optimization problem which we call the\nedge ID problem. We propose efficient algorithms to approximate this problem\nand evaluate them against both real-world networks and randomly generated\ngraphs."}, "http://arxiv.org/abs/2211.00268": {"title": "Stacking designs: designing multi-fidelity computer experiments with target predictive accuracy", "link": "http://arxiv.org/abs/2211.00268", "description": "In an era where scientific experiments can be very costly, multi-fidelity\nemulators provide a useful tool for cost-efficient predictive scientific\ncomputing. For scientific applications, the experimenter is often limited by a\ntight computational budget, and thus wishes to (i) maximize predictive power of\nthe multi-fidelity emulator via a careful design of experiments, and (ii)\nensure this model achieves a desired error tolerance with some notion of\nconfidence. Existing design methods, however, do not jointly tackle objectives\n(i) and (ii). We propose a novel stacking design approach that addresses both\ngoals. A multi-level reproducing kernel Hilbert space (RKHS) interpolator is\nfirst introduced to build the emulator, under which our stacking design\nprovides a sequential approach for designing multi-fidelity runs such that a\ndesired prediction error of $\\epsilon &gt; 0$ is met under regularity assumptions.\nWe then prove a novel cost complexity theorem that, under this multi-level\ninterpolator, establishes a bound on the computation cost (for training data\nsimulation) needed to achieve a prediction bound of $\\epsilon$. This result\nprovides novel insights on conditions under which the proposed multi-fidelity\napproach improves upon a conventional RKHS interpolator which relies on a\nsingle fidelity level. Finally, we demonstrate the effectiveness of stacking\ndesigns in a suite of simulation experiments and an application to finite\nelement analysis."}, "http://arxiv.org/abs/2211.05357": {"title": "Bayesian score calibration for approximate models", "link": "http://arxiv.org/abs/2211.05357", "description": "Scientists continue to develop increasingly complex mechanistic models to\nreflect their knowledge more realistically. Statistical inference using these\nmodels can be challenging since the corresponding likelihood function is often\nintractable and model simulation may be computationally burdensome.\nFortunately, in many of these situations, it is possible to adopt a surrogate\nmodel or approximate likelihood function. It may be convenient to conduct\nBayesian inference directly with the surrogate, but this can result in bias and\npoor uncertainty quantification. In this paper we propose a new method for\nadjusting approximate posterior samples to reduce bias and produce more\naccurate uncertainty quantification. We do this by optimizing a transform of\nthe approximate posterior that maximizes a scoring rule. Our approach requires\nonly a (fixed) small number of complex model simulations and is numerically\nstable. We demonstrate good performance of the new method on several examples\nof increasing complexity."}, "http://arxiv.org/abs/2302.00993": {"title": "Unpaired Multi-Domain Causal Representation Learning", "link": "http://arxiv.org/abs/2302.00993", "description": "The goal of causal representation learning is to find a representation of\ndata that consists of causally related latent variables. We consider a setup\nwhere one has access to data from multiple domains that potentially share a\ncausal representation. Crucially, observations in different domains are assumed\nto be unpaired, that is, we only observe the marginal distribution in each\ndomain but not their joint distribution. In this paper, we give sufficient\nconditions for identifiability of the joint distribution and the shared causal\ngraph in a linear setup. Identifiability holds if we can uniquely recover the\njoint distribution and the shared causal representation from the marginal\ndistributions in each domain. We transform our identifiability results into a\npractical method to recover the shared latent causal graph."}, "http://arxiv.org/abs/2303.17277": {"title": "Cross-temporal probabilistic forecast reconciliation: Methodological and practical issues", "link": "http://arxiv.org/abs/2303.17277", "description": "Forecast reconciliation is a post-forecasting process that involves\ntransforming a set of incoherent forecasts into coherent forecasts which\nsatisfy a given set of linear constraints for a multivariate time series. In\nthis paper we extend the current state-of-the-art cross-sectional probabilistic\nforecast reconciliation approach to encompass a cross-temporal framework, where\ntemporal constraints are also applied. Our proposed methodology employs both\nparametric Gaussian and non-parametric bootstrap approaches to draw samples\nfrom an incoherent cross-temporal distribution. To improve the estimation of\nthe forecast error covariance matrix, we propose using multi-step residuals,\nespecially in the time dimension where the usual one-step residuals fail. To\naddress high-dimensionality issues, we present four alternatives for the\ncovariance matrix, where we exploit the two-fold nature (cross-sectional and\ntemporal) of the cross-temporal structure, and introduce the idea of\noverlapping residuals. We assess the effectiveness of the proposed\ncross-temporal reconciliation approaches through a simulation study that\ninvestigates their theoretical and empirical properties and two forecasting\nexperiments, using the Australian GDP and the Australian Tourism Demand\ndatasets. For both applications, the optimal cross-temporal reconciliation\napproaches significantly outperform the incoherent base forecasts in terms of\nthe Continuous Ranked Probability Score and the Energy Score. Overall, the\nresults highlight the potential of the proposed methods to improve the accuracy\nof probabilistic forecasts and to address the challenge of integrating\ndisparate scenarios while coherently taking into account short-term\noperational, medium-term tactical, and long-term strategic planning."}, "http://arxiv.org/abs/2309.07867": {"title": "Beta Diffusion", "link": "http://arxiv.org/abs/2309.07867", "description": "We introduce beta diffusion, a novel generative modeling method that\nintegrates demasking and denoising to generate data within bounded ranges.\nUsing scaled and shifted beta distributions, beta diffusion utilizes\nmultiplicative transitions over time to create both forward and reverse\ndiffusion processes, maintaining beta distributions in both the forward\nmarginals and the reverse conditionals, given the data at any point in time.\nUnlike traditional diffusion-based generative models relying on additive\nGaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is\nmultiplicative and optimized with KL-divergence upper bounds (KLUBs) derived\nfrom the convexity of the KL divergence. We demonstrate that the proposed KLUBs\nare more effective for optimizing beta diffusion compared to negative ELBOs,\nwhich can also be derived as the KLUBs of the same KL divergence with its two\narguments swapped. The loss function of beta diffusion, expressed in terms of\nBregman divergence, further supports the efficacy of KLUBs for optimization.\nExperimental results on both synthetic data and natural images demonstrate the\nunique capabilities of beta diffusion in generative modeling of range-bounded\ndata and validate the effectiveness of KLUBs in optimizing diffusion models,\nthereby making them valuable additions to the family of diffusion-based\ngenerative models and the optimization techniques used to train them."}, "http://arxiv.org/abs/2310.18422": {"title": "Inference via Wild Bootstrap and Multiple Imputation under Fine-Gray Models with Incomplete Data", "link": "http://arxiv.org/abs/2310.18422", "description": "Fine-Gray models specify the subdistribution hazards for one out of multiple\ncompeting risks to be proportional. The estimators of parameters and cumulative\nincidence functions under Fine-Gray models have a simpler structure when data\nare censoring-complete than when they are more generally incomplete. This paper\nconsiders the case of incomplete data but it exploits the above-mentioned\nsimpler estimator structure for which there exists a wild bootstrap approach\nfor inferential purposes. The present idea is to link the methodology under\ncensoring-completeness with the more general right-censoring regime with the\nhelp of multiple imputation. In a simulation study, this approach is compared\nto the estimation procedure proposed in the original paper by Fine and Gray\nwhen it is combined with a bootstrap approach. An application to a data set\nabout hospital-acquired infections illustrates the method."}, "http://arxiv.org/abs/2310.18474": {"title": "Robust Bayesian Graphical Regression Models for Assessing Tumor Heterogeneity in Proteomic Networks", "link": "http://arxiv.org/abs/2310.18474", "description": "Graphical models are powerful tools to investigate complex dependency\nstructures in high-throughput datasets. However, most existing graphical models\nmake one of the two canonical assumptions: (i) a homogeneous graph with a\ncommon network for all subjects; or (ii) an assumption of normality especially\nin the context of Gaussian graphical models. Both assumptions are restrictive\nand can fail to hold in certain applications such as proteomic networks in\ncancer. To this end, we propose an approach termed robust Bayesian graphical\nregression (rBGR) to estimate heterogeneous graphs for non-normally distributed\ndata. rBGR is a flexible framework that accommodates non-normality through\nrandom marginal transformations and constructs covariate-dependent graphs to\naccommodate heterogeneity through graphical regression techniques. We formulate\na new characterization of edge dependencies in such models called conditional\nsign independence with covariates along with an efficient posterior sampling\nalgorithm. In simulation studies, we demonstrate that rBGR outperforms existing\ngraphical regression models for data generated under various levels of\nnon-normality in both edge and covariate selection. We use rBGR to assess\nproteomic networks across two cancers: lung and ovarian, to systematically\ninvestigate the effects of immunogenic heterogeneity within tumors. Our\nanalyses reveal several important protein-protein interactions that are\ndifferentially impacted by the immune cell abundance; some corroborate existing\nbiological knowledge whereas others are novel findings."}, "http://arxiv.org/abs/2310.18500": {"title": "Designing Randomized Experiments to Predict Unit-Specific Treatment Effects", "link": "http://arxiv.org/abs/2310.18500", "description": "Typically, a randomized experiment is designed to test a hypothesis about the\naverage treatment effect and sometimes hypotheses about treatment effect\nvariation. The results of such a study may then be used to inform policy and\npractice for units not in the study. In this paper, we argue that given this\nuse, randomized experiments should instead be designed to predict unit-specific\ntreatment effects in a well-defined population. We then consider how different\nsampling processes and models affect the bias, variance, and mean squared\nprediction error of these predictions. The results indicate, for example, that\nproblems of generalizability (differences between samples and populations) can\ngreatly affect bias both in predictive models and in measures of error in these\nmodels. We also examine when the average treatment effect estimate outperforms\nunit-specific treatment effect predictive models and implications of this for\nplanning studies."}, "http://arxiv.org/abs/2310.18527": {"title": "Multiple Imputation Method for High-Dimensional Neuroimaging Data", "link": "http://arxiv.org/abs/2310.18527", "description": "Missingness is a common issue for neuroimaging data, and neglecting it in\ndownstream statistical analysis can introduce bias and lead to misguided\ninferential conclusions. It is therefore crucial to conduct appropriate\nstatistical methods to address this issue. While multiple imputation is a\npopular technique for handling missing data, its application to neuroimaging\ndata is hindered by high dimensionality and complex dependence structures of\nmultivariate neuroimaging variables. To tackle this challenge, we propose a\nnovel approach, named High Dimensional Multiple Imputation (HIMA), based on\nBayesian models. HIMA develops a new computational strategy for sampling large\ncovariance matrices based on a robustly estimated posterior mode, which\ndrastically enhances computational efficiency and numerical stability. To\nassess the effectiveness of HIMA, we conducted extensive simulation studies and\nreal-data analysis using neuroimaging data from a Schizophrenia study. HIMA\nshowcases a computational efficiency improvement of over 2000 times when\ncompared to traditional approaches, while also producing imputed datasets with\nimproved precision and stability."}, "http://arxiv.org/abs/2310.18533": {"title": "Evaluating the effects of high-throughput structural neuroimaging predictors on whole-brain functional connectome outcomes via network-based vector-on-matrix regression", "link": "http://arxiv.org/abs/2310.18533", "description": "The joint analysis of multimodal neuroimaging data is critical in the field\nof brain research because it reveals complex interactive relationships between\nneurobiological structures and functions. In this study, we focus on\ninvestigating the effects of structural imaging (SI) features, including white\nmatter micro-structure integrity (WMMI) and cortical thickness, on the whole\nbrain functional connectome (FC) network. To achieve this goal, we propose a\nnetwork-based vector-on-matrix regression model to characterize the FC-SI\nassociation patterns. We have developed a novel multi-level dense bipartite and\nclique subgraph extraction method to identify which subsets of spatially\nspecific SI features intensively influence organized FC sub-networks. The\nproposed method can simultaneously identify highly correlated\nstructural-connectomic association patterns and suppress false positive\nfindings while handling millions of potential interactions. We apply our method\nto a multimodal neuroimaging dataset of 4,242 participants from the UK Biobank\nto evaluate the effects of whole-brain WMMI and cortical thickness on the\nresting-state FC. The results reveal that the WMMI on corticospinal tracts and\ninferior cerebellar peduncle significantly affect functional connections of\nsensorimotor, salience, and executive sub-networks with an average correlation\nof 0.81 (p&lt;0.001)."}, "http://arxiv.org/abs/2310.18536": {"title": "Efficient Fully Bayesian Approach to Brain Activity Mapping with Complex-Valued fMRI Data", "link": "http://arxiv.org/abs/2310.18536", "description": "Functional magnetic resonance imaging (fMRI) enables indirect detection of\nbrain activity changes via the blood-oxygen-level-dependent (BOLD) signal.\nConventional analysis methods mainly rely on the real-valued magnitude of these\nsignals. In contrast, research suggests that analyzing both real and imaginary\ncomponents of the complex-valued fMRI (cv-fMRI) signal provides a more holistic\napproach that can increase power to detect neuronal activation. We propose a\nfully Bayesian model for brain activity mapping with cv-fMRI data. Our model\naccommodates temporal and spatial dynamics. Additionally, we propose a\ncomputationally efficient sampling algorithm, which enhances processing speed\nthrough image partitioning. Our approach is shown to be computationally\nefficient via image partitioning and parallel computation while being\ncompetitive with state-of-the-art methods. We support these claims with both\nsimulated numerical studies and an application to real cv-fMRI data obtained\nfrom a finger-tapping experiment."}, "http://arxiv.org/abs/2310.18556": {"title": "Design-Based Causal Inference with Missing Outcomes: Missingness Mechanisms, Imputation-Assisted Randomization Tests, and Covariate Adjustment", "link": "http://arxiv.org/abs/2310.18556", "description": "Design-based causal inference is one of the most widely used frameworks for\ntesting causal null hypotheses or inferring about causal parameters from\nexperimental or observational data. The most significant merit of design-based\ncausal inference is that its statistical validity only comes from the study\ndesign (e.g., randomization design) and does not require assuming any\noutcome-generating distributions or models. Although immune to model\nmisspecification, design-based causal inference can still suffer from other\ndata challenges, among which missingness in outcomes is a significant one.\nHowever, compared with model-based causal inference, outcome missingness in\ndesign-based causal inference is much less studied, largely due to the\nchallenge that design-based causal inference does not assume any outcome\ndistributions/models and, therefore, cannot directly adopt any existing\nmodel-based approaches for missing data. To fill this gap, we systematically\nstudy the missing outcomes problem in design-based causal inference. First, we\nuse the potential outcomes framework to clarify the minimal assumption\n(concerning the outcome missingness mechanism) needed for conducting\nfinite-population-exact randomization tests for the null effect (i.e., Fisher's\nsharp null) and that needed for constructing finite-population-exact confidence\nsets with missing outcomes. Second, we propose a general framework called\n``imputation and re-imputation\" for conducting finite-population-exact\nrandomization tests in design-based causal studies with missing outcomes. Our\nframework can incorporate any existing outcome imputation algorithms and\nmeanwhile guarantee finite-population-exact type-I error rate control. Third,\nwe extend our framework to conduct covariate adjustment in an exact\nrandomization test with missing outcomes and to construct\nfinite-population-exact confidence sets with missing outcomes."}, "http://arxiv.org/abs/2310.18563": {"title": "Covariate Balancing and the Equivalence of Weighting and Doubly Robust Estimators of Average Treatment Effects", "link": "http://arxiv.org/abs/2310.18563", "description": "We show that when the propensity score is estimated using a suitable\ncovariate balancing procedure, the commonly used inverse probability weighting\n(IPW) estimator, augmented inverse probability weighting (AIPW) with linear\nconditional mean, and inverse probability weighted regression adjustment\n(IPWRA) with linear conditional mean are all numerically the same for\nestimating the average treatment effect (ATE) or the average treatment effect\non the treated (ATT). Further, suitably chosen covariate balancing weights are\nautomatically normalized, which means that normalized and unnormalized versions\nof IPW and AIPW are identical. For estimating the ATE, the weights that achieve\nthe algebraic equivalence of IPW, AIPW, and IPWRA are based on propensity\nscores estimated using the inverse probability tilting (IPT) method of Graham,\nPinto and Egel (2012). For the ATT, the weights are obtained using the\ncovariate balancing propensity score (CBPS) method developed in Imai and\nRatkovic (2014). These equivalences also make covariate balancing methods\nattractive when the treatment is confounded and one is interested in the local\naverage treatment effect."}, "http://arxiv.org/abs/2310.18611": {"title": "Sequential Kalman filter for fast online changepoint detection in longitudinal health records", "link": "http://arxiv.org/abs/2310.18611", "description": "This article introduces the sequential Kalman filter, a computationally\nscalable approach for online changepoint detection with temporally correlated\ndata. The temporal correlation was not considered in the Bayesian online\nchangepoint detection approach due to the large computational cost. Motivated\nby detecting COVID-19 infections for dialysis patients from massive\nlongitudinal health records with a large number of covariates, we develop a\nscalable approach to detect multiple changepoints from correlated data by\nsequentially stitching Kalman filters of subsequences to compute the joint\ndistribution of the observations, which has linear computational complexity\nwith respect to the number of observations between the last detected\nchangepoint and the current observation at each time point, without\napproximating the likelihood function. Compared to other online changepoint\ndetection methods, simulated experiments show that our approach is more precise\nin detecting single or multiple changes in mean, variance, or correlation for\ntemporally correlated data. Furthermore, we propose a new way to integrate\nclassification and changepoint detection approaches that improve the detection\ndelay and accuracy for detecting COVID-19 infection compared to other\nalternatives."}, "http://arxiv.org/abs/2310.18733": {"title": "Threshold detection under a semiparametric regression model", "link": "http://arxiv.org/abs/2310.18733", "description": "Linear regression models have been extensively considered in the literature.\nHowever, in some practical applications they may not be appropriate all over\nthe range of the covariate. In this paper, a more flexible model is introduced\nby considering a regression model $Y=r(X)+\\varepsilon$ where the regression\nfunction $r(\\cdot)$ is assumed to be linear for large values in the domain of\nthe predictor variable $X$. More precisely, we assume that\n$r(x)=\\alpha_0+\\beta_0 x$ for $x&gt; u_0$, where the value $u_0$ is identified as\nthe smallest value satisfying such a property. A penalized procedure is\nintroduced to estimate the threshold $u_0$. The considered proposal focusses on\na semiparametric approach since no parametric model is assumed for the\nregression function for values smaller than $u_0$. Consistency properties of\nboth the threshold estimator and the estimators of $(\\alpha_0,\\beta_0)$ are\nderived, under mild assumptions. Through a numerical study, the small sample\nproperties of the proposed procedure and the importance of introducing a\npenalization are investigated. The analysis of a real data set allows us to\ndemonstrate the usefulness of the penalized estimators."}, "http://arxiv.org/abs/2310.18766": {"title": "Discussion of ''A Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks''", "link": "http://arxiv.org/abs/2310.18766", "description": "This review discusses the paper ''A Tale of Two Datasets: Representativeness\nand Generalisability of Inference for Samples of Networks'' by Krivitsky,\nColetti, and Hens, published in the Journal of the American Statistical\nAssociation in 2023."}, "http://arxiv.org/abs/2310.18836": {"title": "Design of Cluster-Randomized Trials with Cross-Cluster Interference", "link": "http://arxiv.org/abs/2310.18836", "description": "Cluster-randomized trials often involve units that are irregularly\ndistributed in space without well-separated communities. In these settings,\ncluster construction is a critical aspect of the design due to the potential\nfor cross-cluster interference. The existing literature relies on partial\ninterference models, which take clusters as given and assume no cross-cluster\ninterference. We relax this assumption by allowing interference to decay with\ngeographic distance between units. This induces a bias-variance trade-off:\nconstructing fewer, larger clusters reduces bias due to interference but\nincreases variance. We propose new estimators that exclude units most\npotentially impacted by cross-cluster interference and show that this\nsubstantially reduces asymptotic bias relative to conventional\ndifference-in-means estimators. We then study the design of clusters to\noptimize the estimators' rates of convergence. We provide formal justification\nfor a new design that chooses the number of clusters to balance the asymptotic\nbias and variance of our estimators and uses unsupervised learning to automate\ncluster construction."}, "http://arxiv.org/abs/2310.18858": {"title": "Estimating a function of the scale parameter in a gamma distribution with bounded variance", "link": "http://arxiv.org/abs/2310.18858", "description": "Given a gamma population with known shape parameter $\\alpha$, we develop a\ngeneral theory for estimating a function $g(\\cdot)$ of the scale parameter\n$\\beta$ with bounded variance. We begin by defining a sequential sampling\nprocedure with $g(\\cdot)$ satisfying some desired condition in proposing the\nstopping rule, and show the procedure enjoys appealing asymptotic properties.\nAfter these general conditions, we substitute $g(\\cdot)$ with specific\nfunctions including the gamma mean, the gamma variance, the gamma rate\nparameter, and a gamma survival probability as four possible illustrations. For\neach illustration, Monte Carlo simulations are carried out to justify the\nremarkable performance of our proposed sequential procedure. This is further\nsubstantiated with a real data study on weights of newly born babies."}, "http://arxiv.org/abs/2310.18875": {"title": "Feature calibration for computer models", "link": "http://arxiv.org/abs/2310.18875", "description": "Computer model calibration involves using partial and imperfect observations\nof the real world to learn which values of a model's input parameters lead to\noutputs that are consistent with real-world observations. When calibrating\nmodels with high-dimensional output (e.g. a spatial field), it is common to\nrepresent the output as a linear combination of a small set of basis vectors.\nOften, when trying to calibrate to such output, what is important to the\ncredibility of the model is that key emergent physical phenomena are\nrepresented, even if not faithfully or in the right place. In these cases,\ncomparison of model output and data in a linear subspace is inappropriate and\nwill usually lead to poor model calibration. To overcome this, we present\nkernel-based history matching (KHM), generalising the meaning of the technique\nsufficiently to be able to project model outputs and observations into a\nhigher-dimensional feature space, where patterns can be compared without their\nlocation necessarily being fixed. We develop the technical methodology, present\nan expert-driven kernel selection algorithm, and then apply the techniques to\nthe calibration of boundary layer clouds for the French climate model IPSL-CM."}, "http://arxiv.org/abs/2310.18905": {"title": "Incorporating nonparametric methods for estimating causal excursion effects in mobile health with zero-inflated count outcomes", "link": "http://arxiv.org/abs/2310.18905", "description": "In the domain of mobile health, tailoring interventions for real-time\ndelivery is of paramount importance. Micro-randomized trials have emerged as\nthe \"gold-standard\" methodology for developing such interventions. Analyzing\ndata from these trials provides insights into the efficacy of interventions and\nthe potential moderation by specific covariates. The \"causal excursion effect\",\na novel class of causal estimand, addresses these inquiries, backed by current\nsemiparametric inference techniques. Yet, existing methods mainly focus on\ncontinuous or binary data, leaving count data largely unexplored. The current\nwork is motivated by the Drink Less micro-randomized trial from the UK, which\nfocuses on a zero-inflated proximal outcome, the number of screen views in the\nsubsequent hour following the intervention decision point. In the current\npaper, we revisit the concept of causal excursion effects, specifically for\nzero-inflated count outcomes, and introduce novel estimation approaches that\nincorporate nonparametric techniques. Bidirectional asymptotics are derived for\nthe proposed estimators. Through extensive simulation studies, we evaluate the\nperformance of the proposed estimators. As an illustration, we also employ the\nproposed methods to the Drink Less trial data."}, "http://arxiv.org/abs/2310.18963": {"title": "Expectile-based conditional tail moments with covariates", "link": "http://arxiv.org/abs/2310.18963", "description": "Expectile, as the minimizer of an asymmetric quadratic loss function, is a\ncoherent risk measure and is helpful to use more information about the\ndistribution of the considered risk. In this paper, we propose a new risk\nmeasure by replacing quantiles by expectiles, called expectile-based\nconditional tail moment, and focus on the estimation of this new risk measure\nas the conditional survival function of the risk, given the risk exceeding the\nexpectile and given a value of the covariates, is heavy tail. Under some\nregular conditions, asymptotic properties of this new estimator are considered.\nThe extrapolated estimation of the conditional tail moments is also\ninvestigated. These results are illustrated both on simulated data and on a\nreal insurance data."}, "http://arxiv.org/abs/2310.19043": {"title": "Differentially Private Permutation Tests: Applications to Kernel Methods", "link": "http://arxiv.org/abs/2310.19043", "description": "Recent years have witnessed growing concerns about the privacy of sensitive\ndata. In response to these concerns, differential privacy has emerged as a\nrigorous framework for privacy protection, gaining widespread recognition in\nboth academic and industrial circles. While substantial progress has been made\nin private data analysis, existing methods often suffer from impracticality or\na significant loss of statistical efficiency. This paper aims to alleviate\nthese concerns in the context of hypothesis testing by introducing\ndifferentially private permutation tests. The proposed framework extends\nclassical non-private permutation tests to private settings, maintaining both\nfinite-sample validity and differential privacy in a rigorous manner. The power\nof the proposed test depends on the choice of a test statistic, and we\nestablish general conditions for consistency and non-asymptotic uniform power.\nTo demonstrate the utility and practicality of our framework, we focus on\nreproducing kernel-based test statistics and introduce differentially private\nkernel tests for two-sample and independence testing: dpMMD and dpHSIC. The\nproposed kernel tests are straightforward to implement, applicable to various\ntypes of data, and attain minimax optimal power across different privacy\nregimes. Our empirical evaluations further highlight their competitive power\nunder various synthetic and real-world scenarios, emphasizing their practical\nvalue. The code is publicly available to facilitate the implementation of our\nframework."}, "http://arxiv.org/abs/2310.19051": {"title": "A Survey of Methods for Estimating Hurst Exponent of Time Sequence", "link": "http://arxiv.org/abs/2310.19051", "description": "The Hurst exponent is a significant indicator for characterizing the\nself-similarity and long-term memory properties of time sequences. It has wide\napplications in physics, technologies, engineering, mathematics, statistics,\neconomics, psychology and so on. Currently, available methods for estimating\nthe Hurst exponent of time sequences can be divided into different categories:\ntime-domain methods and spectrum-domain methods based on the representation of\ntime sequence, linear regression methods and Bayesian methods based on\nparameter estimation methods. Although various methods are discussed in\nliterature, there are still some deficiencies: the descriptions of the\nestimation algorithms are just mathematics-oriented and the pseudo-codes are\nmissing; the effectiveness and accuracy of the estimation algorithms are not\nclear; the classification of estimation methods is not considered and there is\na lack of guidance for selecting the estimation methods. In this work, the\nemphasis is put on thirteen dominant methods for estimating the Hurst exponent.\nFor the purpose of decreasing the difficulty of implementing the estimation\nmethods with computer programs, the mathematical principles are discussed\nbriefly and the pseudo-codes of algorithms are presented with necessary\ndetails. It is expected that the survey could help the researchers to select,\nimplement and apply the estimation algorithms of interest in practical\nsituations in an easy way."}, "http://arxiv.org/abs/2310.19091": {"title": "Bridging the Gap: Towards an Expanded Toolkit for ML-Supported Decision-Making in the Public Sector", "link": "http://arxiv.org/abs/2310.19091", "description": "Machine Learning (ML) systems are becoming instrumental in the public sector,\nwith applications spanning areas like criminal justice, social welfare,\nfinancial fraud detection, and public health. While these systems offer great\npotential benefits to institutional decision-making processes, such as improved\nefficiency and reliability, they still face the challenge of aligning intricate\nand nuanced policy objectives with the precise formalization requirements\nnecessitated by ML models. In this paper, we aim to bridge the gap between ML\nand public sector decision-making by presenting a comprehensive overview of key\ntechnical challenges where disjunctions between policy goals and ML models\ncommonly arise. We concentrate on pivotal points of the ML pipeline that\nconnect the model to its operational environment, delving into the significance\nof representative training data and highlighting the importance of a model\nsetup that facilitates effective decision-making. Additionally, we link these\nchallenges with emerging methodological advancements, encompassing causal ML,\ndomain adaptation, uncertainty quantification, and multi-objective\noptimization, illustrating the path forward for harmonizing ML and public\nsector objectives."}, "http://arxiv.org/abs/2310.19114": {"title": "Sparse Fr\\'echet Sufficient Dimension Reduction with Graphical Structure Among Predictors", "link": "http://arxiv.org/abs/2310.19114", "description": "Fr\\'echet regression has received considerable attention to model\nmetric-space valued responses that are complex and non-Euclidean data, such as\nprobability distributions and vectors on the unit sphere. However, existing\nFr\\'echet regression literature focuses on the classical setting where the\npredictor dimension is fixed, and the sample size goes to infinity. This paper\nproposes sparse Fr\\'echet sufficient dimension reduction with graphical\nstructure among high-dimensional Euclidean predictors. In particular, we\npropose a convex optimization problem that leverages the graphical information\namong predictors and avoids inverting the high-dimensional covariance matrix.\nWe also provide the Alternating Direction Method of Multipliers (ADMM)\nalgorithm to solve the optimization problem. Theoretically, the proposed method\nachieves subspace estimation and variable selection consistency under suitable\nconditions. Extensive simulations and a real data analysis are carried out to\nillustrate the finite-sample performance of the proposed method."}, "http://arxiv.org/abs/2310.19246": {"title": "A spectral regularisation framework for latent variable models designed for single channel applications", "link": "http://arxiv.org/abs/2310.19246", "description": "Latent variable models (LVMs) are commonly used to capture the underlying\ndependencies, patterns, and hidden structure in observed data. Source\nduplication is a by-product of the data hankelisation pre-processing step\ncommon to single channel LVM applications, which hinders practical LVM\nutilisation. In this article, a Python package titled\nspectrally-regularised-LVMs is presented. The proposed package addresses the\nsource duplication issue via the addition of a novel spectral regularisation\nterm. This package provides a framework for spectral regularisation in single\nchannel LVM applications, thereby making it easier to investigate and utilise\nLVMs with spectral regularisation. This is achieved via the use of symbolic or\nexplicit representations of potential LVM objective functions which are\nincorporated into a framework that uses spectral regularisation during the LVM\nparameter estimation process. The objective of this package is to provide a\nconsistent linear LVM optimisation framework which incorporates spectral\nregularisation and caters to single channel time-series applications."}, "http://arxiv.org/abs/2310.19253": {"title": "Flow-based Distributionally Robust Optimization", "link": "http://arxiv.org/abs/2310.19253", "description": "We present a computationally efficient framework, called \\texttt{FlowDRO},\nfor solving flow-based distributionally robust optimization (DRO) problems with\nWasserstein uncertainty sets, when requiring the worst-case distribution (also\ncalled the Least Favorable Distribution, LFD) to be continuous so that the\nalgorithm can be scalable to problems with larger sample sizes and achieve\nbetter generalization capability for the induced robust algorithms. To tackle\nthe computationally challenging infinitely dimensional optimization problem, we\nleverage flow-based models, continuous-time invertible transport maps between\nthe data distribution and the target distribution, and develop a Wasserstein\nproximal gradient flow type of algorithm. In practice, we parameterize the\ntransport maps by a sequence of neural networks progressively trained in blocks\nby gradient descent. Our computational framework is general, can handle\nhigh-dimensional data with large sample sizes, and can be useful for various\napplications. We demonstrate its usage in adversarial learning,\ndistributionally robust hypothesis testing, and a new mechanism for data-driven\ndistribution perturbation differential privacy, where the proposed method gives\nstrong empirical performance on real high-dimensional data."}, "http://arxiv.org/abs/2310.19343": {"title": "Quantile Super Learning for independent and online settings with application to solar power forecasting", "link": "http://arxiv.org/abs/2310.19343", "description": "Estimating quantiles of an outcome conditional on covariates is of\nfundamental interest in statistics with broad application in probabilistic\nprediction and forecasting. We propose an ensemble method for conditional\nquantile estimation, Quantile Super Learning, that combines predictions from\nmultiple candidate algorithms based on their empirical performance measured\nwith respect to a cross-validated empirical risk of the quantile loss function.\nWe present theoretical guarantees for both iid and online data scenarios. The\nperformance of our approach for quantile estimation and in forming prediction\nintervals is tested in simulation studies. Two case studies related to solar\nenergy are used to illustrate Quantile Super Learning: in an iid setting, we\npredict the physical properties of perovskite materials for photovoltaic cells,\nand in an online setting we forecast ground solar irradiance based on output\nfrom dynamic weather ensemble models."}, "http://arxiv.org/abs/2310.19433": {"title": "Ordinal classification for interval-valued data and interval-valued functional data", "link": "http://arxiv.org/abs/2310.19433", "description": "The aim of ordinal classification is to predict the ordered labels of the\noutput from a set of observed inputs. Interval-valued data refers to data in\nthe form of intervals. For the first time, interval-valued data and\ninterval-valued functional data are considered as inputs in an ordinal\nclassification problem. Six ordinal classifiers for interval data and\ninterval-valued functional data are proposed. Three of them are parametric, one\nof them is based on ordinal binary decompositions and the other two are based\non ordered logistic regression. The other three methods are based on the use of\ndistances between interval data and kernels on interval data. One of the\nmethods uses the weighted $k$-nearest-neighbor technique for ordinal\nclassification. Another method considers kernel principal component analysis\nplus an ordinal classifier. And the sixth method, which is the method that\nperforms best, uses a kernel-induced ordinal random forest. They are compared\nwith na\\\"ive approaches in an extensive experimental study with synthetic and\noriginal real data sets, about human global development, and weather data. The\nresults show that considering ordering and interval-valued information improves\nthe accuracy. The source code and data sets are available at\nhttps://github.com/aleixalcacer/OCFIVD."}, "http://arxiv.org/abs/2310.19435": {"title": "A novel characterization of structures in smooth regression curves: from a viewpoint of persistent homology", "link": "http://arxiv.org/abs/2310.19435", "description": "We characterize structures such as monotonicity, convexity, and modality in\nsmooth regression curves using persistent homology. Persistent homology is a\nkey tool in topological data analysis that detects higher dimensional\ntopological features such as connected components and holes (cycles or loops)\nin the data. In other words, persistent homology is a multiscale version of\nhomology that characterizes sets based on the connected components and holes.\nWe use super-level sets of functions to extract geometric features via\npersistent homology. In particular, we explore structures in regression curves\nvia the persistent homology of super-level sets of a function, where the\nfunction of interest is - the first derivative of the regression function.\n\nIn the course of this study, we extend an existing procedure of estimating\nthe persistent homology for the first derivative of a regression function and\nestablish its consistency. Moreover, as an application of the proposed\nmethodology, we demonstrate that the persistent homology of the derivative of a\nfunction can reveal hidden structures in the function that are not visible from\nthe persistent homology of the function itself. In addition, we also illustrate\nthat the proposed procedure can be used to compare the shapes of two or more\nregression curves which is not possible merely from the persistent homology of\nthe function itself."}, "http://arxiv.org/abs/2310.19519": {"title": "A General Neural Causal Model for Interactive Recommendation", "link": "http://arxiv.org/abs/2310.19519", "description": "Survivor bias in observational data leads the optimization of recommender\nsystems towards local optima. Currently most solutions re-mines existing\nhuman-system collaboration patterns to maximize longer-term satisfaction by\nreinforcement learning. However, from the causal perspective, mitigating\nsurvivor effects requires answering a counterfactual problem, which is\ngenerally unidentifiable and inestimable. In this work, we propose a neural\ncausal model to achieve counterfactual inference. Specifically, we first build\na learnable structural causal model based on its available graphical\nrepresentations which qualitatively characterizes the preference transitions.\nMitigation of the survivor bias is achieved though counterfactual consistency.\nTo identify the consistency, we use the Gumbel-max function as structural\nconstrains. To estimate the consistency, we apply reinforcement optimizations,\nand use Gumbel-Softmax as a trade-off to get a differentiable function. Both\ntheoretical and empirical studies demonstrate the effectiveness of our\nsolution."}, "http://arxiv.org/abs/2310.19621": {"title": "A Bayesian Methodology for Estimation for Sparse Canonical Correlation", "link": "http://arxiv.org/abs/2310.19621", "description": "It can be challenging to perform an integrative statistical analysis of\nmulti-view high-dimensional data acquired from different experiments on each\nsubject who participated in a joint study. Canonical Correlation Analysis (CCA)\nis a statistical procedure for identifying relationships between such data\nsets. In that context, Structured Sparse CCA (ScSCCA) is a rapidly emerging\nmethodological area that aims for robust modeling of the interrelations between\nthe different data modalities by assuming the corresponding CCA directional\nvectors to be sparse. Although it is a rapidly growing area of statistical\nmethodology development, there is a need for developing related methodologies\nin the Bayesian paradigm. In this manuscript, we propose a novel ScSCCA\napproach where we employ a Bayesian infinite factor model and aim to achieve\nrobust estimation by encouraging sparsity in two different levels of the\nmodeling framework. Firstly, we utilize a multiplicative Half-Cauchy process\nprior to encourage sparsity at the level of the latent variable loading\nmatrices. Additionally, we promote further sparsity in the covariance matrix by\nusing graphical horseshoe prior or diagonal structure. We conduct multiple\nsimulations to compare the performance of the proposed method with that of\nother frequently used CCA procedures, and we apply the developed procedures to\nanalyze multi-omics data arising from a breast cancer study."}, "http://arxiv.org/abs/2310.19683": {"title": "An Online Bootstrap for Time Series", "link": "http://arxiv.org/abs/2310.19683", "description": "Resampling methods such as the bootstrap have proven invaluable in the field\nof machine learning. However, the applicability of traditional bootstrap\nmethods is limited when dealing with large streams of dependent data, such as\ntime series or spatially correlated observations. In this paper, we propose a\nnovel bootstrap method that is designed to account for data dependencies and\ncan be executed online, making it particularly suitable for real-time\napplications. This method is based on an autoregressive sequence of\nincreasingly dependent resampling weights. We prove the theoretical validity of\nthe proposed bootstrap scheme under general conditions. We demonstrate the\neffectiveness of our approach through extensive simulations and show that it\nprovides reliable uncertainty quantification even in the presence of complex\ndata dependencies. Our work bridges the gap between classical resampling\ntechniques and the demands of modern data analysis, providing a valuable tool\nfor researchers and practitioners in dynamic, data-rich environments."}, "http://arxiv.org/abs/2310.19787": {"title": "$e^{\\text{RPCA}}$: Robust Principal Component Analysis for Exponential Family Distributions", "link": "http://arxiv.org/abs/2310.19787", "description": "Robust Principal Component Analysis (RPCA) is a widely used method for\nrecovering low-rank structure from data matrices corrupted by significant and\nsparse outliers. These corruptions may arise from occlusions, malicious\ntampering, or other causes for anomalies, and the joint identification of such\ncorruptions with low-rank background is critical for process monitoring and\ndiagnosis. However, existing RPCA methods and their extensions largely do not\naccount for the underlying probabilistic distribution for the data matrices,\nwhich in many applications are known and can be highly non-Gaussian. We thus\npropose a new method called Robust Principal Component Analysis for Exponential\nFamily distributions ($e^{\\text{RPCA}}$), which can perform the desired\ndecomposition into low-rank and sparse matrices when such a distribution falls\nwithin the exponential family. We present a novel alternating direction method\nof multiplier optimization algorithm for efficient $e^{\\text{RPCA}}$\ndecomposition. The effectiveness of $e^{\\text{RPCA}}$ is then demonstrated in\ntwo applications: the first for steel sheet defect detection, and the second\nfor crime activity monitoring in the Atlanta metropolitan area."}, "http://arxiv.org/abs/2310.19788": {"title": "Locally Optimal Best Arm Identification with a Fixed Budget", "link": "http://arxiv.org/abs/2310.19788", "description": "This study investigates the problem of identifying the best treatment arm, a\ntreatment arm with the highest expected outcome. We aim to identify the best\ntreatment arm with a lower probability of misidentification, which has been\nexplored under various names across numerous research fields, including\n\\emph{best arm identification} (BAI) and ordinal optimization. In our\nexperiments, the number of treatment-allocation rounds is fixed. In each round,\na decision-maker allocates a treatment arm to an experimental unit and observes\na corresponding outcome, which follows a Gaussian distribution with a variance\ndifferent among treatment arms. At the end of the experiment, we recommend one\nof the treatment arms as an estimate of the best treatment arm based on the\nobservations. The objective of the decision-maker is to design an experiment\nthat minimizes the probability of misidentifying the best treatment arm. With\nthis objective in mind, we develop lower bounds for the probability of\nmisidentification under the small-gap regime, where the gaps of the expected\noutcomes between the best and suboptimal treatment arms approach zero. Then,\nassuming that the variances are known, we design the\nGeneralized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, which is\nan extension of the Neyman allocation proposed by Neyman (1934) and the\nUniform-EBA strategy proposed by Bubeck et al. (2011). For the GNA-EBA\nstrategy, we show that the strategy is asymptotically optimal because its\nprobability of misidentification aligns with the lower bounds as the sample\nsize approaches infinity under the small-gap regime. We refer to such optimal\nstrategies as locally asymptotic optimal because their performance aligns with\nthe lower bounds within restricted situations characterized by the small-gap\nregime."}, "http://arxiv.org/abs/2110.00152": {"title": "ebnm: An R Package for Solving the Empirical Bayes Normal Means Problem Using a Variety of Prior Families", "link": "http://arxiv.org/abs/2110.00152", "description": "The empirical Bayes normal means (EBNM) model is important to many areas of\nstatistics, including (but not limited to) multiple testing, wavelet denoising,\nmultiple linear regression, and matrix factorization. There are several\nexisting software packages that can fit EBNM models under different prior\nassumptions and using different algorithms; however, the differences across\ninterfaces complicate direct comparisons. Further, a number of important prior\nassumptions do not yet have implementations. Motivated by these issues, we\ndeveloped the R package ebnm, which provides a unified interface for\nefficiently fitting EBNM models using a variety of prior assumptions, including\nnonparametric approaches. In some cases, we incorporated existing\nimplementations into ebnm; in others, we implemented new fitting procedures\nwith a focus on speed and numerical stability. To demonstrate the capabilities\nof the unified interface, we compare results using different prior assumptions\nin two extended examples: the shrinkage estimation of baseball statistics; and\nthe matrix factorization of genetics data (via the new R package flashier). In\nsummary, ebnm is a convenient and comprehensive package for performing EBNM\nanalyses under a wide range of prior assumptions."}, "http://arxiv.org/abs/2110.02440": {"title": "Inverse Probability Weighting-based Mediation Analysis for Microbiome Data", "link": "http://arxiv.org/abs/2110.02440", "description": "Mediation analysis is an important tool to study causal associations in\nbiomedical and other scientific areas and has recently gained attention in\nmicrobiome studies. Using a microbiome study of acute myeloid leukemia (AML)\npatients, we investigate whether the effect of induction chemotherapy intensity\nlevels on the infection status is mediated by the microbial taxa abundance. The\nunique characteristics of the microbial mediators -- high-dimensionality,\nzero-inflation, and dependence -- call for new methodological developments in\nmediation analysis. The presence of an exposure-induced mediator-outcome\nconfounder, antibiotic use, further requires a delicate treatment in the\nanalysis. To address these unique challenges in our motivating AML microbiome\nstudy, we propose a novel nonparametric identification formula for the\ninterventional indirect effect (IIE), a measure recently developed for studying\nmediation effects. We develop the corresponding estimation algorithm using the\ninverse probability weighting method. We also test the presence of mediation\neffects via constructing the standard normal bootstrap confidence intervals.\nSimulation studies show that the proposed method has good finite-sample\nperformance in terms of the IIE estimation, and type-I error rate and power of\nthe corresponding test. In the AML microbiome study, our findings suggest that\nthe effect of induction chemotherapy intensity levels on infection is mainly\nmediated by patients' gut microbiome."}, "http://arxiv.org/abs/2203.03532": {"title": "E-detectors: a nonparametric framework for sequential change detection", "link": "http://arxiv.org/abs/2203.03532", "description": "Sequential change detection is a classical problem with a variety of\napplications. However, the majority of prior work has been parametric, for\nexample, focusing on exponential families. We develop a fundamentally new and\ngeneral framework for sequential change detection when the pre- and post-change\ndistributions are nonparametrically specified (and thus composite). Our\nprocedures come with clean, nonasymptotic bounds on the average run length\n(frequency of false alarms). In certain nonparametric cases (like sub-Gaussian\nor sub-exponential), we also provide near-optimal bounds on the detection delay\nfollowing a changepoint. The primary technical tool that we introduce is called\nan \\emph{e-detector}, which is composed of sums of e-processes -- a fundamental\ngeneralization of nonnegative supermartingales -- that are started at\nconsecutive times. We first introduce simple Shiryaev-Roberts and CUSUM-style\ne-detectors, and then show how to design their mixtures in order to achieve\nboth statistical and computational efficiency. Our e-detector framework can be\ninstantiated to recover classical likelihood-based procedures for parametric\nproblems, as well as yielding the first change detection method for many\nnonparametric problems. As a running example, we tackle the problem of\ndetecting changes in the mean of a bounded random variable without i.i.d.\nassumptions, with an application to tracking the performance of a basketball\nteam over multiple seasons."}, "http://arxiv.org/abs/2208.02942": {"title": "sparsegl: An R Package for Estimating Sparse Group Lasso", "link": "http://arxiv.org/abs/2208.02942", "description": "The sparse group lasso is a high-dimensional regression technique that is\nuseful for problems whose predictors have a naturally grouped structure and\nwhere sparsity is encouraged at both the group and individual predictor level.\nIn this paper we discuss a new R package for computing such regularized models.\nThe intention is to provide highly optimized solution routines enabling\nanalysis of very large datasets, especially in the context of sparse design\nmatrices."}, "http://arxiv.org/abs/2208.06236": {"title": "Differentially Private Kolmogorov-Smirnov-Type Tests", "link": "http://arxiv.org/abs/2208.06236", "description": "Hypothesis testing is a central problem in statistical analysis, and there is\ncurrently a lack of differentially private tests which are both statistically\nvalid and powerful. In this paper, we develop several new differentially\nprivate (DP) nonparametric hypothesis tests. Our tests are based on\nKolmogorov-Smirnov, Kuiper, Cram\\'er-von Mises, and Wasserstein test\nstatistics, which can all be expressed as a pseudo-metric on empirical\ncumulative distribution functions (ecdfs), and can be used to test hypotheses\non goodness-of-fit, two samples, and paired data. We show that these test\nstatistics have low sensitivity, requiring minimal noise to satisfy DP. In\nparticular, we show that the sensitivity of these test statistics can be\nexpressed in terms of the base sensitivity, which is the pseudo-metric distance\nbetween the ecdfs of adjacent databases and is easily calculated. The sampling\ndistribution of our test statistics are distribution-free under the null\nhypothesis, enabling easy computation of $p$-values by Monte Carlo methods. We\nshow that in several settings, especially with small privacy budgets or\nheavy-tailed data, our new DP tests outperform alternative nonparametric DP\ntests."}, "http://arxiv.org/abs/2208.10027": {"title": "Learning Invariant Representations under General Interventions on the Response", "link": "http://arxiv.org/abs/2208.10027", "description": "It has become increasingly common nowadays to collect observations of feature\nand response pairs from different environments. As a consequence, one has to\napply learned predictors to data with a different distribution due to\ndistribution shifts. One principled approach is to adopt the structural causal\nmodels to describe training and test models, following the invariance principle\nwhich says that the conditional distribution of the response given its\npredictors remains the same across environments. However, this principle might\nbe violated in practical settings when the response is intervened. A natural\nquestion is whether it is still possible to identify other forms of invariance\nto facilitate prediction in unseen environments. To shed light on this\nchallenging scenario, we focus on linear structural causal models (SCMs) and\nintroduce invariant matching property (IMP), an explicit relation to capture\ninterventions through an additional feature, leading to an alternative form of\ninvariance that enables a unified treatment of general interventions on the\nresponse as well as the predictors. We analyze the asymptotic generalization\nerrors of our method under both the discrete and continuous environment\nsettings, where the continuous case is handled by relating it to the\nsemiparametric varying coefficient models. We present algorithms that show\ncompetitive performance compared to existing methods over various experimental\nsettings including a COVID dataset."}, "http://arxiv.org/abs/2208.11756": {"title": "Testing Many Constraints in Possibly Irregular Models Using Incomplete U-Statistics", "link": "http://arxiv.org/abs/2208.11756", "description": "We consider the problem of testing a null hypothesis defined by equality and\ninequality constraints on a statistical parameter. Testing such hypotheses can\nbe challenging because the number of relevant constraints may be on the same\norder or even larger than the number of observed samples. Moreover, standard\ndistributional approximations may be invalid due to irregularities in the null\nhypothesis. We propose a general testing methodology that aims to circumvent\nthese difficulties. The constraints are estimated by incomplete U-statistics,\nand we derive critical values by Gaussian multiplier bootstrap. We show that\nthe bootstrap approximation of incomplete U-statistics is valid for kernels\nthat we call mixed degenerate when the number of combinations used to compute\nthe incomplete U-statistic is of the same order as the sample size. It follows\nthat our test controls type I error even in irregular settings. Furthermore,\nthe bootstrap approximation covers high-dimensional settings making our testing\nstrategy applicable for problems with many constraints. The methodology is\napplicable, in particular, when the constraints to be tested are polynomials in\nU-estimable parameters. As an application, we consider goodness-of-fit tests of\nlatent tree models for multivariate data."}, "http://arxiv.org/abs/2210.05538": {"title": "Estimating optimal treatment regimes in survival contexts using an instrumental variable", "link": "http://arxiv.org/abs/2210.05538", "description": "In survival contexts, substantial literature exists on estimating optimal\ntreatment regimes, where treatments are assigned based on personal\ncharacteristics for the purpose of maximizing the survival probability. These\nmethods assume that a set of covariates is sufficient to deconfound the\ntreatment-outcome relationship. Nevertheless, the assumption can be limited in\nobservational studies or randomized trials in which non-adherence occurs. Thus,\nwe propose a novel approach for estimating the optimal treatment regime when\ncertain confounders are not observable and a binary instrumental variable is\navailable. Specifically, via a binary instrumental variable, we propose two\nsemiparametric estimators for the optimal treatment regime by maximizing\nKaplan-Meier-like estimators within a pre-defined class of regimes, one of\nwhich possesses the desirable property of double robustness. Because the\nKaplan-Meier-like estimators are jagged, we incorporate kernel smoothing\nmethods to enhance their performance. Under appropriate regularity conditions,\nthe asymptotic properties are rigorously established. Furthermore, the finite\nsample performance is assessed through simulation studies. Finally, we\nexemplify our method using data from the National Cancer Institute's (NCI)\nprostate, lung, colorectal, and ovarian cancer screening trial."}, "http://arxiv.org/abs/2302.00878": {"title": "The Contextual Lasso: Sparse Linear Models via Deep Neural Networks", "link": "http://arxiv.org/abs/2302.00878", "description": "Sparse linear models are one of several core tools for interpretable machine\nlearning, a field of emerging importance as predictive models permeate\ndecision-making in many domains. Unfortunately, sparse linear models are far\nless flexible as functions of their input features than black-box models like\ndeep neural networks. With this capability gap in mind, we study a not-uncommon\nsituation where the input features dichotomize into two groups: explanatory\nfeatures, which are candidates for inclusion as variables in an interpretable\nmodel, and contextual features, which select from the candidate variables and\ndetermine their effects. This dichotomy leads us to the contextual lasso, a new\nstatistical estimator that fits a sparse linear model to the explanatory\nfeatures such that the sparsity pattern and coefficients vary as a function of\nthe contextual features. The fitting process learns this function\nnonparametrically via a deep neural network. To attain sparse coefficients, we\ntrain the network with a novel lasso regularizer in the form of a projection\nlayer that maps the network's output onto the space of $\\ell_1$-constrained\nlinear models. An extensive suite of experiments on real and synthetic data\nsuggests that the learned models, which remain highly transparent, can be\nsparser than the regular lasso without sacrificing the predictive power of a\nstandard deep neural network."}, "http://arxiv.org/abs/2302.02560": {"title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "link": "http://arxiv.org/abs/2302.02560", "description": "In policy research, one of the most critical analytic tasks is to estimate\nthe causal effect of a policy-relevant shift to the distribution of a\ncontinuous exposure/treatment on an outcome of interest. We call this problem\nshift-response function (SRF) estimation. Existing neural network methods\ninvolving robust causal-effect estimators lack theoretical guarantees and\npractical implementations for SRF estimation. Motivated by a key\npolicy-relevant question in public health, we develop a neural network method\nand its theoretical underpinnings to estimate SRFs with robustness and\nefficiency guarantees. We then apply our method to data consisting of 68\nmillion individuals and 27 million deaths across the U.S. to estimate the\ncausal effect from revising the US National Ambient Air Quality Standards\n(NAAQS) for PM 2.5 from 12 $\\mu g/m^3$ to 9 $\\mu g/m^3$. This change has been\nrecently proposed by the US Environmental Protection Agency (EPA). Our goal is\nto estimate, for the first time, the reduction in deaths that would result from\nthis anticipated revision using causal methods for SRFs. Our proposed method,\ncalled {T}argeted {R}egularization for {E}xposure {S}hifts with Neural\n{Net}works (TRESNET), contributes to the neural network literature for causal\ninference in two ways: first, it proposes a targeted regularization loss with\ntheoretical properties that ensure double robustness and achieves asymptotic\nefficiency specific for SRF estimation; second, it enables loss functions from\nthe exponential family of distributions to accommodate non-continuous outcome\ndistributions (such as hospitalization or mortality counts). We complement our\napplication with benchmark experiments that demonstrate TRESNET's broad\napplicability and competitiveness."}, "http://arxiv.org/abs/2303.03502": {"title": "A Semi-Parametric Model Simultaneously Handling Unmeasured Confounding, Informative Cluster Size, and Truncation by Death with a Data Application in Medicare Claims", "link": "http://arxiv.org/abs/2303.03502", "description": "Nearly 300,000 older adults experience a hip fracture every year, the\nmajority of which occur following a fall. Unfortunately, recovery after\nfall-related trauma such as hip fracture is poor, where older adults diagnosed\nwith Alzheimer's Disease and Related Dementia (ADRD) spend a particularly long\ntime in hospitals or rehabilitation facilities during the post-operative\nrecuperation period. Because older adults value functional recovery and\nspending time at home versus facilities as key outcomes after hospitalization,\nidentifying factors that influence days spent at home after hospitalization is\nimperative. While several individual-level factors have been identified, the\ncharacteristics of the treating hospital have recently been identified as\ncontributors. However, few methodological rigorous approaches are available to\nhelp overcome potential sources of bias such as hospital-level unmeasured\nconfounders, informative hospital size, and loss to follow-up due to death.\nThis article develops a useful tool equipped with unsupervised learning to\nsimultaneously handle statistical complexities that are often encountered in\nhealth services research, especially when using large administrative claims\ndatabases. The proposed estimator has a closed form, thus only requiring light\ncomputation load in a large-scale study. We further develop its asymptotic\nproperties that can be used to make statistical inference in practice.\nExtensive simulation studies demonstrate superiority of the proposed estimator\ncompared to existing estimators."}, "http://arxiv.org/abs/2305.04113": {"title": "Inferring Covariance Structure from Multiple Data Sources via Subspace Factor Analysis", "link": "http://arxiv.org/abs/2305.04113", "description": "Factor analysis provides a canonical framework for imposing lower-dimensional\nstructure such as sparse covariance in high-dimensional data. High-dimensional\ndata on the same set of variables are often collected under different\nconditions, for instance in reproducing studies across research groups. In such\ncases, it is natural to seek to learn the shared versus condition-specific\nstructure. Existing hierarchical extensions of factor analysis have been\nproposed, but face practical issues including identifiability problems. To\naddress these shortcomings, we propose a class of SUbspace Factor Analysis\n(SUFA) models, which characterize variation across groups at the level of a\nlower-dimensional subspace. We prove that the proposed class of SUFA models\nlead to identifiability of the shared versus group-specific components of the\ncovariance, and study their posterior contraction properties. Taking a Bayesian\napproach, these contributions are developed alongside efficient posterior\ncomputation algorithms. Our sampler fully integrates out latent variables, is\neasily parallelizable and has complexity that does not depend on sample size.\nWe illustrate the methods through application to integration of multiple gene\nexpression datasets relevant to immunology."}, "http://arxiv.org/abs/2305.17570": {"title": "Auditing Fairness by Betting", "link": "http://arxiv.org/abs/2305.17570", "description": "We provide practical, efficient, and nonparametric methods for auditing the\nfairness of deployed classification and regression models. Whereas previous\nwork relies on a fixed-sample size, our methods are sequential and allow for\nthe continuous monitoring of incoming data, making them highly amenable to\ntracking the fairness of real-world systems. We also allow the data to be\ncollected by a probabilistic policy as opposed to sampled uniformly from the\npopulation. This enables auditing to be conducted on data gathered for another\npurpose. Moreover, this policy may change over time and different policies may\nbe used on different subpopulations. Finally, our methods can handle\ndistribution shift resulting from either changes to the model or changes in the\nunderlying population. Our approach is based on recent progress in\nanytime-valid inference and game-theoretic statistics-the \"testing by betting\"\nframework in particular. These connections ensure that our methods are\ninterpretable, fast, and easy to implement. We demonstrate the efficacy of our\napproach on three benchmark fairness datasets."}, "http://arxiv.org/abs/2306.02948": {"title": "Learning under random distributional shifts", "link": "http://arxiv.org/abs/2306.02948", "description": "Many existing approaches for generating predictions in settings with\ndistribution shift model distribution shifts as adversarial or low-rank in\nsuitable representations. In various real-world settings, however, we might\nexpect shifts to arise through the superposition of many small and random\nchanges in the population and environment. Thus, we consider a class of random\ndistribution shift models that capture arbitrary changes in the underlying\ncovariate space, and dense, random shocks to the relationship between the\ncovariates and the outcomes. In this setting, we characterize the benefits and\ndrawbacks of several alternative prediction strategies: the standard approach\nthat directly predicts the long-term outcome of interest, the proxy approach\nthat directly predicts a shorter-term proxy outcome, and a hybrid approach that\nutilizes both the long-term policy outcome and (shorter-term) proxy outcome(s).\nWe show that the hybrid approach is robust to the strength of the distribution\nshift and the proxy relationship. We apply this method to datasets in two\nhigh-impact domains: asylum-seeker assignment and early childhood education. In\nboth settings, we find that the proposed approach results in substantially\nlower mean-squared error than current approaches."}, "http://arxiv.org/abs/2306.05751": {"title": "Advancing Counterfactual Inference through Quantile Regression", "link": "http://arxiv.org/abs/2306.05751", "description": "The capacity to address counterfactual \"what if\" inquiries is crucial for\nunderstanding and making use of causal influences. Traditional counterfactual\ninference usually assumes the availability of a structural causal model. Yet,\nin practice, such a causal model is often unknown and may not be identifiable.\nThis paper aims to perform reliable counterfactual inference based on the\n(learned) qualitative causal structure and observational data, without\nnecessitating a given causal model or even the direct estimation of conditional\ndistributions. We re-cast counterfactual reasoning as an extended quantile\nregression problem, implemented with deep neural networks to capture general\ncausal relationships and data distributions. The proposed approach offers\nsuperior statistical efficiency compared to existing ones, and further, it\nenhances the potential for generalizing the estimated counterfactual outcomes\nto previously unseen data, providing an upper bound on the generalization\nerror. Empirical results conducted on multiple datasets offer compelling\nsupport for our theoretical assertions."}, "http://arxiv.org/abs/2306.06155": {"title": "Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks", "link": "http://arxiv.org/abs/2306.06155", "description": "We present a new representation learning framework, Intensity Profile\nProjection, for continuous-time dynamic network data. Given triples $(i,j,t)$,\neach representing a time-stamped ($t$) interaction between two entities\n($i,j$), our procedure returns a continuous-time trajectory for each node,\nrepresenting its behaviour over time. The framework consists of three stages:\nestimating pairwise intensity functions, e.g. via kernel smoothing; learning a\nprojection which minimises a notion of intensity reconstruction error; and\nconstructing evolving node representations via the learned projection. The\ntrajectories satisfy two properties, known as structural and temporal\ncoherence, which we see as fundamental for reliable inference. Moreoever, we\ndevelop estimation theory providing tight control on the error of any estimated\ntrajectory, indicating that the representations could even be used in quite\nnoise-sensitive follow-on analyses. The theory also elucidates the role of\nsmoothing as a bias-variance trade-off, and shows how we can reduce the level\nof smoothing as the signal-to-noise ratio increases on account of the algorithm\n`borrowing strength' across the network."}, "http://arxiv.org/abs/2306.08777": {"title": "MMD-FUSE: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting", "link": "http://arxiv.org/abs/2306.08777", "description": "We propose novel statistics which maximise the power of a two-sample test\nbased on the Maximum Mean Discrepancy (MMD), by adapting over the set of\nkernels used in defining it. For finite sets, this reduces to combining\n(normalised) MMD values under each of these kernels via a weighted soft\nmaximum. Exponential concentration bounds are proved for our proposed\nstatistics under the null and alternative. We further show how these kernels\ncan be chosen in a data-dependent but permutation-independent way, in a\nwell-calibrated test, avoiding data splitting. This technique applies more\nbroadly to general permutation-based MMD testing, and includes the use of deep\nkernels with features learnt using unsupervised models such as auto-encoders.\nWe highlight the applicability of our MMD-FUSE test on both synthetic\nlow-dimensional and real-world high-dimensional data, and compare its\nperformance in terms of power against current state-of-the-art kernel tests."}, "http://arxiv.org/abs/2306.09335": {"title": "Class-Conditional Conformal Prediction with Many Classes", "link": "http://arxiv.org/abs/2306.09335", "description": "Standard conformal prediction methods provide a marginal coverage guarantee,\nwhich means that for a random test point, the conformal prediction set contains\nthe true label with a user-specified probability. In many classification\nproblems, we would like to obtain a stronger guarantee--that for test points of\na specific class, the prediction set contains the true label with the same\nuser-chosen probability. For the latter goal, existing conformal prediction\nmethods do not work well when there is a limited amount of labeled data per\nclass, as is often the case in real applications where the number of classes is\nlarge. We propose a method called clustered conformal prediction that clusters\ntogether classes having \"similar\" conformal scores and performs conformal\nprediction at the cluster level. Based on empirical evaluation across four\nimage data sets with many (up to 1000) classes, we find that clustered\nconformal typically outperforms existing methods in terms of class-conditional\ncoverage and set size metrics."}, "http://arxiv.org/abs/2306.11839": {"title": "Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations", "link": "http://arxiv.org/abs/2306.11839", "description": "Randomized experiments often need to be stopped prematurely due to the\ntreatment having an unintended harmful effect. Existing methods that determine\nwhen to stop an experiment early are typically applied to the data in aggregate\nand do not account for treatment effect heterogeneity. In this paper, we study\nthe early stopping of experiments for harm on heterogeneous populations. We\nfirst establish that current methods often fail to stop experiments when the\ntreatment harms a minority group of participants. We then use causal machine\nlearning to develop CLASH, the first broadly-applicable method for\nheterogeneous early stopping. We demonstrate CLASH's performance on simulated\nand real data and show that it yields effective early stopping for both\nclinical trials and A/B tests."}, "http://arxiv.org/abs/2307.01357": {"title": "Adaptive Principal Component Regression with Applications to Panel Data", "link": "http://arxiv.org/abs/2307.01357", "description": "Principal component regression (PCR) is a popular technique for fixed-design\nerror-in-variables regression, a generalization of the linear regression\nsetting in which the observed covariates are corrupted with random noise. We\nprovide the first time-uniform finite sample guarantees for online\n(regularized) PCR whenever data is collected adaptively. Since the proof\ntechniques for analyzing PCR in the fixed design setting do not readily extend\nto the online setting, our results rely on adapting tools from modern\nmartingale concentration to the error-in-variables setting. As an application\nof our bounds, we provide a framework for experiment design in panel data\nsettings when interventions are assigned adaptively. Our framework may be\nthought of as a generalization of the synthetic control and synthetic\ninterventions frameworks, where data is collected via an adaptive intervention\nassignment policy."}, "http://arxiv.org/abs/2307.02520": {"title": "Conditional independence testing under misspecified inductive biases", "link": "http://arxiv.org/abs/2307.02520", "description": "Conditional independence (CI) testing is a fundamental and challenging task\nin modern statistics and machine learning. Many modern methods for CI testing\nrely on powerful supervised learning methods to learn regression functions or\nBayes predictors as an intermediate step; we refer to this class of tests as\nregression-based tests. Although these methods are guaranteed to control Type-I\nerror when the supervised learning methods accurately estimate the regression\nfunctions or Bayes predictors of interest, their behavior is less understood\nwhen they fail due to misspecified inductive biases; in other words, when the\nemployed models are not flexible enough or when the training algorithm does not\ninduce the desired predictors. Then, we study the performance of\nregression-based CI tests under misspecified inductive biases. Namely, we\npropose new approximations or upper bounds for the testing errors of three\nregression-based tests that depend on misspecification errors. Moreover, we\nintroduce the Rao-Blackwellized Predictor Test (RBPT), a regression-based CI\ntest robust against misspecified inductive biases. Finally, we conduct\nexperiments with artificial and real data, showcasing the usefulness of our\ntheory and methods."}, "http://arxiv.org/abs/2308.03801": {"title": "On problematic practice of using normalization in Self-modeling/Multivariate Curve Resolution (S/MCR)", "link": "http://arxiv.org/abs/2308.03801", "description": "The paper is briefly dealing with greater or lesser misused normalization in\nself-modeling/multivariate curve resolution (S/MCR) practice. The importance of\nthe correct use of the ode solvers and apt kinetic illustrations are\nelucidated. The new terms, external and internal normalizations are defined and\ninterpreted. The problem of reducibility of a matrix is touched. Improper\ngeneralization/development of normalization-based methods are cited as\nexamples. The position of the extreme values of the signal contribution\nfunction is clarified. An Executable Notebook with Matlab Live Editor was\ncreated for algorithmic explanations and depictions."}, "http://arxiv.org/abs/2308.05373": {"title": "Conditional Independence Testing for Discrete Distributions: Beyond $\\chi^2$- and $G$-tests", "link": "http://arxiv.org/abs/2308.05373", "description": "This paper is concerned with the problem of conditional independence testing\nfor discrete data. In recent years, researchers have shed new light on this\nfundamental problem, emphasizing finite-sample optimality. The non-asymptotic\nviewpoint adapted in these works has led to novel conditional independence\ntests that enjoy certain optimality under various regimes. Despite their\nattractive theoretical properties, the considered tests are not necessarily\npractical, relying on a Poissonization trick and unspecified constants in their\ncritical values. In this work, we attempt to bridge the gap between theory and\npractice by reproving optimality without Poissonization and calibrating tests\nusing Monte Carlo permutations. Along the way, we also prove that classical\nasymptotic $\\chi^2$- and $G$-tests are notably sub-optimal in a\nhigh-dimensional regime, which justifies the demand for new tools. Our\ntheoretical results are complemented by experiments on both simulated and\nreal-world datasets. Accompanying this paper is an R package UCI that\nimplements the proposed tests."}, "http://arxiv.org/abs/2309.03875": {"title": "Network Sampling Methods for Estimating Social Networks, Population Percentages, and Totals of People Experiencing Unsheltered Homelessness", "link": "http://arxiv.org/abs/2309.03875", "description": "In this article, we propose using network-based sampling strategies to\nestimate the number of unsheltered people experiencing homelessness within a\ngiven administrative service unit, known as a Continuum of Care. We demonstrate\nthe effectiveness of network sampling methods to solve this problem. Here, we\nfocus on Respondent Driven Sampling (RDS), which has been shown to provide\nunbiased or low-biased estimates of totals and proportions for hard-to-reach\npopulations in contexts where a sampling frame (e.g., housing addresses) is not\navailable. To make the RDS estimator work for estimating the total number of\npeople living unsheltered, we introduce a new method that leverages\nadministrative data from the HUD-mandated Homeless Management Information\nSystem (HMIS). The HMIS provides high-quality counts and demographics for\npeople experiencing homelessness who sleep in emergency shelters. We then\ndemonstrate this method using network data collected in Nashville, TN, combined\nwith simulation methods to illustrate the efficacy of this approach and\nintroduce a method for performing a power analysis to find the optimal sample\nsize in this setting. We conclude with the RDS unsheltered PIT count conducted\nby King County Regional Homelessness Authority in 2022 (data publicly available\non the HUD website) and perform a comparative analysis between the 2022 RDS\nestimate of unsheltered people experiencing homelessness and an ARIMA forecast\nof the visual unsheltered PIT count. Finally, we discuss how this method works\nfor estimating the unsheltered population of people experiencing homelessness\nand future areas of research."}}