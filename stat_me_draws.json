{"http://arxiv.org/abs/2310.03114": {"title": "Bayesian Parameter Inference for Partially Observed Stochastic Volterra Equations", "link": "http://arxiv.org/abs/2310.03114", "description": "In this article we consider Bayesian parameter inference for a type of\npartially observed stochastic Volterra equation (SVE). SVEs are found in many\nareas such as physics and mathematical finance. In the latter field they can be\nused to represent long memory in unobserved volatility processes. In many cases\nof practical interest, SVEs must be time-discretized and then parameter\ninference is based upon the posterior associated to this time-discretized\nprocess. Based upon recent studies on time-discretization of SVEs (e.g. Richard\net al. 2021), we use Euler-Maruyama methods for the afore-mentioned\ndiscretization. We then show how multilevel Markov chain Monte Carlo (MCMC)\nmethods (Jasra et al. 2018) can be applied in this context. In the examples we\nstudy, we give a proof that shows that the cost to achieve a mean square error\n(MSE) of $\\mathcal{O}(\\epsilon^2)$, $\\epsilon&gt;0$, is\n$\\mathcal{O}(\\epsilon^{-20/9})$. If one uses a single level MCMC method then\nthe cost is $\\mathcal{O}(\\epsilon^{-38/9})$ to achieve the same MSE. We\nillustrate these results in the context of state-space and stochastic\nvolatility models, with the latter applied to real data."}, "http://arxiv.org/abs/2310.03164": {"title": "A Hierarchical Random Effects State-space Model for Modeling Brain Activities from Electroencephalogram Data", "link": "http://arxiv.org/abs/2310.03164", "description": "Mental disorders present challenges in diagnosis and treatment due to their\ncomplex and heterogeneous nature. Electroencephalogram (EEG) has shown promise\nas a potential biomarker for these disorders. However, existing methods for\nanalyzing EEG signals have limitations in addressing heterogeneity and\ncapturing complex brain activity patterns between regions. This paper proposes\na novel random effects state-space model (RESSM) for analyzing large-scale\nmulti-channel resting-state EEG signals, accounting for the heterogeneity of\nbrain connectivities between groups and individual subjects. We incorporate\nmulti-level random effects for temporal dynamical and spatial mapping matrices\nand address nonstationarity so that the brain connectivity patterns can vary\nover time. The model is fitted under a Bayesian hierarchical model framework\ncoupled with a Gibbs sampler. Compared to previous mixed-effects state-space\nmodels, we directly model high-dimensional random effects matrices without\nstructural constraints and tackle the challenge of identifiability. Through\nextensive simulation studies, we demonstrate that our approach yields valid\nestimation and inference. We apply RESSM to a multi-site clinical trial of\nMajor Depressive Disorder (MDD). Our analysis uncovers significant differences\nin resting-state brain temporal dynamics among MDD patients compared to healthy\nindividuals. In addition, we show the subject-level EEG features derived from\nRESSM exhibit a superior predictive value for the heterogeneous treatment\neffect compared to the EEG frequency band power, suggesting the potential of\nEEG as a valuable biomarker for MDD."}, "http://arxiv.org/abs/2310.03258": {"title": "Detecting Electricity Service Equity Issues with Transfer Counterfactual Learning on Large-Scale Outage Datasets", "link": "http://arxiv.org/abs/2310.03258", "description": "Energy justice is a growing area of interest in interdisciplinary energy\nresearch. However, identifying systematic biases in the energy sector remains\nchallenging due to confounding variables, intricate heterogeneity in treatment\neffects, and limited data availability. To address these challenges, we\nintroduce a novel approach for counterfactual causal analysis centered on\nenergy justice. We use subgroup analysis to manage diverse factors and leverage\nthe idea of transfer learning to mitigate data scarcity in each subgroup. In\nour numerical analysis, we apply our method to a large-scale customer-level\npower outage data set and investigate the counterfactual effect of demographic\nfactors, such as income and age of the population, on power outage durations.\nOur results indicate that low-income and elderly-populated areas consistently\nexperience longer power outages, regardless of weather conditions. This points\nto existing biases in the power system and highlights the need for focused\nimprovements in areas with economic challenges."}, "http://arxiv.org/abs/2310.03351": {"title": "Efficiently analyzing large patient registries with Bayesian joint models for longitudinal and time-to-event data", "link": "http://arxiv.org/abs/2310.03351", "description": "The joint modeling of longitudinal and time-to-event outcomes has become a\npopular tool in follow-up studies. However, fitting Bayesian joint models to\nlarge datasets, such as patient registries, can require extended computing\ntimes. To speed up sampling, we divided a patient registry dataset into\nsubsamples, analyzed them in parallel, and combined the resulting Markov chain\nMonte Carlo draws into a consensus distribution. We used a simulation study to\ninvestigate how different consensus strategies perform with joint models. In\nparticular, we compared grouping all draws together with using equal- and\nprecision-weighted averages. We considered scenarios reflecting different\nsample sizes, numbers of data splits, and processor characteristics.\nParallelization of the sampling process substantially decreased the time\nrequired to run the model. We found that the weighted-average consensus\ndistributions for large sample sizes were nearly identical to the target\nposterior distribution. The proposed algorithm has been made available in an R\npackage for joint models, JMbayes2. This work was motivated by the clinical\ninterest in investigating the association between ppFEV1, a commonly measured\nmarker of lung function, and the risk of lung transplant or death, using data\nfrom the US Cystic Fibrosis Foundation Patient Registry (35,153 individuals\nwith 372,366 years of cumulative follow-up). Splitting the registry into five\nsubsamples resulted in an 85\\% decrease in computing time, from 9.22 to 1.39\nhours. Splitting the data and finding a consensus distribution by\nprecision-weighted averaging proved to be a computationally efficient and\nrobust approach to handling large datasets under the joint modeling framework."}, "http://arxiv.org/abs/2310.03521": {"title": "Cutting Feedback in Misspecified Copula Models", "link": "http://arxiv.org/abs/2310.03521", "description": "In copula models the marginal distributions and copula function are specified\nseparately. We treat these as two modules in a modular Bayesian inference\nframework, and propose conducting modified Bayesian inference by ``cutting\nfeedback''. Cutting feedback limits the influence of potentially misspecified\nmodules in posterior inference. We consider two types of cuts. The first limits\nthe influence of a misspecified copula on inference for the marginals, which is\na Bayesian analogue of the popular Inference for Margins (IFM) estimator. The\nsecond limits the influence of misspecified marginals on inference for the\ncopula parameters by using a rank likelihood to define the cut model. We\nestablish that if only one of the modules is misspecified, then the appropriate\ncut posterior gives accurate uncertainty quantification asymptotically for the\nparameters in the other module. Computation of the cut posteriors is difficult,\nand new variational inference methods to do so are proposed. The efficacy of\nthe new methodology is demonstrated using both simulated data and a substantive\nmultivariate time series application from macroeconomic forecasting. In the\nlatter, cutting feedback from misspecified marginals to a 1096 dimension copula\nimproves posterior inference and predictive accuracy greatly, compared to\nconventional Bayesian inference."}, "http://arxiv.org/abs/2310.03630": {"title": "Model-based Clustering for Network Data via a Latent Shrinkage Position Cluster Model", "link": "http://arxiv.org/abs/2310.03630", "description": "Low-dimensional representation and clustering of network data are tasks of\ngreat interest across various fields. Latent position models are routinely used\nfor this purpose by assuming that each node has a location in a low-dimensional\nlatent space, and enabling node clustering. However, these models fall short in\nsimultaneously determining the optimal latent space dimension and the number of\nclusters. Here we introduce the latent shrinkage position cluster model\n(LSPCM), which addresses this limitation. The LSPCM posits a Bayesian\nnonparametric shrinkage prior on the latent positions' variance parameters\nresulting in higher dimensions having increasingly smaller variances, aiding in\nthe identification of dimensions with non-negligible variance. Further, the\nLSPCM assumes the latent positions follow a sparse finite Gaussian mixture\nmodel, allowing for automatic inference on the number of clusters related to\nnon-empty mixture components. As a result, the LSPCM simultaneously infers the\nlatent space dimensionality and the number of clusters, eliminating the need to\nfit and compare multiple models. The performance of the LSPCM is assessed via\nsimulation studies and demonstrated through application to two real Twitter\nnetwork datasets from sporting and political contexts. Open source software is\navailable to promote widespread use of the LSPCM."}, "http://arxiv.org/abs/2310.03722": {"title": "Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance", "link": "http://arxiv.org/abs/2310.03722", "description": "In 1976, Lai constructed a nontrivial confidence sequence for the mean $\\mu$\nof a Gaussian distribution with unknown variance $\\sigma$. Curiously, he\nemployed both an improper (right Haar) mixture over $\\sigma$ and an improper\n(flat) mixture over $\\mu$. Here, we elaborate carefully on the details of his\nconstruction, which use generalized nonintegrable martingales and an extended\nVille's inequality. While this does yield a sequential t-test, it does not\nyield an ``e-process'' (due to the nonintegrability of his martingale). In this\npaper, we develop two new e-processes and confidence sequences for the same\nsetting: one is a test martingale in a reduced filtration, while the other is\nan e-process in the canonical data filtration. These are respectively obtained\nby swapping Lai's flat mixture for a Gaussian mixture, and swapping the right\nHaar mixture over $\\sigma$ with the maximum likelihood estimate under the null,\nas done in universal inference. We also analyze the width of resulting\nconfidence sequences, which have a curious dependence on the error probability\n$\\alpha$. Numerical experiments are provided along the way to compare and\ncontrast the various approaches."}, "http://arxiv.org/abs/2103.10875": {"title": "Scalable Bayesian computation for crossed and nested hierarchical models", "link": "http://arxiv.org/abs/2103.10875", "description": "We develop sampling algorithms to fit Bayesian hierarchical models, the\ncomputational complexity of which scales linearly with the number of\nobservations and the number of parameters in the model. We focus on crossed\nrandom effect and nested multilevel models, which are used ubiquitously in\napplied sciences. The posterior dependence in both classes is sparse: in\ncrossed random effects models it resembles a random graph, whereas in nested\nmultilevel models it is tree-structured. For each class we identify a framework\nfor scalable computation, building on previous work. Methods for crossed models\nare based on extensions of appropriately designed collapsed Gibbs samplers,\nwhere we introduce the idea of local centering; while methods for nested models\nare based on sparse linear algebra and data augmentation. We provide a\ntheoretical analysis of the proposed algorithms in some simplified settings,\nincluding a comparison with previously proposed methodologies and an\naverage-case analysis based on random graph theory. Numerical experiments,\nincluding two challenging real data analyses on predicting electoral results\nand real estate prices, compare with off-the-shelf Hamiltonian Monte Carlo,\ndisplaying drastic improvement in performance."}, "http://arxiv.org/abs/2106.04106": {"title": "A Regression-based Approach to Robust Estimation and Inference for Genetic Covariance", "link": "http://arxiv.org/abs/2106.04106", "description": "Genome-wide association studies (GWAS) have identified thousands of genetic\nvariants associated with complex traits, and some variants are shown to be\nassociated with multiple complex traits. Genetic covariance between two traits\nis defined as the underlying covariance of genetic effects and can be used to\nmeasure the shared genetic architecture. The data used to estimate such a\ngenetic covariance can be from the same group or different groups of\nindividuals, and the traits can be of different types or collected based on\ndifferent study designs. This paper proposes a unified regression-based\napproach to robust estimation and inference for genetic covariance of general\ntraits that may be associated with genetic variants nonlinearly. The asymptotic\nproperties of the proposed estimator are provided and are shown to be robust\nunder certain model mis-specification. Our method under linear working models\nprovides a robust inference for the narrow-sense genetic covariance, even when\nboth linear models are mis-specified. Numerical experiments are performed to\nsupport the theoretical results. Our method is applied to an outbred mice GWAS\ndata set to study the overlapping genetic effects between the behavioral and\nphysiological phenotypes. The real data results reveal interesting genetic\ncovariance among different mice developmental traits."}, "http://arxiv.org/abs/2112.08417": {"title": "Characterization of causal ancestral graphs for time series with latent confounders", "link": "http://arxiv.org/abs/2112.08417", "description": "In this paper, we introduce a novel class of graphical models for\nrepresenting time lag specific causal relationships and independencies of\nmultivariate time series with unobserved confounders. We completely\ncharacterize these graphs and show that they constitute proper subsets of the\ncurrently employed model classes. As we show, from the novel graphs one can\nthus draw stronger causal inferences -- without additional assumptions. We\nfurther introduce a graphical representation of Markov equivalence classes of\nthe novel graphs. This graphical representation contains more causal knowledge\nthan what current state-of-the-art causal discovery algorithms learn."}, "http://arxiv.org/abs/2112.09313": {"title": "Federated Adaptive Causal Estimation (FACE) of Target Treatment Effects", "link": "http://arxiv.org/abs/2112.09313", "description": "Federated learning of causal estimands may greatly improve estimation\nefficiency by leveraging data from multiple study sites, but robustness to\nheterogeneity and model misspecifications is vital for ensuring validity. We\ndevelop a Federated Adaptive Causal Estimation (FACE) framework to incorporate\nheterogeneous data from multiple sites to provide treatment effect estimation\nand inference for a flexibly specified target population of interest. FACE\naccounts for site-level heterogeneity in the distribution of covariates through\ndensity ratio weighting. To safely incorporate source sites and avoid negative\ntransfer, we introduce an adaptive weighting procedure via a penalized\nregression, which achieves both consistency and optimal efficiency. Our\nstrategy is communication-efficient and privacy-preserving, allowing\nparticipating sites to share summary statistics only once with other sites. We\nconduct both theoretical and numerical evaluations of FACE and apply it to\nconduct a comparative effectiveness study of BNT162b2 (Pfizer) and mRNA-1273\n(Moderna) vaccines on COVID-19 outcomes in U.S. veterans using electronic\nhealth records from five VA regional sites. We show that compared to\ntraditional methods, FACE meaningfully increases the precision of treatment\neffect estimates, with reductions in standard errors ranging from $26\\%$ to\n$67\\%$."}, "http://arxiv.org/abs/2208.03246": {"title": "Non-Asymptotic Analysis of Ensemble Kalman Updates: Effective Dimension and Localization", "link": "http://arxiv.org/abs/2208.03246", "description": "Many modern algorithms for inverse problems and data assimilation rely on\nensemble Kalman updates to blend prior predictions with observed data. Ensemble\nKalman methods often perform well with a small ensemble size, which is\nessential in applications where generating each particle is costly. This paper\ndevelops a non-asymptotic analysis of ensemble Kalman updates that rigorously\nexplains why a small ensemble size suffices if the prior covariance has\nmoderate effective dimension due to fast spectrum decay or approximate\nsparsity. We present our theory in a unified framework, comparing several\nimplementations of ensemble Kalman updates that use perturbed observations,\nsquare root filtering, and localization. As part of our analysis, we develop\nnew dimension-free covariance estimation bounds for approximately sparse\nmatrices that may be of independent interest."}, "http://arxiv.org/abs/2307.10972": {"title": "Adaptively Weighted Audits of Instant-Runoff Voting Elections: AWAIRE", "link": "http://arxiv.org/abs/2307.10972", "description": "An election audit is risk-limiting if the audit limits (to a pre-specified\nthreshold) the chance that an erroneous electoral outcome will be certified.\nExtant methods for auditing instant-runoff voting (IRV) elections are either\nnot risk-limiting or require cast vote records (CVRs), the voting system's\nelectronic record of the votes on each ballot. CVRs are not always available,\nfor instance, in jurisdictions that tabulate IRV contests manually.\n\nWe develop an RLA method (AWAIRE) that uses adaptively weighted averages of\ntest supermartingales to efficiently audit IRV elections when CVRs are not\navailable. The adaptive weighting 'learns' an efficient set of hypotheses to\ntest to confirm the election outcome. When accurate CVRs are available, AWAIRE\ncan use them to increase the efficiency to match the performance of existing\nmethods that require CVRs.\n\nWe provide an open-source prototype implementation that can handle elections\nwith up to six candidates. Simulations using data from real elections show that\nAWAIRE is likely to be efficient in practice. We discuss how to extend the\ncomputational approach to handle elections with more candidates.\n\nAdaptively weighted averages of test supermartingales are a general tool,\nuseful beyond election audits to test collections of hypotheses sequentially\nwhile rigorously controlling the familywise error rate."}, "http://arxiv.org/abs/2309.10514": {"title": "Partially Specified Causal Simulations", "link": "http://arxiv.org/abs/2309.10514", "description": "Simulation studies play a key role in the validation of causal inference\nmethods. The simulation results are reliable only if the study is designed\naccording to the promised operational conditions of the method-in-test. Still,\nmany causal inference literature tend to design over-restricted or misspecified\nstudies. In this paper, we elaborate on the problem of improper simulation\ndesign for causal methods and compile a list of desiderata for an effective\nsimulation framework. We then introduce partially randomized causal simulation\n(PARCS), a simulation framework that meets those desiderata. PARCS synthesizes\ndata based on graphical causal models and a wide range of adjustable\nparameters. There is a legible mapping from usual causal assumptions to the\nparameters, thus, users can identify and specify the subset of related\nparameters and randomize the remaining ones to generate a range of complying\ndata-generating processes for their causal method. The result is a more\ncomprehensive and inclusive empirical investigation for causal claims. Using\nPARCS, we reproduce and extend the simulation studies of two well-known causal\ndiscovery and missing data analysis papers to emphasize the necessity of a\nproper simulation design. Our results show that those papers would have\nimproved and extended the findings, had they used PARCS for simulation. The\nframework is implemented as a Python package, too. By discussing the\ncomprehensiveness and transparency of PARCS, we encourage causal inference\nresearchers to utilize it as a standard tool for future works."}, "http://arxiv.org/abs/2310.03776": {"title": "Significance of the negative binomial distribution in multiplicity phenomena", "link": "http://arxiv.org/abs/2310.03776", "description": "The negative binomial distribution (NBD) has been theorized to express a\nscale-invariant property of many-body systems and has been consistently shown\nto outperform other statistical models in both describing the multiplicity of\nquantum-scale events in particle collision experiments and predicting the\nprevalence of cosmological observables, such as the number of galaxies in a\nregion of space. Despite its widespread applicability and empirical success in\nthese contexts, a theoretical justification for the NBD from first principles\nhas remained elusive for fifty years. The accuracy of the NBD in modeling\nhadronic, leptonic, and semileptonic processes is suggestive of a highly\ngeneral principle, which is yet to be understood. This study demonstrates that\na statistical event of the NBD can in fact be derived in a general context via\nthe dynamical equations of a canonical ensemble of particles in Minkowski\nspace. These results describe a fundamental feature of many-body systems that\nis consistent with data from the ALICE and ATLAS experiments and provides an\nexplanation for the emergence of the NBD in these multiplicity observations.\nTwo methods are used to derive this correspondence: the Feynman path integral\nand a hypersurface parametrization of a propagating ensemble."}, "http://arxiv.org/abs/2310.04030": {"title": "Robust inference with GhostKnockoffs in genome-wide association studies", "link": "http://arxiv.org/abs/2310.04030", "description": "Genome-wide association studies (GWASs) have been extensively adopted to\ndepict the underlying genetic architecture of complex diseases. Motivated by\nGWASs' limitations in identifying small effect loci to understand complex\ntraits' polygenicity and fine-mapping putative causal variants from proxy ones,\nwe propose a knockoff-based method which only requires summary statistics from\nGWASs and demonstrate its validity in the presence of relatedness. We show that\nGhostKnockoffs inference is robust to its input Z-scores as long as they are\nfrom valid marginal association tests and their correlations are consistent\nwith the correlations among the corresponding genetic variants. The property\ngeneralizes GhostKnockoffs to other GWASs settings, such as the meta-analysis\nof multiple overlapping studies and studies based on association test\nstatistics deviated from score tests. We demonstrate GhostKnockoffs'\nperformance using empirical simulation and a meta-analysis of nine European\nancestral genome-wide association studies and whole exome/genome sequencing\nstudies. Both results demonstrate that GhostKnockoffs identify more putative\ncausal variants with weak genotype-phenotype associations that are missed by\nconventional GWASs."}, "http://arxiv.org/abs/2310.04082": {"title": "An energy-based model approach to rare event probability estimation", "link": "http://arxiv.org/abs/2310.04082", "description": "The estimation of rare event probabilities plays a pivotal role in diverse\nfields. Our aim is to determine the probability of a hazard or system failure\noccurring when a quantity of interest exceeds a critical value. In our\napproach, the distribution of the quantity of interest is represented by an\nenergy density, characterized by a free energy function. To efficiently\nestimate the free energy, a bias potential is introduced. Using concepts from\nenergy-based models (EBM), this bias potential is optimized such that the\ncorresponding probability density function approximates a pre-defined\ndistribution targeting the failure region of interest. Given the optimal bias\npotential, the free energy function and the rare event probability of interest\ncan be determined. The approach is applicable not just in traditional rare\nevent settings where the variable upon which the quantity of interest relies\nhas a known distribution, but also in inversion settings where the variable\nfollows a posterior distribution. By combining the EBM approach with a Stein\ndiscrepancy-based stopping criterion, we aim for a balanced accuracy-efficiency\ntrade-off. Furthermore, we explore both parametric and non-parametric\napproaches for the bias potential, with the latter eliminating the need for\nchoosing a particular parameterization, but depending strongly on the accuracy\nof the kernel density estimate used in the optimization process. Through three\nillustrative test cases encompassing both traditional and inversion settings,\nwe show that the proposed EBM approach, when properly configured, (i) allows\nstable and efficient estimation of rare event probabilities and (ii) compares\nfavorably against subset sampling approaches."}, "http://arxiv.org/abs/2310.04165": {"title": "When Composite Likelihood Meets Stochastic Approximation", "link": "http://arxiv.org/abs/2310.04165", "description": "A composite likelihood is an inference function derived by multiplying a set\nof likelihood components. This approach provides a flexible framework for\ndrawing inference when the likelihood function of a statistical model is\ncomputationally intractable. While composite likelihood has computational\nadvantages, it can still be demanding when dealing with numerous likelihood\ncomponents and a large sample size. This paper tackles this challenge by\nemploying an approximation of the conventional composite likelihood estimator,\nwhich is derived from an optimization procedure relying on stochastic\ngradients. This novel estimator is shown to be asymptotically normally\ndistributed around the true parameter. In particular, based on the relative\ndivergent rate of the sample size and the number of iterations of the\noptimization, the variance of the limiting distribution is shown to compound\nfor two sources of uncertainty: the sampling variability of the data and the\noptimization noise, with the latter depending on the sampling distribution used\nto construct the stochastic gradients. The advantages of the proposed framework\nare illustrated through simulation studies on two working examples: an Ising\nmodel for binary data and a gamma frailty model for count data. Finally, a\nreal-data application is presented, showing its effectiveness in a large-scale\nmental health survey."}, "http://arxiv.org/abs/1904.06340": {"title": "A Composite Likelihood-based Approach for Change-point Detection in Spatio-temporal Processes", "link": "http://arxiv.org/abs/1904.06340", "description": "This paper develops a unified and computationally efficient method for\nchange-point estimation along the time dimension in a non-stationary\nspatio-temporal process. By modeling a non-stationary spatio-temporal process\nas a piecewise stationary spatio-temporal process, we consider simultaneous\nestimation of the number and locations of change-points, and model parameters\nin each segment. A composite likelihood-based criterion is developed for\nchange-point and parameters estimation. Under the framework of increasing\ndomain asymptotics, theoretical results including consistency and distribution\nof the estimators are derived under mild conditions. In contrast to classical\nresults in fixed dimensional time series that the localization error of\nchange-point estimator is $O_{p}(1)$, exact recovery of true change-points can\nbe achieved in the spatio-temporal setting. More surprisingly, the consistency\nof change-point estimation can be achieved without any penalty term in the\ncriterion function. In addition, we further establish consistency of the number\nand locations of the change-point estimator under the infill asymptotics\nframework where the time domain is increasing while the spatial sampling domain\nis fixed. A computationally efficient pruned dynamic programming algorithm is\ndeveloped for the challenging criterion optimization problem. Extensive\nsimulation studies and an application to U.S. precipitation data are provided\nto demonstrate the effectiveness and practicality of the proposed method."}, "http://arxiv.org/abs/2201.12936": {"title": "Pigeonhole Design: Balancing Sequential Experiments from an Online Matching Perspective", "link": "http://arxiv.org/abs/2201.12936", "description": "Practitioners and academics have long appreciated the benefits of covariate\nbalancing when they conduct randomized experiments. For web-facing firms\nrunning online A/B tests, however, it still remains challenging in balancing\ncovariate information when experimental subjects arrive sequentially. In this\npaper, we study an online experimental design problem, which we refer to as the\n\"Online Blocking Problem.\" In this problem, experimental subjects with\nheterogeneous covariate information arrive sequentially and must be immediately\nassigned into either the control or the treated group. The objective is to\nminimize the total discrepancy, which is defined as the minimum weight perfect\nmatching between the two groups. To solve this problem, we propose a randomized\ndesign of experiment, which we refer to as the \"Pigeonhole Design.\" The\npigeonhole design first partitions the covariate space into smaller spaces,\nwhich we refer to as pigeonholes, and then, when the experimental subjects\narrive at each pigeonhole, balances the number of control and treated subjects\nfor each pigeonhole. We analyze the theoretical performance of the pigeonhole\ndesign and show its effectiveness by comparing against two well-known benchmark\ndesigns: the match-pair design and the completely randomized design. We\nidentify scenarios when the pigeonhole design demonstrates more benefits over\nthe benchmark design. To conclude, we conduct extensive simulations using\nYahoo! data to show a 10.2% reduction in variance if we use the pigeonhole\ndesign to estimate the average treatment effect."}, "http://arxiv.org/abs/2208.00137": {"title": "Efficient estimation and inference for the signed $\\beta$-model in directed signed networks", "link": "http://arxiv.org/abs/2208.00137", "description": "This paper proposes a novel signed $\\beta$-model for directed signed network,\nwhich is frequently encountered in application domains but largely neglected in\nliterature. The proposed signed $\\beta$-model decomposes a directed signed\nnetwork as the difference of two unsigned networks and embeds each node with\ntwo latent factors for in-status and out-status. The presence of negative edges\nleads to a non-concave log-likelihood, and a one-step estimation algorithm is\ndeveloped to facilitate parameter estimation, which is efficient both\ntheoretically and computationally. We also develop an inferential procedure for\npairwise and multiple node comparisons under the signed $\\beta$-model, which\nfills the void of lacking uncertainty quantification for node ranking.\nTheoretical results are established for the coverage probability of confidence\ninterval, as well as the false discovery rate (FDR) control for multiple node\ncomparison. The finite sample performance of the signed $\\beta$-model is also\nexamined through extensive numerical experiments on both synthetic and\nreal-life networks."}, "http://arxiv.org/abs/2208.08401": {"title": "Conformal Inference for Online Prediction with Arbitrary Distribution Shifts", "link": "http://arxiv.org/abs/2208.08401", "description": "We consider the problem of forming prediction sets in an online setting where\nthe distribution generating the data is allowed to vary over time. Previous\napproaches to this problem suffer from over-weighting historical data and thus\nmay fail to quickly react to the underlying dynamics. Here we correct this\nissue and develop a novel procedure with provably small regret over all local\ntime intervals of a given width. We achieve this by modifying the adaptive\nconformal inference (ACI) algorithm of Gibbs and Cand\\`{e}s (2021) to contain\nan additional step in which the step-size parameter of ACI's gradient descent\nupdate is tuned over time. Crucially, this means that unlike ACI, which\nrequires knowledge of the rate of change of the data-generating mechanism, our\nnew procedure is adaptive to both the size and type of the distribution shift.\nOur methods are highly flexible and can be used in combination with any\nbaseline predictive algorithm that produces point estimates or estimated\nquantiles of the target without the need for distributional assumptions. We\ntest our techniques on two real-world datasets aimed at predicting stock market\nvolatility and COVID-19 case counts and find that they are robust and adaptive\nto real-world distribution shifts."}, "http://arxiv.org/abs/2303.01031": {"title": "Identifiability and Consistent Estimation of the Gaussian Chain Graph Model", "link": "http://arxiv.org/abs/2303.01031", "description": "The chain graph model admits both undirected and directed edges in one graph,\nwhere symmetric conditional dependencies are encoded via undirected edges and\nasymmetric causal relations are encoded via directed edges. Though frequently\nencountered in practice, the chain graph model has been largely under\ninvestigated in literature, possibly due to the lack of identifiability\nconditions between undirected and directed edges. In this paper, we first\nestablish a set of novel identifiability conditions for the Gaussian chain\ngraph model, exploiting a low rank plus sparse decomposition of the precision\nmatrix. Further, an efficient learning algorithm is built upon the\nidentifiability conditions to fully recover the chain graph structure.\nTheoretical analysis on the proposed method is conducted, assuring its\nasymptotic consistency in recovering the exact chain graph structure. The\nadvantage of the proposed method is also supported by numerical experiments on\nboth simulated examples and a real application on the Standard &amp; Poor 500 index\ndata."}, "http://arxiv.org/abs/2305.10817": {"title": "Robust inference of causality in high-dimensional dynamical processes from the Information Imbalance of distance ranks", "link": "http://arxiv.org/abs/2305.10817", "description": "We introduce an approach which allows detecting causal relationships between\nvariables for which the time evolution is available. Causality is assessed by a\nvariational scheme based on the Information Imbalance of distance ranks, a\nstatistical test capable of inferring the relative information content of\ndifferent distance measures. We test whether the predictability of a putative\ndriven system Y can be improved by incorporating information from a potential\ndriver system X, without making assumptions on the underlying dynamics and\nwithout the need to compute probability densities of the dynamic variables.\nThis framework makes causality detection possible even for high-dimensional\nsystems where only few of the variables are known or measured. Benchmark tests\non coupled chaotic dynamical systems demonstrate that our approach outperforms\nother model-free causality detection methods, successfully handling both\nunidirectional and bidirectional couplings. We also show that the method can be\nused to robustly detect causality in human electroencephalography data."}, "http://arxiv.org/abs/2309.06264": {"title": "Spectral clustering algorithm for the allometric extension model", "link": "http://arxiv.org/abs/2309.06264", "description": "The spectral clustering algorithm is often used as a binary clustering method\nfor unclassified data by applying the principal component analysis. To study\ntheoretical properties of the algorithm, the assumption of conditional\nhomoscedasticity is often supposed in existing studies. However, this\nassumption is restrictive and often unrealistic in practice. Therefore, in this\npaper, we consider the allometric extension model, that is, the directions of\nthe first eigenvectors of two covariance matrices and the direction of the\ndifference of two mean vectors coincide, and we provide a non-asymptotic bound\nof the error probability of the spectral clustering algorithm for the\nallometric extension model. As a byproduct of the result, we obtain the\nconsistency of the clustering method in high-dimensional settings."}, "http://arxiv.org/abs/2309.12833": {"title": "Model-based causal feature selection for general response types", "link": "http://arxiv.org/abs/2309.12833", "description": "Discovering causal relationships from observational data is a fundamental yet\nchallenging task. Invariant causal prediction (ICP, Peters et al., 2016) is a\nmethod for causal feature selection which requires data from heterogeneous\nsettings and exploits that causal models are invariant. ICP has been extended\nto general additive noise models and to nonparametric settings using\nconditional independence tests. However, the latter often suffer from low power\n(or poor type I error control) and additive noise models are not suitable for\napplications in which the response is not measured on a continuous scale, but\nreflects categories or counts. Here, we develop transformation-model (TRAM)\nbased ICP, allowing for continuous, categorical, count-type, and\nuninformatively censored responses (these model classes, generally, do not\nallow for identifiability when there is no exogenous heterogeneity). As an\ninvariance test, we propose TRAM-GCM based on the expected conditional\ncovariance between environments and score residuals with uniform asymptotic\nlevel guarantees. For the special case of linear shift TRAMs, we also consider\nTRAM-Wald, which tests invariance based on the Wald statistic. We provide an\nopen-source R package 'tramicp' and evaluate our approach on simulated data and\nin a case study investigating causal features of survival in critically ill\npatients."}, "http://arxiv.org/abs/2310.04452": {"title": "Short text classification with machine learning in the social sciences: The case of climate change on Twitter", "link": "http://arxiv.org/abs/2310.04452", "description": "To analyse large numbers of texts, social science researchers are\nincreasingly confronting the challenge of text classification. When manual\nlabeling is not possible and researchers have to find automatized ways to\nclassify texts, computer science provides a useful toolbox of machine-learning\nmethods whose performance remains understudied in the social sciences. In this\narticle, we compare the performance of the most widely used text classifiers by\napplying them to a typical research scenario in social science research: a\nrelatively small labeled dataset with infrequent occurrence of categories of\ninterest, which is a part of a large unlabeled dataset. As an example case, we\nlook at Twitter communication regarding climate change, a topic of increasing\nscholarly interest in interdisciplinary social science research. Using a novel\ndataset including 5,750 tweets from various international organizations\nregarding the highly ambiguous concept of climate change, we evaluate the\nperformance of methods in automatically classifying tweets based on whether\nthey are about climate change or not. In this context, we highlight two main\nfindings. First, supervised machine-learning methods perform better than\nstate-of-the-art lexicons, in particular as class balance increases. Second,\ntraditional machine-learning methods, such as logistic regression and random\nforest, perform similarly to sophisticated deep-learning methods, whilst\nrequiring much less training time and computational resources. The results have\nimportant implications for the analysis of short texts in social science\nresearch."}, "http://arxiv.org/abs/2310.04563": {"title": "Modeling the Risk of In-Person Instruction during the COVID-19 Pandemic", "link": "http://arxiv.org/abs/2310.04563", "description": "During the COVID-19 pandemic, implementing in-person indoor instruction in a\nsafe manner was a high priority for universities nationwide. To support this\neffort at the University, we developed a mathematical model for estimating the\nrisk of SARS-CoV-2 transmission in university classrooms. This model was used\nto design a safe classroom environment at the University during the COVID-19\npandemic that supported the higher occupancy levels needed to match\npre-pandemic numbers of in-person courses, despite a limited number of large\nclassrooms. A retrospective analysis at the end of the semester confirmed the\nmodel's assessment that the proposed classroom configuration would be safe. Our\nframework is generalizable and was also used to support reopening decisions at\nStanford University. In addition, our methods are flexible; our modeling\nframework was repurposed to plan for large university events and gatherings. We\nfound that our approach and methods work in a wide range of indoor settings and\ncould be used to support reopening planning across various industries, from\nsecondary schools to movie theaters and restaurants."}, "http://arxiv.org/abs/2310.04578": {"title": "TNDDR: Efficient and doubly robust estimation of COVID-19 vaccine effectiveness under the test-negative design", "link": "http://arxiv.org/abs/2310.04578", "description": "While the test-negative design (TND), which is routinely used for monitoring\nseasonal flu vaccine effectiveness (VE), has recently become integral to\nCOVID-19 vaccine surveillance, it is susceptible to selection bias due to\noutcome-dependent sampling. Some studies have addressed the identifiability and\nestimation of causal parameters under the TND, but efficiency bounds for\nnonparametric estimators of the target parameter under the unconfoundedness\nassumption have not yet been investigated. We propose a one-step doubly robust\nand locally efficient estimator called TNDDR (TND doubly robust), which\nutilizes sample splitting and can incorporate machine learning techniques to\nestimate the nuisance functions. We derive the efficient influence function\n(EIF) for the marginal expectation of the outcome under a vaccination\nintervention, explore the von Mises expansion, and establish the conditions for\n$\\sqrt{n}-$consistency, asymptotic normality and double robustness of TNDDR.\nThe proposed TNDDR is supported by both theoretical and empirical\njustifications, and we apply it to estimate COVID-19 VE in an administrative\ndataset of community-dwelling older people (aged $\\geq 60$y) in the province of\nQu\\'ebec, Canada."}, "http://arxiv.org/abs/2310.04660": {"title": "Balancing Weights for Causal Inference in Observational Factorial Studies", "link": "http://arxiv.org/abs/2310.04660", "description": "Many scientific questions in biomedical, environmental, and psychological\nresearch involve understanding the impact of multiple factors on outcomes.\nWhile randomized factorial experiments are ideal for this purpose,\nrandomization is infeasible in many empirical studies. Therefore, investigators\noften rely on observational data, where drawing reliable causal inferences for\nmultiple factors remains challenging. As the number of treatment combinations\ngrows exponentially with the number of factors, some treatment combinations can\nbe rare or even missing by chance in observed data, further complicating\nfactorial effects estimation. To address these challenges, we propose a novel\nweighting method tailored to observational studies with multiple factors. Our\napproach uses weighted observational data to emulate a randomized factorial\nexperiment, enabling simultaneous estimation of the effects of multiple factors\nand their interactions. Our investigations reveal a crucial nuance: achieving\nbalance among covariates, as in single-factor scenarios, is necessary but\ninsufficient for unbiasedly estimating factorial effects. Our findings suggest\nthat balancing the factors is also essential in multi-factor settings.\nMoreover, we extend our weighting method to handle missing treatment\ncombinations in observed data. Finally, we study the asymptotic behavior of the\nnew weighting estimators and propose a consistent variance estimator, providing\nreliable inferences on factorial effects in observational studies."}, "http://arxiv.org/abs/2310.04709": {"title": "Time-dependent mediators in survival analysis: Graphical representation of causal assumptions", "link": "http://arxiv.org/abs/2310.04709", "description": "We study time-dependent mediators in survival analysis using a treatment\nseparation approach due to Didelez [2019] and based on earlier work by Robins\nand Richardson [2011]. This approach avoids nested counterfactuals and\ncrossworld assumptions which are otherwise common in mediation analysis. The\ncausal model of treatment, mediators, covariates, confounders and outcome is\nrepresented by causal directed acyclic graphs (DAGs). However, the DAGs tend to\nbe very complex when we have measurements at a large number of time points. We\ntherefore suggest using so-called rolled graphs in which a node represents an\nentire coordinate process instead of a single random variable, leading us to\nfar simpler graphical representations. The rolled graphs are not necessarily\nacyclic; they can be analyzed by $\\delta$-separation which is the appropriate\ngraphical separation criterion in this class of graphs and analogous to\n$d$-separation. In particular, $\\delta$-separation is a graphical tool for\nevaluating if the conditions of the mediation analysis are met or if unmeasured\nconfounders influence the estimated effects. We also state a mediational\ng-formula. This is similar to the approach in Vansteelandt et al. [2019]\nalthough that paper has a different conceptual basis. Finally, we apply this\nframework to a statistical model based on a Cox model with an added treatment\neffect.survival analysis; mediation; causal inference; graphical models; local\nindependence graphs"}, "http://arxiv.org/abs/2310.04853": {"title": "On changepoint detection in functional data using empirical energy distance", "link": "http://arxiv.org/abs/2310.04853", "description": "We propose a novel family of test statistics to detect the presence of\nchangepoints in a sequence of dependent, possibly multivariate,\nfunctional-valued observations. Our approach allows to test for a very general\nclass of changepoints, including the \"classical\" case of changes in the mean,\nand even changes in the whole distribution. Our statistics are based on a\ngeneralisation of the empirical energy distance; we propose weighted\nfunctionals of the energy distance process, which are designed in order to\nenhance the ability to detect breaks occurring at sample endpoints. The\nlimiting distribution of the maximally selected version of our statistics\nrequires only the computation of the eigenvalues of the covariance function,\nthus being readily implementable in the most commonly employed packages, e.g.\nR. We show that, under the alternative, our statistics are able to detect\nchangepoints occurring even very close to the beginning/end of the sample. In\nthe presence of multiple changepoints, we propose a binary segmentation\nalgorithm to estimate the number of breaks and the locations thereof.\nSimulations show that our procedures work very well in finite samples. We\ncomplement our theory with applications to financial and temperature data."}, "http://arxiv.org/abs/2310.04919": {"title": "The Conditional Prediction Function: A Novel Technique to Control False Discovery Rate for Complex Models", "link": "http://arxiv.org/abs/2310.04919", "description": "In modern scientific research, the objective is often to identify which\nvariables are associated with an outcome among a large class of potential\npredictors. This goal can be achieved by selecting variables in a manner that\ncontrols the the false discovery rate (FDR), the proportion of irrelevant\npredictors among the selections. Knockoff filtering is a cutting-edge approach\nto variable selection that provides FDR control. Existing knockoff statistics\nfrequently employ linear models to assess relationships between features and\nthe response, but the linearity assumption is often violated in real world\napplications. This may result in poor power to detect truly prognostic\nvariables. We introduce a knockoff statistic based on the conditional\nprediction function (CPF), which can pair with state-of-art machine learning\npredictive models, such as deep neural networks. The CPF statistics can capture\nthe nonlinear relationships between predictors and outcomes while also\naccounting for correlation between features. We illustrate the capability of\nthe CPF statistics to provide superior power over common knockoff statistics\nwith continuous, categorical, and survival outcomes using repeated simulations.\nKnockoff filtering with the CPF statistics is demonstrated using (1) a\nresidential building dataset to select predictors for the actual sales prices\nand (2) the TCGA dataset to select genes that are correlated with disease\nstaging in lung cancer patients."}, "http://arxiv.org/abs/2310.04924": {"title": "Markov Chain Monte Carlo Significance Tests", "link": "http://arxiv.org/abs/2310.04924", "description": "Markov chain Monte Carlo significance tests were first introduced by Besag\nand Clifford in [4]. These methods produce statistical valid p-values in\nproblems where sampling from the null hypotheses is intractable. We give an\noverview of the methods of Besag and Clifford and some recent developments. A\nrange of examples and applications are discussed."}, "http://arxiv.org/abs/2310.04934": {"title": "UBSea: A Unified Community Detection Framework", "link": "http://arxiv.org/abs/2310.04934", "description": "Detecting communities in networks and graphs is an important task across many\ndisciplines such as statistics, social science and engineering. There are\ngenerally three different kinds of mixing patterns for the case of two\ncommunities: assortative mixing, disassortative mixing and core-periphery\nstructure. Modularity optimization is a classical way for fitting network\nmodels with communities. However, it can only deal with assortative mixing and\ndisassortative mixing when the mixing pattern is known and fails to discover\nthe core-periphery structure. In this paper, we extend modularity in a\nstrategic way and propose a new framework based on Unified Bigroups Standadized\nEdge-count Analysis (UBSea). It can address all the formerly mentioned\ncommunity mixing structures. In addition, this new framework is able to\nautomatically choose the mixing type to fit the networks. Simulation studies\nshow that the new framework has superb performance in a wide range of settings\nunder the stochastic block model and the degree-corrected stochastic block\nmodel. We show that the new approach produces consistent estimate of the\ncommunities under a suitable signal-to-noise-ratio condition, for the case of a\nblock model with two communities, for both undirected and directed networks.\nThe new method is illustrated through applications to several real-world\ndatasets."}, "http://arxiv.org/abs/2310.05049": {"title": "On Estimation of Optimal Dynamic Treatment Regimes with Multiple Treatments for Survival Data-With Application to Colorectal Cancer Study", "link": "http://arxiv.org/abs/2310.05049", "description": "Dynamic treatment regimes (DTR) are sequential decision rules corresponding\nto several stages of intervention. Each rule maps patients' covariates to\noptional treatments. The optimal dynamic treatment regime is the one that\nmaximizes the mean outcome of interest if followed by the overall population.\nMotivated by a clinical study on advanced colorectal cancer with traditional\nChinese medicine, we propose a censored C-learning (CC-learning) method to\nestimate the dynamic treatment regime with multiple treatments using survival\ndata. To address the challenges of multiple stages with right censoring, we\nmodify the backward recursion algorithm in order to adapt to the flexible\nnumber and timing of treatments. For handling the problem of multiple\ntreatments, we propose a framework from the classification perspective by\ntransferring the problem of optimization with multiple treatment comparisons\ninto an example-dependent cost-sensitive classification problem. With\nclassification and regression tree (CART) as the classifier, the CC-learning\nmethod can produce an estimated optimal DTR with good interpretability. We\ntheoretically prove the optimality of our method and numerically evaluate its\nfinite sample performances through simulation. With the proposed method, we\nidentify the interpretable tree treatment regimes at each stage for the\nadvanced colorectal cancer treatment data from Xiyuan Hospital."}, "http://arxiv.org/abs/2310.05151": {"title": "Sequential linear regression for conditional mean imputation of longitudinal continuous outcomes under reference-based assumptions", "link": "http://arxiv.org/abs/2310.05151", "description": "In clinical trials of longitudinal continuous outcomes, reference based\nimputation (RBI) has commonly been applied to handle missing outcome data in\nsettings where the estimand incorporates the effects of intercurrent events,\ne.g. treatment discontinuation. RBI was originally developed in the multiple\nimputation framework, however recently conditional mean imputation (CMI)\ncombined with the jackknife estimator of the standard error was proposed as a\nway to obtain deterministic treatment effect estimates and correct frequentist\ninference. For both multiple and CMI, a mixed model for repeated measures\n(MMRM) is often used for the imputation model, but this can be computationally\nintensive to fit to multiple data sets (e.g. the jackknife samples) and lead to\nconvergence issues with complex MMRM models with many parameters. Therefore, a\nstep-wise approach based on sequential linear regression (SLR) of the outcomes\nat each visit was developed for the imputation model in the multiple imputation\nframework, but similar developments in the CMI framework are lacking. In this\narticle, we fill this gap in the literature by proposing a SLR approach to\nimplement RBI in the CMI framework, and justify its validity using theoretical\nresults and simulations. We also illustrate our proposal on a real data\napplication."}, "http://arxiv.org/abs/2310.05398": {"title": "Statistical Inference for Modulation Index in Phase-Amplitude Coupling", "link": "http://arxiv.org/abs/2310.05398", "description": "Phase-amplitude coupling is a phenomenon observed in several neurological\nprocesses, where the phase of one signal modulates the amplitude of another\nsignal with a distinct frequency. The modulation index (MI) is a common\ntechnique used to quantify this interaction by assessing the Kullback-Leibler\ndivergence between a uniform distribution and the empirical conditional\ndistribution of amplitudes with respect to the phases of the observed signals.\nThe uniform distribution is an ideal representation that is expected to appear\nunder the absence of coupling. However, it does not reflect the statistical\nproperties of coupling values caused by random chance. In this paper, we\npropose a statistical framework for evaluating the significance of an observed\nMI value based on a null hypothesis that a MI value can be entirely explained\nby chance. Significance is obtained by comparing the value with a reference\ndistribution derived under the null hypothesis of independence (i.e., no\ncoupling) between signals. We derived a closed-form distribution of this null\nmodel, resulting in a scaled beta distribution. To validate the efficacy of our\nproposed framework, we conducted comprehensive Monte Carlo simulations,\nassessing the significance of MI values under various experimental scenarios,\nincluding amplitude modulation, trains of spikes, and sequences of\nhigh-frequency oscillations. Furthermore, we corroborated the reliability of\nour model by comparing its statistical significance thresholds with reported\nvalues from other research studies conducted under different experimental\nsettings. Our method offers several advantages such as meta-analysis\nreliability, simplicity and computational efficiency, as it provides p-values\nand significance levels without resorting to generating surrogate data through\nsampling procedures."}, "http://arxiv.org/abs/2310.05526": {"title": "Projecting infinite time series graphs to finite marginal graphs using number theory", "link": "http://arxiv.org/abs/2310.05526", "description": "In recent years, a growing number of method and application works have\nadapted and applied the causal-graphical-model framework to time series data.\nMany of these works employ time-resolved causal graphs that extend infinitely\ninto the past and future and whose edges are repetitive in time, thereby\nreflecting the assumption of stationary causal relationships. However, most\nresults and algorithms from the causal-graphical-model framework are not\ndesigned for infinite graphs. In this work, we develop a method for projecting\ninfinite time series graphs with repetitive edges to marginal graphical models\non a finite time window. These finite marginal graphs provide the answers to\n$m$-separation queries with respect to the infinite graph, a task that was\npreviously unresolved. Moreover, we argue that these marginal graphs are useful\nfor causal discovery and causal effect estimation in time series, effectively\nenabling to apply results developed for finite graphs to the infinite graphs.\nThe projection procedure relies on finding common ancestors in the\nto-be-projected graph and is, by itself, not new. However, the projection\nprocedure has not yet been algorithmically implemented for time series graphs\nsince in these infinite graphs there can be infinite sets of paths that might\ngive rise to common ancestors. We solve the search over these possibly infinite\nsets of paths by an intriguing combination of path-finding techniques for\nfinite directed graphs and solution theory for linear Diophantine equations. By\nproviding an algorithm that carries out the projection, our paper makes an\nimportant step towards a theoretically-grounded and method-agnostic\ngeneralization of a range of causal inference methods and results to time\nseries."}, "http://arxiv.org/abs/2310.05539": {"title": "Testing High-Dimensional Mediation Effect with Arbitrary Exposure-Mediator Coefficients", "link": "http://arxiv.org/abs/2310.05539", "description": "In response to the unique challenge created by high-dimensional mediators in\nmediation analysis, this paper presents a novel procedure for testing the\nnullity of the mediation effect in the presence of high-dimensional mediators.\nThe procedure incorporates two distinct features. Firstly, the test remains\nvalid under all cases of the composite null hypothesis, including the\nchallenging scenario where both exposure-mediator and mediator-outcome\ncoefficients are zero. Secondly, it does not impose structural assumptions on\nthe exposure-mediator coefficients, thereby allowing for an arbitrarily strong\nexposure-mediator relationship. To the best of our knowledge, the proposed test\nis the first of its kind to provably possess these two features in\nhigh-dimensional mediation analysis. The validity and consistency of the\nproposed test are established, and its numerical performance is showcased\nthrough simulation studies. The application of the proposed test is\ndemonstrated by examining the mediation effect of DNA methylation between\nsmoking status and lung cancer development."}, "http://arxiv.org/abs/2310.05548": {"title": "Cokrig-and-Regress for Spatially Misaligned Environmental Data", "link": "http://arxiv.org/abs/2310.05548", "description": "Spatially misaligned data, where the response and covariates are observed at\ndifferent spatial locations, commonly arise in many environmental studies. Much\nof the statistical literature on handling spatially misaligned data has been\ndevoted to the case of a single covariate and a linear relationship between the\nresponse and this covariate. Motivated by spatially misaligned data collected\non air pollution and weather in China, we propose a cokrig-and-regress (CNR)\nmethod to estimate spatial regression models involving multiple covariates and\npotentially non-linear associations. The CNR estimator is constructed by\nreplacing the unobserved covariates (at the response locations) by their\ncokriging predictor derived from the observed but misaligned covariates under a\nmultivariate Gaussian assumption, where a generalized Kronecker product\ncovariance is used to account for spatial correlations within and between\ncovariates. A parametric bootstrap approach is employed to bias-correct the CNR\nestimates of the spatial covariance parameters and for uncertainty\nquantification. Simulation studies demonstrate that CNR outperforms several\nexisting methods for handling spatially misaligned data, such as\nnearest-neighbor interpolation. Applying CNR to the spatially misaligned air\npollution and weather data in China reveals a number of non-linear\nrelationships between PM$_{2.5}$ concentration and several meteorological\ncovariates."}, "http://arxiv.org/abs/2310.05622": {"title": "A neutral comparison of statistical methods for time-to-event analyses under non-proportional hazards", "link": "http://arxiv.org/abs/2310.05622", "description": "While well-established methods for time-to-event data are available when the\nproportional hazards assumption holds, there is no consensus on the best\ninferential approach under non-proportional hazards (NPH). However, a wide\nrange of parametric and non-parametric methods for testing and estimation in\nthis scenario have been proposed. To provide recommendations on the statistical\nanalysis of clinical trials where non proportional hazards are expected, we\nconducted a comprehensive simulation study under different scenarios of\nnon-proportional hazards, including delayed onset of treatment effect, crossing\nhazard curves, subgroups with different treatment effect and changing hazards\nafter disease progression. We assessed type I error rate control, power and\nconfidence interval coverage, where applicable, for a wide range of methods\nincluding weighted log-rank tests, the MaxCombo test, summary measures such as\nthe restricted mean survival time (RMST), average hazard ratios, and milestone\nsurvival probabilities as well as accelerated failure time regression models.\nWe found a trade-off between interpretability and power when choosing an\nanalysis strategy under NPH scenarios. While analysis methods based on weighted\nlogrank tests typically were favorable in terms of power, they do not provide\nan easily interpretable treatment effect estimate. Also, depending on the\nweight function, they test a narrow null hypothesis of equal hazard functions\nand rejection of this null hypothesis may not allow for a direct conclusion of\ntreatment benefit in terms of the survival function. In contrast,\nnon-parametric procedures based on well interpretable measures as the RMST\ndifference had lower power in most scenarios. Model based methods based on\nspecific survival distributions had larger power, however often gave biased\nestimates and lower than nominal confidence interval coverage."}, "http://arxiv.org/abs/2310.05646": {"title": "Transfer learning for piecewise-constant mean estimation: Optimality, $\\ell_1$- and $\\ell_0$-penalisation", "link": "http://arxiv.org/abs/2310.05646", "description": "We study transfer learning in the context of estimating piecewise-constant\nsignals when source data, which may be relevant but disparate, are available in\naddition to the target data. We initially investigate transfer learning\nestimators that respectively employ $\\ell_1$- and $\\ell_0$-penalties for\nunisource data scenarios and then generalise these estimators to accommodate\nmultisource data. To further reduce estimation errors, especially in scenarios\nwhere some sources significantly differ from the target, we introduce an\ninformative source selection algorithm. We then examine these estimators with\nmultisource selection and establish their minimax optimality under specific\nregularity conditions. It is worth emphasising that, unlike the prevalent\nnarrative in the transfer learning literature that the performance is enhanced\nthrough large source sample sizes, our approaches leverage higher observation\nfrequencies and accommodate diverse frequencies across multiple sources. Our\ntheoretical findings are empirically validated through extensive numerical\nexperiments, with the code available online, see\nhttps://github.com/chrisfanwang/transferlearning"}, "http://arxiv.org/abs/2310.05685": {"title": "Post-Selection Inference for Sparse Estimation", "link": "http://arxiv.org/abs/2310.05685", "description": "When the model is not known and parameter testing or interval estimation is\nconducted after model selection, it is necessary to consider selective\ninference. This paper discusses this issue in the context of sparse estimation.\nFirstly, we describe selective inference related to Lasso as per \\cite{lee},\nand then present polyhedra and truncated distributions when applying it to\nmethods such as Forward Stepwise and LARS. Lastly, we discuss the Significance\nTest for Lasso by \\cite{significant} and the Spacing Test for LARS by\n\\cite{ryan_exact}. This paper serves as a review article.\n\nKeywords: post-selective inference, polyhedron, LARS, lasso, forward\nstepwise, significance test, spacing test."}, "http://arxiv.org/abs/2310.05921": {"title": "Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions", "link": "http://arxiv.org/abs/2310.05921", "description": "We introduce Conformal Decision Theory, a framework for producing safe\nautonomous decisions despite imperfect machine learning predictions. Examples\nof such decisions are ubiquitous, from robot planning algorithms that rely on\npedestrian predictions, to calibrating autonomous manufacturing to exhibit high\nthroughput and low error, to the choice of trusting a nominal policy versus\nswitching to a safe backup policy at run-time. The decisions produced by our\nalgorithms are safe in the sense that they come with provable statistical\nguarantees of having low risk without any assumptions on the world model\nwhatsoever; the observations need not be I.I.D. and can even be adversarial.\nThe theory extends results from conformal prediction to calibrate decisions\ndirectly, without requiring the construction of prediction sets. Experiments\ndemonstrate the utility of our approach in robot motion planning around humans,\nautomated stock trading, and robot manufacturin"}, "http://arxiv.org/abs/2101.06950": {"title": "Learning and scoring Gaussian latent variable causal models with unknown additive interventions", "link": "http://arxiv.org/abs/2101.06950", "description": "With observational data alone, causal structure learning is a challenging\nproblem. The task becomes easier when having access to data collected from\nperturbations of the underlying system, even when the nature of these is\nunknown. Existing methods either do not allow for the presence of latent\nvariables or assume that these remain unperturbed. However, these assumptions\nare hard to justify if the nature of the perturbations is unknown. We provide\nresults that enable scoring causal structures in the setting with additive, but\nunknown interventions. Specifically, we propose a maximum-likelihood estimator\nin a structural equation model that exploits system-wide invariances to output\nan equivalence class of causal structures from perturbation data. Furthermore,\nunder certain structural assumptions on the population model, we provide a\nsimple graphical characterization of all the DAGs in the interventional\nequivalence class. We illustrate the utility of our framework on synthetic data\nas well as real data involving California reservoirs and protein expressions.\nThe software implementation is available as the Python package \\emph{utlvce}."}, "http://arxiv.org/abs/2107.14151": {"title": "Modern Non-Linear Function-on-Function Regression", "link": "http://arxiv.org/abs/2107.14151", "description": "We introduce a new class of non-linear function-on-function regression models\nfor functional data using neural networks. We propose a framework using a\nhidden layer consisting of continuous neurons, called a continuous hidden\nlayer, for functional response modeling and give two model fitting strategies,\nFunctional Direct Neural Network (FDNN) and Functional Basis Neural Network\n(FBNN). Both are designed explicitly to exploit the structure inherent in\nfunctional data and capture the complex relations existing between the\nfunctional predictors and the functional response. We fit these models by\nderiving functional gradients and implement regularization techniques for more\nparsimonious results. We demonstrate the power and flexibility of our proposed\nmethod in handling complex functional models through extensive simulation\nstudies as well as real data examples."}, "http://arxiv.org/abs/2112.00832": {"title": "On the mixed-model analysis of covariance in cluster-randomized trials", "link": "http://arxiv.org/abs/2112.00832", "description": "In the analyses of cluster-randomized trials, mixed-model analysis of\ncovariance (ANCOVA) is a standard approach for covariate adjustment and\nhandling within-cluster correlations. However, when the normality, linearity,\nor the random-intercept assumption is violated, the validity and efficiency of\nthe mixed-model ANCOVA estimators for estimating the average treatment effect\nremain unclear. Under the potential outcomes framework, we prove that the\nmixed-model ANCOVA estimators for the average treatment effect are consistent\nand asymptotically normal under arbitrary misspecification of its working\nmodel. If the probability of receiving treatment is 0.5 for each cluster, we\nfurther show that the model-based variance estimator under mixed-model ANCOVA1\n(ANCOVA without treatment-covariate interactions) remains consistent,\nclarifying that the confidence interval given by standard software is\nasymptotically valid even under model misspecification. Beyond robustness, we\ndiscuss several insights on precision among classical methods for analyzing\ncluster-randomized trials, including the mixed-model ANCOVA, individual-level\nANCOVA, and cluster-level ANCOVA estimators. These insights may inform the\nchoice of methods in practice. Our analytical results and insights are\nillustrated via simulation studies and analyses of three cluster-randomized\ntrials."}, "http://arxiv.org/abs/2201.10770": {"title": "Confidence intervals for the Cox model test error from cross-validation", "link": "http://arxiv.org/abs/2201.10770", "description": "Cross-validation (CV) is one of the most widely used techniques in\nstatistical learning for estimating the test error of a model, but its behavior\nis not yet fully understood. It has been shown that standard confidence\nintervals for test error using estimates from CV may have coverage below\nnominal levels. This phenomenon occurs because each sample is used in both the\ntraining and testing procedures during CV and as a result, the CV estimates of\nthe errors become correlated. Without accounting for this correlation, the\nestimate of the variance is smaller than it should be. One way to mitigate this\nissue is by estimating the mean squared error of the prediction error instead\nusing nested CV. This approach has been shown to achieve superior coverage\ncompared to intervals derived from standard CV. In this work, we generalize the\nnested CV idea to the Cox proportional hazards model and explore various\nchoices of test error for this setting."}, "http://arxiv.org/abs/2202.08419": {"title": "High-Dimensional Time-Varying Coefficient Estimation", "link": "http://arxiv.org/abs/2202.08419", "description": "In this paper, we develop a novel high-dimensional time-varying coefficient\nestimation method, based on high-dimensional Ito diffusion processes. To\naccount for high-dimensional time-varying coefficients, we first estimate local\n(or instantaneous) coefficients using a time-localized Dantzig selection scheme\nunder a sparsity condition, which results in biased local coefficient\nestimators due to the regularization. To handle the bias, we propose a\ndebiasing scheme, which provides well-performing unbiased local coefficient\nestimators. With the unbiased local coefficient estimators, we estimate the\nintegrated coefficient, and to further account for the sparsity of the\ncoefficient process, we apply thresholding schemes. We call this Thresholding\ndEbiased Dantzig (TED). We establish asymptotic properties of the proposed TED\nestimator. In the empirical analysis, we apply the TED procedure to analyzing\nhigh-dimensional factor models using high-frequency data."}, "http://arxiv.org/abs/2206.12525": {"title": "Causality of Functional Longitudinal Data", "link": "http://arxiv.org/abs/2206.12525", "description": "\"Treatment-confounder feedback\" is the central complication to resolve in\nlongitudinal studies, to infer causality. The existing frameworks for\nidentifying causal effects for longitudinal studies with discrete repeated\nmeasures hinge heavily on assuming that time advances in discrete time steps or\ntreatment changes as a jumping process, rendering the number of \"feedbacks\"\nfinite. However, medical studies nowadays with real-time monitoring involve\nfunctional time-varying outcomes, treatment, and confounders, which leads to an\nuncountably infinite number of feedbacks between treatment and confounders.\nTherefore more general and advanced theory is needed. We generalize the\ndefinition of causal effects under user-specified stochastic treatment regimes\nto longitudinal studies with continuous monitoring and develop an\nidentification framework, allowing right censoring and truncation by death. We\nprovide sufficient identification assumptions including a generalized\nconsistency assumption, a sequential randomization assumption, a positivity\nassumption, and a novel \"intervenable\" assumption designed for the\ncontinuous-time case. Under these assumptions, we propose a g-computation\nprocess and an inverse probability weighting process, which suggest a\ng-computation formula and an inverse probability weighting formula for\nidentification. For practical purposes, we also construct two classes of\npopulation estimating equations to identify these two processes, respectively,\nwhich further suggest a doubly robust identification formula with extra\nrobustness against process misspecification. We prove that our framework fully\ngeneralize the existing frameworks and is nonparametric."}, "http://arxiv.org/abs/2209.08139": {"title": "Sparse high-dimensional linear regression with a partitioned empirical Bayes ECM algorithm", "link": "http://arxiv.org/abs/2209.08139", "description": "Bayesian variable selection methods are powerful techniques for fitting and\ninferring on sparse high-dimensional linear regression models. However, many\nare computationally intensive or require restrictive prior distributions on\nmodel parameters. In this paper, we proposed a computationally efficient and\npowerful Bayesian approach for sparse high-dimensional linear regression.\nMinimal prior assumptions on the parameters are required through the use of\nplug-in empirical Bayes estimates of hyperparameters. Efficient maximum a\nposteriori (MAP) estimation is completed through a Parameter-Expanded\nExpectation-Conditional-Maximization (PX-ECM) algorithm. The PX-ECM results in\na robust computationally efficient coordinate-wise optimization which -- when\nupdating the coefficient for a particular predictor -- adjusts for the impact\nof other predictor variables. The completion of the E-step uses an approach\nmotivated by the popular two-group approach to multiple testing. The result is\na PaRtitiOned empirical Bayes Ecm (PROBE) algorithm applied to sparse\nhigh-dimensional linear regression, which can be completed using one-at-a-time\nor all-at-once type optimization. We compare the empirical properties of PROBE\nto comparable approaches with numerous simulation studies and analyses of\ncancer cell drug responses. The proposed approach is implemented in the R\npackage probe."}, "http://arxiv.org/abs/2212.02709": {"title": "SURE-tuned Bridge Regression", "link": "http://arxiv.org/abs/2212.02709", "description": "Consider the {$\\ell_{\\alpha}$} regularized linear regression, also termed\nBridge regression. For $\\alpha\\in (0,1)$, Bridge regression enjoys several\nstatistical properties of interest such as sparsity and near-unbiasedness of\nthe estimates (Fan and Li, 2001). However, the main difficulty lies in the\nnon-convex nature of the penalty for these values of $\\alpha$, which makes an\noptimization procedure challenging and usually it is only possible to find a\nlocal optimum. To address this issue, Polson et al. (2013) took a sampling\nbased fully Bayesian approach to this problem, using the correspondence between\nthe Bridge penalty and a power exponential prior on the regression\ncoefficients. However, their sampling procedure relies on Markov chain Monte\nCarlo (MCMC) techniques, which are inherently sequential and not scalable to\nlarge problem dimensions. Cross validation approaches are similarly\ncomputation-intensive. To this end, our contribution is a novel\n\\emph{non-iterative} method to fit a Bridge regression model. The main\ncontribution lies in an explicit formula for Stein's unbiased risk estimate for\nthe out of sample prediction risk of Bridge regression, which can then be\noptimized to select the desired tuning parameters, allowing us to completely\nbypass MCMC as well as computation-intensive cross validation approaches. Our\nprocedure yields results in a fraction of computational times compared to\niterative schemes, without any appreciable loss in statistical performance. An\nR implementation is publicly available online at:\nhttps://github.com/loriaJ/Sure-tuned_BridgeRegression ."}, "http://arxiv.org/abs/2212.03122": {"title": "Robust convex biclustering with a tuning-free method", "link": "http://arxiv.org/abs/2212.03122", "description": "Biclustering is widely used in different kinds of fields including gene\ninformation analysis, text mining, and recommendation system by effectively\ndiscovering the local correlation between samples and features. However, many\nbiclustering algorithms will collapse when facing heavy-tailed data. In this\npaper, we propose a robust version of convex biclustering algorithm with Huber\nloss. Yet, the newly introduced robustification parameter brings an extra\nburden to selecting the optimal parameters. Therefore, we propose a tuning-free\nmethod for automatically selecting the optimal robustification parameter with\nhigh efficiency. The simulation study demonstrates the more fabulous\nperformance of our proposed method than traditional biclustering methods when\nencountering heavy-tailed noise. A real-life biomedical application is also\npresented. The R package RcvxBiclustr is available at\nhttps://github.com/YifanChen3/RcvxBiclustr."}, "http://arxiv.org/abs/2301.09661": {"title": "Estimating marginal treatment effects from observational studies and indirect treatment comparisons: When are standardization-based methods preferable to those based on propensity score weighting?", "link": "http://arxiv.org/abs/2301.09661", "description": "In light of newly developed standardization methods, we evaluate, via\nsimulation study, how propensity score weighting and standardization -based\napproaches compare for obtaining estimates of the marginal odds ratio and the\nmarginal hazard ratio. Specifically, we consider how the two approaches compare\nin two different scenarios: (1) in a single observational study, and (2) in an\nanchored indirect treatment comparison (ITC) of randomized controlled trials.\nWe present the material in such a way so that the matching-adjusted indirect\ncomparison (MAIC) and the (novel) simulated treatment comparison (STC) methods\nin the ITC setting may be viewed as analogous to the propensity score weighting\nand standardization methods in the single observational study setting. Our\nresults suggest that current recommendations for conducting ITCs can be\nimproved and underscore the importance of adjusting for purely prognostic\nfactors."}, "http://arxiv.org/abs/2302.11746": {"title": "Logistic Regression and Classification with non-Euclidean Covariates", "link": "http://arxiv.org/abs/2302.11746", "description": "We introduce a logistic regression model for data pairs consisting of a\nbinary response and a covariate residing in a non-Euclidean metric space\nwithout vector structures. Based on the proposed model we also develop a binary\nclassifier for non-Euclidean objects. We propose a maximum likelihood estimator\nfor the non-Euclidean regression coefficient in the model, and provide upper\nbounds on the estimation error under various metric entropy conditions that\nquantify complexity of the underlying metric space. Matching lower bounds are\nderived for the important metric spaces commonly seen in statistics,\nestablishing optimality of the proposed estimator in such spaces. Similarly, an\nupper bound on the excess risk of the developed classifier is provided for\ngeneral metric spaces. A finer upper bound and a matching lower bound, and thus\noptimality of the proposed classifier, are established for Riemannian\nmanifolds. We investigate the numerical performance of the proposed estimator\nand classifier via simulation studies, and illustrate their practical merits\nvia an application to task-related fMRI data."}, "http://arxiv.org/abs/2302.13658": {"title": "Robust High-Dimensional Time-Varying Coefficient Estimation", "link": "http://arxiv.org/abs/2302.13658", "description": "In this paper, we develop a novel high-dimensional coefficient estimation\nprocedure based on high-frequency data. Unlike usual high-dimensional\nregression procedure such as LASSO, we additionally handle the heavy-tailedness\nof high-frequency observations as well as time variations of coefficient\nprocesses. Specifically, we employ Huber loss and truncation scheme to handle\nheavy-tailed observations, while $\\ell_{1}$-regularization is adopted to\novercome the curse of dimensionality. To account for the time-varying\ncoefficient, we estimate local coefficients which are biased due to the\n$\\ell_{1}$-regularization. Thus, when estimating integrated coefficients, we\npropose a debiasing scheme to enjoy the law of large number property and employ\na thresholding scheme to further accommodate the sparsity of the coefficients.\nWe call this Robust thrEsholding Debiased LASSO (RED-LASSO) estimator. We show\nthat the RED-LASSO estimator can achieve a near-optimal convergence rate. In\nthe empirical study, we apply the RED-LASSO procedure to the high-dimensional\nintegrated coefficient estimation using high-frequency trading data."}, "http://arxiv.org/abs/2307.04754": {"title": "Action-State Dependent Dynamic Model Selection", "link": "http://arxiv.org/abs/2307.04754", "description": "A model among many may only be best under certain states of the world.\nSwitching from a model to another can also be costly. Finding a procedure to\ndynamically choose a model in these circumstances requires to solve a complex\nestimation procedure and a dynamic programming problem. A Reinforcement\nlearning algorithm is used to approximate and estimate from the data the\noptimal solution to this dynamic programming problem. The algorithm is shown to\nconsistently estimate the optimal policy that may choose different models based\non a set of covariates. A typical example is the one of switching between\ndifferent portfolio models under rebalancing costs, using macroeconomic\ninformation. Using a set of macroeconomic variables and price data, an\nempirical application to the aforementioned portfolio problem shows superior\nperformance to choosing the best portfolio model with hindsight."}, "http://arxiv.org/abs/2307.14828": {"title": "Identifying regime switches through Bayesian wavelet estimation: evidence from flood detection in the Taquari River Valley", "link": "http://arxiv.org/abs/2307.14828", "description": "Two-component mixture models have proved to be a powerful tool for modeling\nheterogeneity in several cluster analysis contexts. However, most methods based\non these models assume a constant behavior for the mixture weights, which can\nbe restrictive and unsuitable for some applications. In this paper, we relax\nthis assumption and allow the mixture weights to vary according to the index\n(e.g., time) to make the model more adaptive to a broader range of data sets.\nWe propose an efficient MCMC algorithm to jointly estimate both component\nparameters and dynamic weights from their posterior samples. We evaluate the\nmethod's performance by running Monte Carlo simulation studies under different\nscenarios for the dynamic weights. In addition, we apply the algorithm to a\ntime series that records the level reached by a river in southern Brazil. The\nTaquari River is a water body whose frequent flood inundations have caused\nvarious damage to riverside communities. Implementing a dynamic mixture model\nallows us to properly describe the flood regimes for the areas most affected by\nthese phenomena."}, "http://arxiv.org/abs/2310.06130": {"title": "Statistical inference for radially-stable generalized Pareto distributions and return level-sets in geometric extremes", "link": "http://arxiv.org/abs/2310.06130", "description": "We obtain a functional analogue of the quantile function for probability\nmeasures admitting a continuous Lebesgue density on $\\mathbb{R}^d$, and use it\nto characterize the class of non-trivial limit distributions of radially\nrecentered and rescaled multivariate exceedances in geometric extremes. A new\nclass of multivariate distributions is identified, termed radially stable\ngeneralized Pareto distributions, and is shown to admit certain stability\nproperties that permit extrapolation to extremal sets along any direction in\n$\\mathbb{R}^d$. Based on the limit Poisson point process likelihood of the\nradially renormalized point process of exceedances, we develop parsimonious\nstatistical models that exploit theoretical links between structural\nstar-bodies and are amenable to Bayesian inference. The star-bodies determine\nthe mean measure of the limit Poisson process through a hierarchical structure.\nOur framework sharpens statistical inference by suitably including additional\ninformation from the angular directions of the geometric exceedances and\nfacilitates efficient computations in dimensions $d=2$ and $d=3$. Additionally,\nit naturally leads to the notion of the return level-set, which is a canonical\nquantile set expressed in terms of its average recurrence interval, and a\ngeometric analogue of the uni-dimensional return level. We illustrate our\nmethods with a simulation study showing superior predictive performance of\nprobabilities of rare events, and with two case studies, one associated with\nriver flow extremes, and the other with oceanographic extremes."}, "http://arxiv.org/abs/2310.06242": {"title": "Treatment Choice, Mean Square Regret and Partial Identification", "link": "http://arxiv.org/abs/2310.06242", "description": "We consider a decision maker who faces a binary treatment choice when their\nwelfare is only partially identified from data. We contribute to the literature\nby anchoring our finite-sample analysis on mean square regret, a decision\ncriterion advocated by Kitagawa, Lee, and Qiu (2022). We find that optimal\nrules are always fractional, irrespective of the width of the identified set\nand precision of its estimate. The optimal treatment fraction is a simple\nlogistic transformation of the commonly used t-statistic multiplied by a factor\ncalculated by a simple constrained optimization. This treatment fraction gets\ncloser to 0.5 as the width of the identified set becomes wider, implying the\ndecision maker becomes more cautious against the adversarial Nature."}, "http://arxiv.org/abs/2310.06252": {"title": "Power and sample size calculation of two-sample projection-based testing for sparsely observed functional data", "link": "http://arxiv.org/abs/2310.06252", "description": "Projection-based testing for mean trajectory differences in two groups of\nirregularly and sparsely observed functional data has garnered significant\nattention in the literature because it accommodates a wide spectrum of group\ndifferences and (non-stationary) covariance structures. This article presents\nthe derivation of the theoretical power function and the introduction of a\ncomprehensive power and sample size (PASS) calculation toolkit tailored to the\nprojection-based testing method developed by Wang (2021). Our approach\naccommodates a wide spectrum of group difference scenarios and a broad class of\ncovariance structures governing the underlying processes. Through extensive\nnumerical simulation, we demonstrate the robustness of this testing method by\nshowcasing that its statistical power remains nearly unaffected even when a\ncertain percentage of observations are missing, rendering it 'missing-immune'.\nFurthermore, we illustrate the practical utility of this test through analysis\nof two randomized controlled trials of Parkinson's disease. To facilitate\nimplementation, we provide a user-friendly R package fPASS, complete with a\ndetailed vignette to guide users through its practical application. We\nanticipate that this article will significantly enhance the usability of this\npotent statistical tool across a range of biostatistical applications, with a\nparticular focus on its relevance in the design of clinical trials."}, "http://arxiv.org/abs/2310.06315": {"title": "Ultra-high dimensional confounder selection algorithms comparison with application to radiomics data", "link": "http://arxiv.org/abs/2310.06315", "description": "Radiomics is an emerging area of medical imaging data analysis particularly\nfor cancer. It involves the conversion of digital medical images into mineable\nultra-high dimensional data. Machine learning algorithms are widely used in\nradiomics data analysis to develop powerful decision support model to improve\nprecision in diagnosis, assessment of prognosis and prediction of therapy\nresponse. However, machine learning algorithms for causal inference have not\nbeen previously employed in radiomics analysis. In this paper, we evaluate the\nvalue of machine learning algorithms for causal inference in radiomics. We\nselect three recent competitive variable selection algorithms for causal\ninference: outcome-adaptive lasso (OAL), generalized outcome-adaptive lasso\n(GOAL) and causal ball screening (CBS). We used a sure independence screening\nprocedure to propose an extension of GOAL and OAL for ultra-high dimensional\ndata, SIS + GOAL and SIS + OAL. We compared SIS + GOAL, SIS + OAL and CBS using\nsimulation study and two radiomics datasets in cancer, osteosarcoma and\ngliosarcoma. The two radiomics studies and the simulation study identified SIS\n+ GOAL as the optimal variable selection algorithm."}, "http://arxiv.org/abs/2310.06330": {"title": "Multivariate moment least-squares estimators for reversible Markov chains", "link": "http://arxiv.org/abs/2310.06330", "description": "Markov chain Monte Carlo (MCMC) is a commonly used method for approximating\nexpectations with respect to probability distributions. Uncertainty assessment\nfor MCMC estimators is essential in practical applications. Moreover, for\nmultivariate functions of a Markov chain, it is important to estimate not only\nthe auto-correlation for each component but also to estimate\ncross-correlations, in order to better assess sample quality, improve estimates\nof effective sample size, and use more effective stopping rules. Berg and Song\n[2022] introduced the moment least squares (momentLS) estimator, a\nshape-constrained estimator for the autocovariance sequence from a reversible\nMarkov chain, for univariate functions of the Markov chain. Based on this\nsequence estimator, they proposed an estimator of the asymptotic variance of\nthe sample mean from MCMC samples. In this study, we propose novel\nautocovariance sequence and asymptotic variance estimators for Markov chain\nfunctions with multiple components, based on the univariate momentLS estimators\nfrom Berg and Song [2022]. We demonstrate strong consistency of the proposed\nauto(cross)-covariance sequence and asymptotic variance matrix estimators. We\nconduct empirical comparisons of our method with other state-of-the-art\napproaches on simulated and real-data examples, using popular samplers\nincluding the random-walk Metropolis sampler and the No-U-Turn sampler from\nSTAN."}, "http://arxiv.org/abs/2310.06357": {"title": "Adaptive Storey's null proportion estimator", "link": "http://arxiv.org/abs/2310.06357", "description": "False discovery rate (FDR) is a commonly used criterion in multiple testing\nand the Benjamini-Hochberg (BH) procedure is arguably the most popular approach\nwith FDR guarantee. To improve power, the adaptive BH procedure has been\nproposed by incorporating various null proportion estimators, among which\nStorey's estimator has gained substantial popularity. The performance of\nStorey's estimator hinges on a critical hyper-parameter, where a pre-fixed\nconfiguration lacks power and existing data-driven hyper-parameters compromise\nthe FDR control. In this work, we propose a novel class of adaptive\nhyper-parameters and establish the FDR control of the associated BH procedure\nusing a martingale argument. Within this class of data-driven hyper-parameters,\nwe present a specific configuration designed to maximize the number of\nrejections and characterize the convergence of this proposal to the optimal\nhyper-parameter under a commonly-used mixture model. We evaluate our adaptive\nStorey's null proportion estimator and the associated BH procedure on extensive\nsimulated data and a motivating protein dataset. Our proposal exhibits\nsignificant power gains when dealing with a considerable proportion of weak\nnon-nulls or a conservative null distribution."}, "http://arxiv.org/abs/2310.06467": {"title": "Advances in Kth nearest-neighbour clutter removal", "link": "http://arxiv.org/abs/2310.06467", "description": "We consider the problem of feature detection in the presence of clutter in\nspatial point processes. Classification methods have been developed in previous\nstudies. Among these, Byers and Raftery (1998) models the observed Kth nearest\nneighbour distances as a mixture distribution and classifies the clutter and\nfeature points consequently. In this paper, we enhance such approach in two\nmanners. First, we propose an automatic procedure for selecting the number of\nnearest neighbours to consider in the classification method by means of\nsegmented regression models. Secondly, with the aim of applying the procedure\nmultiple times to get a ``better\" end result, we propose a stopping criterion\nthat minimizes the overall entropy measure of cluster separation between\nclutter and feature points. The proposed procedures are suitable for a feature\nwith clutter as two superimposed Poisson processes on any space, including\nlinear networks. We present simulations and two case studies of environmental\ndata to illustrate the method."}, "http://arxiv.org/abs/2310.06533": {"title": "Multilevel Monte Carlo for a class of Partially Observed Processes in Neuroscience", "link": "http://arxiv.org/abs/2310.06533", "description": "In this paper we consider Bayesian parameter inference associated to a class\nof partially observed stochastic differential equations (SDE) driven by jump\nprocesses. Such type of models can be routinely found in applications, of which\nwe focus upon the case of neuroscience. The data are assumed to be observed\nregularly in time and driven by the SDE model with unknown parameters. In\npractice the SDE may not have an analytically tractable solution and this leads\nnaturally to a time-discretization. We adapt the multilevel Markov chain Monte\nCarlo method of [11], which works with a hierarchy of time discretizations and\nshow empirically and theoretically that this is preferable to using one single\ntime discretization. The improvement is in terms of the computational cost\nneeded to obtain a pre-specified numerical error. Our approach is illustrated\non models that are found in neuroscience."}, "http://arxiv.org/abs/2310.06653": {"title": "Evaluating causal effects on time-to-event outcomes in an RCT in Oncology with treatment discontinuation due to adverse events", "link": "http://arxiv.org/abs/2310.06653", "description": "In clinical trials, patients sometimes discontinue study treatments\nprematurely due to reasons such as adverse events. Treatment discontinuation\noccurs after the randomisation as an intercurrent event, making causal\ninference more challenging. The Intention-To-Treat (ITT) analysis provides\nvalid causal estimates of the effect of treatment assignment; still, it does\nnot take into account whether or not patients had to discontinue the treatment\nprematurely. We propose to deal with the problem of treatment discontinuation\nusing principal stratification, recognised in the ICH E9(R1) addendum as a\nstrategy for handling intercurrent events. Under this approach, we can\ndecompose the overall ITT effect into principal causal effects for groups of\npatients defined by their potential discontinuation behaviour in continuous\ntime. In this framework, we must consider that discontinuation happening in\ncontinuous time generates an infinite number of principal strata and that\ndiscontinuation time is not defined for patients who would never discontinue.\nAn additional complication is that discontinuation time and time-to-event\noutcomes are subject to administrative censoring. We employ a flexible\nmodel-based Bayesian approach to deal with such complications. We apply the\nBayesian principal stratification framework to analyse synthetic data based on\na recent RCT in Oncology, aiming to assess the causal effects of a new\ninvestigational drug combined with standard of care vs. standard of care alone\non progression-free survival. We simulate data under different assumptions that\nreflect real situations where patients' behaviour depends on critical baseline\ncovariates. Finally, we highlight how such an approach makes it straightforward\nto characterise patients' discontinuation behaviour with respect to the\navailable covariates with the help of a simulation study."}, "http://arxiv.org/abs/2310.06673": {"title": "Assurance Methods for designing a clinical trial with a delayed treatment effect", "link": "http://arxiv.org/abs/2310.06673", "description": "An assurance calculation is a Bayesian alternative to a power calculation.\nOne may be performed to aid the planning of a clinical trial, specifically\nsetting the sample size or to support decisions about whether or not to perform\na study. Immuno-oncology (IO) is a rapidly evolving area in the development of\nanticancer drugs. A common phenomenon that arises from IO trials is one of\ndelayed treatment effects, that is, there is a delay in the separation of the\nsurvival curves. To calculate assurance for a trial in which a delayed\ntreatment effect is likely to be present, uncertainty about key parameters\nneeds to be considered. If uncertainty is not considered, then the number of\npatients recruited may not be enough to ensure we have adequate statistical\npower to detect a clinically relevant treatment effect. We present a new\nelicitation technique for when a delayed treatment effect is likely to be\npresent and show how to compute assurance using these elicited prior\ndistributions. We provide an example to illustrate how this could be used in\npractice. Open-source software is provided for implementing our methods. Our\nmethodology makes the benefits of assurance methods available for the planning\nof IO trials (and others where a delayed treatment expect is likely to occur)."}, "http://arxiv.org/abs/2310.06696": {"title": "Variable selection with FDR control for noisy data -- an application to screening metabolites that are associated with breast and colorectal cancer", "link": "http://arxiv.org/abs/2310.06696", "description": "The rapidly expanding field of metabolomics presents an invaluable resource\nfor understanding the associations between metabolites and various diseases.\nHowever, the high dimensionality, presence of missing values, and measurement\nerrors associated with metabolomics data can present challenges in developing\nreliable and reproducible methodologies for disease association studies.\nTherefore, there is a compelling need to develop robust statistical methods\nthat can navigate these complexities to achieve reliable and reproducible\ndisease association studies. In this paper, we focus on developing such a\nmethodology with an emphasis on controlling the False Discovery Rate during the\nscreening of mutual metabolomic signals for multiple disease outcomes. We\nillustrate the versatility and performance of this procedure in a variety of\nscenarios, dealing with missing data and measurement errors. As a specific\napplication of this novel methodology, we target two of the most prevalent\ncancers among US women: breast cancer and colorectal cancer. By applying our\nmethod to the Wome's Health Initiative data, we successfully identify\nmetabolites that are associated with either or both of these cancers,\ndemonstrating the practical utility and potential of our method in identifying\nconsistent risk factors and understanding shared mechanisms between diseases."}, "http://arxiv.org/abs/2310.06708": {"title": "Adjustment with Three Continuous Variables", "link": "http://arxiv.org/abs/2310.06708", "description": "Spurious association between X and Y may be due to a confounding variable W.\nStatisticians may adjust for W using a variety of techniques. This paper\npresents the results of simulations conducted to assess the performance of\nthose techniques under various, elementary, data-generating processes. The\nresults indicate that no technique is best overall and that specific techniques\nshould be selected based on the particulars of the data-generating process.\nHere we show how causal graphs can guide the selection or design of techniques\nfor statistical adjustment. R programs are provided for researchers interested\nin generalization."}, "http://arxiv.org/abs/2310.06720": {"title": "Asymptotic theory for Bayesian inference and prediction: from the ordinary to a conditional Peaks-Over-Threshold method", "link": "http://arxiv.org/abs/2310.06720", "description": "The Peaks Over Threshold (POT) method is the most popular statistical method\nfor the analysis of univariate extremes. Even though there is a rich applied\nliterature on Bayesian inference for the POT method there is no asymptotic\ntheory for such proposals. Even more importantly, the ambitious and challenging\nproblem of predicting future extreme events according to a proper probabilistic\nforecasting approach has received no attention to date. In this paper we\ndevelop the asymptotic theory (consistency, contraction rates, asymptotic\nnormality and asymptotic coverage of credible intervals) for the Bayesian\ninference based on the POT method. We extend such an asymptotic theory to cover\nthe Bayesian inference on the tail properties of the conditional distribution\nof a response random variable conditionally to a vector of random covariates.\nWith the aim to make accurate predictions of severer extreme events than those\noccurred in the past, we specify the posterior predictive distribution of a\nfuture unobservable excess variable in the unconditional and conditional\napproach and we prove that is Wasserstein consistent and derive its contraction\nrates. Simulations show the good performances of the proposed Bayesian\ninferential methods. The analysis of the change in the frequency of financial\ncrises over time shows the utility of our methodology."}, "http://arxiv.org/abs/2310.06730": {"title": "Sparse topic modeling via spectral decomposition and thresholding", "link": "http://arxiv.org/abs/2310.06730", "description": "The probabilistic Latent Semantic Indexing model assumes that the expectation\nof the corpus matrix is low-rank and can be written as the product of a\ntopic-word matrix and a word-document matrix. In this paper, we study the\nestimation of the topic-word matrix under the additional assumption that the\nordered entries of its columns rapidly decay to zero. This sparsity assumption\nis motivated by the empirical observation that the word frequencies in a text\noften adhere to Zipf's law. We introduce a new spectral procedure for\nestimating the topic-word matrix that thresholds words based on their corpus\nfrequencies, and show that its $\\ell_1$-error rate under our sparsity\nassumption depends on the vocabulary size $p$ only via a logarithmic term. Our\nerror bound is valid for all parameter regimes and in particular for the\nsetting where $p$ is extremely large; this high-dimensional setting is commonly\nencountered but has not been adequately addressed in prior literature.\nFurthermore, our procedure also accommodates datasets that violate the\nseparability assumption, which is necessary for most prior approaches in topic\nmodeling. Experiments with synthetic data confirm that our procedure is\ncomputationally fast and allows for consistent estimation of the topic-word\nmatrix in a wide variety of parameter regimes. Our procedure also performs well\nrelative to well-established methods when applied to a large corpus of research\npaper abstracts, as well as the analysis of single-cell and microbiome data\nwhere the same statistical model is relevant but the parameter regimes are\nvastly different."}, "http://arxiv.org/abs/2310.06746": {"title": "Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules", "link": "http://arxiv.org/abs/2310.06746", "description": "Interpretability is a key concern in estimating heterogeneous treatment\neffects using machine learning methods, especially for healthcare applications\nwhere high-stake decisions are often made. Inspired by the Predictive,\nDescriptive, Relevant framework of interpretability, we propose causal rule\nlearning which finds a refined set of causal rules characterizing potential\nsubgroups to estimate and enhance our understanding of heterogeneous treatment\neffects. Causal rule learning involves three phases: rule discovery, rule\nselection, and rule analysis. In the rule discovery phase, we utilize a causal\nforest to generate a pool of causal rules with corresponding subgroup average\ntreatment effects. The selection phase then employs a D-learning method to\nselect a subset of these rules to deconstruct individual-level treatment\neffects as a linear combination of the subgroup-level effects. This helps to\nanswer an ignored question by previous literature: what if an individual\nsimultaneously belongs to multiple groups with different average treatment\neffects? The rule analysis phase outlines a detailed procedure to further\nanalyze each rule in the subset from multiple perspectives, revealing the most\npromising rules for further validation. The rules themselves, their\ncorresponding subgroup treatment effects, and their weights in the linear\ncombination give us more insights into heterogeneous treatment effects.\nSimulation and real-world data analysis demonstrate the superior performance of\ncausal rule learning on the interpretable estimation of heterogeneous treatment\neffect when the ground truth is complex and the sample size is sufficient."}, "http://arxiv.org/abs/2310.06808": {"title": "Odds are the sign is right", "link": "http://arxiv.org/abs/2310.06808", "description": "This article introduces a new condition based on odds ratios for sensitivity\nanalysis. The analysis involves the average effect of a treatment or exposure\non a response or outcome with estimates adjusted for and conditional on a\nsingle, unmeasured, dichotomous covariate. Results of statistical simulations\nare displayed to show that the odds ratio condition is as reliable as other\ncommonly used conditions for sensitivity analysis. Other conditions utilize\nquantities reflective of a mediating covariate. The odds ratio condition can be\napplied when the covariate is a confounding variable. As an example application\nwe use the odds ratio condition to analyze and interpret a positive association\nobserved between Zika virus infection and birth defects."}, "http://arxiv.org/abs/2009.07551": {"title": "Manipulation-Robust Regression Discontinuity Designs", "link": "http://arxiv.org/abs/2009.07551", "description": "We present a new identification condition for regression discontinuity\ndesigns. We replace the local randomization of Lee (2008) with two restrictions\non its threat, namely, the manipulation of the running variable. Furthermore,\nwe provide the first auxiliary assumption of McCrary's (2008) diagnostic test\nto detect manipulation. Based on our auxiliary assumption, we derive a novel\nexpression of moments that immediately implies the worst-case bounds of Gerard,\nRokkanen, and Rothe (2020) and an enhanced interpretation of their target\nparameters. We highlight two issues: an overlooked source of identification\nfailure, and a missing auxiliary assumption to detect manipulation. In the case\nstudies, we illustrate our solution to these issues using institutional details\nand economic theories."}, "http://arxiv.org/abs/2204.06030": {"title": "Variable importance measures for heterogeneous causal effects", "link": "http://arxiv.org/abs/2204.06030", "description": "The recognition that personalised treatment decisions lead to better clinical\noutcomes has sparked recent research activity in the following two domains.\nPolicy learning focuses on finding optimal treatment rules (OTRs), which\nexpress whether an individual would be better off with or without treatment,\ngiven their measured characteristics. OTRs optimize a pre-set population\ncriterion, but do not provide insight into the extent to which treatment\nbenefits or harms individual subjects. Estimates of conditional average\ntreatment effects (CATEs) do offer such insights, but valid inference is\ncurrently difficult to obtain when data-adaptive methods are used. Moreover,\nclinicians are (rightly) hesitant to blindly adopt OTR or CATE estimates, not\nleast since both may represent complicated functions of patient characteristics\nthat provide little insight into the key drivers of heterogeneity. To address\nthese limitations, we introduce novel nonparametric treatment effect variable\nimportance measures (TE-VIMs). TE-VIMs extend recent regression-VIMs, viewed as\nnonparametric analogues to ANOVA statistics. By not being tied to a particular\nmodel, they are amenable to data-adaptive (machine learning) estimation of the\nCATE, itself an active area of research. Estimators for the proposed statistics\nare derived from their efficient influence curves and these are illustrated\nthrough a simulation study and an applied example."}, "http://arxiv.org/abs/2204.07907": {"title": "Just Identified Indirect Inference Estimator: Accurate Inference through Bias Correction", "link": "http://arxiv.org/abs/2204.07907", "description": "An important challenge in statistical analysis lies in controlling the\nestimation bias when handling the ever-increasing data size and model\ncomplexity of modern data settings. In this paper, we propose a reliable\nestimation and inference approach for parametric models based on the Just\nIdentified iNdirect Inference estimator (JINI). The key advantage of our\napproach is that it allows to construct a consistent estimator in a simple\nmanner, while providing strong bias correction guarantees that lead to accurate\ninference. Our approach is particularly useful for complex parametric models,\nas it allows to bypass the analytical and computational difficulties (e.g., due\nto intractable estimating equation) typically encountered in standard\nprocedures. The properties of JINI (including consistency, asymptotic\nnormality, and its bias correction property) are also studied when the\nparameter dimension is allowed to diverge, which provide the theoretical\nfoundation to explain the advantageous performance of JINI in increasing\ndimensional covariates settings. Our simulations and an alcohol consumption\ndata analysis highlight the practical usefulness and excellent performance of\nJINI when data present features (e.g., misclassification, rounding) as well as\nin robust estimation."}, "http://arxiv.org/abs/2209.05598": {"title": "Learning domain-specific causal discovery from time series", "link": "http://arxiv.org/abs/2209.05598", "description": "Causal discovery (CD) from time-varying data is important in neuroscience,\nmedicine, and machine learning. Techniques for CD encompass randomized\nexperiments, which are generally unbiased but expensive, and algorithms such as\nGranger causality, conditional-independence-based, structural-equation-based,\nand score-based methods that are only accurate under strong assumptions made by\nhuman designers. However, as demonstrated in other areas of machine learning,\nhuman expertise is often not entirely accurate and tends to be outperformed in\ndomains with abundant data. In this study, we examine whether we can enhance\ndomain-specific causal discovery for time series using a data-driven approach.\nOur findings indicate that this procedure significantly outperforms\nhuman-designed, domain-agnostic causal discovery methods, such as Mutual\nInformation, VAR-LiNGAM, and Granger Causality on the MOS 6502 microprocessor,\nthe NetSim fMRI dataset, and the Dream3 gene dataset. We argue that, when\nfeasible, the causality field should consider a supervised approach in which\ndomain-specific CD procedures are learned from extensive datasets with known\ncausal relationships, rather than being designed by human specialists. Our\nfindings promise a new approach toward improving CD in neural and medical data\nand for the broader machine learning community."}, "http://arxiv.org/abs/2209.05795": {"title": "Joint modelling of the body and tail of bivariate data", "link": "http://arxiv.org/abs/2209.05795", "description": "In situations where both extreme and non-extreme data are of interest,\nmodelling the whole data set accurately is important. In a univariate\nframework, modelling the bulk and tail of a distribution has been extensively\nstudied before. However, when more than one variable is of concern, models that\naim specifically at capturing both regions correctly are scarce in the\nliterature. A dependence model that blends two copulas with different\ncharacteristics over the whole range of the data support is proposed. One\ncopula is tailored to the bulk and the other to the tail, with a dynamic\nweighting function employed to transition smoothly between them. Tail\ndependence properties are investigated numerically and simulation is used to\nconfirm that the blended model is sufficiently flexible to capture a wide\nvariety of structures. The model is applied to study the dependence between\ntemperature and ozone concentration at two sites in the UK and compared with a\nsingle copula fit. The proposed model provides a better, more flexible, fit to\nthe data, and is also capable of capturing complex dependence structures."}, "http://arxiv.org/abs/2212.14650": {"title": "Two-step estimators of high dimensional correlation matrices", "link": "http://arxiv.org/abs/2212.14650", "description": "We investigate block diagonal and hierarchical nested stochastic multivariate\nGaussian models by studying their sample cross-correlation matrix on high\ndimensions. By performing numerical simulations, we compare a filtered sample\ncross-correlation with the population cross-correlation matrices by using\nseveral rotationally invariant estimators (RIE) and hierarchical clustering\nestimators (HCE) under several loss functions. We show that at large but finite\nsample size, sample cross-correlation filtered by RIE estimators are often\noutperformed by HCE estimators for several of the loss functions. We also show\nthat for block models and for hierarchically nested block models the best\ndetermination of the filtered sample cross-correlation is achieved by\nintroducing two-step estimators combining state-of-the-art non-linear shrinkage\nmodels with hierarchical clustering estimators."}, "http://arxiv.org/abs/2302.02457": {"title": "Scalable inference in functional linear regression with streaming data", "link": "http://arxiv.org/abs/2302.02457", "description": "Traditional static functional data analysis is facing new challenges due to\nstreaming data, where data constantly flow in. A major challenge is that\nstoring such an ever-increasing amount of data in memory is nearly impossible.\nIn addition, existing inferential tools in online learning are mainly developed\nfor finite-dimensional problems, while inference methods for functional data\nare focused on the batch learning setting. In this paper, we tackle these\nissues by developing functional stochastic gradient descent algorithms and\nproposing an online bootstrap resampling procedure to systematically study the\ninference problem for functional linear regression. In particular, the proposed\nestimation and inference procedures use only one pass over the data; thus they\nare easy to implement and suitable to the situation where data arrive in a\nstreaming manner. Furthermore, we establish the convergence rate as well as the\nasymptotic distribution of the proposed estimator. Meanwhile, the proposed\nperturbed estimator from the bootstrap procedure is shown to enjoy the same\ntheoretical properties, which provide the theoretical justification for our\nonline inference tool. As far as we know, this is the first inference result on\nthe functional linear regression model with streaming data. Simulation studies\nare conducted to investigate the finite-sample performance of the proposed\nprocedure. An application is illustrated with the Beijing multi-site\nair-quality data."}, "http://arxiv.org/abs/2303.09598": {"title": "Variational Bayesian analysis of survival data using a log-logistic accelerated failure time model", "link": "http://arxiv.org/abs/2303.09598", "description": "The log-logistic regression model is one of the most commonly used\naccelerated failure time (AFT) models in survival analysis, for which\nstatistical inference methods are mainly established under the frequentist\nframework. Recently, Bayesian inference for log-logistic AFT models using\nMarkov chain Monte Carlo (MCMC) techniques has also been widely developed. In\nthis work, we develop an alternative approach to MCMC methods and infer the\nparameters of the log-logistic AFT model via a mean-field variational Bayes\n(VB) algorithm. A piecewise approximation technique is embedded in deriving the\nVB algorithm to achieve conjugacy. The proposed VB algorithm is evaluated and\ncompared with typical frequentist inferences and MCMC inference using simulated\ndata under various scenarios. A publicly available dataset is employed for\nillustration. We demonstrate that the proposed VB algorithm can achieve good\nestimation accuracy and has a lower computational cost compared with MCMC\nmethods."}, "http://arxiv.org/abs/2304.03853": {"title": "StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables", "link": "http://arxiv.org/abs/2304.03853", "description": "StepMix is an open-source Python package for the pseudo-likelihood estimation\n(one-, two- and three-step approaches) of generalized finite mixture models\n(latent profile and latent class analysis) with external variables (covariates\nand distal outcomes). In many applications in social sciences, the main\nobjective is not only to cluster individuals into latent classes, but also to\nuse these classes to develop more complex statistical models. These models\ngenerally divide into a measurement model that relates the latent classes to\nobserved indicators, and a structural model that relates covariates and outcome\nvariables to the latent classes. The measurement and structural models can be\nestimated jointly using the so-called one-step approach or sequentially using\nstepwise methods, which present significant advantages for practitioners\nregarding the interpretability of the estimated latent classes. In addition to\nthe one-step approach, StepMix implements the most important stepwise\nestimation methods from the literature, including the bias-adjusted three-step\nmethods with Bolk-Croon-Hagenaars and maximum likelihood corrections and the\nmore recent two-step approach. These pseudo-likelihood estimators are presented\nin this paper under a unified framework as specific expectation-maximization\nsubroutines. To facilitate and promote their adoption among the data science\ncommunity, StepMix follows the object-oriented design of the scikit-learn\nlibrary and provides an additional R wrapper."}, "http://arxiv.org/abs/2310.06926": {"title": "Bayesian inference and cure rate modeling for event history data", "link": "http://arxiv.org/abs/2310.06926", "description": "Estimating model parameters of a general family of cure models is always a\nchallenging task mainly due to flatness and multimodality of the likelihood\nfunction. In this work, we propose a fully Bayesian approach in order to\novercome these issues. Posterior inference is carried out by constructing a\nMetropolis-coupled Markov chain Monte Carlo (MCMC) sampler, which combines\nGibbs sampling for the latent cure indicators and Metropolis-Hastings steps\nwith Langevin diffusion dynamics for parameter updates. The main MCMC algorithm\nis embedded within a parallel tempering scheme by considering heated versions\nof the target posterior distribution. It is demonstrated via simulations that\nthe proposed algorithm freely explores the multimodal posterior distribution\nand produces robust point estimates, while it outperforms maximum likelihood\nestimation via the Expectation-Maximization algorithm. A by-product of our\nBayesian implementation is to control the False Discovery Rate when classifying\nitems as cured or not. Finally, the proposed method is illustrated in a real\ndataset which refers to recidivism for offenders released from prison; the\nevent of interest is whether the offender was re-incarcerated after probation\nor not."}, "http://arxiv.org/abs/2310.06969": {"title": "Positivity-free Policy Learning with Observational Data", "link": "http://arxiv.org/abs/2310.06969", "description": "Policy learning utilizing observational data is pivotal across various\ndomains, with the objective of learning the optimal treatment assignment policy\nwhile adhering to specific constraints such as fairness, budget, and\nsimplicity. This study introduces a novel positivity-free (stochastic) policy\nlearning framework designed to address the challenges posed by the\nimpracticality of the positivity assumption in real-world scenarios. This\nframework leverages incremental propensity score policies to adjust propensity\nscore values instead of assigning fixed values to treatments. We characterize\nthese incremental propensity score policies and establish identification\nconditions, employing semiparametric efficiency theory to propose efficient\nestimators capable of achieving rapid convergence rates, even when integrated\nwith advanced machine learning algorithms. This paper provides a thorough\nexploration of the theoretical guarantees associated with policy learning and\nvalidates the proposed framework's finite-sample performance through\ncomprehensive numerical experiments, ensuring the identification of causal\neffects from observational data is both robust and reliable."}, "http://arxiv.org/abs/2310.07002": {"title": "Bayesian cross-validation by parallel Markov Chain Monte Carlo", "link": "http://arxiv.org/abs/2310.07002", "description": "Brute force cross-validation (CV) is a method for predictive assessment and\nmodel selection that is general and applicable to a wide range of Bayesian\nmodels. However, in many cases brute force CV is too computationally burdensome\nto form part of interactive modeling workflows, especially when inference\nrelies on Markov chain Monte Carlo (MCMC). In this paper we present a method\nfor conducting fast Bayesian CV by massively parallel MCMC. On suitable\naccelerator hardware, for many applications our approach is about as fast (in\nwall clock time) as a single full-data model fit.\n\nParallel CV is more flexible than existing fast CV approximation methods\nbecause it can easily exploit a wide range of scoring rules and data\npartitioning schemes. This is particularly useful for CV methods designed for\nnon-exchangeable data. Our approach also delivers accurate estimates of Monte\nCarlo and CV uncertainty. In addition to parallelizing computations, parallel\nCV speeds up inference by reusing information from earlier MCMC adaptation and\ninference obtained during initial model fitting and checking of the full-data\nmodel.\n\nWe propose MCMC diagnostics for parallel CV applications, including a summary\nof MCMC mixing based on the popular potential scale reduction factor\n($\\hat{R}$) and MCMC effective sample size ($\\widehat{ESS}$) measures.\nFurthermore, we describe a method for determining whether an $\\hat{R}$\ndiagnostic indicates approximate stationarity of the chains, that may be of\nmore general interest for applications beyond parallel CV.\n\nFor parallel CV to work on memory-constrained computing accelerators, we show\nthat parallel CV and associated diagnostics can be implemented using online\n(streaming) algorithms ideal for parallel computing environments with limited\nmemory. Constant memory algorithms allow parallel CV to scale up to very large\nblocking designs."}, "http://arxiv.org/abs/2310.07016": {"title": "Discovering the Unknowns: A First Step", "link": "http://arxiv.org/abs/2310.07016", "description": "This article aims at discovering the unknown variables in the system through\ndata analysis. The main idea is to use the time of data collection as a\nsurrogate variable and try to identify the unknown variables by modeling\ngradual and sudden changes in the data. We use Gaussian process modeling and a\nsparse representation of the sudden changes to efficiently estimate the large\nnumber of parameters in the proposed statistical model. The method is tested on\na realistic dataset generated using a one-dimensional implementation of a\nMagnetized Liner Inertial Fusion (MagLIF) simulation model and encouraging\nresults are obtained."}, "http://arxiv.org/abs/2310.07107": {"title": "Root n consistent extremile regression and its supervised and semi-supervised learning", "link": "http://arxiv.org/abs/2310.07107", "description": "Extremile (Daouia, Gijbels and Stupfler,2019) is a novel and coherent measure\nof risk, determined by weighted expectations rather than tail probabilities. It\nfinds application in risk management, and, in contrast to quantiles, it\nfulfills the axioms of consistency, taking into account the severity of tail\nlosses. However, existing studies (Daouia, Gijbels and Stupfler,2019,2022) on\nextremile involve unknown distribution functions, making it challenging to\nobtain a root n-consistent estimator for unknown parameters in linear extremile\nregression. This article introduces a new definition of linear extremile\nregression and its estimation method, where the estimator is root n-consistent.\nAdditionally, while the analysis of unlabeled data for extremes presents a\nsignificant challenge and is currently a topic of great interest in machine\nlearning for various classification problems, we have developed a\nsemi-supervised framework for the proposed extremile regression using unlabeled\ndata. This framework can also enhance estimation accuracy under model\nmisspecification. Both simulations and real data analyses have been conducted\nto illustrate the finite sample performance of the proposed methods."}, "http://arxiv.org/abs/2310.07124": {"title": "Systematic simulation of age-period-cohort analysis: Demonstrating bias of Bayesian regularization", "link": "http://arxiv.org/abs/2310.07124", "description": "Age-period-cohort (APC) analysis is one of the fundamental time-series\nanalyses used in the social sciences. This paper evaluates APC analysis via\nsystematic simulation in term of how well the artificial parameters are\nrecovered. We consider three models of Bayesian regularization using normal\nprior distributions: the random effects model with reference to multilevel\nanalysis, the ridge regression model equivalent to the intrinsic estimator, and\nthe random walk model referred to as the Bayesian cohort model. The proposed\nsimulation generates artificial data through combinations of the linear\ncomponents, focusing on the fact that the identification problem affects the\nlinear components of the three effects. Among the 13 cases of artificial data,\nthe random walk model recovered the artificial parameters well in 10 cases,\nwhile the random effects model and the ridge regression model did so in 4\ncases. The cases in which the models failed to recover the artificial\nparameters show the estimated linear component of the cohort effects as close\nto zero. In conclusion, the models of Bayesian regularization in APC analysis\nhave a bias: the index weights have a large influence on the cohort effects and\nthese constraints drive the linear component of the cohort effects close to\nzero. However, the random walk model mitigates underestimating the linear\ncomponent of the cohort effects."}, "http://arxiv.org/abs/2310.07330": {"title": "Functional Generalized Canonical Correlation Analysis for studying multiple longitudinal variables", "link": "http://arxiv.org/abs/2310.07330", "description": "In this paper, we introduce Functional Generalized Canonical Correlation\nAnalysis (FGCCA), a new framework for exploring associations between multiple\nrandom processes observed jointly. The framework is based on the multiblock\nRegularized Generalized Canonical Correlation Analysis (RGCCA) framework. It is\nrobust to sparsely and irregularly observed data, making it applicable in many\nsettings. We establish the monotonic property of the solving procedure and\nintroduce a Bayesian approach for estimating canonical components. We propose\nan extension of the framework that allows the integration of a univariate or\nmultivariate response into the analysis, paving the way for predictive\napplications. We evaluate the method's efficiency in simulation studies and\npresent a use case on a longitudinal dataset."}, "http://arxiv.org/abs/2310.07364": {"title": "Statistical inference of high-dimensional vector autoregressive time series with non-i", "link": "http://arxiv.org/abs/2310.07364", "description": "Independent or i.i.d. innovations is an essential assumption in the\nliterature for analyzing a vector time series. However, this assumption is\neither too restrictive for a real-life time series to satisfy or is hard to\nverify through a hypothesis test. This paper performs statistical inference on\na sparse high-dimensional vector autoregressive time series, allowing its white\nnoise innovations to be dependent, even non-stationary. To achieve this goal,\nit adopts a post-selection estimator to fit the vector autoregressive model and\nderives the asymptotic distribution of the post-selection estimator. The\ninnovations in the autoregressive time series are not assumed to be\nindependent, thus making the covariance matrices of the autoregressive\ncoefficient estimators complex and difficult to estimate. Our work develops a\nbootstrap algorithm to facilitate practitioners in performing statistical\ninference without having to engage in sophisticated calculations. Simulations\nand real-life data experiments reveal the validity of the proposed methods and\ntheoretical results.\n\nReal-life data is rarely considered to exactly satisfy an autoregressive\nmodel with independent or i.i.d. innovations, so our work should better reflect\nthe reality compared to the literature that assumes i.i.d. innovations."}, "http://arxiv.org/abs/2310.07399": {"title": "Randomized Runge-Kutta-Nystr\\\"om", "link": "http://arxiv.org/abs/2310.07399", "description": "We present 5/2- and 7/2-order $L^2$-accurate randomized Runge-Kutta-Nystr\\\"om\nmethods to approximate the Hamiltonian flow underlying various non-reversible\nMarkov chain Monte Carlo chains including unadjusted Hamiltonian Monte Carlo\nand unadjusted kinetic Langevin chains. Quantitative 5/2-order $L^2$-accuracy\nupper bounds are provided under gradient and Hessian Lipschitz assumptions on\nthe potential energy function. The superior complexity of the corresponding\nMarkov chains is numerically demonstrated for a selection of `well-behaved',\nhigh-dimensional target distributions."}, "http://arxiv.org/abs/2310.07456": {"title": "Hierarchical Bayesian Claim Count modeling with Overdispersed Outcome and Mismeasured Covariates in Actuarial Practice", "link": "http://arxiv.org/abs/2310.07456", "description": "The problem of overdispersed claim counts and mismeasured covariates is\ncommon in insurance. On the one hand, the presence of overdispersion in the\ncount data violates the homogeneity assumption, and on the other hand,\nmeasurement errors in covariates highlight the model risk issue in actuarial\npractice. The consequence can be inaccurate premium pricing which would\nnegatively affect business competitiveness. Our goal is to address these two\nmodelling problems simultaneously by capturing the unobservable correlations\nbetween observations that arise from overdispersed outcome and mismeasured\ncovariate in actuarial process. To this end, we establish novel connections\nbetween the count-based generalized linear mixed model (GLMM) and a popular\nerror-correction tool for non-linear modelling - Simulation Extrapolation\n(SIMEX). We consider a modelling framework based on the hierarchical Bayesian\nparadigm. To our knowledge, the approach of combining a hierarchical Bayes with\nSIMEX has not previously been discussed in the literature. We demonstrate the\napplicability of our approach on the workplace absenteeism data. Our results\nindicate that the hierarchical Bayesian GLMM incorporated with the SIMEX\noutperforms naive GLMM / SIMEX in terms of goodness of fit."}, "http://arxiv.org/abs/2310.07567": {"title": "Comparing the effectiveness of k-different treatments through the area under the ROC curve", "link": "http://arxiv.org/abs/2310.07567", "description": "The area under the receiver-operating characteristic curve (AUC) has become a\npopular index not only for measuring the overall prediction capacity of a\nmarker but also the association strength between continuous and binary\nvariables. In the current study, it has been used for comparing the association\nsize of four different interventions involving impulsive decision making,\nstudied through an animal model, in which each animal provides several negative\n(pre-treatment) and positive (post-treatment) measures. The problem of the full\ncomparison of the average AUCs arises therefore in a natural way. We construct\nan analysis of variance (ANOVA) type test for testing the equality of the\nimpact of these treatments measured through the respective AUCs, and\nconsidering the random-effect represented by the animal. The use (and\ndevelopment) of a post-hoc Tukey's HSD type test is also considered. We explore\nthe finite-sample behavior of our proposal via Monte Carlo simulations, and\nanalyze the data generated from the original problem. An R package implementing\nthe procedures is provided as supplementary material."}, "http://arxiv.org/abs/2310.07605": {"title": "Split Knockoffs for Multiple Comparisons: Controlling the Directional False Discovery Rate", "link": "http://arxiv.org/abs/2310.07605", "description": "Multiple comparisons in hypothesis testing often encounter structural\nconstraints in various applications. For instance, in structural Magnetic\nResonance Imaging for Alzheimer's Disease, the focus extends beyond examining\natrophic brain regions to include comparisons of anatomically adjacent regions.\nThese constraints can be modeled as linear transformations of parameters, where\nthe sign patterns play a crucial role in estimating directional effects. This\nclass of problems, encompassing total variations, wavelet transforms, fused\nLASSO, trend filtering, and more, presents an open challenge in effectively\ncontrolling the directional false discovery rate. In this paper, we propose an\nextended Split Knockoff method specifically designed to address the control of\ndirectional false discovery rate under linear transformations. Our proposed\napproach relaxes the stringent linear manifold constraint to its neighborhood,\nemploying a variable splitting technique commonly used in optimization. This\nmethodology yields an orthogonal design that benefits both power and\ndirectional false discovery rate control. By incorporating a sample splitting\nscheme, we achieve effective control of the directional false discovery rate,\nwith a notable reduction to zero as the relaxed neighborhood expands. To\ndemonstrate the efficacy of our method, we conduct simulation experiments and\napply it to two real-world scenarios: Alzheimer's Disease analysis and human\nage comparisons."}, "http://arxiv.org/abs/2310.07680": {"title": "Hamiltonian Dynamics of Bayesian Inference Formalised by Arc Hamiltonian Systems", "link": "http://arxiv.org/abs/2310.07680", "description": "This paper makes two theoretical contributions. First, we establish a novel\nclass of Hamiltonian systems, called arc Hamiltonian systems, for saddle\nHamiltonian functions over infinite-dimensional metric spaces. Arc Hamiltonian\nsystems generate a flow that satisfies the law of conservation of energy\neverywhere in a metric space. They are governed by an extension of Hamilton's\nequation formulated based on (i) the framework of arc fields and (ii) an\ninfinite-dimensional gradient, termed the arc gradient, of a Hamiltonian\nfunction. We derive conditions for the existence of a flow generated by an arc\nHamiltonian system, showing that they reduce to local Lipschitz continuity of\nthe arc gradient under sufficient regularity. Second, we present two\nHamiltonian functions, called the cumulant generating functional and the\ncentred cumulant generating functional, over a metric space of log-likelihoods\nand measures. The former characterises the posterior of Bayesian inference as a\npart of the arc gradient that induces a flow of log-likelihoods and\nnon-negative measures. The latter characterises the difference of the posterior\nand the prior as a part of the arc gradient that induces a flow of\nlog-likelihoods and probability measures. Our results reveal an implication of\nthe belief updating mechanism from the prior to the posterior as an\ninfinitesimal change of a measure in the infinite-dimensional Hamiltonian\nflows."}, "http://arxiv.org/abs/2009.12217": {"title": "Latent Causal Socioeconomic Health Index", "link": "http://arxiv.org/abs/2009.12217", "description": "This research develops a model-based LAtent Causal Socioeconomic Health\n(LACSH) index at the national level. Motivated by the need for a holistic\nnational well-being index, we build upon the latent health factor index (LHFI)\napproach that has been used to assess the unobservable ecological/ecosystem\nhealth. LHFI integratively models the relationship between metrics, latent\nhealth, and covariates that drive the notion of health. In this paper, the LHFI\nstructure is integrated with spatial modeling and statistical causal modeling.\nOur efforts are focused on developing the integrated framework to facilitate\nthe understanding of how an observational continuous variable might have\ncausally affected a latent trait that exhibits spatial correlation. A novel\nvisualization technique to evaluate covariate balance is also introduced for\nthe case of a continuous policy (treatment) variable. Our resulting LACSH\nframework and visualization tool are illustrated through two global case\nstudies on national socioeconomic health (latent trait), each with various\nmetrics and covariates pertaining to different aspects of societal health, and\nthe treatment variable being mandatory maternity leave days and government\nexpenditure on healthcare, respectively. We validate our model by two\nsimulation studies. All approaches are structured in a Bayesian hierarchical\nframework and results are obtained by Markov chain Monte Carlo techniques."}, "http://arxiv.org/abs/2201.02958": {"title": "Smooth Nested Simulation: Bridging Cubic and Square Root Convergence Rates in High Dimensions", "link": "http://arxiv.org/abs/2201.02958", "description": "Nested simulation concerns estimating functionals of a conditional\nexpectation via simulation. In this paper, we propose a new method based on\nkernel ridge regression to exploit the smoothness of the conditional\nexpectation as a function of the multidimensional conditioning variable.\nAsymptotic analysis shows that the proposed method can effectively alleviate\nthe curse of dimensionality on the convergence rate as the simulation budget\nincreases, provided that the conditional expectation is sufficiently smooth.\nThe smoothness bridges the gap between the cubic root convergence rate (that\nis, the optimal rate for the standard nested simulation) and the square root\nconvergence rate (that is, the canonical rate for the standard Monte Carlo\nsimulation). We demonstrate the performance of the proposed method via\nnumerical examples from portfolio risk management and input uncertainty\nquantification."}, "http://arxiv.org/abs/2204.12635": {"title": "Multivariate and regression models for directional data based on projected P\\'olya trees", "link": "http://arxiv.org/abs/2204.12635", "description": "Projected distributions have proved to be useful in the study of circular and\ndirectional data. Although any multivariate distribution can be used to produce\na projected model, these distributions are typically parametric. In this\narticle we consider a multivariate P\\'olya tree on $R^k$ and project it to the\nunit hypersphere $S^k$ to define a new Bayesian nonparametric model for\ndirectional data. We study the properties of the proposed model and in\nparticular, concentrate on the implied conditional distributions of some\ndirections given the others to define a directional-directional regression\nmodel. We also define a multivariate linear regression model with P\\'olya tree\nerror and project it to define a linear-directional regression model. We obtain\nthe posterior characterisation of all models and show their performance with\nsimulated and real datasets."}, "http://arxiv.org/abs/2207.13250": {"title": "Spatio-Temporal Wildfire Prediction using Multi-Modal Data", "link": "http://arxiv.org/abs/2207.13250", "description": "Due to severe societal and environmental impacts, wildfire prediction using\nmulti-modal sensing data has become a highly sought-after data-analytical tool\nby various stakeholders (such as state governments and power utility companies)\nto achieve a more informed understanding of wildfire activities and plan\npreventive measures. A desirable algorithm should precisely predict fire risk\nand magnitude for a location in real time. In this paper, we develop a flexible\nspatio-temporal wildfire prediction framework using multi-modal time series\ndata. We first predict the wildfire risk (the chance of a wildfire event) in\nreal-time, considering the historical events using discrete mutually exciting\npoint process models. Then we further develop a wildfire magnitude prediction\nset method based on the flexible distribution-free time-series conformal\nprediction (CP) approach. Theoretically, we prove a risk model parameter\nrecovery guarantee, as well as coverage and set size guarantees for the CP\nsets. Through extensive real-data experiments with wildfire data in California,\nwe demonstrate the effectiveness of our methods, as well as their flexibility\nand scalability in large regions."}, "http://arxiv.org/abs/2210.13550": {"title": "Regularized Nonlinear Regression with Dependent Errors and its Application to a Biomechanical Model", "link": "http://arxiv.org/abs/2210.13550", "description": "A biomechanical model often requires parameter estimation and selection in a\nknown but complicated nonlinear function. Motivated by observing that data from\na head-neck position tracking system, one of biomechanical models, show\nmultiplicative time dependent errors, we develop a modified penalized weighted\nleast squares estimator. The proposed method can be also applied to a model\nwith non-zero mean time dependent additive errors. Asymptotic properties of the\nproposed estimator are investigated under mild conditions on a weight matrix\nand the error process. A simulation study demonstrates that the proposed\nestimation works well in both parameter estimation and selection with time\ndependent error. The analysis and comparison with an existing method for\nhead-neck position tracking data show better performance of the proposed method\nin terms of the variance accounted for (VAF)."}, "http://arxiv.org/abs/2210.14965": {"title": "Topology-Driven Goodness-of-Fit Tests in Arbitrary Dimensions", "link": "http://arxiv.org/abs/2210.14965", "description": "This paper adopts a tool from computational topology, the Euler\ncharacteristic curve (ECC) of a sample, to perform one- and two-sample goodness\nof fit tests. We call our procedure TopoTests. The presented tests work for\nsamples of arbitrary dimension, having comparable power to the state-of-the-art\ntests in the one-dimensional case. It is demonstrated that the type I error of\nTopoTests can be controlled and their type II error vanishes exponentially with\nincreasing sample size. Extensive numerical simulations of TopoTests are\nconducted to demonstrate their power for samples of various sizes."}, "http://arxiv.org/abs/2211.03860": {"title": "Automatic Change-Point Detection in Time Series via Deep Learning", "link": "http://arxiv.org/abs/2211.03860", "description": "Detecting change-points in data is challenging because of the range of\npossible types of change and types of behaviour of data when there is no\nchange. Statistically efficient methods for detecting a change will depend on\nboth of these features, and it can be difficult for a practitioner to develop\nan appropriate detection method for their application of interest. We show how\nto automatically generate new offline detection methods based on training a\nneural network. Our approach is motivated by many existing tests for the\npresence of a change-point being representable by a simple neural network, and\nthus a neural network trained with sufficient data should have performance at\nleast as good as these methods. We present theory that quantifies the error\nrate for such an approach, and how it depends on the amount of training data.\nEmpirical results show that, even with limited training data, its performance\nis competitive with the standard CUSUM-based classifier for detecting a change\nin mean when the noise is independent and Gaussian, and can substantially\noutperform it in the presence of auto-correlated or heavy-tailed noise. Our\nmethod also shows strong results in detecting and localising changes in\nactivity based on accelerometer data."}, "http://arxiv.org/abs/2211.09099": {"title": "Selecting Subpopulations for Causal Inference in Regression Discontinuity Designs", "link": "http://arxiv.org/abs/2211.09099", "description": "The Brazil Bolsa Familia (BF) program is a conditional cash transfer program\naimed to reduce short-term poverty by direct cash transfers and to fight\nlong-term poverty by increasing human capital among poor Brazilian people.\nEligibility for Bolsa Familia benefits depends on a cutoff rule, which\nclassifies the BF study as a regression discontinuity (RD) design. Extracting\ncausal information from RD studies is challenging. Following Li et al (2015)\nand Branson and Mealli (2019), we formally describe the BF RD design as a local\nrandomized experiment within the potential outcome approach. Under this\nframework, causal effects can be identified and estimated on a subpopulation\nwhere a local overlap assumption, a local SUTVA and a local ignorability\nassumption hold. We first discuss the potential advantages of this framework\nover local regression methods based on continuity assumptions, which concern\nthe definition of the causal estimands, the design and the analysis of the\nstudy, and the interpretation and generalizability of the results. A critical\nissue of this local randomization approach is how to choose subpopulations for\nwhich we can draw valid causal inference. We propose a Bayesian model-based\nfinite mixture approach to clustering to classify observations into\nsubpopulations where the RD assumptions hold and do not hold. This approach has\nimportant advantages: a) it allows to account for the uncertainty in the\nsubpopulation membership, which is typically neglected; b) it does not impose\nany constraint on the shape of the subpopulation; c) it is scalable to\nhigh-dimensional settings; e) it allows to target alternative causal estimands\nthan the average treatment effect (ATE); and f) it is robust to a certain\ndegree of manipulation/selection of the running variable. We apply our proposed\napproach to assess causal effects of the Bolsa Familia program on leprosy\nincidence in 2009."}, "http://arxiv.org/abs/2301.08276": {"title": "Cross-validatory model selection for Bayesian autoregressions with exogenous regressors", "link": "http://arxiv.org/abs/2301.08276", "description": "Bayesian cross-validation (CV) is a popular method for predictive model\nassessment that is simple to implement and broadly applicable. A wide range of\nCV schemes is available for time series applications, including generic\nleave-one-out (LOO) and K-fold methods, as well as specialized approaches\nintended to deal with serial dependence such as leave-future-out (LFO),\nh-block, and hv-block.\n\nExisting large-sample results show that both specialized and generic methods\nare applicable to models of serially-dependent data. However, large sample\nconsistency results overlook the impact of sampling variability on accuracy in\nfinite samples. Moreover, the accuracy of a CV scheme depends on many aspects\nof the procedure. We show that poor design choices can lead to elevated rates\nof adverse selection.\n\nIn this paper, we consider the problem of identifying the regression\ncomponent of an important class of models of data with serial dependence,\nautoregressions of order p with q exogenous regressors (ARX(p,q)), under the\nlogarithmic scoring rule. We show that when serial dependence is present,\nscores computed using the joint (multivariate) density have lower variance and\nbetter model selection accuracy than the popular pointwise estimator. In\naddition, we present a detailed case study of the special case of ARX models\nwith fixed autoregressive structure and variance. For this class, we derive the\nfinite-sample distribution of the CV estimators and the model selection\nstatistic. We conclude with recommendations for practitioners."}, "http://arxiv.org/abs/2301.12026": {"title": "G-formula for causal inference via multiple imputation", "link": "http://arxiv.org/abs/2301.12026", "description": "G-formula is a popular approach for estimating treatment or exposure effects\nfrom longitudinal data that are subject to time-varying confounding. G-formula\nestimation is typically performed by Monte-Carlo simulation, with\nnon-parametric bootstrapping used for inference. We show that G-formula can be\nimplemented by exploiting existing methods for multiple imputation (MI) for\nsynthetic data. This involves using an existing modified version of Rubin's\nvariance estimator. In practice missing data is ubiquitous in longitudinal\ndatasets. We show that such missing data can be readily accommodated as part of\nthe MI procedure when using G-formula, and describe how MI software can be used\nto implement the approach. We explore its performance using a simulation study\nand an application from cystic fibrosis."}, "http://arxiv.org/abs/2306.01292": {"title": "Alternative Measures of Direct and Indirect Effects", "link": "http://arxiv.org/abs/2306.01292", "description": "There are a number of measures of direct and indirect effects in the\nliterature. They are suitable in some cases and unsuitable in others. We\ndescribe a case where the existing measures are unsuitable and propose new\nsuitable ones. We also show that the new measures can partially handle\nunmeasured treatment-outcome confounding, and bound long-term effects by\ncombining experimental and observational data."}, "http://arxiv.org/abs/2308.00913": {"title": "The Bayesian Context Trees State Space Model for time series modelling and forecasting", "link": "http://arxiv.org/abs/2308.00913", "description": "A hierarchical Bayesian framework is introduced for developing rich mixture\nmodels for real-valued time series, partly motivated by important applications\nin financial time series analysis. At the top level, meaningful discrete states\nare identified as appropriately quantised values of some of the most recent\nsamples. These observable states are described as a discrete context-tree\nmodel. At the bottom level, a different, arbitrary model for real-valued time\nseries -- a base model -- is associated with each state. This defines a very\ngeneral framework that can be used in conjunction with any existing model class\nto build flexible and interpretable mixture models. We call this the Bayesian\nContext Trees State Space Model, or the BCT-X framework. Efficient algorithms\nare introduced that allow for effective, exact Bayesian inference and learning\nin this setting; in particular, the maximum a posteriori probability (MAP)\ncontext-tree model can be identified. These algorithms can be updated\nsequentially, facilitating efficient online forecasting. The utility of the\ngeneral framework is illustrated in two particular instances: When\nautoregressive (AR) models are used as base models, resulting in a nonlinear AR\nmixture model, and when conditional heteroscedastic (ARCH) models are used,\nresulting in a mixture model that offers a powerful and systematic way of\nmodelling the well-known volatility asymmetries in financial data. In\nforecasting, the BCT-X methods are found to outperform state-of-the-art\ntechniques on simulated and real-world data, both in terms of accuracy and\ncomputational requirements. In modelling, the BCT-X structure finds natural\nstructure present in the data. In particular, the BCT-ARCH model reveals a\nnovel, important feature of stock market index data, in the form of an enhanced\nleverage effect."}, "http://arxiv.org/abs/2309.11942": {"title": "On the Probability of Immunity", "link": "http://arxiv.org/abs/2309.11942", "description": "This work is devoted to the study of the probability of immunity, i.e. the\neffect occurs whether exposed or not. We derive necessary and sufficient\nconditions for non-immunity and $\\epsilon$-bounded immunity, i.e. the\nprobability of immunity is zero and $\\epsilon$-bounded, respectively. The\nformer allows us to estimate the probability of benefit (i.e., the effect\noccurs if and only if exposed) from a randomized controlled trial, and the\nlatter allows us to produce bounds of the probability of benefit that are\ntighter than the existing ones. We also introduce the concept of indirect\nimmunity (i.e., through a mediator) and repeat our previous analysis for it.\nFinally, we propose a method for sensitivity analysis of the probability of\nimmunity under unmeasured confounding."}, "http://arxiv.org/abs/2309.13441": {"title": "Anytime valid and asymptotically optimal statistical inference driven by predictive recursion", "link": "http://arxiv.org/abs/2309.13441", "description": "Distinguishing two classes of candidate models is a fundamental and\npractically important problem in statistical inference. Error rate control is\ncrucial to the logic but, in complex nonparametric settings, such guarantees\ncan be difficult to achieve, especially when the stopping rule that determines\nthe data collection process is not available. In this paper we develop a novel\ne-process construction that leverages the so-called predictive recursion (PR)\nalgorithm designed to rapidly and recursively fit nonparametric mixture models.\nThe resulting PRe-process affords anytime valid inference uniformly over\nstopping rules and is shown to be efficient in the sense that it achieves the\nmaximal growth rate under the alternative relative to the mixture model being\nfit by PR. In the special case of testing for a log-concave density, the\nPRe-process test is computationally simpler and faster, more stable, and no\nless efficient compared to a recently proposed anytime valid test."}, "http://arxiv.org/abs/2309.16598": {"title": "Cross-Prediction-Powered Inference", "link": "http://arxiv.org/abs/2309.16598", "description": "While reliable data-driven decision-making hinges on high-quality labeled\ndata, the acquisition of quality labels often involves laborious human\nannotations or slow and expensive scientific measurements. Machine learning is\nbecoming an appealing alternative as sophisticated predictive techniques are\nbeing used to quickly and cheaply produce large amounts of predicted labels;\ne.g., predicted protein structures are used to supplement experimentally\nderived structures, predictions of socioeconomic indicators from satellite\nimagery are used to supplement accurate survey data, and so on. Since\npredictions are imperfect and potentially biased, this practice brings into\nquestion the validity of downstream inferences. We introduce cross-prediction:\na method for valid inference powered by machine learning. With a small labeled\ndataset and a large unlabeled dataset, cross-prediction imputes the missing\nlabels via machine learning and applies a form of debiasing to remedy the\nprediction inaccuracies. The resulting inferences achieve the desired error\nprobability and are more powerful than those that only leverage the labeled\ndata. Closely related is the recent proposal of prediction-powered inference,\nwhich assumes that a good pre-trained model is already available. We show that\ncross-prediction is consistently more powerful than an adaptation of\nprediction-powered inference in which a fraction of the labeled data is split\noff and used to train the model. Finally, we observe that cross-prediction\ngives more stable conclusions than its competitors; its confidence intervals\ntypically have significantly lower variability."}, "http://arxiv.org/abs/2310.07801": {"title": "Trajectory-aware Principal Manifold Framework for Data Augmentation and Image Generation", "link": "http://arxiv.org/abs/2310.07801", "description": "Data augmentation for deep learning benefits model training, image\ntransformation, medical imaging analysis and many other fields. Many existing\nmethods generate new samples from a parametric distribution, like the Gaussian,\nwith little attention to generate samples along the data manifold in either the\ninput or feature space. In this paper, we verify that there are theoretical and\npractical advantages of using the principal manifold hidden in the feature\nspace than the Gaussian distribution. We then propose a novel trajectory-aware\nprincipal manifold framework to restore the manifold backbone and generate\nsamples along a specific trajectory. On top of the autoencoder architecture, we\nfurther introduce an intrinsic dimension regularization term to make the\nmanifold more compact and enable few-shot image generation. Experimental\nresults show that the novel framework is able to extract more compact manifold\nrepresentation, improve classification accuracy and generate smooth\ntransformation among few samples."}, "http://arxiv.org/abs/2310.07817": {"title": "Nonlinear global Fr\\'echet regression for random objects via weak conditional expectation", "link": "http://arxiv.org/abs/2310.07817", "description": "Random objects are complex non-Euclidean data taking value in general metric\nspace, possibly devoid of any underlying vector space structure. Such data are\ngetting increasingly abundant with the rapid advancement in technology.\nExamples include probability distributions, positive semi-definite matrices,\nand data on Riemannian manifolds. However, except for regression for\nobject-valued response with Euclidean predictors and\ndistribution-on-distribution regression, there has been limited development of\na general framework for object-valued response with object-valued predictors in\nthe literature. To fill this gap, we introduce the notion of a weak conditional\nFr\\'echet mean based on Carleman operators and then propose a global nonlinear\nFr\\'echet regression model through the reproducing kernel Hilbert space (RKHS)\nembedding. Furthermore, we establish the relationships between the conditional\nFr\\'echet mean and the weak conditional Fr\\'echet mean for both Euclidean and\nobject-valued data. We also show that the state-of-the-art global Fr\\'echet\nregression developed by Petersen and Mueller, 2019 emerges as a special case of\nour method by choosing a linear kernel. We require that the metric space for\nthe predictor admits a reproducing kernel, while the intrinsic geometry of the\nmetric space for the response is utilized to study the asymptotic properties of\nthe proposed estimates. Numerical studies, including extensive simulations and\na real application, are conducted to investigate the performance of our\nestimator in a finite sample."}, "http://arxiv.org/abs/2310.07839": {"title": "Marital Sorting, Household Inequality and Selection", "link": "http://arxiv.org/abs/2310.07839", "description": "Using CPS data for 1976 to 2022 we explore how wage inequality has evolved\nfor married couples with both spouses working full time full year, and its\nimpact on household income inequality. We also investigate how marriage sorting\npatterns have changed over this period. To determine the factors driving income\ninequality we estimate a model explaining the joint distribution of wages which\naccounts for the spouses' employment decisions. We find that income inequality\nhas increased for these households and increased assortative matching of wages\nhas exacerbated the inequality resulting from individual wage growth. We find\nthat positive sorting partially reflects the correlation across unobservables\ninfluencing both members' of the marriage wages. We decompose the changes in\nsorting patterns over the 47 years comprising our sample into structural,\ncomposition and selection effects and find that the increase in positive\nsorting primarily reflects the increased skill premia for both observed and\nunobserved characteristics."}, "http://arxiv.org/abs/2310.07850": {"title": "Conformal prediction with local weights: randomization enables local guarantees", "link": "http://arxiv.org/abs/2310.07850", "description": "In this work, we consider the problem of building distribution-free\nprediction intervals with finite-sample conditional coverage guarantees.\nConformal prediction (CP) is an increasingly popular framework for building\nprediction intervals with distribution-free guarantees, but these guarantees\nonly ensure marginal coverage: the probability of coverage is averaged over a\nrandom draw of both the training and test data, meaning that there might be\nsubstantial undercoverage within certain subpopulations. Instead, ideally, we\nwould want to have local coverage guarantees that hold for each possible value\nof the test point's features. While the impossibility of achieving pointwise\nlocal coverage is well established in the literature, many variants of\nconformal prediction algorithm show favorable local coverage properties\nempirically. Relaxing the definition of local coverage can allow for a\ntheoretical understanding of this empirical phenomenon. We aim to bridge this\ngap between theoretical validation and empirical performance by proving\nachievable and interpretable guarantees for a relaxed notion of local coverage.\nBuilding on the localized CP method of Guan (2023) and the weighted CP\nframework of Tibshirani et al. (2019), we propose a new method,\nrandomly-localized conformal prediction (RLCP), which returns prediction\nintervals that are not only marginally valid but also achieve a relaxed local\ncoverage guarantee and guarantees under covariate shift. Through a series of\nsimulations and real data experiments, we validate these coverage guarantees of\nRLCP while comparing it with the other local conformal prediction methods."}, "http://arxiv.org/abs/2310.07852": {"title": "On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism", "link": "http://arxiv.org/abs/2310.07852", "description": "We consider the problem of model selection in a high-dimensional sparse\nlinear regression model under the differential privacy framework. In\nparticular, we consider the problem of differentially private best subset\nselection and study its utility guarantee. We adopt the well-known exponential\nmechanism for selecting the best model, and under a certain margin condition,\nwe establish its strong model recovery property. However, the exponential\nsearch space of the exponential mechanism poses a serious computational\nbottleneck. To overcome this challenge, we propose a Metropolis-Hastings\nalgorithm for the sampling step and establish its polynomial mixing time to its\nstationary distribution in the problem parameters $n,p$, and $s$. Furthermore,\nwe also establish approximate differential privacy for the final estimates of\nthe Metropolis-Hastings random walk using its mixing property. Finally, we also\nperform some illustrative simulations that echo the theoretical findings of our\nmain results."}, "http://arxiv.org/abs/2310.07935": {"title": "Estimating the Likelihood of Arrest from Police Records in Presence of Unreported Crimes", "link": "http://arxiv.org/abs/2310.07935", "description": "Many important policy decisions concerning policing hinge on our\nunderstanding of how likely various criminal offenses are to result in arrests.\nSince many crimes are never reported to law enforcement, estimates based on\npolice records alone must be adjusted to account for the likelihood that each\ncrime would have been reported to the police. In this paper, we present a\nmethodological framework for estimating the likelihood of arrest from police\ndata that incorporates estimates of crime reporting rates computed from a\nvictimization survey. We propose a parametric regression-based two-step\nestimator that (i) estimates the likelihood of crime reporting using logistic\nregression with survey weights; and then (ii) applies a second regression step\nto model the likelihood of arrest. Our empirical analysis focuses on racial\ndisparities in arrests for violent crimes (sex offenses, robbery, aggravated\nand simple assaults) from 2006--2015 police records from the National Incident\nBased Reporting System (NIBRS), with estimates of crime reporting obtained\nusing 2003--2020 data from the National Crime Victimization Survey (NCVS). We\nfind that, after adjusting for unreported crimes, the likelihood of arrest\ncomputed from police records decreases significantly. We also find that, while\nincidents with white offenders on average result in arrests more often than\nthose with black offenders, the disparities tend to be small after accounting\nfor crime characteristics and unreported crimes."}, "http://arxiv.org/abs/2310.07953": {"title": "Enhancing Sample Quality through Minimum Energy Importance Weights", "link": "http://arxiv.org/abs/2310.07953", "description": "Importance sampling is a powerful tool for correcting the distributional\nmismatch in many statistical and machine learning problems, but in practice its\nperformance is limited by the usage of simple proposals whose importance\nweights can be computed analytically. To address this limitation, Liu and Lee\n(2017) proposed a Black-Box Importance Sampling (BBIS) algorithm that computes\nthe importance weights for arbitrary simulated samples by minimizing the\nkernelized Stein discrepancy. However, this requires knowing the score function\nof the target distribution, which is not easy to compute for many Bayesian\nproblems. Hence, in this paper we propose another novel BBIS algorithm using\nminimum energy design, BBIS-MED, that requires only the unnormalized density\nfunction, which can be utilized as a post-processing step to improve the\nquality of Markov Chain Monte Carlo samples. We demonstrate the effectiveness\nand wide applicability of our proposed BBIS-MED algorithm on extensive\nsimulations and a real-world Bayesian model calibration problem where the score\nfunction cannot be derived analytically."}, "http://arxiv.org/abs/2310.07958": {"title": "Towards Causal Deep Learning for Vulnerability Detection", "link": "http://arxiv.org/abs/2310.07958", "description": "Deep learning vulnerability detection has shown promising results in recent\nyears. However, an important challenge that still blocks it from being very\nuseful in practice is that the model is not robust under perturbation and it\ncannot generalize well over the out-of-distribution (OOD) data, e.g., applying\na trained model to unseen projects in real world. We hypothesize that this is\nbecause the model learned non-robust features, e.g., variable names, that have\nspurious correlations with labels. When the perturbed and OOD datasets no\nlonger have the same spurious features, the model prediction fails. To address\nthe challenge, in this paper, we introduced causality into deep learning\nvulnerability detection. Our approach CausalVul consists of two phases. First,\nwe designed novel perturbations to discover spurious features that the model\nmay use to make predictions. Second, we applied the causal learning algorithms,\nspecifically, do-calculus, on top of existing deep learning models to\nsystematically remove the use of spurious features and thus promote causal\nbased prediction. Our results show that CausalVul consistently improved the\nmodel accuracy, robustness and OOD performance for all the state-of-the-art\nmodels and datasets we experimented. To the best of our knowledge, this is the\nfirst work that introduces do calculus based causal learning to software\nengineering models and shows it's indeed useful for improving the model\naccuracy, robustness and generalization. Our replication package is located at\nhttps://figshare.com/s/0ffda320dcb96c249ef2."}, "http://arxiv.org/abs/2310.07973": {"title": "Statistical Performance Guarantee for Selecting Those Predicted to Benefit Most from Treatment", "link": "http://arxiv.org/abs/2310.07973", "description": "Across a wide array of disciplines, many researchers use machine learning\n(ML) algorithms to identify a subgroup of individuals, called exceptional\nresponders, who are likely to be helped by a treatment the most. A common\napproach consists of two steps. One first estimates the conditional average\ntreatment effect or its proxy using an ML algorithm. They then determine the\ncutoff of the resulting treatment prioritization score to select those\npredicted to benefit most from the treatment. Unfortunately, these estimated\ntreatment prioritization scores are often biased and noisy. Furthermore,\nutilizing the same data to both choose a cutoff value and estimate the average\ntreatment effect among the selected individuals suffer from a multiple testing\nproblem. To address these challenges, we develop a uniform confidence band for\nexperimentally evaluating the sorted average treatment effect (GATES) among the\nindividuals whose treatment prioritization score is at least as high as any\ngiven quantile value, regardless of how the quantile is chosen. This provides a\nstatistical guarantee that the GATES for the selected subgroup exceeds a\ncertain threshold. The validity of the proposed methodology depends solely on\nrandomization of treatment and random sampling of units without requiring\nmodeling assumptions or resampling methods. This widens its applicability\nincluding a wide range of other causal quantities. A simulation study shows\nthat the empirical coverage of the proposed uniform confidence bands is close\nto the nominal coverage when the sample is as small as 100. We analyze a\nclinical trial of late-stage prostate cancer and find a relatively large\nproportion of exceptional responders with a statistical performance guarantee."}, "http://arxiv.org/abs/2310.08020": {"title": "Assessing Copula Models for Mixed Continuous-Ordinal Variables", "link": "http://arxiv.org/abs/2310.08020", "description": "Vine pair-copula constructions exist for a mix of continuous and ordinal\nvariables. In some steps, this can involve estimating a bivariate copula for a\npair of mixed continuous-ordinal variables. To assess the adequacy of copula\nfits for such a pair, diagnostic and visualization methods based on normal\nscore plots and conditional Q-Q plots are proposed. The former utilizes a\nlatent continuous variable for the ordinal variable. Using the Kullback-Leibler\ndivergence, existing probability models for mixed continuous-ordinal variable\npair are assessed for the adequacy of fit with simple parametric copula\nfamilies. The effectiveness of the proposed visualization and diagnostic\nmethods is illustrated on simulated and real datasets."}, "http://arxiv.org/abs/2310.08115": {"title": "Model-Agnostic Covariate-Assisted Inference on Partially Identified Causal Effects", "link": "http://arxiv.org/abs/2310.08115", "description": "Many causal estimands are only partially identifiable since they depend on\nthe unobservable joint distribution between potential outcomes. Stratification\non pretreatment covariates can yield sharper partial identification bounds;\nhowever, unless the covariates are discrete with relatively small support, this\napproach typically requires consistent estimation of the conditional\ndistributions of the potential outcomes given the covariates. Thus, existing\napproaches may fail under model misspecification or if consistency assumptions\nare violated. In this study, we propose a unified and model-agnostic\ninferential approach for a wide class of partially identified estimands, based\non duality theory for optimal transport problems. In randomized experiments,\nour approach can wrap around any estimates of the conditional distributions and\nprovide uniformly valid inference, even if the initial estimates are\narbitrarily inaccurate. Also, our approach is doubly robust in observational\nstudies. Notably, this property allows analysts to use the multiplier bootstrap\nto select covariates and models without sacrificing validity even if the true\nmodel is not included. Furthermore, if the conditional distributions are\nestimated at semiparametric rates, our approach matches the performance of an\noracle with perfect knowledge of the outcome model. Finally, we propose an\nefficient computational framework, enabling implementation on many practical\nproblems in causal inference."}, "http://arxiv.org/abs/2310.08193": {"title": "Are sanctions for losers? A network study of trade sanctions", "link": "http://arxiv.org/abs/2310.08193", "description": "Studies built on dependency and world-system theory using network approaches\nhave shown that international trade is structured into clusters of 'core' and\n'peripheral' countries performing distinct functions. However, few have used\nthese methods to investigate how sanctions affect the position of the countries\ninvolved in the capitalist world-economy. Yet, this topic has acquired pressing\nrelevance due to the emergence of economic warfare as a key geopolitical weapon\nsince the 1950s. And even more so in light of the preeminent role that\nsanctions have played in the US and their allies' response to the\nRussian-Ukrainian war. Applying several clustering techniques designed for\ncomplex and temporal networks, this paper shows that a shift in the pattern of\ncommerce away from sanctioning countries and towards neutral or friendly ones.\nAdditionally, there are suggestions that these shifts may lead to the creation\nof an alternative 'core' that interacts with the world-economy's periphery\nbypassing traditional 'core' countries such as EU member States and the US."}, "http://arxiv.org/abs/2310.08268": {"title": "Change point detection in dynamic heterogeneous networks via subspace tracking", "link": "http://arxiv.org/abs/2310.08268", "description": "Dynamic networks consist of a sequence of time-varying networks, and it is of\ngreat importance to detect the network change points. Most existing methods\nfocus on detecting abrupt change points, necessitating the assumption that the\nunderlying network probability matrix remains constant between adjacent change\npoints. This paper introduces a new model that allows the network probability\nmatrix to undergo continuous shifting, while the latent network structure,\nrepresented via the embedding subspace, only changes at certain time points.\nTwo novel statistics are proposed to jointly detect these network subspace\nchange points, followed by a carefully refined detection procedure.\nTheoretically, we show that the proposed method is asymptotically consistent in\nterms of change point detection, and also establish the impossibility region\nfor detecting these network subspace change points. The advantage of the\nproposed method is also supported by extensive numerical experiments on both\nsynthetic networks and a UK politician social network."}, "http://arxiv.org/abs/2310.08397": {"title": "Assessing Marine Mammal Abundance: A Novel Data Fusion", "link": "http://arxiv.org/abs/2310.08397", "description": "Marine mammals are increasingly vulnerable to human disturbance and climate\nchange. Their diving behavior leads to limited visual access during data\ncollection, making studying the abundance and distribution of marine mammals\nchallenging. In theory, using data from more than one observation modality\nshould lead to better informed predictions of abundance and distribution. With\nfocus on North Atlantic right whales, we consider the fusion of two data\nsources to inform about their abundance and distribution. The first source is\naerial distance sampling which provides the spatial locations of whales\ndetected in the region. The second source is passive acoustic monitoring (PAM),\nreturning calls received at hydrophones placed on the ocean floor. Due to\nlimited time on the surface and detection limitations arising from sampling\neffort, aerial distance sampling only provides a partial realization of\nlocations. With PAM, we never observe numbers or locations of individuals. To\naddress these challenges, we develop a novel thinned point pattern data fusion.\nOur approach leads to improved inference regarding abundance and distribution\nof North Atlantic right whales throughout Cape Cod Bay, Massachusetts in the\nUS. We demonstrate performance gains of our approach compared to that from a\nsingle source through both simulation and real data."}, "http://arxiv.org/abs/2310.08410": {"title": "Evaluation of ChatGPT-Generated Medical Responses: A Systematic Review and Meta-Analysis", "link": "http://arxiv.org/abs/2310.08410", "description": "Large language models such as ChatGPT are increasingly explored in medical\ndomains. However, the absence of standard guidelines for performance evaluation\nhas led to methodological inconsistencies. This study aims to summarize the\navailable evidence on evaluating ChatGPT's performance in medicine and provide\ndirection for future research. We searched ten medical literature databases on\nJune 15, 2023, using the keyword \"ChatGPT\". A total of 3520 articles were\nidentified, of which 60 were reviewed and summarized in this paper and 17 were\nincluded in the meta-analysis. The analysis showed that ChatGPT displayed an\noverall integrated accuracy of 56% (95% CI: 51%-60%, I2 = 87%) in addressing\nmedical queries. However, the studies varied in question resource,\nquestion-asking process, and evaluation metrics. Moreover, many studies failed\nto report methodological details, including the version of ChatGPT and whether\neach question was used independently or repeatedly. Our findings revealed that\nalthough ChatGPT demonstrated considerable potential for application in\nhealthcare, the heterogeneity of the studies and insufficient reporting may\naffect the reliability of these results. Further well-designed studies with\ncomprehensive and transparent reporting are needed to evaluate ChatGPT's\nperformance in medicine."}, "http://arxiv.org/abs/2310.08414": {"title": "Confidence bounds for the true discovery proportion based on the exact distribution of the number of rejections", "link": "http://arxiv.org/abs/2310.08414", "description": "In multiple hypotheses testing it has become widely popular to make inference\non the true discovery proportion (TDP) of a set $\\mathcal{M}$ of null\nhypotheses. This approach is useful for several application fields, such as\nneuroimaging and genomics. Several procedures to compute simultaneous lower\nconfidence bounds for the TDP have been suggested in prior literature.\nSimultaneity allows for post-hoc selection of $\\mathcal{M}$. If sets of\ninterest are specified a priori, it is possible to gain power by removing the\nsimultaneity requirement. We present an approach to compute lower confidence\nbounds for the TDP if the set of null hypotheses is defined a priori. The\nproposed method determines the bounds using the exact distribution of the\nnumber of rejections based on a step-up multiple testing procedure under\nindependence assumptions. We assess robustness properties of our procedure and\napply it to real data from the field of functional magnetic resonance imaging."}, "http://arxiv.org/abs/2310.08426": {"title": "Extensions of Heterogeneity in Integration and Prediction (HIP) with R Shiny Application", "link": "http://arxiv.org/abs/2310.08426", "description": "Multiple data views measured on the same set of participants is becoming more\ncommon and has the potential to deepen our understanding of many complex\ndiseases by analyzing these different views simultaneously. Equally important,\nmany of these complex diseases show evidence of subgroup heterogeneity (e.g.,\nby sex or race). HIP (Heterogeneity in Integration and Prediction) is among the\nfirst methods proposed to integrate multiple data views while also accounting\nfor subgroup heterogeneity to identify common and subgroup-specific markers of\na particular disease. However, HIP is applicable to continuous outcomes and\nrequires programming expertise by the user. Here we propose extensions to HIP\nthat accommodate multi-class, Poisson, and Zero-Inflated Poisson outcomes while\nretaining the benefits of HIP. Additionally, we introduce an R Shiny\napplication, accessible on shinyapps.io at\nhttps://multi-viewlearn.shinyapps.io/HIP_ShinyApp/, that provides an interface\nwith the Python implementation of HIP to allow more researchers to use the\nmethod anywhere and on any device. We applied HIP to identify genes and\nproteins common and specific to males and females that are associated with\nexacerbation frequency. Although some of the identified genes and proteins show\nevidence of a relationship with chronic obstructive pulmonary disease (COPD) in\nexisting literature, others may be candidates for future research investigating\ntheir relationship with COPD. We demonstrate the use of the Shiny application\nwith a publicly available data. An R-package for HIP would be made available at\nhttps://github.com/lasandrall/HIP."}, "http://arxiv.org/abs/2310.08479": {"title": "Personalised dynamic super learning: an application in predicting hemodiafiltration's convection volumes", "link": "http://arxiv.org/abs/2310.08479", "description": "Obtaining continuously updated predictions is a major challenge for\npersonalised medicine. Leveraging combinations of parametric regressions and\nmachine learning approaches, the personalised online super learner (POSL) can\nachieve such dynamic and personalised predictions. We adapt POSL to predict a\nrepeated continuous outcome dynamically and propose a new way to validate such\npersonalised or dynamic prediction models. We illustrate its performance by\npredicting the convection volume of patients undergoing hemodiafiltration. POSL\noutperformed its candidate learners with respect to median absolute error,\ncalibration-in-the-large, discrimination, and net benefit. We finally discuss\nthe choices and challenges underlying the use of POSL."}, "http://arxiv.org/abs/1903.00037": {"title": "Distance-Based Independence Screening for Canonical Analysis", "link": "http://arxiv.org/abs/1903.00037", "description": "This paper introduces a novel method called Distance-Based Independence\nScreening for Canonical Analysis (DISCA) that performs simultaneous dimension\nreduction for a pair of random variables by optimizing the distance covariance\n(dCov). dCov is a statistic first proposed by Sz\\'ekely et al. [2009] for\nindependence testing. Compared with sufficient dimension reduction (SDR) and\ncanonical correlation analysis (CCA)-based approaches, DISCA is a model-free\napproach that does not impose dimensional or distributional restrictions on\nvariables and is more sensitive to nonlinear relationships. Theoretically, we\nestablish a non-asymptotic error bound to provide a guarantee of our method's\nperformance. Numerically, DISCA performs comparable to or better than other\nstate-of-the-art algorithms and is computationally faster. All codes of our\nDISCA method can be found on GitHub https : //github.com/Yijin911/DISCA.git,\nincluding an R package named DISCA."}, "http://arxiv.org/abs/2105.13952": {"title": "Generalized Permutation Framework for Testing Model Variable Significance", "link": "http://arxiv.org/abs/2105.13952", "description": "A common problem in machine learning is determining if a variable\nsignificantly contributes to a model's prediction performance. This problem is\naggravated for datasets, such as gene expression datasets, that suffer the\nworst case of dimensionality: a low number of observations along with a high\nnumber of possible explanatory variables. In such scenarios, traditional\nmethods for testing variable statistical significance or constructing variable\nconfidence intervals do not apply. To address these problems, we developed a\nnovel permutation framework for testing the significance of variables in\nsupervised models. Our permutation framework has three main advantages. First,\nit is non-parametric and does not rely on distributional assumptions or\nasymptotic results. Second, it not only ranks model variables in terms of\nrelative importance, but also tests for statistical significance of each\nvariable. Third, it can test for the significance of the interaction between\nmodel variables. We applied this permutation framework to multi-class\nclassification of the Iris flower dataset and of brain regions in RNA\nexpression data, and using this framework showed variable-level statistical\nsignificance and interactions."}, "http://arxiv.org/abs/2210.02002": {"title": "Factor Augmented Sparse Throughput Deep ReLU Neural Networks for High Dimensional Regression", "link": "http://arxiv.org/abs/2210.02002", "description": "This paper introduces a Factor Augmented Sparse Throughput (FAST) model that\nutilizes both latent factors and sparse idiosyncratic components for\nnonparametric regression. The FAST model bridges factor models on one end and\nsparse nonparametric models on the other end. It encompasses structured\nnonparametric models such as factor augmented additive models and sparse\nlow-dimensional nonparametric interaction models and covers the cases where the\ncovariates do not admit factor structures. Via diversified projections as\nestimation of latent factor space, we employ truncated deep ReLU networks to\nnonparametric factor regression without regularization and to a more general\nFAST model using nonconvex regularization, resulting in factor augmented\nregression using neural network (FAR-NN) and FAST-NN estimators respectively.\nWe show that FAR-NN and FAST-NN estimators adapt to the unknown low-dimensional\nstructure using hierarchical composition models in nonasymptotic minimax rates.\nWe also study statistical learning for the factor augmented sparse additive\nmodel using a more specific neural network architecture. Our results are\napplicable to the weak dependent cases without factor structures. In proving\nthe main technical result for FAST-NN, we establish a new deep ReLU network\napproximation result that contributes to the foundation of neural network\ntheory. Our theory and methods are further supported by simulation studies and\nan application to macroeconomic data."}, "http://arxiv.org/abs/2210.04482": {"title": "Leave-group-out cross-validation for latent Gaussian models", "link": "http://arxiv.org/abs/2210.04482", "description": "Evaluating the predictive performance of a statistical model is commonly done\nusing cross-validation. Although the leave-one-out method is frequently\nemployed, its application is justified primarily for independent and\nidentically distributed observations. However, this method tends to mimic\ninterpolation rather than prediction when dealing with dependent observations.\nThis paper proposes a modified cross-validation for dependent observations.\nThis is achieved by excluding an automatically determined set of observations\nfrom the training set to mimic a more reasonable prediction scenario. Also,\nwithin the framework of latent Gaussian models, we illustrate a method to\nadjust the joint posterior for this modified cross-validation to avoid model\nrefitting. This new approach is accessible in the R-INLA package\n(www.r-inla.org)."}, "http://arxiv.org/abs/2210.11355": {"title": "Network Synthetic Interventions: A Causal Framework for Panel Data Under Network Interference", "link": "http://arxiv.org/abs/2210.11355", "description": "We propose a generalization of the synthetic controls and synthetic\ninterventions methodology to incorporate network interference. We consider the\nestimation of unit-specific potential outcomes from panel data in the presence\nof spillover across units and unobserved confounding. Key to our approach is a\nnovel latent factor model that takes into account network interference and\ngeneralizes the factor models typically used in panel data settings. We propose\nan estimator, Network Synthetic Interventions (NSI), and show that it\nconsistently estimates the mean outcomes for a unit under an arbitrary set of\ncounterfactual treatments for the network. We further establish that the\nestimator is asymptotically normal. We furnish two validity tests for whether\nthe NSI estimator reliably generalizes to produce accurate counterfactual\nestimates. We provide a novel graph-based experiment design that guarantees the\nNSI estimator produces accurate counterfactual estimates, and also analyze the\nsample complexity of the proposed design. We conclude with simulations that\ncorroborate our theoretical findings."}, "http://arxiv.org/abs/2212.01179": {"title": "Feasibility of using survey data and semi-variogram kriging to obtain bespoke indices of neighbourhood characteristics: a simulation and a case study", "link": "http://arxiv.org/abs/2212.01179", "description": "Data on neighbourhood characteristics are not typically collected in\nepidemiological studies. They are however useful in the study of small-area\nhealth inequalities. Neighbourhood characteristics are collected in some\nsurveys and could be linked to the data of other studies. We propose to use\nkriging based on semi-variogram models to predict values at non-observed\nlocations with the aim of constructing bespoke indices of neighbourhood\ncharacteristics to be linked to data from epidemiological studies. We perform a\nsimulation study to assess the feasibility of the method as well as a case\nstudy using data from the RECORD study. Apart from having enough observed data\nat small distances to the non-observed locations, a good fitting\nsemi-variogram, a larger range and the absence of nugget effects for the\nsemi-variogram models are factors leading to a higher reliability."}, "http://arxiv.org/abs/2303.17823": {"title": "An interpretable neural network-based non-proportional odds model for ordinal regression", "link": "http://arxiv.org/abs/2303.17823", "description": "This study proposes an interpretable neural network-based non-proportional\nodds model (N$^3$POM) for ordinal regression. N$^3$POM is different from\nconventional approaches to ordinal regression with non-proportional models in\nseveral ways: (1) N$^3$POM is designed to directly handle continuous responses,\nwhereas standard methods typically treat de facto ordered continuous variables\nas discrete, (2) instead of estimating response-dependent finite coefficients\nof linear models from discrete responses as is done in conventional approaches,\nwe train a non-linear neural network to serve as a coefficient function. Thanks\nto the neural network, N$^3$POM offers flexibility while preserving the\ninterpretability of conventional ordinal regression. We establish a sufficient\ncondition under which the predicted conditional cumulative probability locally\nsatisfies the monotonicity constraint over a user-specified region in the\ncovariate space. Additionally, we provide a monotonicity-preserving stochastic\n(MPS) algorithm for effectively training the neural network. We apply N$^3$POM\nto several real-world datasets."}, "http://arxiv.org/abs/2306.16335": {"title": "Emulating the dynamics of complex systems using autoregressive models on manifolds (mNARX)", "link": "http://arxiv.org/abs/2306.16335", "description": "We propose a novel surrogate modelling approach to efficiently and accurately\napproximate the response of complex dynamical systems driven by time-varying\nexogenous excitations over extended time periods. Our approach, namely manifold\nnonlinear autoregressive modelling with exogenous input (mNARX), involves\nconstructing a problem-specific exogenous input manifold that is optimal for\nconstructing autoregressive surrogates. The manifold, which forms the core of\nmNARX, is constructed incrementally by incorporating the physics of the system,\nas well as prior expert- and domain- knowledge. Because mNARX decomposes the\nfull problem into a series of smaller sub-problems, each with a lower\ncomplexity than the original, it scales well with the complexity of the\nproblem, both in terms of training and evaluation costs of the final surrogate.\nFurthermore, mNARX synergizes well with traditional dimensionality reduction\ntechniques, making it highly suitable for modelling dynamical systems with\nhigh-dimensional exogenous inputs, a class of problems that is typically\nchallenging to solve. Since domain knowledge is particularly abundant in\nphysical systems, such as those found in civil and mechanical engineering,\nmNARX is well suited for these applications. We demonstrate that mNARX\noutperforms traditional autoregressive surrogates in predicting the response of\na classical coupled spring-mass system excited by a one-dimensional random\nexcitation. Additionally, we show that mNARX is well suited for emulating very\nhigh-dimensional time- and state-dependent systems, even when affected by\nactive controllers, by surrogating the dynamics of a realistic\naero-servo-elastic onshore wind turbine simulator. In general, our results\ndemonstrate that mNARX offers promising prospects for modelling complex\ndynamical systems, in terms of accuracy and efficiency."}, "http://arxiv.org/abs/2307.02236": {"title": "D-optimal Subsampling Design for Massive Data Linear Regression", "link": "http://arxiv.org/abs/2307.02236", "description": "Data reduction is a fundamental challenge of modern technology, where\nclassical statistical methods are not applicable because of computational\nlimitations. We consider linear regression for an extraordinarily large number\nof observations, but only a few covariates. Subsampling aims at the selection\nof a given percentage of the existing original data. Under distributional\nassumptions on the covariates, we derive D-optimal subsampling designs and\nstudy their theoretical properties. We make use of fundamental concepts of\noptimal design theory and an equivalence theorem from constrained convex\noptimization. The thus obtained subsampling designs provide simple rules for\nwhether to accept or reject a data point, allowing for an easy algorithmic\nimplementation. In addition, we propose a simplified subsampling method that\ndiffers from the D-optimal design but requires lower computing time. We present\na simulation study, comparing both subsampling schemes with the IBOSS method."}, "http://arxiv.org/abs/2310.08672": {"title": "Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal", "link": "http://arxiv.org/abs/2310.08672", "description": "In many settings, interventions may be more effective for some individuals\nthan others, so that targeting interventions may be beneficial. We analyze the\nvalue of targeting in the context of a large-scale field experiment with over\n53,000 college students, where the goal was to use \"nudges\" to encourage\nstudents to renew their financial-aid applications before a non-binding\ndeadline. We begin with baseline approaches to targeting. First, we target\nbased on a causal forest that estimates heterogeneous treatment effects and\nthen assigns students to treatment according to those estimated to have the\nhighest treatment effects. Next, we evaluate two alternative targeting\npolicies, one targeting students with low predicted probability of renewing\nfinancial aid in the absence of the treatment, the other targeting those with\nhigh probability. The predicted baseline outcome is not the ideal criterion for\ntargeting, nor is it a priori clear whether to prioritize low, high, or\nintermediate predicted probability. Nonetheless, targeting on low baseline\noutcomes is common in practice, for example because the relationship between\nindividual characteristics and treatment effects is often difficult or\nimpossible to estimate with historical data. We propose hybrid approaches that\nincorporate the strengths of both predictive approaches (accurate estimation)\nand causal approaches (correct criterion); we show that targeting intermediate\nbaseline outcomes is most effective, while targeting based on low baseline\noutcomes is detrimental. In one year of the experiment, nudging all students\nimproved early filing by an average of 6.4 percentage points over a baseline\naverage of 37% filing, and we estimate that targeting half of the students\nusing our preferred policy attains around 75% of this benefit."}, "http://arxiv.org/abs/2310.08726": {"title": "Design-Based RCT Estimators and Central Limit Theorems for Baseline Subgroup and Related Analyses", "link": "http://arxiv.org/abs/2310.08726", "description": "There is a growing literature on design-based methods to estimate average\ntreatment effects (ATEs) for randomized controlled trials (RCTs) for full\nsample analyses. This article extends these methods to estimate ATEs for\ndiscrete subgroups defined by pre-treatment variables, with an application to\nan RCT testing subgroup effects for a school voucher experiment in New York\nCity. We consider ratio estimators for subgroup effects using regression\nmethods, allowing for model covariates to improve precision, and prove a finite\npopulation central limit theorem. We discuss extensions to blocked and\nclustered RCT designs, and to other common estimators with random\ntreatment-control sample sizes (or weights): post-stratification estimators,\nweighted estimators that adjust for data nonresponse, and estimators for\nBernoulli trials. We also develop simple variance estimators that share\nfeatures with robust estimators. Simulations show that the design-based\nsubgroup estimators yield confidence interval coverage near nominal levels,\neven for small subgroups."}, "http://arxiv.org/abs/2310.08798": {"title": "Alteration Detection of Tensor Dependence Structure via Sparsity-Exploited Reranking Algorithm", "link": "http://arxiv.org/abs/2310.08798", "description": "Tensor-valued data arise frequently from a wide variety of scientific\napplications, and many among them can be translated into an alteration\ndetection problem of tensor dependence structures. In this article, we\nformulate the problem under the popularly adopted tensor-normal distributions\nand aim at two-sample correlation/partial correlation comparisons of\ntensor-valued observations. Through decorrelation and centralization, a\nseparable covariance structure is employed to pool sample information from\ndifferent tensor modes to enhance the power of the test. Additionally, we\npropose a novel Sparsity-Exploited Reranking Algorithm (SERA) to further\nimprove the multiple testing efficiency. The algorithm is approached through\nreranking of the p-values derived from the primary test statistics, by\nincorporating a carefully constructed auxiliary tensor sequence. Besides the\ntensor framework, SERA is also generally applicable to a wide range of\ntwo-sample large-scale inference problems with sparsity structures, and is of\nindependent interest. The asymptotic properties of the proposed test are\nderived and the algorithm is shown to control the false discovery at the\npre-specified level. We demonstrate the efficacy of the proposed method through\nintensive simulations and two scientific applications."}, "http://arxiv.org/abs/2310.08812": {"title": "A Nonlinear Method for time series forecasting using VMD-GARCH-LSTM model", "link": "http://arxiv.org/abs/2310.08812", "description": "Time series forecasting represents a significant and challenging task across\nvarious fields. Recently, methods based on mode decomposition have dominated\nthe forecasting of complex time series because of the advantages of capturing\nlocal characteristics and extracting intrinsic modes from data. Unfortunately,\nmost models fail to capture the implied volatilities that contain significant\ninformation. To enhance the forecasting of current, rapidly evolving, and\nvolatile time series, we propose a novel decomposition-ensemble paradigm, the\nVMD-LSTM-GARCH model. The Variational Mode Decomposition algorithm is employed\nto decompose the time series into K sub-modes. Subsequently, the GARCH model\nextracts the volatility information from these sub-modes, which serve as the\ninput for the LSTM. The numerical and volatility information of each sub-mode\nis utilized to train a Long Short-Term Memory network. This network predicts\nthe sub-mode, and then we aggregate the predictions from all sub-modes to\nproduce the output. By integrating econometric and artificial intelligence\nmethods, and taking into account both the numerical and volatility information\nof the time series, our proposed model demonstrates superior performance in\ntime series forecasting, as evidenced by the significant decrease in MSE, RMSE,\nand MAPE in our comparative experimental results."}, "http://arxiv.org/abs/2310.08867": {"title": "A Survey of Methods for Handling Disk Data Imbalance", "link": "http://arxiv.org/abs/2310.08867", "description": "Class imbalance exists in many classification problems, and since the data is\ndesigned for accuracy, imbalance in data classes can lead to classification\nchallenges with a few classes having higher misclassification costs. The\nBackblaze dataset, a widely used dataset related to hard discs, has a small\namount of failure data and a large amount of health data, which exhibits a\nserious class imbalance. This paper provides a comprehensive overview of\nresearch in the field of imbalanced data classification. The discussion is\norganized into three main aspects: data-level methods, algorithmic-level\nmethods, and hybrid methods. For each type of method, we summarize and analyze\nthe existing problems, algorithmic ideas, strengths, and weaknesses.\nAdditionally, the challenges of unbalanced data classification are discussed,\nalong with strategies to address them. It is convenient for researchers to\nchoose the appropriate method according to their needs."}, "http://arxiv.org/abs/2310.08939": {"title": "Fast Screening Rules for Optimal Design via Quadratic Lasso Reformulation", "link": "http://arxiv.org/abs/2310.08939", "description": "The problems of Lasso regression and optimal design of experiments share a\ncritical property: their optimal solutions are typically \\emph{sparse}, i.e.,\nonly a small fraction of the optimal variables are non-zero. Therefore, the\nidentification of the support of an optimal solution reduces the dimensionality\nof the problem and can yield a substantial simplification of the calculations.\nIt has recently been shown that linear regression with a \\emph{squared}\n$\\ell_1$-norm sparsity-inducing penalty is equivalent to an optimal\nexperimental design problem. In this work, we use this equivalence to derive\nsafe screening rules that can be used to discard inessential samples. Compared\nto previously existing rules, the new tests are much faster to compute,\nespecially for problems involving a parameter space of high dimension, and can\nbe used dynamically within any iterative solver, with negligible computational\noverhead. Moreover, we show how an existing homotopy algorithm to compute the\nregularization path of the lasso method can be reparametrized with respect to\nthe squared $\\ell_1$-penalty. This allows the computation of a Bayes\n$c$-optimal design in a finite number of steps and can be several orders of\nmagnitude faster than standard first-order algorithms. The efficiency of the\nnew screening rules and of the homotopy algorithm are demonstrated on different\nexamples based on real data."}, "http://arxiv.org/abs/2310.09013": {"title": "Smoothed instrumental variables quantile regression", "link": "http://arxiv.org/abs/2310.09013", "description": "In this article, I introduce the sivqr command, which estimates the\ncoefficients of the instrumental variables (IV) quantile regression model\nintroduced by Chernozhukov and Hansen (2005). The sivqr command offers several\nadvantages over the existing ivqreg and ivqreg2 commands for estimating this IV\nquantile regression model, which complements the alternative \"triangular model\"\nbehind cqiv and the \"local quantile treatment effect\" model of ivqte.\nComputationally, sivqr implements the smoothed estimator of Kaplan and Sun\n(2017), who show that smoothing improves both computation time and statistical\naccuracy. Standard errors are computed analytically or by Bayesian bootstrap;\nfor non-iid sampling, sivqr is compatible with bootstrap. I discuss syntax and\nthe underlying methodology, and I compare sivqr with other commands in an\nexample."}, "http://arxiv.org/abs/2310.09100": {"title": "Time-Uniform Self-Normalized Concentration for Vector-Valued Processes", "link": "http://arxiv.org/abs/2310.09100", "description": "Self-normalized processes arise naturally in many statistical tasks. While\nself-normalized concentration has been extensively studied for scalar-valued\nprocesses, there is less work on multidimensional processes outside of the\nsub-Gaussian setting. In this work, we construct a general, self-normalized\ninequality for $\\mathbb{R}^d$-valued processes that satisfy a simple yet broad\n\"sub-$\\psi$\" tail condition, which generalizes assumptions based on cumulant\ngenerating functions. From this general inequality, we derive an upper law of\nthe iterated logarithm for sub-$\\psi$ vector-valued processes, which is tight\nup to small constants. We demonstrate applications in prototypical statistical\ntasks, such as parameter estimation in online linear regression and\nauto-regressive modeling, and bounded mean estimation via a new (multivariate)\nempirical Bernstein concentration inequality."}, "http://arxiv.org/abs/2310.09185": {"title": "Mediation Analysis using Semi-parametric Shape-Restricted Regression with Applications", "link": "http://arxiv.org/abs/2310.09185", "description": "Often linear regression is used to perform mediation analysis. However, in\nmany instances, the underlying relationships may not be linear, as in the case\nof placental-fetal hormones and fetal development. Although, the exact\nfunctional form of the relationship may be unknown, one may hypothesize the\ngeneral shape of the relationship. For these reasons, we develop a novel\nshape-restricted inference-based methodology for conducting mediation analysis.\nThis work is motivated by an application in fetal endocrinology where\nresearchers are interested in understanding the effects of pesticide\napplication on birth weight, with human chorionic gonadotropin (hCG) as the\nmediator. We assume a practically plausible set of nonlinear effects of hCG on\nthe birth weight and a linear relationship between pesticide exposure and hCG,\nwith both exposure-outcome and exposure-mediator models being linear in the\nconfounding factors. Using the proposed methodology on a population-level\nprenatal screening program data, with hCG as the mediator, we discovered that,\nwhile the natural direct effects suggest a positive association between\npesticide application and birth weight, the natural indirect effects were\nnegative."}, "http://arxiv.org/abs/2310.09214": {"title": "An Introduction to the Calibration of Computer Models", "link": "http://arxiv.org/abs/2310.09214", "description": "In the context of computer models, calibration is the process of estimating\nunknown simulator parameters from observational data. Calibration is variously\nreferred to as model fitting, parameter estimation/inference, an inverse\nproblem, and model tuning. The need for calibration occurs in most areas of\nscience and engineering, and has been used to estimate hard to measure\nparameters in climate, cardiology, drug therapy response, hydrology, and many\nother disciplines. Although the statistical method used for calibration can\nvary substantially, the underlying approach is essentially the same and can be\nconsidered abstractly. In this survey, we review the decisions that need to be\ntaken when calibrating a model, and discuss a range of computational methods\nthat can be used to compute Bayesian posterior distributions."}, "http://arxiv.org/abs/2310.09239": {"title": "Estimating weighted quantile treatment effects with missing outcome data by double sampling", "link": "http://arxiv.org/abs/2310.09239", "description": "Causal weighted quantile treatment effects (WQTE) are a useful compliment to\nstandard causal contrasts that focus on the mean when interest lies at the\ntails of the counterfactual distribution. To-date, however, methods for\nestimation and inference regarding causal WQTEs have assumed complete data on\nall relevant factors. Missing or incomplete data, however, is a widespread\nchallenge in practical settings, particularly when the data are not collected\nfor research purposes such as electronic health records and disease registries.\nFurthermore, in such settings may be particularly susceptible to the outcome\ndata being missing-not-at-random (MNAR). In this paper, we consider the use of\ndouble-sampling, through which the otherwise missing data is ascertained on a\nsub-sample of study units, as a strategy to mitigate bias due to MNAR data in\nthe estimation of causal WQTEs. With the additional data in-hand, we present\nidentifying conditions that do not require assumptions regarding missingness in\nthe original data. We then propose a novel inverse-probability weighted\nestimator and derive its' asymptotic properties, both pointwise at specific\nquantiles and uniform across a range of quantiles in (0,1), when the propensity\nscore and double-sampling probabilities are estimated. For practical inference,\nwe develop a bootstrap method that can be used for both pointwise and uniform\ninference. A simulation study is conducted to examine the finite sample\nperformance of the proposed estimators."}, "http://arxiv.org/abs/2310.09257": {"title": "A SIMPLE Approach to Provably Reconstruct Ising Model with Global Optimality", "link": "http://arxiv.org/abs/2310.09257", "description": "Reconstruction of interaction network between random events is a critical\nproblem arising from statistical physics and politics to sociology, biology,\nand psychology, and beyond. The Ising model lays the foundation for this\nreconstruction process, but finding the underlying Ising model from the least\namount of observed samples in a computationally efficient manner has been\nhistorically challenging for half a century. By using the idea of sparsity\nlearning, we present a approach named SIMPLE that has a dominant sample\ncomplexity from theoretical limit. Furthermore, a tuning-free algorithm is\ndeveloped to give a statistically consistent solution of SIMPLE in polynomial\ntime with high probability. On extensive benchmarked cases, the SIMPLE approach\nprovably reconstructs underlying Ising models with global optimality. The\napplication on the U.S. senators voting in the last six congresses reveals that\nboth the Republicans and Democrats noticeably assemble in each congresses;\ninterestingly, the assembling of Democrats is particularly pronounced in the\nlatest congress."}, "http://arxiv.org/abs/2208.09817": {"title": "High-Dimensional Composite Quantile Regression: Optimal Statistical Guarantees and Fast Algorithms", "link": "http://arxiv.org/abs/2208.09817", "description": "The composite quantile regression (CQR) was introduced by Zou and Yuan [Ann.\nStatist. 36 (2008) 1108--1126] as a robust regression method for linear models\nwith heavy-tailed errors while achieving high efficiency. Its penalized\ncounterpart for high-dimensional sparse models was recently studied in Gu and\nZou [IEEE Trans. Inf. Theory 66 (2020) 7132--7154], along with a specialized\noptimization algorithm based on the alternating direct method of multipliers\n(ADMM). Compared to the various first-order algorithms for penalized least\nsquares, ADMM-based algorithms are not well-adapted to large-scale problems. To\novercome this computational hardness, in this paper we employ a\nconvolution-smoothed technique to CQR, complemented with iteratively reweighted\n$\\ell_1$-regularization. The smoothed composite loss function is convex, twice\ncontinuously differentiable, and locally strong convex with high probability.\nWe propose a gradient-based algorithm for penalized smoothed CQR via a variant\nof the majorize-minimization principal, which gains substantial computational\nefficiency over ADMM. Theoretically, we show that the iteratively reweighted\n$\\ell_1$-penalized smoothed CQR estimator achieves near-minimax optimal\nconvergence rate under heavy-tailed errors without any moment constraint, and\nfurther achieves near-oracle convergence rate under a weaker minimum signal\nstrength condition than needed in Gu and Zou (2020). Numerical studies\ndemonstrate that the proposed method exhibits significant computational\nadvantages without compromising statistical performance compared to two\nstate-of-the-art methods that achieve robustness and high efficiency\nsimultaneously."}, "http://arxiv.org/abs/2210.14292": {"title": "Statistical Inference for H\\\"usler-Reiss Graphical Models Through Matrix Completions", "link": "http://arxiv.org/abs/2210.14292", "description": "The severity of multivariate extreme events is driven by the dependence\nbetween the largest marginal observations. The H\\\"usler-Reiss distribution is a\nversatile model for this extremal dependence, and it is usually parameterized\nby a variogram matrix. In order to represent conditional independence relations\nand obtain sparse parameterizations, we introduce the novel H\\\"usler-Reiss\nprecision matrix. Similarly to the Gaussian case, this matrix appears naturally\nin density representations of the H\\\"usler-Reiss Pareto distribution and\nencodes the extremal graphical structure through its zero pattern. For a given,\narbitrary graph we prove the existence and uniqueness of the completion of a\npartially specified H\\\"usler-Reiss variogram matrix so that its precision\nmatrix has zeros on non-edges in the graph. Using suitable estimators for the\nparameters on the edges, our theory provides the first consistent estimator of\ngraph structured H\\\"usler-Reiss distributions. If the graph is unknown, our\nmethod can be combined with recent structure learning algorithms to jointly\ninfer the graph and the corresponding parameter matrix. Based on our\nmethodology, we propose new tools for statistical inference of sparse\nH\\\"usler-Reiss models and illustrate them on large flight delay data in the\nU.S., as well as Danube river flow data."}, "http://arxiv.org/abs/2302.02288": {"title": "Efficient Adaptive Joint Significance Tests and Sobel-Type Confidence Intervals for Mediation Effects", "link": "http://arxiv.org/abs/2302.02288", "description": "Mediation analysis is an important statistical tool in many research fields.\nIts aim is to investigate the mechanism along the causal pathway between an\nexposure and an outcome. The joint significance test is widely utilized as a\nprominent statistical approach for examining mediation effects in practical\napplications. Nevertheless, the limitation of this mediation testing method\nstems from its conservative Type I error, which reduces its statistical power\nand imposes certain constraints on its popularity and utility. The proposed\nsolution to address this gap is the adaptive joint significance test for one\nmediator, a novel data-adaptive test for mediation effect that exhibits\nsignificant advancements compared to traditional joint significance test. The\nproposed method is designed to be user-friendly, eliminating the need for\ncomplicated procedures. We have derived explicit expressions for size and\npower, ensuring the theoretical validity of our approach. Furthermore, we\nextend the proposed adaptive joint significance tests for small-scale mediation\nhypotheses with family-wise error rate (FWER) control. Additionally, a novel\nadaptive Sobel-type approach is proposed for the estimation of confidence\nintervals for the mediation effects, demonstrating significant advancements\nover conventional Sobel's confidence intervals in terms of achieving desirable\ncoverage probabilities. Our mediation testing and confidence intervals\nprocedure is evaluated through comprehensive simulations, and compared with\nnumerous existing approaches. Finally, we illustrate the usefulness of our\nmethod by analysing three real-world datasets with continuous, binary and\ntime-to-event outcomes, respectively."}, "http://arxiv.org/abs/2302.02747": {"title": "Testing Quantile Forecast Optimality", "link": "http://arxiv.org/abs/2302.02747", "description": "Quantile forecasts made across multiple horizons have become an important\noutput of many financial institutions, central banks and international\norganisations. This paper proposes misspecification tests for such quantile\nforecasts that assess optimality over a set of multiple forecast horizons\nand/or quantiles. The tests build on multiple Mincer-Zarnowitz quantile\nregressions cast in a moment equality framework. Our main test is for the null\nhypothesis of autocalibration, a concept which assesses optimality with respect\nto the information contained in the forecasts themselves. We provide an\nextension that allows to test for optimality with respect to larger information\nsets and a multivariate extension. Importantly, our tests do not just inform\nabout general violations of optimality, but may also provide useful insights\ninto specific forms of sub-optimality. A simulation study investigates the\nfinite sample performance of our tests, and two empirical applications to\nfinancial returns and U.S. macroeconomic series illustrate that our tests can\nyield interesting insights into quantile forecast sub-optimality and its\ncauses."}, "http://arxiv.org/abs/2305.00700": {"title": "Double and Single Descent in Causal Inference with an Application to High-Dimensional Synthetic Control", "link": "http://arxiv.org/abs/2305.00700", "description": "Motivated by a recent literature on the double-descent phenomenon in machine\nlearning, we consider highly over-parameterized models in causal inference,\nincluding synthetic control with many control units. In such models, there may\nbe so many free parameters that the model fits the training data perfectly. We\nfirst investigate high-dimensional linear regression for imputing wage data and\nestimating average treatment effects, where we find that models with many more\ncovariates than sample size can outperform simple ones. We then document the\nperformance of high-dimensional synthetic control estimators with many control\nunits. We find that adding control units can help improve imputation\nperformance even beyond the point where the pre-treatment fit is perfect. We\nprovide a unified theoretical perspective on the performance of these\nhigh-dimensional models. Specifically, we show that more complex models can be\ninterpreted as model-averaging estimators over simpler ones, which we link to\nan improvement in average performance. This perspective yields concrete\ninsights into the use of synthetic control when control units are many relative\nto the number of pre-treatment periods."}, "http://arxiv.org/abs/2305.15742": {"title": "Counterfactual Generative Models for Time-Varying Treatments", "link": "http://arxiv.org/abs/2305.15742", "description": "Estimating the counterfactual outcome of treatment is essential for\ndecision-making in public health and clinical science, among others. Often,\ntreatments are administered in a sequential, time-varying manner, leading to an\nexponentially increased number of possible counterfactual outcomes.\nFurthermore, in modern applications, the outcomes are high-dimensional and\nconventional average treatment effect estimation fails to capture disparities\nin individuals. To tackle these challenges, we propose a novel conditional\ngenerative framework capable of producing counterfactual samples under\ntime-varying treatment, without the need for explicit density estimation. Our\nmethod carefully addresses the distribution mismatch between the observed and\ncounterfactual distributions via a loss function based on inverse probability\nweighting. We present a thorough evaluation of our method using both synthetic\nand real-world data. Our results demonstrate that our method is capable of\ngenerating high-quality counterfactual samples and outperforms the\nstate-of-the-art baselines."}, "http://arxiv.org/abs/2309.09115": {"title": "Fully Synthetic Data for Complex Surveys", "link": "http://arxiv.org/abs/2309.09115", "description": "When seeking to release public use files for confidential data, statistical\nagencies can generate fully synthetic data. We propose an approach for making\nfully synthetic data from surveys collected with complex sampling designs.\nSpecifically, we generate pseudo-populations by applying the weighted finite\npopulation Bayesian bootstrap to account for survey weights, take simple random\nsamples from those pseudo-populations, estimate synthesis models using these\nsimple random samples, and release simulated data drawn from the models as the\npublic use files. We use the framework of multiple imputation to enable\nvariance estimation using two data generation strategies. In the first, we\ngenerate multiple data sets from each simple random sample, whereas in the\nsecond, we generate a single synthetic data set from each simple random sample.\nWe present multiple imputation combining rules for each setting. We illustrate\neach approach and the repeated sampling properties of the combining rules using\nsimulation studies."}, "http://arxiv.org/abs/2309.09323": {"title": "Answering Layer 3 queries with DiscoSCMs", "link": "http://arxiv.org/abs/2309.09323", "description": "Addressing causal queries across the Pearl Causal Hierarchy (PCH) (i.e.,\nassociational, interventional and counterfactual), which is formalized as\n\\Layer{} Valuations, is a central task in contemporary causal inference\nresearch. Counterfactual questions, in particular, pose a significant challenge\nas they often necessitate a complete knowledge of structural equations. This\npaper identifies \\textbf{the degeneracy problem} caused by the consistency\nrule. To tackle this, the \\textit{Distribution-consistency Structural Causal\nModels} (DiscoSCMs) is introduced, which extends both the structural causal\nmodels (SCM) and the potential outcome framework. The correlation pattern of\npotential outcomes in personalized incentive scenarios, described by $P(y_x,\ny'_{x'})$, is used as a case study for elucidation. Although counterfactuals\nare no longer degenerate, they remain indeterminable. As a result, the\ncondition of independent potential noise is incorporated into DiscoSCM. It is\nfound that by adeptly using homogeneity, counterfactuals can be identified.\nFurthermore, more refined results are achieved in the unit problem scenario. In\nsimpler terms, when modeling counterfactuals, one should contemplate: \"Consider\na person with average ability who takes a test and, due to good luck, achieves\nan exceptionally high score. If this person were to retake the test under\nidentical external conditions, what score will he obtain? An exceptionally high\nscore or an average score?\" If your choose is predicting an average score, then\nyou are essentially choosing DiscoSCM over the traditional frameworks based on\nthe consistency rule."}, "http://arxiv.org/abs/2310.01748": {"title": "A generative approach to frame-level multi-competitor races", "link": "http://arxiv.org/abs/2310.01748", "description": "Multi-competitor races often feature complicated within-race strategies that\nare difficult to capture when training data on race outcome level data.\nFurther, models which do not account for such strategic effects may suffer from\nconfounded inferences and predictions. In this work we develop a general\ngenerative model for multi-competitor races which allows analysts to explicitly\nmodel certain strategic effects such as changing lanes or drafting and separate\nthese impacts from competitor ability. The generative model allows one to\nsimulate full races from any real or created starting position which opens new\navenues for attributing value to within-race actions and to perform\ncounter-factual analyses. This methodology is sufficiently general to apply to\nany track based multi-competitor races where both tracking data is available\nand competitor movement is well described by simultaneous forward and lateral\nmovements. We apply this methodology to one-mile horse races using data\nprovided by the New York Racing Association (NYRA) and the New York\nThoroughbred Horsemen's Association (NYTHA) for the Big Data Derby 2022 Kaggle\nCompetition. This data features granular tracking data for all horses at the\nframe-level (occurring at approximately 4hz). We demonstrate how this model can\nyield new inferences, such as the estimation of horse-specific speed profiles\nwhich vary over phases of the race, and examples of posterior predictive\ncounterfactual simulations to answer questions of interest such as starting\nlane impacts on race outcomes."}, "http://arxiv.org/abs/2310.09345": {"title": "A Unified Bayesian Framework for Modeling Measurement Error in Multinomial Data", "link": "http://arxiv.org/abs/2310.09345", "description": "Measurement error in multinomial data is a well-known and well-studied\ninferential problem that is encountered in many fields, including engineering,\nbiomedical and omics research, ecology, finance, and social sciences.\nSurprisingly, methods developed to accommodate measurement error in multinomial\ndata are typically equipped to handle false negatives or false positives, but\nnot both. We provide a unified framework for accommodating both forms of\nmeasurement error using a Bayesian hierarchical approach. We demonstrate the\nproposed method's performance on simulated data and apply it to acoustic bat\nmonitoring data."}, "http://arxiv.org/abs/2310.09384": {"title": "Modeling Missing at Random Neuropsychological Test Scores Using a Mixture of Binomial Product Experts", "link": "http://arxiv.org/abs/2310.09384", "description": "Multivariate bounded discrete data arises in many fields. In the setting of\nlongitudinal dementia studies, such data is collected when individuals complete\nneuropsychological tests. We outline a modeling and inference procedure that\ncan model the joint distribution conditional on baseline covariates, leveraging\nprevious work on mixtures of experts and latent class models. Furthermore, we\nillustrate how the work can be extended when the outcome data is missing at\nrandom using a nested EM algorithm. The proposed model can incorporate\ncovariate information, perform imputation and clustering, and infer latent\ntrajectories. We apply our model on simulated data and an Alzheimer's disease\ndata set."}, "http://arxiv.org/abs/2310.09398": {"title": "An In-Depth Examination of Requirements for Disclosure Risk Assessment", "link": "http://arxiv.org/abs/2310.09398", "description": "The use of formal privacy to protect the confidentiality of responses in the\n2020 Decennial Census of Population and Housing has triggered renewed interest\nand debate over how to measure the disclosure risks and societal benefits of\nthe published data products. Following long-established precedent in economics\nand statistics, we argue that any proposal for quantifying disclosure risk\nshould be based on pre-specified, objective criteria. Such criteria should be\nused to compare methodologies to identify those with the most desirable\nproperties. We illustrate this approach, using simple desiderata, to evaluate\nthe absolute disclosure risk framework, the counterfactual framework underlying\ndifferential privacy, and prior-to-posterior comparisons. We conclude that\nsatisfying all the desiderata is impossible, but counterfactual comparisons\nsatisfy the most while absolute disclosure risk satisfies the fewest.\nFurthermore, we explain that many of the criticisms levied against differential\nprivacy would be levied against any technology that is not equivalent to\ndirect, unrestricted access to confidential data. Thus, more research is\nneeded, but in the near-term, the counterfactual approach appears best-suited\nfor privacy-utility analysis."}, "http://arxiv.org/abs/2310.09428": {"title": "Sparse higher order partial least squares for simultaneous variable selection, dimension reduction, and tensor denoising", "link": "http://arxiv.org/abs/2310.09428", "description": "Partial Least Squares (PLS) regression emerged as an alternative to ordinary\nleast squares for addressing multicollinearity in a wide range of scientific\napplications. As multidimensional tensor data is becoming more widespread,\ntensor adaptations of PLS have been developed. Our investigations reveal that\nthe previously established asymptotic result of the PLS estimator for a tensor\nresponse breaks down as the tensor dimensions and the number of features\nincrease relative to the sample size. To address this, we propose Sparse Higher\nOrder Partial Least Squares (SHOPS) regression and an accompanying algorithm.\nSHOPS simultaneously accommodates variable selection, dimension reduction, and\ntensor association denoising. We establish the asymptotic accuracy of the SHOPS\nalgorithm under a high-dimensional regime and verify these results through\ncomprehensive simulation experiments, and applications to two contemporary\nhigh-dimensional biological data analysis."}, "http://arxiv.org/abs/2310.09493": {"title": "Summary Statistics Knockoffs Inference with Family-wise Error Rate Control", "link": "http://arxiv.org/abs/2310.09493", "description": "Testing multiple hypotheses of conditional independence with provable error\nrate control is a fundamental problem with various applications. To infer\nconditional independence with family-wise error rate (FWER) control when only\nsummary statistics of marginal dependence are accessible, we adopt\nGhostKnockoff to directly generate knockoff copies of summary statistics and\npropose a new filter to select features conditionally dependent to the response\nwith provable FWER control. In addition, we develop a computationally efficient\nalgorithm to greatly reduce the computational cost of knockoff copies\ngeneration without sacrificing power and FWER control. Experiments on simulated\ndata and a real dataset of Alzheimer's disease genetics demonstrate the\nadvantage of proposed method over the existing alternatives in both statistical\npower and computational efficiency."}, "http://arxiv.org/abs/2310.09545": {"title": "A Semiparametric Instrumented Difference-in-Differences Approach to Policy Learning", "link": "http://arxiv.org/abs/2310.09545", "description": "Recently, there has been a surge in methodological development for the\ndifference-in-differences (DiD) approach to evaluate causal effects. Standard\nmethods in the literature rely on the parallel trends assumption to identify\nthe average treatment effect on the treated. However, the parallel trends\nassumption may be violated in the presence of unmeasured confounding, and the\naverage treatment effect on the treated may not be useful in learning a\ntreatment assignment policy for the entire population. In this article, we\npropose a general instrumented DiD approach for learning the optimal treatment\npolicy. Specifically, we establish identification results using a binary\ninstrumental variable (IV) when the parallel trends assumption fails to hold.\nAdditionally, we construct a Wald estimator, novel inverse probability\nweighting (IPW) estimators, and a class of semiparametric efficient and\nmultiply robust estimators, with theoretical guarantees on consistency and\nasymptotic normality, even when relying on flexible machine learning algorithms\nfor nuisance parameters estimation. Furthermore, we extend the instrumented DiD\nto the panel data setting. We evaluate our methods in extensive simulations and\na real data application."}, "http://arxiv.org/abs/2310.09646": {"title": "Jackknife empirical likelihood confidence intervals for the categorical Gini correlation", "link": "http://arxiv.org/abs/2310.09646", "description": "The categorical Gini correlation, $\\rho_g$, was proposed by Dang et al. to\nmeasure the dependence between a categorical variable, $Y$ , and a numerical\nvariable, $X$. It has been shown that $\\rho_g$ has more appealing properties\nthan current existing dependence measurements. In this paper, we develop the\njackknife empirical likelihood (JEL) method for $\\rho_g$. Confidence intervals\nfor the Gini correlation are constructed without estimating the asymptotic\nvariance. Adjusted and weighted JEL are explored to improve the performance of\nthe standard JEL. Simulation studies show that our methods are competitive to\nexisting methods in terms of coverage accuracy and shortness of confidence\nintervals. The proposed methods are illustrated in an application on two real\ndatasets."}, "http://arxiv.org/abs/2310.09673": {"title": "Robust Quickest Change Detection in Non-Stationary Processes", "link": "http://arxiv.org/abs/2310.09673", "description": "Optimal algorithms are developed for robust detection of changes in\nnon-stationary processes. These are processes in which the distribution of the\ndata after change varies with time. The decision-maker does not have access to\nprecise information on the post-change distribution. It is shown that if the\npost-change non-stationary family has a distribution that is least favorable in\na well-defined sense, then the algorithms designed using the least favorable\ndistributions are robust and optimal. Non-stationary processes are encountered\nin public health monitoring and space and military applications. The robust\nalgorithms are applied to real and simulated data to show their effectiveness."}, "http://arxiv.org/abs/2310.09701": {"title": "A powerful empirical Bayes approach for high dimensional replicability analysis", "link": "http://arxiv.org/abs/2310.09701", "description": "Identifying replicable signals across different studies provides stronger\nscientific evidence and more powerful inference. Existing literature on high\ndimensional applicability analysis either imposes strong modeling assumptions\nor has low power. We develop a powerful and robust empirical Bayes approach for\nhigh dimensional replicability analysis. Our method effectively borrows\ninformation from different features and studies while accounting for\nheterogeneity. We show that the proposed method has better power than competing\nmethods while controlling the false discovery rate, both empirically and\ntheoretically. Analyzing datasets from the genome-wide association studies\nreveals new biological insights that otherwise cannot be obtained by using\nexisting methods."}, "http://arxiv.org/abs/2310.09702": {"title": "Inference with Mondrian Random Forests", "link": "http://arxiv.org/abs/2310.09702", "description": "Random forests are popular methods for classification and regression, and\nmany different variants have been proposed in recent years. One interesting\nexample is the Mondrian random forest, in which the underlying trees are\nconstructed according to a Mondrian process. In this paper we give a central\nlimit theorem for the estimates made by a Mondrian random forest in the\nregression setting. When combined with a bias characterization and a consistent\nvariance estimator, this allows one to perform asymptotically valid statistical\ninference, such as constructing confidence intervals, on the unknown regression\nfunction. We also provide a debiasing procedure for Mondrian random forests\nwhich allows them to achieve minimax-optimal estimation rates with\n$\\beta$-H\\\"older regression functions, for all $\\beta$ and in arbitrary\ndimension, assuming appropriate parameter tuning."}, "http://arxiv.org/abs/2310.09818": {"title": "MCMC for Bayesian nonparametric mixture modeling under differential privacy", "link": "http://arxiv.org/abs/2310.09818", "description": "Estimating the probability density of a population while preserving the\nprivacy of individuals in that population is an important and challenging\nproblem that has received considerable attention in recent years. While the\nprevious literature focused on frequentist approaches, in this paper, we\npropose a Bayesian nonparametric mixture model under differential privacy (DP)\nand present two Markov chain Monte Carlo (MCMC) algorithms for posterior\ninference. One is a marginal approach, resembling Neal's algorithm 5 with a\npseudo-marginal Metropolis-Hastings move, and the other is a conditional\napproach. Although our focus is primarily on local DP, we show that our MCMC\nalgorithms can be easily extended to deal with global differential privacy\nmechanisms. Moreover, for certain classes of mechanisms and mixture kernels, we\nshow how standard algorithms can be employed, resulting in substantial\nefficiency gains. Our approach is general and applicable to any mixture model\nand privacy mechanism. In several simulations and a real case study, we discuss\nthe performance of our algorithms and evaluate different privacy mechanisms\nproposed in the frequentist literature."}, "http://arxiv.org/abs/2310.09955": {"title": "On the Statistical Foundations of H-likelihood for Unobserved Random Variables", "link": "http://arxiv.org/abs/2310.09955", "description": "The maximum likelihood estimation is widely used for statistical inferences.\nIn this study, we reformulate the h-likelihood proposed by Lee and Nelder in\n1996, whose maximization yields maximum likelihood estimators for fixed\nparameters and asymptotically best unbiased predictors for random parameters.\nWe establish the statistical foundations for h-likelihood theories, which\nextend classical likelihood theories to embrace broad classes of statistical\nmodels with random parameters. The maximum h-likelihood estimators\nasymptotically achieve the generalized Cramer-Rao lower bound. Furthermore, we\nexplore asymptotic theory when the consistency of either fixed parameter\nestimation or random parameter prediction is violated. The introduction of this\nnew h-likelihood framework enables likelihood theories to cover inferences for\na much broader class of models, while also providing computationally efficient\nfitting algorithms to give asymptotically optimal estimators for fixed\nparameters and predictors for random parameters."}, "http://arxiv.org/abs/2310.09960": {"title": "Point Mass in the Confidence Distribution: Is it a Drawback or an Advantage?", "link": "http://arxiv.org/abs/2310.09960", "description": "Stein's (1959) problem highlights the phenomenon called the probability\ndilution in high dimensional cases, which is known as a fundamental deficiency\nin probabilistic inference. The satellite conjunction problem also suffers from\nprobability dilution that poor-quality data can lead to a dilution of collision\nprobability. Though various methods have been proposed, such as generalized\nfiducial distribution and the reference posterior, they could not maintain the\ncoverage probability of confidence intervals (CIs) in both problems. On the\nother hand, the confidence distribution (CD) has a point mass at zero, which\nhas been interpreted paradoxical. However, we show that this point mass is an\nadvantage rather than a drawback, because it gives a way to maintain the\ncoverage probability of CIs. More recently, `false confidence theorem' was\npresented as another deficiency in probabilistic inferences, called the false\nconfidence. It was further claimed that the use of consonant belief can\nmitigate this deficiency. However, we show that the false confidence theorem\ncannot be applied to the CD in both Stein's and satellite conjunction problems.\nIt is crucial that a confidence feature, not a consonant one, is the key to\novercome the deficiencies in probabilistic inferences. Our findings reveal that\nthe CD outperforms the other existing methods, including the consonant belief,\nin the context of Stein's and satellite conjunction problems. Additionally, we\ndemonstrate the ambiguity of coverage probability in an observed CI from the\nfrequentist CI procedure, and show that the CD provides valuable information\nregarding this ambiguity."}, "http://arxiv.org/abs/2310.09961": {"title": "Theoretical Evaluation of Asymmetric Shapley Values for Root-Cause Analysis", "link": "http://arxiv.org/abs/2310.09961", "description": "In this work, we examine Asymmetric Shapley Values (ASV), a variant of the\npopular SHAP additive local explanation method. ASV proposes a way to improve\nmodel explanations incorporating known causal relations between variables, and\nis also considered as a way to test for unfair discrimination in model\npredictions. Unexplored in previous literature, relaxing symmetry in Shapley\nvalues can have counter-intuitive consequences for model explanation. To better\nunderstand the method, we first show how local contributions correspond to\nglobal contributions of variance reduction. Using variance, we demonstrate\nmultiple cases where ASV yields counter-intuitive attributions, arguably\nproducing incorrect results for root-cause analysis. Second, we identify\ngeneralized additive models (GAM) as a restricted class for which ASV exhibits\ndesirable properties. We support our arguments by proving multiple theoretical\nresults about the method. Finally, we demonstrate the use of asymmetric\nattributions on multiple real-world datasets, comparing the results with and\nwithout restricted model families using gradient boosting and deep learning\nmodels."}, "http://arxiv.org/abs/2310.10003": {"title": "Conformal Contextual Robust Optimization", "link": "http://arxiv.org/abs/2310.10003", "description": "Data-driven approaches to predict-then-optimize decision-making problems seek\nto mitigate the risk of uncertainty region misspecification in safety-critical\nsettings. Current approaches, however, suffer from considering overly\nconservative uncertainty regions, often resulting in suboptimal decisionmaking.\nTo this end, we propose Conformal-Predict-Then-Optimize (CPO), a framework for\nleveraging highly informative, nonconvex conformal prediction regions over\nhigh-dimensional spaces based on conditional generative models, which have the\ndesired distribution-free coverage guarantees. Despite guaranteeing robustness,\nsuch black-box optimization procedures alone inspire little confidence owing to\nthe lack of explanation of why a particular decision was found to be optimal.\nWe, therefore, augment CPO to additionally provide semantically meaningful\nvisual summaries of the uncertainty regions to give qualitative intuition for\nthe optimal decision. We highlight the CPO framework by demonstrating results\non a suite of simulation-based inference benchmark tasks and a vehicle routing\ntask based on probabilistic weather prediction."}, "http://arxiv.org/abs/2310.10048": {"title": "Evaluation of transplant benefits with the U", "link": "http://arxiv.org/abs/2310.10048", "description": "Kidney transplantation is the most effective renal replacement therapy for\nend stage renal disease patients. With the severe shortage of kidney supplies\nand for the clinical effectiveness of transplantation, patient's life\nexpectancy post transplantation is used to prioritize patients for\ntransplantation; however, severe comorbidity conditions and old age are the\nmost dominant factors that negatively impact post-transplantation life\nexpectancy, effectively precluding sick or old patients from receiving\ntransplants. It would be crucial to design objective measures to quantify the\ntransplantation benefit by comparing the mean residual life with and without a\ntransplant, after adjusting for comorbidity and demographic conditions. To\naddress this urgent need, we propose a new class of semiparametric\ncovariate-dependent mean residual life models. Our method estimates covariate\neffects semiparametrically efficiently and the mean residual life function\nnonparametrically, enabling us to predict the residual life increment potential\nfor any given patient. Our method potentially leads to a more fair system that\nprioritizes patients who would have the largest residual life gains. Our\nanalysis of the kidney transplant data from the U.S. Scientific Registry of\nTransplant Recipients also suggests that a single index of covariates summarize\nwell the impacts of multiple covariates, which may facilitate interpretations\nof each covariate's effect. Our subgroup analysis further disclosed\ninequalities in survival gains across groups defined by race, gender and\ninsurance type (reflecting socioeconomic status)."}, "http://arxiv.org/abs/2310.10052": {"title": "Group-Orthogonal Subsampling for Hierarchical Data Based on Linear Mixed Models", "link": "http://arxiv.org/abs/2310.10052", "description": "Hierarchical data analysis is crucial in various fields for making\ndiscoveries. The linear mixed model is often used for training hierarchical\ndata, but its parameter estimation is computationally expensive, especially\nwith big data. Subsampling techniques have been developed to address this\nchallenge. However, most existing subsampling methods assume homogeneous data\nand do not consider the possible heterogeneity in hierarchical data. To address\nthis limitation, we develop a new approach called group-orthogonal subsampling\n(GOSS) for selecting informative subsets of hierarchical data that may exhibit\nheterogeneity. GOSS selects subdata with balanced data size among groups and\ncombinatorial orthogonality within each group, resulting in subdata that are\n$D$- and $A$-optimal for building linear mixed models. Estimators of parameters\ntrained on GOSS subdata are consistent and asymptotically normal. GOSS is shown\nto be numerically appealing via simulations and a real data application.\nTheoretical proofs, R codes, and supplementary numerical results are accessible\nonline as Supplementary Materials."}, "http://arxiv.org/abs/2310.10239": {"title": "Structural transfer learning of non-Gaussian DAG", "link": "http://arxiv.org/abs/2310.10239", "description": "Directed acyclic graph (DAG) has been widely employed to represent\ndirectional relationships among a set of collected nodes. Yet, the available\ndata in one single study is often limited for accurate DAG reconstruction,\nwhereas heterogeneous data may be collected from multiple relevant studies. It\nremains an open question how to pool the heterogeneous data together for better\nDAG structure reconstruction in the target study. In this paper, we first\nintroduce a novel set of structural similarity measures for DAG and then\npresent a transfer DAG learning framework by effectively leveraging information\nfrom auxiliary DAGs of different levels of similarities. Our theoretical\nanalysis shows substantial improvement in terms of DAG reconstruction in the\ntarget study, even when no auxiliary DAG is overall similar to the target DAG,\nwhich is in sharp contrast to most existing transfer learning methods. The\nadvantage of the proposed transfer DAG learning is also supported by extensive\nnumerical experiments on both synthetic data and multi-site brain functional\nconnectivity network data."}, "http://arxiv.org/abs/2310.10271": {"title": "A geometric power analysis for general log-linear models", "link": "http://arxiv.org/abs/2310.10271", "description": "General log-linear models are widely used to express the association in\nmultivariate frequency data on contingency tables. The paper focuses on the\npower analysis for testing the goodness-of-fit hypothesis for these models.\nConventionally, for the power-related sample size calculations a deviation from\nthe null hypothesis, aka effect size, is specified by means of the chi-square\ngoodness-of-fit index. It is argued that the odds ratio is a more natural\nmeasure of effect size, with the advantage of having a data-relevant\ninterpretation. Therefore, a class of log-affine models that are specified by\nodds ratios whose values deviate from those of the null by a small amount can\nbe chosen as an alternative. Being expressed as sets of constraints on odds\nratios, both hypotheses are represented by smooth surfaces in the probability\nsimplex, and thus, the power analysis can be given a geometric interpretation\nas well. A concept of geometric power is introduced and a Monte-Carlo algorithm\nfor its estimation is proposed. The framework is applied to the power analysis\nof goodness-of-fit in the context of multinomial sampling. An iterative scaling\nprocedure for generating distributions from a log-affine model is described and\nits convergence is proved. To illustrate, the geometric power analysis is\ncarried out for data from a clinical study."}, "http://arxiv.org/abs/2310.10324": {"title": "Assessing univariate and bivariate risks of late-frost and drought using vine copulas: A historical study for Bavaria", "link": "http://arxiv.org/abs/2310.10324", "description": "In light of climate change's impacts on forests, including extreme drought\nand late-frost, leading to vitality decline and regional forest die-back, we\nassess univariate drought and late-frost risks and perform a joint risk\nanalysis in Bavaria, Germany, from 1952 to 2020. Utilizing a vast dataset with\n26 bioclimatic and topographic variables, we employ vine copula models due to\nthe data's non-Gaussian and asymmetric dependencies. We use D-vine regression\nfor univariate and Y-vine regression for bivariate analysis, and propose\ncorresponding univariate and bivariate conditional probability risk measures.\nWe identify \"at-risk\" regions, emphasizing the need for forest adaptation due\nto climate change."}, "http://arxiv.org/abs/2310.10329": {"title": "Towards Data-Conditional Simulation for ABC Inference in Stochastic Differential Equations", "link": "http://arxiv.org/abs/2310.10329", "description": "We develop a Bayesian inference method for discretely-observed stochastic\ndifferential equations (SDEs). Inference is challenging for most SDEs, due to\nthe analytical intractability of the likelihood function. Nevertheless, forward\nsimulation via numerical methods is straightforward, motivating the use of\napproximate Bayesian computation (ABC). We propose a conditional simulation\nscheme for SDEs that is based on lookahead strategies for sequential Monte\nCarlo (SMC) and particle smoothing using backward simulation. This leads to the\nsimulation of trajectories that are consistent with the observed trajectory,\nthereby increasing the ABC acceptance rate. We additionally employ an invariant\nneural network, previously developed for Markov processes, to learn the summary\nstatistics function required in ABC. The neural network is incrementally\nretrained by exploiting an ABC-SMC sampler, which provides new training data at\neach round. Since the SDE simulation scheme differs from standard forward\nsimulation, we propose a suitable importance sampling correction, which has the\nadded advantage of guiding the parameters towards regions of high posterior\ndensity, especially in the first ABC-SMC round. Our approach achieves accurate\ninference and is about three times faster than standard (forward-only) ABC-SMC.\nWe illustrate our method in four simulation studies, including three examples\nfrom the Chan-Karaolyi-Longstaff-Sanders SDE family."}, "http://arxiv.org/abs/2310.10331": {"title": "Specifications tests for count time series models with covariates", "link": "http://arxiv.org/abs/2310.10331", "description": "We propose a goodness-of-fit test for a class of count time series models\nwith covariates which includes the Poisson autoregressive model with covariates\n(PARX) as a special case. The test criteria are derived from a specific\ncharacterization for the conditional probability generating function and the\ntest statistic is formulated as a $L_2$ weighting norm of the corresponding\nsample counterpart. The asymptotic properties of the proposed test statistic\nare provided under the null hypothesis as well as under specific alternatives.\nA bootstrap version of the test is explored in a Monte--Carlo study and\nillustrated on a real data set on road safety."}, "http://arxiv.org/abs/2310.10373": {"title": "False Discovery Proportion control for aggregated Knockoffs", "link": "http://arxiv.org/abs/2310.10373", "description": "Controlled variable selection is an important analytical step in various\nscientific fields, such as brain imaging or genomics. In these high-dimensional\ndata settings, considering too many variables leads to poor models and high\ncosts, hence the need for statistical guarantees on false positives. Knockoffs\nare a popular statistical tool for conditional variable selection in high\ndimension. However, they control for the expected proportion of false\ndiscoveries (FDR) and not their actual proportion (FDP). We present a new\nmethod, KOPI, that controls the proportion of false discoveries for\nKnockoff-based inference. The proposed method also relies on a new type of\naggregation to address the undesirable randomness associated with classical\nKnockoff inference. We demonstrate FDP control and substantial power gains over\nexisting Knockoff-based methods in various simulation settings and achieve good\nsensitivity/specificity tradeoffs on brain imaging and genomic data."}, "http://arxiv.org/abs/2310.10393": {"title": "Statistical and Causal Robustness for Causal Null Hypothesis Tests", "link": "http://arxiv.org/abs/2310.10393", "description": "Prior work applying semiparametric theory to causal inference has primarily\nfocused on deriving estimators that exhibit statistical robustness under a\nprespecified causal model that permits identification of a desired causal\nparameter. However, a fundamental challenge is correct specification of such a\nmodel, which usually involves making untestable assumptions. Evidence factors\nis an approach to combining hypothesis tests of a common causal null hypothesis\nunder two or more candidate causal models. Under certain conditions, this\nyields a test that is valid if at least one of the underlying models is\ncorrect, which is a form of causal robustness. We propose a method of combining\nsemiparametric theory with evidence factors. We develop a causal null\nhypothesis test based on joint asymptotic normality of K asymptotically linear\nsemiparametric estimators, where each estimator is based on a distinct\nidentifying functional derived from each of K candidate causal models. We show\nthat this test provides both statistical and causal robustness in the sense\nthat it is valid if at least one of the K proposed causal models is correct,\nwhile also allowing for slower than parametric rates of convergence in\nestimating nuisance functions. We demonstrate the efficacy of our method via\nsimulations and an application to the Framingham Heart Study."}, "http://arxiv.org/abs/2310.10407": {"title": "Ensemble methods for testing a global null", "link": "http://arxiv.org/abs/2310.10407", "description": "Testing a global null is a canonical problem in statistics and has a wide\nrange of applications. In view of the fact that no uniformly most powerful test\nexists, prior and/or domain knowledge are commonly used to focus on a certain\nclass of alternatives to improve the testing power. However, it is generally\nchallenging to develop tests that are particularly powerful against a certain\nclass of alternatives. In this paper, motivated by the success of ensemble\nlearning methods for prediction or classification, we propose an ensemble\nframework for testing that mimics the spirit of random forests to deal with the\nchallenges. Our ensemble testing framework aggregates a collection of weak base\ntests to form a final ensemble test that maintains strong and robust power for\nglobal nulls. We apply the framework to four problems about global testing in\ndifferent classes of alternatives arising from Whole Genome Sequencing (WGS)\nassociation studies. Specific ensemble tests are proposed for each of these\nproblems, and their theoretical optimality is established in terms of Bahadur\nefficiency. Extensive simulations and an analysis of a real WGS dataset are\nconducted to demonstrate the type I error control and/or power gain of the\nproposed ensemble tests."}, "http://arxiv.org/abs/2310.10422": {"title": "A Neural Network-Based Approach to Normality Testing for Dependent Data", "link": "http://arxiv.org/abs/2310.10422", "description": "There is a wide availability of methods for testing normality under the\nassumption of independent and identically distributed data. When data are\ndependent in space and/or time, however, assessing and testing the marginal\nbehavior is considerably more challenging, as the marginal behavior is impacted\nby the degree of dependence. We propose a new approach to assess normality for\ndependent data by non-linearly incorporating existing statistics from normality\ntests as well as sample moments such as skewness and kurtosis through a neural\nnetwork. We calibrate (deep) neural networks by simulated normal and non-normal\ndata with a wide range of dependence structures and we determine the\nprobability of rejecting the null hypothesis. We compare several approaches for\nnormality tests and demonstrate the superiority of our method in terms of\nstatistical power through an extensive simulation study. A real world\napplication to global temperature data further demonstrates how the degree of\nspatio-temporal aggregation affects the marginal normality in the data."}, "http://arxiv.org/abs/2310.10494": {"title": "Multivariate Scalar on Multidimensional Distribution Regression", "link": "http://arxiv.org/abs/2310.10494", "description": "We develop a new method for multivariate scalar on multidimensional\ndistribution regression. Traditional approaches typically analyze isolated\nunivariate scalar outcomes or consider unidimensional distributional\nrepresentations as predictors. However, these approaches are sub-optimal\nbecause: i) they fail to utilize the dependence between the distributional\npredictors: ii) neglect the correlation structure of the response. To overcome\nthese limitations, we propose a multivariate distributional analysis framework\nthat harnesses the power of multivariate density functions and multitask\nlearning. We develop a computationally efficient semiparametric estimation\nmethod for modelling the effect of the latent joint density on multivariate\nresponse of interest. Additionally, we introduce a new conformal algorithm for\nquantifying the uncertainty of regression models with multivariate responses\nand distributional predictors, providing valuable insights into the conditional\ndistribution of the response. We have validated the effectiveness of our\nproposed method through comprehensive numerical simulations, clearly\ndemonstrating its superior performance compared to traditional methods. The\napplication of the proposed method is demonstrated on tri-axial accelerometer\ndata from the National Health and Nutrition Examination Survey (NHANES)\n2011-2014 for modelling the association between cognitive scores across various\ndomains and distributional representation of physical activity among older\nadult population. Our results highlight the advantages of the proposed\napproach, emphasizing the significance of incorporating complete spatial\ninformation derived from the accelerometer device."}, "http://arxiv.org/abs/2310.10588": {"title": "Max-convolution processes with random shape indicator kernels", "link": "http://arxiv.org/abs/2310.10588", "description": "In this paper, we introduce a new class of models for spatial data obtained\nfrom max-convolution processes based on indicator kernels with random shape. We\nshow that this class of models have appealing dependence properties including\ntail dependence at short distances and independence at long distances. We\nfurther consider max-convolutions between such processes and processes with\ntail independence, in order to separately control the bulk and tail dependence\nbehaviors, and to increase flexibility of the model at longer distances, in\nparticular, to capture intermediate tail dependence. We show how parameters can\nbe estimated using a weighted pairwise likelihood approach, and we conduct an\nextensive simulation study to show that the proposed inference approach is\nfeasible in high dimensions and it yields accurate parameter estimates in most\ncases. We apply the proposed methodology to analyse daily temperature maxima\nmeasured at 100 monitoring stations in the state of Oklahoma, US. Our results\nindicate that our proposed model provides a good fit to the data, and that it\ncaptures both the bulk and the tail dependence structures accurately."}, "http://arxiv.org/abs/1805.07301": {"title": "Enhanced Pricing and Management of Bundled Insurance Risks with Dependence-aware Prediction using Pair Copula Construction", "link": "http://arxiv.org/abs/1805.07301", "description": "We propose a dependence-aware predictive modeling framework for multivariate\nrisks stemmed from an insurance contract with bundling features - an important\ntype of policy increasingly offered by major insurance companies. The bundling\nfeature naturally leads to longitudinal measurements of multiple insurance\nrisks, and correct pricing and management of such risks is of fundamental\ninterest to financial stability of the macroeconomy. We build a novel\npredictive model that fully captures the dependence among the multivariate\nrepeated risk measurements. Specifically, the longitudinal measurement of each\nindividual risk is first modeled using pair copula construction with a D-vine\nstructure, and the multiple D-vines are then integrated by a flexible copula.\nThe proposed model provides a unified modeling framework for multivariate\nlongitudinal data that can accommodate different scales of measurements,\nincluding continuous, discrete, and mixed observations, and thus can be\npotentially useful for various economic studies. A computationally efficient\nsequential method is proposed for model estimation and inference, and its\nperformance is investigated both theoretically and via simulation studies. In\nthe application, we examine multivariate bundled risks in multi-peril property\ninsurance using proprietary data from a commercial property insurance provider.\nThe proposed model is found to provide improved decision making for several key\ninsurance operations. For underwriting, we show that the experience rate priced\nby the proposed model leads to a 9% lift in the insurer's net revenue. For\nreinsurance, we show that the insurer underestimates the risk of the retained\ninsurance portfolio by 10% when ignoring the dependence among bundled insurance\nrisks."}, "http://arxiv.org/abs/2005.04721": {"title": "Decision Making in Drug Development via Inference on Power", "link": "http://arxiv.org/abs/2005.04721", "description": "A typical power calculation is performed by replacing unknown\npopulation-level quantities in the power function with what is observed in\nexternal studies. Many authors and practitioners view this as an assumed value\nof power and offer the Bayesian quantity probability of success or assurance as\nan alternative. The claim is by averaging over a prior or posterior\ndistribution, probability of success transcends power by capturing the\nuncertainty around the unknown true treatment effect and any other\npopulation-level parameters. We use p-value functions to frame both the\nprobability of success calculation and the typical power calculation as merely\nproducing two different point estimates of power. We demonstrate that Go/No-Go\ndecisions based on either point estimate of power do not adequately quantify\nand control the risk involved, and instead we argue for Go/No-Go decisions that\nutilize inference on power for better risk management and decision making."}, "http://arxiv.org/abs/2103.00674": {"title": "BEAUTY Powered BEAST", "link": "http://arxiv.org/abs/2103.00674", "description": "We study distribution-free goodness-of-fit tests with the proposed Binary\nExpansion Approximation of UniformiTY (BEAUTY) approach. This method\ngeneralizes the renowned Euler's formula, and approximates the characteristic\nfunction of any copula through a linear combination of expectations of binary\ninteractions from marginal binary expansions. This novel theory enables a\nunification of many important tests of independence via approximations from\nspecific quadratic forms of symmetry statistics, where the deterministic weight\nmatrix characterizes the power properties of each test. To achieve a robust\npower, we examine test statistics with data-adaptive weights, referred to as\nthe Binary Expansion Adaptive Symmetry Test (BEAST). Using properties of the\nbinary expansion filtration, we demonstrate that the Neyman-Pearson test of\nuniformity can be approximated by an oracle weighted sum of symmetry\nstatistics. The BEAST with this oracle provides a useful benchmark of feasible\npower. To approach this oracle power, we devise the BEAST through a regularized\nresampling approximation of the oracle test. The BEAST improves the empirical\npower of many existing tests against a wide spectrum of common alternatives and\ndelivers a clear interpretation of dependency forms when significant."}, "http://arxiv.org/abs/2103.16159": {"title": "Controlling the False Discovery Rate in Transformational Sparsity: Split Knockoffs", "link": "http://arxiv.org/abs/2103.16159", "description": "Controlling the False Discovery Rate (FDR) in a variable selection procedure\nis critical for reproducible discoveries, and it has been extensively studied\nin sparse linear models. However, it remains largely open in scenarios where\nthe sparsity constraint is not directly imposed on the parameters but on a\nlinear transformation of the parameters to be estimated. Examples of such\nscenarios include total variations, wavelet transforms, fused LASSO, and trend\nfiltering. In this paper, we propose a data-adaptive FDR control method, called\nthe Split Knockoff method, for this transformational sparsity setting. The\nproposed method exploits both variable and data splitting. The linear\ntransformation constraint is relaxed to its Euclidean proximity in a lifted\nparameter space, which yields an orthogonal design that enables the orthogonal\nSplit Knockoff construction. To overcome the challenge that exchangeability\nfails due to the heterogeneous noise brought by the transformation, new inverse\nsupermartingale structures are developed via data splitting for provable FDR\ncontrol without sacrificing power. Simulation experiments demonstrate that the\nproposed methodology achieves the desired FDR and power. We also provide an\napplication to Alzheimer's Disease study, where atrophy brain regions and their\nabnormal connections can be discovered based on a structural Magnetic Resonance\nImaging dataset (ADNI)."}, "http://arxiv.org/abs/2201.05967": {"title": "Uniform Inference for Kernel Density Estimators with Dyadic Data", "link": "http://arxiv.org/abs/2201.05967", "description": "Dyadic data is often encountered when quantities of interest are associated\nwith the edges of a network. As such it plays an important role in statistics,\neconometrics and many other data science disciplines. We consider the problem\nof uniformly estimating a dyadic Lebesgue density function, focusing on\nnonparametric kernel-based estimators taking the form of dyadic empirical\nprocesses. Our main contributions include the minimax-optimal uniform\nconvergence rate of the dyadic kernel density estimator, along with strong\napproximation results for the associated standardized and Studentized\n$t$-processes. A consistent variance estimator enables the construction of\nvalid and feasible uniform confidence bands for the unknown density function.\nWe showcase the broad applicability of our results by developing novel\ncounterfactual density estimation and inference methodology for dyadic data,\nwhich can be used for causal inference and program evaluation. A crucial\nfeature of dyadic distributions is that they may be \"degenerate\" at certain\npoints in the support of the data, a property making our analysis somewhat\ndelicate. Nonetheless our methods for uniform inference remain robust to the\npotential presence of such points. For implementation purposes, we discuss\ninference procedures based on positive semi-definite covariance estimators,\nmean squared error optimal bandwidth selectors and robust bias correction\ntechniques. We illustrate the empirical finite-sample performance of our\nmethods both in simulations and with real-world trade data, for which we make\ncomparisons between observed and counterfactual trade distributions in\ndifferent years. Our technical results concerning strong approximations and\nmaximal inequalities are of potential independent interest."}, "http://arxiv.org/abs/2206.01076": {"title": "Likelihood-based Inference for Random Networks with Changepoints", "link": "http://arxiv.org/abs/2206.01076", "description": "Generative, temporal network models play an important role in analyzing the\ndependence structure and evolution patterns of complex networks. Due to the\ncomplicated nature of real network data, it is often naive to assume that the\nunderlying data-generative mechanism itself is invariant with time. Such\nobservation leads to the study of changepoints or sudden shifts in the\ndistributional structure of the evolving network. In this paper, we propose a\nlikelihood-based methodology to detect changepoints in undirected, affine\npreferential attachment networks, and establish a hypothesis testing framework\nto detect a single changepoint, together with a consistent estimator for the\nchangepoint. Such results require establishing consistency and asymptotic\nnormality of the MLE under the changepoint regime, which suffers from long\nrange dependence. The methodology is then extended to the multiple changepoint\nsetting via both a sliding window method and a more computationally efficient\nscore statistic. We also compare the proposed methodology with previously\ndeveloped non-parametric estimators of the changepoint via simulation, and the\nmethods developed herein are applied to modeling the popularity of a topic in a\nTwitter network over time."}, "http://arxiv.org/abs/2301.01616": {"title": "Locally Private Causal Inference for Randomized Experiments", "link": "http://arxiv.org/abs/2301.01616", "description": "Local differential privacy is a differential privacy paradigm in which\nindividuals first apply a privacy mechanism to their data (often by adding\nnoise) before transmitting the result to a curator. The noise for privacy\nresults in additional bias and variance in their analyses. Thus it is of great\nimportance for analysts to incorporate the privacy noise into valid inference.\nIn this article, we develop methodologies to infer causal effects from locally\nprivatized data under randomized experiments. First, we present frequentist\nestimators under various privacy scenarios with their variance estimators and\nplug-in confidence intervals. We show a na\\\"ive debiased estimator results in\ninferior mean-squared error (MSE) compared to minimax lower bounds. In\ncontrast, we show that using a customized privacy mechanism, we can match the\nlower bound, giving minimax optimal inference. We also develop a Bayesian\nnonparametric methodology along with a blocked Gibbs sampling algorithm, which\ncan be applied to any of our proposed privacy mechanisms, and which performs\nespecially well in terms of MSE for tight privacy budgets. Finally, we present\nsimulation studies to evaluate the performance of our proposed frequentist and\nBayesian methodologies for various privacy budgets, resulting in useful\nsuggestions for performing causal inference for privatized data."}, "http://arxiv.org/abs/2303.03215": {"title": "Quantile-Quantile Methodology -- Detailed Results", "link": "http://arxiv.org/abs/2303.03215", "description": "The linear quantile-quantile relationship provides an easy-to-implement yet\neffective tool for transformation to and testing for normality. Its good\nperformance is verified in this report."}, "http://arxiv.org/abs/2305.06645": {"title": "Causal Inference for Continuous Multiple Time Point Interventions", "link": "http://arxiv.org/abs/2305.06645", "description": "There are limited options to estimate the treatment effects of variables\nwhich are continuous and measured at multiple time points, particularly if the\ntrue dose-response curve should be estimated as closely as possible. However,\nthese situations may be of relevance: in pharmacology, one may be interested in\nhow outcomes of people living with -- and treated for -- HIV, such as viral\nfailure, would vary for time-varying interventions such as different drug\nconcentration trajectories. A challenge for doing causal inference with\ncontinuous interventions is that the positivity assumption is typically\nviolated. To address positivity violations, we develop projection functions,\nwhich reweigh and redefine the estimand of interest based on functions of the\nconditional support for the respective interventions. With these functions, we\nobtain the desired dose-response curve in areas of enough support, and\notherwise a meaningful estimand that does not require the positivity\nassumption. We develop $g$-computation type plug-in estimators for this case.\nThose are contrasted with g-computation estimators which are applied to\ncontinuous interventions without specifically addressing positivity violations,\nwhich we propose to be presented with diagnostics. The ideas are illustrated\nwith longitudinal data from HIV positive children treated with an\nefavirenz-based regimen as part of the CHAPAS-3 trial, which enrolled children\n$&lt;13$ years in Zambia/Uganda. Simulations show in which situations a standard\n$g$-computation approach is appropriate, and in which it leads to bias and how\nthe proposed weighted estimation approach then recovers the alternative\nestimand of interest."}, "http://arxiv.org/abs/2305.14275": {"title": "Amortized Variational Inference with Coverage Guarantees", "link": "http://arxiv.org/abs/2305.14275", "description": "Amortized variational inference produces a posterior approximation that can\nbe rapidly computed given any new observation. Unfortunately, there are few\nguarantees about the quality of these approximate posteriors. We propose\nConformalized Amortized Neural Variational Inference (CANVI), a procedure that\nis scalable, easily implemented, and provides guaranteed marginal coverage.\nGiven a collection of candidate amortized posterior approximators, CANVI\nconstructs conformalized predictors based on each candidate, compares the\npredictors using a metric known as predictive efficiency, and returns the most\nefficient predictor. CANVI ensures that the resulting predictor constructs\nregions that contain the truth with a user-specified level of probability.\nCANVI is agnostic to design decisions in formulating the candidate\napproximators and only requires access to samples from the forward model,\npermitting its use in likelihood-free settings. We prove lower bounds on the\npredictive efficiency of the regions produced by CANVI and explore how the\nquality of a posterior approximation relates to the predictive efficiency of\nprediction regions based on that approximation. Finally, we demonstrate the\naccurate calibration and high predictive efficiency of CANVI on a suite of\nsimulation-based inference benchmark tasks and an important scientific task:\nanalyzing galaxy emission spectra."}, "http://arxiv.org/abs/2305.17187": {"title": "Clip-OGD: An Experimental Design for Adaptive Neyman Allocation in Sequential Experiments", "link": "http://arxiv.org/abs/2305.17187", "description": "From clinical development of cancer therapies to investigations into partisan\nbias, adaptive sequential designs have become increasingly popular method for\ncausal inference, as they offer the possibility of improved precision over\ntheir non-adaptive counterparts. However, even in simple settings (e.g. two\ntreatments) the extent to which adaptive designs can improve precision is not\nsufficiently well understood. In this work, we study the problem of Adaptive\nNeyman Allocation in a design-based potential outcomes framework, where the\nexperimenter seeks to construct an adaptive design which is nearly as efficient\nas the optimal (but infeasible) non-adaptive Neyman design, which has access to\nall potential outcomes. Motivated by connections to online optimization, we\npropose Neyman Ratio and Neyman Regret as two (equivalent) performance measures\nof adaptive designs for this problem. We present Clip-OGD, an adaptive design\nwhich achieves $\\widetilde{O}(\\sqrt{T})$ expected Neyman regret and thereby\nrecovers the optimal Neyman variance in large samples. Finally, we construct a\nconservative variance estimator which facilitates the development of\nasymptotically valid confidence intervals. To complement our theoretical\nresults, we conduct simulations using data from a microeconomic experiment."}, "http://arxiv.org/abs/2306.15622": {"title": "Biclustering random matrix partitions with an application to classification of forensic body fluids", "link": "http://arxiv.org/abs/2306.15622", "description": "Classification of unlabeled data is usually achieved by supervised learning\nfrom labeled samples. Although there exist many sophisticated supervised\nmachine learning methods that can predict the missing labels with a high level\nof accuracy, they often lack the required transparency in situations where it\nis important to provide interpretable results and meaningful measures of\nconfidence. Body fluid classification of forensic casework data is the case in\npoint. We develop a new Biclustering Dirichlet Process for Class-assignment\nwith Random Matrices (BDP-CaRMa), with a three-level hierarchy of clustering,\nand a model-based approach to classification that adapts to block structure in\nthe data matrix. As the class labels of some observations are missing, the\nnumber of rows in the data matrix for each class is unknown. BDP-CaRMa handles\nthis and extends existing biclustering methods by simultaneously biclustering\nmultiple matrices each having a randomly variable number of rows. We\ndemonstrate our method by applying it to the motivating problem, which is the\nclassification of body fluids based on mRNA profiles taken from crime scenes.\nThe analyses of casework-like data show that our method is interpretable and\nproduces well-calibrated posterior probabilities. Our model can be more\ngenerally applied to other types of data with a similar structure to the\nforensic data."}, "http://arxiv.org/abs/2307.05644": {"title": "Lambert W random variables and their applications in loss modelling", "link": "http://arxiv.org/abs/2307.05644", "description": "Several distributions and families of distributions are proposed to model\nskewed data, think, e.g., of skew-normal and related distributions. Lambert W\nrandom variables offer an alternative approach where, instead of constructing a\nnew distribution, a certain transform is proposed (Goerg, 2011). Such an\napproach allows the construction of a Lambert W skewed version from any\ndistribution. We choose Lambert W normal distribution as a natural starting\npoint and also include Lambert W exponential distribution due to the simplicity\nand shape of the exponential distribution, which, after skewing, may produce a\nreasonably heavy tail for loss models. In the theoretical part, we focus on the\nmathematical properties of obtained distributions, including the range of\nskewness. In the practical part, the suitability of corresponding Lambert W\ntransformed distributions is evaluated on real insurance data. The results are\ncompared with those obtained using common loss distributions."}, "http://arxiv.org/abs/2307.06840": {"title": "Ensemble learning for blending gridded satellite and gauge-measured precipitation data", "link": "http://arxiv.org/abs/2307.06840", "description": "Regression algorithms are regularly used for improving the accuracy of\nsatellite precipitation products. In this context, satellite precipitation and\ntopography data are the predictor variables, and gauged-measured precipitation\ndata are the dependent variables. Alongside this, it is increasingly recognised\nin many fields that combinations of algorithms through ensemble learning can\nlead to substantial predictive performance improvements. Still, a sufficient\nnumber of ensemble learners for improving the accuracy of satellite\nprecipitation products and their large-scale comparison are currently missing\nfrom the literature. In this study, we work towards filling in this specific\ngap by proposing 11 new ensemble learners in the field and by extensively\ncomparing them. We apply the ensemble learners to monthly data from the\nPERSIANN (Precipitation Estimation from Remotely Sensed Information using\nArtificial Neural Networks) and IMERG (Integrated Multi-satellitE Retrievals\nfor GPM) gridded datasets that span over a 15-year period and over the entire\nthe contiguous United States (CONUS). We also use gauge-measured precipitation\ndata from the Global Historical Climatology Network monthly database, version 2\n(GHCNm). The ensemble learners combine the predictions of six machine learning\nregression algorithms (base learners), namely the multivariate adaptive\nregression splines (MARS), multivariate adaptive polynomial splines\n(poly-MARS), random forests (RF), gradient boosting machines (GBM), extreme\ngradient boosting (XGBoost) and Bayesian regularized neural networks (BRNN),\nand each of them is based on a different combiner. The combiners include the\nequal-weight combiner, the median combiner, two best learners and seven\nvariants of a sophisticated stacking method. The latter stacks a regression\nalgorithm on top of the base learners to combine their independent\npredictions..."}, "http://arxiv.org/abs/2309.12819": {"title": "Doubly Robust Proximal Causal Learning for Continuous Treatments", "link": "http://arxiv.org/abs/2309.12819", "description": "Proximal causal learning is a promising framework for identifying the causal\neffect under the existence of unmeasured confounders. Within this framework,\nthe doubly robust (DR) estimator was derived and has shown its effectiveness in\nestimation, especially when the model assumption is violated. However, the\ncurrent form of the DR estimator is restricted to binary treatments, while the\ntreatment can be continuous in many real-world applications. The primary\nobstacle to continuous treatments resides in the delta function present in the\noriginal DR estimator, making it infeasible in causal effect estimation and\nintroducing a heavy computational burden in nuisance function estimation. To\naddress these challenges, we propose a kernel-based DR estimator that can well\nhandle continuous treatments. Equipped with its smoothness, we show that its\noracle form is a consistent approximation of the influence function. Further,\nwe propose a new approach to efficiently solve the nuisance functions. We then\nprovide a comprehensive convergence analysis in terms of the mean square error.\nWe demonstrate the utility of our estimator on synthetic datasets and\nreal-world applications."}, "http://arxiv.org/abs/2309.17283": {"title": "The Blessings of Multiple Treatments and Outcomes in Treatment Effect Estimation", "link": "http://arxiv.org/abs/2309.17283", "description": "Assessing causal effects in the presence of unobserved confounding is a\nchallenging problem. Existing studies leveraged proxy variables or multiple\ntreatments to adjust for the confounding bias. In particular, the latter\napproach attributes the impact on a single outcome to multiple treatments,\nallowing estimating latent variables for confounding control. Nevertheless,\nthese methods primarily focus on a single outcome, whereas in many real-world\nscenarios, there is greater interest in studying the effects on multiple\noutcomes. Besides, these outcomes are often coupled with multiple treatments.\nExamples include the intensive care unit (ICU), where health providers evaluate\nthe effectiveness of therapies on multiple health indicators. To accommodate\nthese scenarios, we consider a new setting dubbed as multiple treatments and\nmultiple outcomes. We then show that parallel studies of multiple outcomes\ninvolved in this setting can assist each other in causal identification, in the\nsense that we can exploit other treatments and outcomes as proxies for each\ntreatment effect under study. We proceed with a causal discovery method that\ncan effectively identify such proxies for causal estimation. The utility of our\nmethod is demonstrated in synthetic data and sepsis disease."}, "http://arxiv.org/abs/2310.10740": {"title": "Unbiased Estimation of Structured Prediction Error", "link": "http://arxiv.org/abs/2310.10740", "description": "Many modern datasets, such as those in ecology and geology, are composed of\nsamples with spatial structure and dependence. With such data violating the\nusual independent and identically distributed (IID) assumption in machine\nlearning and classical statistics, it is unclear a priori how one should\nmeasure the performance and generalization of models. Several authors have\nempirically investigated cross-validation (CV) methods in this setting,\nreaching mixed conclusions. We provide a class of unbiased estimation methods\nfor general quadratic errors, correlated Gaussian response, and arbitrary\nprediction function $g$, for a noise-elevated version of the error. Our\napproach generalizes the coupled bootstrap (CB) from the normal means problem\nto general normal data, allowing correlation both within and between the\ntraining and test sets. CB relies on creating bootstrap samples that are\nintelligently decoupled, in the sense of being statistically independent.\nSpecifically, the key to CB lies in generating two independent \"views\" of our\ndata and using them as stand-ins for the usual independent training and test\nsamples. Beginning with Mallows' $C_p$, we generalize the estimator to develop\nour generalized $C_p$ estimators (GC). We show at under only a moment condition\non $g$, this noise-elevated error estimate converges smoothly to the noiseless\nerror estimate. We show that when Stein's unbiased risk estimator (SURE)\napplies, GC converges to SURE as in the normal means problem. Further, we use\nthese same tools to analyze CV and provide some theoretical analysis to help\nunderstand when CV will provide good estimates of error. Simulations align with\nour theoretical results, demonstrating the effectiveness of GC and illustrating\nthe behavior of CV methods. Lastly, we apply our estimator to a model selection\ntask on geothermal data in Nevada."}, "http://arxiv.org/abs/2310.10761": {"title": "Simulation Based Composite Likelihood", "link": "http://arxiv.org/abs/2310.10761", "description": "Inference for high-dimensional hidden Markov models is challenging due to the\nexponential-in-dimension computational cost of the forward algorithm. To\naddress this issue, we introduce an innovative composite likelihood approach\ncalled \"Simulation Based Composite Likelihood\" (SimBa-CL). With SimBa-CL, we\napproximate the likelihood by the product of its marginals, which we estimate\nusing Monte Carlo sampling. In a similar vein to approximate Bayesian\ncomputation (ABC), SimBa-CL requires multiple simulations from the model, but,\nin contrast to ABC, it provides a likelihood approximation that guides the\noptimization of the parameters. Leveraging automatic differentiation libraries,\nit is simple to calculate gradients and Hessians to not only speed-up\noptimization, but also to build approximate confidence sets. We conclude with\nan extensive experimental section, where we empirically validate our\ntheoretical results, conduct a comparative analysis with SMC, and apply\nSimBa-CL to real-world Aphtovirus data."}, "http://arxiv.org/abs/2310.10798": {"title": "Poisson Count Time Series", "link": "http://arxiv.org/abs/2310.10798", "description": "This paper reviews and compares popular methods, some old and some very\nrecent, that produce time series having Poisson marginal distributions. The\npaper begins by narrating ways where time series with Poisson marginal\ndistributions can be produced. Modeling nonstationary series with covariates\nmotivates consideration of methods where the Poisson parameter depends on time.\nHere, estimation methods are developed for some of the more flexible methods.\nThe results are used in the analysis of 1) a count sequence of tropical\ncyclones occurring in the North Atlantic Basin since 1970, and 2) the number of\nno-hitter games pitched in major league baseball since 1893. Tests for whether\nthe Poisson marginal distribution is appropriate are included."}, "http://arxiv.org/abs/2310.10915": {"title": "Identifiability of the Multinomial Processing Tree-IRT model for the Philadelphia Naming Test", "link": "http://arxiv.org/abs/2310.10915", "description": "For persons with aphasia, naming tests are used to evaluate the severity of\nthe disease and observing progress toward recovery. The Philadelphia Naming\nTest (PNT) is a leading naming test composed of 175 items. The items are common\nnouns which are one to four syllables in length and with low, medium, and high\nfrequency. Since the target word is known to the administrator, the response\nfrom the patient can be classified as correct or an error. If the patient\ncommits an error, the PNT provides procedures for classifying the type of error\nin the response. Item response theory can be applied to PNT data to provide\nestimates of item difficulty and subject naming ability. Walker et al. (2018)\ndeveloped a IRT multinomial processing tree (IRT-MPT) model to attempt to\nunderstand the pathways through which the different errors are made by patients\nwhen responding to an item. The MPT model expands on existing models by\nconsidering items to be heterogeneous and estimating multiple latent parameters\nfor patients to more precisely determine at which step of word of production a\npatient's ability has been affected. These latent parameters represent the\ntheoretical cognitive steps taken in responding to an item. Given the\ncomplexity of the model proposed in Walker et al. (2018), here we investigate\nthe identifiability of the parameters included in the IRT-MPT model."}, "http://arxiv.org/abs/2310.10976": {"title": "Exact nonlinear state estimation", "link": "http://arxiv.org/abs/2310.10976", "description": "The majority of data assimilation (DA) methods in the geosciences are based\non Gaussian assumptions. While these assumptions facilitate efficient\nalgorithms, they cause analysis biases and subsequent forecast degradations.\nNon-parametric, particle-based DA algorithms have superior accuracy, but their\napplication to high-dimensional models still poses operational challenges.\nDrawing inspiration from recent advances in the field of generative artificial\nintelligence (AI), this article introduces a new nonlinear estimation theory\nwhich attempts to bridge the existing gap in DA methodology. Specifically, a\nConjugate Transform Filter (CTF) is derived and shown to generalize the\ncelebrated Kalman filter to arbitrarily non-Gaussian distributions. The new\nfilter has several desirable properties, such as its ability to preserve\nstatistical relationships in the prior state and convergence to highly accurate\nobservations. An ensemble approximation of the new theory (ECTF) is also\npresented and validated using idealized statistical experiments that feature\nbounded quantities with non-Gaussian distributions, a prevalent challenge in\nEarth system models. Results from these experiments indicate that the greatest\nbenefits from ECTF occur when observation errors are small relative to the\nforecast uncertainty and when state variables exhibit strong nonlinear\ndependencies. Ultimately, the new filtering theory offers exciting avenues for\nimproving conventional DA algorithms through their principled integration with\nAI techniques."}, "http://arxiv.org/abs/2310.11122": {"title": "Sensitivity-Aware Amortized Bayesian Inference", "link": "http://arxiv.org/abs/2310.11122", "description": "Bayesian inference is a powerful framework for making probabilistic\ninferences and decisions under uncertainty. Fundamental choices in modern\nBayesian workflows concern the specification of the likelihood function and\nprior distributions, the posterior approximator, and the data. Each choice can\nsignificantly influence model-based inference and subsequent decisions, thereby\nnecessitating sensitivity analysis. In this work, we propose a multifaceted\napproach to integrate sensitivity analyses into amortized Bayesian inference\n(ABI, i.e., simulation-based inference with neural networks). First, we utilize\nweight sharing to encode the structural similarities between alternative\nlikelihood and prior specifications in the training process with minimal\ncomputational overhead. Second, we leverage the rapid inference of neural\nnetworks to assess sensitivity to various data perturbations or pre-processing\nprocedures. In contrast to most other Bayesian approaches, both steps\ncircumvent the costly bottleneck of refitting the model(s) for each choice of\nlikelihood, prior, or dataset. Finally, we propose to use neural network\nensembles to evaluate variation in results induced by unreliable approximation\non unseen data. We demonstrate the effectiveness of our method in applied\nmodeling problems, ranging from the estimation of disease outbreak dynamics and\nglobal warming thresholds to the comparison of human decision-making models.\nOur experiments showcase how our approach enables practitioners to effectively\nunveil hidden relationships between modeling choices and inferential\nconclusions."}, "http://arxiv.org/abs/2310.11357": {"title": "A Pseudo-likelihood Approach to Under-5 Mortality Estimation", "link": "http://arxiv.org/abs/2310.11357", "description": "Accurate and precise estimates of under-5 mortality rates (U5MR) are an\nimportant health summary for countries. Full survival curves are additionally\nof interest to better understand the pattern of mortality in children under 5.\nModern demographic methods for estimating a full mortality schedule for\nchildren have been developed for countries with good vital registration and\nreliable census data, but perform poorly in many low- and middle-income\ncountries. In these countries, the need to utilize nationally representative\nsurveys to estimate U5MR requires additional statistical care to mitigate\npotential biases in survey data, acknowledge the survey design, and handle\naspects of survival data (i.e., censoring and truncation). In this paper, we\ndevelop parametric and non-parametric pseudo-likelihood approaches to\nestimating under-5 mortality across time from complex survey data. We argue\nthat the parametric approach is particularly useful in scenarios where data are\nsparse and estimation may require stronger assumptions. The nonparametric\napproach provides an aid to model validation. We compare a variety of\nparametric models to three existing methods for obtaining a full survival curve\nfor children under the age of 5, and argue that a parametric pseudo-likelihood\napproach is advantageous in low- and middle-income countries. We apply our\nproposed approaches to survey data from Burkina Faso, Malawi, Senegal, and\nNamibia. All code for fitting the models described in this paper is available\nin the R package pssst."}, "http://arxiv.org/abs/2006.00767": {"title": "Generative Multiple-purpose Sampler for Weighted M-estimation", "link": "http://arxiv.org/abs/2006.00767", "description": "To overcome the computational bottleneck of various data perturbation\nprocedures such as the bootstrap and cross validations, we propose the\nGenerative Multiple-purpose Sampler (GMS), which constructs a generator\nfunction to produce solutions of weighted M-estimators from a set of given\nweights and tuning parameters. The GMS is implemented by a single optimization\nwithout having to repeatedly evaluate the minimizers of weighted losses, and is\nthus capable of significantly reducing the computational time. We demonstrate\nthat the GMS framework enables the implementation of various statistical\nprocedures that would be unfeasible in a conventional framework, such as the\niterated bootstrap, bootstrapped cross-validation for penalized likelihood,\nbootstrapped empirical Bayes with nonparametric maximum likelihood, etc. To\nconstruct a computationally efficient generator function, we also propose a\nnovel form of neural network called the \\emph{weight multiplicative multilayer\nperceptron} to achieve fast convergence. Our numerical results demonstrate that\nthe new neural network structure enjoys a few orders of magnitude speed\nadvantage in comparison to the conventional one. An R package called GMS is\nprovided, which runs under Pytorch to implement the proposed methods and allows\nthe user to provide a customized loss function to tailor to their own models of\ninterest."}, "http://arxiv.org/abs/2012.03593": {"title": "Algebraic geometry of discrete interventional models", "link": "http://arxiv.org/abs/2012.03593", "description": "We investigate the algebra and geometry of general interventions in discrete\nDAG models. To this end, we introduce a theory for modeling soft interventions\nin the more general family of staged tree models and develop the formalism to\nstudy these models as parametrized subvarieties of a product of probability\nsimplices. We then consider the problem of finding their defining equations,\nand we derive a combinatorial criterion for identifying interventional staged\ntree models for which the defining ideal is toric. We apply these results to\nthe class of discrete interventional DAG models and establish a criteria to\ndetermine when these models are toric varieties."}, "http://arxiv.org/abs/2105.12720": {"title": "Marginal structural models with Latent Class Growth Modeling of Treatment Trajectories", "link": "http://arxiv.org/abs/2105.12720", "description": "In a real-life setting, little is known regarding the effectiveness of\nstatins for primary prevention among older adults, and analysis of\nobservational data can add crucial information on the benefits of actual\npatterns of use. Latent class growth models (LCGM) are increasingly proposed as\na solution to summarize the observed longitudinal treatment in a few distinct\ngroups. When combined with standard approaches like Cox proportional hazards\nmodels, LCGM can fail to control time-dependent confounding bias because of\ntime-varying covariates that have a double role of confounders and mediators.\nWe propose to use LCGM to classify individuals into a few latent classes based\non their medication adherence pattern, then choose a working marginal\nstructural model (MSM) that relates the outcome to these groups. The parameter\nof interest is nonparametrically defined as the projection of the true MSM onto\nthe chosen working model. The combination of LCGM with MSM is a convenient way\nto describe treatment adherence and can effectively control time-dependent\nconfounding. Simulation studies were used to illustrate our approach and\ncompare it with unadjusted, baseline covariates-adjusted, time-varying\ncovariates adjusted and inverse probability of trajectory groups weighting\nadjusted models. We found that our proposed approach yielded estimators with\nlittle or no bias."}, "http://arxiv.org/abs/2208.07610": {"title": "E-Statistics, Group Invariance and Anytime Valid Testing", "link": "http://arxiv.org/abs/2208.07610", "description": "We study worst-case-growth-rate-optimal (GROW) e-statistics for hypothesis\ntesting between two group models. It is known that under a mild condition on\nthe action of the underlying group G on the data, there exists a maximally\ninvariant statistic. We show that among all e-statistics, invariant or not, the\nlikelihood ratio of the maximally invariant statistic is GROW, both in the\nabsolute and in the relative sense, and that an anytime-valid test can be based\non it. The GROW e-statistic is equal to a Bayes factor with a right Haar prior\non G. Our treatment avoids nonuniqueness issues that sometimes arise for such\npriors in Bayesian contexts. A crucial assumption on the group G is its\namenability, a well-known group-theoretical condition, which holds, for\ninstance, in scale-location families. Our results also apply to\nfinite-dimensional linear regression."}, "http://arxiv.org/abs/2302.03246": {"title": "CDANs: Temporal Causal Discovery from Autocorrelated and Non-Stationary Time Series Data", "link": "http://arxiv.org/abs/2302.03246", "description": "Time series data are found in many areas of healthcare such as medical time\nseries, electronic health records (EHR), measurements of vitals, and wearable\ndevices. Causal discovery, which involves estimating causal relationships from\nobservational data, holds the potential to play a significant role in\nextracting actionable insights about human health. In this study, we present a\nnovel constraint-based causal discovery approach for autocorrelated and\nnon-stationary time series data (CDANs). Our proposed method addresses several\nlimitations of existing causal discovery methods for autocorrelated and\nnon-stationary time series data, such as high dimensionality, the inability to\nidentify lagged causal relationships, and overlooking changing modules. Our\napproach identifies lagged and instantaneous/contemporaneous causal\nrelationships along with changing modules that vary over time. The method\noptimizes the conditioning sets in a constraint-based search by considering\nlagged parents instead of conditioning on the entire past that addresses high\ndimensionality. The changing modules are detected by considering both\ncontemporaneous and lagged parents. The approach first detects the lagged\nadjacencies, then identifies the changing modules and contemporaneous\nadjacencies, and finally determines the causal direction. We extensively\nevaluated our proposed method on synthetic and real-world clinical datasets,\nand compared its performance with several baseline approaches. The experimental\nresults demonstrate the effectiveness of the proposed method in detecting\ncausal relationships and changing modules for autocorrelated and non-stationary\ntime series data."}, "http://arxiv.org/abs/2305.07089": {"title": "Hierarchically Coherent Multivariate Mixture Networks", "link": "http://arxiv.org/abs/2305.07089", "description": "Large collections of time series data are often organized into hierarchies\nwith different levels of aggregation; examples include product and geographical\ngroupings. Probabilistic coherent forecasting is tasked to produce forecasts\nconsistent across levels of aggregation. In this study, we propose to augment\nneural forecasting architectures with a coherent multivariate mixture output.\nWe optimize the networks with a composite likelihood objective, allowing us to\ncapture time series' relationships while maintaining high computational\nefficiency. Our approach demonstrates 13.2% average accuracy improvements on\nmost datasets compared to state-of-the-art baselines. We conduct ablation\nstudies of the framework components and provide theoretical foundations for\nthem. To assist related work, the code is available at this\nhttps://github.com/Nixtla/neuralforecast."}, "http://arxiv.org/abs/2307.16720": {"title": "The epigraph and the hypograph indexes as useful tools for clustering multivariate functional data", "link": "http://arxiv.org/abs/2307.16720", "description": "The proliferation of data generation has spurred advancements in functional\ndata analysis. With the ability to analyze multiple variables simultaneously,\nthe demand for working with multivariate functional data has increased. This\nstudy proposes a novel formulation of the epigraph and hypograph indexes, as\nwell as their generalized expressions, specifically tailored for the\nmultivariate functional context. These definitions take into account the\ninterrelations between components. Furthermore, the proposed indexes are\nemployed to cluster multivariate functional data. In the clustering process,\nthe indexes are applied to both the data and their first and second\nderivatives. This generates a reduced-dimension dataset from the original\nmultivariate functional data, enabling the application of well-established\nmultivariate clustering techniques which have been extensively studied in the\nliterature. This methodology has been tested through simulated and real\ndatasets, performing comparative analyses against state-of-the-art to assess\nits performance."}, "http://arxiv.org/abs/2309.07810": {"title": "Spectrum-Aware Adjustment: A New Debiasing Framework with Applications to Principal Component Regression", "link": "http://arxiv.org/abs/2309.07810", "description": "We introduce a new debiasing framework for high-dimensional linear regression\nthat bypasses the restrictions on covariate distributions imposed by modern\ndebiasing technology. We study the prevalent setting where the number of\nfeatures and samples are both large and comparable. In this context,\nstate-of-the-art debiasing technology uses a degrees-of-freedom correction to\nremove the shrinkage bias of regularized estimators and conduct inference.\nHowever, this method requires that the observed samples are i.i.d., the\ncovariates follow a mean zero Gaussian distribution, and reliable covariance\nmatrix estimates for observed features are available. This approach struggles\nwhen (i) covariates are non-Gaussian with heavy tails or asymmetric\ndistributions, (ii) rows of the design exhibit heterogeneity or dependencies,\nand (iii) reliable feature covariance estimates are lacking.\n\nTo address these, we develop a new strategy where the debiasing correction is\na rescaled gradient descent step (suitably initialized) with step size\ndetermined by the spectrum of the sample covariance matrix. Unlike prior work,\nwe assume that eigenvectors of this matrix are uniform draws from the\northogonal group. We show this assumption remains valid in diverse situations\nwhere traditional debiasing fails, including designs with complex row-column\ndependencies, heavy tails, asymmetric properties, and latent low-rank\nstructures. We establish asymptotic normality of our proposed estimator\n(centered and scaled) under various convergence notions. Moreover, we develop a\nconsistent estimator for its asymptotic variance. Lastly, we introduce a\ndebiased Principal Components Regression (PCR) technique using our\nSpectrum-Aware approach. In varied simulations and real data experiments, we\nobserve that our method outperforms degrees-of-freedom debiasing by a margin."}, "http://arxiv.org/abs/2310.11471": {"title": "Modeling lower-truncated and right-censored insurance claims with an extension of the MBBEFD class", "link": "http://arxiv.org/abs/2310.11471", "description": "In general insurance, claims are often lower-truncated and right-censored\nbecause insurance contracts may involve deductibles and maximal covers. Most\nclassical statistical models are not (directly) suited to model lower-truncated\nand right-censored claims. A surprisingly flexible family of distributions that\ncan cope with lower-truncated and right-censored claims is the class of MBBEFD\ndistributions that originally has been introduced by Bernegger (1997) for\nreinsurance pricing, but which has not gained much attention outside the\nreinsurance literature. We derive properties of the class of MBBEFD\ndistributions, and we extend it to a bigger family of distribution functions\nsuitable for modeling lower-truncated and right-censored claims. Interestingly,\nin general insurance, we mainly rely on unimodal skewed densities, whereas the\nreinsurance literature typically proposes monotonically decreasing densities\nwithin the MBBEFD class."}, "http://arxiv.org/abs/2310.11603": {"title": "Group sequential two-stage preference designs", "link": "http://arxiv.org/abs/2310.11603", "description": "The two-stage preference design (TSPD) enables the inference for treatment\nefficacy while allowing for incorporation of patient preference to treatment.\nIt can provide unbiased estimates for selection and preference effects, where a\nselection effect occurs when patients who prefer one treatment respond\ndifferently than those who prefer another, and a preference effect is the\ndifference in response caused by an interaction between the patient's\npreference and the actual treatment they receive. One potential barrier to\nadopting TSPD in practice, however, is the relatively large sample size\nrequired to estimate selection and preference effects with sufficient power. To\naddress this concern, we propose a group sequential two-stage preference design\n(GS-TSPD), which combines TSPD with sequential monitoring for early stopping.\nIn the GS-TSPD, pre-planned sequential monitoring allows investigators to\nconduct repeated hypothesis tests on accumulated data prior to full enrollment\nto assess study eligibility for early trial termination without inflating type\nI error rates. Thus, the procedure allows investigators to terminate the study\nwhen there is sufficient evidence of treatment, selection, or preference\neffects during an interim analysis, thereby reducing the design resource in\nexpectation. To formalize such a procedure, we verify the independent\nincrements assumption for testing the selection and preference effects and\napply group sequential stopping boundaries from the approximate sequential\ndensity functions. Simulations are then conducted to investigate the operating\ncharacteristics of our proposed GS-TSPD compared to the traditional TSPD. We\ndemonstrate the applicability of the design using a study of Hepatitis C\ntreatment modality."}, "http://arxiv.org/abs/2310.11620": {"title": "Enhancing modified treatment policy effect estimation with weighted energy distance", "link": "http://arxiv.org/abs/2310.11620", "description": "The effects of continuous treatments are often characterized through the\naverage dose response function, which is challenging to estimate from\nobservational data due to confounding and positivity violations. Modified\ntreatment policies (MTPs) are an alternative approach that aim to assess the\neffect of a modification to observed treatment values and work under relaxed\nassumptions. Estimators for MTPs generally focus on estimating the conditional\ndensity of treatment given covariates and using it to construct weights.\nHowever, weighting using conditional density models has well-documented\nchallenges. Further, MTPs with larger treatment modifications have stronger\nconfounding and no tools exist to help choose an appropriate modification\nmagnitude. This paper investigates the role of weights for MTPs showing that to\ncontrol confounding, weights should balance the weighted data to an unobserved\nhypothetical target population, that can be characterized with observed data.\nLeveraging this insight, we present a versatile set of tools to enhance\nestimation for MTPs. We introduce a distance that measures imbalance of\ncovariate distributions under the MTP and use it to develop new weighting\nmethods and tools to aid in the estimation of MTPs. We illustrate our methods\nthrough an example studying the effect of mechanical power of ventilation on\nin-hospital mortality."}, "http://arxiv.org/abs/2310.11630": {"title": "Adaptive Bootstrap Tests for Composite Null Hypotheses in the Mediation Pathway Analysis", "link": "http://arxiv.org/abs/2310.11630", "description": "Mediation analysis aims to assess if, and how, a certain exposure influences\nan outcome of interest through intermediate variables. This problem has\nrecently gained a surge of attention due to the tremendous need for such\nanalyses in scientific fields. Testing for the mediation effect is greatly\nchallenged by the fact that the underlying null hypothesis (i.e. the absence of\nmediation effects) is composite. Most existing mediation tests are overly\nconservative and thus underpowered. To overcome this significant methodological\nhurdle, we develop an adaptive bootstrap testing framework that can accommodate\ndifferent types of composite null hypotheses in the mediation pathway analysis.\nApplied to the product of coefficients (PoC) test and the joint significance\n(JS) test, our adaptive testing procedures provide type I error control under\nthe composite null, resulting in much improved statistical power compared to\nexisting tests. Both theoretical properties and numerical examples of the\nproposed methodology are discussed."}, "http://arxiv.org/abs/2310.11683": {"title": "Are we bootstrapping the right thing? A new approach to quantify uncertainty of Average Treatment Effect Estimate", "link": "http://arxiv.org/abs/2310.11683", "description": "Existing approaches of using the bootstrap method to derive standard error\nand confidence interval of average treatment effect estimate has one potential\nissue, which is that they are actually bootstrapping the wrong thing, resulting\nin unvalid statistical inference. In this paper, we discuss this important\nissue and propose a new non-parametric bootstrap method that can more precisely\nquantify the uncertainty associated with average treatment effect estimates. We\ndemonstrate the validity of this approach through a simulation study and a\nreal-world example, and highlight the importance of deriving standard error and\nconfidence interval of average treatment effect estimates that both remove\nextra undesired noise and are easy to interpret when applied in real world\nscenarios."}, "http://arxiv.org/abs/2310.11724": {"title": "Simultaneous Nonparametric Inference of M-regression under Complex Temporal Dynamics", "link": "http://arxiv.org/abs/2310.11724", "description": "The paper considers simultaneous nonparametric inference for a wide class of\nM-regression models with time-varying coefficients. The covariates and errors\nof the regression model are tackled as a general class of piece-wise locally\nstationary time series and are allowed to be cross-dependent. We introduce an\nintegration technique to study the M-estimators, whose limiting properties are\ndisclosed using Bahadur representation and Gaussian approximation theory.\nFacilitated by a self-convolved bootstrap proposed in this paper, we introduce\na unified framework to conduct general classes of Exact Function Tests,\nLack-of-fit Tests, and Qualitative Tests for the time-varying coefficient\nM-regression under complex temporal dynamics. As an application, our method is\napplied to studying the anthropogenic warming trend and time-varying structures\nof the ENSO effect using global climate data from 1882 to 2005."}, "http://arxiv.org/abs/2310.11741": {"title": "Graph Sphere: From Nodes to Supernodes in Graphical Models", "link": "http://arxiv.org/abs/2310.11741", "description": "High-dimensional data analysis typically focuses on low-dimensional\nstructure, often to aid interpretation and computational efficiency. Graphical\nmodels provide a powerful methodology for learning the conditional independence\nstructure in multivariate data by representing variables as nodes and\ndependencies as edges. Inference is often focused on individual edges in the\nlatent graph. Nonetheless, there is increasing interest in determining more\ncomplex structures, such as communities of nodes, for multiple reasons,\nincluding more effective information retrieval and better interpretability. In\nthis work, we propose a multilayer graphical model where we first cluster nodes\nand then, at the second layer, investigate the relationships among groups of\nnodes. Specifically, nodes are partitioned into \"supernodes\" with a\ndata-coherent size-biased tessellation prior which combines ideas from Bayesian\nnonparametrics and Voronoi tessellations. This construct allows accounting also\nfor dependence of nodes within supernodes. At the second layer, dependence\nstructure among supernodes is modelled through a Gaussian graphical model,\nwhere the focus of inference is on \"superedges\". We provide theoretical\njustification for our modelling choices. We design tailored Markov chain Monte\nCarlo schemes, which also enable parallel computations. We demonstrate the\neffectiveness of our approach for large-scale structure learning in simulations\nand a transcriptomics application."}, "http://arxiv.org/abs/2310.11779": {"title": "A Multivariate Skew-Normal-Tukey-h Distribution", "link": "http://arxiv.org/abs/2310.11779", "description": "We introduce a new family of multivariate distributions by taking the\ncomponent-wise Tukey-h transformation of a random vector following a\nskew-normal distribution. The proposed distribution is named the\nskew-normal-Tukey-h distribution and is an extension of the skew-normal\ndistribution for handling heavy-tailed data. We compare this proposed\ndistribution to the skew-t distribution, which is another extension of the\nskew-normal distribution for modeling tail-thickness, and demonstrate that when\nthere are substantial differences in marginal kurtosis, the proposed\ndistribution is more appropriate. Moreover, we derive many appealing stochastic\nproperties of the proposed distribution and provide a methodology for the\nestimation of the parameters in which the computational requirement increases\nlinearly with the dimension. Using simulations, as well as a wine and a wind\nspeed data application, we illustrate how to draw inferences based on the\nmultivariate skew-normal-Tukey-h distribution."}, "http://arxiv.org/abs/2310.11799": {"title": "Testing for patterns and structures in covariance and correlation matrices", "link": "http://arxiv.org/abs/2310.11799", "description": "Covariance matrices of random vectors contain information that is crucial for\nmodelling. Certain structures and patterns of the covariances (or correlations)\nmay be used to justify parametric models, e.g., autoregressive models. Until\nnow, there have been only few approaches for testing such covariance structures\nsystematically and in a unified way. In the present paper, we propose such a\nunified testing procedure, and we will exemplify the approach with a large\nvariety of covariance structure models. This includes common structures such as\ndiagonal matrices, Toeplitz matrices, and compound symmetry but also the more\ninvolved autoregressive matrices. We propose hypothesis tests for these\nstructures, and we use bootstrap techniques for better small-sample\napproximation. The structures of the proposed tests invite for adaptations to\nother covariance patterns by choosing the hypothesis matrix appropriately. We\nprove their correctness for large sample sizes. The proposed methods require\nonly weak assumptions.\n\nWith the help of a simulation study, we assess the small sample properties of\nthe tests.\n\nWe also analyze a real data set to illustrate the application of the\nprocedure."}, "http://arxiv.org/abs/2310.11822": {"title": "Post-clustering Inference under Dependency", "link": "http://arxiv.org/abs/2310.11822", "description": "Recent work by Gao et al. has laid the foundations for post-clustering\ninference. For the first time, the authors established a theoretical framework\nallowing to test for differences between means of estimated clusters.\nAdditionally, they studied the estimation of unknown parameters while\ncontrolling the selective type I error. However, their theory was developed for\nindependent observations identically distributed as $p$-dimensional Gaussian\nvariables with a spherical covariance matrix. Here, we aim at extending this\nframework to a more convenient scenario for practical applications, where\narbitrary dependence structures between observations and features are allowed.\nWe show that a $p$-value for post-clustering inference under general dependency\ncan be defined, and we assess the theoretical conditions allowing the\ncompatible estimation of a covariance matrix. The theory is developed for\nhierarchical agglomerative clustering algorithms with several types of\nlinkages, and for the $k$-means algorithm. We illustrate our method with\nsynthetic data and real data of protein structures."}, "http://arxiv.org/abs/2310.11969": {"title": "Survey calibration for causal inference: a simple method to balance covariate distributions", "link": "http://arxiv.org/abs/2310.11969", "description": "This paper proposes a simple method for balancing distributions of covariates\nfor causal inference based on observational studies. The method makes it\npossible to balance an arbitrary number of quantiles (e.g., medians, quartiles,\nor deciles) together with means if necessary. The proposed approach is based on\nthe theory of calibration estimators (Deville and S\\\"arndal 1992), in\nparticular, calibration estimators for quantiles, proposed by Harms and\nDuchesne (2006). By modifying the entropy balancing method and the covariate\nbalancing propensity score method, it is possible to balance the distributions\nof the treatment and control groups. The method does not require numerical\nintegration, kernel density estimation or assumptions about the distributions;\nvalid estimates can be obtained by drawing on existing asymptotic theory.\nResults of a simulation study indicate that the method efficiently estimates\naverage treatment effects on the treated (ATT), the average treatment effect\n(ATE), the quantile treatment effect on the treated (QTT) and the quantile\ntreatment effect (QTE), especially in the presence of non-linearity and\nmis-specification of the models. The proposed methods are implemented in an\nopen source R package jointCalib."}, "http://arxiv.org/abs/2310.12000": {"title": "Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models", "link": "http://arxiv.org/abs/2310.12000", "description": "Latent Gaussian process (GP) models are flexible probabilistic non-parametric\nfunction models. Vecchia approximations are accurate approximations for GPs to\novercome computational bottlenecks for large data, and the Laplace\napproximation is a fast method with asymptotic convergence guarantees to\napproximate marginal likelihoods and posterior predictive distributions for\nnon-Gaussian likelihoods. Unfortunately, the computational complexity of\ncombined Vecchia-Laplace approximations grows faster than linearly in the\nsample size when used in combination with direct solver methods such as the\nCholesky decomposition. Computations with Vecchia-Laplace approximations thus\nbecome prohibitively slow precisely when the approximations are usually the\nmost accurate, i.e., on large data sets. In this article, we present several\niterative methods for inference with Vecchia-Laplace approximations which make\ncomputations considerably faster compared to Cholesky-based calculations. We\nanalyze our proposed methods theoretically and in experiments with simulated\nand real-world data. In particular, we obtain a speed-up of an order of\nmagnitude compared to Cholesky-based inference and a threefold increase in\nprediction accuracy in terms of the continuous ranked probability score\ncompared to a state-of-the-art method on a large satellite data set. All\nmethods are implemented in a free C++ software library with high-level Python\nand R packages."}, "http://arxiv.org/abs/2310.12010": {"title": "A Note on Improving Variational Estimation for Multidimensional Item Response Theory", "link": "http://arxiv.org/abs/2310.12010", "description": "Survey instruments and assessments are frequently used in many domains of\nsocial science. When the constructs that these assessments try to measure\nbecome multifaceted, multidimensional item response theory (MIRT) provides a\nunified framework and convenient statistical tool for item analysis,\ncalibration, and scoring. However, the computational challenge of estimating\nMIRT models prohibits its wide use because many of the extant methods can\nhardly provide results in a realistic time frame when the number of dimensions,\nsample size, and test length are large. Instead, variational estimation\nmethods, such as Gaussian Variational Expectation Maximization (GVEM)\nalgorithm, have been recently proposed to solve the estimation challenge by\nproviding a fast and accurate solution. However, results have shown that\nvariational estimation methods may produce some bias on discrimination\nparameters during confirmatory model estimation, and this note proposes an\nimportance weighted version of GVEM (i.e., IW-GVEM) to correct for such bias\nunder MIRT models. We also use the adaptive moment estimation method to update\nthe learning rate for gradient descent automatically. Our simulations show that\nIW-GVEM can effectively correct bias with modest increase of computation time,\ncompared with GVEM. The proposed method may also shed light on improving the\nvariational estimation for other psychometrics models."}, "http://arxiv.org/abs/2310.12115": {"title": "MMD-based Variable Importance for Distributional Random Forest", "link": "http://arxiv.org/abs/2310.12115", "description": "Distributional Random Forest (DRF) is a flexible forest-based method to\nestimate the full conditional distribution of a multivariate output of interest\ngiven input variables. In this article, we introduce a variable importance\nalgorithm for DRFs, based on the well-established drop and relearn principle\nand MMD distance. While traditional importance measures only detect variables\nwith an influence on the output mean, our algorithm detects variables impacting\nthe output distribution more generally. We show that the introduced importance\nmeasure is consistent, exhibits high empirical performance on both real and\nsimulated data, and outperforms competitors. In particular, our algorithm is\nhighly efficient to select variables through recursive feature elimination, and\ncan therefore provide small sets of variables to build accurate estimates of\nconditional output distributions."}, "http://arxiv.org/abs/2310.12140": {"title": "Online Estimation with Rolling Validation: Adaptive Nonparametric Estimation with Stream Data", "link": "http://arxiv.org/abs/2310.12140", "description": "Online nonparametric estimators are gaining popularity due to their efficient\ncomputation and competitive generalization abilities. An important example\nincludes variants of stochastic gradient descent. These algorithms often take\none sample point at a time and instantly update the parameter estimate of\ninterest. In this work we consider model selection and hyperparameter tuning\nfor such online algorithms. We propose a weighted rolling-validation procedure,\nan online variant of leave-one-out cross-validation, that costs minimal extra\ncomputation for many typical stochastic gradient descent estimators. Similar to\nbatch cross-validation, it can boost base estimators to achieve a better,\nadaptive convergence rate. Our theoretical analysis is straightforward, relying\nmainly on some general statistical stability assumptions. The simulation study\nunderscores the significance of diverging weights in rolling validation in\npractice and demonstrates its sensitivity even when there is only a slim\ndifference between candidate estimators."}, "http://arxiv.org/abs/2010.02968": {"title": "Modelling of functional profiles and explainable shape shifts detection: An approach combining the notion of the Fr\\'echet mean with the shape invariant model", "link": "http://arxiv.org/abs/2010.02968", "description": "A modelling framework suitable for detecting shape shifts in functional\nprofiles combining the notion of Fr\\'echet mean and the concept of deformation\nmodels is developed and proposed. The generalized mean sense offered by the\nFr\\'echet mean notion is employed to capture the typical pattern of the\nprofiles under study, while the concept of deformation models, and in\nparticular of the shape invariant model, allows for interpretable\nparameterizations of profile's deviations from the typical shape. EWMA-type\ncontrol charts compatible with the functional nature of data and the employed\ndeformation model are built and proposed, exploiting certain shape\ncharacteristics of the profiles under study with respect to the generalized\nmean sense, allowing for the identification of potential shifts concerning the\nshape and/or the deformation process. Potential shifts in the shape deformation\nprocess, are further distinguished to significant shifts with respect to\namplitude and/or the phase of the profile under study. The proposed modelling\nand shift detection framework is implemented to a real world case study, where\ndaily concentration profiles concerning air pollutants from an area in the city\nof Athens are modelled, while profiles indicating hazardous concentration\nlevels are successfully identified in most of the cases."}, "http://arxiv.org/abs/2207.07218": {"title": "On the Selection of Tuning Parameters for Patch-Stitching Embedding Methods", "link": "http://arxiv.org/abs/2207.07218", "description": "While classical scaling, just like principal component analysis, is\nparameter-free, other methods for embedding multivariate data require the\nselection of one or several tuning parameters. This tuning can be difficult due\nto the unsupervised nature of the situation. We propose a simple, almost\nobvious, approach to supervise the choice of tuning parameter(s): minimize a\nnotion of stress. We apply this approach to the selection of the patch size in\na prototypical patch-stitching embedding method, both in the multidimensional\nscaling (aka network localization) setting and in the dimensionality reduction\n(aka manifold learning) setting. In our study, we uncover a new bias--variance\ntradeoff phenomenon."}, "http://arxiv.org/abs/2303.17856": {"title": "Bootstrapping multiple systems estimates to account for model selection", "link": "http://arxiv.org/abs/2303.17856", "description": "Multiple systems estimation using a Poisson loglinear model is a standard\napproach to quantifying hidden populations where data sources are based on\nlists of known cases. Information criteria are often used for selecting between\nthe large number of possible models. Confidence intervals are often reported\nconditional on the model selected, providing an over-optimistic impression of\nestimation accuracy. A bootstrap approach is a natural way to account for the\nmodel selection. However, because the model selection step has to be carried\nout for every bootstrap replication, there may be a high or even prohibitive\ncomputational burden. We explore the merit of modifying the model selection\nprocedure in the bootstrap to look only among a subset of models, chosen on the\nbasis of their information criterion score on the original data. This provides\nlarge computational gains with little apparent effect on inference. We also\nincorporate rigorous and economical ways of approaching issues of the existence\nof estimators when applying the method to sparse data tables."}, "http://arxiv.org/abs/2308.07319": {"title": "Partial identification for discrete data with nonignorable missing outcomes", "link": "http://arxiv.org/abs/2308.07319", "description": "Nonignorable missing outcomes are common in real world datasets and often\nrequire strong parametric assumptions to achieve identification. These\nassumptions can be implausible or untestable, and so we may forgo them in\nfavour of partially identified models that narrow the set of a priori possible\nvalues to an identification region. Here we propose a new nonparametric Bayes\nmethod that allows for the incorporation of multiple clinically relevant\nrestrictions of the parameter space simultaneously. We focus on two common\nrestrictions, instrumental variables and the direction of missing data bias,\nand investigate how these restrictions narrow the identification region for\nparameters of interest. Additionally, we propose a rejection sampling algorithm\nthat allows us to quantify the evidence for these assumptions in the data. We\ncompare our method to a standard Heckman selection model in both simulation\nstudies and in an applied problem examining the effectiveness of cash-transfers\nfor people experiencing homelessness."}, "http://arxiv.org/abs/2310.12285": {"title": "Sparse high-dimensional linear mixed modeling with a partitioned empirical Bayes ECM algorithm", "link": "http://arxiv.org/abs/2310.12285", "description": "High-dimensional longitudinal data is increasingly used in a wide range of\nscientific studies. However, there are few statistical methods for\nhigh-dimensional linear mixed models (LMMs), as most Bayesian variable\nselection or penalization methods are designed for independent observations.\nAdditionally, the few available software packages for high-dimensional LMMs\nsuffer from scalability issues. This work presents an efficient and accurate\nBayesian framework for high-dimensional LMMs. We use empirical Bayes estimators\nof hyperparameters for increased flexibility and an\nExpectation-Conditional-Minimization (ECM) algorithm for computationally\nefficient maximum a posteriori probability (MAP) estimation of parameters. The\nnovelty of the approach lies in its partitioning and parameter expansion as\nwell as its fast and scalable computation. We illustrate Linear Mixed Modeling\nwith PaRtitiOned empirical Bayes ECM (LMM-PROBE) in simulation studies\nevaluating fixed and random effects estimation along with computation time. A\nreal-world example is provided using data from a study of lupus in children,\nwhere we identify genes and clinical factors associated with a new lupus\nbiomarker and predict the biomarker over time."}, "http://arxiv.org/abs/2310.12348": {"title": "Goodness--of--Fit Tests Based on the Min--Characteristic Function", "link": "http://arxiv.org/abs/2310.12348", "description": "We propose tests of fit for classes of distributions that include the\nWeibull, the Pareto and the Fr\\'echet, distributions. The new tests employ the\nnovel tool of the min--characteristic function and are based on an L2--type\nweighted distance between this function and its empirical counterpart applied\non suitably standardized data. If data--standardization is performed using the\nMLE of the distributional parameters then the method reduces to testing for the\nstandard member of the family, with parameter values known and set equal to\none. We investigate asymptotic properties of the tests, while a Monte Carlo\nstudy is presented that includes the new procedure as well as competitors for\nthe purpose of specification testing with three extreme value distributions.\nThe new tests are also applied on a few real--data sets."}, "http://arxiv.org/abs/2310.12358": {"title": "causalBETA: An R Package for Bayesian Semiparametric Casual Inference with Event-Time Outcomes", "link": "http://arxiv.org/abs/2310.12358", "description": "Observational studies are often conducted to estimate causal effects of\ntreatments or exposures on event-time outcomes. Since treatments are not\nrandomized in observational studies, techniques from causal inference are\nrequired to adjust for confounding. Bayesian approaches to causal estimates are\ndesirable because they provide 1) prior smoothing provides useful\nregularization of causal effect estimates, 2) flexible models that are robust\nto misspecification, 3) full inference (i.e. both point and uncertainty\nestimates) for causal estimands. However, Bayesian causal inference is\ndifficult to implement manually and there is a lack of user-friendly software,\npresenting a significant barrier to wide-spread use. We address this gap by\ndeveloping causalBETA (Bayesian Event Time Analysis) - an open-source R package\nfor estimating causal effects on event-time outcomes using Bayesian\nsemiparametric models. The package provides a familiar front-end to users, with\nsyntax identical to existing survival analysis R packages such as survival. At\nthe same time, it back-ends to Stan - a popular platform for Bayesian modeling\nand high performance statistical computing - for efficient posterior\ncomputation. To improve user experience, the package is built using customized\nS3 class objects and methods to facilitate visualizations and summaries of\nresults using familiar generic functions like plot() and summary(). In this\npaper, we provide the methodological details of the package, a demonstration\nusing publicly-available data, and computational guidance."}, "http://arxiv.org/abs/2310.12391": {"title": "Real-time Semiparametric Regression via Sequential Monte Carlo", "link": "http://arxiv.org/abs/2310.12391", "description": "We develop and describe online algorithms for performing real-time\nsemiparametric regression analyses. Earlier work on this topic is in Luts,\nBroderick &amp; Wand (J. Comput. Graph. Statist., 2014) where online mean field\nvariational Bayes was employed. In this article we instead develop sequential\nMonte Carlo approaches to circumvent well-known inaccuracies inherent in\nvariational approaches. Even though sequential Monte Carlo is not as fast as\nonline mean field variational Bayes, it can be a viable alternative for\napplications where the data rate is not overly high. For Gaussian response\nsemiparametric regression models our new algorithms share the online mean field\nvariational Bayes property of only requiring updating and storage of sufficient\nstatistics quantities of streaming data. In the non-Gaussian case accurate\nreal-time semiparametric regression requires the full data to be kept in\nstorage. The new algorithms allow for new options concerning accuracy/speed\ntrade-offs for real-time semiparametric regression."}, "http://arxiv.org/abs/2310.12402": {"title": "Data visualization and dimension reduction for metric-valued response regression", "link": "http://arxiv.org/abs/2310.12402", "description": "As novel data collection becomes increasingly common, traditional dimension\nreduction and data visualization techniques are becoming inadequate to analyze\nthese complex data. A surrogate-assisted sufficient dimension reduction (SDR)\nmethod for regression with a general metric-valued response on Euclidean\npredictors is proposed. The response objects are mapped to a real-valued\ndistance matrix using an appropriate metric and then projected onto a large\nsample of random unit vectors to obtain scalar-valued surrogate responses. An\nensemble estimate of the subspaces for the regression of the surrogate\nresponses versus the predictor is used to estimate the original central space.\nUnder this framework, classical SDR methods such as ordinary least squares and\nsliced inverse regression are extended. The surrogate-assisted method applies\nto responses on compact metric spaces including but not limited to Euclidean,\ndistributional, and functional. An extensive simulation experiment demonstrates\nthe superior performance of the proposed surrogate-assisted method on synthetic\ndata compared to existing competing methods where applicable. The analysis of\nthe distributions and functional trajectories of county-level COVID-19\ntransmission rates in the U.S. as a function of demographic characteristics is\nalso provided. The theoretical justifications are included as well."}, "http://arxiv.org/abs/2310.12424": {"title": "Optimal heteroskedasticity testing in nonparametric regression", "link": "http://arxiv.org/abs/2310.12424", "description": "Heteroskedasticity testing in nonparametric regression is a classic\nstatistical problem with important practical applications, yet fundamental\nlimits are unknown. Adopting a minimax perspective, this article considers the\ntesting problem in the context of an $\\alpha$-H\\\"{o}lder mean and a\n$\\beta$-H\\\"{o}lder variance function. For $\\alpha &gt; 0$ and $\\beta \\in (0,\n\\frac{1}{2})$, the sharp minimax separation rate $n^{-4\\alpha} +\nn^{-\\frac{4\\beta}{4\\beta+1}} + n^{-2\\beta}$ is established. To achieve the\nminimax separation rate, a kernel-based statistic using first-order squared\ndifferences is developed. Notably, the statistic estimates a proxy rather than\na natural quadratic functional (the squared distance between the variance\nfunction and its best $L^2$ approximation by a constant) suggested in previous\nwork.\n\nThe setting where no smoothness is assumed on the variance function is also\nstudied; the variance profile across the design points can be arbitrary.\nDespite the lack of structure, consistent testing turns out to still be\npossible by using the Gaussian character of the noise, and the minimax rate is\nshown to be $n^{-4\\alpha} + n^{-1/2}$. Exploiting noise information happens to\nbe a fundamental necessity as consistent testing is impossible if nothing more\nthan zero mean and unit variance is known about the noise distribution.\nFurthermore, in the setting where $V$ is $\\beta$-H\\\"{o}lder but\nheteroskedasticity is measured only with respect to the design points, the\nminimax separation rate is shown to be $n^{-4\\alpha} + n^{-\\left(\\frac{1}{2}\n\\vee \\frac{4\\beta}{4\\beta+1}\\right)}$ when the noise is Gaussian and\n$n^{-4\\alpha} + n^{-\\frac{4\\beta}{4\\beta+1}} + n^{-2\\beta}$ when the noise\ndistribution is unknown."}, "http://arxiv.org/abs/2310.12427": {"title": "Fast Power Curve Approximation for Posterior Analyses", "link": "http://arxiv.org/abs/2310.12427", "description": "Bayesian hypothesis testing leverages posterior probabilities, Bayes factors,\nor credible intervals to assess characteristics that summarize data. We propose\na framework for power curve approximation with such hypothesis tests that\nassumes data are generated using statistical models with fixed parameters for\nthe purposes of sample size determination. We present a fast approach to\nexplore the sampling distribution of posterior probabilities when the\nconditions for the Bernstein-von Mises theorem are satisfied. We extend that\napproach to facilitate targeted sampling from the approximate sampling\ndistribution of posterior probabilities for each sample size explored. These\nsampling distributions are used to construct power curves for various types of\nposterior analyses. Our resulting method for power curve approximation is\norders of magnitude faster than conventional power curve estimation for\nBayesian hypothesis tests. We also prove the consistency of the corresponding\npower estimates and sample size recommendations under certain conditions."}, "http://arxiv.org/abs/2310.12428": {"title": "Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach", "link": "http://arxiv.org/abs/2310.12428", "description": "We initiate a novel approach to explain the out of sample performance of\nrandom forest (RF) models by exploiting the fact that any RF can be formulated\nas an adaptive weighted K nearest-neighbors model. Specifically, we use the\nproximity between points in the feature space learned by the RF to re-write\nrandom forest predictions exactly as a weighted average of the target labels of\ntraining data points. This linearity facilitates a local notion of\nexplainability of RF predictions that generates attributions for any model\nprediction across observations in the training set, and thereby complements\nestablished methods like SHAP, which instead generates attributions for a model\nprediction across dimensions of the feature space. We demonstrate this approach\nin the context of a bond pricing model trained on US corporate bond trades, and\ncompare our approach to various existing approaches to model explainability."}, "http://arxiv.org/abs/2310.12460": {"title": "Linear Source Apportionment using Generalized Least Squares", "link": "http://arxiv.org/abs/2310.12460", "description": "Motivated by applications to water quality monitoring using fluorescence\nspectroscopy, we develop the source apportionment model for high dimensional\nprofiles of dissolved organic matter (DOM). We describe simple methods to\nestimate the parameters of a linear source apportionment model, and show how\nthe estimates are related to those of ordinary and generalized least squares.\nUsing this least squares framework, we analyze the variability of the\nestimates, and we propose predictors for missing elements of a DOM profile. We\ndemonstrate the practical utility of our results on fluorescence spectroscopy\ndata collected from the Neuse River in North Carolina."}, "http://arxiv.org/abs/2310.12711": {"title": "Modelling multivariate extremes through angular-radial decomposition of the density function", "link": "http://arxiv.org/abs/2310.12711", "description": "We present a new framework for modelling multivariate extremes, based on an\nangular-radial representation of the probability density function. Under this\nrepresentation, the problem of modelling multivariate extremes is transformed\nto that of modelling an angular density and the tail of the radial variable,\nconditional on angle. Motivated by univariate theory, we assume that the tail\nof the conditional radial distribution converges to a generalised Pareto (GP)\ndistribution. To simplify inference, we also assume that the angular density is\ncontinuous and finite and the GP parameter functions are continuous with angle.\nWe refer to the resulting model as the semi-parametric angular-radial (SPAR)\nmodel for multivariate extremes. We consider the effect of the choice of polar\ncoordinate system and introduce generalised concepts of angular-radial\ncoordinate systems and generalised scalar angles in two dimensions. We show\nthat under certain conditions, the choice of polar coordinate system does not\naffect the validity of the SPAR assumptions. However, some choices of\ncoordinate system lead to simpler representations. In contrast, we show that\nthe choice of margin does affect whether the model assumptions are satisfied.\nIn particular, the use of Laplace margins results in a form of the density\nfunction for which the SPAR assumptions are satisfied for many common families\nof copula, with various dependence classes. We show that the SPAR model\nprovides a more versatile framework for characterising multivariate extremes\nthan provided by existing approaches, and that several commonly-used approaches\nare special cases of the SPAR model. Moreover, the SPAR framework provides a\nmeans of characterising all `extreme regions' of a joint distribution using a\nsingle inference. Applications in which this is useful are discussed."}, "http://arxiv.org/abs/2310.12757": {"title": "Conservative Inference for Counterfactuals", "link": "http://arxiv.org/abs/2310.12757", "description": "In causal inference, the joint law of a set of counterfactual random\nvariables is generally not identified. We show that a conservative version of\nthe joint law - corresponding to the smallest treatment effect - is identified.\nFinding this law uses recent results from optimal transport theory. Under this\nconservative law we can bound causal effects and we may construct inferences\nfor each individual's counterfactual dose-response curve. Intuitively, this is\nthe flattest counterfactual curve for each subject that is consistent with the\ndistribution of the observables. If the outcome is univariate then, under mild\nconditions, this curve is simply the quantile function of the counterfactual\ndistribution that passes through the observed point. This curve corresponds to\na nonparametric rank preserving structural model."}, "http://arxiv.org/abs/2310.12882": {"title": "Sequential Gibbs Posteriors with Applications to Principal Component Analysis", "link": "http://arxiv.org/abs/2310.12882", "description": "Gibbs posteriors are proportional to a prior distribution multiplied by an\nexponentiated loss function, with a key tuning parameter weighting information\nin the loss relative to the prior and providing a control of posterior\nuncertainty. Gibbs posteriors provide a principled framework for\nlikelihood-free Bayesian inference, but in many situations, including a single\ntuning parameter inevitably leads to poor uncertainty quantification. In\nparticular, regardless of the value of the parameter, credible regions have far\nfrom the nominal frequentist coverage even in large samples. We propose a\nsequential extension to Gibbs posteriors to address this problem. We prove the\nproposed sequential posterior exhibits concentration and a Bernstein-von Mises\ntheorem, which holds under easy to verify conditions in Euclidean space and on\nmanifolds. As a byproduct, we obtain the first Bernstein-von Mises theorem for\ntraditional likelihood-based Bayesian posteriors on manifolds. All methods are\nillustrated with an application to principal component analysis."}, "http://arxiv.org/abs/2207.06949": {"title": "Seeking the Truth Beyond the Data", "link": "http://arxiv.org/abs/2207.06949", "description": "Clustering is an unsupervised machine learning methodology where unlabeled\nelements/objects are grouped together aiming to the construction of\nwell-established clusters that their elements are classified according to their\nsimilarity. The goal of this process is to provide a useful aid to the\nresearcher that will help her/him to identify patterns among the data. Dealing\nwith large databases, such patterns may not be easily detectable without the\ncontribution of a clustering algorithm. This article provides a deep\ndescription of the most widely used clustering methodologies accompanied by\nuseful presentations concerning suitable parameter selection and\ninitializations. Simultaneously, this article not only represents a review\nhighlighting the major elements of examined clustering techniques but\nemphasizes the comparison of these algorithms' clustering efficiency based on 3\ndatasets, revealing their existing weaknesses and capabilities through accuracy\nand complexity, during the confrontation of discrete and continuous\nobservations. The produced results help us extract valuable conclusions about\nthe appropriateness of the examined clustering techniques in accordance with\nthe dataset's size."}, "http://arxiv.org/abs/2208.07831": {"title": "Structured prior distributions for the covariance matrix in latent factor models", "link": "http://arxiv.org/abs/2208.07831", "description": "Factor models are widely used for dimension reduction in the analysis of\nmultivariate data. This is achieved through decomposition of a p x p covariance\nmatrix into the sum of two components. Through a latent factor representation,\nthey can be interpreted as a diagonal matrix of idiosyncratic variances and a\nshared variation matrix, that is, the product of a p x k factor loadings matrix\nand its transpose. If k &lt;&lt; p, this defines a sparse factorisation of the\ncovariance matrix. Historically, little attention has been paid to\nincorporating prior information in Bayesian analyses using factor models where,\nat best, the prior for the factor loadings is order invariant. In this work, a\nclass of structured priors is developed that can encode ideas of dependence\nstructure about the shared variation matrix. The construction allows\ndata-informed shrinkage towards sensible parametric structures while also\nfacilitating inference over the number of factors. Using an unconstrained\nreparameterisation of stationary vector autoregressions, the methodology is\nextended to stationary dynamic factor models. For computational inference,\nparameter-expanded Markov chain Monte Carlo samplers are proposed, including an\nefficient adaptive Gibbs sampler. Two substantive applications showcase the\nscope of the methodology and its inferential benefits."}, "http://arxiv.org/abs/2209.11840": {"title": "Revisiting the Analysis of Matched-Pair and Stratified Experiments in the Presence of Attrition", "link": "http://arxiv.org/abs/2209.11840", "description": "In this paper we revisit some common recommendations regarding the analysis\nof matched-pair and stratified experimental designs in the presence of\nattrition. Our main objective is to clarify a number of well-known claims about\nthe practice of dropping pairs with an attrited unit when analyzing\nmatched-pair designs. Contradictory advice appears in the literature about\nwhether or not dropping pairs is beneficial or harmful, and stratifying into\nlarger groups has been recommended as a resolution to the issue. To address\nthese claims, we derive the estimands obtained from the difference-in-means\nestimator in a matched-pair design both when the observations from pairs with\nan attrited unit are retained and when they are dropped. We find limited\nevidence to support the claims that dropping pairs helps recover the average\ntreatment effect, but we find that it may potentially help in recovering a\nconvex weighted average of conditional average treatment effects. We report\nsimilar findings for stratified designs when studying the estimands obtained\nfrom a regression of outcomes on treatment with and without strata fixed\neffects."}, "http://arxiv.org/abs/2211.01746": {"title": "Log-density gradient covariance and automatic metric tensors for Riemann manifold Monte Carlo methods", "link": "http://arxiv.org/abs/2211.01746", "description": "A metric tensor for Riemann manifold Monte Carlo particularly suited for\nnon-linear Bayesian hierarchical models is proposed. The metric tensor is built\nfrom symmetric positive semidefinite log-density gradient covariance (LGC)\nmatrices, which are also proposed and further explored here. The LGCs\ngeneralize the Fisher information matrix by measuring the joint information\ncontent and dependence structure of both a random variable and the parameters\nof said variable. Consequently, positive definite Fisher/LGC-based metric\ntensors may be constructed not only from the observation likelihoods as is\ncurrent practice, but also from arbitrarily complicated non-linear prior/latent\nvariable structures, provided the LGC may be derived for each conditional\ndistribution used to construct said structures. The proposed methodology is\nhighly automatic and allows for exploitation of any sparsity associated with\nthe model in question. When implemented in conjunction with a Riemann manifold\nvariant of the recently proposed numerical generalized randomized Hamiltonian\nMonte Carlo processes, the proposed methodology is highly competitive, in\nparticular for the more challenging target distributions associated with\nBayesian hierarchical models."}, "http://arxiv.org/abs/2211.02383": {"title": "Simulation-Based Calibration Checking for Bayesian Computation: The Choice of Test Quantities Shapes Sensitivity", "link": "http://arxiv.org/abs/2211.02383", "description": "Simulation-based calibration checking (SBC) is a practical method to validate\ncomputationally-derived posterior distributions or their approximations. In\nthis paper, we introduce a new variant of SBC to alleviate several known\nproblems. Our variant allows the user to in principle detect any possible issue\nwith the posterior, while previously reported implementations could never\ndetect large classes of problems including when the posterior is equal to the\nprior. This is made possible by including additional data-dependent test\nquantities when running SBC. We argue and demonstrate that the joint likelihood\nof the data is an especially useful test quantity. Some other types of test\nquantities and their theoretical and practical benefits are also investigated.\nWe provide theoretical analysis of SBC, thereby providing a more complete\nunderstanding of the underlying statistical mechanisms. We also bring attention\nto a relatively common mistake in the literature and clarify the difference\nbetween SBC and checks based on the data-averaged posterior. We support our\nrecommendations with numerical case studies on a multivariate normal example\nand a case study in implementing an ordered simplex data type for use with\nHamiltonian Monte Carlo. The SBC variant introduced in this paper is\nimplemented in the $\\mathtt{SBC}$ R package."}, "http://arxiv.org/abs/2310.13081": {"title": "Metastable Hidden Markov Processes: a theory for modeling financial markets", "link": "http://arxiv.org/abs/2310.13081", "description": "The modeling of financial time series by hidden Markov models has been\nperformed successfully in the literature. In this paper, we propose a theory\nthat justifies such a modeling under the assumption that there exists a market\nformed by agents whose states evolve on time as an interacting Markov system\nthat has a metastable behavior described by the hidden Markov chain. This\ntheory is a rare application of metastability outside the modeling of physical\nsystems, and may inspire the development of new interacting Markov systems with\nfinancial constraints. In the context of financial economics and causal factor\ninvestment, the theory implies a new paradigm in which fluctuations in\ninvestment performance are primarily driven by the state of the market, rather\nthan being directly caused by other variables. Even though the usual approach\nto causal factor investment based on causal inference is not completely\ninconsistent with the proposed theory, the latter has the advantage of\naccounting for the non-stationary evolution of the time series through the\nchange between hidden market states. By accounting for this possibility, one\ncan more effectively assess risks and implement mitigation strategies."}, "http://arxiv.org/abs/2310.13162": {"title": "Network Meta-Analysis of Time-to-Event Endpoints with Individual Participant Data using Restricted Mean Survival Time Regression", "link": "http://arxiv.org/abs/2310.13162", "description": "Restricted mean survival time (RMST) models have gained popularity when\nanalyzing time-to-event outcomes because RMST models offer more straightforward\ninterpretations of treatment effects with fewer assumptions than hazard ratios\ncommonly estimated from Cox models. However, few network meta-analysis (NMA)\nmethods have been developed using RMST. In this paper, we propose advanced RMST\nNMA models when individual participant data are available. Our models allow us\nto study treatment effect moderation and provide comprehensive understanding\nabout comparative effectiveness of treatments and subgroup effects. An\nextensive simulation study and a real data example about treatments for\npatients with atrial fibrillation are presented."}, "http://arxiv.org/abs/2310.13178": {"title": "Exact Inference for Common Odds Ratio in Meta-Analysis with Zero-Total-Event Studies", "link": "http://arxiv.org/abs/2310.13178", "description": "Stemming from the high profile publication of Nissen and Wolski (2007) and\nsubsequent discussions with divergent views on how to handle observed\nzero-total-event studies, defined to be studies which observe zero events in\nboth treatment and control arms, the research topic concerning the common odds\nratio model with zero-total-event studies remains to be an unresolved problem\nin meta-analysis. In this article, we address this problem by proposing a novel\nrepro samples method to handle zero-total-event studies and make inference for\nthe parameter of common odds ratio. The development explicitly accounts for\nsampling scheme and does not rely on large sample approximation. It is\ntheoretically justified with a guaranteed finite sample performance. The\nempirical performance of the proposed method is demonstrated through simulation\nstudies. It shows that the proposed confidence set achieves the desired\nempirical coverage rate and also that the zero-total-event studies contains\ninformation and impacts the inference for the common odds ratio. The proposed\nmethod is applied to combine information in the Nissen and Wolski study."}, "http://arxiv.org/abs/2310.13232": {"title": "Interaction Screening and Pseudolikelihood Approaches for Tensor Learning in Ising Models", "link": "http://arxiv.org/abs/2310.13232", "description": "In this paper, we study two well known methods of Ising structure learning,\nnamely the pseudolikelihood approach and the interaction screening approach, in\nthe context of tensor recovery in $k$-spin Ising models. We show that both\nthese approaches, with proper regularization, retrieve the underlying\nhypernetwork structure using a sample size logarithmic in the number of network\nnodes, and exponential in the maximum interaction strength and maximum\nnode-degree. We also track down the exact dependence of the rate of tensor\nrecovery on the interaction order $k$, that is allowed to grow with the number\nof samples and nodes, for both the approaches. Finally, we provide a\ncomparative discussion of the performance of the two approaches based on\nsimulation studies, which also demonstrate the exponential dependence of the\ntensor recovery rate on the maximum coupling strength."}, "http://arxiv.org/abs/2310.13387": {"title": "Assumption violations in causal discovery and the robustness of score matching", "link": "http://arxiv.org/abs/2310.13387", "description": "When domain knowledge is limited and experimentation is restricted by\nethical, financial, or time constraints, practitioners turn to observational\ncausal discovery methods to recover the causal structure, exploiting the\nstatistical properties of their data. Because causal discovery without further\nassumptions is an ill-posed problem, each algorithm comes with its own set of\nusually untestable assumptions, some of which are hard to meet in real\ndatasets. Motivated by these considerations, this paper extensively benchmarks\nthe empirical performance of recent causal discovery methods on observational\ni.i.d. data generated under different background conditions, allowing for\nviolations of the critical assumptions required by each selected approach. Our\nexperimental findings show that score matching-based methods demonstrate\nsurprising performance in the false positive and false negative rate of the\ninferred graph in these challenging scenarios, and we provide theoretical\ninsights into their performance. This work is also the first effort to\nbenchmark the stability of causal discovery algorithms with respect to the\nvalues of their hyperparameters. Finally, we hope this paper will set a new\nstandard for the evaluation of causal discovery methods and can serve as an\naccessible entry point for practitioners interested in the field, highlighting\nthe empirical implications of different algorithm choices."}, "http://arxiv.org/abs/2310.13444": {"title": "Testing for the extent of instability in nearly unstable processes", "link": "http://arxiv.org/abs/2310.13444", "description": "This paper deals with unit root issues in time series analysis. It has been\nknown for a long time that unit root tests may be flawed when a series although\nstationary has a root close to unity. That motivated recent papers dedicated to\nautoregressive processes where the bridge between stability and instability is\nexpressed by means of time-varying coefficients. In this vein the process we\nconsider has a companion matrix $A_{n}$ with spectral radius $\\rho(A_{n}) &lt; 1$\nsatisfying $\\rho(A_{n}) \\rightarrow 1$, a situation that we describe as `nearly\nunstable'. The question we investigate is the following: given an observed path\nsupposed to come from a nearly-unstable process, is it possible to test for the\n`extent of instability', \\textit{i.e.} to test how close we are to the unit\nroot? In this regard, we develop a strategy to evaluate $\\alpha$ and to test\nfor $\\mathcal{H}_0 : \"\\alpha = \\alpha_0\"$ against $\\mathcal{H}_1 : \"\\alpha &gt;\n\\alpha_0\"$ when $\\rho(A_{n})$ lies in an inner $O(n^{-\\alpha})$-neighborhood of\nthe unity, for some $0 &lt; \\alpha &lt; 1$. Empirical evidence is given (on\nsimulations and real time series) about the advantages of the flexibility\ninduced by such a procedure compared to the usual unit root tests and their\nbinary responses. As a by-product, we also build a symmetric procedure for the\nusually left out situation where the dominant root lies around $-1$."}, "http://arxiv.org/abs/2310.13446": {"title": "Simple binning algorithm and SimDec visualization for comprehensive sensitivity analysis of complex computational models", "link": "http://arxiv.org/abs/2310.13446", "description": "Models of complex technological systems inherently contain interactions and\ndependencies among their input variables that affect their joint influence on\nthe output. Such models are often computationally expensive and few sensitivity\nanalysis methods can effectively process such complexities. Moreover, the\nsensitivity analysis field as a whole pays limited attention to the nature of\ninteraction effects, whose understanding can prove to be critical for the\ndesign of safe and reliable systems. In this paper, we introduce and\nextensively test a simple binning approach for computing sensitivity indices\nand demonstrate how complementing it with the smart visualization method,\nsimulation decomposition (SimDec), can permit important insights into the\nbehavior of complex engineering models. The simple binning approach computes\nfirst-, second-order effects, and a combined sensitivity index, and is\nconsiderably more computationally efficient than Sobol' indices. The totality\nof the sensitivity analysis framework provides an efficient and intuitive way\nto analyze the behavior of complex systems containing interactions and\ndependencies."}, "http://arxiv.org/abs/2310.13487": {"title": "Two-stage weighted least squares estimator of multivariate discrete-valued observation-driven models", "link": "http://arxiv.org/abs/2310.13487", "description": "In this work a general semi-parametric multivariate model where the first two\nconditional moments are assumed to be multivariate time series is introduced.\nThe focus of the estimation is the conditional mean parameter vector for\ndiscrete-valued distributions. Quasi-Maximum Likelihood Estimators (QMLEs)\nbased on the linear exponential family are typically employed for such\nestimation problems when the true multivariate conditional probability\ndistribution is unknown or too complex. Although QMLEs provide consistent\nestimates they may be inefficient. In this paper novel two-stage Multivariate\nWeighted Least Square Estimators (MWLSEs) are introduced which enjoy the same\nconsistency property as the QMLEs but can provide improved efficiency with\nsuitable choice of the covariance matrix of the observations. The proposed\nmethod allows for a more accurate estimation of model parameters in particular\nfor count and categorical data when maximum likelihood estimation is\nunfeasible. Moreover, consistency and asymptotic normality of MWLSEs are\nderived. The estimation performance of QMLEs and MWLSEs is compared through\nsimulation experiments and a real data application, showing superior accuracy\nof the proposed methodology."}, "http://arxiv.org/abs/2310.13511": {"title": "Dynamic Realized Minimum Variance Portfolio Models", "link": "http://arxiv.org/abs/2310.13511", "description": "This paper introduces a dynamic minimum variance portfolio (MVP) model using\nnonlinear volatility dynamic models, based on high-frequency financial data.\nSpecifically, we impose an autoregressive dynamic structure on MVP processes,\nwhich helps capture the MVP dynamics directly. To evaluate the dynamic MVP\nmodel, we estimate the inverse volatility matrix using the constrained\n$\\ell_1$-minimization for inverse matrix estimation (CLIME) and calculate daily\nrealized non-normalized MVP weights. Based on the realized non-normalized MVP\nweight estimator, we propose the dynamic MVP model, which we call the dynamic\nrealized minimum variance portfolio (DR-MVP) model. To estimate a large number\nof parameters, we employ the least absolute shrinkage and selection operator\n(LASSO) and predict the future MVP and establish its asymptotic properties.\nUsing high-frequency trading data, we apply the proposed method to MVP\nprediction."}, "http://arxiv.org/abs/2310.13580": {"title": "Bayesian Hierarchical Modeling for Bivariate Multiscale Spatial Data with Application to Blood Test Monitoring", "link": "http://arxiv.org/abs/2310.13580", "description": "In public health applications, spatial data collected are often recorded at\ndifferent spatial scales and over different correlated variables. Spatial\nchange of support is a key inferential problem in these applications and have\nbecome standard in univariate settings; however, it is less standard in\nmultivariate settings. There are several existing multivariate spatial models\nthat can be easily combined with multiscale spatial approach to analyze\nmultivariate multiscale spatial data. In this paper, we propose three new\nmodels from such combinations for bivariate multiscale spatial data in a\nBayesian context. In particular, we extend spatial random effects models,\nmultivariate conditional autoregressive models, and ordered hierarchical models\nthrough a multiscale spatial approach. We run simulation studies for the three\nmodels and compare them in terms of prediction performance and computational\nefficiency. We motivate our models through an analysis of 2015 Texas annual\naverage percentage receiving two blood tests from the Dartmouth Atlas Project."}, "http://arxiv.org/abs/2102.13209": {"title": "Wielding Occam's razor: Fast and frugal retail forecasting", "link": "http://arxiv.org/abs/2102.13209", "description": "The algorithms available for retail forecasting have increased in complexity.\nNewer methods, such as machine learning, are inherently complex. The more\ntraditional families of forecasting models, such as exponential smoothing and\nautoregressive integrated moving averages, have expanded to contain multiple\npossible forms and forecasting profiles. We question complexity in forecasting\nand the need to consider such large families of models. Our argument is that\nparsimoniously identifying suitable subsets of models will not decrease\nforecasting accuracy nor will it reduce the ability to estimate forecast\nuncertainty. We propose a framework that balances forecasting performance\nversus computational cost, resulting in the consideration of only a reduced set\nof models. We empirically demonstrate that a reduced set performs well.\nFinally, we translate computational benefits to monetary cost savings and\nenvironmental impact and discuss the implications of our results in the context\nof large retailers."}, "http://arxiv.org/abs/2211.04666": {"title": "Fast and Locally Adaptive Bayesian Quantile Smoothing using Calibrated Variational Approximations", "link": "http://arxiv.org/abs/2211.04666", "description": "Quantiles are useful characteristics of random variables that can provide\nsubstantial information on distributions compared with commonly used summary\nstatistics such as means. In this paper, we propose a Bayesian quantile trend\nfiltering method to estimate non-stationary trend of quantiles. We introduce\ngeneral shrinkage priors to induce locally adaptive Bayesian inference on\ntrends and mixture representation of the asymmetric Laplace likelihood. To\nquickly compute the posterior distribution, we develop calibrated mean-field\nvariational approximations to guarantee that the frequentist coverage of\ncredible intervals obtained from the approximated posterior is a specified\nnominal level. Simulation and empirical studies show that the proposed\nalgorithm is computationally much more efficient than the Gibbs sampler and\ntends to provide stable inference results, especially for high/low quantiles."}, "http://arxiv.org/abs/2305.17631": {"title": "A Bayesian Approach for Clustering Constant-wise Change-point Data", "link": "http://arxiv.org/abs/2305.17631", "description": "Change-point models deal with ordered data sequences. Their primary goal is\nto infer the locations where an aspect of the data sequence changes. In this\npaper, we propose and implement a nonparametric Bayesian model for clustering\nobservations based on their constant-wise change-point profiles via Gibbs\nsampler. Our model incorporates a Dirichlet Process on the constant-wise\nchange-point structures to cluster observations while simultaneously performing\nchange-point estimation. Additionally, our approach controls the number of\nclusters in the model, not requiring the specification of the number of\nclusters a priori. Our method's performance is evaluated on simulated data\nunder various scenarios and on a real dataset from single-cell genomic\nsequencing."}, "http://arxiv.org/abs/2306.06342": {"title": "Distribution-free inference with hierarchical data", "link": "http://arxiv.org/abs/2306.06342", "description": "This paper studies distribution-free inference in settings where the data set\nhas a hierarchical structure -- for example, groups of observations, or\nrepeated measurements. In such settings, standard notions of exchangeability\nmay not hold. To address this challenge, a hierarchical form of exchangeability\nis derived, facilitating extensions of distribution-free methods, including\nconformal prediction and jackknife+. While the standard theoretical guarantee\nobtained by the conformal prediction framework is a marginal predictive\ncoverage guarantee, in the special case of independent repeated measurements,\nit is possible to achieve a stronger form of coverage -- the \"second-moment\ncoverage\" property -- to provide better control of conditional miscoverage\nrates, and distribution-free prediction sets that achieve this property are\nconstructed. Simulations illustrate that this guarantee indeed leads to\nuniformly small conditional miscoverage rates. Empirically, this stronger\nguarantee comes at the cost of a larger width of the prediction set in\nscenarios where the fitted model is poorly calibrated, but this cost is very\nmild in cases where the fitted model is accurate."}, "http://arxiv.org/abs/2307.15205": {"title": "Robust graph-based methods for overcoming the curse of dimensionality", "link": "http://arxiv.org/abs/2307.15205", "description": "Graph-based two-sample tests and graph-based change-point detection that\nutilize a similarity graph provide a powerful tool for analyzing\nhigh-dimensional and non-Euclidean data as these methods do not impose\ndistributional assumptions on data and have good performance across various\nscenarios. Current graph-based tests that deliver efficacy across a broad\nspectrum of alternatives typically reply on the $K$-nearest neighbor graph or\nthe $K$-minimum spanning tree. However, these graphs can be vulnerable for\nhigh-dimensional data due to the curse of dimensionality. To mitigate this\nissue, we propose to use a robust graph that is considerably less influenced by\nthe curse of dimensionality. We also establish a theoretical foundation for\ngraph-based methods utilizing this proposed robust graph and demonstrate its\nconsistency under fixed alternatives for both low-dimensional and\nhigh-dimensional data."}, "http://arxiv.org/abs/2310.13764": {"title": "Random Flows of Covariance Operators and their Statistical Inference", "link": "http://arxiv.org/abs/2310.13764", "description": "We develop a statistical framework for conducting inference on collections of\ntime-varying covariance operators (covariance flows) over a general, possibly\ninfinite dimensional, Hilbert space. We model the intrinsically non-linear\nstructure of covariances by means of the Bures-Wasserstein metric geometry. We\nmake use of the Riemmanian-like structure induced by this metric to define a\nnotion of mean and covariance of a random flow, and develop an associated\nKarhunen-Lo\\`eve expansion. We then treat the problem of estimation and\nconstruction of functional principal components from a finite collection of\ncovariance flows. Our theoretical results are motivated by modern problems in\nfunctional data analysis, where one observes operator-valued random processes\n-- for instance when analysing dynamic functional connectivity and fMRI data,\nor when analysing multiple functional time series in the frequency domain.\n{Nevertheless, our framework is also novel in the finite-dimensions (matrix\ncase), and we demonstrate what simplifications can be afforded then}. We\nillustrate our methodology by means of simulations and a data analyses."}, "http://arxiv.org/abs/2310.13796": {"title": "Faithful graphical representations of local independence", "link": "http://arxiv.org/abs/2310.13796", "description": "Graphical models use graphs to represent conditional independence structure\nin the distribution of a random vector. In stochastic processes, graphs may\nrepresent so-called local independence or conditional Granger causality. Under\nsome regularity conditions, a local independence graph implies a set of\nindependences using a graphical criterion known as $\\delta$-separation, or\nusing its generalization, $\\mu$-separation. This is a stochastic process\nanalogue of $d$-separation in DAGs. However, there may be more independences\nthan implied by this graph and this is a violation of so-called faithfulness.\nWe characterize faithfulness in local independence graphs and give a method to\nconstruct a faithful graph from any local independence model such that the\noutput equals the true graph when Markov and faithfulness assumptions hold. We\ndiscuss various assumptions that are weaker than faithfulness, and we explore\ndifferent structure learning algorithms and their properties under varying\nassumptions."}, "http://arxiv.org/abs/2310.13826": {"title": "A p-value for Process Tracing and other N=1 Studies", "link": "http://arxiv.org/abs/2310.13826", "description": "The paper introduces a \\(p\\)-value that summarizes the evidence against a\nrival causal theory that explains an observed outcome in a single case. We show\nhow to represent the probability distribution characterizing a theorized rival\nhypothesis (the null) in the absence of randomization of treatment and when\ncounting on qualitative data, for instance when conducting process tracing. As\nin Fisher's \\autocite*{fisher1935design} original design, our \\(p\\)-value\nindicates how frequently one would find the same observations or even more\nfavorable observations under a theory that is compatible with our observations\nbut antagonistic to the working hypothesis. We also present an extension that\nallows researchers assess the sensitivity of their results to confirmation\nbias. Finally, we illustrate the application of our hypothesis test using the\nstudy by Snow \\autocite*{Snow1855} about the cause of Cholera in Soho, a\nclassic in Process Tracing, Epidemiology, and Microbiology. Our framework suits\nany type of case studies and evidence, such as data from interviews, archives,\nor participant observation."}, "http://arxiv.org/abs/2310.13858": {"title": "Likelihood-based surrogate dimension reduction", "link": "http://arxiv.org/abs/2310.13858", "description": "We consider the problem of surrogate sufficient dimension reduction, that is,\nestimating the central subspace of a regression model, when the covariates are\ncontaminated by measurement error. When no measurement error is present, a\nlikelihood-based dimension reduction method that relies on maximizing the\nlikelihood of a Gaussian inverse regression model on the Grassmann manifold is\nwell-known to have superior performance to traditional inverse moment methods.\nWe propose two likelihood-based estimators for the central subspace in\nmeasurement error settings, which make different adjustments to the observed\nsurrogates. Both estimators are computed based on maximizing objective\nfunctions on the Grassmann manifold and are shown to consistently recover the\ntrue central subspace. When the central subspace is assumed to depend on only a\nfew covariates, we further propose to augment the likelihood function with a\npenalty term that induces sparsity on the Grassmann manifold to obtain sparse\nestimators. The resulting objective function has a closed-form Riemann gradient\nwhich facilitates efficient computation of the penalized estimator. We leverage\nthe state-of-the-art trust region algorithm on the Grassmann manifold to\ncompute the proposed estimators efficiently. Simulation studies and a data\napplication demonstrate the proposed likelihood-based estimators perform better\nthan inverse moment-based estimators in terms of both estimation and variable\nselection accuracy."}, "http://arxiv.org/abs/2310.13874": {"title": "A Linear Errors-in-Variables Model with Unknown Heteroscedastic Measurement Errors", "link": "http://arxiv.org/abs/2310.13874", "description": "In the classic measurement error framework, covariates are contaminated by\nindependent additive noise. This paper considers parameter estimation in such a\nlinear errors-in-variables model where the unknown measurement error\ndistribution is heteroscedastic across observations. We propose a new\ngeneralized method of moment (GMM) estimator that combines a moment correction\napproach and a phase function-based approach. The former requires distributions\nto have four finite moments, while the latter relies on covariates having\nasymmetric distributions. The new estimator is shown to be consistent and\nasymptotically normal under appropriate regularity conditions. The asymptotic\ncovariance of the estimator is derived, and the estimated standard error is\ncomputed using a fast bootstrap procedure. The GMM estimator is demonstrated to\nhave strong finite sample performance in numerical studies, especially when the\nmeasurement errors follow non-Gaussian distributions."}, "http://arxiv.org/abs/2310.13911": {"title": "Multilevel Matrix Factor Model", "link": "http://arxiv.org/abs/2310.13911", "description": "Large-scale matrix data has been widely discovered and continuously studied\nin various fields recently. Considering the multi-level factor structure and\nutilizing the matrix structure, we propose a multilevel matrix factor model\nwith both global and local factors. The global factors can affect all matrix\ntimes series, whereas the local factors are only allow to affect within each\nspecific matrix time series. The estimation procedures can consistently\nestimate the factor loadings and determine the number of factors. We establish\nthe asymptotic properties of the estimators. The simulation is presented to\nillustrate the performance of the proposed estimation method. We utilize the\nmodel to analyze eight indicators across 200 stocks from ten distinct\nindustries, demonstrating the empirical utility of our proposed approach."}, "http://arxiv.org/abs/2310.13966": {"title": "Minimax Optimal Transfer Learning for Kernel-based Nonparametric Regression", "link": "http://arxiv.org/abs/2310.13966", "description": "In recent years, transfer learning has garnered significant attention in the\nmachine learning community. Its ability to leverage knowledge from related\nstudies to improve generalization performance in a target study has made it\nhighly appealing. This paper focuses on investigating the transfer learning\nproblem within the context of nonparametric regression over a reproducing\nkernel Hilbert space. The aim is to bridge the gap between practical\neffectiveness and theoretical guarantees. We specifically consider two\nscenarios: one where the transferable sources are known and another where they\nare unknown. For the known transferable source case, we propose a two-step\nkernel-based estimator by solely using kernel ridge regression. For the unknown\ncase, we develop a novel method based on an efficient aggregation algorithm,\nwhich can automatically detect and alleviate the effects of negative sources.\nThis paper provides the statistical properties of the desired estimators and\nestablishes the minimax optimal rate. Through extensive numerical experiments\non synthetic data and real examples, we validate our theoretical findings and\ndemonstrate the effectiveness of our proposed method."}, "http://arxiv.org/abs/2310.13973": {"title": "Estimation and convergence rates in the distributional single index model", "link": "http://arxiv.org/abs/2310.13973", "description": "The distributional single index model is a semiparametric regression model in\nwhich the conditional distribution functions $P(Y \\leq y | X = x) =\nF_0(\\theta_0(x), y)$ of a real-valued outcome variable $Y$ depend on\n$d$-dimensional covariates $X$ through a univariate, parametric index function\n$\\theta_0(x)$, and increase stochastically as $\\theta_0(x)$ increases. We\npropose least squares approaches for the joint estimation of $\\theta_0$ and\n$F_0$ in the important case where $\\theta_0(x) = \\alpha_0^{\\top}x$ and obtain\nconvergence rates of $n^{-1/3}$, thereby improving an existing result that\ngives a rate of $n^{-1/6}$. A simulation study indicates that the convergence\nrate for the estimation of $\\alpha_0$ might be faster. Furthermore, we\nillustrate our methods in a real data application that demonstrates the\nadvantages of shape restrictions in single index models."}, "http://arxiv.org/abs/2310.14068": {"title": "Unobserved Grouped Heteroskedasticity and Fixed Effects", "link": "http://arxiv.org/abs/2310.14068", "description": "This paper extends the linear grouped fixed effects (GFE) panel model to\nallow for heteroskedasticity from a discrete latent group variable. Key\nfeatures of GFE are preserved, such as individuals belonging to one of a finite\nnumber of groups and group membership is unrestricted and estimated. Ignoring\ngroup heteroskedasticity may lead to poor classification, which is detrimental\nto finite sample bias and standard errors of estimators. I introduce the\n\"weighted grouped fixed effects\" (WGFE) estimator that minimizes a weighted\naverage of group sum of squared residuals. I establish $\\sqrt{NT}$-consistency\nand normality under a concept of group separation based on second moments. A\ntest of group homoskedasticity is discussed. A fast computation procedure is\nprovided. Simulations show that WGFE outperforms alternatives that exclude\nsecond moment information. I demonstrate this approach by considering the link\nbetween income and democracy and the effect of unionization on earnings."}, "http://arxiv.org/abs/2310.14246": {"title": "Shortcuts for causal discovery of nonlinear models by score matching", "link": "http://arxiv.org/abs/2310.14246", "description": "The use of simulated data in the field of causal discovery is ubiquitous due\nto the scarcity of annotated real data. Recently, Reisach et al., 2021\nhighlighted the emergence of patterns in simulated linear data, which displays\nincreasing marginal variance in the casual direction. As an ablation in their\nexperiments, Montagna et al., 2023 found that similar patterns may emerge in\nnonlinear models for the variance of the score vector $\\nabla \\log\np_{\\mathbf{X}}$, and introduced the ScoreSort algorithm. In this work, we\nformally define and characterize this score-sortability pattern of nonlinear\nadditive noise models. We find that it defines a class of identifiable\n(bivariate) causal models overlapping with nonlinear additive noise models. We\ntheoretically demonstrate the advantages of ScoreSort in terms of statistical\nefficiency compared to prior state-of-the-art score matching-based methods and\nempirically show the score-sortability of the most common synthetic benchmarks\nin the literature. Our findings remark (1) the lack of diversity in the data as\nan important limitation in the evaluation of nonlinear causal discovery\napproaches, (2) the importance of thoroughly testing different settings within\na problem class, and (3) the importance of analyzing statistical properties in\ncausal discovery, where research is often limited to defining identifiability\nconditions of the model."}, "http://arxiv.org/abs/2310.14293": {"title": "Testing exchangeability by pairwise betting", "link": "http://arxiv.org/abs/2310.14293", "description": "In this paper, we address the problem of testing exchangeability of a\nsequence of random variables, $X_1, X_2,\\cdots$. This problem has been studied\nunder the recently popular framework of testing by betting. But the mapping of\ntesting problems to game is not one to one: many games can be designed for the\nsame test. Past work established that it is futile to play single game betting\non every observation: test martingales in the data filtration are powerless.\nTwo avenues have been explored to circumvent this impossibility: betting in a\nreduced filtration (wealth is a test martingale in a coarsened filtration), or\nplaying many games in parallel (wealth is an e-process in the data filtration).\nThe former has proved to be difficult to theoretically analyze, while the\nlatter only works for binary or discrete observation spaces. Here, we introduce\na different approach that circumvents both drawbacks. We design a new (yet\nsimple) game in which we observe the data sequence in pairs. Despite the fact\nthat betting on individual observations is futile, we show that betting on\npairs of observations is not. To elaborate, we prove that our game leads to a\nnontrivial test martingale, which is interesting because it has been obtained\nby shrinking the filtration very slightly. We show that our test controls\ntype-1 error despite continuous monitoring, and achieves power one for both\nbinary and continuous observations, under a broad class of alternatives. Due to\nthe shrunk filtration, optional stopping is only allowed at even stopping\ntimes, not at odd ones: a relatively minor price. We provide a wide array of\nsimulations that align with our theoretical findings."}, "http://arxiv.org/abs/2310.14399": {"title": "The role of randomization inference in unraveling individual treatment effects in clinical trials: Application to HIV vaccine trials", "link": "http://arxiv.org/abs/2310.14399", "description": "Randomization inference is a powerful tool in early phase vaccine trials to\nestimate the causal effect of a regimen against a placebo or another regimen.\nTraditionally, randomization-based inference often focuses on testing either\nFisher's sharp null hypothesis of no treatment effect for any unit or Neyman's\nweak null hypothesis of no sample average treatment effect. Many recent efforts\nhave explored conducting exact randomization-based inference for other\nsummaries of the treatment effect profile, for instance, quantiles of the\ntreatment effect distribution function. In this article, we systematically\nreview methods that conduct exact, randomization-based inference for quantiles\nof individual treatment effects (ITEs) and extend some results by incorporating\nauxiliary information often available in a vaccine trial. These methods are\nsuitable for four scenarios: (i) a randomized controlled trial (RCT) where the\npotential outcomes under one regimen are constant; (ii) an RCT with no\nrestriction on any potential outcomes; (iii) an RCT with some user-specified\nbounds on potential outcomes; and (iv) a matched study comparing two\nnon-randomized, possibly confounded treatment arms. We then conduct two\nextensive simulation studies, one comparing the performance of each method in\nmany practical clinical settings and the other evaluating the usefulness of the\nmethods in ranking and advancing experimental therapies. We apply these methods\nto an early-phase clinical trail, HIV Vaccine Trials Network Study 086 (HVTN\n086), to showcase the usefulness of the methods."}, "http://arxiv.org/abs/2310.14419": {"title": "An RKHS Approach for Variable Selection in High-dimensional Functional Linear Models", "link": "http://arxiv.org/abs/2310.14419", "description": "High-dimensional functional data has become increasingly prevalent in modern\napplications such as high-frequency financial data and neuroimaging data\nanalysis. We investigate a class of high-dimensional linear regression models,\nwhere each predictor is a random element in an infinite dimensional function\nspace, and the number of functional predictors p can potentially be much\ngreater than the sample size n. Assuming that each of the unknown coefficient\nfunctions belongs to some reproducing kernel Hilbert space (RKHS), we\nregularized the fitting of the model by imposing a group elastic-net type of\npenalty on the RKHS norms of the coefficient functions. We show that our loss\nfunction is Gateaux sub-differentiable, and our functional elastic-net\nestimator exists uniquely in the product RKHS. Under suitable sparsity\nassumptions and a functional version of the irrepresentible condition, we\nestablish the variable selection consistency property of our approach. The\nproposed method is illustrated through simulation studies and a real-data\napplication from the Human Connectome Project."}, "http://arxiv.org/abs/2310.14448": {"title": "Semiparametrically Efficient Score for the Survival Odds Ratio", "link": "http://arxiv.org/abs/2310.14448", "description": "We consider a general proportional odds model for survival data under binary\ntreatment, where the functional form of the covariates is left unspecified. We\nderive the efficient score for the conditional survival odds ratio given the\ncovariates using modern semiparametric theory. The efficient score may be\nuseful in the development of doubly robust estimators, although computational\nchallenges remain."}, "http://arxiv.org/abs/2310.14763": {"title": "Externally Valid Policy Evaluation Combining Trial and Observational Data", "link": "http://arxiv.org/abs/2310.14763", "description": "Randomized trials are widely considered as the gold standard for evaluating\nthe effects of decision policies. Trial data is, however, drawn from a\npopulation which may differ from the intended target population and this raises\na problem of external validity (aka. generalizability). In this paper we seek\nto use trial data to draw valid inferences about the outcome of a policy on the\ntarget population. Additional covariate data from the target population is used\nto model the sampling of individuals in the trial study. We develop a method\nthat yields certifiably valid trial-based policy evaluations under any\nspecified range of model miscalibrations. The method is nonparametric and the\nvalidity is assured even with finite samples. The certified policy evaluations\nare illustrated using both simulated and real data."}, "http://arxiv.org/abs/2310.14922": {"title": "The Complex Network Patterns of Human Migration at Different Geographical Scales: Network Science meets Regression Analysis", "link": "http://arxiv.org/abs/2310.14922", "description": "Migration's influence in shaping population dynamics in times of impending\nclimate and population crises exposes its crucial role in upholding societal\ncohesion. As migration impacts virtually all aspects of life, it continues to\nrequire attention across scientific disciplines. This study delves into two\ndistinctive substrates of Migration Studies: the \"why\" substrate, which deals\nwith identifying the factors driving migration relying primarily on regression\nmodeling, encompassing economic, demographic, geographic, cultural, political,\nand other variables; and the \"how\" substrate, which focuses on identifying\nmigration flows and patterns, drawing from Network Science tools and\nvisualization techniques to depict complex migration networks. Despite the\ngrowing percentage of Network Science studies in migration, the explanations of\nthe identified network traits remain very scarce, highlighting the detachment\nbetween the two research substrates. Our study includes real-world network\nanalyses of human migration across different geographical levels: city,\ncountry, and global. We examine inter-district migration in Vienna at the city\nlevel, review internal migration networks in Austria and Croatia at the country\nlevel, and analyze migration exchange between Croatia and the world at the\nglobal level. By comparing network structures, we demonstrate how distinct\nnetwork traits impact regression modeling. This work not only uncovers\nmigration network patterns in previously unexplored areas but also presents a\ncomprehensive overview of recent research, highlighting gaps in each field and\ntheir interconnectedness. Our contribution offers suggestions for integrating\nboth fields to enhance methodological rigor and support future research."}, "http://arxiv.org/abs/2310.14983": {"title": "Causal clustering: design of cluster experiments under network interference", "link": "http://arxiv.org/abs/2310.14983", "description": "This paper studies the design of cluster experiments to estimate the global\ntreatment effect in the presence of spillovers on a single network. We provide\nan econometric framework to choose the clustering that minimizes the worst-case\nmean-squared error of the estimated global treatment effect. We show that the\noptimal clustering can be approximated as the solution of a novel penalized\nmin-cut optimization problem computed via off-the-shelf semi-definite\nprogramming algorithms. Our analysis also characterizes easy-to-check\nconditions to choose between a cluster or individual-level randomization. We\nillustrate the method's properties using unique network data from the universe\nof Facebook's users and existing network data from a field experiment."}, "http://arxiv.org/abs/2310.15016": {"title": "Impact of Record-Linkage Errors in Covid-19 Vaccine-Safety Analyses using German Health-Care Data: A Simulation Study", "link": "http://arxiv.org/abs/2310.15016", "description": "With unprecedented speed, 192,248,678 doses of Covid-19 vaccines were\nadministered in Germany by July 11, 2023 to combat the pandemic. Limitations of\nclinical trials imply that the safety profile of these vaccines is not fully\nknown before marketing. However, routine health-care data can help address\nthese issues. Despite the high proportion of insured people, the analysis of\nvaccination-related data is challenging in Germany. Generally, the Covid-19\nvaccination status and other health-care data are stored in separate databases,\nwithout persistent and database-independent person identifiers. Error-prone\nrecord-linkage techniques must be used to merge these databases. Our aim was to\nquantify the impact of record-linkage errors on the power and bias of different\nanalysis methods designed to assess Covid-19 vaccine safety when using German\nhealth-care data with a Monte-Carlo simulation study. We used a discrete-time\nsimulation and empirical data to generate realistic data with varying amounts\nof record-linkage errors. Afterwards, we analysed this data using a Cox model\nand the self-controlled case series (SCCS) method. Realistic proportions of\nrandom linkage errors only had little effect on the power of either method. The\nSCCS method produced unbiased results even with a high percentage of linkage\nerrors, while the Cox model underestimated the true effect."}, "http://arxiv.org/abs/2310.15069": {"title": "Second-order group knockoffs with applications to GWAS", "link": "http://arxiv.org/abs/2310.15069", "description": "Conditional testing via the knockoff framework allows one to identify --\namong large number of possible explanatory variables -- those that carry unique\ninformation about an outcome of interest, and also provides a false discovery\nrate guarantee on the selection. This approach is particularly well suited to\nthe analysis of genome wide association studies (GWAS), which have the goal of\nidentifying genetic variants which influence traits of medical relevance.\n\nWhile conditional testing can be both more powerful and precise than\ntraditional GWAS analysis methods, its vanilla implementation encounters a\ndifficulty common to all multivariate analysis methods: it is challenging to\ndistinguish among multiple, highly correlated regressors. This impasse can be\novercome by shifting the object of inference from single variables to groups of\ncorrelated variables. To achieve this, it is necessary to construct \"group\nknockoffs.\" While successful examples are already documented in the literature,\nthis paper substantially expands the set of algorithms and software for group\nknockoffs. We focus in particular on second-order knockoffs, for which we\ndescribe correlation matrix approximations that are appropriate for GWAS data\nand that result in considerable computational savings. We illustrate the\neffectiveness of the proposed methods with simulations and with the analysis of\nalbuminuria data from the UK Biobank.\n\nThe described algorithms are implemented in an open-source Julia package\nKnockoffs.jl, for which both R and Python wrappers are available."}, "http://arxiv.org/abs/2310.15070": {"title": "Improving estimation efficiency of case-cohort study with interval-censored failure time data", "link": "http://arxiv.org/abs/2310.15070", "description": "The case-cohort design is a commonly used cost-effective sampling strategy\nfor large cohort studies, where some covariates are expensive to measure or\nobtain. In this paper, we consider regression analysis under a case-cohort\nstudy with interval-censored failure time data, where the failure time is only\nknown to fall within an interval instead of being exactly observed. A common\napproach to analyze data from a case-cohort study is the inverse probability\nweighting approach, where only subjects in the case-cohort sample are used in\nestimation, and the subjects are weighted based on the probability of inclusion\ninto the case-cohort sample. This approach, though consistent, is generally\ninefficient as it does not incorporate information outside the case-cohort\nsample. To improve efficiency, we first develop a sieve maximum weighted\nlikelihood estimator under the Cox model based on the case-cohort sample, and\nthen propose a procedure to update this estimator by using information in the\nfull cohort. We show that the update estimator is consistent, asymptotically\nnormal, and more efficient than the original estimator. The proposed method can\nflexibly incorporate auxiliary variables to further improve estimation\nefficiency. We employ a weighted bootstrap procedure for variance estimation.\nSimulation results indicate that the proposed method works well in practical\nsituations. A real study on diabetes is provided for illustration."}, "http://arxiv.org/abs/2310.15108": {"title": "Evaluating machine learning models in non-standard settings: An overview and new findings", "link": "http://arxiv.org/abs/2310.15108", "description": "Estimating the generalization error (GE) of machine learning models is\nfundamental, with resampling methods being the most common approach. However,\nin non-standard settings, particularly those where observations are not\nindependently and identically distributed, resampling using simple random data\ndivisions may lead to biased GE estimates. This paper strives to present\nwell-grounded guidelines for GE estimation in various such non-standard\nsettings: clustered data, spatial data, unequal sampling probabilities, concept\ndrift, and hierarchically structured outcomes. Our overview combines\nwell-established methodologies with other existing methods that, to our\nknowledge, have not been frequently considered in these particular settings. A\nunifying principle among these techniques is that the test data used in each\niteration of the resampling procedure should reflect the new observations to\nwhich the model will be applied, while the training data should be\nrepresentative of the entire data set used to obtain the final model. Beyond\nproviding an overview, we address literature gaps by conducting simulation\nstudies. These studies assess the necessity of using GE-estimation methods\ntailored to the respective setting. Our findings corroborate the concern that\nstandard resampling methods often yield biased GE estimates in non-standard\nsettings, underscoring the importance of tailored GE estimation."}, "http://arxiv.org/abs/2310.15124": {"title": "Mixed-Variable Global Sensitivity Analysis For Knowledge Discovery And Efficient Combinatorial Materials Design", "link": "http://arxiv.org/abs/2310.15124", "description": "Global Sensitivity Analysis (GSA) is the study of the influence of any given\ninputs on the outputs of a model. In the context of engineering design, GSA has\nbeen widely used to understand both individual and collective contributions of\ndesign variables on the design objectives. So far, global sensitivity studies\nhave often been limited to design spaces with only quantitative (numerical)\ndesign variables. However, many engineering systems also contain, if not only,\nqualitative (categorical) design variables in addition to quantitative design\nvariables. In this paper, we integrate Latent Variable Gaussian Process (LVGP)\nwith Sobol' analysis to develop the first metamodel-based mixed-variable GSA\nmethod. Through numerical case studies, we validate and demonstrate the\neffectiveness of our proposed method for mixed-variable problems. Furthermore,\nwhile the proposed GSA method is general enough to benefit various engineering\ndesign applications, we integrate it with multi-objective Bayesian optimization\n(BO) to create a sensitivity-aware design framework in accelerating the Pareto\nfront design exploration for metal-organic framework (MOF) materials with\nmany-level combinatorial design spaces. Although MOFs are constructed only from\nqualitative variables that are notoriously difficult to design, our method can\nutilize sensitivity analysis to navigate the optimization in the many-level\nlarge combinatorial design space, greatly expediting the exploration of novel\nMOF candidates."}, "http://arxiv.org/abs/2003.04433": {"title": "Least Squares Estimation of a Quasiconvex Regression Function", "link": "http://arxiv.org/abs/2003.04433", "description": "We develop a new approach for the estimation of a multivariate function based\non the economic axioms of quasiconvexity (and monotonicity). On the\ncomputational side, we prove the existence of the quasiconvex constrained least\nsquares estimator (LSE) and provide a characterization of the function space to\ncompute the LSE via a mixed integer quadratic programme. On the theoretical\nside, we provide finite sample risk bounds for the LSE via a sharp oracle\ninequality. Our results allow for errors to depend on the covariates and to\nhave only two finite moments. We illustrate the superior performance of the LSE\nagainst some competing estimators via simulation. Finally, we use the LSE to\nestimate the production function for the Japanese plywood industry and the cost\nfunction for hospitals across the US."}, "http://arxiv.org/abs/2004.08318": {"title": "Causal Inference under Outcome-Based Sampling with Monotonicity Assumptions", "link": "http://arxiv.org/abs/2004.08318", "description": "We study causal inference under case-control and case-population sampling.\nSpecifically, we focus on the binary-outcome and binary-treatment case, where\nthe parameters of interest are causal relative and attributable risks defined\nvia the potential outcome framework. It is shown that strong ignorability is\nnot always as powerful as it is under random sampling and that certain\nmonotonicity assumptions yield comparable results in terms of sharp identified\nintervals. Specifically, the usual odds ratio is shown to be a sharp identified\nupper bound on causal relative risk under the monotone treatment response and\nmonotone treatment selection assumptions. We offer algorithms for inference on\nthe causal parameters that are aggregated over the true population distribution\nof the covariates. We show the usefulness of our approach by studying three\nempirical examples: the benefit of attending private school for entering a\nprestigious university in Pakistan; the relationship between staying in school\nand getting involved with drug-trafficking gangs in Brazil; and the link\nbetween physicians' hours and size of the group practice in the United States."}, "http://arxiv.org/abs/2008.10296": {"title": "Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison", "link": "http://arxiv.org/abs/2008.10296", "description": "Leave-one-out cross-validation (LOO-CV) is a popular method for comparing\nBayesian models based on their estimated predictive performance on new, unseen,\ndata. As leave-one-out cross-validation is based on finite observed data, there\nis uncertainty about the expected predictive performance on new data. By\nmodeling this uncertainty when comparing two models, we can compute the\nprobability that one model has a better predictive performance than the other.\nModeling this uncertainty well is not trivial, and for example, it is known\nthat the commonly used standard error estimate is often too small. We study the\nproperties of the Bayesian LOO-CV estimator and the related uncertainty\nestimates when comparing two models. We provide new results of the properties\nboth theoretically in the linear regression case and empirically for multiple\ndifferent models and discuss the challenges of modeling the uncertainty. We\nshow that problematic cases include: comparing models with similar predictions,\nmisspecified models, and small data. In these cases, there is a weak connection\nin the skewness of the individual leave-one-out terms and the distribution of\nthe error of the Bayesian LOO-CV estimator. We show that it is possible that\nthe problematic skewness of the error distribution, which occurs when the\nmodels make similar predictions, does not fade away when the data size grows to\ninfinity in certain situations. Based on the results, we also provide practical\nrecommendations for the users of Bayesian LOO-CV for model comparison."}, "http://arxiv.org/abs/2105.04981": {"title": "Uncovering patterns for adverse pregnancy outcomes with a Bayesian spatial model: Evidence from Philadelphia", "link": "http://arxiv.org/abs/2105.04981", "description": "We introduce a Bayesian conditional autoregressive model for analyzing\npatient-specific and neighborhood risks of stillbirth and preterm birth within\na city. Our fully Bayesian approach automatically learns the amount of spatial\nheterogeneity and spatial dependence between neighborhoods. Our model provides\nmeaningful inferences and uncertainty quantification for both covariate effects\nand neighborhood risk probabilities through their posterior distributions. We\napply our methodology to data from the city of Philadelphia. Using electronic\nhealth records (45,919 deliveries at hospitals within the University of\nPennsylvania Health System) and United States Census Bureau data from 363\ncensus tracts in Philadelphia, we find that both patient-level characteristics\n(e.g. self-identified race/ethnicity) and neighborhood-level characteristics\n(e.g. violent crime) are highly associated with patients' odds of stillbirth or\npreterm birth. Our neighborhood risk analysis further reveals that census\ntracts in West Philadelphia and North Philadelphia are at highest risk of these\noutcomes. Specifically, neighborhoods with higher rates of women in poverty or\non public assistance have greater neighborhood risk for these outcomes, while\nneighborhoods with higher rates of college-educated women or women in the labor\nforce have lower risk. Our findings could be useful for targeted individual and\nneighborhood interventions."}, "http://arxiv.org/abs/2107.07317": {"title": "Nonparametric Statistical Inference via Metric Distribution Function in Metric Spaces", "link": "http://arxiv.org/abs/2107.07317", "description": "Distribution function is essential in statistical inference, and connected\nwith samples to form a directed closed loop by the correspondence theorem in\nmeasure theory and the Glivenko-Cantelli and Donsker properties. This\nconnection creates a paradigm for statistical inference. However, existing\ndistribution functions are defined in Euclidean spaces and no longer convenient\nto use in rapidly evolving data objects of complex nature. It is imperative to\ndevelop the concept of distribution function in a more general space to meet\nemerging needs. Note that the linearity allows us to use hypercubes to define\nthe distribution function in a Euclidean space, but without the linearity in a\nmetric space, we must work with the metric to investigate the probability\nmeasure. We introduce a class of metric distribution functions through the\nmetric between random objects and a fixed location in metric spaces. We\novercome this challenging step by proving the correspondence theorem and the\nGlivenko-Cantelli theorem for metric distribution functions in metric spaces\nthat lie the foundation for conducting rational statistical inference for\nmetric space-valued data. Then, we develop homogeneity test and mutual\nindependence test for non-Euclidean random objects, and present comprehensive\nempirical evidence to support the performance of our proposed methods."}, "http://arxiv.org/abs/2108.07455": {"title": "Causal Inference with Noncompliance and Unknown Interference", "link": "http://arxiv.org/abs/2108.07455", "description": "We consider a causal inference model in which individuals interact in a\nsocial network and they may not comply with the assigned treatments. In\nparticular, we suppose that the form of network interference is unknown to\nresearchers. To estimate meaningful causal parameters in this situation, we\nintroduce a new concept of exposure mapping, which summarizes potentially\ncomplicated spillover effects into a fixed dimensional statistic of\ninstrumental variables. We investigate identification conditions for the\nintention-to-treat effects and the average treatment effects for compliers,\nwhile explicitly considering the possibility of misspecification of exposure\nmapping. Based on our identification results, we develop nonparametric\nestimation procedures via inverse probability weighting. Their asymptotic\nproperties, including consistency and asymptotic normality, are investigated\nusing an approximate neighborhood interference framework. For an empirical\nillustration, we apply our method to experimental data on the anti-conflict\nintervention school program. The proposed methods are readily available with\nthe companion R package latenetwork."}, "http://arxiv.org/abs/2109.03694": {"title": "Parameterizing and Simulating from Causal Models", "link": "http://arxiv.org/abs/2109.03694", "description": "Many statistical problems in causal inference involve a probability\ndistribution other than the one from which data are actually observed; as an\nadditional complication, the object of interest is often a marginal quantity of\nthis other probability distribution. This creates many practical complications\nfor statistical inference, even where the problem is non-parametrically\nidentified. In particular, it is difficult to perform likelihood-based\ninference, or even to simulate from the model in a general way.\n\nWe introduce the `frugal parameterization', which places the causal effect of\ninterest at its centre, and then builds the rest of the model around it. We do\nthis in a way that provides a recipe for constructing a regular, non-redundant\nparameterization using causal quantities of interest. In the case of discrete\nvariables we can use odds ratios to complete the parameterization, while in the\ncontinuous case copulas are the natural choice; other possibilities are also\ndiscussed.\n\nOur methods allow us to construct and simulate from models with\nparametrically specified causal distributions, and fit them using\nlikelihood-based methods, including fully Bayesian approaches. Our proposal\nincludes parameterizations for the average causal effect and effect of\ntreatment on the treated, as well as other causal quantities of interest."}, "http://arxiv.org/abs/2112.03872": {"title": "Nonparametric Treatment Effect Identification in School Choice", "link": "http://arxiv.org/abs/2112.03872", "description": "This paper studies nonparametric identification and estimation of causal\neffects in centralized school assignment. In many centralized assignment\nsettings, students are subjected to both lottery-driven variation and\nregression discontinuity (RD) driven variation. We characterize the full set of\nidentified atomic treatment effects (aTEs), defined as the conditional average\ntreatment effect between a pair of schools, given student characteristics.\nAtomic treatment effects are the building blocks of more aggregated notions of\ntreatment contrasts, and common approaches estimating aggregations of aTEs can\nmask important heterogeneity. In particular, many aggregations of aTEs put zero\nweight on aTEs driven by RD variation, and estimators of such aggregations put\nasymptotically vanishing weight on the RD-driven aTEs. We develop a diagnostic\ntool for empirically assessing the weight put on aTEs driven by RD variation.\nLastly, we provide estimators and accompanying asymptotic results for inference\non aggregations of RD-driven aTEs."}, "http://arxiv.org/abs/2202.09534": {"title": "Locally Adaptive Spatial Quantile Smoothing: Application to Monitoring Crime Density in Tokyo", "link": "http://arxiv.org/abs/2202.09534", "description": "Spatial trend estimation under potential heterogeneity is an important\nproblem to extract spatial characteristics and hazards such as criminal\nactivity. By focusing on quantiles, which provide substantial information on\ndistributions compared with commonly used summary statistics such as means, it\nis often useful to estimate not only the average trend but also the high (low)\nrisk trend additionally. In this paper, we propose a Bayesian quantile trend\nfiltering method to estimate the non-stationary trend of quantiles on graphs\nand apply it to crime data in Tokyo between 2013 and 2017. By modeling multiple\nobservation cases, we can estimate the potential heterogeneity of spatial crime\ntrends over multiple years in the application. To induce locally adaptive\nBayesian inference on trends, we introduce general shrinkage priors for graph\ndifferences. Introducing so-called shadow priors with multivariate distribution\nfor local scale parameters and mixture representation of the asymmetric Laplace\ndistribution, we provide a simple Gibbs sampling algorithm to generate\nposterior samples. The numerical performance of the proposed method is\ndemonstrated through simulation studies."}, "http://arxiv.org/abs/2203.16710": {"title": "Detecting Treatment Interference under the K-Nearest-Neighbors Interference Model", "link": "http://arxiv.org/abs/2203.16710", "description": "We propose a model of treatment interference where the response of a unit\ndepends only on its treatment status and the statuses of units within its\nK-neighborhood. Current methods for detecting interference include carefully\ndesigned randomized experiments and conditional randomization tests on a set of\nfocal units. We give guidance on how to choose focal units under this model of\ninterference. We then conduct a simulation study to evaluate the efficacy of\nexisting methods for detecting network interference. We show that this choice\nof focal units leads to powerful tests of treatment interference which\noutperform current experimental methods."}, "http://arxiv.org/abs/2206.00646": {"title": "Importance sampling for stochastic reaction-diffusion equations in the moderate deviation regime", "link": "http://arxiv.org/abs/2206.00646", "description": "We develop a provably efficient importance sampling scheme that estimates\nexit probabilities of solutions to small-noise stochastic reaction-diffusion\nequations from scaled neighborhoods of a stable equilibrium. The moderate\ndeviation scaling allows for a local approximation of the nonlinear dynamics by\ntheir linearized version. In addition, we identify a finite-dimensional\nsubspace where exits take place with high probability. Using stochastic control\nand variational methods we show that our scheme performs well both in the zero\nnoise limit and pre-asymptotically. Simulation studies for stochastically\nperturbed bistable dynamics illustrate the theoretical results."}, "http://arxiv.org/abs/2206.12084": {"title": "Functional Mixed Membership Models", "link": "http://arxiv.org/abs/2206.12084", "description": "Mixed membership models, or partial membership models, are a flexible\nunsupervised learning method that allows each observation to belong to multiple\nclusters. In this paper, we propose a Bayesian mixed membership model for\nfunctional data. By using the multivariate Karhunen-Lo\\`eve theorem, we are\nable to derive a scalable representation of Gaussian processes that maintains\ndata-driven learning of the covariance structure. Within this framework, we\nestablish conditional posterior consistency given a known feature allocation\nmatrix. Compared to previous work on mixed membership models, our proposal\nallows for increased modeling flexibility, with the benefit of a directly\ninterpretable mean and covariance structure. Our work is motivated by studies\nin functional brain imaging through electroencephalography (EEG) of children\nwith autism spectrum disorder (ASD). In this context, our work formalizes the\nclinical notion of \"spectrum\" in terms of feature membership proportions."}, "http://arxiv.org/abs/2208.07614": {"title": "Reweighting the RCT for generalization: finite sample error and variable selection", "link": "http://arxiv.org/abs/2208.07614", "description": "Randomized Controlled Trials (RCTs) may suffer from limited scope. In\nparticular, samples may be unrepresentative: some RCTs over- or under- sample\nindividuals with certain characteristics compared to the target population, for\nwhich one wants conclusions on treatment effectiveness. Re-weighting trial\nindividuals to match the target population can improve the treatment effect\nestimation. In this work, we establish the exact expressions of the bias and\nvariance of such reweighting procedures -- also called Inverse Propensity of\nSampling Weighting (IPSW) -- in presence of categorical covariates for any\nsample size. Such results allow us to compare the theoretical performance of\ndifferent versions of IPSW estimates. Besides, our results show how the\nperformance (bias, variance, and quadratic risk) of IPSW estimates depends on\nthe two sample sizes (RCT and target population). A by-product of our work is\nthe proof of consistency of IPSW estimates. Results also reveal that IPSW\nperformances are improved when the trial probability to be treated is estimated\n(rather than using its oracle counterpart). In addition, we study choice of\nvariables: how including covariates that are not necessary for identifiability\nof the causal effect may impact the asymptotic variance. Including covariates\nthat are shifted between the two samples but not treatment effect modifiers\nincreases the variance while non-shifted but treatment effect modifiers do not.\nWe illustrate all the takeaways in a didactic example, and on a semi-synthetic\nsimulation inspired from critical care medicine."}, "http://arxiv.org/abs/2209.15448": {"title": "Blessing from Human-AI Interaction: Super Reinforcement Learning in Confounded Environments", "link": "http://arxiv.org/abs/2209.15448", "description": "As AI becomes more prevalent throughout society, effective methods of\nintegrating humans and AI systems that leverage their respective strengths and\nmitigate risk have become an important priority. In this paper, we introduce\nthe paradigm of super reinforcement learning that takes advantage of Human-AI\ninteraction for data driven sequential decision making. This approach utilizes\nthe observed action, either from AI or humans, as input for achieving a\nstronger oracle in policy learning for the decision maker (humans or AI). In\nthe decision process with unmeasured confounding, the actions taken by past\nagents can offer valuable insights into undisclosed information. By including\nthis information for the policy search in a novel and legitimate manner, the\nproposed super reinforcement learning will yield a super-policy that is\nguaranteed to outperform both the standard optimal policy and the behavior one\n(e.g., past agents' actions). We call this stronger oracle a blessing from\nhuman-AI interaction. Furthermore, to address the issue of unmeasured\nconfounding in finding super-policies using the batch data, a number of\nnonparametric and causal identifications are established. Building upon on\nthese novel identification results, we develop several super-policy learning\nalgorithms and systematically study their theoretical properties such as\nfinite-sample regret guarantee. Finally, we illustrate the effectiveness of our\nproposal through extensive simulations and real-world applications."}, "http://arxiv.org/abs/2212.06906": {"title": "Flexible Regularized Estimation in High-Dimensional Mixed Membership Models", "link": "http://arxiv.org/abs/2212.06906", "description": "Mixed membership models are an extension of finite mixture models, where each\nobservation can partially belong to more than one mixture component. A\nprobabilistic framework for mixed membership models of high-dimensional\ncontinuous data is proposed with a focus on scalability and interpretability.\nThe novel probabilistic representation of mixed membership is based on convex\ncombinations of dependent multivariate Gaussian random vectors. In this\nsetting, scalability is ensured through approximations of a tensor covariance\nstructure through multivariate eigen-approximations with adaptive\nregularization imposed through shrinkage priors. Conditional weak posterior\nconsistency is established on an unconstrained model, allowing for a simple\nposterior sampling scheme while keeping many of the desired theoretical\nproperties of our model. The model is motivated by two biomedical case studies:\na case study on functional brain imaging of children with autism spectrum\ndisorder (ASD) and a case study on gene expression data from breast cancer\ntissue. These applications highlight how the typical assumption made in cluster\nanalysis, that each observation comes from one homogeneous subgroup, may often\nbe restrictive in several applications, leading to unnatural interpretations of\ndata features."}, "http://arxiv.org/abs/2301.09020": {"title": "On the Role of Volterra Integral Equations in Self-Consistent, Product-Limit, Inverse Probability of Censoring Weighted, and Redistribution-to-the-Right Estimators for the Survival Function", "link": "http://arxiv.org/abs/2301.09020", "description": "This paper reconsiders several results of historical and current importance\nto nonparametric estimation of the survival distribution for failure in the\npresence of right-censored observation times, demonstrating in particular how\nVolterra integral equations of the first kind help inter-connect the resulting\nestimators. The paper begins by considering Efron's self-consistency equation,\nintroduced in a seminal 1967 Berkeley symposium paper. Novel insights provided\nin the current work include the observations that (i) the self-consistency\nequation leads directly to an anticipating Volterra integral equation of the\nfirst kind whose solution is given by a product-limit estimator for the\ncensoring survival function; (ii) a definition used in this argument\nimmediately establishes the familiar product-limit estimator for the failure\nsurvival function; (iii) the usual Volterra integral equation for the\nproduct-limit estimator of the failure survival function leads to an immediate\nand simple proof that it can be represented as an inverse probability of\ncensoring weighted estimator (i.e., under appropriate conditions). Finally, we\nshow that the resulting inverse probability of censoring weighted estimators,\nattributed to a highly influential 1992 paper of Robins and Rotnitzky, were\nimplicitly introduced in Efron's 1967 paper in its development of the\nredistribution-to-the-right algorithm. All results developed herein allow for\nties between failure and/or censored observations."}, "http://arxiv.org/abs/2302.01576": {"title": "ResMem: Learn what you can and memorize the rest", "link": "http://arxiv.org/abs/2302.01576", "description": "The impressive generalization performance of modern neural networks is\nattributed in part to their ability to implicitly memorize complex training\npatterns. Inspired by this, we explore a novel mechanism to improve model\ngeneralization via explicit memorization. Specifically, we propose the\nresidual-memorization (ResMem) algorithm, a new method that augments an\nexisting prediction model (e.g. a neural network) by fitting the model's\nresiduals with a $k$-nearest neighbor based regressor. The final prediction is\nthen the sum of the original model and the fitted residual regressor. By\nconstruction, ResMem can explicitly memorize the training labels. Empirically,\nwe show that ResMem consistently improves the test set generalization of the\noriginal prediction model across various standard vision and natural language\nprocessing benchmarks. Theoretically, we formulate a stylized linear regression\nproblem and rigorously show that ResMem results in a more favorable test risk\nover the base predictor."}, "http://arxiv.org/abs/2303.05032": {"title": "Sensitivity analysis for principal ignorability violation in estimating complier and noncomplier average causal effects", "link": "http://arxiv.org/abs/2303.05032", "description": "An important strategy for identifying principal causal effects, which are\noften used in settings with noncompliance, is to invoke the principal\nignorability (PI) assumption. As PI is untestable, it is important to gauge how\nsensitive effect estimates are to its violation. We focus on this task for the\ncommon one-sided noncompliance setting where there are two principal strata,\ncompliers and noncompliers. Under PI, compliers and noncompliers share the same\noutcome-mean-given-covariates function under the control condition. For\nsensitivity analysis, we allow this function to differ between compliers and\nnoncompliers in several ways, indexed by an odds ratio, a generalized odds\nratio, a mean ratio, or a standardized mean difference sensitivity parameter.\nWe tailor sensitivity analysis techniques (with any sensitivity parameter\nchoice) to several types of PI-based main analysis methods, including outcome\nregression, influence function (IF) based and weighting methods. We illustrate\nthe proposed sensitivity analyses using several outcome types from the JOBS II\nstudy. This application estimates nuisance functions parametrically -- for\nsimplicity and accessibility. In addition, we establish rate conditions on\nnonparametric nuisance estimation for IF-based estimators to be asymptotically\nnormal -- with a view to inform nonparametric inference."}, "http://arxiv.org/abs/2304.13307": {"title": "A Statistical Interpretation of the Maximum Subarray Problem", "link": "http://arxiv.org/abs/2304.13307", "description": "Maximum subarray is a classical problem in computer science that given an\narray of numbers aims to find a contiguous subarray with the largest sum. We\nfocus on its use for a noisy statistical problem of localizing an interval with\na mean different from background. While a naive application of maximum subarray\nfails at this task, both a penalized and a constrained version can succeed. We\nshow that the penalized version can be derived for common exponential family\ndistributions, in a manner similar to the change-point detection literature,\nand we interpret the resulting optimal penalty value. The failure of the naive\nformulation is then explained by an analysis of the estimated interval\nboundaries. Experiments further quantify the effect of deviating from the\noptimal penalty. We also relate the penalized and constrained formulations and\nshow that the solutions to the former lie on the convex hull of the solutions\nto the latter."}, "http://arxiv.org/abs/2305.10637": {"title": "Conformalized matrix completion", "link": "http://arxiv.org/abs/2305.10637", "description": "Matrix completion aims to estimate missing entries in a data matrix, using\nthe assumption of a low-complexity structure (e.g., low rank) so that\nimputation is possible. While many effective estimation algorithms exist in the\nliterature, uncertainty quantification for this problem has proved to be\nchallenging, and existing methods are extremely sensitive to model\nmisspecification. In this work, we propose a distribution-free method for\npredictive inference in the matrix completion problem. Our method adapts the\nframework of conformal prediction, which provides confidence intervals with\nguaranteed distribution-free validity in the setting of regression, to the\nproblem of matrix completion. Our resulting method, conformalized matrix\ncompletion (cmc), offers provable predictive coverage regardless of the\naccuracy of the low-rank model. Empirical results on simulated and real data\ndemonstrate that cmc is robust to model misspecification while matching the\nperformance of existing model-based methods when the model is correct."}, "http://arxiv.org/abs/2305.15027": {"title": "A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods", "link": "http://arxiv.org/abs/2305.15027", "description": "We establish the first mathematically rigorous link between Bayesian,\nvariational Bayesian, and ensemble methods. A key step towards this it to\nreformulate the non-convex optimisation problem typically encountered in deep\nlearning as a convex optimisation in the space of probability measures. On a\ntechnical level, our contribution amounts to studying generalised variational\ninference through the lense of Wasserstein gradient flows. The result is a\nunified theory of various seemingly disconnected approaches that are commonly\nused for uncertainty quantification in deep learning -- including deep\nensembles and (variational) Bayesian methods. This offers a fresh perspective\non the reasons behind the success of deep ensembles over procedures based on\nparameterised variational inference, and allows the derivation of new\nensembling schemes with convergence guarantees. We showcase this by proposing a\nfamily of interacting deep ensembles with direct parallels to the interactions\nof particle systems in thermodynamics, and use our theory to prove the\nconvergence of these algorithms to a well-defined global minimiser on the space\nof probability measures."}, "http://arxiv.org/abs/2306.02584": {"title": "Synthetic Regressing Control Method", "link": "http://arxiv.org/abs/2306.02584", "description": "Estimating weights in the synthetic control method, typically resulting in\nsparse weights where only a few control units have non-zero weights, involves\nan optimization procedure that simultaneously selects and aligns control units\nto closely match the treated unit. However, this simultaneous selection and\nalignment of control units may lead to a loss of efficiency. Another concern\narising from the aforementioned procedure is its susceptibility to\nunder-fitting due to imperfect pre-treatment fit. It is not uncommon for the\nlinear combination, using nonnegative weights, of pre-treatment period outcomes\nfor the control units to inadequately approximate the pre-treatment outcomes\nfor the treated unit. To address both of these issues, this paper proposes a\nsimple and effective method called Synthetic Regressing Control (SRC). The SRC\nmethod begins by performing the univariate linear regression to appropriately\nalign the pre-treatment periods of the control units with the treated unit.\nSubsequently, a SRC estimator is obtained by synthesizing (taking a weighted\naverage) the fitted controls. To determine the weights in the synthesis\nprocedure, we propose an approach that utilizes a criterion of unbiased risk\nestimator. Theoretically, we show that the synthesis way is asymptotically\noptimal in the sense of achieving the lowest possible squared error. Extensive\nnumerical experiments highlight the advantages of the SRC method."}, "http://arxiv.org/abs/2308.05858": {"title": "Inconsistency and Acausality of Model Selection in Bayesian Inverse Problems", "link": "http://arxiv.org/abs/2308.05858", "description": "Bayesian inference paradigms are regarded as powerful tools for solution of\ninverse problems. However, when applied to inverse problems in physical\nsciences, Bayesian formulations suffer from a number of inconsistencies that\nare often overlooked. A well known, but mostly neglected, difficulty is\nconnected to the notion of conditional probability densities. Borel, and later\nKolmogorov's (1933/1956), found that the traditional definition of conditional\ndensities is incomplete: In different parameterizations it leads to different\nresults. We will show an example where two apparently correct procedures\napplied to the same problem lead to two widely different results. Another type\nof inconsistency involves violation of causality. This problem is found in\nmodel selection strategies in Bayesian inversion, such as Hierarchical Bayes\nand Trans-Dimensional Inversion where so-called hyperparameters are included as\nvariables to control either the number (or type) of unknowns, or the prior\nuncertainties on data or model parameters. For Hierarchical Bayes we\ndemonstrate that the calculated 'prior' distributions of data or model\nparameters are not prior-, but posterior information. In fact, the calculated\n'standard deviations' of the data are a measure of the inability of the forward\nfunction to model the data, rather than uncertainties of the data. For\ntrans-dimensional inverse problems we show that the so-called evidence is, in\nfact, not a measure of the success of fitting the data for the given choice (or\nnumber) of parameters, as often claimed. We also find that the notion of\nNatural Parsimony is ill-defined, because of its dependence on the parameter\nprior. Based on this study, we find that careful rethinking of Bayesian\ninversion practices is required, with special emphasis on ways of avoiding the\nBorel-Kolmogorov inconsistency, and on the way we interpret model selection\nresults."}, "http://arxiv.org/abs/2308.12470": {"title": "Scalable Estimation of Multinomial Response Models with Uncertain Consideration Sets", "link": "http://arxiv.org/abs/2308.12470", "description": "A standard assumption in the fitting of unordered multinomial response models\nfor $J$ mutually exclusive nominal categories, on cross-sectional or\nlongitudinal data, is that the responses arise from the same set of $J$\ncategories between subjects. However, when responses measure a choice made by\nthe subject, it is more appropriate to assume that the distribution of\nmultinomial responses is conditioned on a subject-specific consideration set,\nwhere this consideration set is drawn from the power set of $\\{1,2,\\ldots,J\\}$.\nBecause the cardinality of this power set is exponential in $J$, estimation is\ninfeasible in general. In this paper, we provide an approach to overcoming this\nproblem. A key step in the approach is a probability model over consideration\nsets, based on a general representation of probability distributions on\ncontingency tables, which results in mixtures of independent consideration\nmodels. Although the support of this distribution is exponentially large, the\nposterior distribution over consideration sets given parameters is typically\nsparse, and is easily sampled in an MCMC scheme. We show posterior consistency\nof the parameters of the conditional response model and the distribution of\nconsideration sets. The effectiveness of the methodology is documented in\nsimulated longitudinal data sets with $J=100$ categories and real data from the\ncereal market with $J=68$ brands."}, "http://arxiv.org/abs/2310.15266": {"title": "Causal progress with imperfect placebo treatments and outcomes", "link": "http://arxiv.org/abs/2310.15266", "description": "In the quest to make defensible causal claims from observational data, it is\nsometimes possible to leverage information from \"placebo treatments\" and\n\"placebo outcomes\" (or \"negative outcome controls\"). Existing approaches\nemploying such information focus largely on point identification and assume (i)\n\"perfect placebos\", meaning placebo treatments have precisely zero effect on\nthe outcome and the real treatment has precisely zero effect on a placebo\noutcome; and (ii) \"equiconfounding\", meaning that the treatment-outcome\nrelationship where one is a placebo suffers the same amount of confounding as\ndoes the real treatment-outcome relationship, on some scale. We instead\nconsider an omitted variable bias framework, in which users can postulate\nnon-zero effects of placebo treatment on real outcomes or of real treatments on\nplacebo outcomes, and the relative strengths of confounding suffered by a\nplacebo treatment/outcome compared to the true treatment-outcome relationship.\nOnce postulated, these assumptions identify or bound the linear estimates of\ntreatment effects. While applicable in many settings, one ubiquitous use-case\nfor this approach is to employ pre-treatment outcomes as (perfect) placebo\noutcomes. In this setting, the parallel trends assumption of\ndifference-in-difference is in fact a strict equiconfounding assumption on a\nparticular scale, which can be relaxed in our framework. Finally, we\ndemonstrate the use of our framework with two applications, employing an R\npackage that implements these approaches."}, "http://arxiv.org/abs/2310.15333": {"title": "Estimating Trustworthy and Safe Optimal Treatment Regimes", "link": "http://arxiv.org/abs/2310.15333", "description": "Recent statistical and reinforcement learning methods have significantly\nadvanced patient care strategies. However, these approaches face substantial\nchallenges in high-stakes contexts, including missing data, inherent\nstochasticity, and the critical requirements for interpretability and patient\nsafety. Our work operationalizes a safe and interpretable framework to identify\noptimal treatment regimes. This approach involves matching patients with\nsimilar medical and pharmacological characteristics, allowing us to construct\nan optimal policy via interpolation. We perform a comprehensive simulation\nstudy to demonstrate the framework's ability to identify optimal policies even\nin complex settings. Ultimately, we operationalize our approach to study\nregimes for treating seizures in critically ill patients. Our findings strongly\nsupport personalized treatment strategies based on a patient's medical history\nand pharmacological features. Notably, we identify that reducing medication\ndoses for patients with mild and brief seizure episodes while adopting\naggressive treatment for patients in intensive care unit experiencing intense\nseizures leads to more favorable outcomes."}, "http://arxiv.org/abs/2310.15459": {"title": "Strategies to mitigate bias from time recording errors in pharmacokinetic studies", "link": "http://arxiv.org/abs/2310.15459", "description": "Opportunistic pharmacokinetic (PK) studies have sparse and imbalanced\nclinical measurement data, and the impact of sample time errors is an important\nconcern when seeking accurate estimates of treatment response. We evaluated an\napproximate Bayesian model for individualized pharmacokinetics in the presence\nof time recording errors (TREs), considering both a short and long infusion\ndosing pattern. We found that the long infusion schedule generally had lower\nbias in estimates of the pharmacodynamic (PD) endpoint relative to the short\ninfusion schedule. We investigated three different design strategies for their\nability to mitigate the impact of TREs: (i) shifting blood draws taken during\nan active infusion to the post-infusion period, (ii) identifying the best next\nsample time by minimizing bias in the presence of TREs, and (iii) collecting\nadditional information on a subset of patients based on estimate uncertainty or\nquadrature-estimated variance in the presence of TREs. Generally, the proposed\nstrategies led to a decrease in bias of the PD estimate for the short infusion\nschedule, but had a negligible impact for the long infusion schedule. Dosing\nregimens with periods of high non-linearity may benefit from design\nmodifications, while more stable concentration-time profiles are generally more\nrobust to TREs with no design modifications."}, "http://arxiv.org/abs/2310.15497": {"title": "Generalized Box-Cox method to estimate sample mean and standard deviation for Meta-analysis", "link": "http://arxiv.org/abs/2310.15497", "description": "Meta-analysis is the aggregation of data from multiple studies to find\npatterns across a broad range relating to a particular subject. It is becoming\nincreasingly useful to apply meta-analysis to summarize these studies being\ndone across various fields. In meta-analysis, it is common to use the mean and\nstandard deviation from each study to compare for analysis. While many studies\nreported mean and standard deviation for their summary statistics, some report\nother values including the minimum, maximum, median, and first and third\nquantiles. Often, the quantiles and median are reported when the data is skewed\nand does not follow a normal distribution. In order to correctly summarize the\ndata and draw conclusions from multiple studies, it is necessary to estimate\nthe mean and standard deviation from each study, considering variation and\nskewness within each study. In past literature, methods have been proposed to\nestimate the mean and standard deviation, but do not consider negative values.\nData that include negative values are common and would increase the accuracy\nand impact of the me-ta-analysis. We propose a method that implements a\ngeneralized Box-Cox transformation to estimate the mean and standard deviation\naccounting for such negative values while maintaining similar accuracy."}, "http://arxiv.org/abs/2310.15877": {"title": "Regression analysis of multiplicative hazards model with time-dependent coefficient for sparse longitudinal covariates", "link": "http://arxiv.org/abs/2310.15877", "description": "We study the multiplicative hazards model with intermittently observed\nlongitudinal covariates and time-varying coefficients. For such models, the\nexisting {\\it ad hoc} approach, such as the last value carried forward, is\nbiased. We propose a kernel weighting approach to get an unbiased estimation of\nthe non-parametric coefficient function and establish asymptotic normality for\nany fixed time point. Furthermore, we construct the simultaneous confidence\nband to examine the overall magnitude of the variation. Simulation studies\nsupport our theoretical predictions and show favorable performance of the\nproposed method. A data set from cerebral infarction is used to illustrate our\nmethodology."}, "http://arxiv.org/abs/2310.15956": {"title": "Likelihood-Based Inference for Semi-Parametric Transformation Cure Models with Interval Censored Data", "link": "http://arxiv.org/abs/2310.15956", "description": "A simple yet effective way of modeling survival data with cure fraction is by\nconsidering Box-Cox transformation cure model (BCTM) that unifies mixture and\npromotion time cure models. In this article, we numerically study the\nstatistical properties of the BCTM when applied to interval censored data.\nTime-to-events associated with susceptible subjects are modeled through\nproportional hazards structure that allows for non-homogeneity across subjects,\nwhere the baseline hazard function is estimated by distribution-free piecewise\nlinear function with varied degrees of non-parametricity. Due to missing cured\nstatuses for right censored subjects, maximum likelihood estimates of model\nparameters are obtained by developing an expectation-maximization (EM)\nalgorithm. Under the EM framework, the conditional expectation of the complete\ndata log-likelihood function is maximized by considering all parameters\n(including the Box-Cox transformation parameter $\\alpha$) simultaneously, in\ncontrast to conventional profile-likelihood technique of estimating $\\alpha$.\nThe robustness and accuracy of the model and estimation method are established\nthrough a detailed simulation study under various parameter settings, and an\nanalysis of real-life data obtained from a smoking cessation study."}, "http://arxiv.org/abs/1901.04916": {"title": "Pairwise accelerated failure time regression models for infectious disease transmission in close-contact groups with external sources of infection", "link": "http://arxiv.org/abs/1901.04916", "description": "Many important questions in infectious disease epidemiology involve the\neffects of covariates (e.g., age or vaccination status) on infectiousness and\nsusceptibility, which can be measured in studies of transmission in households\nor other close-contact groups. Because the transmission of disease produces\ndependent outcomes, these questions are difficult or impossible to address\nusing standard regression models from biostatistics. Pairwise survival analysis\nhandles dependent outcomes by calculating likelihoods in terms of contact\ninterval distributions in ordered pairs of individuals. The contact interval in\nthe ordered pair ij is the time from the onset of infectiousness in i to\ninfectious contact from i to j, where an infectious contact is sufficient to\ninfect j if they are susceptible. Here, we introduce a pairwise accelerated\nfailure time regression model for infectious disease transmission that allows\nthe rate parameter of the contact interval distribution to depend on\ninfectiousness covariates for i, susceptibility covariates for j, and pairwise\ncovariates. This model can simultaneously handle internal infections (caused by\ntransmission between individuals under observation) and external infections\n(caused by environmental or community sources of infection). In a simulation\nstudy, we show that these models produce valid point and interval estimates of\nparameters governing the contact interval distributions. We also explore the\nrole of epidemiologic study design and the consequences of model\nmisspecification. We use this regression model to analyze household data from\nLos Angeles County during the 2009 influenza A (H1N1) pandemic, where we find\nthat the ability to account for external sources of infection is critical to\nestimating the effect of antiviral prophylaxis."}, "http://arxiv.org/abs/2003.06416": {"title": "VCBART: Bayesian trees for varying coefficients", "link": "http://arxiv.org/abs/2003.06416", "description": "The linear varying coefficient models posits a linear relationship between an\noutcome and covariates in which the covariate effects are modeled as functions\nof additional effect modifiers. Despite a long history of study and use in\nstatistics and econometrics, state-of-the-art varying coefficient modeling\nmethods cannot accommodate multivariate effect modifiers without imposing\nrestrictive functional form assumptions or involving computationally intensive\nhyperparameter tuning. In response, we introduce VCBART, which flexibly\nestimates the covariate effect in a varying coefficient model using Bayesian\nAdditive Regression Trees. With simple default settings, VCBART outperforms\nexisting varying coefficient methods in terms of covariate effect estimation,\nuncertainty quantification, and outcome prediction. We illustrate the utility\nof VCBART with two case studies: one examining how the association between\nlater-life cognition and measures of socioeconomic position vary with respect\nto age and socio-demographics and another estimating how temporal trends in\nurban crime vary at the neighborhood level. An R package implementing VCBART is\navailable at https://github.com/skdeshpande91/VCBART"}, "http://arxiv.org/abs/2204.05870": {"title": "How much of the past matters? Using dynamic survival models for the monitoring of potassium in heart failure patients using electronic health records", "link": "http://arxiv.org/abs/2204.05870", "description": "Statistical methods to study the association between a longitudinal biomarker\nand the risk of death are very relevant for the long-term care of subjects\naffected by chronic illnesses, such as potassium in heart failure patients.\nParticularly in the presence of comorbidities or pharmacological treatments,\nsudden crises can cause potassium to undergo very abrupt yet transient changes.\nIn the context of the monitoring of potassium, there is a need for a dynamic\nmodel that can be used in clinical practice to assess the risk of death related\nto an observed patient's potassium trajectory. We considered different dynamic\nsurvival approaches, starting from the simple approach considering the most\nrecent measurement, to the joint model. We then propose a novel method based on\nwavelet filtering and landmarking to retrieve the prognostic role of past\nshort-term potassium shifts. We argue that while taking into account past\ninformation is important, not all past information is equally informative.\nState-of-the-art dynamic survival models are prone to give more importance to\nthe mean long-term value of potassium. However, our findings suggest that it is\nessential to take into account also recent potassium instability to capture all\nthe relevant prognostic information. The data used comes from over 2000\nsubjects, with a total of over 80 000 repeated potassium measurements collected\nthrough Administrative Health Records and Outpatient and Inpatient Clinic\nE-charts. A novel dynamic survival approach is proposed in this work for the\nmonitoring of potassium in heart failure. The proposed wavelet landmark method\nshows promising results revealing the prognostic role of past short-term\nchanges, according to their different duration, and achieving higher\nperformances in predicting the survival probability of individuals."}, "http://arxiv.org/abs/2212.09494": {"title": "Optimal Treatment Regimes for Proximal Causal Learning", "link": "http://arxiv.org/abs/2212.09494", "description": "A common concern when a policymaker draws causal inferences from and makes\ndecisions based on observational data is that the measured covariates are\ninsufficiently rich to account for all sources of confounding, i.e., the\nstandard no confoundedness assumption fails to hold. The recently proposed\nproximal causal inference framework shows that proxy variables that abound in\nreal-life scenarios can be leveraged to identify causal effects and therefore\nfacilitate decision-making. Building upon this line of work, we propose a novel\noptimal individualized treatment regime based on so-called outcome and\ntreatment confounding bridges. We then show that the value function of this new\noptimal treatment regime is superior to that of existing ones in the\nliterature. Theoretical guarantees, including identification, superiority,\nexcess value bound, and consistency of the estimated regime, are established.\nFurthermore, we demonstrate the proposed optimal regime via numerical\nexperiments and a real data application."}, "http://arxiv.org/abs/2301.09016": {"title": "Inference for Two-stage Experiments under Covariate-Adaptive Randomization", "link": "http://arxiv.org/abs/2301.09016", "description": "This paper studies inference in two-stage randomized experiments under\ncovariate-adaptive randomization. In the initial stage of this experimental\ndesign, clusters (e.g., households, schools, or graph partitions) are\nstratified and randomly assigned to control or treatment groups based on\ncluster-level covariates. Subsequently, an independent second-stage design is\ncarried out, wherein units within each treated cluster are further stratified\nand randomly assigned to either control or treatment groups, based on\nindividual-level covariates. Under the homogeneous partial interference\nassumption, I establish conditions under which the proposed\ndifference-in-\"average of averages\" estimators are consistent and\nasymptotically normal for the corresponding average primary and spillover\neffects and develop consistent estimators of their asymptotic variances.\nCombining these results establishes the asymptotic validity of tests based on\nthese estimators. My findings suggest that ignoring covariate information in\nthe design stage can result in efficiency loss, and commonly used inference\nmethods that ignore or improperly use covariate information can lead to either\nconservative or invalid inference. Finally, I apply these results to studying\noptimal use of covariate information under covariate-adaptive randomization in\nlarge samples, and demonstrate that a specific generalized matched-pair design\nachieves minimum asymptotic variance for each proposed estimator. The practical\nrelevance of the theoretical results is illustrated through a simulation study\nand an empirical application."}, "http://arxiv.org/abs/2302.07294": {"title": "Derandomized Novelty Detection with FDR Control via Conformal E-values", "link": "http://arxiv.org/abs/2302.07294", "description": "Conformal inference provides a general distribution-free method to rigorously\ncalibrate the output of any machine learning algorithm for novelty detection.\nWhile this approach has many strengths, it has the limitation of being\nrandomized, in the sense that it may lead to different results when analyzing\ntwice the same data, and this can hinder the interpretation of any findings. We\npropose to make conformal inferences more stable by leveraging suitable\nconformal e-values instead of p-values to quantify statistical significance.\nThis solution allows the evidence gathered from multiple analyses of the same\ndata to be aggregated effectively while provably controlling the false\ndiscovery rate. Further, we show that the proposed method can reduce randomness\nwithout much loss of power compared to standard conformal inference, partly\nthanks to an innovative way of weighting conformal e-values based on additional\nside information carefully extracted from the same data. Simulations with\nsynthetic and real data confirm this solution can be effective at eliminating\nrandom noise in the inferences obtained with state-of-the-art alternative\ntechniques, sometimes also leading to higher power."}, "http://arxiv.org/abs/2304.02127": {"title": "A Bayesian Collocation Integral Method for Parameter Estimation in Ordinary Differential Equations", "link": "http://arxiv.org/abs/2304.02127", "description": "Inferring the parameters of ordinary differential equations (ODEs) from noisy\nobservations is an important problem in many scientific fields. Currently, most\nparameter estimation methods that bypass numerical integration tend to rely on\nbasis functions or Gaussian processes to approximate the ODE solution and its\nderivatives. Due to the sensitivity of the ODE solution to its derivatives,\nthese methods can be hindered by estimation error, especially when only sparse\ntime-course observations are available. We present a Bayesian collocation\nframework that operates on the integrated form of the ODEs and also avoids the\nexpensive use of numerical solvers. Our methodology has the capability to\nhandle general nonlinear ODE systems. We demonstrate the accuracy of the\nproposed method through simulation studies, where the estimated parameters and\nrecovered system trajectories are compared with other recent methods. A real\ndata example is also provided."}, "http://arxiv.org/abs/2307.00127": {"title": "Large-scale Bayesian Structure Learning for Gaussian Graphical Models using Marginal Pseudo-likelihood", "link": "http://arxiv.org/abs/2307.00127", "description": "Bayesian methods for learning Gaussian graphical models offer a robust\nframework that addresses model uncertainty and incorporates prior knowledge.\nDespite their theoretical strengths, the applicability of Bayesian methods is\noften constrained by computational needs, especially in modern contexts\ninvolving thousands of variables. To overcome this issue, we introduce two\nnovel Markov chain Monte Carlo (MCMC) search algorithms that have a\nsignificantly lower computational cost than leading Bayesian approaches. Our\nproposed MCMC-based search algorithms use the marginal pseudo-likelihood\napproach to bypass the complexities of computing intractable normalizing\nconstants and iterative precision matrix sampling. These algorithms can deliver\nreliable results in mere minutes on standard computers, even for large-scale\nproblems with one thousand variables. Furthermore, our proposed method is\ncapable of addressing model uncertainty by efficiently exploring the full\nposterior graph space. Our simulation study indicates that the proposed\nalgorithms, particularly for large-scale sparse graphs, outperform the leading\nBayesian approaches in terms of computational efficiency and precision. The\nimplementation supporting the new approach is available through the R package\nBDgraph."}, "http://arxiv.org/abs/2307.09302": {"title": "Conformal prediction under ambiguous ground truth", "link": "http://arxiv.org/abs/2307.09302", "description": "Conformal Prediction (CP) allows to perform rigorous uncertainty\nquantification by constructing a prediction set $C(X)$ satisfying $\\mathbb{P}(Y\n\\in C(X))\\geq 1-\\alpha$ for a user-chosen $\\alpha \\in [0,1]$ by relying on\ncalibration data $(X_1,Y_1),...,(X_n,Y_n)$ from $\\mathbb{P}=\\mathbb{P}^{X}\n\\otimes \\mathbb{P}^{Y|X}$. It is typically implicitly assumed that\n$\\mathbb{P}^{Y|X}$ is the \"true\" posterior label distribution. However, in many\nreal-world scenarios, the labels $Y_1,...,Y_n$ are obtained by aggregating\nexpert opinions using a voting procedure, resulting in a one-hot distribution\n$\\mathbb{P}_{vote}^{Y|X}$. For such ``voted'' labels, CP guarantees are thus\nw.r.t. $\\mathbb{P}_{vote}=\\mathbb{P}^X \\otimes \\mathbb{P}_{vote}^{Y|X}$ rather\nthan the true distribution $\\mathbb{P}$. In cases with unambiguous ground truth\nlabels, the distinction between $\\mathbb{P}_{vote}$ and $\\mathbb{P}$ is\nirrelevant. However, when experts do not agree because of ambiguous labels,\napproximating $\\mathbb{P}^{Y|X}$ with a one-hot distribution\n$\\mathbb{P}_{vote}^{Y|X}$ ignores this uncertainty. In this paper, we propose\nto leverage expert opinions to approximate $\\mathbb{P}^{Y|X}$ using a\nnon-degenerate distribution $\\mathbb{P}_{agg}^{Y|X}$. We develop Monte Carlo CP\nprocedures which provide guarantees w.r.t. $\\mathbb{P}_{agg}=\\mathbb{P}^X\n\\otimes \\mathbb{P}_{agg}^{Y|X}$ by sampling multiple synthetic pseudo-labels\nfrom $\\mathbb{P}_{agg}^{Y|X}$ for each calibration example $X_1,...,X_n$. In a\ncase study of skin condition classification with significant disagreement among\nexpert annotators, we show that applying CP w.r.t. $\\mathbb{P}_{vote}$\nunder-covers expert annotations: calibrated for $72\\%$ coverage, it falls short\nby on average $10\\%$; our Monte Carlo CP closes this gap both empirically and\ntheoretically."}, "http://arxiv.org/abs/2310.16203": {"title": "Multivariate Dynamic Mediation Analysis under a Reinforcement Learning Framework", "link": "http://arxiv.org/abs/2310.16203", "description": "Mediation analysis is an important analytic tool commonly used in a broad\nrange of scientific applications. In this article, we study the problem of\nmediation analysis when there are multivariate and conditionally dependent\nmediators, and when the variables are observed over multiple time points. The\nproblem is challenging, because the effect of a mediator involves not only the\npath from the treatment to this mediator itself at the current time point, but\nalso all possible paths pointed to this mediator from its upstream mediators,\nas well as the carryover effects from all previous time points. We propose a\nnovel multivariate dynamic mediation analysis approach. Drawing inspiration\nfrom the Markov decision process model that is frequently employed in\nreinforcement learning, we introduce a Markov mediation process paired with a\nsystem of time-varying linear structural equation models to formulate the\nproblem. We then formally define the individual mediation effect, built upon\nthe idea of simultaneous interventions and intervention calculus. We next\nderive the closed-form expression and propose an iterative estimation procedure\nunder the Markov mediation process model. We study both the asymptotic property\nand the empirical performance of the proposed estimator, and further illustrate\nour method with a mobile health application."}, "http://arxiv.org/abs/2310.16207": {"title": "Propensity score weighting plus an adjusted proportional hazards model does not equal doubly robust away from the null", "link": "http://arxiv.org/abs/2310.16207", "description": "Recently it has become common for applied works to combine commonly used\nsurvival analysis modeling methods, such as the multivariable Cox model, and\npropensity score weighting with the intention of forming a doubly robust\nestimator that is unbiased in large samples when either the Cox model or the\npropensity score model is correctly specified. This combination does not, in\ngeneral, produce a doubly robust estimator, even after regression\nstandardization, when there is truly a causal effect. We demonstrate via\nsimulation this lack of double robustness for the semiparametric Cox model, the\nWeibull proportional hazards model, and a simple proportional hazards flexible\nparametric model, with both the latter models fit via maximum likelihood. We\nprovide a novel proof that the combination of propensity score weighting and a\nproportional hazards survival model, fit either via full or partial likelihood,\nis consistent under the null of no causal effect of the exposure on the outcome\nunder particular censoring mechanisms if either the propensity score or the\noutcome model is correctly specified and contains all confounders. Given our\nresults suggesting that double robustness only exists under the null, we\noutline two simple alternative estimators that are doubly robust for the\nsurvival difference at a given time point (in the above sense), provided the\ncensoring mechanism can be correctly modeled, and one doubly robust method of\nestimation for the full survival curve. We provide R code to use these\nestimators for estimation and inference in the supplementary materials."}, "http://arxiv.org/abs/2310.16213": {"title": "Bayes factor functions", "link": "http://arxiv.org/abs/2310.16213", "description": "We describe Bayes factors functions based on z, t, $\\chi^2$, and F statistics\nand the prior distributions used to define alternative hypotheses. The\nnon-local alternative prior distributions are centered on standardized effects,\nwhich index the Bayes factor function. The prior densities include a dispersion\nparameter that models the variation of effect sizes across replicated\nexperiments. We examine the convergence rates of Bayes factor functions under\ntrue null and true alternative hypotheses. Several examples illustrate the\napplication of the Bayes factor functions to replicated experimental designs\nand compare the conclusions from these analyses to other default Bayes factor\nmethods."}, "http://arxiv.org/abs/2310.16256": {"title": "A Causal Disentangled Multi-Granularity Graph Classification Method", "link": "http://arxiv.org/abs/2310.16256", "description": "Graph data widely exists in real life, with large amounts of data and complex\nstructures. It is necessary to map graph data to low-dimensional embedding.\nGraph classification, a critical graph task, mainly relies on identifying the\nimportant substructures within the graph. At present, some graph classification\nmethods do not combine the multi-granularity characteristics of graph data.\nThis lack of granularity distinction in modeling leads to a conflation of key\ninformation and false correlations within the model. So, achieving the desired\ngoal of a credible and interpretable model becomes challenging. This paper\nproposes a causal disentangled multi-granularity graph representation learning\nmethod (CDM-GNN) to solve this challenge. The CDM-GNN model disentangles the\nimportant substructures and bias parts within the graph from a\nmulti-granularity perspective. The disentanglement of the CDM-GNN model reveals\nimportant and bias parts, forming the foundation for its classification task,\nspecifically, model interpretations. The CDM-GNN model exhibits strong\nclassification performance and generates explanatory outcomes aligning with\nhuman cognitive patterns. In order to verify the effectiveness of the model,\nthis paper compares the three real-world datasets MUTAG, PTC, and IMDM-M. Six\nstate-of-the-art models, namely GCN, GAT, Top-k, ASAPool, SUGAR, and SAT are\nemployed for comparison purposes. Additionally, a qualitative analysis of the\ninterpretation results is conducted."}, "http://arxiv.org/abs/2310.16260": {"title": "Private Estimation and Inference in High-Dimensional Regression with FDR Control", "link": "http://arxiv.org/abs/2310.16260", "description": "This paper presents novel methodologies for conducting practical\ndifferentially private (DP) estimation and inference in high-dimensional linear\nregression. We start by proposing a differentially private Bayesian Information\nCriterion (BIC) for selecting the unknown sparsity parameter in DP-Lasso,\neliminating the need for prior knowledge of model sparsity, a requisite in the\nexisting literature. Then we propose a differentially private debiased LASSO\nalgorithm that enables privacy-preserving inference on regression parameters.\nOur proposed method enables accurate and private inference on the regression\nparameters by leveraging the inherent sparsity of high-dimensional linear\nregression models. Additionally, we address the issue of multiple testing in\nhigh-dimensional linear regression by introducing a differentially private\nmultiple testing procedure that controls the false discovery rate (FDR). This\nallows for accurate and privacy-preserving identification of significant\npredictors in the regression model. Through extensive simulations and real data\nanalysis, we demonstrate the efficacy of our proposed methods in conducting\ninference for high-dimensional linear models while safeguarding privacy and\ncontrolling the FDR."}, "http://arxiv.org/abs/2310.16284": {"title": "Bayesian Image Mediation Analysis", "link": "http://arxiv.org/abs/2310.16284", "description": "Mediation analysis aims to separate the indirect effect through mediators\nfrom the direct effect of the exposure on the outcome. It is challenging to\nperform mediation analysis with neuroimaging data which involves high\ndimensionality, complex spatial correlations, sparse activation patterns and\nrelatively low signal-to-noise ratio. To address these issues, we develop a new\nspatially varying coefficient structural equation model for Bayesian Image\nMediation Analysis (BIMA). We define spatially varying mediation effects within\nthe potential outcome framework, employing the soft-thresholded Gaussian\nprocess prior for functional parameters. We establish the posterior consistency\nfor spatially varying mediation effects along with selection consistency on\nimportant regions that contribute to the mediation effects. We develop an\nefficient posterior computation algorithm scalable to analysis of large-scale\nimaging data. Through extensive simulations, we show that BIMA can improve the\nestimation accuracy and computational efficiency for high-dimensional mediation\nanalysis over the existing methods. We apply BIMA to analyze the behavioral and\nfMRI data in the Adolescent Brain Cognitive Development (ABCD) study with a\nfocus on inferring the mediation effects of the parental education level on the\nchildren's general cognitive ability that are mediated through the working\nmemory brain activities."}, "http://arxiv.org/abs/2310.16290": {"title": "Fair Adaptive Experiments", "link": "http://arxiv.org/abs/2310.16290", "description": "Randomized experiments have been the gold standard for assessing the\neffectiveness of a treatment or policy. The classical complete randomization\napproach assigns treatments based on a prespecified probability and may lead to\ninefficient use of data. Adaptive experiments improve upon complete\nrandomization by sequentially learning and updating treatment assignment\nprobabilities. However, their application can also raise fairness and equity\nconcerns, as assignment probabilities may vary drastically across groups of\nparticipants. Furthermore, when treatment is expected to be extremely\nbeneficial to certain groups of participants, it is more appropriate to expose\nmany of these participants to favorable treatment. In response to these\nchallenges, we propose a fair adaptive experiment strategy that simultaneously\nenhances data use efficiency, achieves an envy-free treatment assignment\nguarantee, and improves the overall welfare of participants. An important\nfeature of our proposed strategy is that we do not impose parametric modeling\nassumptions on the outcome variables, making it more versatile and applicable\nto a wider array of applications. Through our theoretical investigation, we\ncharacterize the convergence rate of the estimated treatment effects and the\nassociated standard deviations at the group level and further prove that our\nadaptive treatment assignment algorithm, despite not having a closed-form\nexpression, approaches the optimal allocation rule asymptotically. Our proof\nstrategy takes into account the fact that the allocation decisions in our\ndesign depend on sequentially accumulated data, which poses a significant\nchallenge in characterizing the properties and conducting statistical inference\nof our method. We further provide simulation evidence to showcase the\nperformance of our fair adaptive experiment strategy."}, "http://arxiv.org/abs/2310.16294": {"title": "Producer-Side Experiments Based on Counterfactual Interleaving Designs for Online Recommender Systems", "link": "http://arxiv.org/abs/2310.16294", "description": "Recommender systems have become an integral part of online platforms,\nproviding personalized suggestions for purchasing items, consuming contents,\nand connecting with individuals. An online recommender system consists of two\nsides of components: the producer side comprises product sellers, content\ncreators, or service providers, etc., and the consumer side includes buyers,\nviewers, or guests, etc. To optimize an online recommender system, A/B tests\nserve as the golden standard for comparing different ranking models and\nevaluating their impact on both the consumers and producers. While\nconsumer-side experiments are relatively straightforward to design and commonly\nused to gauge the impact of ranking changes on the behavior of consumers\n(buyers, viewers, etc.), designing producer-side experiments presents a\nconsiderable challenge because producer items in the treatment and control\ngroups need to be ranked by different models and then merged into a single\nranking for the recommender to show to each consumer. In this paper, we review\nissues with the existing methods, propose new design principles for\nproducer-side experiments, and develop a rigorous solution based on\ncounterfactual interleaving designs for accurately measuring the effects of\nranking changes on the producers (sellers, creators, etc.)."}, "http://arxiv.org/abs/2310.16466": {"title": "Learning Continuous Network Emerging Dynamics from Scarce Observations via Data-Adaptive Stochastic Processes", "link": "http://arxiv.org/abs/2310.16466", "description": "Learning network dynamics from the empirical structure and spatio-temporal\nobservation data is crucial to revealing the interaction mechanisms of complex\nnetworks in a wide range of domains. However, most existing methods only aim at\nlearning network dynamic behaviors generated by a specific ordinary\ndifferential equation instance, resulting in ineffectiveness for new ones, and\ngenerally require dense observations. The observed data, especially from\nnetwork emerging dynamics, are usually difficult to obtain, which brings\ntrouble to model learning. Therefore, how to learn accurate network dynamics\nwith sparse, irregularly-sampled, partial, and noisy observations remains a\nfundamental challenge. We introduce Neural ODE Processes for Network Dynamics\n(NDP4ND), a new class of stochastic processes governed by stochastic\ndata-adaptive network dynamics, to overcome the challenge and learn continuous\nnetwork dynamics from scarce observations. Intensive experiments conducted on\nvarious network dynamics in ecological population evolution, phototaxis\nmovement, brain activity, epidemic spreading, and real-world empirical systems,\ndemonstrate that the proposed method has excellent data adaptability and\ncomputational efficiency, and can adapt to unseen network emerging dynamics,\nproducing accurate interpolation and extrapolation with reducing the ratio of\nrequired observation data to only about 6\\% and improving the learning speed\nfor new dynamics by three orders of magnitude."}, "http://arxiv.org/abs/2310.16489": {"title": "Latent event history models for quasi-reaction systems", "link": "http://arxiv.org/abs/2310.16489", "description": "Various processes can be modelled as quasi-reaction systems of stochastic\ndifferential equations, such as cell differentiation and disease spreading.\nSince the underlying data of particle interactions, such as reactions between\nproteins or contacts between people, are typically unobserved, statistical\ninference of the parameters driving these systems is developed from\nconcentration data measuring each unit in the system over time. While observing\nthe continuous time process at a time scale as fine as possible should in\ntheory help with parameter estimation, the existing Local Linear Approximation\n(LLA) methods fail in this case, due to numerical instability caused by small\nchanges of the system at successive time points. On the other hand, one may be\nable to reconstruct the underlying unobserved interactions from the observed\ncount data. Motivated by this, we first formalise the latent event history\nmodel underlying the observed count process. We then propose a computationally\nefficient Expectation-Maximation algorithm for parameter estimation, with an\nextended Kalman filtering procedure for the prediction of the latent states. A\nsimulation study shows the performance of the proposed method and highlights\nthe settings where it is particularly advantageous compared to the existing LLA\napproaches. Finally, we present an illustration of the methodology on the\nspreading of the COVID-19 pandemic in Italy."}, "http://arxiv.org/abs/2310.16502": {"title": "Assessing the overall and partial causal well-specification of nonlinear additive noise models", "link": "http://arxiv.org/abs/2310.16502", "description": "We propose a method to detect model misspecifications in nonlinear causal\nadditive and potentially heteroscedastic noise models. We aim to identify\npredictor variables for which we can infer the causal effect even in cases of\nsuch misspecification. We develop a general framework based on knowledge of the\nmultivariate observational data distribution and we then propose an algorithm\nfor finite sample data, discuss its asymptotic properties, and illustrate its\nperformance on simulated and real data."}, "http://arxiv.org/abs/2310.16600": {"title": "Balancing central and marginal rejection when combining independent significance tests", "link": "http://arxiv.org/abs/2310.16600", "description": "A common approach to evaluating the significance of a collection of\n$p$-values combines them with a pooling function, in particular when the\noriginal data are not available. These pooled $p$-values convert a sample of\n$p$-values into a single number which behaves like a univariate $p$-value. To\nclarify discussion of these functions, a telescoping series of alternative\nhypotheses are introduced that communicate the strength and prevalence of\nnon-null evidence in the $p$-values before general pooling formulae are\ndiscussed. A pattern noticed in the UMP pooled $p$-value for a particular\nalternative motivates the definition and discussion of central and marginal\nrejection levels at $\\alpha$. It is proven that central rejection is always\ngreater than or equal to marginal rejection, motivating a quotient to measure\nthe balance between the two for pooled $p$-values. A combining function based\non the $\\chi^2_{\\kappa}$ quantile transformation is proposed to control this\nquotient and shown to be robust to mis-specified parameters relative to the\nUMP. Different powers for different parameter settings motivate a map of\nplausible alternatives based on where this pooled $p$-value is minimized."}, "http://arxiv.org/abs/2310.16626": {"title": "Scalable Causal Structure Learning via Amortized Conditional Independence Testing", "link": "http://arxiv.org/abs/2310.16626", "description": "Controlling false positives (Type I errors) through statistical hypothesis\ntesting is a foundation of modern scientific data analysis. Existing causal\nstructure discovery algorithms either do not provide Type I error control or\ncannot scale to the size of modern scientific datasets. We consider a variant\nof the causal discovery problem with two sets of nodes, where the only edges of\ninterest form a bipartite causal subgraph between the sets. We develop Scalable\nCausal Structure Learning (SCSL), a method for causal structure discovery on\nbipartite subgraphs that provides Type I error control. SCSL recasts the\ndiscovery problem as a simultaneous hypothesis testing problem and uses\ndiscrete optimization over the set of possible confounders to obtain an upper\nbound on the test statistic for each edge. Semi-synthetic simulations\ndemonstrate that SCSL scales to handle graphs with hundreds of nodes while\nmaintaining error control and good power. We demonstrate the practical\napplicability of the method by applying it to a cancer dataset to reveal\nconnections between somatic gene mutations and metastases to different tissues."}, "http://arxiv.org/abs/2310.16638": {"title": "Covariate Shift Adaptation Robust to Density-Ratio Estimation", "link": "http://arxiv.org/abs/2310.16638", "description": "Consider a scenario where we have access to train data with both covariates\nand outcomes while test data only contains covariates. In this scenario, our\nprimary aim is to predict the missing outcomes of the test data. With this\nobjective in mind, we train parametric regression models under a covariate\nshift, where covariate distributions are different between the train and test\ndata. For this problem, existing studies have proposed covariate shift\nadaptation via importance weighting using the density ratio. This approach\naverages the train data losses, each weighted by an estimated ratio of the\ncovariate densities between the train and test data, to approximate the\ntest-data risk. Although it allows us to obtain a test-data risk minimizer, its\nperformance heavily relies on the accuracy of the density ratio estimation.\nMoreover, even if the density ratio can be consistently estimated, the\nestimation errors of the density ratio also yield bias in the estimators of the\nregression model's parameters of interest. To mitigate these challenges, we\nintroduce a doubly robust estimator for covariate shift adaptation via\nimportance weighting, which incorporates an additional estimator for the\nregression function. Leveraging double machine learning techniques, our\nestimator reduces the bias arising from the density ratio estimation errors. We\ndemonstrate the asymptotic distribution of the regression parameter estimator.\nNotably, our estimator remains consistent if either the density ratio estimator\nor the regression function is consistent, showcasing its robustness against\npotential errors in density ratio estimation. Finally, we confirm the soundness\nof our proposed method via simulation studies."}, "http://arxiv.org/abs/2310.16650": {"title": "Data-integration with pseudoweights and survey-calibration: application to developing US-representative lung cancer risk models for use in screening", "link": "http://arxiv.org/abs/2310.16650", "description": "Accurate cancer risk estimation is crucial to clinical decision-making, such\nas identifying high-risk people for screening. However, most existing cancer\nrisk models incorporate data from epidemiologic studies, which usually cannot\nrepresent the target population. While population-based health surveys are\nideal for making inference to the target population, they typically do not\ncollect time-to-cancer incidence data. Instead, time-to-cancer specific\nmortality is often readily available on surveys via linkage to vital\nstatistics. We develop calibrated pseudoweighting methods that integrate\nindividual-level data from a cohort and a survey, and summary statistics of\ncancer incidence from national cancer registries. By leveraging\nindividual-level cancer mortality data in the survey, the proposed methods\nimpute time-to-cancer incidence for survey sample individuals and use survey\ncalibration with auxiliary variables of influence functions generated from Cox\nregression to improve robustness and efficiency of the inverse-propensity\npseudoweighting method in estimating pure risks. We develop a lung cancer\nincidence pure risk model from the Prostate, Lung, Colorectal, and Ovarian\n(PLCO) Cancer Screening Trial using our proposed methods by integrating data\nfrom the National Health Interview Survey (NHIS) and cancer registries."}, "http://arxiv.org/abs/2310.16653": {"title": "Adaptive importance sampling for heavy-tailed distributions via $\\alpha$-divergence minimization", "link": "http://arxiv.org/abs/2310.16653", "description": "Adaptive importance sampling (AIS) algorithms are widely used to approximate\nexpectations with respect to complicated target probability distributions. When\nthe target has heavy tails, existing AIS algorithms can provide inconsistent\nestimators or exhibit slow convergence, as they often neglect the target's tail\nbehaviour. To avoid this pitfall, we propose an AIS algorithm that approximates\nthe target by Student-t proposal distributions. We adapt location and scale\nparameters by matching the escort moments - which are defined even for\nheavy-tailed distributions - of the target and the proposal. These updates\nminimize the $\\alpha$-divergence between the target and the proposal, thereby\nconnecting with variational inference. We then show that the\n$\\alpha$-divergence can be approximated by a generalized notion of effective\nsample size and leverage this new perspective to adapt the tail parameter with\nBayesian optimization. We demonstrate the efficacy of our approach through\napplications to synthetic targets and a Bayesian Student-t regression task on a\nreal example with clinical trial data."}, "http://arxiv.org/abs/2310.16690": {"title": "Dynamic treatment effect phenotyping through functional survival analysis", "link": "http://arxiv.org/abs/2310.16690", "description": "In recent years, research interest in personalised treatments has been\ngrowing. However, treatment effect heterogeneity and possibly time-varying\ntreatment effects are still often overlooked in clinical studies. Statistical\ntools are needed for the identification of treatment response patterns, taking\ninto account that treatment response is not constant over time. We aim to\nprovide an innovative method to obtain dynamic treatment effect phenotypes on a\ntime-to-event outcome, conditioned on a set of relevant effect modifiers. The\nproposed method does not require the assumption of proportional hazards for the\ntreatment effect, which is rarely realistic. We propose a spline-based survival\nneural network, inspired by the Royston-Parmar survival model, to estimate\ntime-varying conditional treatment effects. We then exploit the functional\nnature of the resulting estimates to apply a functional clustering of the\ntreatment effect curves in order to identify different patterns of treatment\neffects. The application that motivated this work is the discontinuation of\ntreatment with Mineralocorticoid receptor Antagonists (MRAs) in patients with\nheart failure, where there is no clear evidence as to which patients it is the\nsafest choice to discontinue treatment and, conversely, when it leads to a\nhigher risk of adverse events. The data come from an electronic health record\ndatabase. A simulation study was performed to assess the performance of the\nspline-based neural network and the stability of the treatment response\nphenotyping procedure. We provide a novel method to inform individualized\nmedical decisions by characterising subject-specific treatment responses over\ntime."}, "http://arxiv.org/abs/2310.16698": {"title": "Causal Discovery with Generalized Linear Models through Peeling Algorithms", "link": "http://arxiv.org/abs/2310.16698", "description": "This article presents a novel method for causal discovery with generalized\nstructural equation models suited for analyzing diverse types of outcomes,\nincluding discrete, continuous, and mixed data. Causal discovery often faces\nchallenges due to unmeasured confounders that hinder the identification of\ncausal relationships. The proposed approach addresses this issue by developing\ntwo peeling algorithms (bottom-up and top-down) to ascertain causal\nrelationships and valid instruments. This approach first reconstructs a\nsuper-graph to represent ancestral relationships between variables, using a\npeeling algorithm based on nodewise GLM regressions that exploit relationships\nbetween primary and instrumental variables. Then, it estimates parent-child\neffects from the ancestral relationships using another peeling algorithm while\ndeconfounding a child's model with information borrowed from its parents'\nmodels. The article offers a theoretical analysis of the proposed approach,\nwhich establishes conditions for model identifiability and provides statistical\nguarantees for accurately discovering parent-child relationships via the\npeeling algorithms. Furthermore, the article presents numerical experiments\nshowcasing the effectiveness of our approach in comparison to state-of-the-art\nstructure learning methods without confounders. Lastly, it demonstrates an\napplication to Alzheimer's disease (AD), highlighting the utility of the method\nin constructing gene-to-gene and gene-to-disease regulatory networks involving\nSingle Nucleotide Polymorphisms (SNPs) for healthy and AD subjects."}, "http://arxiv.org/abs/2310.16813": {"title": "Improving the Aggregation and Evaluation of NBA Mock Drafts", "link": "http://arxiv.org/abs/2310.16813", "description": "Many enthusiasts and experts publish forecasts of the order players are\ndrafted into professional sports leagues, known as mock drafts. Using a novel\ndataset of mock drafts for the National Basketball Association (NBA), we\nanalyze authors' mock draft accuracy over time and ask how we can reasonably\nuse information from multiple authors. To measure how accurate mock drafts are,\nwe assume that both mock drafts and the actual draft are ranked lists, and we\npropose that rank-biased distance (RBD) of Webber et al. (2010) is the\nappropriate error metric for mock draft accuracy. This is because RBD allows\nmock drafts to have a different length than the actual draft, accounts for\nplayers not appearing in both lists, and weights errors early in the draft more\nthan errors later on. We validate that mock drafts, as expected, improve in\naccuracy over the course of a season, and that accuracy of the mock drafts\nproduced right before their drafts is fairly stable across seasons. To be able\nto combine information from multiple mock drafts into a single consensus mock\ndraft, we also propose a ranked-list combination method based on the ideas of\nranked-choice voting. We show that our method provides improved forecasts over\nthe standard Borda count combination method used for most similar analyses in\nsports, and that either combination method provides a more accurate forecast\nover time than any single author."}, "http://arxiv.org/abs/2310.16819": {"title": "CATE Lasso: Conditional Average Treatment Effect Estimation with High-Dimensional Linear Regression", "link": "http://arxiv.org/abs/2310.16819", "description": "In causal inference about two treatments, Conditional Average Treatment\nEffects (CATEs) play an important role as a quantity representing an\nindividualized causal effect, defined as a difference between the expected\noutcomes of the two treatments conditioned on covariates. This study assumes\ntwo linear regression models between a potential outcome and covariates of the\ntwo treatments and defines CATEs as a difference between the linear regression\nmodels. Then, we propose a method for consistently estimating CATEs even under\nhigh-dimensional and non-sparse parameters. In our study, we demonstrate that\ndesirable theoretical properties, such as consistency, remain attainable even\nwithout assuming sparsity explicitly if we assume a weaker assumption called\nimplicit sparsity originating from the definition of CATEs. In this assumption,\nwe suppose that parameters of linear models in potential outcomes can be\ndivided into treatment-specific and common parameters, where the\ntreatment-specific parameters take difference values between each linear\nregression model, while the common parameters remain identical. Thus, in a\ndifference between two linear regression models, the common parameters\ndisappear, leaving only differences in the treatment-specific parameters.\nConsequently, the non-zero parameters in CATEs correspond to the differences in\nthe treatment-specific parameters. Leveraging this assumption, we develop a\nLasso regression method specialized for CATE estimation and present that the\nestimator is consistent. Finally, we confirm the soundness of the proposed\nmethod by simulation studies."}, "http://arxiv.org/abs/2310.16824": {"title": "Parametric model for post-processing visibility ensemble forecasts", "link": "http://arxiv.org/abs/2310.16824", "description": "Despite the continuous development of the different operational ensemble\nprediction systems over the past decades, ensemble forecasts still might suffer\nfrom lack of calibration and/or display systematic bias, thus require some\npost-processing to improve their forecast skill. Here we focus on visibility,\nwhich quantity plays a crucial role e.g. in aviation and road safety or in ship\nnavigation, and propose a parametric model where the predictive distribution is\na mixture of a gamma and a truncated normal distribution, both right censored\nat the maximal reported visibility value. The new model is evaluated in two\ncase studies based on visibility ensemble forecasts of the European Centre for\nMedium-Range Weather Forecasts covering two distinct domains in Central and\nWestern Europe and two different time periods. The results of the case studies\nindicate that climatology is substantially superior to the raw ensemble;\nnevertheless, the forecast skill can be further improved by post-processing, at\nleast for short lead times. Moreover, the proposed mixture model consistently\noutperforms the Bayesian model averaging approach used as reference\npost-processing technique."}, "http://arxiv.org/abs/2109.09339": {"title": "Improving the accuracy of estimating indexes in contingency tables using Bayesian estimators", "link": "http://arxiv.org/abs/2109.09339", "description": "In contingency table analysis, one is interested in testing whether a model\nof interest (e.g., the independent or symmetry model) holds using\ngoodness-of-fit tests. When the null hypothesis where the model is true is\nrejected, the interest turns to the degree to which the probability structure\nof the contingency table deviates from the model. Many indexes have been\nstudied to measure the degree of the departure, such as the Yule coefficient\nand Cram\\'er coefficient for the independence model, and Tomizawa's symmetry\nindex for the symmetry model. The inference of these indexes is performed using\nsample proportions, which are estimates of cell probabilities, but it is\nwell-known that the bias and mean square error (MSE) values become large\nwithout a sufficient number of samples. To address the problem, this study\nproposes a new estimator for indexes using Bayesian estimators of cell\nprobabilities. Assuming the Dirichlet distribution for the prior of cell\nprobabilities, we asymptotically evaluate the value of MSE when plugging the\nposterior means of cell probabilities into the index, and propose an estimator\nof the index using the Dirichlet hyperparameter that minimizes the value.\nNumerical experiments show that when the number of samples per cell is small,\nthe proposed method has smaller values of bias and MSE than other methods of\ncorrecting estimation accuracy. We also show that the values of bias and MSE\nare smaller than those obtained by using the uniform and Jeffreys priors."}, "http://arxiv.org/abs/2110.01031": {"title": "A general framework for formulating structured variable selection", "link": "http://arxiv.org/abs/2110.01031", "description": "In variable selection, a selection rule that prescribes the permissible sets\nof selected variables (called a \"selection dictionary\") is desirable due to the\ninherent structural constraints among the candidate variables. Such selection\nrules can be complex in real-world data analyses, and failing to incorporate\nsuch restrictions could not only compromise the interpretability of the model\nbut also lead to decreased prediction accuracy. However, no general framework\nhas been proposed to formalize selection rules and their applications, which\nposes a significant challenge for practitioners seeking to integrate these\nrules into their analyses. In this work, we establish a framework for\nstructured variable selection that can incorporate universal structural\nconstraints. We develop a mathematical language for constructing arbitrary\nselection rules, where the selection dictionary is formally defined. We\ndemonstrate that all selection rules can be expressed as combinations of\noperations on constructs, facilitating the identification of the corresponding\nselection dictionary. Once this selection dictionary is derived, practitioners\ncan apply their own user-defined criteria to select the optimal model.\nAdditionally, our framework enhances existing penalized regression methods for\nvariable selection by providing guidance on how to appropriately group\nvariables to achieve the desired selection rule. Furthermore, our innovative\nframework opens the door to establishing new l0 norm-based penalized regression\ntechniques that can be tailored to respect arbitrary selection rules, thereby\nexpanding the possibilities for more robust and tailored model development."}, "http://arxiv.org/abs/2203.14223": {"title": "Identifying Peer Influence in Therapeutic Communities", "link": "http://arxiv.org/abs/2203.14223", "description": "We investigate if there is a peer influence or role model effect on\nsuccessful graduation from Therapeutic Communities (TCs). We analyze anonymized\nindividual-level observational data from 3 TCs that kept records of written\nexchanges of affirmations and corrections among residents, and their precise\nentry and exit dates. The affirmations allow us to form peer networks, and the\nentry and exit dates allow us to define a causal effect of interest. We\nconceptualize the causal role model effect as measuring the difference in the\nexpected outcome of a resident (ego) who can observe one of their social\ncontacts (e.g., peers who gave affirmations), to be successful in graduating\nbefore the ego's exit vs not successfully graduating before the ego's exit.\nSince peer influence is usually confounded with unobserved homophily in\nobservational data, we model the network with a latent variable model to\nestimate homophily and include it in the outcome equation. We provide a\ntheoretical guarantee that the bias of our peer influence estimator decreases\nwith sample size. Our results indicate there is an effect of peers' graduation\non the graduation of residents. The magnitude of peer influence differs based\non gender, race, and the definition of the role model effect. A counterfactual\nexercise quantifies the potential benefits of intervention of assigning a buddy\nto \"at-risk\" individuals directly on the treated resident and indirectly on\ntheir peers through network propagation."}, "http://arxiv.org/abs/2207.03182": {"title": "Chilled Sampling for Uncertainty Quantification: A Motivation From A Meteorological Inverse Problem", "link": "http://arxiv.org/abs/2207.03182", "description": "Atmospheric motion vectors (AMVs) extracted from satellite imagery are the\nonly wind observations with good global coverage. They are important features\nfor feeding numerical weather prediction (NWP) models. Several Bayesian models\nhave been proposed to estimate AMVs. Although critical for correct assimilation\ninto NWP models, very few methods provide a thorough characterization of the\nestimation errors. The difficulty of estimating errors stems from the\nspecificity of the posterior distribution, which is both very high dimensional,\nand highly ill-conditioned due to a singular likelihood. Motivated by this\ndifficult inverse problem, this work studies the evaluation of the (expected)\nestimation errors using gradient-based Markov Chain Monte Carlo (MCMC)\nalgorithms. The main contribution is to propose a general strategy, called here\nchilling, which amounts to sampling a local approximation of the posterior\ndistribution in the neighborhood of a point estimate. From a theoretical point\nof view, we show that under regularity assumptions, the family of chilled\nposterior distributions converges in distribution as temperature decreases to\nan optimal Gaussian approximation at a point estimate given by the Maximum A\nPosteriori, also known as the Laplace approximation. Chilled sampling therefore\nprovides access to this approximation generally out of reach in such\nhigh-dimensional nonlinear contexts. From an empirical perspective, we evaluate\nthe proposed approach based on some quantitative Bayesian criteria. Our\nnumerical simulations are performed on synthetic and real meteorological data.\nThey reveal that not only the proposed chilling exhibits a significant gain in\nterms of accuracy of the point estimates and of their associated expected\nerrors, but also a substantial acceleration in the convergence speed of the\nMCMC algorithms."}, "http://arxiv.org/abs/2207.13612": {"title": "Robust Output Analysis with Monte-Carlo Methodology", "link": "http://arxiv.org/abs/2207.13612", "description": "In predictive modeling with simulation or machine learning, it is critical to\naccurately assess the quality of estimated values through output analysis. In\nrecent decades output analysis has become enriched with methods that quantify\nthe impact of input data uncertainty in the model outputs to increase\nrobustness. However, most developments are applicable assuming that the input\ndata adheres to a parametric family of distributions. We propose a unified\noutput analysis framework for simulation and machine learning outputs through\nthe lens of Monte Carlo sampling. This framework provides nonparametric\nquantification of the variance and bias induced in the outputs with\nhigher-order accuracy. Our new bias-corrected estimation from the model outputs\nleverages the extension of fast iterative bootstrap sampling and higher-order\ninfluence functions. For the scalability of the proposed estimation methods, we\ndevise budget-optimal rules and leverage control variates for variance\nreduction. Our theoretical and numerical results demonstrate a clear advantage\nin building more robust confidence intervals from the model outputs with higher\ncoverage probability."}, "http://arxiv.org/abs/2208.06685": {"title": "Adaptive novelty detection with false discovery rate guarantee", "link": "http://arxiv.org/abs/2208.06685", "description": "This paper studies the semi-supervised novelty detection problem where a set\nof \"typical\" measurements is available to the researcher. Motivated by recent\nadvances in multiple testing and conformal inference, we propose AdaDetect, a\nflexible method that is able to wrap around any probabilistic classification\nalgorithm and control the false discovery rate (FDR) on detected novelties in\nfinite samples without any distributional assumption other than\nexchangeability. In contrast to classical FDR-controlling procedures that are\noften committed to a pre-specified p-value function, AdaDetect learns the\ntransformation in a data-adaptive manner to focus the power on the directions\nthat distinguish between inliers and outliers. Inspired by the multiple testing\nliterature, we further propose variants of AdaDetect that are adaptive to the\nproportion of nulls while maintaining the finite-sample FDR control. The\nmethods are illustrated on synthetic datasets and real-world datasets,\nincluding an application in astrophysics."}, "http://arxiv.org/abs/2211.02582": {"title": "Inference for Network Count Time Series with the R Package PNAR", "link": "http://arxiv.org/abs/2211.02582", "description": "We introduce a new R package useful for inference about network count time\nseries. Such data are frequently encountered in statistics and they are usually\ntreated as multivariate time series. Their statistical analysis is based on\nlinear or log linear models. Nonlinear models, which have been applied\nsuccessfully in several research areas, have been neglected from such\napplications mainly because of their computational complexity. We provide R\nusers the flexibility to fit and study nonlinear network count time series\nmodels which include either a drift in the intercept or a regime switching\nmechanism. We develop several computational tools including estimation of\nvarious count Network Autoregressive models and fast computational algorithms\nfor testing linearity in standard cases and when non-identifiable parameters\nhamper the analysis. Finally, we introduce a copula Poisson algorithm for\nsimulating multivariate network count time series. We illustrate the\nmethodology by modeling weekly number of influenza cases in Germany."}, "http://arxiv.org/abs/2212.08642": {"title": "Estimating Higher-Order Mixed Memberships via the $\\ell_{2,\\infty}$ Tensor Perturbation Bound", "link": "http://arxiv.org/abs/2212.08642", "description": "Higher-order multiway data is ubiquitous in machine learning and statistics\nand often exhibits community-like structures, where each component (node) along\neach different mode has a community membership associated with it. In this\npaper we propose the tensor mixed-membership blockmodel, a generalization of\nthe tensor blockmodel positing that memberships need not be discrete, but\ninstead are convex combinations of latent communities. We establish the\nidentifiability of our model and propose a computationally efficient estimation\nprocedure based on the higher-order orthogonal iteration algorithm (HOOI) for\ntensor SVD composed with a simplex corner-finding algorithm. We then\ndemonstrate the consistency of our estimation procedure by providing a per-node\nerror bound, which showcases the effect of higher-order structures on\nestimation accuracy. To prove our consistency result, we develop the\n$\\ell_{2,\\infty}$ tensor perturbation bound for HOOI under independent,\npossibly heteroskedastic, subgaussian noise that may be of independent\ninterest. Our analysis uses a novel leave-one-out construction for the\niterates, and our bounds depend only on spectral properties of the underlying\nlow-rank tensor under nearly optimal signal-to-noise ratio conditions such that\ntensor SVD is computationally feasible. Whereas other leave-one-out analyses\ntypically focus on sequences constructed by analyzing the output of a given\nalgorithm with a small part of the noise removed, our leave-one-out analysis\nconstructions use both the previous iterates and the additional tensor\nstructure to eliminate a potential additional source of error. Finally, we\napply our methodology to real and simulated data, including applications to two\nflight datasets and a trade network dataset, demonstrating some effects not\nidentifiable from the model with discrete community memberships."}, "http://arxiv.org/abs/2304.10372": {"title": "Statistical inference for Gaussian Whittle-Mat\\'ern fields on metric graphs", "link": "http://arxiv.org/abs/2304.10372", "description": "Whittle-Mat\\'ern fields are a recently introduced class of Gaussian processes\non metric graphs, which are specified as solutions to a fractional-order\nstochastic differential equation. Unlike earlier covariance-based approaches\nfor specifying Gaussian fields on metric graphs, the Whittle-Mat\\'ern fields\nare well-defined for any compact metric graph and can provide Gaussian\nprocesses with differentiable sample paths. We derive the main statistical\nproperties of the model class, particularly the consistency and asymptotic\nnormality of maximum likelihood estimators of model parameters and the\nnecessary and sufficient conditions for asymptotic optimality properties of\nlinear prediction based on the model with misspecified parameters.\n\nThe covariance function of the Whittle-Mat\\'ern fields is generally\nunavailable in closed form, and they have therefore been challenging to use for\nstatistical inference. However, we show that for specific values of the\nfractional exponent, when the fields have Markov properties, likelihood-based\ninference and spatial prediction can be performed exactly and computationally\nefficiently. This facilitates using the Whittle-Mat\\'ern fields in statistical\napplications involving big datasets without the need for any approximations.\nThe methods are illustrated via an application to modeling of traffic data,\nwhere allowing for differentiable processes dramatically improves the results."}, "http://arxiv.org/abs/2305.09282": {"title": "Errors-in-variables Fr\\'echet Regression with Low-rank Covariate Approximation", "link": "http://arxiv.org/abs/2305.09282", "description": "Fr\\'echet regression has emerged as a promising approach for regression\nanalysis involving non-Euclidean response variables. However, its practical\napplicability has been hindered by its reliance on ideal scenarios with\nabundant and noiseless covariate data. In this paper, we present a novel\nestimation method that tackles these limitations by leveraging the low-rank\nstructure inherent in the covariate matrix. Our proposed framework combines the\nconcepts of global Fr\\'echet regression and principal component regression,\naiming to improve the efficiency and accuracy of the regression estimator. By\nincorporating the low-rank structure, our method enables more effective\nmodeling and estimation, particularly in high-dimensional and\nerrors-in-variables regression settings. We provide a theoretical analysis of\nthe proposed estimator's large-sample properties, including a comprehensive\nrate analysis of bias, variance, and additional variations due to measurement\nerrors. Furthermore, our numerical experiments provide empirical evidence that\nsupports the theoretical findings, demonstrating the superior performance of\nour approach. Overall, this work introduces a promising framework for\nregression analysis of non-Euclidean variables, effectively addressing the\nchallenges associated with limited and noisy covariate data, with potential\napplications in diverse fields."}, "http://arxiv.org/abs/2305.19417": {"title": "Model averaging approaches to data subset selection", "link": "http://arxiv.org/abs/2305.19417", "description": "Model averaging is a useful and robust method for dealing with model\nuncertainty in statistical analysis. Often, it is useful to consider data\nsubset selection at the same time, in which model selection criteria are used\nto compare models across different subsets of the data. Two different criteria\nhave been proposed in the literature for how the data subsets should be\nweighted. We compare the two criteria closely in a unified treatment based on\nthe Kullback-Leibler divergence, and conclude that one of them is subtly flawed\nand will tend to yield larger uncertainties due to loss of information.\nAnalytical and numerical examples are provided."}, "http://arxiv.org/abs/2309.06053": {"title": "Confounder selection via iterative graph expansion", "link": "http://arxiv.org/abs/2309.06053", "description": "Confounder selection, namely choosing a set of covariates to control for\nconfounding between a treatment and an outcome, is arguably the most important\nstep in the design of observational studies. Previous methods, such as Pearl's\ncelebrated back-door criterion, typically require pre-specifying a causal\ngraph, which can often be difficult in practice. We propose an interactive\nprocedure for confounder selection that does not require pre-specifying the\ngraph or the set of observed variables. This procedure iteratively expands the\ncausal graph by finding what we call \"primary adjustment sets\" for a pair of\npossibly confounded variables. This can be viewed as inverting a sequence of\nlatent projections of the underlying causal graph. Structural information in\nthe form of primary adjustment sets is elicited from the user, bit by bit,\nuntil either a set of covariates are found to control for confounding or it can\nbe determined that no such set exists. Other information, such as the causal\nrelations between confounders, is not required by the procedure. We show that\nif the user correctly specifies the primary adjustment sets in every step, our\nprocedure is both sound and complete."}, "http://arxiv.org/abs/2310.16989": {"title": "Randomization Inference When N Equals One", "link": "http://arxiv.org/abs/2310.16989", "description": "N-of-1 experiments, where a unit serves as its own control and treatment in\ndifferent time windows, have been used in certain medical contexts for decades.\nHowever, due to effects that accumulate over long time windows and\ninterventions that have complex evolution, a lack of robust inference tools has\nlimited the widespread applicability of such N-of-1 designs. This work combines\ntechniques from experiment design in causal inference and system identification\nfrom control theory to provide such an inference framework. We derive a model\nof the dynamic interference effect that arises in linear time-invariant\ndynamical systems. We show that a family of causal estimands analogous to those\nstudied in potential outcomes are estimable via a standard estimator derived\nfrom the method of moments. We derive formulae for higher moments of this\nestimator and describe conditions under which N-of-1 designs may provide faster\nways to estimate the effects of interventions in dynamical systems. We also\nprovide conditions under which our estimator is asymptotically normal and\nderive valid confidence intervals for this setting."}, "http://arxiv.org/abs/2310.17009": {"title": "Simulation based stacking", "link": "http://arxiv.org/abs/2310.17009", "description": "Simulation-based inference has been popular for amortized Bayesian\ncomputation. It is typical to have more than one posterior approximation, from\ndifferent inference algorithms, different architectures, or simply the\nrandomness of initialization and stochastic gradients. With a provable\nasymptotic guarantee, we present a general stacking framework to make use of\nall available posterior approximations. Our stacking method is able to combine\ndensities, simulation draws, confidence intervals, and moments, and address the\noverall precision, calibration, coverage, and bias at the same time. We\nillustrate our method on several benchmark simulations and a challenging\ncosmological inference task."}, "http://arxiv.org/abs/2310.17153": {"title": "Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration", "link": "http://arxiv.org/abs/2310.17153", "description": "Semi-implicit variational inference (SIVI) has been introduced to expand the\nanalytical variational families by defining expressive semi-implicit\ndistributions in a hierarchical manner. However, the single-layer architecture\ncommonly used in current SIVI methods can be insufficient when the target\nposterior has complicated structures. In this paper, we propose hierarchical\nsemi-implicit variational inference, called HSIVI, which generalizes SIVI to\nallow more expressive multi-layer construction of semi-implicit distributions.\nBy introducing auxiliary distributions that interpolate between a simple base\ndistribution and the target distribution, the conditional layers can be trained\nby progressively matching these auxiliary distributions one layer after\nanother. Moreover, given pre-trained score networks, HSIVI can be used to\naccelerate the sampling process of diffusion models with the score matching\nobjective. We show that HSIVI significantly enhances the expressiveness of SIVI\non several Bayesian inference problems with complicated target distributions.\nWhen used for diffusion model acceleration, we show that HSIVI can produce high\nquality samples comparable to or better than the existing fast diffusion model\nbased samplers with a small number of function evaluations on various datasets."}, "http://arxiv.org/abs/2310.17165": {"title": "Price Experimentation and Interference in Online Platforms", "link": "http://arxiv.org/abs/2310.17165", "description": "In this paper, we examine the biases arising in A/B tests where a firm\nmodifies a continuous parameter, such as price, to estimate the global\ntreatment effect associated to a given performance metric. Such biases emerge\nfrom canonical designs and estimators due to interference among market\nparticipants. We employ structural modeling and differential calculus to derive\nintuitive structural characterizations of this bias. We then specialize our\ngeneral model to a standard revenue management pricing problem. This setting\nhighlights a key potential pitfall in the use of pricing experiments to guide\nprofit maximization: notably, the canonical estimator for the change in profits\ncan have the {\\em wrong sign}. In other words, following the guidance of the\ncanonical estimator may lead the firm to move prices in the wrong direction,\nand thereby decrease profits relative to the status quo. We apply these results\nto a two-sided market model and show how this ``change of sign\" regime depends\non model parameters, and discuss structural and practical implications for\nplatform operators."}, "http://arxiv.org/abs/2310.17248": {"title": "The observed Fisher information attached to the EM algorithm, illustrated on Shepp and Vardi estimation procedure for positron emission tomography", "link": "http://arxiv.org/abs/2310.17248", "description": "The Shepp &amp; Vardi (1982) implementation of the EM algorithm for PET scan\ntumor estimation provides a point estimate of the tumor. The current study\npresents a closed-form formula of the observed Fisher information for Shepp &amp;\nVardi PET scan tumor estimation. Keywords: PET scan, EM algorithm, Fisher\ninformation matrix, standard errors."}, "http://arxiv.org/abs/2310.17308": {"title": "Wild Bootstrap for Counting Process-Based Statistics", "link": "http://arxiv.org/abs/2310.17308", "description": "The wild bootstrap is a popular resampling method in the context of\ntime-to-event data analyses. Previous works established the large sample\nproperties of it for applications to different estimators and test statistics.\nIt can be used to justify the accuracy of inference procedures such as\nhypothesis tests or time-simultaneous confidence bands. This paper consists of\ntwo parts: in Part~I, a general framework is developed in which the large\nsample properties are established in a unified way by using martingale\nstructures. The framework includes most of the well-known non- and\nsemiparametric statistical methods in time-to-event analysis and parametric\napproaches. In Part II, the Fine-Gray proportional sub-hazards model\nexemplifies the theory for inference on cumulative incidence functions given\nthe covariates. The model falls within the framework if the data are\ncensoring-complete. A simulation study demonstrates the reliability of the\nmethod and an application to a data set about hospital-acquired infections\nillustrates the statistical procedure."}, "http://arxiv.org/abs/2310.17334": {"title": "Bayesian Optimization for Personalized Dose-Finding Trials with Combination Therapies", "link": "http://arxiv.org/abs/2310.17334", "description": "Identification of optimal dose combinations in early phase dose-finding\ntrials is challenging, due to the trade-off between precisely estimating the\nmany parameters required to flexibly model the dose-response surface, and the\nsmall sample sizes in early phase trials. Existing methods often restrict the\nsearch to pre-defined dose combinations, which may fail to identify regions of\noptimality in the dose combination space. These difficulties are even more\npertinent in the context of personalized dose-finding, where patient\ncharacteristics are used to identify tailored optimal dose combinations. To\novercome these challenges, we propose the use of Bayesian optimization for\nfinding optimal dose combinations in standard (\"one size fits all\") and\npersonalized multi-agent dose-finding trials. Bayesian optimization is a method\nfor estimating the global optima of expensive-to-evaluate objective functions.\nThe objective function is approximated by a surrogate model, commonly a\nGaussian process, paired with a sequential design strategy to select the next\npoint via an acquisition function. This work is motivated by an\nindustry-sponsored problem, where focus is on optimizing a dual-agent therapy\nin a setting featuring minimal toxicity. To compare the performance of the\nstandard and personalized methods under this setting, simulation studies are\nperformed for a variety of scenarios. Our study concludes that taking a\npersonalized approach is highly beneficial in the presence of heterogeneity."}, "http://arxiv.org/abs/2310.17434": {"title": "The `Why' behind including `Y' in your imputation model", "link": "http://arxiv.org/abs/2310.17434", "description": "Missing data is a common challenge when analyzing epidemiological data, and\nimputation is often used to address this issue. Here, we investigate the\nscenario where a covariate used in an analysis has missingness and will be\nimputed. There are recommendations to include the outcome from the analysis\nmodel in the imputation model for missing covariates, but it is not necessarily\nclear if this recommmendation always holds and why this is sometimes true. We\nexamine deterministic imputation (i.e., single imputation where the imputed\nvalues are treated as fixed) and stochastic imputation (i.e., single imputation\nwith a random value or multiple imputation) methods and their implications for\nestimating the relationship between the imputed covariate and the outcome. We\nmathematically demonstrate that including the outcome variable in imputation\nmodels is not just a recommendation but a requirement to achieve unbiased\nresults when using stochastic imputation methods. Moreover, we dispel common\nmisconceptions about deterministic imputation models and demonstrate why the\noutcome should not be included in these models. This paper aims to bridge the\ngap between imputation in theory and in practice, providing mathematical\nderivations to explain common statistical recommendations. We offer a better\nunderstanding of the considerations involved in imputing missing covariates and\nemphasize when it is necessary to include the outcome variable in the\nimputation model."}, "http://arxiv.org/abs/2310.17440": {"title": "Gibbs optimal design of experiments", "link": "http://arxiv.org/abs/2310.17440", "description": "Bayesian optimal design of experiments is a well-established approach to\nplanning experiments. Briefly, a probability distribution, known as a\nstatistical model, for the responses is assumed which is dependent on a vector\nof unknown parameters. A utility function is then specified which gives the\ngain in information for estimating the true value of the parameters using the\nBayesian posterior distribution. A Bayesian optimal design is given by\nmaximising the expectation of the utility with respect to the joint\ndistribution given by the statistical model and prior distribution for the true\nparameter values. The approach takes account of the experimental aim via\nspecification of the utility and of all assumed sources of uncertainty via the\nexpected utility. However, it is predicated on the specification of the\nstatistical model. Recently, a new type of statistical inference, known as\nGibbs (or General Bayesian) inference, has been advanced. This is\nBayesian-like, in that uncertainty on unknown quantities is represented by a\nposterior distribution, but does not necessarily rely on specification of a\nstatistical model. Thus the resulting inference should be less sensitive to\nmisspecification of the statistical model. The purpose of this paper is to\npropose Gibbs optimal design: a framework for optimal design of experiments for\nGibbs inference. The concept behind the framework is introduced along with a\ncomputational approach to find Gibbs optimal designs in practice. The framework\nis demonstrated on exemplars including linear models, and experiments with\ncount and time-to-event responses."}, "http://arxiv.org/abs/2310.17496": {"title": "Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach", "link": "http://arxiv.org/abs/2310.17496", "description": "In modern recommendation systems, the standard pipeline involves training\nmachine learning models on historical data to predict user behaviors and\nimprove recommendations continuously. However, these data training loops can\nintroduce interference in A/B tests, where data generated by control and\ntreatment algorithms, potentially with different distributions, are combined.\nTo address these challenges, we introduce a novel approach called weighted\ntraining. This approach entails training a model to predict the probability of\neach data point appearing in either the treatment or control data and\nsubsequently applying weighted losses during model training. We demonstrate\nthat this approach achieves the least variance among all estimators without\ncausing shifts in the training distributions. Through simulation studies, we\ndemonstrate the lower bias and variance of our approach compared to other\nmethods."}, "http://arxiv.org/abs/2310.17546": {"title": "A changepoint approach to modelling non-stationary soil moisture dynamics", "link": "http://arxiv.org/abs/2310.17546", "description": "Soil moisture dynamics provide an indicator of soil health that scientists\nmodel via soil drydown curves. The typical modeling process requires the soil\nmoisture time series to be manually separated into drydown segments and then\nexponential decay models are fitted to them independently. Sensor development\nover recent years means that experiments that were previously conducted over a\nfew field campaigns can now be scaled to months or even years, often at a\nhigher sampling rate. Manual identification of drydown segments is no longer\npractical. To better meet the challenge of increasing data size, this paper\nproposes a novel changepoint-based approach to automatically identify\nstructural changes in the soil drying process, and estimate the parameters\ncharacterizing the drying processes simultaneously. A simulation study is\ncarried out to assess the performance of the method. The results demonstrate\nits ability to identify structural changes and retrieve key parameters of\ninterest to soil scientists. The method is applied to hourly soil moisture time\nseries from the NEON data portal to investigate the temporal dynamics of soil\nmoisture drydown. We recover known relationships previously identified\nmanually, alongside delivering new insights into the temporal variability\nacross soil types and locations."}, "http://arxiv.org/abs/2310.17629": {"title": "Approximate Leave-one-out Cross Validation for Regression with $\\ell_1$ Regularizers (extended version)", "link": "http://arxiv.org/abs/2310.17629", "description": "The out-of-sample error (OO) is the main quantity of interest in risk\nestimation and model selection. Leave-one-out cross validation (LO) offers a\n(nearly) distribution-free yet computationally demanding approach to estimate\nOO. Recent theoretical work showed that approximate leave-one-out cross\nvalidation (ALO) is a computationally efficient and statistically reliable\nestimate of LO (and OO) for generalized linear models with differentiable\nregularizers. For problems involving non-differentiable regularizers, despite\nsignificant empirical evidence, the theoretical understanding of ALO's error\nremains unknown. In this paper, we present a novel theory for a wide class of\nproblems in the generalized linear model family with non-differentiable\nregularizers. We bound the error |ALO - LO| in terms of intuitive metrics such\nas the size of leave-i-out perturbations in active sets, sample size n, number\nof features p and regularization parameters. As a consequence, for the\n$\\ell_1$-regularized problems, we show that |ALO - LO| goes to zero as p goes\nto infinity while n/p and SNR are fixed and bounded."}, "http://arxiv.org/abs/2108.04201": {"title": "Guaranteed Functional Tensor Singular Value Decomposition", "link": "http://arxiv.org/abs/2108.04201", "description": "This paper introduces the functional tensor singular value decomposition\n(FTSVD), a novel dimension reduction framework for tensors with one functional\nmode and several tabular modes. The problem is motivated by high-order\nlongitudinal data analysis. Our model assumes the observed data to be a random\nrealization of an approximate CP low-rank functional tensor measured on a\ndiscrete time grid. Incorporating tensor algebra and the theory of Reproducing\nKernel Hilbert Space (RKHS), we propose a novel RKHS-based constrained power\niteration with spectral initialization. Our method can successfully estimate\nboth singular vectors and functions of the low-rank structure in the observed\ndata. With mild assumptions, we establish the non-asymptotic contractive error\nbounds for the proposed algorithm. The superiority of the proposed framework is\ndemonstrated via extensive experiments on both simulated and real data."}, "http://arxiv.org/abs/2202.02146": {"title": "Elastic Gradient Descent, an Iterative Optimization Method Approximating the Solution Paths of the Elastic Net", "link": "http://arxiv.org/abs/2202.02146", "description": "The elastic net combines lasso and ridge regression to fuse the sparsity\nproperty of lasso with the grouping property of ridge regression. The\nconnections between ridge regression and gradient descent and between lasso and\nforward stagewise regression have previously been shown. Similar to how the\nelastic net generalizes lasso and ridge regression, we introduce elastic\ngradient descent, a generalization of gradient descent and forward stagewise\nregression. We theoretically analyze elastic gradient descent and compare it to\nthe elastic net and forward stagewise regression. Parts of the analysis are\nbased on elastic gradient flow, a piecewise analytical construction, obtained\nfor elastic gradient descent with infinitesimal step size. We also compare\nelastic gradient descent to the elastic net on real and simulated data and show\nthat it provides similar solution paths, but is several orders of magnitude\nfaster. Compared to forward stagewise regression, elastic gradient descent\nselects a model that, although still sparse, provides considerably lower\nprediction and estimation errors."}, "http://arxiv.org/abs/2202.03897": {"title": "Inference from Sampling with Response Probabilities Estimated via Calibration", "link": "http://arxiv.org/abs/2202.03897", "description": "A solution to control for nonresponse bias consists of multiplying the design\nweights of respondents by the inverse of estimated response probabilities to\ncompensate for the nonrespondents. Maximum likelihood and calibration are two\napproaches that can be applied to obtain estimated response probabilities. We\nconsider a common framework in which these approaches can be compared. We\ndevelop an asymptotic study of the behavior of the resulting estimator when\ncalibration is applied. A logistic regression model for the response\nprobabilities is postulated. Missing at random and unclustered data are\nsupposed. Three main contributions of this work are: 1) we show that the\nestimators with the response probabilities estimated via calibration are\nasymptotically equivalent to unbiased estimators and that a gain in efficiency\nis obtained when estimating the response probabilities via calibration as\ncompared to the estimator with the true response probabilities, 2) we show that\nthe estimators with the response probabilities estimated via calibration are\ndoubly robust to model misspecification and explain why double robustness is\nnot guaranteed when maximum likelihood is applied, and 3) we discuss and\nillustrate problems related to response probabilities estimation, namely\nexistence of a solution to the estimating equations, problems of convergence,\nand extreme weights. We explain and illustrate why the first aforementioned\nproblem is more likely with calibration than with maximum likelihood\nestimation. We present the results of a simulation study in order to illustrate\nthese elements."}, "http://arxiv.org/abs/2208.14951": {"title": "Statistical inference for multivariate extremes via a geometric approach", "link": "http://arxiv.org/abs/2208.14951", "description": "A geometric representation for multivariate extremes, based on the shapes of\nscaled sample clouds in light-tailed margins and their so-called limit sets,\nhas recently been shown to connect several existing extremal dependence\nconcepts. However, these results are purely probabilistic, and the geometric\napproach itself has not been fully exploited for statistical inference. We\noutline a method for parametric estimation of the limit set shape, which\nincludes a useful non/semi-parametric estimate as a pre-processing step. More\nfundamentally, our approach provides a new class of asymptotically-motivated\nstatistical models for the tails of multivariate distributions, and such models\ncan accommodate any combination of simultaneous or non-simultaneous extremes\nthrough appropriate parametric forms for the limit set shape. Extrapolation\nfurther into the tail of the distribution is possible via simulation from the\nfitted model. A simulation study confirms that our methodology is very\ncompetitive with existing approaches, and can successfully allow estimation of\nsmall probabilities in regions where other methods struggle. We apply the\nmethodology to two environmental datasets, with diagnostics demonstrating a\ngood fit."}, "http://arxiv.org/abs/2209.08889": {"title": "Inference of nonlinear causal effects with GWAS summary data", "link": "http://arxiv.org/abs/2209.08889", "description": "Large-scale genome-wide association studies (GWAS) have offered an exciting\nopportunity to discover putative causal genes or risk factors associated with\ndiseases by using SNPs as instrumental variables (IVs). However, conventional\napproaches assume linear causal relations partly for simplicity and partly for\nthe availability of GWAS summary data. In this work, we propose a novel model\n{for transcriptome-wide association studies (TWAS)} to incorporate nonlinear\nrelationships across IVs, an exposure/gene, and an outcome, which is robust\nagainst violations of the valid IV assumptions, permits the use of GWAS summary\ndata, and covers two-stage least squares as a special case. We decouple the\nestimation of a marginal causal effect and a nonlinear transformation, where\nthe former is estimated via sliced inverse regression and a sparse instrumental\nvariable regression, and the latter is estimated by a ratio-adjusted inverse\nregression. On this ground, we propose an inferential procedure. An application\nof the proposed method to the ADNI gene expression data and the IGAP GWAS\nsummary data identifies 18 causal genes associated with Alzheimer's disease,\nincluding APOE and TOMM40, in addition to 7 other genes missed by two-stage\nleast squares considering only linear relationships. Our findings suggest that\nnonlinear modeling is required to unleash the power of IV regression for\nidentifying potentially nonlinear gene-trait associations. Accompanying this\npaper is our Python library \\texttt{nl-causal}\n(\\url{https://nonlinear-causal.readthedocs.io/}) that implements the proposed\nmethod."}, "http://arxiv.org/abs/2301.03038": {"title": "Skewed Bernstein-von Mises theorem and skew-modal approximations", "link": "http://arxiv.org/abs/2301.03038", "description": "Gaussian approximations are routinely employed in Bayesian statistics to ease\ninference when the target posterior is intractable. Although these\napproximations are asymptotically justified by Bernstein-von Mises type\nresults, in practice the expected Gaussian behavior may poorly represent the\nshape of the posterior, thus affecting approximation accuracy. Motivated by\nthese considerations, we derive an improved class of closed-form approximations\nof posterior distributions which arise from a new treatment of a third-order\nversion of the Laplace method yielding approximations in a tractable family of\nskew-symmetric distributions. Under general assumptions which account for\nmisspecified models and non-i.i.d. settings, this family of approximations is\nshown to have a total variation distance from the target posterior whose rate\nof convergence improves by at least one order of magnitude the one established\nby the classical Bernstein-von Mises theorem. Specializing this result to the\ncase of regular parametric models shows that the same improvement in\napproximation accuracy can be also derived for polynomially bounded posterior\nfunctionals. Unlike other higher-order approximations, our results prove that\nit is possible to derive closed-form and valid densities which are expected to\nprovide, in practice, a more accurate, yet similarly-tractable, alternative to\nGaussian approximations of the target posterior, while inheriting its limiting\nfrequentist properties. We strengthen such arguments by developing a practical\nskew-modal approximation for both joint and marginal posteriors that achieves\nthe same theoretical guarantees of its theoretical counterpart by replacing the\nunknown model parameters with the corresponding MAP estimate. Empirical studies\nconfirm that our theoretical results closely match the remarkable performance\nobserved in practice, even in finite, possibly small, sample regimes."}, "http://arxiv.org/abs/2303.05878": {"title": "Identification and Estimation of Causal Effects with Confounders Missing Not at Random", "link": "http://arxiv.org/abs/2303.05878", "description": "Making causal inferences from observational studies can be challenging when\nconfounders are missing not at random. In such cases, identifying causal\neffects is often not guaranteed. Motivated by a real example, we consider a\ntreatment-independent missingness assumption under which we establish the\nidentification of causal effects when confounders are missing not at random. We\npropose a weighted estimating equation (WEE) approach for estimating model\nparameters and introduce three estimators for the average causal effect, based\non regression, propensity score weighting, and doubly robust estimation. We\nevaluate the performance of these estimators through simulations, and provide a\nreal data analysis to illustrate our proposed method."}, "http://arxiv.org/abs/2305.12283": {"title": "Distribution-Free Model-Agnostic Regression Calibration via Nonparametric Methods", "link": "http://arxiv.org/abs/2305.12283", "description": "In this paper, we consider the uncertainty quantification problem for\nregression models. Specifically, we consider an individual calibration\nobjective for characterizing the quantiles of the prediction model. While such\nan objective is well-motivated from downstream tasks such as newsvendor cost,\nthe existing methods have been largely heuristic and lack of statistical\nguarantee in terms of individual calibration. We show via simple examples that\nthe existing methods focusing on population-level calibration guarantees such\nas average calibration or sharpness can lead to harmful and unexpected results.\nWe propose simple nonparametric calibration methods that are agnostic of the\nunderlying prediction model and enjoy both computational efficiency and\nstatistical consistency. Our approach enables a better understanding of the\npossibility of individual calibration, and we establish matching upper and\nlower bounds for the calibration error of our proposed methods. Technically,\nour analysis combines the nonparametric analysis with a covering number\nargument for parametric analysis, which advances the existing theoretical\nanalyses in the literature of nonparametric density estimation and quantile\nbandit problems. Importantly, the nonparametric perspective sheds new\ntheoretical insights into regression calibration in terms of the curse of\ndimensionality and reconciles the existing results on the impossibility of\nindividual calibration. To our knowledge, we make the first effort to reach\nboth individual calibration and finite-sample guarantee with minimal\nassumptions in terms of conformal prediction. Numerical experiments show the\nadvantage of such a simple approach under various metrics, and also under\ncovariates shift. We hope our work provides a simple benchmark and a starting\npoint of theoretical ground for future research on regression calibration."}, "http://arxiv.org/abs/2305.14943": {"title": "Learning Rate Free Bayesian Inference in Constrained Domains", "link": "http://arxiv.org/abs/2305.14943", "description": "We introduce a suite of new particle-based algorithms for sampling on\nconstrained domains which are entirely learning rate free. Our approach\nleverages coin betting ideas from convex optimisation, and the viewpoint of\nconstrained sampling as a mirrored optimisation problem on the space of\nprobability measures. Based on this viewpoint, we also introduce a unifying\nframework for several existing constrained sampling algorithms, including\nmirrored Langevin dynamics and mirrored Stein variational gradient descent. We\ndemonstrate the performance of our algorithms on a range of numerical examples,\nincluding sampling from targets on the simplex, sampling with fairness\nconstraints, and constrained sampling problems in post-selection inference. Our\nresults indicate that our algorithms achieve competitive performance with\nexisting constrained sampling methods, without the need to tune any\nhyperparameters."}, "http://arxiv.org/abs/2308.07983": {"title": "Monte Carlo guided Diffusion for Bayesian linear inverse problems", "link": "http://arxiv.org/abs/2308.07983", "description": "Ill-posed linear inverse problems arise frequently in various applications,\nfrom computational photography to medical imaging. A recent line of research\nexploits Bayesian inference with informative priors to handle the ill-posedness\nof such problems. Amongst such priors, score-based generative models (SGM) have\nrecently been successfully applied to several different inverse problems. In\nthis study, we exploit the particular structure of the prior defined by the SGM\nto define a sequence of intermediate linear inverse problems. As the noise\nlevel decreases, the posteriors of these inverse problems get closer to the\ntarget posterior of the original inverse problem. To sample from this sequence\nof posteriors, we propose the use of Sequential Monte Carlo (SMC) methods. The\nproposed algorithm, MCGDiff, is shown to be theoretically grounded and we\nprovide numerical simulations showing that it outperforms competing baselines\nwhen dealing with ill-posed inverse problems in a Bayesian setting."}, "http://arxiv.org/abs/2308.12485": {"title": "Optimal Shrinkage Estimation of Fixed Effects in Linear Panel Data Models", "link": "http://arxiv.org/abs/2308.12485", "description": "Shrinkage methods are frequently used to estimate fixed effects to reduce the\nnoisiness of the least squares estimators. However, widely used shrinkage\nestimators guarantee such noise reduction only under strong distributional\nassumptions. I develop an estimator for the fixed effects that obtains the best\npossible mean squared error within a class of shrinkage estimators. This class\nincludes conventional shrinkage estimators and the optimality does not require\ndistributional assumptions. The estimator has an intuitive form and is easy to\nimplement. Moreover, the fixed effects are allowed to vary with time and to be\nserially correlated, and the shrinkage optimally incorporates the underlying\ncorrelation structure in this case. In such a context, I also provide a method\nto forecast fixed effects one period ahead."}, "http://arxiv.org/abs/2309.16843": {"title": "A Mean Field Approach to Empirical Bayes Estimation in High-dimensional Linear Regression", "link": "http://arxiv.org/abs/2309.16843", "description": "We study empirical Bayes estimation in high-dimensional linear regression. To\nfacilitate computationally efficient estimation of the underlying prior, we\nadopt a variational empirical Bayes approach, introduced originally in\nCarbonetto and Stephens (2012) and Kim et al. (2022). We establish asymptotic\nconsistency of the nonparametric maximum likelihood estimator (NPMLE) and its\n(computable) naive mean field variational surrogate under mild assumptions on\nthe design and the prior. Assuming, in addition, that the naive mean field\napproximation has a dominant optimizer, we develop a computationally efficient\napproximation to the oracle posterior distribution, and establish its accuracy\nunder the 1-Wasserstein metric. This enables computationally feasible Bayesian\ninference; e.g., construction of posterior credible intervals with an average\ncoverage guarantee, Bayes optimal estimation for the regression coefficients,\nestimation of the proportion of non-nulls, etc. Our analysis covers both\ndeterministic and random designs, and accommodates correlations among the\nfeatures. To the best of our knowledge, this provides the first rigorous\nnonparametric empirical Bayes method in a high-dimensional regression setting\nwithout sparsity."}, "http://arxiv.org/abs/2310.17679": {"title": "Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score Search and Grow-Shrink Trees", "link": "http://arxiv.org/abs/2310.17679", "description": "Learning graphical conditional independence structures is an important\nmachine learning problem and a cornerstone of causal discovery. However, the\naccuracy and execution time of learning algorithms generally struggle to scale\nto problems with hundreds of highly connected variables -- for instance,\nrecovering brain networks from fMRI data. We introduce the best order score\nsearch (BOSS) and grow-shrink trees (GSTs) for learning directed acyclic graphs\n(DAGs) in this paradigm. BOSS greedily searches over permutations of variables,\nusing GSTs to construct and score DAGs from permutations. GSTs efficiently\ncache scores to eliminate redundant calculations. BOSS achieves\nstate-of-the-art performance in accuracy and execution time, comparing\nfavorably to a variety of combinatorial and gradient-based learning algorithms\nunder a broad range of conditions. To demonstrate its practicality, we apply\nBOSS to two sets of resting-state fMRI data: simulated data with\npseudo-empirical noise distributions derived from randomized empirical fMRI\ncortical signals and clinical data from 3T fMRI scans processed into cortical\nparcels. BOSS is available for use within the TETRAD project which includes\nPython and R wrappers."}, "http://arxiv.org/abs/2310.17712": {"title": "Community Detection and Classification Guarantees Using Embeddings Learned by Node2Vec", "link": "http://arxiv.org/abs/2310.17712", "description": "Embedding the nodes of a large network into an Euclidean space is a common\nobjective in modern machine learning, with a variety of tools available. These\nembeddings can then be used as features for tasks such as community\ndetection/node clustering or link prediction, where they achieve state of the\nart performance. With the exception of spectral clustering methods, there is\nlittle theoretical understanding for other commonly used approaches to learning\nembeddings. In this work we examine the theoretical properties of the\nembeddings learned by node2vec. Our main result shows that the use of k-means\nclustering on the embedding vectors produced by node2vec gives weakly\nconsistent community recovery for the nodes in (degree corrected) stochastic\nblock models. We also discuss the use of these embeddings for node and link\nprediction tasks. We demonstrate this result empirically, and examine how this\nrelates to other embedding tools for network data."}, "http://arxiv.org/abs/2310.17760": {"title": "Novel Models for Multiple Dependent Heteroskedastic Time Series", "link": "http://arxiv.org/abs/2310.17760", "description": "Functional magnetic resonance imaging or functional MRI (fMRI) is a very\npopular tool used for differing brain regions by measuring brain activity. It\nis affected by physiological noise, such as head and brain movement in the\nscanner from breathing, heart beats, or the subject fidgeting. The purpose of\nthis paper is to propose a novel approach to handling fMRI data for infants\nwith high volatility caused by sudden head movements. Another purpose is to\nevaluate the volatility modelling performance of multiple dependent fMRI time\nseries data. The models examined in this paper are AR and GARCH and the\nmodelling performance is evaluated by several statistical performance measures.\nThe conclusions of this paper are that multiple dependent fMRI series data can\nbe fitted with AR + GARCH model if the multiple fMRI data have many sudden head\nmovements. The GARCH model can capture the shared volatility clustering caused\nby head movements across brain regions. However, the multiple fMRI data without\nmany head movements have fitted AR + GARCH model with different performance.\nThe conclusions are supported by statistical tests and measures. This paper\nhighlights the difference between the proposed approach from traditional\napproaches when estimating model parameters and modelling conditional variances\non multiple dependent time series. In the future, the proposed approach can be\napplied to other research fields, such as financial economics, and signal\nprocessing. Code is available at \\url{https://github.<a href=\"https://export.arxiv.org/abs/com/1320494\">com/1320494</a>2/STAT40710}."}, "http://arxiv.org/abs/2310.17766": {"title": "Minibatch Markov chain Monte Carlo Algorithms for Fitting Gaussian Processes", "link": "http://arxiv.org/abs/2310.17766", "description": "Gaussian processes (GPs) are a highly flexible, nonparametric statistical\nmodel that are commonly used to fit nonlinear relationships or account for\ncorrelation between observations. However, the computational load of fitting a\nGaussian process is $\\mathcal{O}(n^3)$ making them infeasible for use on large\ndatasets. To make GPs more feasible for large datasets, this research focuses\non the use of minibatching to estimate GP parameters. Specifically, we outline\nboth approximate and exact minibatch Markov chain Monte Carlo algorithms that\nsubstantially reduce the computation of fitting a GP by only considering small\nsubsets of the data at a time. We demonstrate and compare this methodology\nusing various simulations and real datasets."}, "http://arxiv.org/abs/2310.17806": {"title": "Transporting treatment effects from difference-in-differences studies", "link": "http://arxiv.org/abs/2310.17806", "description": "Difference-in-differences (DID) is a popular approach to identify the causal\neffects of treatments and policies in the presence of unmeasured confounding.\nDID identifies the sample average treatment effect in the treated (SATT).\nHowever, a goal of such research is often to inform decision-making in target\npopulations outside the treated sample. Transportability methods have been\ndeveloped to extend inferences from study samples to external target\npopulations; these methods have primarily been developed and applied in\nsettings where identification is based on conditional independence between the\ntreatment and potential outcomes, such as in a randomized trial. This paper\ndevelops identification and estimators for effects in a target population,\nbased on DID conducted in a study sample that differs from the target\npopulation. We present a range of assumptions under which one may identify\ncausal effects in the target population and employ causal diagrams to\nillustrate these assumptions. In most realistic settings, results depend\ncritically on the assumption that any unmeasured confounders are not effect\nmeasure modifiers on the scale of the effect of interest. We develop several\nestimators of transported effects, including a doubly robust estimator based on\nthe efficient influence function. Simulation results support theoretical\nproperties of the proposed estimators. We discuss the potential application of\nour approach to a study of the effects of a US federal smoke-free housing\npolicy, where the original study was conducted in New York City alone and the\ngoal is extend inferences to other US cities."}, "http://arxiv.org/abs/2310.17816": {"title": "Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs", "link": "http://arxiv.org/abs/2310.17816", "description": "This work addresses the problem of automated covariate selection under\nlimited prior knowledge. Given an exposure-outcome pair {X,Y} and a variable\nset Z of unknown causal structure, the Local Discovery by Partitioning (LDP)\nalgorithm partitions Z into subsets defined by their relation to {X,Y}. We\nenumerate eight exhaustive and mutually exclusive partitions of any arbitrary Z\nand leverage this taxonomy to differentiate confounders from other variable\ntypes. LDP is motivated by valid adjustment set identification, but avoids the\npretreatment assumption commonly made by automated covariate selection methods.\nWe provide theoretical guarantees that LDP returns a valid adjustment set for\nany Z that meets sufficient graphical conditions. Under stronger conditions, we\nprove that partition labels are asymptotically correct. Total independence\ntests is worst-case quadratic in |Z|, with sub-quadratic runtimes observed\nempirically. We numerically validate our theoretical guarantees on synthetic\nand semi-synthetic graphs. Adjustment sets from LDP yield less biased and more\nprecise average treatment effect estimates than baselines, with LDP\noutperforming on confounder recall, test count, and runtime for valid\nadjustment set discovery."}, "http://arxiv.org/abs/2310.17820": {"title": "Sparse Bayesian Multidimensional Item Response Theory", "link": "http://arxiv.org/abs/2310.17820", "description": "Multivariate Item Response Theory (MIRT) is sought-after widely by applied\nresearchers looking for interpretable (sparse) explanations underlying response\npatterns in questionnaire data. There is, however, an unmet demand for such\nsparsity discovery tools in practice. Our paper develops a Bayesian platform\nfor binary and ordinal item MIRT which requires minimal tuning and scales well\non relatively large datasets due to its parallelizable features. Bayesian\nmethodology for MIRT models has traditionally relied on MCMC simulation, which\ncannot only be slow in practice, but also often renders exact sparsity recovery\nimpossible without additional thresholding. In this work, we develop a scalable\nBayesian EM algorithm to estimate sparse factor loadings from binary and\nordinal item responses. We address the seemingly insurmountable problem of\nunknown latent factor dimensionality with tools from Bayesian nonparametrics\nwhich enable estimating the number of factors. Rotations to sparsity through\nparameter expansion further enhance convergence and interpretability without\nidentifiability constraints. In our simulation study, we show that our method\nreliably recovers both the factor dimensionality as well as the latent\nstructure on high-dimensional synthetic data even for small samples. We\ndemonstrate the practical usefulness of our approach on two datasets: an\neducational item response dataset and a quality-of-life measurement dataset.\nBoth demonstrations show that our tool yields interpretable estimates,\nfacilitating interesting discoveries that might otherwise go unnoticed under a\npure confirmatory factor analysis setting. We provide an easy-to-use software\nwhich is a useful new addition to the MIRT toolkit and which will hopefully\nserve as the go-to method for practitioners."}, "http://arxiv.org/abs/2310.17845": {"title": "A Unified and Optimal Multiple Testing Framework based on rho-values", "link": "http://arxiv.org/abs/2310.17845", "description": "Multiple testing is an important research direction that has gained major\nattention in recent years. Currently, most multiple testing procedures are\ndesigned with p-values or Local false discovery rate (Lfdr) statistics.\nHowever, p-values obtained by applying probability integral transform to some\nwell-known test statistics often do not incorporate information from the\nalternatives, resulting in suboptimal procedures. On the other hand, Lfdr based\nprocedures can be asymptotically optimal but their guarantee on false discovery\nrate (FDR) control relies on consistent estimation of Lfdr, which is often\ndifficult in practice especially when the incorporation of side information is\ndesirable. In this article, we propose a novel and flexibly constructed class\nof statistics, called rho-values, which combines the merits of both p-values\nand Lfdr while enjoys superiorities over methods based on these two types of\nstatistics. Specifically, it unifies these two frameworks and operates in two\nsteps, ranking and thresholding. The ranking produced by rho-values mimics that\nproduced by Lfdr statistics, and the strategy for choosing the threshold is\nsimilar to that of p-value based procedures. Therefore, the proposed framework\nguarantees FDR control under weak assumptions; it maintains the integrity of\nthe structural information encoded by the summary statistics and the auxiliary\ncovariates and hence can be asymptotically optimal. We demonstrate the efficacy\nof the new framework through extensive simulations and two data applications."}, "http://arxiv.org/abs/2310.17999": {"title": "Automated threshold selection and associated inference uncertainty for univariate extremes", "link": "http://arxiv.org/abs/2310.17999", "description": "Threshold selection is a fundamental problem in any threshold-based extreme\nvalue analysis. While models are asymptotically motivated, selecting an\nappropriate threshold for finite samples can be difficult through standard\nmethods. Inference can also be highly sensitive to the choice of threshold. Too\nlow a threshold choice leads to bias in the fit of the extreme value model,\nwhile too high a choice leads to unnecessary additional uncertainty in the\nestimation of model parameters. In this paper, we develop a novel methodology\nfor automated threshold selection that directly tackles this bias-variance\ntrade-off. We also develop a method to account for the uncertainty in this\nthreshold choice and propagate this uncertainty through to high quantile\ninference. Through a simulation study, we demonstrate the effectiveness of our\nmethod for threshold selection and subsequent extreme quantile estimation. We\napply our method to the well-known, troublesome example of the River Nidd\ndataset."}, "http://arxiv.org/abs/2310.18027": {"title": "Bayesian Prognostic Covariate Adjustment With Additive Mixture Priors", "link": "http://arxiv.org/abs/2310.18027", "description": "Effective and rapid decision-making from randomized controlled trials (RCTs)\nrequires unbiased and precise treatment effect inferences. Two strategies to\naddress this requirement are to adjust for covariates that are highly\ncorrelated with the outcome, and to leverage historical control information via\nBayes' theorem. We propose a new Bayesian prognostic covariate adjustment\nmethodology, referred to as Bayesian PROCOVA, that combines these two\nstrategies. Covariate adjustment is based on generative artificial intelligence\n(AI) algorithms that construct a digital twin generator (DTG) for RCT\nparticipants. The DTG is trained on historical control data and yields a\ndigital twin (DT) probability distribution for each participant's control\noutcome. The expectation of the DT distribution defines the single covariate\nfor adjustment. Historical control information are leveraged via an additive\nmixture prior with two components: an informative prior probability\ndistribution specified based on historical control data, and a non-informative\nprior distribution. The weight parameter in the mixture has a prior\ndistribution as well, so that the entire additive mixture prior distribution is\ncompletely pre-specifiable and does not involve any information from the RCT.\nWe establish an efficient Gibbs algorithm for sampling from the posterior\ndistribution, and derive closed-form expressions for the posterior mean and\nvariance of the treatment effect conditional on the weight parameter, of\nBayesian PROCOVA. We evaluate the bias control and variance reduction of\nBayesian PROCOVA compared to frequentist prognostic covariate adjustment\n(PROCOVA) via simulation studies that encompass different types of\ndiscrepancies between the historical control and RCT data. Ultimately, Bayesian\nPROCOVA can yield informative treatment effect inferences with fewer control\nparticipants, accelerating effective decision-making."}, "http://arxiv.org/abs/2310.18047": {"title": "Robust Bayesian Inference on Riemannian Submanifold", "link": "http://arxiv.org/abs/2310.18047", "description": "Non-Euclidean spaces routinely arise in modern statistical applications such\nas in medical imaging, robotics, and computer vision, to name a few. While\ntraditional Bayesian approaches are applicable to such settings by considering\nan ambient Euclidean space as the parameter space, we demonstrate the benefits\nof integrating manifold structure into the Bayesian framework, both\ntheoretically and computationally. Moreover, existing Bayesian approaches which\nare designed specifically for manifold-valued parameters are primarily\nmodel-based, which are typically subject to inaccurate uncertainty\nquantification under model misspecification. In this article, we propose a\nrobust model-free Bayesian inference for parameters defined on a Riemannian\nsubmanifold, which is shown to provide valid uncertainty quantification from a\nfrequentist perspective. Computationally, we propose a Markov chain Monte Carlo\nto sample from the posterior on the Riemannian submanifold, where the mixing\ntime, in the large sample regime, is shown to depend only on the intrinsic\ndimension of the parameter space instead of the potentially much larger ambient\ndimension. Our numerical results demonstrate the effectiveness of our approach\non a variety of problems, such as reduced-rank multiple quantile regression,\nprincipal component analysis, and Fr\\'{e}chet mean estimation."}, "http://arxiv.org/abs/2310.18108": {"title": "Transductive conformal inference with adaptive scores", "link": "http://arxiv.org/abs/2310.18108", "description": "Conformal inference is a fundamental and versatile tool that provides\ndistribution-free guarantees for many machine learning tasks. We consider the\ntransductive setting, where decisions are made on a test sample of $m$ new\npoints, giving rise to $m$ conformal $p$-values. {While classical results only\nconcern their marginal distribution, we show that their joint distribution\nfollows a P\\'olya urn model, and establish a concentration inequality for their\nempirical distribution function.} The results hold for arbitrary exchangeable\nscores, including {\\it adaptive} ones that can use the covariates of the\ntest+calibration samples at training stage for increased accuracy. We\ndemonstrate the usefulness of these theoretical results through uniform,\nin-probability guarantees for two machine learning tasks of current interest:\ninterval prediction for transductive transfer learning and novelty detection\nbased on two-class classification."}, "http://arxiv.org/abs/2310.18212": {"title": "Robustness of Algorithms for Causal Structure Learning to Hyperparameter Choice", "link": "http://arxiv.org/abs/2310.18212", "description": "Hyperparameters play a critical role in machine learning. Hyperparameter\ntuning can make the difference between state-of-the-art and poor prediction\nperformance for any algorithm, but it is particularly challenging for structure\nlearning due to its unsupervised nature. As a result, hyperparameter tuning is\noften neglected in favour of using the default values provided by a particular\nimplementation of an algorithm. While there have been numerous studies on\nperformance evaluation of causal discovery algorithms, how hyperparameters\naffect individual algorithms, as well as the choice of the best algorithm for a\nspecific problem, has not been studied in depth before. This work addresses\nthis gap by investigating the influence of hyperparameters on causal structure\nlearning tasks. Specifically, we perform an empirical evaluation of\nhyperparameter selection for some seminal learning algorithms on datasets of\nvarying levels of complexity. We find that, while the choice of algorithm\nremains crucial to obtaining state-of-the-art performance, hyperparameter\nselection in ensemble settings strongly influences the choice of algorithm, in\nthat a poor choice of hyperparameters can lead to analysts using algorithms\nwhich do not give state-of-the-art performance for their data."}, "http://arxiv.org/abs/2310.18261": {"title": "Label Shift Estimators for Non-Ignorable Missing Data", "link": "http://arxiv.org/abs/2310.18261", "description": "We consider the problem of estimating the mean of a random variable Y subject\nto non-ignorable missingness, i.e., where the missingness mechanism depends on\nY . We connect the auxiliary proxy variable framework for non-ignorable\nmissingness (West and Little, 2013) to the label shift setting (Saerens et al.,\n2002). Exploiting this connection, we construct an estimator for non-ignorable\nmissing data that uses high-dimensional covariates (or proxies) without the\nneed for a generative model. In synthetic and semi-synthetic experiments, we\nstudy the behavior of the proposed estimator, comparing it to commonly used\nignorable estimators in both well-specified and misspecified settings.\nAdditionally, we develop a score to assess how consistent the data are with the\nlabel shift assumption. We use our approach to estimate disease prevalence\nusing a large health survey, comparing ignorable and non-ignorable approaches.\nWe show that failing to account for non-ignorable missingness can have profound\nconsequences on conclusions drawn from non-representative samples."}, "http://arxiv.org/abs/2102.12698": {"title": "Improving the Hosmer-Lemeshow Goodness-of-Fit Test in Large Models with Replicated Trials", "link": "http://arxiv.org/abs/2102.12698", "description": "The Hosmer-Lemeshow (HL) test is a commonly used global goodness-of-fit (GOF)\ntest that assesses the quality of the overall fit of a logistic regression\nmodel. In this paper, we give results from simulations showing that the type 1\nerror rate (and hence power) of the HL test decreases as model complexity\ngrows, provided that the sample size remains fixed and binary replicates are\npresent in the data. We demonstrate that the generalized version of the HL test\nby Surjanovic et al. (2020) can offer some protection against this power loss.\nWe conclude with a brief discussion explaining the behaviour of the HL test,\nalong with some guidance on how to choose between the two tests."}, "http://arxiv.org/abs/2110.04852": {"title": "Mixture representations and Bayesian nonparametric inference for likelihood ratio ordered distributions", "link": "http://arxiv.org/abs/2110.04852", "description": "In this article, we introduce mixture representations for likelihood ratio\nordered distributions. Essentially, the ratio of two probability densities, or\nmass functions, is monotone if and only if one can be expressed as a mixture of\none-sided truncations of the other. To illustrate the practical value of the\nmixture representations, we address the problem of density estimation for\nlikelihood ratio ordered distributions. In particular, we propose a\nnonparametric Bayesian solution which takes advantage of the mixture\nrepresentations. The prior distribution is constructed from Dirichlet process\nmixtures and has large support on the space of pairs of densities satisfying\nthe monotone ratio constraint. Posterior consistency holds under reasonable\nconditions on the prior specification and the true unknown densities. To our\nknowledge, this is the first posterior consistency result in the literature on\norder constrained inference. With a simple modification to the prior\ndistribution, we can test the equality of two distributions against the\nalternative of likelihood ratio ordering. We develop a Markov chain Monte Carlo\nalgorithm for posterior inference and demonstrate the method in a biomedical\napplication."}, "http://arxiv.org/abs/2207.08911": {"title": "Deeply-Learned Generalized Linear Models with Missing Data", "link": "http://arxiv.org/abs/2207.08911", "description": "Deep Learning (DL) methods have dramatically increased in popularity in\nrecent years, with significant growth in their application to supervised\nlearning problems in the biomedical sciences. However, the greater prevalence\nand complexity of missing data in modern biomedical datasets present\nsignificant challenges for DL methods. Here, we provide a formal treatment of\nmissing data in the context of deeply learned generalized linear models, a\nsupervised DL architecture for regression and classification problems. We\npropose a new architecture, \\textit{dlglm}, that is one of the first to be able\nto flexibly account for both ignorable and non-ignorable patterns of\nmissingness in input features and response at training time. We demonstrate\nthrough statistical simulation that our method outperforms existing approaches\nfor supervised learning tasks in the presence of missing not at random (MNAR)\nmissingness. We conclude with a case study of a Bank Marketing dataset from the\nUCI Machine Learning Repository, in which we predict whether clients subscribed\nto a product based on phone survey data. Supplementary materials for this\narticle are available online."}, "http://arxiv.org/abs/2208.04627": {"title": "Causal Effect Identification in Uncertain Causal Networks", "link": "http://arxiv.org/abs/2208.04627", "description": "Causal identification is at the core of the causal inference literature,\nwhere complete algorithms have been proposed to identify causal queries of\ninterest. The validity of these algorithms hinges on the restrictive assumption\nof having access to a correctly specified causal structure. In this work, we\nstudy the setting where a probabilistic model of the causal structure is\navailable. Specifically, the edges in a causal graph exist with uncertainties\nwhich may, for example, represent degree of belief from domain experts.\nAlternatively, the uncertainty about an edge may reflect the confidence of a\nparticular statistical test. The question that naturally arises in this setting\nis: Given such a probabilistic graph and a specific causal effect of interest,\nwhat is the subgraph which has the highest plausibility and for which the\ncausal effect is identifiable? We show that answering this question reduces to\nsolving an NP-complete combinatorial optimization problem which we call the\nedge ID problem. We propose efficient algorithms to approximate this problem\nand evaluate them against both real-world networks and randomly generated\ngraphs."}, "http://arxiv.org/abs/2211.00268": {"title": "Stacking designs: designing multi-fidelity computer experiments with target predictive accuracy", "link": "http://arxiv.org/abs/2211.00268", "description": "In an era where scientific experiments can be very costly, multi-fidelity\nemulators provide a useful tool for cost-efficient predictive scientific\ncomputing. For scientific applications, the experimenter is often limited by a\ntight computational budget, and thus wishes to (i) maximize predictive power of\nthe multi-fidelity emulator via a careful design of experiments, and (ii)\nensure this model achieves a desired error tolerance with some notion of\nconfidence. Existing design methods, however, do not jointly tackle objectives\n(i) and (ii). We propose a novel stacking design approach that addresses both\ngoals. A multi-level reproducing kernel Hilbert space (RKHS) interpolator is\nfirst introduced to build the emulator, under which our stacking design\nprovides a sequential approach for designing multi-fidelity runs such that a\ndesired prediction error of $\\epsilon &gt; 0$ is met under regularity assumptions.\nWe then prove a novel cost complexity theorem that, under this multi-level\ninterpolator, establishes a bound on the computation cost (for training data\nsimulation) needed to achieve a prediction bound of $\\epsilon$. This result\nprovides novel insights on conditions under which the proposed multi-fidelity\napproach improves upon a conventional RKHS interpolator which relies on a\nsingle fidelity level. Finally, we demonstrate the effectiveness of stacking\ndesigns in a suite of simulation experiments and an application to finite\nelement analysis."}, "http://arxiv.org/abs/2211.05357": {"title": "Bayesian score calibration for approximate models", "link": "http://arxiv.org/abs/2211.05357", "description": "Scientists continue to develop increasingly complex mechanistic models to\nreflect their knowledge more realistically. Statistical inference using these\nmodels can be challenging since the corresponding likelihood function is often\nintractable and model simulation may be computationally burdensome.\nFortunately, in many of these situations, it is possible to adopt a surrogate\nmodel or approximate likelihood function. It may be convenient to conduct\nBayesian inference directly with the surrogate, but this can result in bias and\npoor uncertainty quantification. In this paper we propose a new method for\nadjusting approximate posterior samples to reduce bias and produce more\naccurate uncertainty quantification. We do this by optimizing a transform of\nthe approximate posterior that maximizes a scoring rule. Our approach requires\nonly a (fixed) small number of complex model simulations and is numerically\nstable. We demonstrate good performance of the new method on several examples\nof increasing complexity."}, "http://arxiv.org/abs/2302.00993": {"title": "Unpaired Multi-Domain Causal Representation Learning", "link": "http://arxiv.org/abs/2302.00993", "description": "The goal of causal representation learning is to find a representation of\ndata that consists of causally related latent variables. We consider a setup\nwhere one has access to data from multiple domains that potentially share a\ncausal representation. Crucially, observations in different domains are assumed\nto be unpaired, that is, we only observe the marginal distribution in each\ndomain but not their joint distribution. In this paper, we give sufficient\nconditions for identifiability of the joint distribution and the shared causal\ngraph in a linear setup. Identifiability holds if we can uniquely recover the\njoint distribution and the shared causal representation from the marginal\ndistributions in each domain. We transform our identifiability results into a\npractical method to recover the shared latent causal graph."}, "http://arxiv.org/abs/2303.17277": {"title": "Cross-temporal probabilistic forecast reconciliation: Methodological and practical issues", "link": "http://arxiv.org/abs/2303.17277", "description": "Forecast reconciliation is a post-forecasting process that involves\ntransforming a set of incoherent forecasts into coherent forecasts which\nsatisfy a given set of linear constraints for a multivariate time series. In\nthis paper we extend the current state-of-the-art cross-sectional probabilistic\nforecast reconciliation approach to encompass a cross-temporal framework, where\ntemporal constraints are also applied. Our proposed methodology employs both\nparametric Gaussian and non-parametric bootstrap approaches to draw samples\nfrom an incoherent cross-temporal distribution. To improve the estimation of\nthe forecast error covariance matrix, we propose using multi-step residuals,\nespecially in the time dimension where the usual one-step residuals fail. To\naddress high-dimensionality issues, we present four alternatives for the\ncovariance matrix, where we exploit the two-fold nature (cross-sectional and\ntemporal) of the cross-temporal structure, and introduce the idea of\noverlapping residuals. We assess the effectiveness of the proposed\ncross-temporal reconciliation approaches through a simulation study that\ninvestigates their theoretical and empirical properties and two forecasting\nexperiments, using the Australian GDP and the Australian Tourism Demand\ndatasets. For both applications, the optimal cross-temporal reconciliation\napproaches significantly outperform the incoherent base forecasts in terms of\nthe Continuous Ranked Probability Score and the Energy Score. Overall, the\nresults highlight the potential of the proposed methods to improve the accuracy\nof probabilistic forecasts and to address the challenge of integrating\ndisparate scenarios while coherently taking into account short-term\noperational, medium-term tactical, and long-term strategic planning."}, "http://arxiv.org/abs/2309.07867": {"title": "Beta Diffusion", "link": "http://arxiv.org/abs/2309.07867", "description": "We introduce beta diffusion, a novel generative modeling method that\nintegrates demasking and denoising to generate data within bounded ranges.\nUsing scaled and shifted beta distributions, beta diffusion utilizes\nmultiplicative transitions over time to create both forward and reverse\ndiffusion processes, maintaining beta distributions in both the forward\nmarginals and the reverse conditionals, given the data at any point in time.\nUnlike traditional diffusion-based generative models relying on additive\nGaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is\nmultiplicative and optimized with KL-divergence upper bounds (KLUBs) derived\nfrom the convexity of the KL divergence. We demonstrate that the proposed KLUBs\nare more effective for optimizing beta diffusion compared to negative ELBOs,\nwhich can also be derived as the KLUBs of the same KL divergence with its two\narguments swapped. The loss function of beta diffusion, expressed in terms of\nBregman divergence, further supports the efficacy of KLUBs for optimization.\nExperimental results on both synthetic data and natural images demonstrate the\nunique capabilities of beta diffusion in generative modeling of range-bounded\ndata and validate the effectiveness of KLUBs in optimizing diffusion models,\nthereby making them valuable additions to the family of diffusion-based\ngenerative models and the optimization techniques used to train them."}, "http://arxiv.org/abs/2310.18422": {"title": "Inference via Wild Bootstrap and Multiple Imputation under Fine-Gray Models with Incomplete Data", "link": "http://arxiv.org/abs/2310.18422", "description": "Fine-Gray models specify the subdistribution hazards for one out of multiple\ncompeting risks to be proportional. The estimators of parameters and cumulative\nincidence functions under Fine-Gray models have a simpler structure when data\nare censoring-complete than when they are more generally incomplete. This paper\nconsiders the case of incomplete data but it exploits the above-mentioned\nsimpler estimator structure for which there exists a wild bootstrap approach\nfor inferential purposes. The present idea is to link the methodology under\ncensoring-completeness with the more general right-censoring regime with the\nhelp of multiple imputation. In a simulation study, this approach is compared\nto the estimation procedure proposed in the original paper by Fine and Gray\nwhen it is combined with a bootstrap approach. An application to a data set\nabout hospital-acquired infections illustrates the method."}, "http://arxiv.org/abs/2310.18474": {"title": "Robust Bayesian Graphical Regression Models for Assessing Tumor Heterogeneity in Proteomic Networks", "link": "http://arxiv.org/abs/2310.18474", "description": "Graphical models are powerful tools to investigate complex dependency\nstructures in high-throughput datasets. However, most existing graphical models\nmake one of the two canonical assumptions: (i) a homogeneous graph with a\ncommon network for all subjects; or (ii) an assumption of normality especially\nin the context of Gaussian graphical models. Both assumptions are restrictive\nand can fail to hold in certain applications such as proteomic networks in\ncancer. To this end, we propose an approach termed robust Bayesian graphical\nregression (rBGR) to estimate heterogeneous graphs for non-normally distributed\ndata. rBGR is a flexible framework that accommodates non-normality through\nrandom marginal transformations and constructs covariate-dependent graphs to\naccommodate heterogeneity through graphical regression techniques. We formulate\na new characterization of edge dependencies in such models called conditional\nsign independence with covariates along with an efficient posterior sampling\nalgorithm. In simulation studies, we demonstrate that rBGR outperforms existing\ngraphical regression models for data generated under various levels of\nnon-normality in both edge and covariate selection. We use rBGR to assess\nproteomic networks across two cancers: lung and ovarian, to systematically\ninvestigate the effects of immunogenic heterogeneity within tumors. Our\nanalyses reveal several important protein-protein interactions that are\ndifferentially impacted by the immune cell abundance; some corroborate existing\nbiological knowledge whereas others are novel findings."}, "http://arxiv.org/abs/2310.18500": {"title": "Designing Randomized Experiments to Predict Unit-Specific Treatment Effects", "link": "http://arxiv.org/abs/2310.18500", "description": "Typically, a randomized experiment is designed to test a hypothesis about the\naverage treatment effect and sometimes hypotheses about treatment effect\nvariation. The results of such a study may then be used to inform policy and\npractice for units not in the study. In this paper, we argue that given this\nuse, randomized experiments should instead be designed to predict unit-specific\ntreatment effects in a well-defined population. We then consider how different\nsampling processes and models affect the bias, variance, and mean squared\nprediction error of these predictions. The results indicate, for example, that\nproblems of generalizability (differences between samples and populations) can\ngreatly affect bias both in predictive models and in measures of error in these\nmodels. We also examine when the average treatment effect estimate outperforms\nunit-specific treatment effect predictive models and implications of this for\nplanning studies."}, "http://arxiv.org/abs/2310.18527": {"title": "Multiple Imputation Method for High-Dimensional Neuroimaging Data", "link": "http://arxiv.org/abs/2310.18527", "description": "Missingness is a common issue for neuroimaging data, and neglecting it in\ndownstream statistical analysis can introduce bias and lead to misguided\ninferential conclusions. It is therefore crucial to conduct appropriate\nstatistical methods to address this issue. While multiple imputation is a\npopular technique for handling missing data, its application to neuroimaging\ndata is hindered by high dimensionality and complex dependence structures of\nmultivariate neuroimaging variables. To tackle this challenge, we propose a\nnovel approach, named High Dimensional Multiple Imputation (HIMA), based on\nBayesian models. HIMA develops a new computational strategy for sampling large\ncovariance matrices based on a robustly estimated posterior mode, which\ndrastically enhances computational efficiency and numerical stability. To\nassess the effectiveness of HIMA, we conducted extensive simulation studies and\nreal-data analysis using neuroimaging data from a Schizophrenia study. HIMA\nshowcases a computational efficiency improvement of over 2000 times when\ncompared to traditional approaches, while also producing imputed datasets with\nimproved precision and stability."}, "http://arxiv.org/abs/2310.18533": {"title": "Evaluating the effects of high-throughput structural neuroimaging predictors on whole-brain functional connectome outcomes via network-based vector-on-matrix regression", "link": "http://arxiv.org/abs/2310.18533", "description": "The joint analysis of multimodal neuroimaging data is critical in the field\nof brain research because it reveals complex interactive relationships between\nneurobiological structures and functions. In this study, we focus on\ninvestigating the effects of structural imaging (SI) features, including white\nmatter micro-structure integrity (WMMI) and cortical thickness, on the whole\nbrain functional connectome (FC) network. To achieve this goal, we propose a\nnetwork-based vector-on-matrix regression model to characterize the FC-SI\nassociation patterns. We have developed a novel multi-level dense bipartite and\nclique subgraph extraction method to identify which subsets of spatially\nspecific SI features intensively influence organized FC sub-networks. The\nproposed method can simultaneously identify highly correlated\nstructural-connectomic association patterns and suppress false positive\nfindings while handling millions of potential interactions. We apply our method\nto a multimodal neuroimaging dataset of 4,242 participants from the UK Biobank\nto evaluate the effects of whole-brain WMMI and cortical thickness on the\nresting-state FC. The results reveal that the WMMI on corticospinal tracts and\ninferior cerebellar peduncle significantly affect functional connections of\nsensorimotor, salience, and executive sub-networks with an average correlation\nof 0.81 (p&lt;0.001)."}, "http://arxiv.org/abs/2310.18536": {"title": "Efficient Fully Bayesian Approach to Brain Activity Mapping with Complex-Valued fMRI Data", "link": "http://arxiv.org/abs/2310.18536", "description": "Functional magnetic resonance imaging (fMRI) enables indirect detection of\nbrain activity changes via the blood-oxygen-level-dependent (BOLD) signal.\nConventional analysis methods mainly rely on the real-valued magnitude of these\nsignals. In contrast, research suggests that analyzing both real and imaginary\ncomponents of the complex-valued fMRI (cv-fMRI) signal provides a more holistic\napproach that can increase power to detect neuronal activation. We propose a\nfully Bayesian model for brain activity mapping with cv-fMRI data. Our model\naccommodates temporal and spatial dynamics. Additionally, we propose a\ncomputationally efficient sampling algorithm, which enhances processing speed\nthrough image partitioning. Our approach is shown to be computationally\nefficient via image partitioning and parallel computation while being\ncompetitive with state-of-the-art methods. We support these claims with both\nsimulated numerical studies and an application to real cv-fMRI data obtained\nfrom a finger-tapping experiment."}, "http://arxiv.org/abs/2310.18556": {"title": "Design-Based Causal Inference with Missing Outcomes: Missingness Mechanisms, Imputation-Assisted Randomization Tests, and Covariate Adjustment", "link": "http://arxiv.org/abs/2310.18556", "description": "Design-based causal inference is one of the most widely used frameworks for\ntesting causal null hypotheses or inferring about causal parameters from\nexperimental or observational data. The most significant merit of design-based\ncausal inference is that its statistical validity only comes from the study\ndesign (e.g., randomization design) and does not require assuming any\noutcome-generating distributions or models. Although immune to model\nmisspecification, design-based causal inference can still suffer from other\ndata challenges, among which missingness in outcomes is a significant one.\nHowever, compared with model-based causal inference, outcome missingness in\ndesign-based causal inference is much less studied, largely due to the\nchallenge that design-based causal inference does not assume any outcome\ndistributions/models and, therefore, cannot directly adopt any existing\nmodel-based approaches for missing data. To fill this gap, we systematically\nstudy the missing outcomes problem in design-based causal inference. First, we\nuse the potential outcomes framework to clarify the minimal assumption\n(concerning the outcome missingness mechanism) needed for conducting\nfinite-population-exact randomization tests for the null effect (i.e., Fisher's\nsharp null) and that needed for constructing finite-population-exact confidence\nsets with missing outcomes. Second, we propose a general framework called\n``imputation and re-imputation\" for conducting finite-population-exact\nrandomization tests in design-based causal studies with missing outcomes. Our\nframework can incorporate any existing outcome imputation algorithms and\nmeanwhile guarantee finite-population-exact type-I error rate control. Third,\nwe extend our framework to conduct covariate adjustment in an exact\nrandomization test with missing outcomes and to construct\nfinite-population-exact confidence sets with missing outcomes."}, "http://arxiv.org/abs/2310.18563": {"title": "Covariate Balancing and the Equivalence of Weighting and Doubly Robust Estimators of Average Treatment Effects", "link": "http://arxiv.org/abs/2310.18563", "description": "We show that when the propensity score is estimated using a suitable\ncovariate balancing procedure, the commonly used inverse probability weighting\n(IPW) estimator, augmented inverse probability weighting (AIPW) with linear\nconditional mean, and inverse probability weighted regression adjustment\n(IPWRA) with linear conditional mean are all numerically the same for\nestimating the average treatment effect (ATE) or the average treatment effect\non the treated (ATT). Further, suitably chosen covariate balancing weights are\nautomatically normalized, which means that normalized and unnormalized versions\nof IPW and AIPW are identical. For estimating the ATE, the weights that achieve\nthe algebraic equivalence of IPW, AIPW, and IPWRA are based on propensity\nscores estimated using the inverse probability tilting (IPT) method of Graham,\nPinto and Egel (2012). For the ATT, the weights are obtained using the\ncovariate balancing propensity score (CBPS) method developed in Imai and\nRatkovic (2014). These equivalences also make covariate balancing methods\nattractive when the treatment is confounded and one is interested in the local\naverage treatment effect."}, "http://arxiv.org/abs/2310.18611": {"title": "Sequential Kalman filter for fast online changepoint detection in longitudinal health records", "link": "http://arxiv.org/abs/2310.18611", "description": "This article introduces the sequential Kalman filter, a computationally\nscalable approach for online changepoint detection with temporally correlated\ndata. The temporal correlation was not considered in the Bayesian online\nchangepoint detection approach due to the large computational cost. Motivated\nby detecting COVID-19 infections for dialysis patients from massive\nlongitudinal health records with a large number of covariates, we develop a\nscalable approach to detect multiple changepoints from correlated data by\nsequentially stitching Kalman filters of subsequences to compute the joint\ndistribution of the observations, which has linear computational complexity\nwith respect to the number of observations between the last detected\nchangepoint and the current observation at each time point, without\napproximating the likelihood function. Compared to other online changepoint\ndetection methods, simulated experiments show that our approach is more precise\nin detecting single or multiple changes in mean, variance, or correlation for\ntemporally correlated data. Furthermore, we propose a new way to integrate\nclassification and changepoint detection approaches that improve the detection\ndelay and accuracy for detecting COVID-19 infection compared to other\nalternatives."}, "http://arxiv.org/abs/2310.18733": {"title": "Threshold detection under a semiparametric regression model", "link": "http://arxiv.org/abs/2310.18733", "description": "Linear regression models have been extensively considered in the literature.\nHowever, in some practical applications they may not be appropriate all over\nthe range of the covariate. In this paper, a more flexible model is introduced\nby considering a regression model $Y=r(X)+\\varepsilon$ where the regression\nfunction $r(\\cdot)$ is assumed to be linear for large values in the domain of\nthe predictor variable $X$. More precisely, we assume that\n$r(x)=\\alpha_0+\\beta_0 x$ for $x&gt; u_0$, where the value $u_0$ is identified as\nthe smallest value satisfying such a property. A penalized procedure is\nintroduced to estimate the threshold $u_0$. The considered proposal focusses on\na semiparametric approach since no parametric model is assumed for the\nregression function for values smaller than $u_0$. Consistency properties of\nboth the threshold estimator and the estimators of $(\\alpha_0,\\beta_0)$ are\nderived, under mild assumptions. Through a numerical study, the small sample\nproperties of the proposed procedure and the importance of introducing a\npenalization are investigated. The analysis of a real data set allows us to\ndemonstrate the usefulness of the penalized estimators."}, "http://arxiv.org/abs/2310.18766": {"title": "Discussion of ''A Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks''", "link": "http://arxiv.org/abs/2310.18766", "description": "This review discusses the paper ''A Tale of Two Datasets: Representativeness\nand Generalisability of Inference for Samples of Networks'' by Krivitsky,\nColetti, and Hens, published in the Journal of the American Statistical\nAssociation in 2023."}, "http://arxiv.org/abs/2310.18836": {"title": "Design of Cluster-Randomized Trials with Cross-Cluster Interference", "link": "http://arxiv.org/abs/2310.18836", "description": "Cluster-randomized trials often involve units that are irregularly\ndistributed in space without well-separated communities. In these settings,\ncluster construction is a critical aspect of the design due to the potential\nfor cross-cluster interference. The existing literature relies on partial\ninterference models, which take clusters as given and assume no cross-cluster\ninterference. We relax this assumption by allowing interference to decay with\ngeographic distance between units. This induces a bias-variance trade-off:\nconstructing fewer, larger clusters reduces bias due to interference but\nincreases variance. We propose new estimators that exclude units most\npotentially impacted by cross-cluster interference and show that this\nsubstantially reduces asymptotic bias relative to conventional\ndifference-in-means estimators. We then study the design of clusters to\noptimize the estimators' rates of convergence. We provide formal justification\nfor a new design that chooses the number of clusters to balance the asymptotic\nbias and variance of our estimators and uses unsupervised learning to automate\ncluster construction."}, "http://arxiv.org/abs/2310.18858": {"title": "Estimating a function of the scale parameter in a gamma distribution with bounded variance", "link": "http://arxiv.org/abs/2310.18858", "description": "Given a gamma population with known shape parameter $\\alpha$, we develop a\ngeneral theory for estimating a function $g(\\cdot)$ of the scale parameter\n$\\beta$ with bounded variance. We begin by defining a sequential sampling\nprocedure with $g(\\cdot)$ satisfying some desired condition in proposing the\nstopping rule, and show the procedure enjoys appealing asymptotic properties.\nAfter these general conditions, we substitute $g(\\cdot)$ with specific\nfunctions including the gamma mean, the gamma variance, the gamma rate\nparameter, and a gamma survival probability as four possible illustrations. For\neach illustration, Monte Carlo simulations are carried out to justify the\nremarkable performance of our proposed sequential procedure. This is further\nsubstantiated with a real data study on weights of newly born babies."}, "http://arxiv.org/abs/2310.18875": {"title": "Feature calibration for computer models", "link": "http://arxiv.org/abs/2310.18875", "description": "Computer model calibration involves using partial and imperfect observations\nof the real world to learn which values of a model's input parameters lead to\noutputs that are consistent with real-world observations. When calibrating\nmodels with high-dimensional output (e.g. a spatial field), it is common to\nrepresent the output as a linear combination of a small set of basis vectors.\nOften, when trying to calibrate to such output, what is important to the\ncredibility of the model is that key emergent physical phenomena are\nrepresented, even if not faithfully or in the right place. In these cases,\ncomparison of model output and data in a linear subspace is inappropriate and\nwill usually lead to poor model calibration. To overcome this, we present\nkernel-based history matching (KHM), generalising the meaning of the technique\nsufficiently to be able to project model outputs and observations into a\nhigher-dimensional feature space, where patterns can be compared without their\nlocation necessarily being fixed. We develop the technical methodology, present\nan expert-driven kernel selection algorithm, and then apply the techniques to\nthe calibration of boundary layer clouds for the French climate model IPSL-CM."}, "http://arxiv.org/abs/2310.18905": {"title": "Incorporating nonparametric methods for estimating causal excursion effects in mobile health with zero-inflated count outcomes", "link": "http://arxiv.org/abs/2310.18905", "description": "In the domain of mobile health, tailoring interventions for real-time\ndelivery is of paramount importance. Micro-randomized trials have emerged as\nthe \"gold-standard\" methodology for developing such interventions. Analyzing\ndata from these trials provides insights into the efficacy of interventions and\nthe potential moderation by specific covariates. The \"causal excursion effect\",\na novel class of causal estimand, addresses these inquiries, backed by current\nsemiparametric inference techniques. Yet, existing methods mainly focus on\ncontinuous or binary data, leaving count data largely unexplored. The current\nwork is motivated by the Drink Less micro-randomized trial from the UK, which\nfocuses on a zero-inflated proximal outcome, the number of screen views in the\nsubsequent hour following the intervention decision point. In the current\npaper, we revisit the concept of causal excursion effects, specifically for\nzero-inflated count outcomes, and introduce novel estimation approaches that\nincorporate nonparametric techniques. Bidirectional asymptotics are derived for\nthe proposed estimators. Through extensive simulation studies, we evaluate the\nperformance of the proposed estimators. As an illustration, we also employ the\nproposed methods to the Drink Less trial data."}, "http://arxiv.org/abs/2310.18963": {"title": "Expectile-based conditional tail moments with covariates", "link": "http://arxiv.org/abs/2310.18963", "description": "Expectile, as the minimizer of an asymmetric quadratic loss function, is a\ncoherent risk measure and is helpful to use more information about the\ndistribution of the considered risk. In this paper, we propose a new risk\nmeasure by replacing quantiles by expectiles, called expectile-based\nconditional tail moment, and focus on the estimation of this new risk measure\nas the conditional survival function of the risk, given the risk exceeding the\nexpectile and given a value of the covariates, is heavy tail. Under some\nregular conditions, asymptotic properties of this new estimator are considered.\nThe extrapolated estimation of the conditional tail moments is also\ninvestigated. These results are illustrated both on simulated data and on a\nreal insurance data."}, "http://arxiv.org/abs/2310.19043": {"title": "Differentially Private Permutation Tests: Applications to Kernel Methods", "link": "http://arxiv.org/abs/2310.19043", "description": "Recent years have witnessed growing concerns about the privacy of sensitive\ndata. In response to these concerns, differential privacy has emerged as a\nrigorous framework for privacy protection, gaining widespread recognition in\nboth academic and industrial circles. While substantial progress has been made\nin private data analysis, existing methods often suffer from impracticality or\na significant loss of statistical efficiency. This paper aims to alleviate\nthese concerns in the context of hypothesis testing by introducing\ndifferentially private permutation tests. The proposed framework extends\nclassical non-private permutation tests to private settings, maintaining both\nfinite-sample validity and differential privacy in a rigorous manner. The power\nof the proposed test depends on the choice of a test statistic, and we\nestablish general conditions for consistency and non-asymptotic uniform power.\nTo demonstrate the utility and practicality of our framework, we focus on\nreproducing kernel-based test statistics and introduce differentially private\nkernel tests for two-sample and independence testing: dpMMD and dpHSIC. The\nproposed kernel tests are straightforward to implement, applicable to various\ntypes of data, and attain minimax optimal power across different privacy\nregimes. Our empirical evaluations further highlight their competitive power\nunder various synthetic and real-world scenarios, emphasizing their practical\nvalue. The code is publicly available to facilitate the implementation of our\nframework."}, "http://arxiv.org/abs/2310.19051": {"title": "A Survey of Methods for Estimating Hurst Exponent of Time Sequence", "link": "http://arxiv.org/abs/2310.19051", "description": "The Hurst exponent is a significant indicator for characterizing the\nself-similarity and long-term memory properties of time sequences. It has wide\napplications in physics, technologies, engineering, mathematics, statistics,\neconomics, psychology and so on. Currently, available methods for estimating\nthe Hurst exponent of time sequences can be divided into different categories:\ntime-domain methods and spectrum-domain methods based on the representation of\ntime sequence, linear regression methods and Bayesian methods based on\nparameter estimation methods. Although various methods are discussed in\nliterature, there are still some deficiencies: the descriptions of the\nestimation algorithms are just mathematics-oriented and the pseudo-codes are\nmissing; the effectiveness and accuracy of the estimation algorithms are not\nclear; the classification of estimation methods is not considered and there is\na lack of guidance for selecting the estimation methods. In this work, the\nemphasis is put on thirteen dominant methods for estimating the Hurst exponent.\nFor the purpose of decreasing the difficulty of implementing the estimation\nmethods with computer programs, the mathematical principles are discussed\nbriefly and the pseudo-codes of algorithms are presented with necessary\ndetails. It is expected that the survey could help the researchers to select,\nimplement and apply the estimation algorithms of interest in practical\nsituations in an easy way."}, "http://arxiv.org/abs/2310.19091": {"title": "Bridging the Gap: Towards an Expanded Toolkit for ML-Supported Decision-Making in the Public Sector", "link": "http://arxiv.org/abs/2310.19091", "description": "Machine Learning (ML) systems are becoming instrumental in the public sector,\nwith applications spanning areas like criminal justice, social welfare,\nfinancial fraud detection, and public health. While these systems offer great\npotential benefits to institutional decision-making processes, such as improved\nefficiency and reliability, they still face the challenge of aligning intricate\nand nuanced policy objectives with the precise formalization requirements\nnecessitated by ML models. In this paper, we aim to bridge the gap between ML\nand public sector decision-making by presenting a comprehensive overview of key\ntechnical challenges where disjunctions between policy goals and ML models\ncommonly arise. We concentrate on pivotal points of the ML pipeline that\nconnect the model to its operational environment, delving into the significance\nof representative training data and highlighting the importance of a model\nsetup that facilitates effective decision-making. Additionally, we link these\nchallenges with emerging methodological advancements, encompassing causal ML,\ndomain adaptation, uncertainty quantification, and multi-objective\noptimization, illustrating the path forward for harmonizing ML and public\nsector objectives."}, "http://arxiv.org/abs/2310.19114": {"title": "Sparse Fr\\'echet Sufficient Dimension Reduction with Graphical Structure Among Predictors", "link": "http://arxiv.org/abs/2310.19114", "description": "Fr\\'echet regression has received considerable attention to model\nmetric-space valued responses that are complex and non-Euclidean data, such as\nprobability distributions and vectors on the unit sphere. However, existing\nFr\\'echet regression literature focuses on the classical setting where the\npredictor dimension is fixed, and the sample size goes to infinity. This paper\nproposes sparse Fr\\'echet sufficient dimension reduction with graphical\nstructure among high-dimensional Euclidean predictors. In particular, we\npropose a convex optimization problem that leverages the graphical information\namong predictors and avoids inverting the high-dimensional covariance matrix.\nWe also provide the Alternating Direction Method of Multipliers (ADMM)\nalgorithm to solve the optimization problem. Theoretically, the proposed method\nachieves subspace estimation and variable selection consistency under suitable\nconditions. Extensive simulations and a real data analysis are carried out to\nillustrate the finite-sample performance of the proposed method."}, "http://arxiv.org/abs/2310.19246": {"title": "A spectral regularisation framework for latent variable models designed for single channel applications", "link": "http://arxiv.org/abs/2310.19246", "description": "Latent variable models (LVMs) are commonly used to capture the underlying\ndependencies, patterns, and hidden structure in observed data. Source\nduplication is a by-product of the data hankelisation pre-processing step\ncommon to single channel LVM applications, which hinders practical LVM\nutilisation. In this article, a Python package titled\nspectrally-regularised-LVMs is presented. The proposed package addresses the\nsource duplication issue via the addition of a novel spectral regularisation\nterm. This package provides a framework for spectral regularisation in single\nchannel LVM applications, thereby making it easier to investigate and utilise\nLVMs with spectral regularisation. This is achieved via the use of symbolic or\nexplicit representations of potential LVM objective functions which are\nincorporated into a framework that uses spectral regularisation during the LVM\nparameter estimation process. The objective of this package is to provide a\nconsistent linear LVM optimisation framework which incorporates spectral\nregularisation and caters to single channel time-series applications."}, "http://arxiv.org/abs/2310.19253": {"title": "Flow-based Distributionally Robust Optimization", "link": "http://arxiv.org/abs/2310.19253", "description": "We present a computationally efficient framework, called \\texttt{FlowDRO},\nfor solving flow-based distributionally robust optimization (DRO) problems with\nWasserstein uncertainty sets, when requiring the worst-case distribution (also\ncalled the Least Favorable Distribution, LFD) to be continuous so that the\nalgorithm can be scalable to problems with larger sample sizes and achieve\nbetter generalization capability for the induced robust algorithms. To tackle\nthe computationally challenging infinitely dimensional optimization problem, we\nleverage flow-based models, continuous-time invertible transport maps between\nthe data distribution and the target distribution, and develop a Wasserstein\nproximal gradient flow type of algorithm. In practice, we parameterize the\ntransport maps by a sequence of neural networks progressively trained in blocks\nby gradient descent. Our computational framework is general, can handle\nhigh-dimensional data with large sample sizes, and can be useful for various\napplications. We demonstrate its usage in adversarial learning,\ndistributionally robust hypothesis testing, and a new mechanism for data-driven\ndistribution perturbation differential privacy, where the proposed method gives\nstrong empirical performance on real high-dimensional data."}, "http://arxiv.org/abs/2310.19343": {"title": "Quantile Super Learning for independent and online settings with application to solar power forecasting", "link": "http://arxiv.org/abs/2310.19343", "description": "Estimating quantiles of an outcome conditional on covariates is of\nfundamental interest in statistics with broad application in probabilistic\nprediction and forecasting. We propose an ensemble method for conditional\nquantile estimation, Quantile Super Learning, that combines predictions from\nmultiple candidate algorithms based on their empirical performance measured\nwith respect to a cross-validated empirical risk of the quantile loss function.\nWe present theoretical guarantees for both iid and online data scenarios. The\nperformance of our approach for quantile estimation and in forming prediction\nintervals is tested in simulation studies. Two case studies related to solar\nenergy are used to illustrate Quantile Super Learning: in an iid setting, we\npredict the physical properties of perovskite materials for photovoltaic cells,\nand in an online setting we forecast ground solar irradiance based on output\nfrom dynamic weather ensemble models."}, "http://arxiv.org/abs/2310.19433": {"title": "Ordinal classification for interval-valued data and interval-valued functional data", "link": "http://arxiv.org/abs/2310.19433", "description": "The aim of ordinal classification is to predict the ordered labels of the\noutput from a set of observed inputs. Interval-valued data refers to data in\nthe form of intervals. For the first time, interval-valued data and\ninterval-valued functional data are considered as inputs in an ordinal\nclassification problem. Six ordinal classifiers for interval data and\ninterval-valued functional data are proposed. Three of them are parametric, one\nof them is based on ordinal binary decompositions and the other two are based\non ordered logistic regression. The other three methods are based on the use of\ndistances between interval data and kernels on interval data. One of the\nmethods uses the weighted $k$-nearest-neighbor technique for ordinal\nclassification. Another method considers kernel principal component analysis\nplus an ordinal classifier. And the sixth method, which is the method that\nperforms best, uses a kernel-induced ordinal random forest. They are compared\nwith na\\\"ive approaches in an extensive experimental study with synthetic and\noriginal real data sets, about human global development, and weather data. The\nresults show that considering ordering and interval-valued information improves\nthe accuracy. The source code and data sets are available at\nhttps://github.com/aleixalcacer/OCFIVD."}, "http://arxiv.org/abs/2310.19435": {"title": "A novel characterization of structures in smooth regression curves: from a viewpoint of persistent homology", "link": "http://arxiv.org/abs/2310.19435", "description": "We characterize structures such as monotonicity, convexity, and modality in\nsmooth regression curves using persistent homology. Persistent homology is a\nkey tool in topological data analysis that detects higher dimensional\ntopological features such as connected components and holes (cycles or loops)\nin the data. In other words, persistent homology is a multiscale version of\nhomology that characterizes sets based on the connected components and holes.\nWe use super-level sets of functions to extract geometric features via\npersistent homology. In particular, we explore structures in regression curves\nvia the persistent homology of super-level sets of a function, where the\nfunction of interest is - the first derivative of the regression function.\n\nIn the course of this study, we extend an existing procedure of estimating\nthe persistent homology for the first derivative of a regression function and\nestablish its consistency. Moreover, as an application of the proposed\nmethodology, we demonstrate that the persistent homology of the derivative of a\nfunction can reveal hidden structures in the function that are not visible from\nthe persistent homology of the function itself. In addition, we also illustrate\nthat the proposed procedure can be used to compare the shapes of two or more\nregression curves which is not possible merely from the persistent homology of\nthe function itself."}, "http://arxiv.org/abs/2310.19519": {"title": "A General Neural Causal Model for Interactive Recommendation", "link": "http://arxiv.org/abs/2310.19519", "description": "Survivor bias in observational data leads the optimization of recommender\nsystems towards local optima. Currently most solutions re-mines existing\nhuman-system collaboration patterns to maximize longer-term satisfaction by\nreinforcement learning. However, from the causal perspective, mitigating\nsurvivor effects requires answering a counterfactual problem, which is\ngenerally unidentifiable and inestimable. In this work, we propose a neural\ncausal model to achieve counterfactual inference. Specifically, we first build\na learnable structural causal model based on its available graphical\nrepresentations which qualitatively characterizes the preference transitions.\nMitigation of the survivor bias is achieved though counterfactual consistency.\nTo identify the consistency, we use the Gumbel-max function as structural\nconstrains. To estimate the consistency, we apply reinforcement optimizations,\nand use Gumbel-Softmax as a trade-off to get a differentiable function. Both\ntheoretical and empirical studies demonstrate the effectiveness of our\nsolution."}, "http://arxiv.org/abs/2310.19621": {"title": "A Bayesian Methodology for Estimation for Sparse Canonical Correlation", "link": "http://arxiv.org/abs/2310.19621", "description": "It can be challenging to perform an integrative statistical analysis of\nmulti-view high-dimensional data acquired from different experiments on each\nsubject who participated in a joint study. Canonical Correlation Analysis (CCA)\nis a statistical procedure for identifying relationships between such data\nsets. In that context, Structured Sparse CCA (ScSCCA) is a rapidly emerging\nmethodological area that aims for robust modeling of the interrelations between\nthe different data modalities by assuming the corresponding CCA directional\nvectors to be sparse. Although it is a rapidly growing area of statistical\nmethodology development, there is a need for developing related methodologies\nin the Bayesian paradigm. In this manuscript, we propose a novel ScSCCA\napproach where we employ a Bayesian infinite factor model and aim to achieve\nrobust estimation by encouraging sparsity in two different levels of the\nmodeling framework. Firstly, we utilize a multiplicative Half-Cauchy process\nprior to encourage sparsity at the level of the latent variable loading\nmatrices. Additionally, we promote further sparsity in the covariance matrix by\nusing graphical horseshoe prior or diagonal structure. We conduct multiple\nsimulations to compare the performance of the proposed method with that of\nother frequently used CCA procedures, and we apply the developed procedures to\nanalyze multi-omics data arising from a breast cancer study."}, "http://arxiv.org/abs/2310.19683": {"title": "An Online Bootstrap for Time Series", "link": "http://arxiv.org/abs/2310.19683", "description": "Resampling methods such as the bootstrap have proven invaluable in the field\nof machine learning. However, the applicability of traditional bootstrap\nmethods is limited when dealing with large streams of dependent data, such as\ntime series or spatially correlated observations. In this paper, we propose a\nnovel bootstrap method that is designed to account for data dependencies and\ncan be executed online, making it particularly suitable for real-time\napplications. This method is based on an autoregressive sequence of\nincreasingly dependent resampling weights. We prove the theoretical validity of\nthe proposed bootstrap scheme under general conditions. We demonstrate the\neffectiveness of our approach through extensive simulations and show that it\nprovides reliable uncertainty quantification even in the presence of complex\ndata dependencies. Our work bridges the gap between classical resampling\ntechniques and the demands of modern data analysis, providing a valuable tool\nfor researchers and practitioners in dynamic, data-rich environments."}, "http://arxiv.org/abs/2310.19787": {"title": "$e^{\\text{RPCA}}$: Robust Principal Component Analysis for Exponential Family Distributions", "link": "http://arxiv.org/abs/2310.19787", "description": "Robust Principal Component Analysis (RPCA) is a widely used method for\nrecovering low-rank structure from data matrices corrupted by significant and\nsparse outliers. These corruptions may arise from occlusions, malicious\ntampering, or other causes for anomalies, and the joint identification of such\ncorruptions with low-rank background is critical for process monitoring and\ndiagnosis. However, existing RPCA methods and their extensions largely do not\naccount for the underlying probabilistic distribution for the data matrices,\nwhich in many applications are known and can be highly non-Gaussian. We thus\npropose a new method called Robust Principal Component Analysis for Exponential\nFamily distributions ($e^{\\text{RPCA}}$), which can perform the desired\ndecomposition into low-rank and sparse matrices when such a distribution falls\nwithin the exponential family. We present a novel alternating direction method\nof multiplier optimization algorithm for efficient $e^{\\text{RPCA}}$\ndecomposition. The effectiveness of $e^{\\text{RPCA}}$ is then demonstrated in\ntwo applications: the first for steel sheet defect detection, and the second\nfor crime activity monitoring in the Atlanta metropolitan area."}, "http://arxiv.org/abs/2310.19788": {"title": "Locally Optimal Best Arm Identification with a Fixed Budget", "link": "http://arxiv.org/abs/2310.19788", "description": "This study investigates the problem of identifying the best treatment arm, a\ntreatment arm with the highest expected outcome. We aim to identify the best\ntreatment arm with a lower probability of misidentification, which has been\nexplored under various names across numerous research fields, including\n\\emph{best arm identification} (BAI) and ordinal optimization. In our\nexperiments, the number of treatment-allocation rounds is fixed. In each round,\na decision-maker allocates a treatment arm to an experimental unit and observes\na corresponding outcome, which follows a Gaussian distribution with a variance\ndifferent among treatment arms. At the end of the experiment, we recommend one\nof the treatment arms as an estimate of the best treatment arm based on the\nobservations. The objective of the decision-maker is to design an experiment\nthat minimizes the probability of misidentifying the best treatment arm. With\nthis objective in mind, we develop lower bounds for the probability of\nmisidentification under the small-gap regime, where the gaps of the expected\noutcomes between the best and suboptimal treatment arms approach zero. Then,\nassuming that the variances are known, we design the\nGeneralized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, which is\nan extension of the Neyman allocation proposed by Neyman (1934) and the\nUniform-EBA strategy proposed by Bubeck et al. (2011). For the GNA-EBA\nstrategy, we show that the strategy is asymptotically optimal because its\nprobability of misidentification aligns with the lower bounds as the sample\nsize approaches infinity under the small-gap regime. We refer to such optimal\nstrategies as locally asymptotic optimal because their performance aligns with\nthe lower bounds within restricted situations characterized by the small-gap\nregime."}, "http://arxiv.org/abs/2110.00152": {"title": "ebnm: An R Package for Solving the Empirical Bayes Normal Means Problem Using a Variety of Prior Families", "link": "http://arxiv.org/abs/2110.00152", "description": "The empirical Bayes normal means (EBNM) model is important to many areas of\nstatistics, including (but not limited to) multiple testing, wavelet denoising,\nmultiple linear regression, and matrix factorization. There are several\nexisting software packages that can fit EBNM models under different prior\nassumptions and using different algorithms; however, the differences across\ninterfaces complicate direct comparisons. Further, a number of important prior\nassumptions do not yet have implementations. Motivated by these issues, we\ndeveloped the R package ebnm, which provides a unified interface for\nefficiently fitting EBNM models using a variety of prior assumptions, including\nnonparametric approaches. In some cases, we incorporated existing\nimplementations into ebnm; in others, we implemented new fitting procedures\nwith a focus on speed and numerical stability. To demonstrate the capabilities\nof the unified interface, we compare results using different prior assumptions\nin two extended examples: the shrinkage estimation of baseball statistics; and\nthe matrix factorization of genetics data (via the new R package flashier). In\nsummary, ebnm is a convenient and comprehensive package for performing EBNM\nanalyses under a wide range of prior assumptions."}, "http://arxiv.org/abs/2110.02440": {"title": "Inverse Probability Weighting-based Mediation Analysis for Microbiome Data", "link": "http://arxiv.org/abs/2110.02440", "description": "Mediation analysis is an important tool to study causal associations in\nbiomedical and other scientific areas and has recently gained attention in\nmicrobiome studies. Using a microbiome study of acute myeloid leukemia (AML)\npatients, we investigate whether the effect of induction chemotherapy intensity\nlevels on the infection status is mediated by the microbial taxa abundance. The\nunique characteristics of the microbial mediators -- high-dimensionality,\nzero-inflation, and dependence -- call for new methodological developments in\nmediation analysis. The presence of an exposure-induced mediator-outcome\nconfounder, antibiotic use, further requires a delicate treatment in the\nanalysis. To address these unique challenges in our motivating AML microbiome\nstudy, we propose a novel nonparametric identification formula for the\ninterventional indirect effect (IIE), a measure recently developed for studying\nmediation effects. We develop the corresponding estimation algorithm using the\ninverse probability weighting method. We also test the presence of mediation\neffects via constructing the standard normal bootstrap confidence intervals.\nSimulation studies show that the proposed method has good finite-sample\nperformance in terms of the IIE estimation, and type-I error rate and power of\nthe corresponding test. In the AML microbiome study, our findings suggest that\nthe effect of induction chemotherapy intensity levels on infection is mainly\nmediated by patients' gut microbiome."}, "http://arxiv.org/abs/2203.03532": {"title": "E-detectors: a nonparametric framework for sequential change detection", "link": "http://arxiv.org/abs/2203.03532", "description": "Sequential change detection is a classical problem with a variety of\napplications. However, the majority of prior work has been parametric, for\nexample, focusing on exponential families. We develop a fundamentally new and\ngeneral framework for sequential change detection when the pre- and post-change\ndistributions are nonparametrically specified (and thus composite). Our\nprocedures come with clean, nonasymptotic bounds on the average run length\n(frequency of false alarms). In certain nonparametric cases (like sub-Gaussian\nor sub-exponential), we also provide near-optimal bounds on the detection delay\nfollowing a changepoint. The primary technical tool that we introduce is called\nan \\emph{e-detector}, which is composed of sums of e-processes -- a fundamental\ngeneralization of nonnegative supermartingales -- that are started at\nconsecutive times. We first introduce simple Shiryaev-Roberts and CUSUM-style\ne-detectors, and then show how to design their mixtures in order to achieve\nboth statistical and computational efficiency. Our e-detector framework can be\ninstantiated to recover classical likelihood-based procedures for parametric\nproblems, as well as yielding the first change detection method for many\nnonparametric problems. As a running example, we tackle the problem of\ndetecting changes in the mean of a bounded random variable without i.i.d.\nassumptions, with an application to tracking the performance of a basketball\nteam over multiple seasons."}, "http://arxiv.org/abs/2208.02942": {"title": "sparsegl: An R Package for Estimating Sparse Group Lasso", "link": "http://arxiv.org/abs/2208.02942", "description": "The sparse group lasso is a high-dimensional regression technique that is\nuseful for problems whose predictors have a naturally grouped structure and\nwhere sparsity is encouraged at both the group and individual predictor level.\nIn this paper we discuss a new R package for computing such regularized models.\nThe intention is to provide highly optimized solution routines enabling\nanalysis of very large datasets, especially in the context of sparse design\nmatrices."}, "http://arxiv.org/abs/2208.06236": {"title": "Differentially Private Kolmogorov-Smirnov-Type Tests", "link": "http://arxiv.org/abs/2208.06236", "description": "Hypothesis testing is a central problem in statistical analysis, and there is\ncurrently a lack of differentially private tests which are both statistically\nvalid and powerful. In this paper, we develop several new differentially\nprivate (DP) nonparametric hypothesis tests. Our tests are based on\nKolmogorov-Smirnov, Kuiper, Cram\\'er-von Mises, and Wasserstein test\nstatistics, which can all be expressed as a pseudo-metric on empirical\ncumulative distribution functions (ecdfs), and can be used to test hypotheses\non goodness-of-fit, two samples, and paired data. We show that these test\nstatistics have low sensitivity, requiring minimal noise to satisfy DP. In\nparticular, we show that the sensitivity of these test statistics can be\nexpressed in terms of the base sensitivity, which is the pseudo-metric distance\nbetween the ecdfs of adjacent databases and is easily calculated. The sampling\ndistribution of our test statistics are distribution-free under the null\nhypothesis, enabling easy computation of $p$-values by Monte Carlo methods. We\nshow that in several settings, especially with small privacy budgets or\nheavy-tailed data, our new DP tests outperform alternative nonparametric DP\ntests."}, "http://arxiv.org/abs/2208.10027": {"title": "Learning Invariant Representations under General Interventions on the Response", "link": "http://arxiv.org/abs/2208.10027", "description": "It has become increasingly common nowadays to collect observations of feature\nand response pairs from different environments. As a consequence, one has to\napply learned predictors to data with a different distribution due to\ndistribution shifts. One principled approach is to adopt the structural causal\nmodels to describe training and test models, following the invariance principle\nwhich says that the conditional distribution of the response given its\npredictors remains the same across environments. However, this principle might\nbe violated in practical settings when the response is intervened. A natural\nquestion is whether it is still possible to identify other forms of invariance\nto facilitate prediction in unseen environments. To shed light on this\nchallenging scenario, we focus on linear structural causal models (SCMs) and\nintroduce invariant matching property (IMP), an explicit relation to capture\ninterventions through an additional feature, leading to an alternative form of\ninvariance that enables a unified treatment of general interventions on the\nresponse as well as the predictors. We analyze the asymptotic generalization\nerrors of our method under both the discrete and continuous environment\nsettings, where the continuous case is handled by relating it to the\nsemiparametric varying coefficient models. We present algorithms that show\ncompetitive performance compared to existing methods over various experimental\nsettings including a COVID dataset."}, "http://arxiv.org/abs/2208.11756": {"title": "Testing Many Constraints in Possibly Irregular Models Using Incomplete U-Statistics", "link": "http://arxiv.org/abs/2208.11756", "description": "We consider the problem of testing a null hypothesis defined by equality and\ninequality constraints on a statistical parameter. Testing such hypotheses can\nbe challenging because the number of relevant constraints may be on the same\norder or even larger than the number of observed samples. Moreover, standard\ndistributional approximations may be invalid due to irregularities in the null\nhypothesis. We propose a general testing methodology that aims to circumvent\nthese difficulties. The constraints are estimated by incomplete U-statistics,\nand we derive critical values by Gaussian multiplier bootstrap. We show that\nthe bootstrap approximation of incomplete U-statistics is valid for kernels\nthat we call mixed degenerate when the number of combinations used to compute\nthe incomplete U-statistic is of the same order as the sample size. It follows\nthat our test controls type I error even in irregular settings. Furthermore,\nthe bootstrap approximation covers high-dimensional settings making our testing\nstrategy applicable for problems with many constraints. The methodology is\napplicable, in particular, when the constraints to be tested are polynomials in\nU-estimable parameters. As an application, we consider goodness-of-fit tests of\nlatent tree models for multivariate data."}, "http://arxiv.org/abs/2210.05538": {"title": "Estimating optimal treatment regimes in survival contexts using an instrumental variable", "link": "http://arxiv.org/abs/2210.05538", "description": "In survival contexts, substantial literature exists on estimating optimal\ntreatment regimes, where treatments are assigned based on personal\ncharacteristics for the purpose of maximizing the survival probability. These\nmethods assume that a set of covariates is sufficient to deconfound the\ntreatment-outcome relationship. Nevertheless, the assumption can be limited in\nobservational studies or randomized trials in which non-adherence occurs. Thus,\nwe propose a novel approach for estimating the optimal treatment regime when\ncertain confounders are not observable and a binary instrumental variable is\navailable. Specifically, via a binary instrumental variable, we propose two\nsemiparametric estimators for the optimal treatment regime by maximizing\nKaplan-Meier-like estimators within a pre-defined class of regimes, one of\nwhich possesses the desirable property of double robustness. Because the\nKaplan-Meier-like estimators are jagged, we incorporate kernel smoothing\nmethods to enhance their performance. Under appropriate regularity conditions,\nthe asymptotic properties are rigorously established. Furthermore, the finite\nsample performance is assessed through simulation studies. Finally, we\nexemplify our method using data from the National Cancer Institute's (NCI)\nprostate, lung, colorectal, and ovarian cancer screening trial."}, "http://arxiv.org/abs/2302.00878": {"title": "The Contextual Lasso: Sparse Linear Models via Deep Neural Networks", "link": "http://arxiv.org/abs/2302.00878", "description": "Sparse linear models are one of several core tools for interpretable machine\nlearning, a field of emerging importance as predictive models permeate\ndecision-making in many domains. Unfortunately, sparse linear models are far\nless flexible as functions of their input features than black-box models like\ndeep neural networks. With this capability gap in mind, we study a not-uncommon\nsituation where the input features dichotomize into two groups: explanatory\nfeatures, which are candidates for inclusion as variables in an interpretable\nmodel, and contextual features, which select from the candidate variables and\ndetermine their effects. This dichotomy leads us to the contextual lasso, a new\nstatistical estimator that fits a sparse linear model to the explanatory\nfeatures such that the sparsity pattern and coefficients vary as a function of\nthe contextual features. The fitting process learns this function\nnonparametrically via a deep neural network. To attain sparse coefficients, we\ntrain the network with a novel lasso regularizer in the form of a projection\nlayer that maps the network's output onto the space of $\\ell_1$-constrained\nlinear models. An extensive suite of experiments on real and synthetic data\nsuggests that the learned models, which remain highly transparent, can be\nsparser than the regular lasso without sacrificing the predictive power of a\nstandard deep neural network."}, "http://arxiv.org/abs/2302.02560": {"title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "link": "http://arxiv.org/abs/2302.02560", "description": "In policy research, one of the most critical analytic tasks is to estimate\nthe causal effect of a policy-relevant shift to the distribution of a\ncontinuous exposure/treatment on an outcome of interest. We call this problem\nshift-response function (SRF) estimation. Existing neural network methods\ninvolving robust causal-effect estimators lack theoretical guarantees and\npractical implementations for SRF estimation. Motivated by a key\npolicy-relevant question in public health, we develop a neural network method\nand its theoretical underpinnings to estimate SRFs with robustness and\nefficiency guarantees. We then apply our method to data consisting of 68\nmillion individuals and 27 million deaths across the U.S. to estimate the\ncausal effect from revising the US National Ambient Air Quality Standards\n(NAAQS) for PM 2.5 from 12 $\\mu g/m^3$ to 9 $\\mu g/m^3$. This change has been\nrecently proposed by the US Environmental Protection Agency (EPA). Our goal is\nto estimate, for the first time, the reduction in deaths that would result from\nthis anticipated revision using causal methods for SRFs. Our proposed method,\ncalled {T}argeted {R}egularization for {E}xposure {S}hifts with Neural\n{Net}works (TRESNET), contributes to the neural network literature for causal\ninference in two ways: first, it proposes a targeted regularization loss with\ntheoretical properties that ensure double robustness and achieves asymptotic\nefficiency specific for SRF estimation; second, it enables loss functions from\nthe exponential family of distributions to accommodate non-continuous outcome\ndistributions (such as hospitalization or mortality counts). We complement our\napplication with benchmark experiments that demonstrate TRESNET's broad\napplicability and competitiveness."}, "http://arxiv.org/abs/2303.03502": {"title": "A Semi-Parametric Model Simultaneously Handling Unmeasured Confounding, Informative Cluster Size, and Truncation by Death with a Data Application in Medicare Claims", "link": "http://arxiv.org/abs/2303.03502", "description": "Nearly 300,000 older adults experience a hip fracture every year, the\nmajority of which occur following a fall. Unfortunately, recovery after\nfall-related trauma such as hip fracture is poor, where older adults diagnosed\nwith Alzheimer's Disease and Related Dementia (ADRD) spend a particularly long\ntime in hospitals or rehabilitation facilities during the post-operative\nrecuperation period. Because older adults value functional recovery and\nspending time at home versus facilities as key outcomes after hospitalization,\nidentifying factors that influence days spent at home after hospitalization is\nimperative. While several individual-level factors have been identified, the\ncharacteristics of the treating hospital have recently been identified as\ncontributors. However, few methodological rigorous approaches are available to\nhelp overcome potential sources of bias such as hospital-level unmeasured\nconfounders, informative hospital size, and loss to follow-up due to death.\nThis article develops a useful tool equipped with unsupervised learning to\nsimultaneously handle statistical complexities that are often encountered in\nhealth services research, especially when using large administrative claims\ndatabases. The proposed estimator has a closed form, thus only requiring light\ncomputation load in a large-scale study. We further develop its asymptotic\nproperties that can be used to make statistical inference in practice.\nExtensive simulation studies demonstrate superiority of the proposed estimator\ncompared to existing estimators."}, "http://arxiv.org/abs/2305.04113": {"title": "Inferring Covariance Structure from Multiple Data Sources via Subspace Factor Analysis", "link": "http://arxiv.org/abs/2305.04113", "description": "Factor analysis provides a canonical framework for imposing lower-dimensional\nstructure such as sparse covariance in high-dimensional data. High-dimensional\ndata on the same set of variables are often collected under different\nconditions, for instance in reproducing studies across research groups. In such\ncases, it is natural to seek to learn the shared versus condition-specific\nstructure. Existing hierarchical extensions of factor analysis have been\nproposed, but face practical issues including identifiability problems. To\naddress these shortcomings, we propose a class of SUbspace Factor Analysis\n(SUFA) models, which characterize variation across groups at the level of a\nlower-dimensional subspace. We prove that the proposed class of SUFA models\nlead to identifiability of the shared versus group-specific components of the\ncovariance, and study their posterior contraction properties. Taking a Bayesian\napproach, these contributions are developed alongside efficient posterior\ncomputation algorithms. Our sampler fully integrates out latent variables, is\neasily parallelizable and has complexity that does not depend on sample size.\nWe illustrate the methods through application to integration of multiple gene\nexpression datasets relevant to immunology."}, "http://arxiv.org/abs/2305.17570": {"title": "Auditing Fairness by Betting", "link": "http://arxiv.org/abs/2305.17570", "description": "We provide practical, efficient, and nonparametric methods for auditing the\nfairness of deployed classification and regression models. Whereas previous\nwork relies on a fixed-sample size, our methods are sequential and allow for\nthe continuous monitoring of incoming data, making them highly amenable to\ntracking the fairness of real-world systems. We also allow the data to be\ncollected by a probabilistic policy as opposed to sampled uniformly from the\npopulation. This enables auditing to be conducted on data gathered for another\npurpose. Moreover, this policy may change over time and different policies may\nbe used on different subpopulations. Finally, our methods can handle\ndistribution shift resulting from either changes to the model or changes in the\nunderlying population. Our approach is based on recent progress in\nanytime-valid inference and game-theoretic statistics-the \"testing by betting\"\nframework in particular. These connections ensure that our methods are\ninterpretable, fast, and easy to implement. We demonstrate the efficacy of our\napproach on three benchmark fairness datasets."}, "http://arxiv.org/abs/2306.02948": {"title": "Learning under random distributional shifts", "link": "http://arxiv.org/abs/2306.02948", "description": "Many existing approaches for generating predictions in settings with\ndistribution shift model distribution shifts as adversarial or low-rank in\nsuitable representations. In various real-world settings, however, we might\nexpect shifts to arise through the superposition of many small and random\nchanges in the population and environment. Thus, we consider a class of random\ndistribution shift models that capture arbitrary changes in the underlying\ncovariate space, and dense, random shocks to the relationship between the\ncovariates and the outcomes. In this setting, we characterize the benefits and\ndrawbacks of several alternative prediction strategies: the standard approach\nthat directly predicts the long-term outcome of interest, the proxy approach\nthat directly predicts a shorter-term proxy outcome, and a hybrid approach that\nutilizes both the long-term policy outcome and (shorter-term) proxy outcome(s).\nWe show that the hybrid approach is robust to the strength of the distribution\nshift and the proxy relationship. We apply this method to datasets in two\nhigh-impact domains: asylum-seeker assignment and early childhood education. In\nboth settings, we find that the proposed approach results in substantially\nlower mean-squared error than current approaches."}, "http://arxiv.org/abs/2306.05751": {"title": "Advancing Counterfactual Inference through Quantile Regression", "link": "http://arxiv.org/abs/2306.05751", "description": "The capacity to address counterfactual \"what if\" inquiries is crucial for\nunderstanding and making use of causal influences. Traditional counterfactual\ninference usually assumes the availability of a structural causal model. Yet,\nin practice, such a causal model is often unknown and may not be identifiable.\nThis paper aims to perform reliable counterfactual inference based on the\n(learned) qualitative causal structure and observational data, without\nnecessitating a given causal model or even the direct estimation of conditional\ndistributions. We re-cast counterfactual reasoning as an extended quantile\nregression problem, implemented with deep neural networks to capture general\ncausal relationships and data distributions. The proposed approach offers\nsuperior statistical efficiency compared to existing ones, and further, it\nenhances the potential for generalizing the estimated counterfactual outcomes\nto previously unseen data, providing an upper bound on the generalization\nerror. Empirical results conducted on multiple datasets offer compelling\nsupport for our theoretical assertions."}, "http://arxiv.org/abs/2306.06155": {"title": "Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks", "link": "http://arxiv.org/abs/2306.06155", "description": "We present a new representation learning framework, Intensity Profile\nProjection, for continuous-time dynamic network data. Given triples $(i,j,t)$,\neach representing a time-stamped ($t$) interaction between two entities\n($i,j$), our procedure returns a continuous-time trajectory for each node,\nrepresenting its behaviour over time. The framework consists of three stages:\nestimating pairwise intensity functions, e.g. via kernel smoothing; learning a\nprojection which minimises a notion of intensity reconstruction error; and\nconstructing evolving node representations via the learned projection. The\ntrajectories satisfy two properties, known as structural and temporal\ncoherence, which we see as fundamental for reliable inference. Moreoever, we\ndevelop estimation theory providing tight control on the error of any estimated\ntrajectory, indicating that the representations could even be used in quite\nnoise-sensitive follow-on analyses. The theory also elucidates the role of\nsmoothing as a bias-variance trade-off, and shows how we can reduce the level\nof smoothing as the signal-to-noise ratio increases on account of the algorithm\n`borrowing strength' across the network."}, "http://arxiv.org/abs/2306.08777": {"title": "MMD-FUSE: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting", "link": "http://arxiv.org/abs/2306.08777", "description": "We propose novel statistics which maximise the power of a two-sample test\nbased on the Maximum Mean Discrepancy (MMD), by adapting over the set of\nkernels used in defining it. For finite sets, this reduces to combining\n(normalised) MMD values under each of these kernels via a weighted soft\nmaximum. Exponential concentration bounds are proved for our proposed\nstatistics under the null and alternative. We further show how these kernels\ncan be chosen in a data-dependent but permutation-independent way, in a\nwell-calibrated test, avoiding data splitting. This technique applies more\nbroadly to general permutation-based MMD testing, and includes the use of deep\nkernels with features learnt using unsupervised models such as auto-encoders.\nWe highlight the applicability of our MMD-FUSE test on both synthetic\nlow-dimensional and real-world high-dimensional data, and compare its\nperformance in terms of power against current state-of-the-art kernel tests."}, "http://arxiv.org/abs/2306.09335": {"title": "Class-Conditional Conformal Prediction with Many Classes", "link": "http://arxiv.org/abs/2306.09335", "description": "Standard conformal prediction methods provide a marginal coverage guarantee,\nwhich means that for a random test point, the conformal prediction set contains\nthe true label with a user-specified probability. In many classification\nproblems, we would like to obtain a stronger guarantee--that for test points of\na specific class, the prediction set contains the true label with the same\nuser-chosen probability. For the latter goal, existing conformal prediction\nmethods do not work well when there is a limited amount of labeled data per\nclass, as is often the case in real applications where the number of classes is\nlarge. We propose a method called clustered conformal prediction that clusters\ntogether classes having \"similar\" conformal scores and performs conformal\nprediction at the cluster level. Based on empirical evaluation across four\nimage data sets with many (up to 1000) classes, we find that clustered\nconformal typically outperforms existing methods in terms of class-conditional\ncoverage and set size metrics."}, "http://arxiv.org/abs/2306.11839": {"title": "Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations", "link": "http://arxiv.org/abs/2306.11839", "description": "Randomized experiments often need to be stopped prematurely due to the\ntreatment having an unintended harmful effect. Existing methods that determine\nwhen to stop an experiment early are typically applied to the data in aggregate\nand do not account for treatment effect heterogeneity. In this paper, we study\nthe early stopping of experiments for harm on heterogeneous populations. We\nfirst establish that current methods often fail to stop experiments when the\ntreatment harms a minority group of participants. We then use causal machine\nlearning to develop CLASH, the first broadly-applicable method for\nheterogeneous early stopping. We demonstrate CLASH's performance on simulated\nand real data and show that it yields effective early stopping for both\nclinical trials and A/B tests."}, "http://arxiv.org/abs/2307.01357": {"title": "Adaptive Principal Component Regression with Applications to Panel Data", "link": "http://arxiv.org/abs/2307.01357", "description": "Principal component regression (PCR) is a popular technique for fixed-design\nerror-in-variables regression, a generalization of the linear regression\nsetting in which the observed covariates are corrupted with random noise. We\nprovide the first time-uniform finite sample guarantees for online\n(regularized) PCR whenever data is collected adaptively. Since the proof\ntechniques for analyzing PCR in the fixed design setting do not readily extend\nto the online setting, our results rely on adapting tools from modern\nmartingale concentration to the error-in-variables setting. As an application\nof our bounds, we provide a framework for experiment design in panel data\nsettings when interventions are assigned adaptively. Our framework may be\nthought of as a generalization of the synthetic control and synthetic\ninterventions frameworks, where data is collected via an adaptive intervention\nassignment policy."}, "http://arxiv.org/abs/2307.02520": {"title": "Conditional independence testing under misspecified inductive biases", "link": "http://arxiv.org/abs/2307.02520", "description": "Conditional independence (CI) testing is a fundamental and challenging task\nin modern statistics and machine learning. Many modern methods for CI testing\nrely on powerful supervised learning methods to learn regression functions or\nBayes predictors as an intermediate step; we refer to this class of tests as\nregression-based tests. Although these methods are guaranteed to control Type-I\nerror when the supervised learning methods accurately estimate the regression\nfunctions or Bayes predictors of interest, their behavior is less understood\nwhen they fail due to misspecified inductive biases; in other words, when the\nemployed models are not flexible enough or when the training algorithm does not\ninduce the desired predictors. Then, we study the performance of\nregression-based CI tests under misspecified inductive biases. Namely, we\npropose new approximations or upper bounds for the testing errors of three\nregression-based tests that depend on misspecification errors. Moreover, we\nintroduce the Rao-Blackwellized Predictor Test (RBPT), a regression-based CI\ntest robust against misspecified inductive biases. Finally, we conduct\nexperiments with artificial and real data, showcasing the usefulness of our\ntheory and methods."}, "http://arxiv.org/abs/2308.03801": {"title": "On problematic practice of using normalization in Self-modeling/Multivariate Curve Resolution (S/MCR)", "link": "http://arxiv.org/abs/2308.03801", "description": "The paper is briefly dealing with greater or lesser misused normalization in\nself-modeling/multivariate curve resolution (S/MCR) practice. The importance of\nthe correct use of the ode solvers and apt kinetic illustrations are\nelucidated. The new terms, external and internal normalizations are defined and\ninterpreted. The problem of reducibility of a matrix is touched. Improper\ngeneralization/development of normalization-based methods are cited as\nexamples. The position of the extreme values of the signal contribution\nfunction is clarified. An Executable Notebook with Matlab Live Editor was\ncreated for algorithmic explanations and depictions."}, "http://arxiv.org/abs/2308.05373": {"title": "Conditional Independence Testing for Discrete Distributions: Beyond $\\chi^2$- and $G$-tests", "link": "http://arxiv.org/abs/2308.05373", "description": "This paper is concerned with the problem of conditional independence testing\nfor discrete data. In recent years, researchers have shed new light on this\nfundamental problem, emphasizing finite-sample optimality. The non-asymptotic\nviewpoint adapted in these works has led to novel conditional independence\ntests that enjoy certain optimality under various regimes. Despite their\nattractive theoretical properties, the considered tests are not necessarily\npractical, relying on a Poissonization trick and unspecified constants in their\ncritical values. In this work, we attempt to bridge the gap between theory and\npractice by reproving optimality without Poissonization and calibrating tests\nusing Monte Carlo permutations. Along the way, we also prove that classical\nasymptotic $\\chi^2$- and $G$-tests are notably sub-optimal in a\nhigh-dimensional regime, which justifies the demand for new tools. Our\ntheoretical results are complemented by experiments on both simulated and\nreal-world datasets. Accompanying this paper is an R package UCI that\nimplements the proposed tests."}, "http://arxiv.org/abs/2309.03875": {"title": "Network Sampling Methods for Estimating Social Networks, Population Percentages, and Totals of People Experiencing Unsheltered Homelessness", "link": "http://arxiv.org/abs/2309.03875", "description": "In this article, we propose using network-based sampling strategies to\nestimate the number of unsheltered people experiencing homelessness within a\ngiven administrative service unit, known as a Continuum of Care. We demonstrate\nthe effectiveness of network sampling methods to solve this problem. Here, we\nfocus on Respondent Driven Sampling (RDS), which has been shown to provide\nunbiased or low-biased estimates of totals and proportions for hard-to-reach\npopulations in contexts where a sampling frame (e.g., housing addresses) is not\navailable. To make the RDS estimator work for estimating the total number of\npeople living unsheltered, we introduce a new method that leverages\nadministrative data from the HUD-mandated Homeless Management Information\nSystem (HMIS). The HMIS provides high-quality counts and demographics for\npeople experiencing homelessness who sleep in emergency shelters. We then\ndemonstrate this method using network data collected in Nashville, TN, combined\nwith simulation methods to illustrate the efficacy of this approach and\nintroduce a method for performing a power analysis to find the optimal sample\nsize in this setting. We conclude with the RDS unsheltered PIT count conducted\nby King County Regional Homelessness Authority in 2022 (data publicly available\non the HUD website) and perform a comparative analysis between the 2022 RDS\nestimate of unsheltered people experiencing homelessness and an ARIMA forecast\nof the visual unsheltered PIT count. Finally, we discuss how this method works\nfor estimating the unsheltered population of people experiencing homelessness\nand future areas of research."}, "http://arxiv.org/abs/2310.19973": {"title": "Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy", "link": "http://arxiv.org/abs/2310.19973", "description": "Differentially private (DP) machine learning algorithms incur many sources of\nrandomness, such as random initialization, random batch subsampling, and\nshuffling. However, such randomness is difficult to take into account when\nproving differential privacy bounds because it induces mixture distributions\nfor the algorithm's output that are difficult to analyze. This paper focuses on\nimproving privacy bounds for shuffling models and one-iteration differentially\nprivate gradient descent (DP-GD) with random initializations using $f$-DP. We\nderive a closed-form expression of the trade-off function for shuffling models\nthat outperforms the most up-to-date results based on $(\\epsilon,\\delta)$-DP.\nMoreover, we investigate the effects of random initialization on the privacy of\none-iteration DP-GD. Our numerical computations of the trade-off function\nindicate that random initialization can enhance the privacy of DP-GD. Our\nanalysis of $f$-DP guarantees for these mixture mechanisms relies on an\ninequality for trade-off functions introduced in this paper. This inequality\nimplies the joint convexity of $F$-divergences. Finally, we study an $f$-DP\nanalog of the advanced joint convexity of the hockey-stick divergence related\nto $(\\epsilon,\\delta)$-DP and apply it to analyze the privacy of mixture\nmechanisms."}, "http://arxiv.org/abs/2310.19985": {"title": "Modeling random directions of changes in simplex-valued data", "link": "http://arxiv.org/abs/2310.19985", "description": "We propose models and algorithms for learning about random directions in\nsimplex-valued data. The models are applied to the study of income level\nproportions and their changes over time in a geostatistical area. There are\nseveral notable challenges in the analysis of simplex-valued data: the\nmeasurements must respect the simplex constraint and the changes exhibit\nspatiotemporal smoothness and may be heterogeneous. To that end, we propose\nBayesian models that draw from and expand upon building blocks in circular and\nspatial statistics by exploiting a suitable transformation for the\nsimplex-valued data. Our models also account for spatial correlation across\nlocations in the simplex and the heterogeneous patterns via mixture modeling.\nWe describe some properties of the models and model fitting via MCMC\ntechniques. Our models and methods are applied to an analysis of movements and\ntrends of income categories using the Home Mortgage Disclosure Act data."}, "http://arxiv.org/abs/2310.19988": {"title": "Counterfactual fairness for small subgroups", "link": "http://arxiv.org/abs/2310.19988", "description": "While methods for measuring and correcting differential performance in risk\nprediction models have proliferated in recent years, most existing techniques\ncan only be used to assess fairness across relatively large subgroups. The\npurpose of algorithmic fairness efforts is often to redress discrimination\nagainst groups that are both marginalized and small, so this sample size\nlimitation often prevents existing techniques from accomplishing their main\naim. We take a three-pronged approach to address the problem of quantifying\nfairness with small subgroups. First, we propose new estimands built on the\n\"counterfactual fairness\" framework that leverage information across groups.\nSecond, we estimate these quantities using a larger volume of data than\nexisting techniques. Finally, we propose a novel data borrowing approach to\nincorporate \"external data\" that lacks outcomes and predictions but contains\ncovariate and group membership information. This less stringent requirement on\nthe external data allows for more possibilities for external data sources. We\ndemonstrate practical application of our estimators to a risk prediction model\nused by a major Midwestern health system during the COVID-19 pandemic."}, "http://arxiv.org/abs/2310.20058": {"title": "New Asymptotic Limit Theory and Inference for Monotone Regression", "link": "http://arxiv.org/abs/2310.20058", "description": "Nonparametric regression problems with qualitative constraints such as\nmonotonicity or convexity are ubiquitous in applications. For example, in\npredicting the yield of a factory in terms of the number of labor hours, the\nmonotonicity of the conditional mean function is a natural constraint. One can\nestimate a monotone conditional mean function using nonparametric least squares\nestimation, which involves no tuning parameters. Several interesting properties\nof the isotonic LSE are known including its rate of convergence, adaptivity\nproperties, and pointwise asymptotic distribution. However, we believe that the\nfull richness of the asymptotic limit theory has not been explored in the\nliterature which we do in this paper. Moreover, the inference problem is not\nfully settled. In this paper, we present some new results for monotone\nregression including an extension of existing results to triangular arrays, and\nprovide asymptotically valid confidence intervals that are uniformly valid over\na large class of distributions."}, "http://arxiv.org/abs/2310.20075": {"title": "Meek Separators and Their Applications in Targeted Causal Discovery", "link": "http://arxiv.org/abs/2310.20075", "description": "Learning causal structures from interventional data is a fundamental problem\nwith broad applications across various fields. While many previous works have\nfocused on recovering the entire causal graph, in practice, there are scenarios\nwhere learning only part of the causal graph suffices. This is called\n$targeted$ causal discovery. In our work, we focus on two such well-motivated\nproblems: subset search and causal matching. We aim to minimize the number of\ninterventions in both cases.\n\nTowards this, we introduce the $Meek~separator$, which is a subset of\nvertices that, when intervened, decomposes the remaining unoriented edges into\nsmaller connected components. We then present an efficient algorithm to find\nMeek separators that are of small sizes. Such a procedure is helpful in\ndesigning various divide-and-conquer-based approaches. In particular, we\npropose two randomized algorithms that achieve logarithmic approximation for\nsubset search and causal matching, respectively. Our results provide the first\nknown average-case provable guarantees for both problems. We believe that this\nopens up possibilities to design near-optimal methods for many other targeted\ncausal structure learning problems arising from various applications."}, "http://arxiv.org/abs/2310.20087": {"title": "PAM-HC: A Bayesian Nonparametric Construction of Hybrid Control for Randomized Clinical Trials Using External Data", "link": "http://arxiv.org/abs/2310.20087", "description": "It is highly desirable to borrow information from external data to augment a\ncontrol arm in a randomized clinical trial, especially in settings where the\nsample size for the control arm is limited. However, a main challenge in\nborrowing information from external data is to accommodate potential\nheterogeneous subpopulations across the external and trial data. We apply a\nBayesian nonparametric model called Plaid Atoms Model (PAM) to identify\noverlapping and unique subpopulations across datasets, with which we restrict\nthe information borrowing to the common subpopulations. This forms a hybrid\ncontrol (HC) that leads to more precise estimation of treatment effects\nSimulation studies demonstrate the robustness of the new method, and an\napplication to an Atopic Dermatitis dataset shows improved treatment effect\nestimation."}, "http://arxiv.org/abs/2310.20088": {"title": "Optimal transport representations and functional principal components for distribution-valued processes", "link": "http://arxiv.org/abs/2310.20088", "description": "We develop statistical models for samples of distribution-valued stochastic\nprocesses through time-varying optimal transport process representations under\nthe Wasserstein metric when the values of the process are univariate\ndistributions. While functional data analysis provides a toolbox for the\nanalysis of samples of real- or vector-valued processes, there is at present no\ncoherent statistical methodology available for samples of distribution-valued\nprocesses, which are increasingly encountered in data analysis. To address the\nneed for such methodology, we introduce a transport model for samples of\ndistribution-valued stochastic processes that implements an intrinsic approach\nwhereby distributions are represented by optimal transports. Substituting\ntransports for distributions addresses the challenge of centering\ndistribution-valued processes and leads to a useful and interpretable\nrepresentation of each realized process by an overall transport and a\nreal-valued trajectory, utilizing a scalar multiplication operation for\ntransports. This representation facilitates a connection to Gaussian processes\nthat proves useful, especially for the case where the distribution-valued\nprocesses are only observed on a sparse grid of time points. We study the\nconvergence of the key components of the proposed representation to their\npopulation targets and demonstrate the practical utility of the proposed\napproach through simulations and application examples."}, "http://arxiv.org/abs/2310.20182": {"title": "Explicit Form of the Asymptotic Variance Estimator for IPW-type Estimators of Certain Estimands", "link": "http://arxiv.org/abs/2310.20182", "description": "Confidence intervals (CI) for the IPW estimators of the ATT and ATO might not\nalways yield conservative CIs when using the 'robust sandwich variance'\nestimator. In this manuscript, we identify scenarios where this variance\nestimator can be employed to derive conservative CIs. Specifically, for the\nATT, a conservative CI can be derived when there's a homogeneous treatment\neffect or the interaction effect surpasses the effect from the covariates\nalone. For the ATO, conservative CIs can be derived under certain conditions,\nsuch as when there are homogeneous treatment effects, when there exists\nsignificant treatment-confounder interactions, or when there's a large number\nof members in the control groups."}, "http://arxiv.org/abs/2310.20294": {"title": "Robust nonparametric regression based on deep ReLU neural networks", "link": "http://arxiv.org/abs/2310.20294", "description": "In this paper, we consider robust nonparametric regression using deep neural\nnetworks with ReLU activation function. While several existing theoretically\njustified methods are geared towards robustness against identical heavy-tailed\nnoise distributions, the rise of adversarial attacks has emphasized the\nimportance of safeguarding estimation procedures against systematic\ncontamination. We approach this statistical issue by shifting our focus towards\nestimating conditional distributions. To address it robustly, we introduce a\nnovel estimation procedure based on $\\ell$-estimation. Under a mild model\nassumption, we establish general non-asymptotic risk bounds for the resulting\nestimators, showcasing their robustness against contamination, outliers, and\nmodel misspecification. We then delve into the application of our approach\nusing deep ReLU neural networks. When the model is well-specified and the\nregression function belongs to an $\\alpha$-H\\\"older class, employing\n$\\ell$-type estimation on suitable networks enables the resulting estimators to\nachieve the minimax optimal rate of convergence. Additionally, we demonstrate\nthat deep $\\ell$-type estimators can circumvent the curse of dimensionality by\nassuming the regression function closely resembles the composition of several\nH\\\"older functions. To attain this, new deep fully-connected ReLU neural\nnetworks have been designed to approximate this composition class. This\napproximation result can be of independent interest."}, "http://arxiv.org/abs/2310.20376": {"title": "Mixture modeling via vectors of normalized independent finite point processes", "link": "http://arxiv.org/abs/2310.20376", "description": "Statistical modeling in presence of hierarchical data is a crucial task in\nBayesian statistics. The Hierarchical Dirichlet Process (HDP) represents the\nutmost tool to handle data organized in groups through mixture modeling.\nAlthough the HDP is mathematically tractable, its computational cost is\ntypically demanding, and its analytical complexity represents a barrier for\npractitioners. The present paper conceives a mixture model based on a novel\nfamily of Bayesian priors designed for multilevel data and obtained by\nnormalizing a finite point process. A full distribution theory for this new\nfamily and the induced clustering is developed, including tractable expressions\nfor marginal, posterior and predictive distributions. Efficient marginal and\nconditional Gibbs samplers are designed for providing posterior inference. The\nproposed mixture model overcomes the HDP in terms of analytical feasibility,\nclustering discovery, and computational time. The motivating application comes\nfrom the analysis of shot put data, which contains performance measurements of\nathletes across different seasons. In this setting, the proposed model is\nexploited to induce clustering of the observations across seasons and athletes.\nBy linking clusters across seasons, similarities and differences in athlete's\nperformances are identified."}, "http://arxiv.org/abs/2310.20409": {"title": "Detection of nonlinearity, discontinuity and interactions in generalized regression models", "link": "http://arxiv.org/abs/2310.20409", "description": "In generalized regression models the effect of continuous covariates is\ncommonly assumed to be linear. This assumption, however, may be too restrictive\nin applications and may lead to biased effect estimates and decreased\npredictive ability. While a multitude of alternatives for the flexible modeling\nof continuous covariates have been proposed, methods that provide guidance for\nchoosing a suitable functional form are still limited. To address this issue,\nwe propose a detection algorithm that evaluates several approaches for modeling\ncontinuous covariates and guides practitioners to choose the most appropriate\nalternative. The algorithm utilizes a unified framework for tree-structured\nmodeling which makes the results easily interpretable. We assessed the\nperformance of the algorithm by conducting a simulation study. To illustrate\nthe proposed algorithm, we analyzed data of patients suffering from chronic\nkidney disease."}, "http://arxiv.org/abs/2310.20450": {"title": "Safe Testing for Large-Scale Experimentation Platforms", "link": "http://arxiv.org/abs/2310.20450", "description": "In the past two decades, AB testing has proliferated to optimise products in\ndigital domains. Traditional AB tests use fixed-horizon testing, determining\nthe sample size of the experiment and continuing until the experiment has\nconcluded. However, due to the feedback provided by modern data infrastructure,\nexperimenters may take incorrect decisions based on preliminary results of the\ntest. For this reason, anytime-valid inference (AVI) is seeing increased\nadoption as the modern experimenters method for rapid decision making in the\nworld of data streaming.\n\nThis work focuses on Safe Testing, a novel framework for experimentation that\nenables continuous analysis without elevating the risk of incorrect\nconclusions. There exist safe testing equivalents of many common statistical\ntests, including the z-test, the t-test, and the proportion test. We compare\nthe efficacy of safe tests against classical tests and another method for AVI,\nthe mixture sequential probability ratio test (mSPRT). Comparisons are\nconducted first on simulation and then by real-world data from a large\ntechnology company, Vinted, a large European online marketplace for second-hand\nclothing. Our findings indicate that safe tests require fewer samples to detect\nsignificant effects, encouraging its potential for broader adoption."}, "http://arxiv.org/abs/2310.20460": {"title": "Aggregating Dependent Signals with Heavy-Tailed Combination Tests", "link": "http://arxiv.org/abs/2310.20460", "description": "Combining dependent p-values to evaluate the global null hypothesis presents\na longstanding challenge in statistical inference, particularly when\naggregating results from diverse methods to boost signal detection. P-value\ncombination tests using heavy-tailed distribution based transformations, such\nas the Cauchy combination test and the harmonic mean p-value, have recently\ngarnered significant interest for their potential to efficiently handle\narbitrary p-value dependencies. Despite their growing popularity in practical\napplications, there is a gap in comprehensive theoretical and empirical\nevaluations of these methods. This paper conducts an extensive investigation,\nrevealing that, theoretically, while these combination tests are asymptotically\nvalid for pairwise quasi-asymptotically independent test statistics, such as\nbivariate normal variables, they are also asymptotically equivalent to the\nBonferroni test under the same conditions. However, extensive simulations\nunveil their practical utility, especially in scenarios where stringent type-I\nerror control is not necessary and signals are dense. Both the heaviness of the\ndistribution and its support substantially impact the tests' non-asymptotic\nvalidity and power, and we recommend using a truncated Cauchy distribution in\npractice. Moreover, we show that under the violation of quasi-asymptotic\nindependence among test statistics, these tests remain valid and, in fact, can\nbe considerably less conservative than the Bonferroni test. We also present two\ncase studies in genetics and genomics, showcasing the potential of the\ncombination tests to significantly enhance statistical power while effectively\ncontrolling type-I errors."}, "http://arxiv.org/abs/2310.20483": {"title": "Measuring multidimensional heterogeneity in emergent social phenomena", "link": "http://arxiv.org/abs/2310.20483", "description": "Measuring inequalities in a multidimensional framework is a challenging\nproblem which is common to most field of science and engineering. Nevertheless,\ndespite the enormous amount of researches illustrating the fields of\napplication of inequality indices, and of the Gini index in particular, very\nfew consider the case of a multidimensional variable. In this paper, we\nconsider in some details a new inequality index, based on the Fourier\ntransform, that can be fruitfully applied to measure the degree of\ninhomogeneity of multivariate probability distributions. This index exhibits a\nnumber of interesting properties that make it very promising in quantifying the\ndegree of inequality in data sets of complex and multifaceted social phenomena."}, "http://arxiv.org/abs/2310.20537": {"title": "Directed Cyclic Graph for Causal Discovery from Multivariate Functional Data", "link": "http://arxiv.org/abs/2310.20537", "description": "Discovering causal relationship using multivariate functional data has\nreceived a significant amount of attention very recently. In this article, we\nintroduce a functional linear structural equation model for causal structure\nlearning when the underlying graph involving the multivariate functions may\nhave cycles. To enhance interpretability, our model involves a low-dimensional\ncausal embedded space such that all the relevant causal information in the\nmultivariate functional data is preserved in this lower-dimensional subspace.\nWe prove that the proposed model is causally identifiable under standard\nassumptions that are often made in the causal discovery literature. To carry\nout inference of our model, we develop a fully Bayesian framework with suitable\nprior specifications and uncertainty quantification through posterior\nsummaries. We illustrate the superior performance of our method over existing\nmethods in terms of causal graph estimation through extensive simulation\nstudies. We also demonstrate the proposed method using a brain EEG dataset."}, "http://arxiv.org/abs/2310.20697": {"title": "Text-Transport: Toward Learning Causal Effects of Natural Language", "link": "http://arxiv.org/abs/2310.20697", "description": "As language technologies gain prominence in real-world settings, it is\nimportant to understand how changes to language affect reader perceptions. This\ncan be formalized as the causal effect of varying a linguistic attribute (e.g.,\nsentiment) on a reader's response to the text. In this paper, we introduce\nText-Transport, a method for estimation of causal effects from natural language\nunder any text distribution. Current approaches for valid causal effect\nestimation require strong assumptions about the data, meaning the data from\nwhich one can estimate valid causal effects often is not representative of the\nactual target domain of interest. To address this issue, we leverage the notion\nof distribution shift to describe an estimator that transports causal effects\nbetween domains, bypassing the need for strong assumptions in the target\ndomain. We derive statistical guarantees on the uncertainty of this estimator,\nand we report empirical results and analyses that support the validity of\nText-Transport across data settings. Finally, we use Text-Transport to study a\nrealistic setting--hate speech on social media--in which causal effects do\nshift significantly between text domains, demonstrating the necessity of\ntransport when conducting causal inference on natural language."}, "http://arxiv.org/abs/2203.15009": {"title": "DAMNETS: A Deep Autoregressive Model for Generating Markovian Network Time Series", "link": "http://arxiv.org/abs/2203.15009", "description": "Generative models for network time series (also known as dynamic graphs) have\ntremendous potential in fields such as epidemiology, biology and economics,\nwhere complex graph-based dynamics are core objects of study. Designing\nflexible and scalable generative models is a very challenging task due to the\nhigh dimensionality of the data, as well as the need to represent temporal\ndependencies and marginal network structure. Here we introduce DAMNETS, a\nscalable deep generative model for network time series. DAMNETS outperforms\ncompeting methods on all of our measures of sample quality, over both real and\nsynthetic data sets."}, "http://arxiv.org/abs/2207.04481": {"title": "Detecting Grouped Local Average Treatment Effects and Selecting True Instruments", "link": "http://arxiv.org/abs/2207.04481", "description": "Under an endogenous binary treatment with heterogeneous effects and multiple\ninstruments, we propose a two-step procedure for identifying complier groups\nwith identical local average treatment effects (LATE) despite relying on\ndistinct instruments, even if several instruments violate the identifying\nassumptions. We use the fact that the LATE is homogeneous for instruments which\n(i) satisfy the LATE assumptions (instrument validity and treatment\nmonotonicity in the instrument) and (ii) generate identical complier groups in\nterms of treatment propensities given the respective instruments. We propose a\ntwo-step procedure, where we first cluster the propensity scores in the first\nstep and find groups of IVs with the same reduced form parameters in the second\nstep. Under the plurality assumption that within each set of instruments with\nidentical treatment propensities, instruments truly satisfying the LATE\nassumptions are the largest group, our procedure permits identifying these true\ninstruments in a data driven way. We show that our procedure is consistent and\nprovides consistent and asymptotically normal estimators of underlying LATEs.\nWe also provide a simulation study investigating the finite sample properties\nof our approach and an empirical application investigating the effect of\nincarceration on recidivism in the US with judge assignments serving as\ninstruments."}, "http://arxiv.org/abs/2212.01792": {"title": "Classification by sparse additive models", "link": "http://arxiv.org/abs/2212.01792", "description": "We consider (nonparametric) sparse additive models (SpAM) for classification.\nThe design of a SpAM classifier is based on minimizing the logistic loss with a\nsparse group Lasso and more general sparse group Slope-type penalties on the\ncoefficients of univariate components' expansions in orthonormal series (e.g.,\nFourier or wavelets). The resulting classifiers are inherently adaptive to the\nunknown sparsity and smoothness. We show that under certain sparse group\nrestricted eigenvalue condition the sparse group Lasso classifier is\nnearly-minimax (up to log-factors) within the entire range of analytic, Sobolev\nand Besov classes while the sparse group Slope classifier achieves the exact\nminimax order (without the extra log-factors) for sparse and moderately dense\nsetups. The performance of the proposed classifier is illustrated on the\nreal-data example."}, "http://arxiv.org/abs/2302.11656": {"title": "Confounder-Dependent Bayesian Mixture Model: Characterizing Heterogeneity of Causal Effects in Air Pollution Epidemiology", "link": "http://arxiv.org/abs/2302.11656", "description": "Several epidemiological studies have provided evidence that long-term\nexposure to fine particulate matter (PM2.5) increases mortality risk.\nFurthermore, some population characteristics (e.g., age, race, and\nsocioeconomic status) might play a crucial role in understanding vulnerability\nto air pollution. To inform policy, it is necessary to identify groups of the\npopulation that are more or less vulnerable to air pollution. In causal\ninference literature, the Group Average Treatment Effect (GATE) is a\ndistinctive facet of the conditional average treatment effect. This widely\nemployed metric serves to characterize the heterogeneity of a treatment effect\nbased on some population characteristics. In this work, we introduce a novel\nConfounder-Dependent Bayesian Mixture Model (CDBMM) to characterize causal\neffect heterogeneity. More specifically, our method leverages the flexibility\nof the dependent Dirichlet process to model the distribution of the potential\noutcomes conditionally to the covariates and the treatment levels, thus\nenabling us to: (i) identify heterogeneous and mutually exclusive population\ngroups defined by similar GATEs in a data-driven way, and (ii) estimate and\ncharacterize the causal effects within each of the identified groups. Through\nsimulations, we demonstrate the effectiveness of our method in uncovering key\ninsights about treatment effects heterogeneity. We apply our method to claims\ndata from Medicare enrollees in Texas. We found six mutually exclusive groups\nwhere the causal effects of PM2.5 on mortality are heterogeneous."}, "http://arxiv.org/abs/2305.03149": {"title": "A Spectral Method for Identifiable Grade of Membership Analysis with Binary Responses", "link": "http://arxiv.org/abs/2305.03149", "description": "Grade of Membership (GoM) models are popular individual-level mixture models\nfor multivariate categorical data. GoM allows each subject to have mixed\nmemberships in multiple extreme latent profiles. Therefore GoM models have a\nricher modeling capacity than latent class models that restrict each subject to\nbelong to a single profile. The flexibility of GoM comes at the cost of more\nchallenging identifiability and estimation problems. In this work, we propose a\nsingular value decomposition (SVD) based spectral approach to GoM analysis with\nmultivariate binary responses. Our approach hinges on the observation that the\nexpectation of the data matrix has a low-rank decomposition under a GoM model.\nFor identifiability, we develop sufficient and almost necessary conditions for\na notion of expectation identifiability. For estimation, we extract only a few\nleading singular vectors of the observed data matrix, and exploit the simplex\ngeometry of these vectors to estimate the mixed membership scores and other\nparameters. We also establish the consistency of our estimator in the\ndouble-asymptotic regime where both the number of subjects and the number of\nitems grow to infinity. Our spectral method has a huge computational advantage\nover Bayesian or likelihood-based methods and is scalable to large-scale and\nhigh-dimensional data. Extensive simulation studies demonstrate the superior\nefficiency and accuracy of our method. We also illustrate our method by\napplying it to a personality test dataset."}, "http://arxiv.org/abs/2305.05276": {"title": "Causal Discovery from Subsampled Time Series with Proxy Variables", "link": "http://arxiv.org/abs/2305.05276", "description": "Inferring causal structures from time series data is the central interest of\nmany scientific inquiries. A major barrier to such inference is the problem of\nsubsampling, i.e., the frequency of measurement is much lower than that of\ncausal influence. To overcome this problem, numerous methods have been\nproposed, yet either was limited to the linear case or failed to achieve\nidentifiability. In this paper, we propose a constraint-based algorithm that\ncan identify the entire causal structure from subsampled time series, without\nany parametric constraint. Our observation is that the challenge of subsampling\narises mainly from hidden variables at the unobserved time steps. Meanwhile,\nevery hidden variable has an observed proxy, which is essentially itself at\nsome observable time in the future, benefiting from the temporal structure.\nBased on these, we can leverage the proxies to remove the bias induced by the\nhidden variables and hence achieve identifiability. Following this intuition,\nwe propose a proxy-based causal discovery algorithm. Our algorithm is\nnonparametric and can achieve full causal identification. Theoretical\nadvantages are reflected in synthetic and real-world experiments."}, "http://arxiv.org/abs/2305.08942": {"title": "Probabilistic forecast of nonlinear dynamical systems with uncertainty quantification", "link": "http://arxiv.org/abs/2305.08942", "description": "Data-driven modeling is useful for reconstructing nonlinear dynamical systems\nwhen the underlying process is unknown or too expensive to compute. Having\nreliable uncertainty assessment of the forecast enables tools to be deployed to\npredict new scenarios unobserved before. In this work, we first extend parallel\npartial Gaussian processes for predicting the vector-valued transition function\nthat links the observations between the current and next time points, and\nquantify the uncertainty of predictions by posterior sampling. Second, we show\nthe equivalence between the dynamic mode decomposition and the maximum\nlikelihood estimator of the linear mapping matrix in the linear state space\nmodel. The connection provides a {probabilistic generative} model of dynamic\nmode decomposition and thus, uncertainty of predictions can be obtained.\nFurthermore, we draw close connections between different data-driven models for\napproximating nonlinear dynamics, through a unified view of generative models.\nWe study two numerical examples, where the inputs of the dynamics are assumed\nto be known in the first example and the inputs are unknown in the second\nexample. The examples indicate that uncertainty of forecast can be properly\nquantified, whereas model or input misspecification can degrade the accuracy of\nuncertainty quantification."}, "http://arxiv.org/abs/2305.16795": {"title": "On Consistent Bayesian Inference from Synthetic Data", "link": "http://arxiv.org/abs/2305.16795", "description": "Generating synthetic data, with or without differential privacy, has\nattracted significant attention as a potential solution to the dilemma between\nmaking data easily available, and the privacy of data subjects. Several works\nhave shown that consistency of downstream analyses from synthetic data,\nincluding accurate uncertainty estimation, requires accounting for the\nsynthetic data generation. There are very few methods of doing so, most of them\nfor frequentist analysis. In this paper, we study how to perform consistent\nBayesian inference from synthetic data. We prove that mixing posterior samples\nobtained separately from multiple large synthetic data sets converges to the\nposterior of the downstream analysis under standard regularity conditions when\nthe analyst's model is compatible with the data provider's model. We also\npresent several examples showing how the theory works in practice, and showing\nhow Bayesian inference can fail when the compatibility assumption is not met,\nor the synthetic data set is not significantly larger than the original."}, "http://arxiv.org/abs/2306.04746": {"title": "Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models", "link": "http://arxiv.org/abs/2306.04746", "description": "In computational social science (CSS), researchers analyze documents to\nexplain social and political phenomena. In most scenarios, CSS researchers\nfirst obtain labels for documents and then explain labels using interpretable\nregression analyses in the second step. One increasingly common way to annotate\ndocuments cheaply at scale is through large language models (LLMs). However,\nlike other scalable ways of producing annotations, such surrogate labels are\noften imperfect and biased. We present a new algorithm for using imperfect\nannotation surrogates for downstream statistical analyses while guaranteeing\nstatistical properties -- like asymptotic unbiasedness and proper uncertainty\nquantification -- which are fundamental to CSS research. We show that direct\nuse of surrogate labels in downstream statistical analyses leads to substantial\nbias and invalid confidence intervals, even with high surrogate accuracy of\n80--90\\%. To address this, we build on debiased machine learning to propose the\ndesign-based supervised learning (DSL) estimator. DSL employs a doubly-robust\nprocedure to combine surrogate labels with a smaller number of high-quality,\ngold-standard labels. Our approach guarantees valid inference for downstream\nstatistical analyses, even when surrogates are arbitrarily biased and without\nrequiring stringent assumptions, by controlling the probability of sampling\ndocuments for gold-standard labeling. Both our theoretical analysis and\nexperimental results show that DSL provides valid statistical inference while\nachieving root mean squared errors comparable to existing alternatives that\nfocus only on prediction without inferential guarantees."}, "http://arxiv.org/abs/2308.13713": {"title": "Causally Sound Priors for Binary Experiments", "link": "http://arxiv.org/abs/2308.13713", "description": "We introduce the BREASE framework for the Bayesian analysis of randomized\ncontrolled trials with a binary treatment and a binary outcome. Approaching the\nproblem from a causal inference perspective, we propose parameterizing the\nlikelihood in terms of the baseline risk, efficacy, and adverse side effects of\nthe treatment, along with a flexible, yet intuitive and tractable jointly\nindependent beta prior distribution on these parameters, which we show to be a\ngeneralization of the Dirichlet prior for the joint distribution of potential\noutcomes. Our approach has a number of desirable characteristics when compared\nto current mainstream alternatives: (i) it naturally induces prior dependence\nbetween expected outcomes in the treatment and control groups; (ii) as the\nbaseline risk, efficacy and risk of adverse side effects are quantities\ncommonly present in the clinicians' vocabulary, the hyperparameters of the\nprior are directly interpretable, thus facilitating the elicitation of prior\nknowledge and sensitivity analysis; and (iii) we provide analytical formulae\nfor the marginal likelihood, Bayes factor, and other posterior quantities, as\nwell as exact posterior sampling via simulation, in cases where traditional\nMCMC fails. Empirical examples demonstrate the utility of our methods for\nestimation, hypothesis testing, and sensitivity analysis of treatment effects."}, "http://arxiv.org/abs/2309.01608": {"title": "Supervised dimensionality reduction for multiple imputation by chained equations", "link": "http://arxiv.org/abs/2309.01608", "description": "Multivariate imputation by chained equations (MICE) is one of the most\npopular approaches to address missing values in a data set. This approach\nrequires specifying a univariate imputation model for every variable under\nimputation. The specification of which predictors should be included in these\nunivariate imputation models can be a daunting task. Principal component\nanalysis (PCA) can simplify this process by replacing all of the potential\nimputation model predictors with a few components summarizing their variance.\nIn this article, we extend the use of PCA with MICE to include a supervised\naspect whereby information from the variables under imputation is incorporated\ninto the principal component estimation. We conducted an extensive simulation\nstudy to assess the statistical properties of MICE with different versions of\nsupervised dimensionality reduction and we compared them with the use of\nclassical unsupervised PCA as a simpler dimensionality reduction technique."}, "http://arxiv.org/abs/2311.00118": {"title": "Extracting the Multiscale Causal Backbone of Brain Dynamics", "link": "http://arxiv.org/abs/2311.00118", "description": "The bulk of the research effort on brain connectivity revolves around\nstatistical associations among brain regions, which do not directly relate to\nthe causal mechanisms governing brain dynamics. Here we propose the multiscale\ncausal backbone (MCB) of brain dynamics shared by a set of individuals across\nmultiple temporal scales, and devise a principled methodology to extract it.\n\nOur approach leverages recent advances in multiscale causal structure\nlearning and optimizes the trade-off between the model fitting and its\ncomplexity. Empirical assessment on synthetic data shows the superiority of our\nmethodology over a baseline based on canonical functional connectivity\nnetworks. When applied to resting-state fMRI data, we find sparse MCBs for both\nthe left and right brain hemispheres. Thanks to its multiscale nature, our\napproach shows that at low-frequency bands, causal dynamics are driven by brain\nregions associated with high-level cognitive functions; at higher frequencies\ninstead, nodes related to sensory processing play a crucial role. Finally, our\nanalysis of individual multiscale causal structures confirms the existence of a\ncausal fingerprint of brain connectivity, thus supporting from a causal\nperspective the existing extensive research in brain connectivity\nfingerprinting."}, "http://arxiv.org/abs/2311.00122": {"title": "Statistical Network Analysis: Past, Present, and Future", "link": "http://arxiv.org/abs/2311.00122", "description": "This article provides a brief overview of statistical network analysis, a\nrapidly evolving field of statistics, which encompasses statistical models,\nalgorithms, and inferential methods for analyzing data in the form of networks.\nParticular emphasis is given to connecting the historical developments in\nnetwork science to today's statistical network analysis, and outlining\nimportant new areas for future research.\n\nThis invited article is intended as a book chapter for the volume \"Frontiers\nof Statistics and Data Science\" edited by Subhashis Ghoshal and Anindya Roy for\nthe International Indian Statistical Association Series on Statistics and Data\nScience, published by Springer. This review article covers the material from\nthe short course titled \"Statistical Network Analysis: Past, Present, and\nFuture\" taught by the author at the Annual Conference of the International\nIndian Statistical Association, June 6-10, 2023, at Golden, Colorado."}, "http://arxiv.org/abs/2311.00210": {"title": "Broken Adaptive Ridge Method for Variable Selection in Generalized Partly Linear Models with Application to the Coronary Artery Disease Data", "link": "http://arxiv.org/abs/2311.00210", "description": "Motivated by the CATHGEN data, we develop a new statistical learning method\nfor simultaneous variable selection and parameter estimation under the context\nof generalized partly linear models for data with high-dimensional covariates.\nThe method is referred to as the broken adaptive ridge (BAR) estimator, which\nis an approximation of the $L_0$-penalized regression by iteratively performing\nreweighted squared $L_2$-penalized regression. The generalized partly linear\nmodel extends the generalized linear model by including a non-parametric\ncomponent to construct a flexible model for modeling various types of covariate\neffects. We employ the Bernstein polynomials as the sieve space to approximate\nthe non-parametric functions so that our method can be implemented easily using\nthe existing R packages. Extensive simulation studies suggest that the proposed\nmethod performs better than other commonly used penalty-based variable\nselection methods. We apply the method to the CATHGEN data with a binary\nresponse from a coronary artery disease study, which motivated our research,\nand obtained new findings in both high-dimensional genetic and low-dimensional\nnon-genetic covariates."}, "http://arxiv.org/abs/2311.00294": {"title": "Multi-step ahead prediction intervals for non-parametric autoregressions via bootstrap: consistency, debiasing and pertinence", "link": "http://arxiv.org/abs/2311.00294", "description": "To address the difficult problem of multi-step ahead prediction of\nnon-parametric autoregressions, we consider a forward bootstrap approach.\nEmploying a local constant estimator, we can analyze a general type of\nnon-parametric time series model, and show that the proposed point predictions\nare consistent with the true optimal predictor. We construct a quantile\nprediction interval that is asymptotically valid. Moreover, using a debiasing\ntechnique, we can asymptotically approximate the distribution of multi-step\nahead non-parametric estimation by bootstrap. As a result, we can build\nbootstrap prediction intervals that are pertinent, i.e., can capture the model\nestimation variability, thus improving upon the standard quantile prediction\nintervals. Simulation studies are given to illustrate the performance of our\npoint predictions and pertinent prediction intervals for finite samples."}, "http://arxiv.org/abs/2311.00528": {"title": "On the Comparative Analysis of Average Treatment Effects Estimation via Data Combination", "link": "http://arxiv.org/abs/2311.00528", "description": "There is growing interest in exploring causal effects in target populations\nby combining multiple datasets. Nevertheless, most approaches are tailored to\nspecific settings and lack comprehensive comparative analyses across different\nsettings. In this article, within the typical scenario of a source dataset and\na target dataset, we establish a unified framework for comparing various\nsettings in causal inference via data combination. We first design six distinct\nsettings, each with different available datasets and identifiability\nassumptions. The six settings cover a wide range of scenarios in the existing\nliterature. We then conduct a comprehensive efficiency comparative analysis\nacross these settings by calculating and comparing the semiparametric\nefficiency bounds for the average treatment effect (ATE) in the target\npopulation. Our findings reveal the key factors contributing to efficiency\ngains or losses across these settings. In addition, we extend our analysis to\nother estimands, including ATE in the source population and the average\ntreatment effect on treated (ATT) in both the source and target populations.\nFurthermore, we empirically validate our findings by constructing locally\nefficient estimators and conducting extensive simulation studies. We\ndemonstrate the proposed approaches using a real application to a MIMIC-III\ndataset as the target population and an eICU dataset as the source population."}, "http://arxiv.org/abs/2311.00541": {"title": "An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek", "link": "http://arxiv.org/abs/2311.00541", "description": "Word meanings change over time, and word senses evolve, emerge or die out in\nthe process. For ancient languages, where the corpora are often small, sparse\nand noisy, modelling such changes accurately proves challenging, and\nquantifying uncertainty in sense-change estimates consequently becomes\nimportant. GASC and DiSC are existing generative models that have been used to\nanalyse sense change for target words from an ancient Greek text corpus, using\nunsupervised learning without the help of any pre-training. These models\nrepresent the senses of a given target word such as \"kosmos\" (meaning\ndecoration, order or world) as distributions over context words, and sense\nprevalence as a distribution over senses. The models are fitted using MCMC\nmethods to measure temporal changes in these representations. In this paper, we\nintroduce EDiSC, an embedded version of DiSC, which combines word embeddings\nwith DiSC to provide superior model performance. We show empirically that EDiSC\noffers improved predictive accuracy, ground-truth recovery and uncertainty\nquantification, as well as better sampling efficiency and scalability\nproperties with MCMC methods. We also discuss the challenges of fitting these\nmodels."}, "http://arxiv.org/abs/2311.00553": {"title": "Polynomial Chaos Surrogate Construction for Random Fields with Parametric Uncertainty", "link": "http://arxiv.org/abs/2311.00553", "description": "Engineering and applied science rely on computational experiments to\nrigorously study physical systems. The mathematical models used to probe these\nsystems are highly complex, and sampling-intensive studies often require\nprohibitively many simulations for acceptable accuracy. Surrogate models\nprovide a means of circumventing the high computational expense of sampling\nsuch complex models. In particular, polynomial chaos expansions (PCEs) have\nbeen successfully used for uncertainty quantification studies of deterministic\nmodels where the dominant source of uncertainty is parametric. We discuss an\nextension to conventional PCE surrogate modeling to enable surrogate\nconstruction for stochastic computational models that have intrinsic noise in\naddition to parametric uncertainty. We develop a PCE surrogate on a joint space\nof intrinsic and parametric uncertainty, enabled by Rosenblatt transformations,\nand then extend the construction to random field data via the Karhunen-Loeve\nexpansion. We then take advantage of closed-form solutions for computing PCE\nSobol indices to perform a global sensitivity analysis of the model which\nquantifies the intrinsic noise contribution to the overall model output\nvariance. Additionally, the resulting joint PCE is generative in the sense that\nit allows generating random realizations at any input parameter setting that\nare statistically approximately equivalent to realizations from the underlying\nstochastic model. The method is demonstrated on a chemical catalysis example\nmodel."}, "http://arxiv.org/abs/2311.00568": {"title": "Scalable kernel balancing weights in a nationwide observational study of hospital profit status and heart attack outcomes", "link": "http://arxiv.org/abs/2311.00568", "description": "Weighting is a general and often-used method for statistical adjustment.\nWeighting has two objectives: first, to balance covariate distributions, and\nsecond, to ensure that the weights have minimal dispersion and thus produce a\nmore stable estimator. A recent, increasingly common approach directly\noptimizes the weights toward these two objectives. However, this approach has\nnot yet been feasible in large-scale datasets when investigators wish to\nflexibly balance general basis functions in an extended feature space. For\nexample, many balancing approaches cannot scale to national-level health\nservices research studies. To address this practical problem, we describe a\nscalable and flexible approach to weighting that integrates a basis expansion\nin a reproducing kernel Hilbert space with state-of-the-art convex optimization\ntechniques. Specifically, we use the rank-restricted Nystr\\\"{o}m method to\nefficiently compute a kernel basis for balancing in {nearly} linear time and\nspace, and then use the specialized first-order alternating direction method of\nmultipliers to rapidly find the optimal weights. In an extensive simulation\nstudy, we provide new insights into the performance of weighting estimators in\nlarge datasets, showing that the proposed approach substantially outperforms\nothers in terms of accuracy and speed. Finally, we use this weighting approach\nto conduct a national study of the relationship between hospital profit status\nand heart attack outcomes in a comprehensive dataset of 1.27 million patients.\nWe find that for-profit hospitals use interventional cardiology to treat heart\nattacks at similar rates as other hospitals, but have higher mortality and\nreadmission rates."}, "http://arxiv.org/abs/2311.00577": {"title": "Personalized Assignment to One of Many Treatment Arms via Regularized and Clustered Joint Assignment Forests", "link": "http://arxiv.org/abs/2311.00577", "description": "We consider learning personalized assignments to one of many treatment arms\nfrom a randomized controlled trial. Standard methods that estimate\nheterogeneous treatment effects separately for each arm may perform poorly in\nthis case due to excess variance. We instead propose methods that pool\ninformation across treatment arms: First, we consider a regularized\nforest-based assignment algorithm based on greedy recursive partitioning that\nshrinks effect estimates across arms. Second, we augment our algorithm by a\nclustering scheme that combines treatment arms with consistently similar\noutcomes. In a simulation study, we compare the performance of these approaches\nto predicting arm-wise outcomes separately, and document gains of directly\noptimizing the treatment assignment with regularization and clustering. In a\ntheoretical model, we illustrate how a high number of treatment arms makes\nfinding the best arm hard, while we can achieve sizable utility gains from\npersonalization by regularized optimization."}, "http://arxiv.org/abs/2311.00596": {"title": "Evaluating Binary Outcome Classifiers Estimated from Survey Data", "link": "http://arxiv.org/abs/2311.00596", "description": "Surveys are commonly used to facilitate research in epidemiology, health, and\nthe social and behavioral sciences. Often, these surveys are not simple random\nsamples, and respondents are given weights reflecting their probability of\nselection into the survey. It is well known that analysts can use these survey\nweights to produce unbiased estimates of population quantities like totals. In\nthis article, we show that survey weights also can be beneficial for evaluating\nthe quality of predictive models when splitting data into training and test\nsets. In particular, we characterize model assessment statistics, such as\nsensitivity and specificity, as finite population quantities, and compute\nsurvey-weighted estimates of these quantities with sample test data comprising\na random subset of the original data.Using simulations with data from the\nNational Survey on Drug Use and Health and the National Comorbidity Survey, we\nshow that unweighted metrics estimated with sample test data can misrepresent\npopulation performance, but weighted metrics appropriately adjust for the\ncomplex sampling design. We also show that this conclusion holds for models\ntrained using upsampling for mitigating class imbalance. The results suggest\nthat weighted metrics should be used when evaluating performance on sample test\ndata."}, "http://arxiv.org/abs/2206.10866": {"title": "Nearest Neighbor Classification based on Imbalanced Data: A Statistical Approach", "link": "http://arxiv.org/abs/2206.10866", "description": "When the competing classes in a classification problem are not of comparable\nsize, many popular classifiers exhibit a bias towards larger classes, and the\nnearest neighbor classifier is no exception. To take care of this problem, we\ndevelop a statistical method for nearest neighbor classification based on such\nimbalanced data sets. First, we construct a classifier for the binary\nclassification problem and then extend it for classification problems involving\nmore than two classes. Unlike the existing oversampling or undersampling\nmethods, our proposed classifiers do not need to generate any pseudo\nobservations or remove any existing observations, hence the results are exactly\nreproducible. We establish the Bayes risk consistency of these classifiers\nunder appropriate regularity conditions. Their superior performance over the\nexisting methods is amply demonstrated by analyzing several benchmark data\nsets."}, "http://arxiv.org/abs/2209.08892": {"title": "High-dimensional data segmentation in regression settings permitting temporal dependence and non-Gaussianity", "link": "http://arxiv.org/abs/2209.08892", "description": "We propose a data segmentation methodology for the high-dimensional linear\nregression problem where regression parameters are allowed to undergo multiple\nchanges. The proposed methodology, MOSEG, proceeds in two stages: first, the\ndata are scanned for multiple change points using a moving window-based\nprocedure, which is followed by a location refinement stage. MOSEG enjoys\ncomputational efficiency thanks to the adoption of a coarse grid in the first\nstage, and achieves theoretical consistency in estimating both the total number\nand the locations of the change points, under general conditions permitting\nserial dependence and non-Gaussianity. We also propose MOSEG.MS, a multiscale\nextension of MOSEG which, while comparable to MOSEG in terms of computational\ncomplexity, achieves theoretical consistency for a broader parameter space\nwhere large parameter shifts over short intervals and small changes over long\nstretches of stationarity are simultaneously allowed. We demonstrate good\nperformance of the proposed methods in comparative simulation studies and in an\napplication to predicting the equity premium."}, "http://arxiv.org/abs/2210.02341": {"title": "A Distributed Block-Split Gibbs Sampler with Hypergraph Structure for High-Dimensional Inverse Problems", "link": "http://arxiv.org/abs/2210.02341", "description": "Sampling-based algorithms are classical approaches to perform Bayesian\ninference in inverse problems. They provide estimators with the associated\ncredibility intervals to quantify the uncertainty on the estimators. Although\nthese methods hardly scale to high dimensional problems, they have recently\nbeen paired with optimization techniques, such as proximal and splitting\napproaches, to address this issue. Such approaches pave the way to distributed\nsamplers, splitting computations to make inference more scalable and faster. We\nintroduce a distributed Split Gibbs sampler (SGS) to efficiently solve such\nproblems involving distributions with multiple smooth and non-smooth functions\ncomposed with linear operators. The proposed approach leverages a recent\napproximate augmentation technique reminiscent of primal-dual optimization\nmethods. It is further combined with a block-coordinate approach to split the\nprimal and dual variables into blocks, leading to a distributed\nblock-coordinate SGS. The resulting algorithm exploits the hypergraph structure\nof the involved linear operators to efficiently distribute the variables over\nmultiple workers under controlled communication costs. It accommodates several\ndistributed architectures, such as the Single Program Multiple Data and\nclient-server architectures. Experiments on a large image deblurring problem\nshow the performance of the proposed approach to produce high quality estimates\nwith credibility intervals in a small amount of time. Supplementary material to\nreproduce the experiments is available online."}, "http://arxiv.org/abs/2210.14086": {"title": "A Global Wavelet Based Bootstrapped Test of Covariance Stationarity", "link": "http://arxiv.org/abs/2210.14086", "description": "We propose a covariance stationarity test for an otherwise dependent and\npossibly globally non-stationary time series. We work in a generalized version\nof the new setting in Jin, Wang and Wang (2015), who exploit Walsh (1923)\nfunctions in order to compare sub-sample covariances with the full sample\ncounterpart. They impose strict stationarity under the null, only consider\nlinear processes under either hypothesis in order to achieve a parametric\nestimator for an inverted high dimensional asymptotic covariance matrix, and do\nnot consider any other orthonormal basis. Conversely, we work with a general\northonormal basis under mild conditions that include Haar wavelet and Walsh\nfunctions; and we allow for linear or nonlinear processes with possibly non-iid\ninnovations. This is important in macroeconomics and finance where nonlinear\nfeedback and random volatility occur in many settings. We completely sidestep\nasymptotic covariance matrix estimation and inversion by bootstrapping a\nmax-correlation difference statistic, where the maximum is taken over the\ncorrelation lag $h$ and basis generated sub-sample counter $k$ (the number of\nsystematic samples). We achieve a higher feasible rate of increase for the\nmaximum lag and counter $\\mathcal{H}_{T}$ and $\\mathcal{K}_{T}$. Of particular\nnote, our test is capable of detecting breaks in variance, and distant, or very\nmild, deviations from stationarity."}, "http://arxiv.org/abs/2211.03031": {"title": "A framework for leveraging machine learning tools to estimate personalized survival curves", "link": "http://arxiv.org/abs/2211.03031", "description": "The conditional survival function of a time-to-event outcome subject to\ncensoring and truncation is a common target of estimation in survival analysis.\nThis parameter may be of scientific interest and also often appears as a\nnuisance in nonparametric and semiparametric problems. In addition to classical\nparametric and semiparametric methods (e.g., based on the Cox proportional\nhazards model), flexible machine learning approaches have been developed to\nestimate the conditional survival function. However, many of these methods are\neither implicitly or explicitly targeted toward risk stratification rather than\noverall survival function estimation. Others apply only to discrete-time\nsettings or require inverse probability of censoring weights, which can be as\ndifficult to estimate as the outcome survival function itself. Here, we employ\na decomposition of the conditional survival function in terms of observable\nregression models in which censoring and truncation play no role. This allows\napplication of an array of flexible regression and classification methods\nrather than only approaches that explicitly handle the complexities inherent to\nsurvival data. We outline estimation procedures based on this decomposition,\nempirically assess their performance, and demonstrate their use on data from an\nHIV vaccine trial."}, "http://arxiv.org/abs/2301.12389": {"title": "On Learning Necessary and Sufficient Causal Graphs", "link": "http://arxiv.org/abs/2301.12389", "description": "The causal revolution has stimulated interest in understanding complex\nrelationships in various fields. Most of the existing methods aim to discover\ncausal relationships among all variables within a complex large-scale graph.\nHowever, in practice, only a small subset of variables in the graph are\nrelevant to the outcomes of interest. Consequently, causal estimation with the\nfull causal graph -- particularly given limited data -- could lead to numerous\nfalsely discovered, spurious variables that exhibit high correlation with, but\nexert no causal impact on, the target outcome. In this paper, we propose\nlearning a class of necessary and sufficient causal graphs (NSCG) that\nexclusively comprises causally relevant variables for an outcome of interest,\nwhich we term causal features. The key idea is to employ probabilities of\ncausation to systematically evaluate the importance of features in the causal\ngraph, allowing us to identify a subgraph relevant to the outcome of interest.\nTo learn NSCG from data, we develop a necessary and sufficient causal\nstructural learning (NSCSL) algorithm, by establishing theoretical properties\nand relationships between probabilities of causation and natural causal effects\nof features. Across empirical studies of simulated and real data, we\ndemonstrate that NSCSL outperforms existing algorithms and can reveal crucial\nyeast genes for target heritable traits of interest."}, "http://arxiv.org/abs/2303.18211": {"title": "A Scale-Invariant Sorting Criterion to Find a Causal Order in Additive Noise Models", "link": "http://arxiv.org/abs/2303.18211", "description": "Additive Noise Models (ANMs) are a common model class for causal discovery\nfrom observational data and are often used to generate synthetic data for\ncausal discovery benchmarking. Specifying an ANM requires choosing all\nparameters, including those not fixed by explicit assumptions. Reisach et al.\n(2021) show that sorting variables by increasing variance often yields an\nordering close to a causal order and introduce var-sortability to quantify this\nalignment. Since increasing variances may be unrealistic and are\nscale-dependent, ANM data are often standardized in benchmarks.\n\nWe show that synthetic ANM data are characterized by another pattern that is\nscale-invariant: the explainable fraction of a variable's variance, as captured\nby the coefficient of determination $R^2$, tends to increase along the causal\norder. The result is high $R^2$-sortability, meaning that sorting the variables\nby increasing $R^2$ yields an ordering close to a causal order. We propose an\nefficient baseline algorithm termed $R^2$-SortnRegress that exploits high\n$R^2$-sortability and that can match and exceed the performance of established\ncausal discovery algorithms. We show analytically that sufficiently high edge\nweights lead to a relative decrease of the noise contributions along causal\nchains, resulting in increasingly deterministic relationships and high $R^2$.\nWe characterize $R^2$-sortability for different simulation parameters and find\nhigh values in common settings. Our findings reveal high $R^2$-sortability as\nan assumption about the data generating process relevant to causal discovery\nand implicit in many ANM sampling schemes. It should be made explicit, as its\nprevalence in real-world data is unknown. For causal discovery benchmarking, we\nimplement $R^2$-sortability, the $R^2$-SortnRegress algorithm, and ANM\nsimulation procedures in our library CausalDisco at\nhttps://causaldisco.github.io/CausalDisco/."}, "http://arxiv.org/abs/2304.11491": {"title": "Bayesian Boundary Trend Filtering", "link": "http://arxiv.org/abs/2304.11491", "description": "Estimating boundary curves has many applications such as economics, climate\nscience, and medicine. Bayesian trend filtering has been developed as one of\nlocally adaptive smoothing methods to estimate the non-stationary trend of\ndata. This paper develops a Bayesian trend filtering for estimating boundary\ntrend. To this end, the truncated multivariate normal working likelihood and\nglobal-local shrinkage priors based on scale mixtures of normal distribution\nare introduced. In particular, well-known horseshoe prior for difference leads\nto locally adaptive shrinkage estimation for boundary trend. However, the full\nconditional distributions of the Gibbs sampler involve high-dimensional\ntruncated multivariate normal distribution. To overcome the difficulty of\nsampling, an approximation of truncated multivariate normal distribution is\nemployed. Using the approximation, the proposed models lead to an efficient\nGibbs sampling algorithm via P\\'olya-Gamma data augmentation. The proposed\nmethod is also extended by considering nearly isotonic constraint. The\nperformance of the proposed method is illustrated through some numerical\nexperiments and real data examples."}, "http://arxiv.org/abs/2304.13237": {"title": "An Efficient Doubly-Robust Test for the Kernel Treatment Effect", "link": "http://arxiv.org/abs/2304.13237", "description": "The average treatment effect, which is the difference in expectation of the\ncounterfactuals, is probably the most popular target effect in causal inference\nwith binary treatments. However, treatments may have effects beyond the mean,\nfor instance decreasing or increasing the variance. We propose a new\nkernel-based test for distributional effects of the treatment. It is, to the\nbest of our knowledge, the first kernel-based, doubly-robust test with provably\nvalid type-I error. Furthermore, our proposed algorithm is computationally\nefficient, avoiding the use of permutations."}, "http://arxiv.org/abs/2305.07981": {"title": "Inferring Stochastic Group Interactions within Structured Populations via Coupled Autoregression", "link": "http://arxiv.org/abs/2305.07981", "description": "The internal behaviour of a population is an important feature to take\naccount of when modelling their dynamics. In line with kin selection theory,\nmany social species tend to cluster into distinct groups in order to enhance\ntheir overall population fitness. Temporal interactions between populations are\noften modelled using classical mathematical models, but these sometimes fail to\ndelve deeper into the, often uncertain, relationships within populations. Here,\nwe introduce a stochastic framework that aims to capture the interactions of\nanimal groups and an auxiliary population over time. We demonstrate the model's\ncapabilities, from a Bayesian perspective, through simulation studies and by\nfitting it to predator-prey count time series data. We then derive an\napproximation to the group correlation structure within such a population,\nwhile also taking account of the effect of the auxiliary population. We finally\ndiscuss how this approximation can lead to ecologically realistic\ninterpretations in a predator-prey context. This approximation also serves as\nverification to whether the population in question satisfies our various\nassumptions. Our modelling approach will be useful for empiricists for\nmonitoring groups within a conservation framework and also theoreticians\nwanting to quantify interactions, to study cooperation and other phenomena\nwithin social populations."}, "http://arxiv.org/abs/2306.09520": {"title": "Ensembled Prediction Intervals for Causal Outcomes Under Hidden Confounding", "link": "http://arxiv.org/abs/2306.09520", "description": "Causal inference of exact individual treatment outcomes in the presence of\nhidden confounders is rarely possible. Recent work has extended prediction\nintervals with finite-sample guarantees to partially identifiable causal\noutcomes, by means of a sensitivity model for hidden confounding. In deep\nlearning, predictors can exploit their inductive biases for better\ngeneralization out of sample. We argue that the structure inherent to a deep\nensemble should inform a tighter partial identification of the causal outcomes\nthat they predict. We therefore introduce an approach termed Caus-Modens, for\ncharacterizing causal outcome intervals by modulated ensembles. We present a\nsimple approach to partial identification using existing causal sensitivity\nmodels and show empirically that Caus-Modens gives tighter outcome intervals,\nas measured by the necessary interval size to achieve sufficient coverage. The\nlast of our three diverse benchmarks is a novel usage of GPT-4 for\nobservational experiments with unknown but probeable ground truth."}, "http://arxiv.org/abs/2306.16838": {"title": "Solving Kernel Ridge Regression with Gradient-Based Optimization Methods", "link": "http://arxiv.org/abs/2306.16838", "description": "Kernel ridge regression, KRR, is a generalization of linear ridge regression\nthat is non-linear in the data, but linear in the parameters. Here, we\nintroduce an equivalent formulation of the objective function of KRR, opening\nup both for using penalties other than the ridge penalty and for studying\nkernel ridge regression from the perspective of gradient descent. Using a\ncontinuous-time perspective, we derive a closed-form solution for solving\nkernel regression with gradient descent, something we refer to as kernel\ngradient flow, KGF, and theoretically bound the differences between KRR and\nKGF, where, for the latter, regularization is obtained through early stopping.\nWe also generalize KRR by replacing the ridge penalty with the $\\ell_1$ and\n$\\ell_\\infty$ penalties, respectively, and use the fact that analogous to the\nsimilarities between KGF and KRR, $\\ell_1$ regularization and forward stagewise\nregression (also known as coordinate descent), and $\\ell_\\infty$ regularization\nand sign gradient descent, follow similar solution paths. We can thus alleviate\nthe need for computationally heavy algorithms based on proximal gradient\ndescent. We show theoretically and empirically how the $\\ell_1$ and\n$\\ell_\\infty$ penalties, and the corresponding gradient-based optimization\nalgorithms, produce sparse and robust kernel regression solutions,\nrespectively."}, "http://arxiv.org/abs/2311.00820": {"title": "Bayesian inference for generalized linear models via quasi-posteriors", "link": "http://arxiv.org/abs/2311.00820", "description": "Generalized linear models (GLMs) are routinely used for modeling\nrelationships between a response variable and a set of covariates. The simple\nform of a GLM comes with easy interpretability, but also leads to concerns\nabout model misspecification impacting inferential conclusions. A popular\nsemi-parametric solution adopted in the frequentist literature is\nquasi-likelihood, which improves robustness by only requiring correct\nspecification of the first two moments. We develop a robust approach to\nBayesian inference in GLMs through quasi-posterior distributions. We show that\nquasi-posteriors provide a coherent generalized Bayes inference method, while\nalso approximating so-called coarsened posteriors. In so doing, we obtain new\ninsights into the choice of coarsening parameter. Asymptotically, the\nquasi-posterior converges in total variation to a normal distribution and has\nimportant connections with the loss-likelihood bootstrap posterior. We\ndemonstrate that it is also well-calibrated in terms of frequentist coverage.\nMoreover, the loss-scale parameter has a clear interpretation as a dispersion,\nand this leads to a consolidated method of moments estimator."}, "http://arxiv.org/abs/2311.00878": {"title": "Backward Joint Model for Dynamic Prediction using Multivariate Longitudinal and Competing Risk Data", "link": "http://arxiv.org/abs/2311.00878", "description": "Joint modeling is a useful approach to dynamic prediction of clinical\noutcomes using longitudinally measured predictors. When the outcomes are\ncompeting risk events, fitting the conventional shared random effects joint\nmodel often involves intensive computation, especially when multiple\nlongitudinal biomarkers are be used as predictors, as is often desired in\nprediction problems. Motivated by a longitudinal cohort study of chronic kidney\ndisease, this paper proposes a new joint model for the dynamic prediction of\nend-stage renal disease with the competing risk of death. The model factorizes\nthe likelihood into the distribution of the competing risks data and the\ndistribution of longitudinal data given the competing risks data. The\nestimation with the EM algorithm is efficient, stable and fast, with a\none-dimensional integral in the E-step and convex optimization for most\nparameters in the M-step, regardless of the number of longitudinal predictors.\nThe model also comes with a consistent albeit less efficient estimation method\nthat can be quickly implemented with standard software, ideal for model\nbuilding and diagnotics. This model enables the prediction of future\nlongitudinal data trajectories conditional on being at risk at a future time, a\npractically significant problem that has not been studied in the statistical\nliterature. We study the properties of the proposed method using simulations\nand a real dataset and compare its performance with the shared random effects\njoint model."}, "http://arxiv.org/abs/2311.00885": {"title": "Controlling the number of significant effects in multiple testing", "link": "http://arxiv.org/abs/2311.00885", "description": "In multiple testing several criteria to control for type I errors exist. The\nfalse discovery rate, which evaluates the expected proportion of false\ndiscoveries among the rejected null hypotheses, has become the standard\napproach in this setting. However, false discovery rate control may be too\nconservative when the effects are weak. In this paper we alternatively propose\nto control the number of significant effects, where 'significant' refers to a\npre-specified threshold $\\gamma$. This means that a $(1-\\alpha)$-lower\nconfidence bound $L$ for the number of non-true null hypothesis with p-values\nbelow $\\gamma$ is provided. When one rejects the nulls corresponding to the $L$\nsmallest p-values, the probability that the number of false positives exceeds\nthe number of false negatives among the significant effects is bounded by\n$\\alpha$. Relative merits of the proposed criterion are discussed. Procedures\nto control for the number of significant effects in practice are introduced and\ninvestigated both theoretically and through simulations. Illustrative real data\napplications are given."}, "http://arxiv.org/abs/2311.00923": {"title": "A Review and Roadmap of Deep Causal Model from Different Causal Structures and Representations", "link": "http://arxiv.org/abs/2311.00923", "description": "The fusion of causal models with deep learning introducing increasingly\nintricate data sets, such as the causal associations within images or between\ntextual components, has surfaced as a focal research area. Nonetheless, the\nbroadening of original causal concepts and theories to such complex,\nnon-statistical data has been met with serious challenges. In response, our\nstudy proposes redefinitions of causal data into three distinct categories from\nthe standpoint of causal structure and representation: definite data,\nsemi-definite data, and indefinite data. Definite data chiefly pertains to\nstatistical data used in conventional causal scenarios, while semi-definite\ndata refers to a spectrum of data formats germane to deep learning, including\ntime-series, images, text, and others. Indefinite data is an emergent research\nsphere inferred from the progression of data forms by us. To comprehensively\npresent these three data paradigms, we elaborate on their formal definitions,\ndifferences manifested in datasets, resolution pathways, and development of\nresearch. We summarize key tasks and achievements pertaining to definite and\nsemi-definite data from myriad research undertakings, present a roadmap for\nindefinite data, beginning with its current research conundrums. Lastly, we\nclassify and scrutinize the key datasets presently utilized within these three\nparadigms."}, "http://arxiv.org/abs/2311.00927": {"title": "Scalable Counterfactual Distribution Estimation in Multivariate Causal Models", "link": "http://arxiv.org/abs/2311.00927", "description": "We consider the problem of estimating the counterfactual joint distribution\nof multiple quantities of interests (e.g., outcomes) in a multivariate causal\nmodel extended from the classical difference-in-difference design. Existing\nmethods for this task either ignore the correlation structures among dimensions\nof the multivariate outcome by considering univariate causal models on each\ndimension separately and hence produce incorrect counterfactual distributions,\nor poorly scale even for moderate-size datasets when directly dealing with such\nmultivariate causal model. We propose a method that alleviates both issues\nsimultaneously by leveraging a robust latent one-dimensional subspace of the\noriginal high-dimension space and exploiting the efficient estimation from the\nunivariate causal model on such space. Since the construction of the\none-dimensional subspace uses information from all the dimensions, our method\ncan capture the correlation structures and produce good estimates of the\ncounterfactual distribution. We demonstrate the advantages of our approach over\nexisting methods on both synthetic and real-world data."}, "http://arxiv.org/abs/2311.01021": {"title": "ABC-based Forecasting in State Space Models", "link": "http://arxiv.org/abs/2311.01021", "description": "Approximate Bayesian Computation (ABC) has gained popularity as a method for\nconducting inference and forecasting in complex models, most notably those\nwhich are intractable in some sense. In this paper we use ABC to produce\nprobabilistic forecasts in state space models (SSMs). Whilst ABC-based\nforecasting in correctly-specified SSMs has been studied, the misspecified case\nhas not been investigated, and it is that case which we emphasize. We invoke\nrecent principles of 'focused' Bayesian prediction, whereby Bayesian updates\nare driven by a scoring rule that rewards predictive accuracy; the aim being to\nproduce predictives that perform well in that rule, despite misspecification.\nTwo methods are investigated for producing the focused predictions. In a\nsimulation setting, 'coherent' predictions are in evidence for both methods:\nthe predictive constructed via the use of a particular scoring rule predicts\nbest according to that rule. Importantly, both focused methods typically\nproduce more accurate forecasts than an exact, but misspecified, predictive. An\nempirical application to a truly intractable SSM completes the paper."}, "http://arxiv.org/abs/2311.01147": {"title": "Variational Inference for Sparse Poisson Regression", "link": "http://arxiv.org/abs/2311.01147", "description": "We have utilized the non-conjugate VB method for the problem of the sparse\nPoisson regression model. To provide an approximated conjugacy in the model,\nthe likelihood is approximated by a quadratic function, which provides the\nconjugacy of the approximation component with the Gaussian prior to the\nregression coefficient. Three sparsity-enforcing priors are used for this\nproblem. The proposed models are compared with each other and two frequentist\nsparse Poisson methods (LASSO and SCAD) to evaluate the prediction performance,\nas well as, the sparsing performance of the proposed methods. Throughout a\nsimulated data example, the accuracy of the VB methods is computed compared to\nthe corresponding benchmark MCMC methods. It can be observed that the proposed\nVB methods have provided a good approximation to the posterior distribution of\nthe parameters, while the VB methods are much faster than the MCMC ones. Using\nseveral benchmark count response data sets, the prediction performance of the\nproposed methods is evaluated in real-world applications."}, "http://arxiv.org/abs/2311.01287": {"title": "Semiparametric Latent ANOVA Model for Event-Related Potentials", "link": "http://arxiv.org/abs/2311.01287", "description": "Event-related potentials (ERPs) extracted from electroencephalography (EEG)\ndata in response to stimuli are widely used in psychological and neuroscience\nexperiments. A major goal is to link ERP characteristic components to\nsubject-level covariates. Existing methods typically follow two-step\napproaches, first identifying ERP components using peak detection methods and\nthen relating them to the covariates. This approach, however, can lead to loss\nof efficiency due to inaccurate estimates in the initial step, especially\nconsidering the low signal-to-noise ratio of EEG data. To address this\nchallenge, we propose a semiparametric latent ANOVA model (SLAM) that unifies\ninference on ERP components and their association to covariates. SLAM models\nERP waveforms via a structured Gaussian process prior that encodes ERP latency\nin its derivative and links the subject-level latencies to covariates using a\nlatent ANOVA. This unified Bayesian framework provides estimation at both\npopulation- and subject- levels, improving the efficiency of the inference by\nleveraging information across subjects. We automate posterior inference and\nhyperparameter tuning using a Monte Carlo expectation-maximization algorithm.\nWe demonstrate the advantages of SLAM over competing methods via simulations.\nOur method allows us to examine how factors or covariates affect the magnitude\nand/or latency of ERP components, which in turn reflect cognitive,\npsychological or neural processes. We exemplify this via an application to data\nfrom an ERP experiment on speech recognition, where we assess the effect of age\non two components of interest. Our results verify the scientific findings that\nolder people take a longer reaction time to respond to external stimuli because\nof the delay in perception and brain processes."}, "http://arxiv.org/abs/2311.01297": {"title": "Bias correction in multiple-systems estimation", "link": "http://arxiv.org/abs/2311.01297", "description": "If part of a population is hidden but two or more sources are available that\neach cover parts of this population, dual- or multiple-system(s) estimation can\nbe applied to estimate this population. For this it is common to use the\nlog-linear model, estimated with maximum likelihood. These maximum likelihood\nestimates are based on a non-linear model and therefore suffer from\nfinite-sample bias, which can be substantial in case of small samples or a\nsmall population size. This problem was recognised by Chapman, who derived an\nestimator with good small sample properties in case of two available sources.\nHowever, he did not derive an estimator for more than two sources. We propose\nan estimator that is an extension of Chapman's estimator to three or more\nsources and compare this estimator with other bias-reduced estimators in a\nsimulation study. The proposed estimator performs well, and much better than\nthe other estimators. A real data example on homelessness in the Netherlands\nshows that our proposed model can make a substantial difference."}, "http://arxiv.org/abs/2311.01301": {"title": "TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models", "link": "http://arxiv.org/abs/2311.01301", "description": "The rapid digitization of real-world data offers an unprecedented opportunity\nfor optimizing healthcare delivery and accelerating biomedical discovery. In\npractice, however, such data is most abundantly available in unstructured\nforms, such as clinical notes in electronic medical records (EMRs), and it is\ngenerally plagued by confounders. In this paper, we present TRIALSCOPE, a\nunifying framework for distilling real-world evidence from population-level\nobservational data. TRIALSCOPE leverages biomedical language models to\nstructure clinical text at scale, employs advanced probabilistic modeling for\ndenoising and imputation, and incorporates state-of-the-art causal inference\ntechniques to combat common confounders. Using clinical trial specification as\ngeneric representation, TRIALSCOPE provides a turn-key solution to generate and\nreason with clinical hypotheses using observational data. In extensive\nexperiments and analyses on a large-scale real-world dataset with over one\nmillion cancer patients from a large US healthcare network, we show that\nTRIALSCOPE can produce high-quality structuring of real-world data and\ngenerates comparable results to marquee cancer trials. In addition to\nfacilitating in-silicon clinical trial design and optimization, TRIALSCOPE may\nbe used to empower synthetic controls, pragmatic trials, post-market\nsurveillance, as well as support fine-grained patient-like-me reasoning in\nprecision diagnosis and treatment."}, "http://arxiv.org/abs/2311.01303": {"title": "Local differential privacy in survival analysis using private failure indicators", "link": "http://arxiv.org/abs/2311.01303", "description": "We consider the estimation of the cumulative hazard function, and\nequivalently the distribution function, with censored data under a setup that\npreserves the privacy of the survival database. This is done through a\n$\\alpha$-locally differentially private mechanism for the failure indicators\nand by proposing a non-parametric kernel estimator for the cumulative hazard\nfunction that remains consistent under the privatization. Under mild\nconditions, we also prove lowers bounds for the minimax rates of convergence\nand show that estimator is minimax optimal under a well-chosen bandwidth."}, "http://arxiv.org/abs/2311.01341": {"title": "Composite Dyadic Models for Spatio-Temporal Data", "link": "http://arxiv.org/abs/2311.01341", "description": "Mechanistic statistical models are commonly used to study the flow of\nbiological processes. For example, in landscape genetics, the aim is to infer\nmechanisms that govern gene flow in populations. Existing statistical\napproaches in landscape genetics do not account for temporal dependence in the\ndata and may be computationally prohibitive. We infer mechanisms with a\nBayesian hierarchical dyadic model that scales well with large data sets and\nthat accounts for spatial and temporal dependence. We construct a\nfully-connected network comprising spatio-temporal data for the dyadic model\nand use normalized composite likelihoods to account for the dependence\nstructure in space and time. Our motivation for developing a dyadic model was\nto account for physical mechanisms commonly found in physical-statistical\nmodels. However, a numerical solver is not required in our approach because we\nmodel first-order changes directly. We apply our methods to ancient human DNA\ndata to infer the mechanisms that affected human movement in Bronze Age Europe."}, "http://arxiv.org/abs/2311.01412": {"title": "Castor: Causal Temporal Regime Structure Learning", "link": "http://arxiv.org/abs/2311.01412", "description": "The task of uncovering causal relationships among multivariate time series\ndata stands as an essential and challenging objective that cuts across a broad\narray of disciplines ranging from climate science to healthcare. Such data\nentails linear or non-linear relationships, and usually follow multiple a\npriori unknown regimes. Existing causal discovery methods can infer summary\ncausal graphs from heterogeneous data with known regimes, but they fall short\nin comprehensively learning both regimes and the corresponding causal graph. In\nthis paper, we introduce CASTOR, a novel framework designed to learn causal\nrelationships in heterogeneous time series data composed of various regimes,\neach governed by a distinct causal graph. Through the maximization of a score\nfunction via the EM algorithm, CASTOR infers the number of regimes and learns\nlinear or non-linear causal relationships in each regime. We demonstrate the\nrobust convergence properties of CASTOR, specifically highlighting its\nproficiency in accurately identifying unique regimes. Empirical evidence,\ngarnered from exhaustive synthetic experiments and two real-world benchmarks,\nconfirm CASTOR's superior performance in causal discovery compared to baseline\nmethods. By learning a full temporal causal graph for each regime, CASTOR\nestablishes itself as a distinctly interpretable method for causal discovery in\nheterogeneous time series."}, "http://arxiv.org/abs/2311.01453": {"title": "PPI++: Efficient Prediction-Powered Inference", "link": "http://arxiv.org/abs/2311.01453", "description": "We present PPI++: a computationally lightweight methodology for estimation\nand inference based on a small labeled dataset and a typically much larger\ndataset of machine-learning predictions. The methods automatically adapt to the\nquality of available predictions, yielding easy-to-compute confidence sets --\nfor parameters of any dimensionality -- that always improve on classical\nintervals using only the labeled data. PPI++ builds on prediction-powered\ninference (PPI), which targets the same problem setting, improving its\ncomputational and statistical efficiency. Real and synthetic experiments\ndemonstrate the benefits of the proposed adaptations."}, "http://arxiv.org/abs/2008.00707": {"title": "Heterogeneous Treatment and Spillover Effects under Clustered Network Interference", "link": "http://arxiv.org/abs/2008.00707", "description": "The bulk of causal inference studies rule out the presence of interference\nbetween units. However, in many real-world scenarios, units are interconnected\nby social, physical, or virtual ties, and the effect of the treatment can spill\nfrom one unit to other connected individuals in the network. In this paper, we\ndevelop a machine learning method that uses tree-based algorithms and a\nHorvitz-Thompson estimator to assess the heterogeneity of treatment and\nspillover effects with respect to individual, neighborhood, and network\ncharacteristics in the context of clustered networks and neighborhood\ninterference within clusters. The proposed Network Causal Tree (NCT) algorithm\nhas several advantages. First, it allows the investigation of the treatment\neffect heterogeneity, avoiding potential bias due to the presence of\ninterference. Second, understanding the heterogeneity of both treatment and\nspillover effects can guide policy-makers in scaling up interventions,\ndesigning targeting strategies, and increasing cost-effectiveness. We\ninvestigate the performance of our NCT method using a Monte Carlo simulation\nstudy, and we illustrate its application to assess the heterogeneous effects of\ninformation sessions on the uptake of a new weather insurance policy in rural\nChina."}, "http://arxiv.org/abs/2107.01773": {"title": "Extending Latent Basis Growth Model to Explore Joint Development in the Framework of Individual Measurement Occasions", "link": "http://arxiv.org/abs/2107.01773", "description": "Longitudinal processes often pose nonlinear change patterns. Latent basis\ngrowth models (LBGMs) provide a versatile solution without requiring specific\nfunctional forms. Building on the LBGM specification for unequally-spaced waves\nand individual occasions proposed by Liu and Perera (2023), we extend LBGMs to\nmultivariate longitudinal outcomes. This provides a unified approach to\nnonlinear, interconnected trajectories. Simulation studies demonstrate that the\nproposed model can provide unbiased and accurate estimates with target coverage\nprobabilities for the parameters of interest. Real-world analyses of reading\nand mathematics scores demonstrates its effectiveness in analyzing joint\ndevelopmental processes that vary in temporal patterns. Computational code is\nincluded."}, "http://arxiv.org/abs/2112.03152": {"title": "Bounding Wasserstein distance with couplings", "link": "http://arxiv.org/abs/2112.03152", "description": "Markov chain Monte Carlo (MCMC) provides asymptotically consistent estimates\nof intractable posterior expectations as the number of iterations tends to\ninfinity. However, in large data applications, MCMC can be computationally\nexpensive per iteration. This has catalyzed interest in approximating MCMC in a\nmanner that improves computational speed per iteration but does not produce\nasymptotically consistent estimates. In this article, we propose estimators\nbased on couplings of Markov chains to assess the quality of such\nasymptotically biased sampling methods. The estimators give empirical upper\nbounds of the Wasserstein distance between the limiting distribution of the\nasymptotically biased sampling method and the original target distribution of\ninterest. We establish theoretical guarantees for our upper bounds and show\nthat our estimators can remain effective in high dimensions. We apply our\nquality measures to stochastic gradient MCMC, variational Bayes, and Laplace\napproximations for tall data and to approximate MCMC for Bayesian logistic\nregression in 4500 dimensions and Bayesian linear regression in 50000\ndimensions."}, "http://arxiv.org/abs/2112.13398": {"title": "Long Story Short: Omitted Variable Bias in Causal Machine Learning", "link": "http://arxiv.org/abs/2112.13398", "description": "We derive general, yet simple, sharp bounds on the size of the omitted\nvariable bias for a broad class of causal parameters that can be identified as\nlinear functionals of the conditional expectation function of the outcome. Such\nfunctionals encompass many of the traditional targets of investigation in\ncausal inference studies, such as, for example, (weighted) average of potential\noutcomes, average treatment effects (including subgroup effects, such as the\neffect on the treated), (weighted) average derivatives, and policy effects from\nshifts in covariate distribution -- all for general, nonparametric causal\nmodels. Our construction relies on the Riesz-Frechet representation of the\ntarget functional. Specifically, we show how the bound on the bias depends only\non the additional variation that the latent variables create both in the\noutcome and in the Riesz representer for the parameter of interest. Moreover,\nin many important cases (e.g, average treatment effects and avearage\nderivatives) the bound is shown to depend on easily interpretable quantities\nthat measure the explanatory power of the omitted variables. Therefore, simple\nplausibility judgments on the maximum explanatory power of omitted variables\n(in explaining treatment and outcome variation) are sufficient to place overall\nbounds on the size of the bias. Furthermore, we use debiased machine learning\nto provide flexible and efficient statistical inference on learnable components\nof the bounds. Finally, empirical examples demonstrate the usefulness of the\napproach."}, "http://arxiv.org/abs/2204.02954": {"title": "Strongly convergent homogeneous approximations to inhomogeneous Markov jump processes and applications", "link": "http://arxiv.org/abs/2204.02954", "description": "The study of time-inhomogeneous Markov jump processes is a traditional topic\nwithin probability theory that has recently attracted substantial attention in\nvarious applications. However, their flexibility also incurs a substantial\nmathematical burden which is usually circumvented by using well-known generic\ndistributional approximations or simulations. This article provides a novel\napproximation method that tailors the dynamics of a time-homogeneous Markov\njump process to meet those of its time-inhomogeneous counterpart on an\nincreasingly fine Poisson grid. Strong convergence of the processes in terms of\nthe Skorokhod $J_1$ metric is established, and convergence rates are provided.\nUnder traditional regularity assumptions, distributional convergence is\nestablished for unconditional proxies, to the same limit. Special attention is\ndevoted to the case where the target process has one absorbing state and the\nremaining ones transient, for which the absorption times also converge. Some\napplications are outlined, such as univariate hazard-rate density estimation,\nruin probabilities, and multivariate phase-type density evaluation."}, "http://arxiv.org/abs/2301.07210": {"title": "Causal Falsification of Digital Twins", "link": "http://arxiv.org/abs/2301.07210", "description": "Digital twins are virtual systems designed to predict how a real-world\nprocess will evolve in response to interventions. This modelling paradigm holds\nsubstantial promise in many applications, but rigorous procedures for assessing\ntheir accuracy are essential for safety-critical settings. We consider how to\nassess the accuracy of a digital twin using real-world data. We formulate this\nas causal inference problem, which leads to a precise definition of what it\nmeans for a twin to be \"correct\" appropriate for many applications.\nUnfortunately, fundamental results from causal inference mean observational\ndata cannot be used to certify that a twin is correct in this sense unless\npotentially tenuous assumptions are made, such as that the data are\nunconfounded. To avoid these assumptions, we propose instead to find situations\nin which the twin is not correct, and present a general-purpose statistical\nprocedure for doing so. Our approach yields reliable and actionable information\nabout the twin under only the assumption of an i.i.d. dataset of observational\ntrajectories, and remains sound even if the data are confounded. We apply our\nmethodology to a large-scale, real-world case study involving sepsis modelling\nwithin the Pulse Physiology Engine, which we assess using the MIMIC-III dataset\nof ICU patients."}, "http://arxiv.org/abs/2301.11472": {"title": "Fast Bayesian Inference for Spatial Mean-Parameterized Conway--Maxwell--Poisson Models", "link": "http://arxiv.org/abs/2301.11472", "description": "Count data with complex features arise in many disciplines, including\necology, agriculture, criminology, medicine, and public health. Zero inflation,\nspatial dependence, and non-equidispersion are common features in count data.\nThere are two classes of models that allow for these features -- the\nmode-parameterized Conway--Maxwell--Poisson (COMP) distribution and the\ngeneralized Poisson model. However both require the use of either constraints\non the parameter space or a parameterization that leads to challenges in\ninterpretability. We propose a spatial mean-parameterized COMP model that\nretains the flexibility of these models while resolving the above issues. We\nuse a Bayesian spatial filtering approach in order to efficiently handle\nhigh-dimensional spatial data and we use reversible-jump MCMC to automatically\nchoose the basis vectors for spatial filtering. The COMP distribution poses two\nadditional computational challenges -- an intractable normalizing function in\nthe likelihood and no closed-form expression for the mean. We propose a fast\ncomputational approach that addresses these challenges by, respectively,\nintroducing an efficient auxiliary variable algorithm and pre-computing key\napproximations for fast likelihood evaluation. We illustrate the application of\nour methodology to simulated and real datasets, including Texas HPV-cancer data\nand US vaccine refusal data."}, "http://arxiv.org/abs/2305.08529": {"title": "Kernel-based Joint Independence Tests for Multivariate Stationary and Non-stationary Time Series", "link": "http://arxiv.org/abs/2305.08529", "description": "Multivariate time series data that capture the temporal evolution of\ninterconnected systems are ubiquitous in diverse areas. Understanding the\ncomplex relationships and potential dependencies among co-observed variables is\ncrucial for the accurate statistical modelling and analysis of such systems.\nHere, we introduce kernel-based statistical tests of joint independence in\nmultivariate time series by extending the $d$-variable Hilbert-Schmidt\nindependence criterion (dHSIC) to encompass both stationary and non-stationary\nprocesses, thus allowing broader real-world applications. By leveraging\nresampling techniques tailored for both single- and multiple-realisation time\nseries, we show how the method robustly uncovers significant higher-order\ndependencies in synthetic examples, including frequency mixing data and logic\ngates, as well as real-world climate, neuroscience, and socioeconomic data. Our\nmethod adds to the mathematical toolbox for the analysis of multivariate time\nseries and can aid in uncovering high-order interactions in data."}, "http://arxiv.org/abs/2306.07769": {"title": "Amortized Simulation-Based Frequentist Inference for Tractable and Intractable Likelihoods", "link": "http://arxiv.org/abs/2306.07769", "description": "High-fidelity simulators that connect theoretical models with observations\nare indispensable tools in many sciences. When coupled with machine learning, a\nsimulator makes it possible to infer the parameters of a theoretical model\ndirectly from real and simulated observations without explicit use of the\nlikelihood function. This is of particular interest when the latter is\nintractable. In this work, we introduce a simple extension of the recently\nproposed likelihood-free frequentist inference (LF2I) approach that has some\ncomputational advantages. Like LF2I, this extension yields provably valid\nconfidence sets in parameter inference problems in which a high-fidelity\nsimulator is available. The utility of our algorithm is illustrated by applying\nit to three pedagogically interesting examples: the first is from cosmology,\nthe second from high-energy physics and astronomy, both with tractable\nlikelihoods, while the third, with an intractable likelihood, is from\nepidemiology."}, "http://arxiv.org/abs/2307.05732": {"title": "Semiparametric Shape-restricted Estimators for Nonparametric Regression", "link": "http://arxiv.org/abs/2307.05732", "description": "Estimating the conditional mean function that relates predictive covariates\nto a response variable of interest is a fundamental task in economics and\nstatistics. In this manuscript, we propose some general nonparametric\nregression approaches that are widely applicable based on a simple yet\nsignificant decomposition of nonparametric functions into a semiparametric\nmodel with shape-restricted components. For instance, we observe that every\nLipschitz function can be expressed as a sum of a monotone function and a\nlinear function. We implement well-established shape-restricted estimation\nprocedures, such as isotonic regression, to handle the ``nonparametric\"\ncomponents of the true regression function and combine them with a simple\nsample-splitting procedure to estimate the parametric components. The resulting\nestimators inherit several favorable properties from the shape-restricted\nregression estimators. Notably, it is practically tuning parameter free,\nconverges at the minimax optimal rate, and exhibits an adaptive rate when the\ntrue regression function is ``simple\". We also confirm these theoretical\nproperties and compare the practice performance with existing methods via a\nseries of numerical studies."}, "http://arxiv.org/abs/2311.01470": {"title": "Preliminary Estimators of Population Mean using Ranked Set Sampling in the Presence of Measurement Error and Non-Response Error", "link": "http://arxiv.org/abs/2311.01470", "description": "In order to estimate the population mean in the presence of both non-response\nand measurement errors that are uncorrelated, the paper presents some novel\nestimators employing ranked set sampling by utilizing auxiliary information.Up\nto the first order of approximation, the equations for the bias and mean\nsquared error of the suggested estimators are produced, and it is found that\nthe proposed estimators outperform the other existing estimators analysed in\nthis study. Investigations using simulation studies and numerical examples show\nhow well the suggested estimators perform in the presence of measurement and\nnon-response errors. The relative efficiency of the suggested estimators\ncompared to the existing estimators has been expressed as a percentage, and the\nimpact of measurement errors has been expressed as a percentage computation of\nmeasurement errors."}, "http://arxiv.org/abs/2311.01484": {"title": "Comparison of methods for analyzing environmental mixtures effects on survival outcomes and application to a population-based cohort study", "link": "http://arxiv.org/abs/2311.01484", "description": "The estimation of the effect of environmental exposures and overall mixtures\non a survival time outcome is common in environmental epidemiological studies.\nWhile advanced statistical methods are increasingly being used for mixture\nanalyses, their applicability and performance for survival outcomes has yet to\nbe explored. We identified readily available methods for analyzing an\nenvironmental mixture's effect on a survival outcome and assessed their\nperformance via simulations replicating various real-life scenarios. Using\nprespecified criteria, we selected Bayesian Additive Regression Trees (BART),\nCox Elastic Net, Cox Proportional Hazards (PH) with and without penalized\nsplines, Gaussian Process Regression (GPR) and Multivariate Adaptive Regression\nSplines (MARS) to compare the bias and efficiency produced when estimating\nindividual exposure, overall mixture, and interaction effects on a survival\noutcome. We illustrate the selected methods in a real-world data application.\nWe estimated the effects of arsenic, cadmium, molybdenum, selenium, tungsten,\nand zinc on incidence of cardiovascular disease in American Indians using data\nfrom the Strong Heart Study (SHS). In the simulation study, there was a\nconsistent bias-variance trade off. The more flexible models (BART, GPR and\nMARS) were found to be most advantageous in the presence of nonproportional\nhazards, where the Cox models often did not capture the true effects due to\ntheir higher bias and lower variance. In the SHS, estimates of the effect of\nselenium and the overall mixture indicated negative effects, but the magnitudes\nof the estimated effects varied across methods. In practice, we recommend\nevaluating if findings are consistent across methods."}, "http://arxiv.org/abs/2311.01485": {"title": "Subgroup identification using individual participant data from multiple trials on low back pain", "link": "http://arxiv.org/abs/2311.01485", "description": "Model-based recursive partitioning (MOB) and its extension, metaMOB, are\npotent tools for identifying subgroups with differential treatment effects. In\nthe metaMOB approach random effects are used to model heterogeneity of the\ntreatment effects when pooling data from various trials. In situations where\ninterventions offer only small overall benefits and require extensive, costly\ntrials with a large participant enrollment, leveraging individual-participant\ndata (IPD) from multiple trials can help identify individuals who are most\nlikely to benefit from the intervention. We explore the application of MOB and\nmetaMOB in the context of non specific low back pain treatment, using\nsynthesized data based on a subset of the individual participant data\nmeta-analysis by Patel et al. Our study underscores the need to explore\nheterogeneity in intercepts and treatment effects to identify subgroups with\ndifferential treatment effects in IPD meta-analyses."}, "http://arxiv.org/abs/2311.01538": {"title": "A reluctant additive model framework for interpretable nonlinear individualized treatment rules", "link": "http://arxiv.org/abs/2311.01538", "description": "Individualized treatment rules (ITRs) for treatment recommendation is an\nimportant topic for precision medicine as not all beneficial treatments work\nwell for all individuals. Interpretability is a desirable property of ITRs, as\nit helps practitioners make sense of treatment decisions, yet there is a need\nfor ITRs to be flexible to effectively model complex biomedical data for\ntreatment decision making. Many ITR approaches either focus on linear ITRs,\nwhich may perform poorly when true optimal ITRs are nonlinear, or black-box\nnonlinear ITRs, which may be hard to interpret and can be overly complex. This\ndilemma indicates a tension between interpretability and accuracy of treatment\ndecisions. Here we propose an additive model-based nonlinear ITR learning\nmethod that balances interpretability and flexibility of the ITR. Our approach\naims to strike this balance by allowing both linear and nonlinear terms of the\ncovariates in the final ITR. Our approach is parsimonious in that the nonlinear\nterm is included in the final ITR only when it substantially improves the ITR\nperformance. To prevent overfitting, we combine cross-fitting and a specialized\ninformation criterion for model selection. Through extensive simulations, we\nshow that our methods are data-adaptive to the degree of nonlinearity and can\nfavorably balance ITR interpretability and flexibility. We further demonstrate\nthe robust performance of our methods with an application to a cancer drug\nsensitive study."}, "http://arxiv.org/abs/2311.01596": {"title": "Local Bayesian Dirichlet mixing of imperfect models", "link": "http://arxiv.org/abs/2311.01596", "description": "To improve the predictability of complex computational models in the\nexperimentally-unknown domains, we propose a Bayesian statistical machine\nlearning framework utilizing the Dirichlet distribution that combines results\nof several imperfect models. This framework can be viewed as an extension of\nBayesian stacking. To illustrate the method, we study the ability of Bayesian\nmodel averaging and mixing techniques to mine nuclear masses. We show that the\nglobal and local mixtures of models reach excellent performance on both\nprediction accuracy and uncertainty quantification and are preferable to\nclassical Bayesian model averaging. Additionally, our statistical analysis\nindicates that improving model predictions through mixing rather than mixing of\ncorrected models leads to more robust extrapolations."}, "http://arxiv.org/abs/2311.01625": {"title": "Topological inference on brain networks across subtypes of post-stroke aphasia", "link": "http://arxiv.org/abs/2311.01625", "description": "Persistent homology (PH) characterizes the shape of brain networks through\nthe persistence features. Group comparison of persistence features from brain\nnetworks can be challenging as they are inherently heterogeneous. A recent\nscale-space representation of persistence diagram (PD) through heat diffusion\nreparameterizes using the finite number of Fourier coefficients with respect to\nthe Laplace-Beltrami (LB) eigenfunction expansion of the domain, which provides\na powerful vectorized algebraic representation for group comparisons of PDs. In\nthis study, we advance a transposition-based permutation test for comparing\nmultiple groups of PDs through the heat-diffusion estimates of the PDs. We\nevaluate the empirical performance of the spectral transposition test in\ncapturing within- and between-group similarity and dissimilarity with respect\nto statistical variation of topological noise and hole location. We also\nillustrate how the method extends naturally into a clustering scheme by\nsubtyping individuals with post-stroke aphasia through the PDs of their\nresting-state functional brain networks."}, "http://arxiv.org/abs/2311.01638": {"title": "Inference on summaries of a model-agnostic longitudinal variable importance trajectory", "link": "http://arxiv.org/abs/2311.01638", "description": "In prediction settings where data are collected over time, it is often of\ninterest to understand both the importance of variables for predicting the\nresponse at each time point and the importance summarized over the time series.\nBuilding on recent advances in estimation and inference for variable importance\nmeasures, we define summaries of variable importance trajectories. These\nmeasures can be estimated and the same approaches for inference can be applied\nregardless of the choice of the algorithm(s) used to estimate the prediction\nfunction. We propose a nonparametric efficient estimation and inference\nprocedure as well as a null hypothesis testing procedure that are valid even\nwhen complex machine learning tools are used for prediction. Through\nsimulations, we demonstrate that our proposed procedures have good operating\ncharacteristics, and we illustrate their use by investigating the longitudinal\nimportance of risk factors for suicide attempt."}, "http://arxiv.org/abs/2311.01681": {"title": "The R", "link": "http://arxiv.org/abs/2311.01681", "description": "We propose a prognostic stratum matching framework that addresses the\ndeficiencies of Randomized trial data subgroup analysis and transforms\nObservAtional Data to be used as if they were randomized, thus paving the road\nfor precision medicine. Our approach counters the effects of unobserved\nconfounding in observational data by correcting the estimated probabilities of\nthe outcome under a treatment through a novel two-step process. These\nprobabilities are then used to train Optimal Policy Trees (OPTs), which are\ndecision trees that optimally assign treatments to subgroups of patients based\non their characteristics. This facilitates the creation of clinically intuitive\ntreatment recommendations. We applied our framework to observational data of\npatients with gastrointestinal stromal tumors (GIST) and validated the OPTs in\nan external cohort using the sensitivity and specificity metrics. We show that\nthese recommendations outperformed those of experts in GIST. We further applied\nthe same framework to randomized clinical trial (RCT) data of patients with\nextremity sarcomas. Remarkably, despite the initial trial results suggesting\nthat all patients should receive treatment, our framework, after addressing\nimbalances in patient distribution due to the trial's small sample size,\nidentified through the OPTs a subset of patients with unique characteristics\nwho may not require treatment. Again, we successfully validated our\nrecommendations in an external cohort."}, "http://arxiv.org/abs/2311.01709": {"title": "Causal inference with Machine Learning-Based Covariate Representation", "link": "http://arxiv.org/abs/2311.01709", "description": "Utilizing covariate information has been a powerful approach to improve the\nefficiency and accuracy for causal inference, which support massive amount of\nrandomized experiments run on data-driven enterprises. However, state-of-art\napproaches can become practically unreliable when the dimension of covariate\nincreases to just 50, whereas experiments on large platforms can observe even\nhigher dimension of covariate. We propose a machine-learning-assisted covariate\nrepresentation approach that can effectively make use of historical experiment\nor observational data that are run on the same platform to understand which\nlower dimensions can effectively represent the higher-dimensional covariate. We\nthen propose design and estimation methods with the covariate representation.\nWe prove statistically reliability and performance guarantees for the proposed\nmethods. The empirical performance is demonstrated using numerical experiments."}, "http://arxiv.org/abs/2311.01762": {"title": "Solving Kernel Ridge Regression with Gradient Descent for a Non-Constant Kernel", "link": "http://arxiv.org/abs/2311.01762", "description": "Kernel ridge regression, KRR, is a generalization of linear ridge regression\nthat is non-linear in the data, but linear in the parameters. The solution can\nbe obtained either as a closed-form solution, which includes a matrix\ninversion, or iteratively through gradient descent. Using the iterative\napproach opens up for changing the kernel during training, something that is\ninvestigated in this paper. We theoretically address the effects this has on\nmodel complexity and generalization. Based on our findings, we propose an\nupdate scheme for the bandwidth of translational-invariant kernels, where we\nlet the bandwidth decrease to zero during training, thus circumventing the need\nfor hyper-parameter selection. We demonstrate on real and synthetic data how\ndecreasing the bandwidth during training outperforms using a constant\nbandwidth, selected by cross-validation and marginal likelihood maximization.\nWe also show theoretically and empirically that using a decreasing bandwidth,\nwe are able to achieve both zero training error in combination with good\ngeneralization, and a double descent behavior, phenomena that do not occur for\nKRR with constant bandwidth but are known to appear for neural networks."}, "http://arxiv.org/abs/2311.01833": {"title": "Similarity network aggregation for the analysis of glacier ecosystems", "link": "http://arxiv.org/abs/2311.01833", "description": "The synthesis of information deriving from complex networks is a topic\nreceiving increasing relevance in ecology and environmental sciences. In\nparticular, the aggregation of multilayer networks, i.e. network structures\nformed by multiple interacting networks (the layers), constitutes a\nfast-growing field. In several environmental applications, the layers of a\nmultilayer network are modelled as a collection of similarity matrices\ndescribing how similar pairs of biological entities are, based on different\ntypes of features (e.g. biological traits). The present paper first discusses\ntwo main techniques for combining the multi-layered information into a single\nnetwork (the so-called monoplex), i.e. Similarity Network Fusion (SNF) and\nSimilarity Matrix Average (SMA). Then, the effectiveness of the two methods is\ntested on a real-world dataset of the relative abundance of microbial species\nin the ecosystems of nine glaciers (four glaciers in the Alps and five in the\nAndes). A preliminary clustering analysis on the monoplexes obtained with\ndifferent methods shows the emergence of a tightly connected community formed\nby species that are typical of cryoconite holes worldwide. Moreover, the\nweights assigned to different layers by the SMA algorithm suggest that two\nlarge South American glaciers (Exploradores and Perito Moreno) are structurally\ndifferent from the smaller glaciers in both Europe and South America. Overall,\nthese results highlight the importance of integration methods in the discovery\nof the underlying organizational structure of biological entities in multilayer\necological networks."}, "http://arxiv.org/abs/2311.01872": {"title": "The use of restricted mean survival time to estimate treatment effect under model misspecification, a simulation study", "link": "http://arxiv.org/abs/2311.01872", "description": "The use of the non-parametric Restricted Mean Survival Time endpoint (RMST)\nhas grown in popularity as trialists look to analyse time-to-event outcomes\nwithout the restrictions of the proportional hazards assumption. In this paper,\nwe evaluate the power and type I error rate of the parametric and\nnon-parametric RMST estimators when treatment effect is explained by multiple\ncovariates, including an interaction term. Utilising the RMST estimator in this\nway allows the combined treatment effect to be summarised as a one-dimensional\nestimator, which is evaluated using a one-sided hypothesis Z-test. The\nestimators are either fully specified or misspecified, both in terms of\nunaccounted covariates or misspecified knot points (where trials exhibit\ncrossing survival curves). A placebo-controlled trial of Gamma interferon is\nused as a motivating example to simulate associated survival times. When\ncorrectly specified, the parametric RMST estimator has the greatest power,\nregardless of the time of analysis. The misspecified RMST estimator generally\nperforms similarly when covariates mirror those of the fitted case study\ndataset. However, as the magnitude of the unaccounted covariate increases, the\nassociated power of the estimator decreases. In all cases, the non-parametric\nRMST estimator has the lowest power, and power remains very reliant on the time\nof analysis (with a later analysis time correlated with greater power)."}, "http://arxiv.org/abs/2311.01902": {"title": "High Precision Causal Model Evaluation with Conditional Randomization", "link": "http://arxiv.org/abs/2311.01902", "description": "The gold standard for causal model evaluation involves comparing model\npredictions with true effects estimated from randomized controlled trials\n(RCT). However, RCTs are not always feasible or ethical to perform. In\ncontrast, conditionally randomized experiments based on inverse probability\nweighting (IPW) offer a more realistic approach but may suffer from high\nestimation variance. To tackle this challenge and enhance causal model\nevaluation in real-world conditional randomization settings, we introduce a\nnovel low-variance estimator for causal error, dubbed as the pairs estimator.\nBy applying the same IPW estimator to both the model and true experimental\neffects, our estimator effectively cancels out the variance due to IPW and\nachieves a smaller asymptotic variance. Empirical studies demonstrate the\nimproved of our estimator, highlighting its potential on achieving near-RCT\nperformance. Our method offers a simple yet powerful solution to evaluate\ncausal inference models in conditional randomization settings without\ncomplicated modification of the IPW estimator itself, paving the way for more\nrobust and reliable model assessments."}, "http://arxiv.org/abs/2311.01913": {"title": "Extended Relative Power Contribution that Allows to Evaluate the Effect of Correlated Noise", "link": "http://arxiv.org/abs/2311.01913", "description": "We proposed an extension of Akaike's relative power contribution that could\nbe applied to data with correlations between noises. This method decomposes the\npower spectrum into a contribution of the terms caused by correlation between\ntwo noises, in addition to the contributions of the independent noises.\nNumerical examples confirm that some of the correlated noise has the effect of\nreducing the power spectrum."}, "http://arxiv.org/abs/2311.02019": {"title": "Reproducible Parameter Inference Using Bagged Posteriors", "link": "http://arxiv.org/abs/2311.02019", "description": "Under model misspecification, it is known that Bayesian posteriors often do\nnot properly quantify uncertainty about true or pseudo-true parameters. Even\nmore fundamentally, misspecification leads to a lack of reproducibility in the\nsense that the same model will yield contradictory posteriors on independent\ndata sets from the true distribution. To define a criterion for reproducible\nuncertainty quantification under misspecification, we consider the probability\nthat two confidence sets constructed from independent data sets have nonempty\noverlap, and we establish a lower bound on this overlap probability that holds\nfor any valid confidence sets. We prove that credible sets from the standard\nposterior can strongly violate this bound, particularly in high-dimensional\nsettings (i.e., with dimension increasing with sample size), indicating that it\nis not internally coherent under misspecification. To improve reproducibility\nin an easy-to-use and widely applicable way, we propose to apply bagging to the\nBayesian posterior (\"BayesBag\"'); that is, to use the average of posterior\ndistributions conditioned on bootstrapped datasets. We motivate BayesBag from\nfirst principles based on Jeffrey conditionalization and show that the bagged\nposterior typically satisfies the overlap lower bound. Further, we prove a\nBernstein--Von Mises theorem for the bagged posterior, establishing its\nasymptotic normal distribution. We demonstrate the benefits of BayesBag via\nsimulation experiments and an application to crime rate prediction."}, "http://arxiv.org/abs/2311.02043": {"title": "Bayesian Quantile Regression with Subset Selection: A Posterior Summarization Perspective", "link": "http://arxiv.org/abs/2311.02043", "description": "Quantile regression is a powerful tool for inferring how covariates affect\nspecific percentiles of the response distribution. Existing methods either\nestimate conditional quantiles separately for each quantile of interest or\nestimate the entire conditional distribution using semi- or non-parametric\nmodels. The former often produce inadequate models for real data and do not\nshare information across quantiles, while the latter are characterized by\ncomplex and constrained models that can be difficult to interpret and\ncomputationally inefficient. Further, neither approach is well-suited for\nquantile-specific subset selection. Instead, we pose the fundamental problems\nof linear quantile estimation, uncertainty quantification, and subset selection\nfrom a Bayesian decision analysis perspective. For any Bayesian regression\nmodel, we derive optimal and interpretable linear estimates and uncertainty\nquantification for each model-based conditional quantile. Our approach\nintroduces a quantile-focused squared error loss, which enables efficient,\nclosed-form computing and maintains a close relationship with Wasserstein-based\ndensity estimation. In an extensive simulation study, our methods demonstrate\nsubstantial gains in quantile estimation accuracy, variable selection, and\ninference over frequentist and Bayesian competitors. We apply these tools to\nidentify the quantile-specific impacts of social and environmental stressors on\neducational outcomes for a large cohort of children in North Carolina."}, "http://arxiv.org/abs/2010.08627": {"title": "Minimax Quasi-Bayesian estimation in sparse canonical correlation analysis via a Rayleigh quotient function", "link": "http://arxiv.org/abs/2010.08627", "description": "Canonical correlation analysis (CCA) is a popular statistical technique for\nexploring relationships between datasets. In recent years, the estimation of\nsparse canonical vectors has emerged as an important but challenging variant of\nthe CCA problem, with widespread applications. Unfortunately, existing\nrate-optimal estimators for sparse canonical vectors have high computational\ncost. We propose a quasi-Bayesian estimation procedure that not only achieves\nthe minimax estimation rate, but also is easy to compute by Markov Chain Monte\nCarlo (MCMC). The method builds on Tan et al. (2018) and uses a re-scaled\nRayleigh quotient function as the quasi-log-likelihood. However, unlike Tan et\nal. (2018), we adopt a Bayesian framework that combines this\nquasi-log-likelihood with a spike-and-slab prior to regularize the inference\nand promote sparsity. We investigate the empirical behavior of the proposed\nmethod on both continuous and truncated data, and we demonstrate that it\noutperforms several state-of-the-art methods. As an application, we use the\nproposed methodology to maximally correlate clinical variables and proteomic\ndata for better understanding the Covid-19 disease."}, "http://arxiv.org/abs/2104.08300": {"title": "Semiparametric Sensitivity Analysis: Unmeasured Confounding In Observational Studies", "link": "http://arxiv.org/abs/2104.08300", "description": "Establishing cause-effect relationships from observational data often relies\non untestable assumptions. It is crucial to know whether, and to what extent,\nthe conclusions drawn from non-experimental studies are robust to potential\nunmeasured confounding. In this paper, we focus on the average causal effect\n(ACE) as our target of inference. We generalize the sensitivity analysis\napproach developed by Robins et al. (2000), Franks et al. (2020) and Zhou and\nYao (2023. We use semiparametric theory to derive the non-parametric efficient\ninfluence function of the ACE, for fixed sensitivity parameters. We use this\ninfluence function to construct a one-step bias-corrected estimator of the ACE.\nOur estimator depends on semiparametric models for the distribution of the\nobserved data; importantly, these models do not impose any restrictions on the\nvalues of sensitivity analysis parameters. We establish sufficient conditions\nensuring that our estimator has root-n asymptotics. We use our methodology to\nevaluate the causal effect of smoking during pregnancy on birth weight. We also\nevaluate the performance of estimation procedure in a simulation study."}, "http://arxiv.org/abs/2206.04157": {"title": "Inference for Matched Tuples and Fully Blocked Factorial Designs", "link": "http://arxiv.org/abs/2206.04157", "description": "This paper studies inference in randomized controlled trials with multiple\ntreatments, where treatment status is determined according to a \"matched\ntuples\" design. Here, by a matched tuples design, we mean an experimental\ndesign where units are sampled i.i.d. from the population of interest, grouped\ninto \"homogeneous\" blocks with cardinality equal to the number of treatments,\nand finally, within each block, each treatment is assigned exactly once\nuniformly at random. We first study estimation and inference for matched tuples\ndesigns in the general setting where the parameter of interest is a vector of\nlinear contrasts over the collection of average potential outcomes for each\ntreatment. Parameters of this form include standard average treatment effects\nused to compare one treatment relative to another, but also include parameters\nwhich may be of interest in the analysis of factorial designs. We first\nestablish conditions under which a sample analogue estimator is asymptotically\nnormal and construct a consistent estimator of its corresponding asymptotic\nvariance. Combining these results establishes the asymptotic exactness of tests\nbased on these estimators. In contrast, we show that, for two common testing\nprocedures based on t-tests constructed from linear regressions, one test is\ngenerally conservative while the other generally invalid. We go on to apply our\nresults to study the asymptotic properties of what we call \"fully-blocked\" 2^K\nfactorial designs, which are simply matched tuples designs applied to a full\nfactorial experiment. Leveraging our previous results, we establish that our\nestimator achieves a lower asymptotic variance under the fully-blocked design\nthan that under any stratified factorial design which stratifies the\nexperimental sample into a finite number of \"large\" strata. A simulation study\nand empirical application illustrate the practical relevance of our results."}, "http://arxiv.org/abs/2207.00100": {"title": "A Bayesian 'sandwich' for variance estimation", "link": "http://arxiv.org/abs/2207.00100", "description": "Large-sample Bayesian analogs exist for many frequentist methods, but are\nless well-known for the widely-used 'sandwich' or 'robust' variance estimates.\nWe review existing approaches to Bayesian analogs of sandwich variance\nestimates and propose a new analog, as the Bayes rule under a form of balanced\nloss function, that combines elements of standard parametric inference with\nfidelity of the data to the model. Our development is general, for essentially\nany regression setting with independent outcomes. Being the large-sample\nequivalent of its frequentist counterpart, we show by simulation that Bayesian\nrobust standard error estimates can faithfully quantify the variability of\nparameter estimates even under model misspecification -- thus retaining the\nmajor attraction of the original frequentist version. We demonstrate our\nBayesian analog of standard error estimates when studying the association\nbetween age and systolic blood pressure in NHANES."}, "http://arxiv.org/abs/2210.17514": {"title": "Cost-aware Generalized $\\alpha$-investing for Multiple Hypothesis Testing", "link": "http://arxiv.org/abs/2210.17514", "description": "We consider the problem of sequential multiple hypothesis testing with\nnontrivial data collection costs. This problem appears, for example, when\nconducting biological experiments to identify differentially expressed genes of\na disease process. This work builds on the generalized $\\alpha$-investing\nframework which enables control of the false discovery rate in a sequential\ntesting setting. We make a theoretical analysis of the long term asymptotic\nbehavior of $\\alpha$-wealth which motivates a consideration of sample size in\nthe $\\alpha$-investing decision rule. Posing the testing process as a game with\nnature, we construct a decision rule that optimizes the expected\n$\\alpha$-wealth reward (ERO) and provides an optimal sample size for each test.\nEmpirical results show that a cost-aware ERO decision rule correctly rejects\nmore false null hypotheses than other methods for $n=1$ where $n$ is the sample\nsize. When the sample size is not fixed cost-aware ERO uses a prior on the null\nhypothesis to adaptively allocate of the sample budget to each test. We extend\ncost-aware ERO investing to finite-horizon testing which enables the decision\nrule to allocate samples in a non-myopic manner. Finally, empirical tests on\nreal data sets from biological experiments show that cost-aware ERO balances\nthe allocation of samples to an individual test against the allocation of\nsamples across multiple tests."}, "http://arxiv.org/abs/2301.01480": {"title": "A new over-dispersed count model", "link": "http://arxiv.org/abs/2301.01480", "description": "A new two-parameter discrete distribution, namely the PoiG distribution is\nderived by the convolution of a Poisson variate and an independently\ndistributed geometric random variable. This distribution generalizes both the\nPoisson and geometric distributions and can be used for modelling\nover-dispersed as well as equi-dispersed count data. A number of important\nstatistical properties of the proposed count model, such as the probability\ngenerating function, the moment generating function, the moments, the survival\nfunction and the hazard rate function. Monotonic properties are studied, such\nas the log concavity and the stochastic ordering are also investigated in\ndetail. Method of moment and the maximum likelihood estimators of the\nparameters of the proposed model are presented. It is envisaged that the\nproposed distribution may prove to be useful for the practitioners for\nmodelling over-dispersed count data compared to its closest competitors."}, "http://arxiv.org/abs/2305.10050": {"title": "The Impact of Missing Data on Causal Discovery: A Multicentric Clinical Study", "link": "http://arxiv.org/abs/2305.10050", "description": "Causal inference for testing clinical hypotheses from observational data\npresents many difficulties because the underlying data-generating model and the\nassociated causal graph are not usually available. Furthermore, observational\ndata may contain missing values, which impact the recovery of the causal graph\nby causal discovery algorithms: a crucial issue often ignored in clinical\nstudies. In this work, we use data from a multi-centric study on endometrial\ncancer to analyze the impact of different missingness mechanisms on the\nrecovered causal graph. This is achieved by extending state-of-the-art causal\ndiscovery algorithms to exploit expert knowledge without sacrificing\ntheoretical soundness. We validate the recovered graph with expert physicians,\nshowing that our approach finds clinically-relevant solutions. Finally, we\ndiscuss the goodness of fit of our graph and its consistency from a clinical\ndecision-making perspective using graphical separation to validate causal\npathways."}, "http://arxiv.org/abs/2309.03952": {"title": "The Causal Roadmap and simulation studies to inform the Statistical Analysis Plan for real-data applications", "link": "http://arxiv.org/abs/2309.03952", "description": "The Causal Roadmap outlines a systematic approach to our research endeavors:\ndefine quantity of interest, evaluate needed assumptions, conduct statistical\nestimation, and carefully interpret of results. At the estimation step, it is\nessential that the estimation algorithm be chosen thoughtfully for its\ntheoretical properties and expected performance. Simulations can help\nresearchers gain a better understanding of an estimator's statistical\nperformance under conditions unique to the real-data application. This in turn\ncan inform the rigorous pre-specification of a Statistical Analysis Plan (SAP),\nnot only stating the estimand (e.g., G-computation formula), the estimator\n(e.g., targeted minimum loss-based estimation [TMLE]), and adjustment\nvariables, but also the implementation of the estimator -- including nuisance\nparameter estimation and approach for variance estimation. Doing so helps\nensure valid inference (e.g., 95% confidence intervals with appropriate\ncoverage). Failing to pre-specify estimation can lead to data dredging and\ninflated Type-I error rates."}, "http://arxiv.org/abs/2311.02273": {"title": "A Sequential Learning Procedure with Applications to Online Sales Examination", "link": "http://arxiv.org/abs/2311.02273", "description": "In this paper, we consider the problem of estimating parameters in a linear\nregression model. We propose a sequential learning procedure to determine the\nsample size for achieving a given small estimation risk, under the widely used\nGauss-Markov setup with independent normal errors. The procedure is proven to\nenjoy the second-order efficiency and risk-efficiency properties, which are\nvalidated through Monte Carlo simulation studies. Using e-commerce data, we\nimplement the procedure to examine the influential factors of online sales."}, "http://arxiv.org/abs/2311.02299": {"title": "The Fragility of Sparsity", "link": "http://arxiv.org/abs/2311.02299", "description": "We show, using three empirical applications, that linear regression estimates\nwhich rely on the assumption of sparsity are fragile in two ways. First, we\ndocument that different choices of the regressor matrix that don't impact\nordinary least squares (OLS) estimates, such as the choice of baseline category\nwith categorical controls, can move sparsity-based estimates two standard\nerrors or more. Second, we develop two tests of the sparsity assumption based\non comparing sparsity-based estimators with OLS. The tests tend to reject the\nsparsity assumption in all three applications. Unless the number of regressors\nis comparable to or exceeds the sample size, OLS yields more robust results at\nlittle efficiency cost."}, "http://arxiv.org/abs/2311.02306": {"title": "Heteroskedastic Tensor Clustering", "link": "http://arxiv.org/abs/2311.02306", "description": "Tensor clustering, which seeks to extract underlying cluster structures from\nnoisy tensor observations, has gained increasing attention. One extensively\nstudied model for tensor clustering is the tensor block model, which postulates\nthe existence of clustering structures along each mode and has found broad\napplications in areas like multi-tissue gene expression analysis and multilayer\nnetwork analysis. However, currently available computationally feasible methods\nfor tensor clustering either are limited to handling i.i.d. sub-Gaussian noise\nor suffer from suboptimal statistical performance, which restrains their\nutility in applications that have to deal with heteroskedastic data and/or low\nsignal-to-noise-ratio (SNR).\n\nTo overcome these challenges, we propose a two-stage method, named\n$\\mathsf{High\\text{-}order~HeteroClustering}$ ($\\mathsf{HHC}$), which starts by\nperforming tensor subspace estimation via a novel spectral algorithm called\n$\\mathsf{Thresholded~Deflated\\text{-}HeteroPCA}$, followed by approximate\n$k$-means to obtain cluster nodes. Encouragingly, our algorithm provably\nachieves exact clustering as long as the SNR exceeds the computational limit\n(ignoring logarithmic factors); here, the SNR refers to the ratio of the\npairwise disparity between nodes to the noise level, and the computational\nlimit indicates the lowest SNR that enables exact clustering with polynomial\nruntime. Comprehensive simulation and real-data experiments suggest that our\nalgorithm outperforms existing algorithms across various settings, delivering\nmore reliable clustering performance."}, "http://arxiv.org/abs/2311.02308": {"title": "Kernel-based sensitivity indices for any model behavior and screening", "link": "http://arxiv.org/abs/2311.02308", "description": "Complex models are often used to understand interactions and drivers of\nhuman-induced and/or natural phenomena. It is worth identifying the input\nvariables that drive the model output(s) in a given domain and/or govern\nspecific model behaviors such as contextual indicators based on\nsocio-environmental models. Using the theory of multivariate weighted\ndistributions to characterize specific model behaviors, we propose new measures\nof association between inputs and such behaviors. Our measures rely on\nsensitivity functionals (SFs) and kernel methods, including variance-based\nsensitivity analysis. The proposed $\\ell_1$-based kernel indices account for\ninteractions among inputs, higher-order moments of SFs, and their upper bounds\nare somehow equivalent to the Morris-type screening measures, including\ndependent elementary effects. Empirical kernel-based indices are derived,\nincluding their statistical properties for the computational issues, and\nnumerical results are provided."}, "http://arxiv.org/abs/2311.02312": {"title": "Efficient Change Point Detection and Estimation in High-Dimensional Correlation Matrices", "link": "http://arxiv.org/abs/2311.02312", "description": "This paper considers the problems of detecting a change point and estimating\nthe location in the correlation matrices of a sequence of high-dimensional\nvectors, where the dimension is large enough to be comparable to the sample\nsize or even much larger. A new break test is proposed based on signflip\nparallel analysis to detect the existence of change points. Furthermore, a\ntwo-step approach combining a signflip permutation dimension reduction step and\na CUSUM statistic is proposed to estimate the change point's location and\nrecover the support of changes. The consistency of the estimator is\nconstructed. Simulation examples and real data applications illustrate the\nsuperior empirical performance of the proposed methods. Especially, the\nproposed methods outperform existing ones for non-Gaussian data and the change\npoint in the extreme tail of a sequence and become more accurate as the\ndimension p increases. Supplementary materials for this article are available\nonline."}, "http://arxiv.org/abs/2311.02450": {"title": "Factor-guided estimation of large covariance matrix function with conditional functional sparsity", "link": "http://arxiv.org/abs/2311.02450", "description": "This paper addresses the fundamental task of estimating covariance matrix\nfunctions for high-dimensional functional data/functional time series. We\nconsider two functional factor structures encompassing either functional\nfactors with scalar loadings or scalar factors with functional loadings, and\npostulate functional sparsity on the covariance of idiosyncratic errors after\ntaking out the common unobserved factors. To facilitate estimation, we rely on\nthe spiked matrix model and its functional generalization, and derive some\nnovel asymptotic identifiability results, based on which we develop DIGIT and\nFPOET estimators under two functional factor models, respectively. Both\nestimators involve performing associated eigenanalysis to estimate the\ncovariance of common components, followed by adaptive functional thresholding\napplied to the residual covariance. We also develop functional information\ncriteria for the purpose of model selection. The convergence rates of estimated\nfactors, loadings, and conditional sparse covariance matrix functions under\nvarious functional matrix norms, are respectively established for DIGIT and\nFPOET estimators. Numerical studies including extensive simulations and two\nreal data applications on mortality rates and functional portfolio allocation\nare conducted to examine the finite-sample performance of the proposed\nmethodology."}, "http://arxiv.org/abs/2311.02467": {"title": "Individualized Policy Evaluation and Learning under Clustered Network Interference", "link": "http://arxiv.org/abs/2311.02467", "description": "While there now exists a large literature on policy evaluation and learning,\nmuch of prior work assumes that the treatment assignment of one unit does not\naffect the outcome of another unit. Unfortunately, ignoring interference may\nlead to biased policy evaluation and yield ineffective learned policies. For\nexample, treating influential individuals who have many friends can generate\npositive spillover effects, thereby improving the overall performance of an\nindividualized treatment rule (ITR). We consider the problem of evaluating and\nlearning an optimal ITR under clustered network (or partial) interference where\nclusters of units are sampled from a population and units may influence one\nanother within each cluster. Under this model, we propose an estimator that can\nbe used to evaluate the empirical performance of an ITR. We show that this\nestimator is substantially more efficient than the standard inverse probability\nweighting estimator, which does not impose any assumption about spillover\neffects. We derive the finite-sample regret bound for a learned ITR, showing\nthat the use of our efficient evaluation estimator leads to the improved\nperformance of learned policies. Finally, we conduct simulation and empirical\nstudies to illustrate the advantages of the proposed methodology."}, "http://arxiv.org/abs/2311.02532": {"title": "Optimal Treatment Allocation for Efficient Policy Evaluation in Sequential Decision Making", "link": "http://arxiv.org/abs/2311.02532", "description": "A/B testing is critical for modern technological companies to evaluate the\neffectiveness of newly developed products against standard baselines. This\npaper studies optimal designs that aim to maximize the amount of information\nobtained from online experiments to estimate treatment effects accurately. We\npropose three optimal allocation strategies in a dynamic setting where\ntreatments are sequentially assigned over time. These strategies are designed\nto minimize the variance of the treatment effect estimator when data follow a\nnon-Markov decision process or a (time-varying) Markov decision process. We\nfurther develop estimation procedures based on existing off-policy evaluation\n(OPE) methods and conduct extensive experiments in various environments to\ndemonstrate the effectiveness of the proposed methodologies. In theory, we\nprove the optimality of the proposed treatment allocation design and establish\nupper bounds for the mean squared errors of the resulting treatment effect\nestimators."}, "http://arxiv.org/abs/2311.02543": {"title": "Pairwise likelihood estimation and limited information goodness-of-fit test statistics for binary factor analysis models under complex survey sampling", "link": "http://arxiv.org/abs/2311.02543", "description": "This paper discusses estimation and limited information goodness-of-fit test\nstatistics in factor models for binary data using pairwise likelihood\nestimation and sampling weights. The paper extends the applicability of\npairwise likelihood estimation for factor models with binary data to\naccommodate complex sampling designs. Additionally, it introduces two key\nlimited information test statistics: the Pearson chi-squared test and the Wald\ntest. To enhance computational efficiency, the paper introduces modifications\nto both test statistics. The performance of the estimation and the proposed\ntest statistics under simple random sampling and unequal probability sampling\nis evaluated using simulated data."}, "http://arxiv.org/abs/2311.02574": {"title": "Semi-supervised Estimation of Event Rate with Doubly-censored Survival Data", "link": "http://arxiv.org/abs/2311.02574", "description": "Electronic Health Record (EHR) has emerged as a valuable source of data for\ntranslational research. To leverage EHR data for risk prediction and\nsubsequently clinical decision support, clinical endpoints are often time to\nonset of a clinical condition of interest. Precise information on clinical\nevent times is often not directly available and requires labor-intensive manual\nchart review to ascertain. In addition, events may occur outside of the\nhospital system, resulting in both left and right censoring often termed double\ncensoring. On the other hand, proxies such as time to the first diagnostic code\nare readily available yet with varying degrees of accuracy. Using error-prone\nevent times derived from these proxies can lead to biased risk estimates while\nonly relying on manually annotated event times, which are typically only\navailable for a small subset of patients, can lead to high variability. This\nsignifies the need for semi-supervised estimation methods that can efficiently\ncombine information from both the small subset of labeled observations and a\nlarge size of surrogate proxies. While semi-supervised estimation methods have\nbeen recently developed for binary and right-censored data, no methods\ncurrently exist in the presence of double censoring. This paper fills the gap\nby developing a robust and efficient Semi-supervised Estimation of Event rate\nwith Doubly-censored Survival data (SEEDS) by leveraging a small set of gold\nstandard labels and a large set of surrogate features. Under regularity\nconditions, we demonstrate that the proposed SEEDS estimator is consistent and\nasymptotically normal. Simulation results illustrate that SEEDS performs well\nin finite samples and can be substantially more efficient compared to the\nsupervised counterpart. We apply the SEEDS to estimate the age-specific\nsurvival rate of type 2 diabetes using EHR data from Mass General Brigham."}, "http://arxiv.org/abs/2311.02610": {"title": "An adaptive standardisation model for Day-Ahead electricity price forecasting", "link": "http://arxiv.org/abs/2311.02610", "description": "The study of Day-Ahead prices in the electricity market is one of the most\npopular problems in time series forecasting. Previous research has focused on\nemploying increasingly complex learning algorithms to capture the sophisticated\ndynamics of the market. However, there is a threshold where increased\ncomplexity fails to yield substantial improvements. In this work, we propose an\nalternative approach by introducing an adaptive standardisation to mitigate the\neffects of dataset shifts that commonly occur in the market. By doing so,\nlearning algorithms can prioritize uncovering the true relationship between the\ntarget variable and the explanatory variables. We investigate four distinct\nmarkets, including two novel datasets, previously unexplored in the literature.\nThese datasets provide a more realistic representation of the current market\ncontext, that conventional datasets do not show. The results demonstrate a\nsignificant improvement across all four markets, using learning algorithms that\nare less complex yet widely accepted in the literature. This significant\nadvancement unveils opens up new lines of research in this field, highlighting\nthe potential of adaptive transformations in enhancing the performance of\nforecasting models."}, "http://arxiv.org/abs/2311.02634": {"title": "Pointwise Data Depth for Univariate and Multivariate Functional Outlier Detection", "link": "http://arxiv.org/abs/2311.02634", "description": "Data depth is an efficient tool for robustly summarizing the distribution of\nfunctional data and detecting potential magnitude and shape outliers. Commonly\nused functional data depth notions, such as the modified band depth and\nextremal depth, are estimated from pointwise depth for each observed functional\nobservation. However, these techniques require calculating one single depth\nvalue for each functional observation, which may not be sufficient to\ncharacterize the distribution of the functional data and detect potential\noutliers. This paper presents an innovative approach to make the best use of\npointwise depth. We propose using the pointwise depth distribution for\nmagnitude outlier visualization and the correlation between pairwise depth for\nshape outlier detection. Furthermore, a bootstrap-based testing procedure has\nbeen introduced for the correlation to test whether there is any shape outlier.\nThe proposed univariate methods are then extended to bivariate functional data.\nThe performance of the proposed methods is examined and compared to\nconventional outlier detection techniques by intensive simulation studies. In\naddition, the developed methods are applied to simulated solar energy datasets\nfrom a photovoltaic system. Results revealed that the proposed method offers\nsuperior detection performance over conventional techniques. These findings\nwill benefit engineers and practitioners in monitoring photovoltaic systems by\ndetecting unnoticed anomalies and outliers."}, "http://arxiv.org/abs/2311.02658": {"title": "Nonparametric Estimation and Comparison of Distance Distributions from Censored Data", "link": "http://arxiv.org/abs/2311.02658", "description": "Transportation distance information is a powerful resource, but location\nrecords are often censored due to privacy concerns or regulatory mandates. We\nconsider the problem of transportation event distance distribution\nreconstruction, which aims to handle this obstacle and has applications to\npublic health informatics, logistics, and more. We propose numerical methods to\napproximate, sample from, and compare distributions of distances between\ncensored location pairs. We validate empirically and demonstrate applicability\nto practical geospatial data analysis tasks. Our code is available on GitHub."}, "http://arxiv.org/abs/2311.02766": {"title": "Riemannian Laplace Approximation with the Fisher Metric", "link": "http://arxiv.org/abs/2311.02766", "description": "The Laplace's method approximates a target density with a Gaussian\ndistribution at its mode. It is computationally efficient and asymptotically\nexact for Bayesian inference due to the Bernstein-von Mises theorem, but for\ncomplex targets and finite-data posteriors it is often too crude an\napproximation. A recent generalization of the Laplace Approximation transforms\nthe Gaussian approximation according to a chosen Riemannian geometry providing\na richer approximation family, while still retaining computational efficiency.\nHowever, as shown here, its properties heavily depend on the chosen metric,\nindeed the metric adopted in previous work results in approximations that are\noverly narrow as well as being biased even at the limit of infinite data. We\ncorrect this shortcoming by developing the approximation family further,\nderiving two alternative variants that are exact at the limit of infinite data,\nextending the theoretical analysis of the method, and demonstrating practical\nimprovements in a range of experiments."}, "http://arxiv.org/abs/2311.02808": {"title": "Nonparametric Estimation of Conditional Copula using Smoothed Checkerboard Bernstein Sieves", "link": "http://arxiv.org/abs/2311.02808", "description": "Conditional copulas are useful tools for modeling the dependence between\nmultiple response variables that may vary with a given set of predictor\nvariables. Conditional dependence measures such as conditional Kendall's tau\nand Spearman's rho that can be expressed as functionals of the conditional\ncopula are often used to evaluate the strength of dependence conditioning on\nthe covariates. In general, semiparametric estimation methods of conditional\ncopulas rely on an assumed parametric copula family where the copula parameter\nis assumed to be a function of the covariates. The functional relationship can\nbe estimated nonparametrically using different techniques but it is required to\nchoose an appropriate copula model from various candidate families. In this\npaper, by employing the empirical checkerboard Bernstein copula (ECBC)\nestimator we propose a fully nonparametric approach for estimating conditional\ncopulas, which doesn't require any selection of parametric copula models.\nClosed-form estimates of the conditional dependence measures are derived\ndirectly from the proposed ECBC-based conditional copula estimator. We provide\nthe large-sample consistency of the proposed estimator as well as the estimates\nof conditional dependence measures. The finite-sample performance of the\nproposed estimator and comparison with semiparametric methods are investigated\nthrough simulation studies. An application to real case studies is also\nprovided."}, "http://arxiv.org/abs/2311.02822": {"title": "Robust estimation of heteroscedastic regression models: a brief overview and new proposals", "link": "http://arxiv.org/abs/2311.02822", "description": "We collect robust proposals given in the field of regression models with\nheteroscedastic errors. Our motivation stems from the fact that the\npractitioner frequently faces the confluence of two phenomena in the context of\ndata analysis: non--linearity and heteroscedasticity. The impact of\nheteroscedasticity on the precision of the estimators is well--known, however\nthe conjunction of these two phenomena makes handling outliers more difficult.\n\nAn iterative procedure to estimate the parameters of a heteroscedastic\nnon--linear model is considered. The studied estimators combine weighted\n$MM-$regression estimators, to control the impact of high leverage points, and\na robust method to estimate the parameters of the variance function."}, "http://arxiv.org/abs/2311.03247": {"title": "Multivariate selfsimilarity: Multiscale eigen-structures for selfsimilarity parameter estimation", "link": "http://arxiv.org/abs/2311.03247", "description": "Scale-free dynamics, formalized by selfsimilarity, provides a versatile\nparadigm massively and ubiquitously used to model temporal dynamics in\nreal-world data. However, its practical use has mostly remained univariate so\nfar. By contrast, modern applications often demand multivariate data analysis.\nAccordingly, models for multivariate selfsimilarity were recently proposed.\nNevertheless, they have remained rarely used in practice because of a lack of\navailable robust estimation procedures for the vector of selfsimilarity\nparameters. Building upon recent mathematical developments, the present work\nputs forth an efficient estimation procedure based on the theoretical study of\nthe multiscale eigenstructure of the wavelet spectrum of multivariate\nselfsimilar processes. The estimation performance is studied theoretically in\nthe asymptotic limits of large scale and sample sizes, and computationally for\nfinite-size samples. As a practical outcome, a fully operational and documented\nmultivariate signal processing estimation toolbox is made freely available and\nis ready for practical use on real-world data. Its potential benefits are\nillustrated in epileptic seizure prediction from multi-channel EEG data."}, "http://arxiv.org/abs/2311.03289": {"title": "Batch effect correction with sample remeasurement in highly confounded case-control studies", "link": "http://arxiv.org/abs/2311.03289", "description": "Batch effects are pervasive in biomedical studies. One approach to address\nthe batch effects is repeatedly measuring a subset of samples in each batch.\nThese remeasured samples are used to estimate and correct the batch effects.\nHowever, rigorous statistical methods for batch effect correction with\nremeasured samples are severely under-developed. In this study, we developed a\nframework for batch effect correction using remeasured samples in highly\nconfounded case-control studies. We provided theoretical analyses of the\nproposed procedure, evaluated its power characteristics, and provided a power\ncalculation tool to aid in the study design. We found that the number of\nsamples that need to be remeasured depends strongly on the between-batch\ncorrelation. When the correlation is high, remeasuring a small subset of\nsamples is possible to rescue most of the power."}, "http://arxiv.org/abs/2311.03343": {"title": "Distribution-uniform anytime-valid inference", "link": "http://arxiv.org/abs/2311.03343", "description": "Are asymptotic confidence sequences and anytime $p$-values uniformly valid\nfor a nontrivial class of distributions $\\mathcal{P}$? We give a positive\nanswer to this question by deriving distribution-uniform anytime-valid\ninference procedures. Historically, anytime-valid methods -- including\nconfidence sequences, anytime $p$-values, and sequential hypothesis tests that\nenable inference at stopping times -- have been justified nonasymptotically.\nNevertheless, asymptotic procedures such as those based on the central limit\ntheorem occupy an important part of statistical toolbox due to their\nsimplicity, universality, and weak assumptions. While recent work has derived\nasymptotic analogues of anytime-valid methods with the aforementioned benefits,\nthese were not shown to be $\\mathcal{P}$-uniform, meaning that their\nasymptotics are not uniformly valid in a class of distributions $\\mathcal{P}$.\nIndeed, the anytime-valid inference literature currently has no central limit\ntheory to draw from that is both uniform in $\\mathcal{P}$ and in the sample\nsize $n$. This paper fills that gap by deriving a novel $\\mathcal{P}$-uniform\nstrong Gaussian approximation theorem, enabling $\\mathcal{P}$-uniform\nanytime-valid inference for the first time. Along the way, our Gaussian\napproximation also yields a $\\mathcal{P}$-uniform law of the iterated\nlogarithm."}, "http://arxiv.org/abs/2009.10780": {"title": "Independent finite approximations for Bayesian nonparametric inference", "link": "http://arxiv.org/abs/2009.10780", "description": "Completely random measures (CRMs) and their normalizations (NCRMs) offer\nflexible models in Bayesian nonparametrics. But their infinite dimensionality\npresents challenges for inference. Two popular finite approximations are\ntruncated finite approximations (TFAs) and independent finite approximations\n(IFAs). While the former have been well-studied, IFAs lack similarly general\nbounds on approximation error, and there has been no systematic comparison\nbetween the two options. In the present work, we propose a general recipe to\nconstruct practical finite-dimensional approximations for homogeneous CRMs and\nNCRMs, in the presence or absence of power laws. We call our construction the\nautomated independent finite approximation (AIFA). Relative to TFAs, we show\nthat AIFAs facilitate more straightforward derivations and use of parallel\ncomputing in approximate inference. We upper bound the approximation error of\nAIFAs for a wide class of common CRMs and NCRMs -- and thereby develop\nguidelines for choosing the approximation level. Our lower bounds in key cases\nsuggest that our upper bounds are tight. We prove that, for worst-case choices\nof observation likelihoods, TFAs are more efficient than AIFAs. Conversely, we\nfind that in real-data experiments with standard likelihoods, AIFAs and TFAs\nperform similarly. Moreover, we demonstrate that AIFAs can be used for\nhyperparameter estimation even when other potential IFA options struggle or do\nnot apply."}, "http://arxiv.org/abs/2111.07517": {"title": "Correlation Improves Group Testing: Capturing the Dilution Effect", "link": "http://arxiv.org/abs/2111.07517", "description": "Population-wide screening to identify and isolate infectious individuals is a\npowerful tool for controlling COVID-19 and other infectious diseases. Group\ntesting can enable such screening despite limited testing resources. Samples'\nviral loads are often positively correlated, either because prevalence and\nsample collection are both correlated with geography, or through intentional\nenhancement, e.g., by pooling samples from people in similar risk groups. Such\ncorrelation is known to improve test efficiency in mathematical models with\nfixed sensitivity. In reality, however, dilution degrades a pooled test's\nsensitivity by an amount that varies with the number of positives in the pool.\nIn the presence of this dilution effect, we study the impact of correlation on\nthe most widely-used group testing procedure, the Dorfman procedure. We show\nthat correlation's effects are significantly altered by the dilution effect. We\nprove that under a general correlation structure, pooling correlated samples\ntogether (called correlated pooling) achieves higher sensitivity but can\ndegrade test efficiency compared to independently pooling the samples (called\nnaive pooling) using the same pool size. We identify an alternative measure of\ntest resource usage, the number of positives found per test consumed, which we\nargue is better aligned with infection control, and show that correlated\npooling outperforms naive pooling on this measure. We build a realistic\nagent-based simulation to contextualize our theoretical results within an\nepidemic control framework. We argue that the dilution effect makes it even\nmore important for policy-makers evaluating group testing protocols for\nlarge-scale screening to incorporate naturally arising correlation and to\nintentionally maximize correlation."}, "http://arxiv.org/abs/2202.05349": {"title": "Robust Parameter Estimation for the Lee-Carter Family: A Probabilistic Principal Component Approach", "link": "http://arxiv.org/abs/2202.05349", "description": "The well-known Lee-Carter model uses a bilinear form\n$\\log(m_{x,t})=a_x+b_xk_t$ to represent the log mortality rate and has been\nwidely researched and developed over the past thirty years. However, there has\nbeen little attention being paid to the robustness of the parameters against\noutliers, especially when estimating $b_x$. In response, we propose a robust\nestimation method for a wide family of Lee-Carter-type models, treating the\nproblem as a Probabilistic Principal Component Analysis (PPCA) with\nmultivariate $t$-distributions. An efficient Expectation-Maximization (EM)\nalgorithm is also derived for implementation.\n\nThe benefits of the method are threefold: 1) it produces more robust\nestimates of both $b_x$ and $k_t$, 2) it can be naturally extended to a large\nfamily of Lee-Carter type models, including those for modelling multiple\npopulations, and 3) it can be integrated with other existing time series models\nfor $k_t$. Using numerical studies based on United States mortality data from\nthe Human Mortality Database, we show the proposed model performs more robust\ncompared to conventional methods in the presence of outliers."}, "http://arxiv.org/abs/2204.11979": {"title": "Semi-Parametric Sensitivity Analysis for Trials with Irregular and Informative Assessment Times", "link": "http://arxiv.org/abs/2204.11979", "description": "Many trials are designed to collect outcomes at or around pre-specified times\nafter randomization. In practice, there can be substantial variability in the\ntimes when participants are actually assessed. Such irregular assessment times\npose a challenge to learning the effect of treatment since not all participants\nhave outcome assessments at the times of interest. Furthermore, observed\noutcome values may not be representative of all participants' outcomes at a\ngiven time. This problem, known as informative assessment times, can arise if\nparticipants tend to have assessments when their outcomes are better (or worse)\nthan at other times, or if participants with better outcomes tend to have more\n(or fewer) assessments. Methods have been developed that account for some types\nof informative assessment; however, since these methods rely on untestable\nassumptions, sensitivity analyses are needed. We develop a sensitivity analysis\nmethodology by extending existing weighting methods. Our method accounts for\nthe possibility that participants with worse outcomes at a given time are more\n(or less) likely than other participants to have an assessment at that time,\neven after controlling for variables observed earlier in the study. We apply\nour method to a randomized trial of low-income individuals with uncontrolled\nasthma. We illustrate implementation of our influence-function based estimation\nprocedure in detail, and we derive the large-sample distribution of our\nestimator and evaluate its finite-sample performance."}, "http://arxiv.org/abs/2205.13935": {"title": "Detecting hidden confounding in observational data using multiple environments", "link": "http://arxiv.org/abs/2205.13935", "description": "A common assumption in causal inference from observational data is that there\nis no hidden confounding. Yet it is, in general, impossible to verify this\nassumption from a single dataset. Under the assumption of independent causal\nmechanisms underlying the data-generating process, we demonstrate a way to\ndetect unobserved confounders when having multiple observational datasets\ncoming from different environments. We present a theory for testable\nconditional independencies that are only absent when there is hidden\nconfounding and examine cases where we violate its assumptions: degenerate &amp;\ndependent mechanisms, and faithfulness violations. Additionally, we propose a\nprocedure to test these independencies and study its empirical finite-sample\nbehavior using simulation studies and semi-synthetic data based on a real-world\ndataset. In most cases, the proposed procedure correctly predicts the presence\nof hidden confounding, particularly when the confounding bias is large."}, "http://arxiv.org/abs/2206.09444": {"title": "Bayesian non-conjugate regression via variational message passing", "link": "http://arxiv.org/abs/2206.09444", "description": "Variational inference is a popular method for approximating the posterior\ndistribution of hierarchical Bayesian models. It is well-recognized in the\nliterature that the choice of the approximation family and the regularity\nproperties of the posterior strongly influence the efficiency and accuracy of\nvariational methods. While model-specific conjugate approximations offer\nsimplicity, they often converge slowly and may yield poor approximations.\nNon-conjugate approximations instead are more flexible but typically require\nthe calculation of expensive multidimensional integrals. This study focuses on\nBayesian regression models that use possibly non-differentiable loss functions\nto measure prediction misfit. The data behavior is modeled using a linear\npredictor, potentially transformed using a bijective link function. Examples\ninclude generalized linear models, mixed additive models, support vector\nmachines, and quantile regression. To address the limitations of non-conjugate\nsettings, the study proposes an efficient non-conjugate variational message\npassing method for approximate posterior inference, which only requires the\ncalculation of univariate numerical integrals when analytical solutions are not\navailable. The approach does not require differentiability, conjugacy, or\nmodel-specific data-augmentation strategies, thereby naturally extending to\nmodels with non-conjugate likelihood functions. Additionally, a stochastic\nimplementation is provided to handle large-scale data problems. The proposed\nmethod's performances are evaluated through extensive simulations and real data\nexamples. Overall, the results highlight the effectiveness of the proposed\nvariational message passing method, demonstrating its computational efficiency\nand approximation accuracy as an alternative to existing methods in Bayesian\ninference for regression models."}, "http://arxiv.org/abs/2206.10143": {"title": "A Contrastive Approach to Online Change Point Detection", "link": "http://arxiv.org/abs/2206.10143", "description": "We suggest a novel procedure for online change point detection. Our approach\nexpands an idea of maximizing a discrepancy measure between points from\npre-change and post-change distributions. This leads to a flexible procedure\nsuitable for both parametric and nonparametric scenarios. We prove\nnon-asymptotic bounds on the average running length of the procedure and its\nexpected detection delay. The efficiency of the algorithm is illustrated with\nnumerical experiments on synthetic and real-world data sets."}, "http://arxiv.org/abs/2206.15367": {"title": "Targeted learning in observational studies with multi-valued treatments: An evaluation of antipsychotic drug treatment safety", "link": "http://arxiv.org/abs/2206.15367", "description": "We investigate estimation of causal effects of multiple competing\n(multi-valued) treatments in the absence of randomization. Our work is\nmotivated by an intention-to-treat study of the relative cardiometabolic risk\nof assignment to one of six commonly prescribed antipsychotic drugs in a cohort\nof nearly 39,000 adults adults with serious mental illness. Doubly-robust\nestimators, such as targeted minimum loss-based estimation (TMLE), require\ncorrect specification of either the treatment model or outcome model to ensure\nconsistent estimation; however, common TMLE implementations estimate treatment\nprobabilities using multiple binomial regressions rather than multinomial\nregression. We implement a TMLE estimator that uses multinomial treatment\nassignment and ensemble machine learning to estimate average treatment effects.\nOur multinomial implementation improves coverage, but does not necessarily\nreduce bias, relative to the binomial implementation in simulation experiments\nwith varying treatment propensity overlap and event rates. Evaluating the\ncausal effects of the antipsychotics on 3-year diabetes risk or death, we find\na safety benefit of moving from a second-generation drug considered among the\nsafest of the second-generation drugs to an infrequently prescribed\nfirst-generation drug thought to pose a generally low cardiometabolic risk."}, "http://arxiv.org/abs/2209.04364": {"title": "Evaluating tests for cluster-randomized trials with few clusters under generalized linear mixed models with covariate adjustment: a simulation study", "link": "http://arxiv.org/abs/2209.04364", "description": "Generalized linear mixed models (GLMM) are commonly used to analyze clustered\ndata, but when the number of clusters is small to moderate, standard\nstatistical tests may produce elevated type I error rates. Small-sample\ncorrections have been proposed for continuous or binary outcomes without\ncovariate adjustment. However, appropriate tests to use for count outcomes or\nunder covariate-adjusted models remains unknown. An important setting in which\nthis issue arises is in cluster-randomized trials (CRTs). Because many CRTs\nhave just a few clusters (e.g., clinics or health systems), covariate\nadjustment is particularly critical to address potential chance imbalance\nand/or low power (e.g., adjustment following stratified randomization or for\nthe baseline value of the outcome). We conducted simulations to evaluate\nGLMM-based tests of the treatment effect that account for the small (10) or\nmoderate (20) number of clusters under a parallel-group CRT setting across\nscenarios of covariate adjustment (including adjustment for one or more\nperson-level or cluster-level covariates) for both binary and count outcomes.\nWe find that when the intraclass correlation is non-negligible ($\\geq 0.01$)\nand the number of covariates is small ($\\leq 2$), likelihood ratio tests with a\nbetween-within denominator degree of freedom have type I error rates close to\nthe nominal level. When the number of covariates is moderate ($\\geq 5$), across\nour simulation scenarios, the relative performance of the tests varied\nconsiderably and no method performed uniformly well. Therefore, we recommend\nadjusting for no more than a few covariates and using likelihood ratio tests\nwith a between-within denominator degree of freedom."}, "http://arxiv.org/abs/2211.14578": {"title": "Estimation and inference for transfer learning with high-dimensional quantile regression", "link": "http://arxiv.org/abs/2211.14578", "description": "Transfer learning has become an essential technique to exploit information\nfrom the source domain to boost performance of the target task. Despite the\nprevalence in high-dimensional data, heterogeneity and heavy tails are\ninsufficiently accounted for by current transfer learning approaches and thus\nmay undermine the resulting performance. We propose a transfer learning\nprocedure in the framework of high-dimensional quantile regression models to\naccommodate heterogeneity and heavy tails in the source and target domains. We\nestablish error bounds of transfer learning estimator based on delicately\nselected transferable source domains, showing that lower error bounds can be\nachieved for critical selection criterion and larger sample size of source\ntasks. We further propose valid confidence interval and hypothesis test\nprocedures for individual component of high-dimensional quantile regression\ncoefficients by advocating a double transfer learning estimator, which is\none-step debiased estimator for the transfer learning estimator wherein the\ntechnique of transfer learning is designed again. By adopting data-splitting\ntechnique, we advocate a transferability detection approach that guarantees to\ncircumvent negative transfer and identify transferable sources with high\nprobability. Simulation results demonstrate that the proposed method exhibits\nsome favorable and compelling performances and the practical utility is further\nillustrated by analyzing a real example."}, "http://arxiv.org/abs/2306.03302": {"title": "Statistical Inference Under Constrained Selection Bias", "link": "http://arxiv.org/abs/2306.03302", "description": "Large-scale datasets are increasingly being used to inform decision making.\nWhile this effort aims to ground policy in real-world evidence, challenges have\narisen as selection bias and other forms of distribution shifts often plague\nobservational data. Previous attempts to provide robust inference have given\nguarantees depending on a user-specified amount of possible distribution shift\n(e.g., the maximum KL divergence between the observed and target\ndistributions). However, decision makers will often have additional knowledge\nabout the target distribution which constrains the kind of possible shifts. To\nleverage such information, we propose a framework that enables statistical\ninference in the presence of selection bias which obeys user-specified\nconstraints in the form of functions whose expectation is known under the\ntarget distribution. The output is high-probability bounds on the value of an\nestimand for the target distribution. Hence, our method leverages domain\nknowledge in order to partially identify a wide class of estimands. We analyze\nthe computational and statistical properties of methods to estimate these\nbounds and show that our method can produce informative bounds on a variety of\nsimulated and semisynthetic tasks, as well as in a real-world use case."}, "http://arxiv.org/abs/2307.15348": {"title": "Stratified principal component analysis", "link": "http://arxiv.org/abs/2307.15348", "description": "This paper investigates a general family of covariance models with repeated\neigenvalues extending probabilistic principal component analysis (PPCA). A\ngeometric interpretation shows that these models are parameterised by flag\nmanifolds and stratify the space of covariance matrices according to the\nsequence of eigenvalue multiplicities. The subsequent analysis sheds light on\nPPCA and answers an important question on the practical identifiability of\nindividual eigenvectors. It notably shows that one rarely has enough samples to\nfit a covariance model with distinct eigenvalues and that block-averaging the\nadjacent sample eigenvalues with small gaps achieves a better\ncomplexity/goodness-of-fit tradeoff."}, "http://arxiv.org/abs/2308.02005": {"title": "Bias Correction for Randomization-Based Estimation in Inexactly Matched Observational Studies", "link": "http://arxiv.org/abs/2308.02005", "description": "Matching has been widely used to mimic a randomized experiment with\nobservational data. Ideally, treated subjects are exactly matched with controls\nfor the covariates, and randomization-based estimation can then be conducted as\nin a randomized experiment (assuming no unobserved covariates). However, when\nthere exists continuous covariates or many covariates, matching typically\nshould be inexact. Previous studies have routinely ignored inexact matching in\nthe downstream randomization-based estimation as long as some covariate balance\ncriteria are satisfied, which can cause severe estimation bias. Built on the\ncovariate-adaptive randomization inference framework, in this research note, we\npropose two new classes of bias-corrected randomization-based estimators to\nreduce estimation bias due to inexact matching: the bias-corrected maximum\n$p$-value estimator for the constant treatment effect and the bias-corrected\ndifference-in-means estimator for the average treatment effect. Our simulation\nresults show that the proposed bias-corrected estimators can effectively reduce\nestimation bias due to inexact matching."}, "http://arxiv.org/abs/2310.01153": {"title": "Online Permutation Tests: $e$-values and Likelihood Ratios for Testing Group Invariance", "link": "http://arxiv.org/abs/2310.01153", "description": "We develop a flexible online version of the permutation test. This allows us\nto test exchangeability as the data is arriving, where we can choose to stop or\ncontinue without invalidating the size of the test. Our methods generalize\nbeyond exchangeability to other forms of invariance under a compact group. Our\napproach relies on constructing an $e$-process that is the running product of\nmultiple conditional $e$-values. To construct $e$-values, we first develop an\nessentially complete class of admissible $e$-values in which one can flexibly\n`plug in' almost any desired test statistic. To make the $e$-values\nconditional, we explore the intersection between the concepts of conditional\ninvariance and sequential invariance, and find that the appropriate conditional\ndistribution can be captured by a compact subgroup. To find powerful $e$-values\nfor given alternatives, we develop the theory of likelihood ratios for testing\ngroup invariance yielding new optimality results for group invariance tests.\nThese statistics turn out to exist in three different flavors, depending on the\nspace on which we specify our alternative. We apply these statistics to test\nagainst a Gaussian location shift, which yields connections to the $t$-test\nwhen testing sphericity, connections to the softmax function and its\ntemperature when testing exchangeability, and yields an improved version of a\nknown $e$-value for testing sign-symmetry. Moreover, we introduce an impatience\nparameter that allows users to obtain more power now in exchange for less power\nin the long run."}, "http://arxiv.org/abs/2311.03381": {"title": "Separating and Learning Latent Confounders to Enhancing User Preferences Modeling", "link": "http://arxiv.org/abs/2311.03381", "description": "Recommender models aim to capture user preferences from historical feedback\nand then predict user-specific feedback on candidate items. However, the\npresence of various unmeasured confounders causes deviations between the user\npreferences in the historical feedback and the true preferences, resulting in\nmodels not meeting their expected performance. Existing debias models either\n(1) specific to solving one particular bias or (2) directly obtain auxiliary\ninformation from user historical feedback, which cannot identify whether the\nlearned preferences are true user preferences or mixed with unmeasured\nconfounders. Moreover, we find that the former recommender system is not only a\nsuccessor to unmeasured confounders but also acts as an unmeasured confounder\naffecting user preference modeling, which has always been neglected in previous\nstudies. To this end, we incorporate the effect of the former recommender\nsystem and treat it as a proxy for all unmeasured confounders. We propose a\nnovel framework, \\textbf{S}eparating and \\textbf{L}earning Latent Confounders\n\\textbf{F}or \\textbf{R}ecommendation (\\textbf{SLFR}), which obtains the\nrepresentation of unmeasured confounders to identify the counterfactual\nfeedback by disentangling user preferences and unmeasured confounders, then\nguides the target model to capture the true preferences of users. Extensive\nexperiments in five real-world datasets validate the advantages of our method."}, "http://arxiv.org/abs/2311.03382": {"title": "Causal Structure Representation Learning of Confounders in Latent Space for Recommendation", "link": "http://arxiv.org/abs/2311.03382", "description": "Inferring user preferences from the historical feedback of users is a\nvaluable problem in recommender systems. Conventional approaches often rely on\nthe assumption that user preferences in the feedback data are equivalent to the\nreal user preferences without additional noise, which simplifies the problem\nmodeling. However, there are various confounders during user-item interactions,\nsuch as weather and even the recommendation system itself. Therefore,\nneglecting the influence of confounders will result in inaccurate user\npreferences and suboptimal performance of the model. Furthermore, the\nunobservability of confounders poses a challenge in further addressing the\nproblem. To address these issues, we refine the problem and propose a more\nrational solution. Specifically, we consider the influence of confounders,\ndisentangle them from user preferences in the latent space, and employ causal\ngraphs to model their interdependencies without specific labels. By cleverly\ncombining local and global causal graphs, we capture the user-specificity of\nconfounders on user preferences. We theoretically demonstrate the\nidentifiability of the obtained causal graph. Finally, we propose our model\nbased on Variational Autoencoders, named Causal Structure representation\nlearning of Confounders in latent space (CSC). We conducted extensive\nexperiments on one synthetic dataset and five real-world datasets,\ndemonstrating the superiority of our model. Furthermore, we demonstrate that\nthe learned causal representations of confounders are controllable, potentially\noffering users fine-grained control over the objectives of their recommendation\nlists with the learned causal graphs."}, "http://arxiv.org/abs/2311.03554": {"title": "Conditional Randomization Tests for Behavioral and Neural Time Series", "link": "http://arxiv.org/abs/2311.03554", "description": "Randomization tests allow simple and unambiguous tests of null hypotheses, by\ncomparing observed data to a null ensemble in which experimentally-controlled\nvariables are randomly resampled. In behavioral and neuroscience experiments,\nhowever, the stimuli presented often depend on the subject's previous actions,\nso simple randomization tests are not possible. We describe how conditional\nrandomization can be used to perform exact hypothesis tests in this situation,\nand illustrate it with two examples. We contrast conditional randomization with\na related approach of tangent randomization, in which stimuli are resampled\nbased only on events occurring in the past, which is not valid for all choices\nof test statistic. We discuss how to design experiments that allow conditional\nrandomization tests to be used."}, "http://arxiv.org/abs/2311.03630": {"title": "Counterfactual Data Augmentation with Contrastive Learning", "link": "http://arxiv.org/abs/2311.03630", "description": "Statistical disparity between distinct treatment groups is one of the most\nsignificant challenges for estimating Conditional Average Treatment Effects\n(CATE). To address this, we introduce a model-agnostic data augmentation method\nthat imputes the counterfactual outcomes for a selected subset of individuals.\nSpecifically, we utilize contrastive learning to learn a representation space\nand a similarity measure such that in the learned representation space close\nindividuals identified by the learned similarity measure have similar potential\noutcomes. This property ensures reliable imputation of counterfactual outcomes\nfor the individuals with close neighbors from the alternative treatment group.\nBy augmenting the original dataset with these reliable imputations, we can\neffectively reduce the discrepancy between different treatment groups, while\ninducing minimal imputation error. The augmented dataset is subsequently\nemployed to train CATE estimation models. Theoretical analysis and experimental\nstudies on synthetic and semi-synthetic benchmarks demonstrate that our method\nachieves significant improvements in both performance and robustness to\noverfitting across state-of-the-art models."}, "http://arxiv.org/abs/2311.03644": {"title": "BOB: Bayesian Optimized Bootstrap with Applications to Gaussian Mixture Models", "link": "http://arxiv.org/abs/2311.03644", "description": "Sampling from the joint posterior distribution of Gaussian mixture models\n(GMMs) via standard Markov chain Monte Carlo (MCMC) imposes several\ncomputational challenges, which have prevented a broader full Bayesian\nimplementation of these models. A growing body of literature has introduced the\nWeighted Likelihood Bootstrap and the Weighted Bayesian Bootstrap as\nalternatives to MCMC sampling. The core idea of these methods is to repeatedly\ncompute maximum a posteriori (MAP) estimates on many randomly weighted\nposterior densities. These MAP estimates then can be treated as approximate\nposterior draws. Nonetheless, a central question remains unanswered: How to\nselect the distribution of the random weights under arbitrary sample sizes.\nThus, we introduce the Bayesian Optimized Bootstrap (BOB), a computational\nmethod to automatically select the weights distribution by minimizing, through\nBayesian Optimization, a black-box and noisy version of the reverse KL\ndivergence between the Bayesian posterior and an approximate posterior obtained\nvia random weighting. Our proposed method allows for uncertainty\nquantification, approximate posterior sampling, and embraces recent\ndevelopments in parallel computing. We show that BOB outperforms competing\napproaches in recovering the Bayesian posterior, while retaining key\ntheoretical properties from existing methods. BOB's performance is demonstrated\nthrough extensive simulations, along with real-world data analyses."}, "http://arxiv.org/abs/2311.03660": {"title": "Sampling via F\\\"ollmer Flow", "link": "http://arxiv.org/abs/2311.03660", "description": "We introduce a novel unit-time ordinary differential equation (ODE) flow\ncalled the preconditioned F\\\"{o}llmer flow, which efficiently transforms a\nGaussian measure into a desired target measure at time 1. To discretize the\nflow, we apply Euler's method, where the velocity field is calculated either\nanalytically or through Monte Carlo approximation using Gaussian samples. Under\nreasonable conditions, we derive a non-asymptotic error bound in the\nWasserstein distance between the sampling distribution and the target\ndistribution. Through numerical experiments on mixture distributions in 1D, 2D,\nand high-dimensional spaces, we demonstrate that the samples generated by our\nproposed flow exhibit higher quality compared to those obtained by several\nexisting methods. Furthermore, we propose leveraging the F\\\"{o}llmer flow as a\nwarmstart strategy for existing Markov Chain Monte Carlo (MCMC) methods, aiming\nto mitigate mode collapses and enhance their performance. Finally, thanks to\nthe deterministic nature of the F\\\"{o}llmer flow, we can leverage deep neural\nnetworks to fit the trajectory of sample evaluations. This allows us to obtain\na generator for one-step sampling as a result."}, "http://arxiv.org/abs/2311.03763": {"title": "Thresholding the higher criticism test statistics for optimality in a heterogeneous setting", "link": "http://arxiv.org/abs/2311.03763", "description": "Donoho and Kipnis (2022) showed that the the higher criticism (HC) test\nstatistic has a non-Gaussian phase transition but remarked that it is probably\nnot optimal, in the detection of sparse differences between two large frequency\ntables when the counts are low. The setting can be considered to be\nheterogeneous, with cells containing larger total counts more able to detect\nsmaller differences. We provide a general study here of sparse detection\narising from such heterogeneous settings, and showed that optimality of the HC\ntest statistic requires thresholding, for example in the case of frequency\ntable comparison, to restrict to p-values of cells with total counts exceeding\na threshold. The use of thresholding also leads to optimality of the HC test\nstatistic when it is applied on the sparse Poisson means model of Arias-Castro\nand Wang (2015). The phase transitions we consider here are non-Gaussian, and\ninvolve an interplay between the rate functions of the response and sample size\ndistributions. We also showed, both theoretically and in a numerical study,\nthat applying thresholding to the Bonferroni test statistic results in better\nsparse mixture detection in heterogeneous settings."}, "http://arxiv.org/abs/2311.03769": {"title": "Nonparametric Screening for Additive Quantile Regression in Ultra-high Dimension", "link": "http://arxiv.org/abs/2311.03769", "description": "In practical applications, one often does not know the \"true\" structure of\nthe underlying conditional quantile function, especially in the ultra-high\ndimensional setting. To deal with ultra-high dimensionality, quantile-adaptive\nmarginal nonparametric screening methods have been recently developed. However,\nthese approaches may miss important covariates that are marginally independent\nof the response, or may select unimportant covariates due to their high\ncorrelations with important covariates. To mitigate such shortcomings, we\ndevelop a conditional nonparametric quantile screening procedure (complemented\nby subsequent selection) for nonparametric additive quantile regression models.\nUnder some mild conditions, we show that the proposed screening method can\nidentify all relevant covariates in a small number of steps with probability\napproaching one. The subsequent narrowed best subset (via a modified Bayesian\ninformation criterion) also contains all the relevant covariates with\noverwhelming probability. The advantages of our proposed procedure are\ndemonstrated through simulation studies and a real data example."}, "http://arxiv.org/abs/2311.03829": {"title": "Multilevel mixtures of latent trait analyzers for clustering multi-layer bipartite networks", "link": "http://arxiv.org/abs/2311.03829", "description": "Within network data analysis, bipartite networks represent a particular type\nof network where relationships occur between two disjoint sets of nodes,\nformally called sending and receiving nodes. In this context, sending nodes may\nbe organized into layers on the basis of some defined characteristics,\nresulting in a special case of multilayer bipartite network, where each layer\nincludes a specific set of sending nodes. To perform a clustering of sending\nnodes in multi-layer bipartite network, we extend the Mixture of Latent Trait\nAnalyzers (MLTA), also taking into account the influence of concomitant\nvariables on clustering formation and the multi-layer structure of the data. To\nthis aim, a multilevel approach offers a useful methodological tool to properly\naccount for the hierarchical structure of the data and for the unobserved\nsources of heterogeneity at multiple levels. A simulation study is conducted to\ntest the performance of the proposal in terms of parameters' and clustering\nrecovery. Furthermore, the model is applied to the European Social Survey data\n(ESS) to i) perform a clustering of individuals (sending nodes) based on their\ndigital skills (receiving nodes); ii) understand how socio-economic and\ndemographic characteristics influence the individual digitalization level; iii)\naccount for the multilevel structure of the data; iv) obtain a clustering of\ncountries in terms of the base-line attitude to digital technologies of their\nresidents."}, "http://arxiv.org/abs/2311.03989": {"title": "Learned Causal Method Prediction", "link": "http://arxiv.org/abs/2311.03989", "description": "For a given causal question, it is important to efficiently decide which\ncausal inference method to use for a given dataset. This is challenging because\ncausal methods typically rely on complex and difficult-to-verify assumptions,\nand cross-validation is not applicable since ground truth causal quantities are\nunobserved.In this work, we propose CAusal Method Predictor (CAMP), a framework\nfor predicting the best method for a given dataset. To this end, we generate\ndatasets from a diverse set of synthetic causal models, score the candidate\nmethods, and train a model to directly predict the highest-scoring method for\nthat dataset. Next, by formulating a self-supervised pre-training objective\ncentered on dataset assumptions relevant for causal inference, we significantly\nreduce the need for costly labeled data and enhance training efficiency. Our\nstrategy learns to map implicit dataset properties to the best method in a\ndata-driven manner. In our experiments, we focus on method prediction for\ncausal discovery. CAMP outperforms selecting any individual candidate method\nand demonstrates promising generalization to unseen semi-synthetic and\nreal-world benchmarks."}, "http://arxiv.org/abs/2311.04017": {"title": "Multivariate quantile-based permutation tests with application to functional data", "link": "http://arxiv.org/abs/2311.04017", "description": "Permutation tests enable testing statistical hypotheses in situations when\nthe distribution of the test statistic is complicated or not available. In some\nsituations, the test statistic under investigation is multivariate, with the\nmultiple testing problem being an important example. The corresponding\nmultivariate permutation tests are then typically based on a\nsuitableone-dimensional transformation of the vector of partial permutation\np-values via so called combining functions. This paper proposes a new approach\nthat utilizes the optimal measure transportation concept. The final single\np-value is computed from the empirical center-outward distribution function of\nthe permuted multivariate test statistics. This method avoids computation of\nthe partial p-values and it is easy to be implemented. In addition, it allows\nto compute and interpret contributions of the components of the multivariate\ntest statistic to the non-conformity score and to the rejection of the null\nhypothesis. Apart from this method, the measure transportation is applied also\nto the vector of partial p-values as an alternative to the classical combining\nfunctions. Both techniques are compared with the standard approaches using\nvarious practical examples in a Monte Carlo study. An application on a\nfunctional data set is provided as well."}, "http://arxiv.org/abs/2311.04037": {"title": "Causal Discovery Under Local Privacy", "link": "http://arxiv.org/abs/2311.04037", "description": "Differential privacy is a widely adopted framework designed to safeguard the\nsensitive information of data providers within a data set. It is based on the\napplication of controlled noise at the interface between the server that stores\nand processes the data, and the data consumers. Local differential privacy is a\nvariant that allows data providers to apply the privatization mechanism\nthemselves on their data individually. Therefore it provides protection also in\ncontexts in which the server, or even the data collector, cannot be trusted.\nThe introduction of noise, however, inevitably affects the utility of the data,\nparticularly by distorting the correlations between individual data components.\nThis distortion can prove detrimental to tasks such as causal discovery. In\nthis paper, we consider various well-known locally differentially private\nmechanisms and compare the trade-off between the privacy they provide, and the\naccuracy of the causal structure produced by algorithms for causal learning\nwhen applied to data obfuscated by these mechanisms. Our analysis yields\nvaluable insights for selecting appropriate local differentially private\nprotocols for causal discovery tasks. We foresee that our findings will aid\nresearchers and practitioners in conducting locally private causal discovery."}, "http://arxiv.org/abs/2311.04103": {"title": "Joint modelling of recurrent and terminal events with discretely-distributed non-parametric frailty: application on re-hospitalizations and death in heart failure patients", "link": "http://arxiv.org/abs/2311.04103", "description": "In the context of clinical and biomedical studies, joint frailty models have\nbeen developed to study the joint temporal evolution of recurrent and terminal\nevents, capturing both the heterogeneous susceptibility to experiencing a new\nepisode and the dependence between the two processes. While\ndiscretely-distributed frailty is usually more exploitable by clinicians and\nhealthcare providers, existing literature on joint frailty models predominantly\nassumes continuous distributions for the random effects. In this article, we\npresent a novel joint frailty model that assumes bivariate\ndiscretely-distributed non-parametric frailties, with an unknown finite number\nof mass points. This approach facilitates the identification of latent\nstructures among subjects, grouping them into sub-populations defined by a\nshared frailty value. We propose an estimation routine via\nExpectation-Maximization algorithm, which not only estimates the number of\nsubgroups but also serves as an unsupervised classification tool. This work is\nmotivated by a study of patients with Heart Failure (HF) receiving ACE\ninhibitors treatment in the Lombardia region of Italy. Recurrent events of\ninterest are hospitalizations due to HF and terminal event is death for any\ncause."}, "http://arxiv.org/abs/2311.04159": {"title": "Statistical Inference on Simulation Output: Batching as an Inferential Device", "link": "http://arxiv.org/abs/2311.04159", "description": "We present {batching} as an omnibus device for statistical inference on\nsimulation output. We consider the classical context of a simulationist\nperforming statistical inference on an estimator $\\theta_n$ (of an unknown\nfixed quantity $\\theta$) using only the output data $(Y_1,Y_2,\\ldots,Y_n)$\ngathered from a simulation. By \\emph{statistical inference}, we mean\napproximating the sampling distribution of the error $\\theta_n-\\theta$ toward:\n(A) estimating an ``assessment'' functional $\\psi$, e.g., bias, variance, or\nquantile; or (B) constructing a $(1-\\alpha)$-confidence region on $\\theta$. We\nargue that batching is a remarkably simple and effective inference device that\nis especially suited for handling dependent output data such as what one\nfrequently encounters in simulation contexts. We demonstrate that if the number\nof batches and the extent of their overlap are chosen correctly, batching\nretains bootstrap's attractive theoretical properties of {strong consistency}\nand {higher-order accuracy}. For constructing confidence regions, we\ncharacterize two limiting distributions associated with a Studentized\nstatistic. Our extensive numerical experience confirms theoretical insight,\nespecially about the effects of batch size and batch overlap."}, "http://arxiv.org/abs/2002.03355": {"title": "Scalable Function-on-Scalar Quantile Regression for Densely Sampled Functional Data", "link": "http://arxiv.org/abs/2002.03355", "description": "Functional quantile regression (FQR) is a useful alternative to mean\nregression for functional data as it provides a comprehensive understanding of\nhow scalar predictors influence the conditional distribution of functional\nresponses. In this article, we study the FQR model for densely sampled,\nhigh-dimensional functional data without relying on parametric error or\nindependent stochastic process assumptions, with the focus on statistical\ninference under this challenging regime along with scalable implementation.\nThis is achieved by a simple but powerful distributed strategy, in which we\nfirst perform separate quantile regression to compute $M$-estimators at each\nsampling location, and then carry out estimation and inference for the entire\ncoefficient functions by properly exploiting the uncertainty quantification and\ndependence structure of $M$-estimators. We derive a uniform Bahadur\nrepresentation and a strong Gaussian approximation result for the\n$M$-estimators on the discrete sampling grid, leading to dimension reduction\nand serving as the basis for inference. An interpolation-based estimator with\nminimax optimality is proposed, and large sample properties for point and\nsimultaneous interval estimators are established. The obtained minimax optimal\nrate under the FQR model shows an interesting phase transition phenomenon that\nhas been previously observed in functional mean regression. The proposed\nmethods are illustrated via simulations and an application to a mass\nspectrometry proteomics dataset."}, "http://arxiv.org/abs/2201.06110": {"title": "FNETS: Factor-adjusted network estimation and forecasting for high-dimensional time series", "link": "http://arxiv.org/abs/2201.06110", "description": "We propose FNETS, a methodology for network estimation and forecasting of\nhigh-dimensional time series exhibiting strong serial- and cross-sectional\ncorrelations. We operate under a factor-adjusted vector autoregressive (VAR)\nmodel which, after accounting for pervasive co-movements of the variables by\n{\\it common} factors, models the remaining {\\it idiosyncratic} dynamic\ndependence between the variables as a sparse VAR process. Network estimation of\nFNETS consists of three steps: (i) factor-adjustment via dynamic principal\ncomponent analysis, (ii) estimation of the latent VAR process via\n$\\ell_1$-regularised Yule-Walker estimator, and (iii) estimation of partial\ncorrelation and long-run partial correlation matrices. In doing so, we learn\nthree networks underpinning the VAR process, namely a directed network\nrepresenting the Granger causal linkages between the variables, an undirected\none embedding their contemporaneous relationships and finally, an undirected\nnetwork that summarises both lead-lag and contemporaneous linkages. In\naddition, FNETS provides a suite of methods for forecasting the factor-driven\nand the idiosyncratic VAR processes. Under general conditions permitting tails\nheavier than the Gaussian one, we derive uniform consistency rates for the\nestimators in both network estimation and forecasting, which hold as the\ndimension of the panel and the sample size diverge. Simulation studies and real\ndata application confirm the good performance of FNETS."}, "http://arxiv.org/abs/2202.01650": {"title": "Exposure Effects on Count Outcomes with Observational Data, with Application to Incarcerated Women", "link": "http://arxiv.org/abs/2202.01650", "description": "Causal inference methods can be applied to estimate the effect of a point\nexposure or treatment on an outcome of interest using data from observational\nstudies. For example, in the Women's Interagency HIV Study, it is of interest\nto understand the effects of incarceration on the number of sexual partners and\nthe number of cigarettes smoked after incarceration. In settings like this\nwhere the outcome is a count, the estimand is often the causal mean ratio,\ni.e., the ratio of the counterfactual mean count under exposure to the\ncounterfactual mean count under no exposure. This paper considers estimators of\nthe causal mean ratio based on inverse probability of treatment weights, the\nparametric g-formula, and doubly robust estimation, each of which can account\nfor overdispersion, zero-inflation, and heaping in the measured outcome.\nMethods are compared in simulations and are applied to data from the Women's\nInteragency HIV Study."}, "http://arxiv.org/abs/2208.14960": {"title": "Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces I: the compact case", "link": "http://arxiv.org/abs/2208.14960", "description": "Gaussian processes are arguably the most important class of spatiotemporal\nmodels within machine learning. They encode prior information about the modeled\nfunction and can be used for exact or approximate Bayesian learning. In many\napplications, particularly in physical sciences and engineering, but also in\nareas such as geostatistics and neuroscience, invariance to symmetries is one\nof the most fundamental forms of prior information one can consider. The\ninvariance of a Gaussian process' covariance to such symmetries gives rise to\nthe most natural generalization of the concept of stationarity to such spaces.\nIn this work, we develop constructive and practical techniques for building\nstationary Gaussian processes on a very large class of non-Euclidean spaces\narising in the context of symmetries. Our techniques make it possible to (i)\ncalculate covariance kernels and (ii) sample from prior and posterior Gaussian\nprocesses defined on such spaces, both in a practical manner. This work is\nsplit into two parts, each involving different technical considerations: part I\nstudies compact spaces, while part II studies non-compact spaces possessing\ncertain structure. Our contributions make the non-Euclidean Gaussian process\nmodels we study compatible with well-understood computational techniques\navailable in standard Gaussian process software packages, thereby making them\naccessible to practitioners."}, "http://arxiv.org/abs/2210.14455": {"title": "Asymmetric predictability in causal discovery: an information theoretic approach", "link": "http://arxiv.org/abs/2210.14455", "description": "Causal investigations in observational studies pose a great challenge in\nresearch where randomized trials or intervention-based studies are not\nfeasible. We develop an information geometric causal discovery and inference\nframework of \"predictive asymmetry\". For $(X, Y)$, predictive asymmetry enables\nassessment of whether $X$ is more likely to cause $Y$ or vice-versa. The\nasymmetry between cause and effect becomes particularly simple if $X$ and $Y$\nare deterministically related. We propose a new metric called the Directed\nMutual Information ($DMI$) and establish its key statistical properties. $DMI$\nis not only able to detect complex non-linear association patterns in bivariate\ndata, but also is able to detect and infer causal relations. Our proposed\nmethodology relies on scalable non-parametric density estimation using Fourier\ntransform. The resulting estimation method is manyfold faster than the\nclassical bandwidth-based density estimation. We investigate key asymptotic\nproperties of the $DMI$ methodology and a data-splitting technique is utilized\nto facilitate causal inference using the $DMI$. Through simulation studies and\nan application, we illustrate the performance of $DMI$."}, "http://arxiv.org/abs/2211.01227": {"title": "Conformalized survival analysis with adaptive cutoffs", "link": "http://arxiv.org/abs/2211.01227", "description": "This paper introduces an assumption-lean method that constructs valid and\nefficient lower predictive bounds (LPBs) for survival times with censored data.\nWe build on recent work by Cand\\`es et al. (2021), whose approach first subsets\nthe data to discard any data points with early censoring times, and then uses a\nreweighting technique (namely, weighted conformal inference (Tibshirani et al.,\n2019)) to correct for the distribution shift introduced by this subsetting\nprocedure.\n\nFor our new method, instead of constraining to a fixed threshold for the\ncensoring time when subsetting the data, we allow for a covariate-dependent and\ndata-adaptive subsetting step, which is better able to capture the\nheterogeneity of the censoring mechanism. As a result, our method can lead to\nLPBs that are less conservative and give more accurate information. We show\nthat in the Type I right-censoring setting, if either of the censoring\nmechanism or the conditional quantile of survival time is well estimated, our\nproposed procedure achieves nearly exact marginal coverage, where in the latter\ncase we additionally have approximate conditional coverage. We evaluate the\nvalidity and efficiency of our proposed algorithm in numerical experiments,\nillustrating its advantage when compared with other competing methods. Finally,\nour method is applied to a real dataset to generate LPBs for users' active\ntimes on a mobile app."}, "http://arxiv.org/abs/2211.07451": {"title": "Additive Covariance Matrix Models: Modelling Regional Electricity Net-Demand in Great Britain", "link": "http://arxiv.org/abs/2211.07451", "description": "Forecasts of regional electricity net-demand, consumption minus embedded\ngeneration, are an essential input for reliable and economic power system\noperation, and energy trading. While such forecasts are typically performed\nregion by region, operations such as managing power flows require spatially\ncoherent joint forecasts, which account for cross-regional dependencies. Here,\nwe forecast the joint distribution of net-demand across the 14 regions\nconstituting Great Britain's electricity network. Joint modelling is\ncomplicated by the fact that the net-demand variability within each region, and\nthe dependencies between regions, vary with temporal, socio-economical and\nweather-related factors. We accommodate for these characteristics by proposing\na multivariate Gaussian model based on a modified Cholesky parametrisation,\nwhich allows us to model each unconstrained parameter via an additive model.\nGiven that the number of model parameters and covariates is large, we adopt a\nsemi-automated approach to model selection, based on gradient boosting. In\naddition to comparing the forecasting performance of several versions of the\nproposed model with that of two non-Gaussian copula-based models, we visually\nexplore the model output to interpret how the covariates affect net-demand\nvariability and dependencies.\n\nThe code for reproducing the results in this paper is available at\nhttps://doi.org/10.5281/zenodo.7315105, while methods for building and fitting\nmultivariate Gaussian additive models are provided by the SCM R package,\navailable at https://github.com/VinGioia90/SCM."}, "http://arxiv.org/abs/2211.16182": {"title": "Residual Permutation Test for High-Dimensional Regression Coefficient Testing", "link": "http://arxiv.org/abs/2211.16182", "description": "We consider the problem of testing whether a single coefficient is equal to\nzero in fixed-design linear models under a moderately high-dimensional regime,\nwhere the dimension of covariates $p$ is allowed to be in the same order of\nmagnitude as sample size $n$. In this regime, to achieve finite-population\nvalidity, existing methods usually require strong distributional assumptions on\nthe noise vector (such as Gaussian or rotationally invariant), which limits\ntheir applications in practice. In this paper, we propose a new method, called\nresidual permutation test (RPT), which is constructed by projecting the\nregression residuals onto the space orthogonal to the union of the column\nspaces of the original and permuted design matrices. RPT can be proved to\nachieve finite-population size validity under fixed design with just\nexchangeable noises, whenever $p &lt; n / 2$. Moreover, RPT is shown to be\nasymptotically powerful for heavy tailed noises with bounded $(1+t)$-th order\nmoment when the true coefficient is at least of order $n^{-t/(1+t)}$ for $t \\in\n[0,1]$. We further proved that this signal size requirement is essentially\nrate-optimal in the minimax sense. Numerical studies confirm that RPT performs\nwell in a wide range of simulation settings with normal and heavy-tailed noise\ndistributions."}, "http://arxiv.org/abs/2303.11399": {"title": "How Much Should We Trust Instrumental Variable Estimates in Political Science? Practical Advice Based on Over 60 Replicated Studies", "link": "http://arxiv.org/abs/2303.11399", "description": "Instrumental variable (IV) strategies are widely used in political science to\nestablish causal relationships. However, the identifying assumptions required\nby an IV design are demanding, and it remains challenging for researchers to\nassess their validity. In this paper, we replicate 67 papers published in three\ntop journals in political science during 2010-2022 and identify several\ntroubling patterns. First, researchers often overestimate the strength of their\nIVs due to non-i.i.d. errors, such as a clustering structure. Second, the most\ncommonly used t-test for the two-stage-least-squares (2SLS) estimates often\nseverely underestimates uncertainty. Using more robust inferential methods, we\nfind that around 19-30% of the 2SLS estimates in our sample are underpowered.\nThird, in the majority of the replicated studies, the 2SLS estimates are much\nlarger than the ordinary-least-squares estimates, and their ratio is negatively\ncorrelated with the strength of the IVs in studies where the IVs are not\nexperimentally generated, suggesting potential violations of unconfoundedness\nor the exclusion restriction. To help researchers avoid these pitfalls, we\nprovide a checklist for better practice."}, "http://arxiv.org/abs/2306.14351": {"title": "Comparing Causal Frameworks: Potential Outcomes, Structural Models, Graphs, and Abstractions", "link": "http://arxiv.org/abs/2306.14351", "description": "The aim of this paper is to make clear and precise the relationship between\nthe Rubin causal model (RCM) and structural causal model (SCM) frameworks for\ncausal inference. Adopting a neutral logical perspective, and drawing on\nprevious work, we show what is required for an RCM to be representable by an\nSCM. A key result then shows that every RCM -- including those that violate\nalgebraic principles implied by the SCM framework -- emerges as an abstraction\nof some representable RCM. Finally, we illustrate the power of this\nconciliatory perspective by pinpointing an important role for SCM principles in\nclassic applications of RCMs; conversely, we offer a characterization of the\nalgebraic constraints implied by a graph, helping to substantiate further\ncomparisons between the two frameworks."}, "http://arxiv.org/abs/2307.07342": {"title": "Bounded-memory adjusted scores estimation in generalized linear models with large data sets", "link": "http://arxiv.org/abs/2307.07342", "description": "The widespread use of maximum Jeffreys'-prior penalized likelihood in\nbinomial-response generalized linear models, and in logistic regression, in\nparticular, are supported by the results of Kosmidis and Firth (2021,\nBiometrika), who show that the resulting estimates are also always\nfinite-valued, even in cases where the maximum likelihood estimates are not,\nwhich is a practical issue regardless of the size of the data set. In logistic\nregression, the implied adjusted score equations are formally bias-reducing in\nasymptotic frameworks with a fixed number of parameters and appear to deliver a\nsubstantial reduction in the persistent bias of the maximum likelihood\nestimator in high-dimensional settings where the number of parameters grows\nasymptotically linearly and slower than the number of observations. In this\nwork, we develop and present two new variants of iteratively reweighted least\nsquares for estimating generalized linear models with adjusted score equations\nfor mean bias reduction and maximization of the likelihood penalized by a\npositive power of the Jeffreys-prior penalty, which eliminate the requirement\nof storing $O(n)$ quantities in memory, and can operate with data sets that\nexceed computer memory or even hard drive capacity. We achieve that through\nincremental QR decompositions, which enable IWLS iterations to have access only\nto data chunks of predetermined size. We assess the procedures through a\nreal-data application with millions of observations."}, "http://arxiv.org/abs/2309.02422": {"title": "Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test", "link": "http://arxiv.org/abs/2309.02422", "description": "Maximum mean discrepancy (MMD) refers to a general class of nonparametric\ntwo-sample tests that are based on maximizing the mean difference over samples\nfrom one distribution $P$ versus another $Q$, over all choices of data\ntransformations $f$ living in some function space $\\mathcal{F}$. Inspired by\nrecent work that connects what are known as functions of $\\textit{Radon bounded\nvariation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study\nthe MMD defined by taking $\\mathcal{F}$ to be the unit ball in the RBV space of\na given smoothness order $k \\geq 0$. This test, which we refer to as the\n$\\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as a\ngeneralization of the well-known and classical Kolmogorov-Smirnov (KS) test to\nmultiple dimensions and higher orders of smoothness. It is also intimately\nconnected to neural networks: we prove that the witness in the RKS test -- the\nfunction $f$ achieving the maximum mean difference -- is always a ridge spline\nof degree $k$, i.e., a single neuron in a neural network. This allows us to\nleverage the power of modern deep learning toolkits to (approximately) optimize\nthe criterion that underlies the RKS test. We prove that the RKS test has\nasymptotically full power at distinguishing any distinct pair $P \\not= Q$ of\ndistributions, derive its asymptotic null distribution, and carry out extensive\nexperiments to elucidate the strengths and weakenesses of the RKS test versus\nthe more traditional kernel MMD test."}, "http://arxiv.org/abs/2311.04318": {"title": "Estimation for multistate models subject to reporting delays and incomplete event adjudication", "link": "http://arxiv.org/abs/2311.04318", "description": "Complete observation of event histories is often impossible due to sampling\neffects such as right-censoring and left-truncation, but also due to reporting\ndelays and incomplete event adjudication. This is for example the case during\ninterim stages of clinical trials and for health insurance claims. In this\npaper, we develop a parametric method that takes the aforementioned effects\ninto account, treating the latter two as partially exogenous. The method, which\ntakes the form of a two-step M-estimation procedure, is applicable to\nmultistate models in general, including competing risks and recurrent event\nmodels. The effect of reporting delays is derived via thinning, extending\nexisting results for Poisson models. To address incomplete event adjudication,\nwe propose an imputed likelihood approach which, compared to existing methods,\nhas the advantage of allowing for dependencies between the event history and\nadjudication processes as well as allowing for unreported events and multiple\nevent types. We establish consistency and asymptotic normality under standard\nidentifiability, integrability, and smoothness conditions, and we demonstrate\nthe validity of the percentile bootstrap. Finally, a simulation study shows\nfavorable finite sample performance of our method compared to other\nalternatives, while an application to disability insurance data illustrates its\npractical potential."}, "http://arxiv.org/abs/2311.04359": {"title": "Flexibly Estimating and Interpreting Heterogeneous Treatment Effects of Laparoscopic Surgery for Cholecystitis Patients", "link": "http://arxiv.org/abs/2311.04359", "description": "Laparoscopic surgery has been shown through a number of randomized trials to\nbe an effective form of treatment for cholecystitis. Given this evidence, one\nnatural question for clinical practice is: does the effectiveness of\nlaparoscopic surgery vary among patients? It might be the case that, while the\noverall effect is positive, some patients treated with laparoscopic surgery may\nrespond positively to the intervention while others do not or may be harmed. In\nour study, we focus on conditional average treatment effects to understand\nwhether treatment effects vary systematically with patient characteristics.\nRecent methodological work has developed a meta-learner framework for flexible\nestimation of conditional causal effects. In this framework, nonparametric\nestimation methods can be used to avoid bias from model misspecification while\npreserving statistical efficiency. In addition, researchers can flexibly and\neffectively explore whether treatment effects vary with a large number of\npossible effect modifiers. However, these methods have certain limitations. For\nexample, conducting inference can be challenging if black-box models are used.\nFurther, interpreting and visualizing the effect estimates can be difficult\nwhen there are multi-valued effect modifiers. In this paper, we develop new\nmethods that allow for interpretable results and inference from the\nmeta-learner framework for heterogeneous treatment effects estimation. We also\ndemonstrate methods that allow for an exploratory analysis to identify possible\neffect modifiers. We apply our methods to a large database for the use of\nlaparoscopic surgery in treating cholecystitis. We also conduct a series of\nsimulation studies to understand the relative performance of the methods we\ndevelop. Our study provides key guidelines for the interpretation of\nconditional causal effects from the meta-learner framework."}, "http://arxiv.org/abs/2311.04540": {"title": "On the estimation of the number of components in multivariate functional principal component analysis", "link": "http://arxiv.org/abs/2311.04540", "description": "Happ and Greven (2018) developed a methodology for principal components\nanalysis of multivariate functional data for data observed on different\ndimensional domains. Their approach relies on an estimation of univariate\nfunctional principal components for each univariate functional feature. In this\npaper, we present extensive simulations to investigate choosing the number of\nprincipal components to retain. We show empirically that the conventional\napproach of using a percentage of variance explained threshold for each\nunivariate functional feature may be unreliable when aiming to explain an\noverall percentage of variance in the multivariate functional data, and thus we\nadvise practitioners to be careful when using it."}, "http://arxiv.org/abs/2311.04585": {"title": "Goodness-of-Fit Tests for Linear Non-Gaussian Structural Equation Models", "link": "http://arxiv.org/abs/2311.04585", "description": "The field of causal discovery develops model selection methods to infer\ncause-effect relations among a set of random variables. For this purpose,\ndifferent modelling assumptions have been proposed to render cause-effect\nrelations identifiable. One prominent assumption is that the joint distribution\nof the observed variables follows a linear non-Gaussian structural equation\nmodel. In this paper, we develop novel goodness-of-fit tests that assess the\nvalidity of this assumption in the basic setting without latent confounders as\nwell as in extension to linear models that incorporate latent confounders. Our\napproach involves testing algebraic relations among second and higher moments\nthat hold as a consequence of the linearity of the structural equations.\nSpecifically, we show that the linearity implies rank constraints on matrices\nand tensors derived from moments. For a practical implementation of our tests,\nwe consider a multiplier bootstrap method that uses incomplete U-statistics to\nestimate subdeterminants, as well as asymptotic approximations to the null\ndistribution of singular values. The methods are illustrated, in particular,\nfor the T\\\"ubingen collection of benchmark data sets on cause-effect pairs."}, "http://arxiv.org/abs/2311.04657": {"title": "Long-Term Causal Inference with Imperfect Surrogates using Many Weak Experiments, Proxies, and Cross-Fold Moments", "link": "http://arxiv.org/abs/2311.04657", "description": "Inferring causal effects on long-term outcomes using short-term surrogates is\ncrucial to rapid innovation. However, even when treatments are randomized and\nsurrogates fully mediate their effect on outcomes, it's possible that we get\nthe direction of causal effects wrong due to confounding between surrogates and\noutcomes -- a situation famously known as the surrogate paradox. The\navailability of many historical experiments offer the opportunity to instrument\nfor the surrogate and bypass this confounding. However, even as the number of\nexperiments grows, two-stage least squares has non-vanishing bias if each\nexperiment has a bounded size, and this bias is exacerbated when most\nexperiments barely move metrics, as occurs in practice. We show how to\neliminate this bias using cross-fold procedures, JIVE being one example, and\nconstruct valid confidence intervals for the long-term effect in new\nexperiments where long-term outcome has not yet been observed. Our methodology\nfurther allows to proxy for effects not perfectly mediated by the surrogates,\nallowing us to handle both confounding and effect leakage as violations of\nstandard statistical surrogacy conditions."}, "http://arxiv.org/abs/2311.04696": {"title": "Generative causality: using Shannon's information theory to infer underlying asymmetry in causal relations", "link": "http://arxiv.org/abs/2311.04696", "description": "Causal investigations in observational studies pose a great challenge in\nscientific research where randomized trials or intervention-based studies are\nnot feasible. Leveraging Shannon's seminal work on information theory, we\nconsider a framework of asymmetry where any causal link between putative cause\nand effect must be explained through a mechanism governing the cause as well as\na generative process yielding an effect of the cause. Under weak assumptions,\nthis framework enables the assessment of whether X is a stronger predictor of Y\nor vice-versa. Under stronger identifiability assumptions our framework is able\nto distinguish between cause and effect using observational data. We establish\nkey statistical properties of this framework. Our proposed methodology relies\non scalable non-parametric density estimation using fast Fourier\ntransformation. The resulting estimation method is manyfold faster than the\nclassical bandwidth-based density estimation while maintaining comparable mean\nintegrated squared error rates. We investigate key asymptotic properties of our\nmethodology and introduce a data-splitting technique to facilitate inference.\nThe key attraction of our framework is its inference toolkit, which allows\nresearchers to quantify uncertainty in causal discovery findings. We illustrate\nthe performance of our methodology through simulation studies as well as\nmultiple real data examples."}, "http://arxiv.org/abs/2311.04812": {"title": "Is it possible to obtain reliable estimates for the prevalence of anemia and childhood stunting among children under 5 in the poorest districts in Peru?", "link": "http://arxiv.org/abs/2311.04812", "description": "In this article we describe and apply the Fay-Herriot model with spatially\ncorrelated random area effects (Pratesi, M., &amp; Salvati, N. (2008)), in order to\npredict the prevalence of anemia and childhood stunting in Peruvian districts,\nbased on the data from the Demographic and Family Health Survey of the year\n2019, which collects data about anemia and childhood stunting for children\nunder the age of 12 years, and the National Census carried out in 2017. Our\nmain objective is to produce reliable predictions for the districts, where\nsample sizes are too small to provide good direct estimates, and for the\ndistricts, which were not included in the sample. The basic Fay-Herriot model\n(Fay &amp; Herriot, 1979) tackles this problem by incorporating auxiliary\ninformation, which is generally available from administrative or census\nrecords. The Fay-Herriot model with spatially correlated random area effects,\nin addition to auxiliary information, incorporates geographic information about\nthe areas, such as latitude and longitude. This permits modeling spatial\nautocorrelations, which are not unusual in socioeconomic and health surveys. To\nevaluate the mean square error of the above-mentioned predictors, we use the\nparametric bootstrap procedure, developed in Molina et al. (2009)."}, "http://arxiv.org/abs/2311.04855": {"title": "Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values", "link": "http://arxiv.org/abs/2311.04855", "description": "Non-negative matrix factorization (NMF) is a dimensionality reduction\ntechnique that has shown promise for analyzing noisy data, especially\nastronomical data. For these datasets, the observed data may contain negative\nvalues due to noise even when the true underlying physical signal is strictly\npositive. Prior NMF work has not treated negative data in a statistically\nconsistent manner, which becomes problematic for low signal-to-noise data with\nmany negative values. In this paper we present two algorithms, Shift-NMF and\nNearly-NMF, that can handle both the noisiness of the input data and also any\nintroduced negativity. Both of these algorithms use the negative data space\nwithout clipping, and correctly recover non-negative signals without any\nintroduced positive offset that occurs when clipping negative data. We\ndemonstrate this numerically on both simple and more realistic examples, and\nprove that both algorithms have monotonically decreasing update rules."}, "http://arxiv.org/abs/2311.04871": {"title": "Integration of Summary Information from External Studies for Semiparametric Models", "link": "http://arxiv.org/abs/2311.04871", "description": "With the development of biomedical science, researchers have increasing\naccess to an abundance of studies focusing on similar research questions. There\nis a growing interest in the integration of summary information from those\nstudies to enhance the efficiency of estimation in their own internal studies.\nIn this work, we present a comprehensive framework on integration of summary\ninformation from external studies when the data are modeled by semiparametric\nmodels. Our novel framework offers straightforward estimators that update\nconventional estimations with auxiliary information. It addresses computational\nchallenges by capitalizing on the intricate mathematical structure inherent to\nthe problem. We demonstrate the conditions when the proposed estimators are\ntheoretically more efficient than initial estimate based solely on internal\ndata. Several special cases such as proportional hazards model in survival\nanalysis are provided with numerical examples."}, "http://arxiv.org/abs/2107.10885": {"title": "Laplace and Saddlepoint Approximations in High Dimensions", "link": "http://arxiv.org/abs/2107.10885", "description": "We examine the behaviour of the Laplace and saddlepoint approximations in the\nhigh-dimensional setting, where the dimension of the model is allowed to\nincrease with the number of observations. Approximations to the joint density,\nthe marginal posterior density and the conditional density are considered. Our\nresults show that under the mildest assumptions on the model, the error of the\njoint density approximation is $O(p^4/n)$ if $p = o(n^{1/4})$ for the Laplace\napproximation and saddlepoint approximation, and $O(p^3/n)$ if $p = o(n^{1/3})$\nunder additional assumptions on the second derivative of the log-likelihood.\nStronger results are obtained for the approximation to the marginal posterior\ndensity."}, "http://arxiv.org/abs/2111.00280": {"title": "Testing semiparametric model-equivalence hypotheses based on the characteristic function", "link": "http://arxiv.org/abs/2111.00280", "description": "We propose three test criteria each of which is appropriate for testing,\nrespectively, the equivalence hypotheses of symmetry, of homogeneity, and of\nindependence, with multivariate data. All quantities have the common feature of\ninvolving weighted--type distances between characteristic functions and are\nconvenient from the computational point of view if the weight function is\nproperly chosen. The asymptotic behavior of the tests under the null hypothesis\nis investigated, and numerical studies are conducted in order to examine the\nperformance of the criteria in finite samples."}, "http://arxiv.org/abs/2112.11079": {"title": "Data fission: splitting a single data point", "link": "http://arxiv.org/abs/2112.11079", "description": "Suppose we observe a random vector $X$ from some distribution $P$ in a known\nfamily with unknown parameters. We ask the following question: when is it\npossible to split $X$ into two parts $f(X)$ and $g(X)$ such that neither part\nis sufficient to reconstruct $X$ by itself, but both together can recover $X$\nfully, and the joint distribution of $(f(X),g(X))$ is tractable? As one\nexample, if $X=(X_1,\\dots,X_n)$ and $P$ is a product distribution, then for any\n$m&lt;n$, we can split the sample to define $f(X)=(X_1,\\dots,X_m)$ and\n$g(X)=(X_{m+1},\\dots,X_n)$. Rasines and Young (2022) offers an alternative\nroute of accomplishing this task through randomization of $X$ with additive\nGaussian noise which enables post-selection inference in finite samples for\nGaussian distributed data and asymptotically for non-Gaussian additive models.\nIn this paper, we offer a more general methodology for achieving such a split\nin finite samples by borrowing ideas from Bayesian inference to yield a\n(frequentist) solution that can be viewed as a continuous analog of data\nsplitting. We call our method data fission, as an alternative to data\nsplitting, data carving and p-value masking. We exemplify the method on a few\nprototypical applications, such as post-selection inference for trend filtering\nand other regression problems."}, "http://arxiv.org/abs/2207.04598": {"title": "Differential item functioning via robust scaling", "link": "http://arxiv.org/abs/2207.04598", "description": "This paper proposes a method for assessing differential item functioning\n(DIF) in item response theory (IRT) models. The method does not require\npre-specification of anchor items, which is its main virtue. It is developed in\ntwo main steps, first by showing how DIF can be re-formulated as a problem of\noutlier detection in IRT-based scaling, then tackling the latter using methods\nfrom robust statistics. The proposal is a redescending M-estimator of IRT\nscaling parameters that is tuned to flag items with DIF at the desired\nasymptotic Type I Error rate. Theoretical results describe the efficiency of\nthe estimator in the absence of DIF and its robustness in the presence of DIF.\nSimulation studies show that the proposed method compares favorably to\ncurrently available approaches for DIF detection, and a real data example\nillustrates its application in a research context where pre-specification of\nanchor items is infeasible. The focus of the paper is the two-parameter\nlogistic model in two independent groups, with extensions to other settings\nconsidered in the conclusion."}, "http://arxiv.org/abs/2208.02028": {"title": "Bootstrap inference in the presence of bias", "link": "http://arxiv.org/abs/2208.02028", "description": "We consider bootstrap inference for estimators which are (asymptotically)\nbiased. We show that, even when the bias term cannot be consistently estimated,\nvalid inference can be obtained by proper implementations of the bootstrap.\nSpecifically, we show that the prepivoting approach of Beran (1987, 1988),\noriginally proposed to deliver higher-order refinements, restores bootstrap\nvalidity by transforming the original bootstrap p-value into an asymptotically\nuniform random variable. We propose two different implementations of\nprepivoting (plug-in and double bootstrap), and provide general high-level\nconditions that imply validity of bootstrap inference. To illustrate the\npractical relevance and implementation of our results, we discuss five\nexamples: (i) inference on a target parameter based on model averaging; (ii)\nridge-type regularized estimators; (iii) nonparametric regression; (iv) a\nlocation model for infinite variance data; and (v) dynamic panel data models."}, "http://arxiv.org/abs/2209.12345": {"title": "Berry-Esseen bounds for design-based causal inference with possibly diverging treatment levels and varying group sizes", "link": "http://arxiv.org/abs/2209.12345", "description": "Neyman (1923/1990) introduced the randomization model, which contains the\nnotation of potential outcomes to define causal effects and a framework for\nlarge-sample inference based on the design of the experiment. However, the\nexisting theory for this framework is far from complete especially when the\nnumber of treatment levels diverges and the treatment group sizes vary. We\nprovide a unified discussion of statistical inference under the randomization\nmodel with general treatment group sizes. We formulate the estimator in terms\nof a linear permutational statistic and use results based on Stein's method to\nderive various Berry--Esseen bounds on the linear and quadratic functions of\nthe estimator. These new Berry--Esseen bounds serve as basis for design-based\ncausal inference with possibly diverging treatment levels and a diverging\nnumber of causal parameters of interest. We also fill an important gap by\nproposing novel variance estimators for experiments with possibly many\ntreatment levels without replications. Equipped with the newly developed\nresults, design-based causal inference in general settings becomes more\nconvenient with stronger theoretical guarantees."}, "http://arxiv.org/abs/2210.02014": {"title": "Doubly Robust Proximal Synthetic Controls", "link": "http://arxiv.org/abs/2210.02014", "description": "To infer the treatment effect for a single treated unit using panel data,\nsynthetic control methods construct a linear combination of control units'\noutcomes that mimics the treated unit's pre-treatment outcome trajectory. This\nlinear combination is subsequently used to impute the counterfactual outcomes\nof the treated unit had it not been treated in the post-treatment period, and\nused to estimate the treatment effect. Existing synthetic control methods rely\non correctly modeling certain aspects of the counterfactual outcome generating\nmechanism and may require near-perfect matching of the pre-treatment\ntrajectory. Inspired by proximal causal inference, we obtain two novel\nnonparametric identifying formulas for the average treatment effect for the\ntreated unit: one is based on weighting, and the other combines models for the\ncounterfactual outcome and the weighting function. We introduce the concept of\ncovariate shift to synthetic controls to obtain these identification results\nconditional on the treatment assignment. We also develop two treatment effect\nestimators based on these two formulas and the generalized method of moments.\nOne new estimator is doubly robust: it is consistent and asymptotically normal\nif at least one of the outcome and weighting models is correctly specified. We\ndemonstrate the performance of the methods via simulations and apply them to\nevaluate the effectiveness of a Pneumococcal conjugate vaccine on the risk of\nall-cause pneumonia in Brazil."}, "http://arxiv.org/abs/2303.01385": {"title": "Hyperlink communities in higher-order networks", "link": "http://arxiv.org/abs/2303.01385", "description": "Many networks can be characterised by the presence of communities, which are\ngroups of units that are closely linked and can be relevant in understanding\nthe system's overall function. Recently, hypergraphs have emerged as a\nfundamental tool for modelling systems where interactions are not limited to\npairs but may involve an arbitrary number of nodes. Using a dual approach to\ncommunity detection, in this study we extend the concept of link communities to\nhypergraphs, allowing us to extract informative clusters of highly related\nhyperedges. We analyze the dendrograms obtained by applying hierarchical\nclustering to distance matrices among hyperedges on a variety of real-world\ndata, showing that hyperlink communities naturally highlight the hierarchical\nand multiscale structure of higher-order networks. Moreover, by using hyperlink\ncommunities, we are able to extract overlapping memberships from nodes,\novercoming limitations of traditional hard clustering methods. Finally, we\nintroduce higher-order network cartography as a practical tool for categorizing\nnodes into different structural roles based on their interaction patterns and\ncommunity participation. This approach helps identify different types of\nindividuals in a variety of real-world social systems. Our work contributes to\na better understanding of the structural organization of real-world\nhigher-order systems."}, "http://arxiv.org/abs/2305.05931": {"title": "Generalised shot noise representations of stochastic systems driven by non-Gaussian L\\'evy processes", "link": "http://arxiv.org/abs/2305.05931", "description": "We consider the problem of obtaining effective representations for the\nsolutions of linear, vector-valued stochastic differential equations (SDEs)\ndriven by non-Gaussian pure-jump L\\'evy processes, and we show how such\nrepresentations lead to efficient simulation methods. The processes considered\nconstitute a broad class of models that find application across the physical\nand biological sciences, mathematics, finance and engineering. Motivated by\nimportant relevant problems in statistical inference, we derive new,\ngeneralised shot-noise simulation methods whenever a normal variance-mean (NVM)\nmixture representation exists for the driving L\\'evy process, including the\ngeneralised hyperbolic, normal-Gamma, and normal tempered stable cases. Simple,\nexplicit conditions are identified for the convergence of the residual of a\ntruncated shot-noise representation to a Brownian motion in the case of the\npure L\\'evy process, and to a Brownian-driven SDE in the case of the\nL\\'evy-driven SDE. These results provide Gaussian approximations to the small\njumps of the process under the NVM representation. The resulting\nrepresentations are of particular importance in state inference and parameter\nestimation for L\\'evy-driven SDE models, since the resulting conditionally\nGaussian structures can be readily incorporated into latent variable inference\nmethods such as Markov chain Monte Carlo (MCMC), Expectation-Maximisation (EM),\nand sequential Monte Carlo."}, "http://arxiv.org/abs/2305.17517": {"title": "Stochastic Nonparametric Estimation of the Density-Flow Curve", "link": "http://arxiv.org/abs/2305.17517", "description": "The fundamental diagram serves as the foundation of traffic flow modeling for\nalmost a century. With the increasing availability of road sensor data,\ndeterministic parametric models have proved inadequate in describing the\nvariability of real-world data, especially in congested area of the\ndensity-flow diagram. In this paper we estimate the stochastic density-flow\nrelation introducing a nonparametric method called convex quantile regression.\nThe proposed method does not depend on any prior functional form assumptions,\nbut thanks to the concavity constraints, the estimated function satisfies the\ntheoretical properties of the density-flow curve. The second contribution is to\ndevelop the new convex quantile regression with bags (CQRb) approach to\nfacilitate practical implementation of CQR to the real-world data. We\nillustrate the CQRb estimation process using the road sensor data from Finland\nin years 2016-2018. Our third contribution is to demonstrate the excellent\nout-of-sample predictive power of the proposed CQRb method in comparison to the\nstandard parametric deterministic approach."}, "http://arxiv.org/abs/2305.19180": {"title": "Transfer Learning With Efficient Estimators to Optimally Leverage Historical Data in Analysis of Randomized Trials", "link": "http://arxiv.org/abs/2305.19180", "description": "Although randomized controlled trials (RCTs) are a cornerstone of comparative\neffectiveness, they typically have much smaller sample size than observational\nstudies because of financial and ethical considerations. Therefore there is\ninterest in using plentiful historical data (either observational data or prior\ntrials) to reduce trial sizes. Previous estimators developed for this purpose\nrely on unrealistic assumptions, without which the added data can bias the\ntreatment effect estimate. Recent work proposed an alternative method\n(prognostic covariate adjustment) that imposes no additional assumptions and\nincreases efficiency in trial analyses. The idea is to use historical data to\nlearn a prognostic model: a regression of the outcome onto the covariates. The\npredictions from this model, generated from the RCT subjects' baseline\nvariables, are then used as a covariate in a linear regression analysis of the\ntrial data. In this work, we extend prognostic adjustment to trial analyses\nwith nonparametric efficient estimators, which are more powerful than linear\nregression. We provide theory that explains why prognostic adjustment improves\nsmall-sample point estimation and inference without any possibility of bias.\nSimulations corroborate the theory: efficient estimators using prognostic\nadjustment compared to without provides greater power (i.e., smaller standard\nerrors) when the trial is small. Population shifts between historical and trial\ndata attenuate benefits but do not introduce bias. We showcase our estimator\nusing clinical trial data provided by Novo Nordisk A/S that evaluates insulin\ntherapy for individuals with type II diabetes."}, "http://arxiv.org/abs/2308.13928": {"title": "A flexible Bayesian tool for CoDa mixed models: logistic-normal distribution with Dirichlet covariance", "link": "http://arxiv.org/abs/2308.13928", "description": "Compositional Data Analysis (CoDa) has gained popularity in recent years.\nThis type of data consists of values from disjoint categories that sum up to a\nconstant. Both Dirichlet regression and logistic-normal regression have become\npopular as CoDa analysis methods. However, fitting this kind of multivariate\nmodels presents challenges, especially when structured random effects are\nincluded in the model, such as temporal or spatial effects.\n\nTo overcome these challenges, we propose the logistic-normal Dirichlet Model\n(LNDM). We seamlessly incorporate this approach into the R-INLA package,\nfacilitating model fitting and model prediction within the framework of Latent\nGaussian Models (LGMs). Moreover, we explore metrics like Deviance Information\nCriteria (DIC), Watanabe Akaike information criterion (WAIC), and\ncross-validation measure conditional predictive ordinate (CPO) for model\nselection in R-INLA for CoDa.\n\nIllustrating LNDM through a simple simulated example and with an ecological\ncase study on Arabidopsis thaliana in the Iberian Peninsula, we underscore its\npotential as an effective tool for managing CoDa and large CoDa databases."}, "http://arxiv.org/abs/2310.02968": {"title": "Sampling depth trade-off in function estimation under a two-level design", "link": "http://arxiv.org/abs/2310.02968", "description": "Many modern statistical applications involve a two-level sampling scheme that\nfirst samples subjects from a population and then samples observations on each\nsubject. These schemes often are designed to learn both the population-level\nfunctional structures shared by the subjects and the functional characteristics\nspecific to individual subjects. Common wisdom suggests that learning\npopulation-level structures benefits from sampling more subjects whereas\nlearning subject-specific structures benefits from deeper sampling within each\nsubject. Oftentimes these two objectives compete for limited sampling\nresources, which raises the question of how to optimally sample at the two\nlevels. We quantify such sampling-depth trade-offs by establishing the $L_2$\nminimax risk rates for learning the population-level and subject-specific\nstructures under a hierarchical Gaussian process model framework where we\nconsider a Bayesian and a frequentist perspective on the unknown\npopulation-level structure. These rates provide general lessons for designing\ntwo-level sampling schemes. Interestingly, subject-specific learning\noccasionally benefits more by sampling more subjects than by deeper\nwithin-subject sampling. We also construct estimators that adapt to unknown\nsmoothness and achieve the corresponding minimax rates. We conduct two\nsimulation experiments validating our theory and illustrating the sampling\ntrade-off in practice, and apply these estimators to two real datasets."}, "http://arxiv.org/abs/2311.05025": {"title": "Unbiased Kinetic Langevin Monte Carlo with Inexact Gradients", "link": "http://arxiv.org/abs/2311.05025", "description": "We present an unbiased method for Bayesian posterior means based on kinetic\nLangevin dynamics that combines advanced splitting methods with enhanced\ngradient approximations. Our approach avoids Metropolis correction by coupling\nMarkov chains at different discretization levels in a multilevel Monte Carlo\napproach. Theoretical analysis demonstrates that our proposed estimator is\nunbiased, attains finite variance, and satisfies a central limit theorem. It\ncan achieve accuracy $\\epsilon&gt;0$ for estimating expectations of Lipschitz\nfunctions in $d$ dimensions with $\\mathcal{O}(d^{1/4}\\epsilon^{-2})$ expected\ngradient evaluations, without assuming warm start. We exhibit similar bounds\nusing both approximate and stochastic gradients, and our method's computational\ncost is shown to scale logarithmically with the size of the dataset. The\nproposed method is tested using a multinomial regression problem on the MNIST\ndataset and a Poisson regression model for soccer scores. Experiments indicate\nthat the number of gradient evaluations per effective sample is independent of\ndimension, even when using inexact gradients. For product distributions, we\ngive dimension-independent variance bounds. Our results demonstrate that the\nunbiased algorithm we present can be much more efficient than the\n``gold-standard\" randomized Hamiltonian Monte Carlo."}, "http://arxiv.org/abs/2311.05056": {"title": "High-dimensional Newey-Powell Test Via Approximate Message Passing", "link": "http://arxiv.org/abs/2311.05056", "description": "Homoscedastic regression error is a common assumption in many\nhigh-dimensional regression models and theories. Although heteroscedastic error\ncommonly exists in real-world datasets, testing heteroscedasticity remains\nlargely underexplored under high-dimensional settings. We consider the\nheteroscedasticity test proposed in Newey and Powell (1987), whose asymptotic\ntheory has been well-established for the low-dimensional setting. We show that\nthe Newey-Powell test can be developed for high-dimensional data. For\nasymptotic theory, we consider the setting where the number of dimensions grows\nwith the sample size at a linear rate. The asymptotic analysis for the test\nstatistic utilizes the Approximate Message Passing (AMP) algorithm, from which\nwe obtain the limiting distribution of the test. The numerical performance of\nthe test is investigated through an extensive simulation study. As real-data\napplications, we present the analysis based on \"international economic growth\"\ndata (Belloni et al. 2011), which is found to be homoscedastic, and\n\"supermarket\" data (Lan et al., 2016), which is found to be heteroscedastic."}, "http://arxiv.org/abs/2311.05200": {"title": "An efficient Bayesian approach to joint functional principal component analysis for complex sampling designs", "link": "http://arxiv.org/abs/2311.05200", "description": "The analysis of multivariate functional curves has the potential to yield\nimportant scientific discoveries in domains such as healthcare, medicine,\neconomics and social sciences. However it is common for real-world settings to\npresent data that are both sparse and irregularly sampled, and this introduces\nimportant challenges for the current functional data methodology. Here we\npropose a Bayesian hierarchical framework for multivariate functional principal\ncomponent analysis which accommodates the intricacies of such sampling designs\nby flexibly pooling information across subjects and correlated curves. Our\nmodel represents common latent dynamics via shared functional principal\ncomponent scores, thereby effectively borrowing strength across curves while\ncircumventing the computationally challenging task of estimating covariance\nmatrices. These scores also provide a parsimonious representation of the major\nmodes of joint variation of the curves, and constitute interpretable scalar\nsummaries that can be employed in follow-up analyses. We perform inference\nusing a variational message passing algorithm which combines efficiency,\nmodularity and approximate posterior density estimation, enabling the joint\nanalysis of large datasets with parameter uncertainty quantification. We\nconduct detailed simulations to assess the effectiveness of our approach in\nsharing information under complex sampling designs. We also exploit it to\nestimate the molecular disease courses of individual patients with SARS-CoV-2\ninfection and characterise patient heterogeneity in recovery outcomes; this\nstudy reveals key coordinated dynamics across the immune, inflammatory and\nmetabolic systems, which are associated with survival and long-COVID symptoms\nup to one year post disease onset. Our approach is implemented in the R package\nbayesFPCA."}, "http://arxiv.org/abs/2311.05248": {"title": "A General Space of Belief Updates for Model Misspecification in Bayesian Networks", "link": "http://arxiv.org/abs/2311.05248", "description": "In an ideal setting for Bayesian agents, a perfect description of the rules\nof the environment (i.e., the objective observation model) is available,\nallowing them to reason through the Bayesian posterior to update their beliefs\nin an optimal way. But such an ideal setting hardly ever exists in the natural\nworld, so agents have to make do with reasoning about how they should update\ntheir beliefs simultaneously. This introduces a number of related challenges\nfor a number of research areas: (1) For Bayesian statistics, this deviation of\nthe subjective model from the true data-generating mechanism is termed model\nmisspecification in the literature. (2) For neuroscience, it introduces the\nnecessity to model how the agents' belief updates (how they use evidence to\nupdate their belief) and how their belief changes over time. The current paper\naddresses these two challenges by (a) providing a general class of\nposteriors/belief updates called cut-posteriors of Bayesian networks that have\na much greater expressivity, and (b) parameterizing the space of possible\nposteriors to make meta-learning (i.e., choosing the belief update from this\nspace in a principled manner) possible. For (a), it is noteworthy that any\ncut-posterior has local computation only, making computation tractable for\nhuman or artificial agents. For (b), a Markov Chain Monte Carlo algorithm to\nperform such meta-learning will be sketched here, though it is only an\nillustration and but no means the only possible meta-learning procedure\npossible for the space of cut-posteriors. Operationally, this work gives a\ngeneral algorithm to take in an arbitrary Bayesian network and output all\npossible cut-posteriors in the space."}, "http://arxiv.org/abs/2311.05272": {"title": "deform: An R Package for Nonstationary Spatial Gaussian Process Models by Deformations and Dimension Expansion", "link": "http://arxiv.org/abs/2311.05272", "description": "Gaussian processes (GP) are a popular and powerful tool for spatial modelling\nof data, especially data that quantify environmental processes. However, in\nstationary form, whether covariance is isotropic or anisotropic, GPs may lack\nthe flexibility to capture dependence across a continuous spatial process,\nespecially across a large domains. The deform package aims to provide users\nwith user-friendly R functions for the fitting and visualization of\nnonstationary spatial Gaussian processes. Users can choose to capture\nnonstationarity with either the spatial deformation approach of Sampson and\nGuttorp (1992) or the dimension expansion approach of Bornn, Shaddick, and\nZidek (2012). Thin plate regression splines are used for both approaches to\nbring transformations of locations to give a new set of locations that bring\nisotropic covariance. Fitted models in deform can be used to predict these new\nlocations and to simulate nonstationary Gaussian processes for an arbitrary set\nof locations."}, "http://arxiv.org/abs/2311.05330": {"title": "Applying a new category association estimator to sentiment analysis on the Web", "link": "http://arxiv.org/abs/2311.05330", "description": "This paper introduces a novel Bayesian method for measuring the degree of\nassociation between categorical variables. The method is grounded in the formal\ndefinition of variable independence and was implemented using MCMC techniques.\nUnlike existing methods, this approach does not assume prior knowledge of the\ntotal number of occurrences for any category, making it particularly\nwell-suited for applications like sentiment analysis. We applied the method to\na dataset comprising 4,613 tweets written in Portuguese, each annotated for 30\npossibly overlapping emotional categories. Through this analysis, we identified\npairs of emotions that exhibit associations and mutually exclusive pairs.\nFurthermore, the method identifies hierarchical relations between categories, a\nfeature observed in our data, and was used to cluster emotions into basic level\ngroups."}, "http://arxiv.org/abs/2311.05339": {"title": "An iterative algorithm for high-dimensional linear models with both sparse and non-sparse structures", "link": "http://arxiv.org/abs/2311.05339", "description": "Numerous practical medical problems often involve data that possess a\ncombination of both sparse and non-sparse structures. Traditional penalized\nregularizations techniques, primarily designed for promoting sparsity, are\ninadequate to capture the optimal solutions in such scenarios. To address these\nchallenges, this paper introduces a novel algorithm named Non-sparse Iteration\n(NSI). The NSI algorithm allows for the existence of both sparse and non-sparse\nstructures and estimates them simultaneously and accurately. We provide\ntheoretical guarantees that the proposed algorithm converges to the oracle\nsolution and achieves the optimal rate for the upper bound of the $l_2$-norm\nerror. Through simulations and practical applications, NSI consistently\nexhibits superior statistical performance in terms of estimation accuracy,\nprediction efficacy, and variable selection compared to several existing\nmethods. The proposed method is also applied to breast cancer data, revealing\nrepeated selection of specific genes for in-depth analysis."}, "http://arxiv.org/abs/2311.05421": {"title": "Diffusion Based Causal Representation Learning", "link": "http://arxiv.org/abs/2311.05421", "description": "Causal reasoning can be considered a cornerstone of intelligent systems.\nHaving access to an underlying causal graph comes with the promise of\ncause-effect estimation and the identification of efficient and safe\ninterventions. However, learning causal representations remains a major\nchallenge, due to the complexity of many real-world systems. Previous works on\ncausal representation learning have mostly focused on Variational Auto-Encoders\n(VAE). These methods only provide representations from a point estimate, and\nthey are unsuitable to handle high dimensions. To overcome these problems, we\nproposed a new Diffusion-based Causal Representation Learning (DCRL) algorithm.\nThis algorithm uses diffusion-based representations for causal discovery. DCRL\noffers access to infinite dimensional latent codes, which encode different\nlevels of information in the latent code. In a first proof of principle, we\ninvestigate the use of DCRL for causal representation learning. We further\ndemonstrate experimentally that this approach performs comparably well in\nidentifying the causal structure and causal variables."}, "http://arxiv.org/abs/2311.05532": {"title": "Uncertainty-Aware Bayes' Rule and Its Applications", "link": "http://arxiv.org/abs/2311.05532", "description": "Bayes' rule has enabled innumerable powerful algorithms of statistical signal\nprocessing and statistical machine learning. However, when there exist model\nmisspecifications in prior distributions and/or data distributions, the direct\napplication of Bayes' rule is questionable. Philosophically, the key is to\nbalance the relative importance of prior and data distributions when\ncalculating posterior distributions: if prior (resp. data) distributions are\noverly conservative, we should upweight the prior belief (resp. data evidence);\nif prior (resp. data) distributions are overly opportunistic, we should\ndownweight the prior belief (resp. data evidence). This paper derives a\ngeneralized Bayes' rule, called uncertainty-aware Bayes' rule, to technically\nrealize the above philosophy, i.e., to combat the model uncertainties in prior\ndistributions and/or data distributions. Simulated and real-world experiments\nshowcase the superiority of the presented uncertainty-aware Bayes' rule over\nthe conventional Bayes' rule: In particular, the uncertainty-aware Kalman\nfilter, the uncertainty-aware particle filter, and the uncertainty-aware\ninteractive multiple model filter are suggested and validated."}, "http://arxiv.org/abs/2107.13737": {"title": "Design-Robust Two-Way-Fixed-Effects Regression For Panel Data", "link": "http://arxiv.org/abs/2107.13737", "description": "We propose a new estimator for average causal effects of a binary treatment\nwith panel data in settings with general treatment patterns. Our approach\naugments the popular two-way-fixed-effects specification with unit-specific\nweights that arise from a model for the assignment mechanism. We show how to\nconstruct these weights in various settings, including the staggered adoption\nsetting, where units opt into the treatment sequentially but permanently. The\nresulting estimator converges to an average (over units and time) treatment\neffect under the correct specification of the assignment model, even if the\nfixed effect model is misspecified. We show that our estimator is more robust\nthan the conventional two-way estimator: it remains consistent if either the\nassignment mechanism or the two-way regression model is correctly specified. In\naddition, the proposed estimator performs better than the two-way-fixed-effect\nestimator if the outcome model and assignment mechanism are locally\nmisspecified. This strong double robustness property underlines and quantifies\nthe benefits of modeling the assignment process and motivates using our\nestimator in practice. We also discuss an extension of our estimator to handle\ndynamic treatment effects."}, "http://arxiv.org/abs/2110.00115": {"title": "Comparing Sequential Forecasters", "link": "http://arxiv.org/abs/2110.00115", "description": "Consider two forecasters, each making a single prediction for a sequence of\nevents over time. We ask a relatively basic question: how might we compare\nthese forecasters, either online or post-hoc, while avoiding unverifiable\nassumptions on how the forecasts and outcomes were generated? In this paper, we\npresent a rigorous answer to this question by designing novel sequential\ninference procedures for estimating the time-varying difference in forecast\nscores. To do this, we employ confidence sequences (CS), which are sequences of\nconfidence intervals that can be continuously monitored and are valid at\narbitrary data-dependent stopping times (\"anytime-valid\"). The widths of our\nCSs are adaptive to the underlying variance of the score differences.\nUnderlying their construction is a game-theoretic statistical framework, in\nwhich we further identify e-processes and p-processes for sequentially testing\na weak null hypothesis -- whether one forecaster outperforms another on average\n(rather than always). Our methods do not make distributional assumptions on the\nforecasts or outcomes; our main theorems apply to any bounded scores, and we\nlater provide alternative methods for unbounded scores. We empirically validate\nour approaches by comparing real-world baseball and weather forecasters."}, "http://arxiv.org/abs/2207.14753": {"title": "Estimating Causal Effects with Hidden Confounding using Instrumental Variables and Environments", "link": "http://arxiv.org/abs/2207.14753", "description": "Recent works have proposed regression models which are invariant across data\ncollection environments. These estimators often have a causal interpretation\nunder conditions on the environments and type of invariance imposed. One recent\nexample, the Causal Dantzig (CD), is consistent under hidden confounding and\nrepresents an alternative to classical instrumental variable estimators such as\nTwo Stage Least Squares (TSLS). In this work we derive the CD as a generalized\nmethod of moments (GMM) estimator. The GMM representation leads to several\npractical results, including 1) creation of the Generalized Causal Dantzig\n(GCD) estimator which can be applied to problems with continuous environments\nwhere the CD cannot be fit 2) a Hybrid (GCD-TSLS combination) estimator which\nhas properties superior to GCD or TSLS alone 3) straightforward asymptotic\nresults for all methods using GMM theory. We compare the CD, GCD, TSLS, and\nHybrid estimators in simulations and an application to a Flow Cytometry data\nset. The newly proposed GCD and Hybrid estimators have superior performance to\nexisting methods in many settings."}, "http://arxiv.org/abs/2208.02948": {"title": "A Feature Selection Method that Controls the False Discovery Rate", "link": "http://arxiv.org/abs/2208.02948", "description": "The problem of selecting a handful of truly relevant variables in supervised\nmachine learning algorithms is a challenging problem in terms of untestable\nassumptions that must hold and unavailability of theoretical assurances that\nselection errors are under control. We propose a distribution-free feature\nselection method, referred to as Data Splitting Selection (DSS) which controls\nFalse Discovery Rate (FDR) of feature selection while obtaining a high power.\nAnother version of DSS is proposed with a higher power which \"almost\" controls\nFDR. No assumptions are made on the distribution of the response or on the\njoint distribution of the features. Extensive simulation is performed to\ncompare the performance of the proposed methods with the existing ones."}, "http://arxiv.org/abs/2209.03474": {"title": "An extension of the Unified Skew-Normal family of distributions and application to Bayesian binary regression", "link": "http://arxiv.org/abs/2209.03474", "description": "We consider the general problem of Bayesian binary regression and we\nintroduce a new class of distributions, the Perturbed Unified Skew Normal\n(pSUN, henceforth), which generalizes the Unified Skew-Normal (SUN) class. We\nshow that the new class is conjugate to any binary regression model, provided\nthat the link function may be expressed as a scale mixture of Gaussian\ndensities. We discuss in detail the popular logit case, and we show that, when\na logistic regression model is combined with a Gaussian prior, posterior\nsummaries such as cumulants and normalizing constants can be easily obtained\nthrough the use of an importance sampling approach, opening the way to\nstraightforward variable selection procedures. For more general priors, the\nproposed methodology is based on a simple Gibbs sampler algorithm. We also\nclaim that, in the p &gt; n case, the proposed methodology shows better\nperformances - both in terms of mixing and accuracy - compared to the existing\nmethods. We illustrate the performance through several simulation studies and\ntwo data analyses."}, "http://arxiv.org/abs/2211.15070": {"title": "Online Kernel CUSUM for Change-Point Detection", "link": "http://arxiv.org/abs/2211.15070", "description": "We present a computationally efficient online kernel Cumulative Sum (CUSUM)\nmethod for change-point detection that utilizes the maximum over a set of\nkernel statistics to account for the unknown change-point location. Our\napproach exhibits increased sensitivity to small changes compared to existing\nkernel-based change-point detection methods, including Scan-B statistic,\ncorresponding to a non-parametric Shewhart chart-type procedure. We provide\naccurate analytic approximations for two key performance metrics: the Average\nRun Length (ARL) and Expected Detection Delay (EDD), which enable us to\nestablish an optimal window length to be on the order of the logarithm of ARL\nto ensure minimal power loss relative to an oracle procedure with infinite\nmemory. Moreover, we introduce a recursive calculation procedure for detection\nstatistics to ensure constant computational and memory complexity, which is\nessential for online implementation. Through extensive experiments on both\nsimulated and real data, we demonstrate the competitive performance of our\nmethod and validate our theoretical results."}, "http://arxiv.org/abs/2301.09633": {"title": "Prediction-Powered Inference", "link": "http://arxiv.org/abs/2301.09633", "description": "Prediction-powered inference is a framework for performing valid statistical\ninference when an experimental dataset is supplemented with predictions from a\nmachine-learning system. The framework yields simple algorithms for computing\nprovably valid confidence intervals for quantities such as means, quantiles,\nand linear and logistic regression coefficients, without making any assumptions\non the machine-learning algorithm that supplies the predictions. Furthermore,\nmore accurate predictions translate to smaller confidence intervals.\nPrediction-powered inference could enable researchers to draw valid and more\ndata-efficient conclusions using machine learning. The benefits of\nprediction-powered inference are demonstrated with datasets from proteomics,\nastronomy, genomics, remote sensing, census analysis, and ecology."}, "http://arxiv.org/abs/2303.03521": {"title": "Bayesian Adaptive Selection of Variables for Function-on-Scalar Regression Models", "link": "http://arxiv.org/abs/2303.03521", "description": "Considering the field of functional data analysis, we developed a new\nBayesian method for variable selection in function-on-scalar regression (FOSR).\nOur approach uses latent variables, allowing an adaptive selection since it can\ndetermine the number of variables and which ones should be selected for a\nfunction-on-scalar regression model. Simulation studies show the proposed\nmethod's main properties, such as its accuracy in estimating the coefficients\nand high capacity to select variables correctly. Furthermore, we conducted\ncomparative studies with the main competing methods, such as the BGLSS method\nas well as the group LASSO, the group MCP and the group SCAD. We also used a\nCOVID-19 dataset and some socioeconomic data from Brazil for real data\napplication. In short, the proposed Bayesian variable selection model is\nextremely competitive, showing significant predictive and selective quality."}, "http://arxiv.org/abs/2305.10564": {"title": "Counterfactually Comparing Abstaining Classifiers", "link": "http://arxiv.org/abs/2305.10564", "description": "Abstaining classifiers have the option to abstain from making predictions on\ninputs that they are unsure about. These classifiers are becoming increasingly\npopular in high-stakes decision-making problems, as they can withhold uncertain\npredictions to improve their reliability and safety. When evaluating black-box\nabstaining classifier(s), however, we lack a principled approach that accounts\nfor what the classifier would have predicted on its abstentions. These missing\npredictions matter when they can eventually be utilized, either directly or as\na backup option in a failure mode. In this paper, we introduce a novel approach\nand perspective to the problem of evaluating and comparing abstaining\nclassifiers by treating abstentions as missing data. Our evaluation approach is\ncentered around defining the counterfactual score of an abstaining classifier,\ndefined as the expected performance of the classifier had it not been allowed\nto abstain. We specify the conditions under which the counterfactual score is\nidentifiable: if the abstentions are stochastic, and if the evaluation data is\nindependent of the training data (ensuring that the predictions are missing at\nrandom), then the score is identifiable. Note that, if abstentions are\ndeterministic, then the score is unidentifiable because the classifier can\nperform arbitrarily poorly on its abstentions. Leveraging tools from\nobservational causal inference, we then develop nonparametric and doubly robust\nmethods to efficiently estimate this quantity under identification. Our\napproach is examined in both simulated and real data experiments."}, "http://arxiv.org/abs/2307.01539": {"title": "Implementing measurement error models with mechanistic mathematical models in a likelihood-based framework for estimation, identifiability analysis, and prediction in the life sciences", "link": "http://arxiv.org/abs/2307.01539", "description": "Throughout the life sciences we routinely seek to interpret measurements and\nobservations using parameterised mechanistic mathematical models. A fundamental\nand often overlooked choice in this approach involves relating the solution of\na mathematical model with noisy and incomplete measurement data. This is often\nachieved by assuming that the data are noisy measurements of the solution of a\ndeterministic mathematical model, and that measurement errors are additive and\nnormally distributed. While this assumption of additive Gaussian noise is\nextremely common and simple to implement and interpret, it is often unjustified\nand can lead to poor parameter estimates and non-physical predictions. One way\nto overcome this challenge is to implement a different measurement error model.\nIn this review, we demonstrate how to implement a range of measurement error\nmodels in a likelihood-based framework for estimation, identifiability\nanalysis, and prediction, called Profile-Wise Analysis. This frequentist\napproach to uncertainty quantification for mechanistic models leverages the\nprofile likelihood for targeting parameters and understanding their influence\non predictions. Case studies, motivated by simple caricature models routinely\nused in systems biology and mathematical biology literature, illustrate how the\nsame ideas apply to different types of mathematical models. Open-source Julia\ncode to reproduce results is available on GitHub."}, "http://arxiv.org/abs/2307.04548": {"title": "Beyond the Two-Trials Rule", "link": "http://arxiv.org/abs/2307.04548", "description": "The two-trials rule for drug approval requires \"at least two adequate and\nwell-controlled studies, each convincing on its own, to establish\neffectiveness\". This is usually implemented by requiring two significant\npivotal trials and is the standard regulatory requirement to provide evidence\nfor a new drug's efficacy. However, there is need to develop suitable\nalternatives to this rule for a number of reasons, including the possible\navailability of data from more than two trials. I consider the case of up to 3\nstudies and stress the importance to control the partial Type-I error rate,\nwhere only some studies have a true null effect, while maintaining the overall\nType-I error rate of the two-trials rule, where all studies have a null effect.\nSome less-known $p$-value combination methods are useful to achieve this:\nPearson's method, Edgington's method and the recently proposed harmonic mean\n$\\chi^2$-test. I study their properties and discuss how they can be extended to\na sequential assessment of success while still ensuring overall Type-I error\ncontrol. I compare the different methods in terms of partial Type-I error rate,\nproject power and the expected number of studies required. Edgington's method\nis eventually recommended as it is easy to implement and communicate, has only\nmoderate partial Type-I error rate inflation but substantially increased\nproject power."}, "http://arxiv.org/abs/2307.06250": {"title": "Identifiability Guarantees for Causal Disentanglement from Soft Interventions", "link": "http://arxiv.org/abs/2307.06250", "description": "Causal disentanglement aims to uncover a representation of data using latent\nvariables that are interrelated through a causal model. Such a representation\nis identifiable if the latent model that explains the data is unique. In this\npaper, we focus on the scenario where unpaired observational and interventional\ndata are available, with each intervention changing the mechanism of a latent\nvariable. When the causal variables are fully observed, statistically\nconsistent algorithms have been developed to identify the causal model under\nfaithfulness assumptions. We here show that identifiability can still be\nachieved with unobserved causal variables, given a generalized notion of\nfaithfulness. Our results guarantee that we can recover the latent causal model\nup to an equivalence class and predict the effect of unseen combinations of\ninterventions, in the limit of infinite data. We implement our causal\ndisentanglement framework by developing an autoencoding variational Bayes\nalgorithm and apply it to the problem of predicting combinatorial perturbation\neffects in genomics."}, "http://arxiv.org/abs/2311.05794": {"title": "An Experimental Design for Anytime-Valid Causal Inference on Multi-Armed Bandits", "link": "http://arxiv.org/abs/2311.05794", "description": "Typically, multi-armed bandit (MAB) experiments are analyzed at the end of\nthe study and thus require the analyst to specify a fixed sample size in\nadvance. However, in many online learning applications, it is advantageous to\ncontinuously produce inference on the average treatment effect (ATE) between\narms as new data arrive and determine a data-driven stopping time for the\nexperiment. Existing work on continuous inference for adaptive experiments\nassumes that the treatment assignment probabilities are bounded away from zero\nand one, thus excluding nearly all standard bandit algorithms. In this work, we\ndevelop the Mixture Adaptive Design (MAD), a new experimental design for\nmulti-armed bandits that enables continuous inference on the ATE with\nguarantees on statistical validity and power for nearly any bandit algorithm.\nOn a high level, the MAD \"mixes\" a bandit algorithm of the user's choice with a\nBernoulli design through a tuning parameter $\\delta_t$, where $\\delta_t$ is a\ndeterministic sequence that controls the priority placed on the Bernoulli\ndesign as the sample size grows. We show that for $\\delta_t =\no\\left(1/t^{1/4}\\right)$, the MAD produces a confidence sequence that is\nasymptotically valid and guaranteed to shrink around the true ATE. We\nempirically show that the MAD improves the coverage and power of ATE inference\nin MAB experiments without significant losses in finite-sample reward."}, "http://arxiv.org/abs/2311.05806": {"title": "Likelihood ratio tests in random graph models with increasing dimensions", "link": "http://arxiv.org/abs/2311.05806", "description": "We explore the Wilks phenomena in two random graph models: the $\\beta$-model\nand the Bradley-Terry model. For two increasing dimensional null hypotheses,\nincluding a specified null $H_0: \\beta_i=\\beta_i^0$ for $i=1,\\ldots, r$ and a\nhomogenous null $H_0: \\beta_1=\\cdots=\\beta_r$, we reveal high dimensional\nWilks' phenomena that the normalized log-likelihood ratio statistic,\n$[2\\{\\ell(\\widehat{\\mathbf{\\beta}}) - \\ell(\\widehat{\\mathbf{\\beta}}^0)\\}\n-r]/(2r)^{1/2}$, converges in distribution to the standard normal distribution\nas $r$ goes to infinity. Here, $\\ell( \\mathbf{\\beta})$ is the log-likelihood\nfunction on the model parameter $\\mathbf{\\beta}=(\\beta_1, \\ldots,\n\\beta_n)^\\top$, $\\widehat{\\mathbf{\\beta}}$ is its maximum likelihood estimator\n(MLE) under the full parameter space, and $\\widehat{\\mathbf{\\beta}}^0$ is the\nrestricted MLE under the null parameter space. For the homogenous null with a\nfixed $r$, we establish Wilks-type theorems that\n$2\\{\\ell(\\widehat{\\mathbf{\\beta}}) - \\ell(\\widehat{\\mathbf{\\beta}}^0)\\}$\nconverges in distribution to a chi-square distribution with $r-1$ degrees of\nfreedom, as the total number of parameters, $n$, goes to infinity. When testing\nthe fixed dimensional specified null, we find that its asymptotic null\ndistribution is a chi-square distribution in the $\\beta$-model. However,\nunexpectedly, this is not true in the Bradley-Terry model. By developing\nseveral novel technical methods for asymptotic expansion, we explore Wilks type\nresults in a principled manner; these principled methods should be applicable\nto a class of random graph models beyond the $\\beta$-model and the\nBradley-Terry model. Simulation studies and real network data applications\nfurther demonstrate the theoretical results."}, "http://arxiv.org/abs/2311.05819": {"title": "A flexible framework for synthesizing human activity patterns with application to sequential categorical data", "link": "http://arxiv.org/abs/2311.05819", "description": "The ability to synthesize realistic data in a parametrizable way is valuable\nfor a number of reasons, including privacy, missing data imputation, and\nevaluating the performance of statistical and computational methods. When the\nunderlying data generating process is complex, data synthesis requires\napproaches that balance realism and simplicity. In this paper, we address the\nproblem of synthesizing sequential categorical data of the type that is\nincreasingly available from mobile applications and sensors that record\nparticipant status continuously over the course of multiple days and weeks. We\npropose the paired Markov Chain (paired-MC) method, a flexible framework that\nproduces sequences that closely mimic real data while providing a\nstraightforward mechanism for modifying characteristics of the synthesized\nsequences. We demonstrate the paired-MC method on two datasets, one reflecting\ndaily human activity patterns collected via a smartphone application, and one\nencoding the intensities of physical activity measured by wearable\naccelerometers. In both settings, sequences synthesized by paired-MC better\ncapture key characteristics of the real data than alternative approaches."}, "http://arxiv.org/abs/2311.05847": {"title": "Threshold distribution of equal states for quantitative amplitude fluctuations", "link": "http://arxiv.org/abs/2311.05847", "description": "Objective. The distribution of equal states (DES) quantifies amplitude\nfluctuations in biomedical signals. However, under certain conditions, such as\na high resolution of data collection or special signal processing techniques,\nequal states may be very rare, whereupon the DES fails to measure the amplitude\nfluctuations. Approach. To address this problem, we develop a novel threshold\nDES (tDES) that measures the distribution of differential states within a\nthreshold. To evaluate the proposed tDES, we first analyze five sets of\nsynthetic signals generated in different frequency bands. We then analyze sleep\nelectroencephalography (EEG) datasets taken from the public PhysioNet. Main\nresults. Synthetic signals and detrend-filtered sleep EEGs have no neighboring\nequal values; however, tDES can effectively measure the amplitude fluctuations\nwithin these data. The tDES of EEG data increases significantly as the sleep\nstage increases, even with datasets covering very short periods, indicating\ndecreased amplitude fluctuations in sleep EEGs. Generally speaking, the\npresence of more low-frequency components in a physiological series reflects\nsmaller amplitude fluctuations and larger DES. Significance.The tDES provides a\nreliable computing method for quantifying amplitude fluctuations, exhibiting\nthe characteristics of conceptual simplicity and computational robustness. Our\nfindings broaden the application of quantitative amplitude fluctuations and\ncontribute to the classification of sleep stages based on EEG data."}, "http://arxiv.org/abs/2311.05914": {"title": "Efficient Case-Cohort Design using Balanced Sampling", "link": "http://arxiv.org/abs/2311.05914", "description": "A case-cohort design is a two-phase sampling design frequently used to\nanalyze censored survival data in a cost-effective way, where a subcohort is\nusually selected using simple random sampling or stratified simple random\nsampling. In this paper, we propose an efficient sampling procedure based on\nbalanced sampling when selecting a subcohort in a case-cohort design. A sample\nselected via a balanced sampling procedure automatically calibrates auxiliary\nvariables. When fitting a Cox model, calibrating sampling weights has been\nshown to lead to more efficient estimators of the regression coefficients\n(Breslow et al., 2009a, b). The reduced variabilities over its counterpart with\na simple random sampling are shown via extensive simulation experiments. The\nproposed design and estimation procedure are also illustrated with the\nwell-known National Wilms Tumor Study dataset."}, "http://arxiv.org/abs/2311.06076": {"title": "Bayesian Tensor Factorisations for Time Series of Counts", "link": "http://arxiv.org/abs/2311.06076", "description": "We propose a flexible nonparametric Bayesian modelling framework for\nmultivariate time series of count data based on tensor factorisations. Our\nmodels can be viewed as infinite state space Markov chains of known maximal\norder with non-linear serial dependence through the introduction of appropriate\nlatent variables. Alternatively, our models can be viewed as Bayesian\nhierarchical models with conditionally independent Poisson distributed\nobservations. Inference about the important lags and their complex interactions\nis achieved via MCMC. When the observed counts are large, we deal with the\nresulting computational complexity of Bayesian inference via a two-step\ninferential strategy based on an initial analysis of a training set of the\ndata. Our methodology is illustrated using simulation experiments and analysis\nof real-world data."}, "http://arxiv.org/abs/2311.06086": {"title": "A three-step approach to production frontier estimation and the Matsuoka's distribution", "link": "http://arxiv.org/abs/2311.06086", "description": "In this work, we introduce a three-step semiparametric methodology for the\nestimation of production frontiers. We consider a model inspired by the\nwell-known Cobb-Douglas production function, wherein input factors operate\nmultiplicatively within the model. Efficiency in the proposed model is assumed\nto follow a continuous univariate uniparametric distribution in $(0,1)$,\nreferred to as Matsuoka's distribution, which is introduced and explored.\nFollowing model linearization, the first step of the procedure is to\nsemiparametrically estimate the regression function through a local linear\nsmoother. The second step focuses on the estimation of the efficiency parameter\nin which the properties of the Matsuoka's distribution are employed. Finally,\nwe estimate the production frontier through a plug-in methodology. We present a\nrigorous asymptotic theory related to the proposed three-step estimation,\nincluding consistency, and asymptotic normality, and derive rates for the\nconvergences presented. Incidentally, we also introduce and study the\nMatsuoka's distribution, deriving its main properties, including quantiles,\nmoments, $\\alpha$-expectiles, entropies, and stress-strength reliability, among\nothers. The Matsuoka's distribution exhibits a versatile array of shapes\ncapable of effectively encapsulating the typical behavior of efficiency within\nproduction frontier models. To complement the large sample results obtained, a\nMonte Carlo simulation study is conducted to assess the finite sample\nperformance of the proposed three-step methodology. An empirical application\nusing a dataset of Danish milk producers is also presented."}, "http://arxiv.org/abs/2311.06220": {"title": "Bayesian nonparametric generative modeling of large multivariate non-Gaussian spatial fields", "link": "http://arxiv.org/abs/2311.06220", "description": "Multivariate spatial fields are of interest in many applications, including\nclimate model emulation. Not only can the marginal spatial fields be subject to\nnonstationarity, but the dependence structure among the marginal fields and\nbetween the fields might also differ substantially. Extending a recently\nproposed Bayesian approach to describe the distribution of a nonstationary\nunivariate spatial field using a triangular transport map, we cast the\ninference problem for a multivariate spatial field for a small number of\nreplicates into a series of independent Gaussian process (GP) regression tasks\nwith Gaussian errors. Due to the potential nonlinearity in the conditional\nmeans, the joint distribution modeled can be non-Gaussian. The resulting\nnonparametric Bayesian methodology scales well to high-dimensional spatial\nfields. It is especially useful when only a few training samples are available,\nbecause it employs regularization priors and quantifies uncertainty. Inference\nis conducted in an empirical Bayes setting by a highly scalable stochastic\ngradient approach. The implementation benefits from mini-batching and could be\naccelerated with parallel computing. We illustrate the extended transport-map\nmodel by studying hydrological variables from non-Gaussian climate-model\noutput."}, "http://arxiv.org/abs/2207.12705": {"title": "Efficient shape-constrained inference for the autocovariance sequence from a reversible Markov chain", "link": "http://arxiv.org/abs/2207.12705", "description": "In this paper, we study the problem of estimating the autocovariance sequence\nresulting from a reversible Markov chain. A motivating application for studying\nthis problem is the estimation of the asymptotic variance in central limit\ntheorems for Markov chains. We propose a novel shape-constrained estimator of\nthe autocovariance sequence, which is based on the key observation that the\nrepresentability of the autocovariance sequence as a moment sequence imposes\ncertain shape constraints. We examine the theoretical properties of the\nproposed estimator and provide strong consistency guarantees for our estimator.\nIn particular, for geometrically ergodic reversible Markov chains, we show that\nour estimator is strongly consistent for the true autocovariance sequence with\nrespect to an $\\ell_2$ distance, and that our estimator leads to strongly\nconsistent estimates of the asymptotic variance. Finally, we perform empirical\nstudies to illustrate the theoretical properties of the proposed estimator as\nwell as to demonstrate the effectiveness of our estimator in comparison with\nother current state-of-the-art methods for Markov chain Monte Carlo variance\nestimation, including batch means, spectral variance estimators, and the\ninitial convex sequence estimator."}, "http://arxiv.org/abs/2209.04321": {"title": "Estimating Racial Disparities in Emergency General Surgery", "link": "http://arxiv.org/abs/2209.04321", "description": "Research documents that Black patients experience worse general surgery\noutcomes than white patients in the United States. In this paper, we focus on\nan important but less-examined category: the surgical treatment of emergency\ngeneral surgery (EGS) conditions, which refers to medical emergencies where the\ninjury is \"endogenous,\" such as a burst appendix. Our goal is to assess racial\ndisparities for common outcomes after EGS treatment using an administrative\ndatabase of hospital claims in New York, Florida, and Pennsylvania, and to\nunderstand the extent to which differences are attributable to patient-level\nrisk factors versus hospital-level factors. To do so, we use a class of linear\nweighting estimators that re-weight white patients to have a similar\ndistribution of baseline characteristics as Black patients. This framework\nnests many common approaches, including matching and linear regression, but\noffers important advantages over these methods in terms of controlling\nimbalance between groups, minimizing extrapolation, and reducing computation\ntime. Applying this approach to the claims data, we find that disparities\nestimates that adjust for the admitting hospital are substantially smaller than\nestimates that adjust for patient baseline characteristics only, suggesting\nthat hospital-specific factors are important drivers of racial disparities in\nEGS outcomes."}, "http://arxiv.org/abs/2210.12759": {"title": "Robust angle-based transfer learning in high dimensions", "link": "http://arxiv.org/abs/2210.12759", "description": "Transfer learning aims to improve the performance of a target model by\nleveraging data from related source populations, which is known to be\nespecially helpful in cases with insufficient target data. In this paper, we\nstudy the problem of how to train a high-dimensional ridge regression model\nusing limited target data and existing regression models trained in\nheterogeneous source populations. We consider a practical setting where only\nthe parameter estimates of the fitted source models are accessible, instead of\nthe individual-level source data. Under the setting with only one source model,\nwe propose a novel flexible angle-based transfer learning (angleTL) method,\nwhich leverages the concordance between the source and the target model\nparameters. We show that angleTL unifies several benchmark methods by\nconstruction, including the target-only model trained using target data alone,\nthe source model fitted on source data, and distance-based transfer learning\nmethod that incorporates the source parameter estimates and the target data\nunder a distance-based similarity constraint. We also provide algorithms to\neffectively incorporate multiple source models accounting for the fact that\nsome source models may be more helpful than others. Our high-dimensional\nasymptotic analysis provides interpretations and insights regarding when a\nsource model can be helpful to the target model, and demonstrates the\nsuperiority of angleTL over other benchmark methods. We perform extensive\nsimulation studies to validate our theoretical conclusions and show the\nfeasibility of applying angleTL to transfer existing genetic risk prediction\nmodels across multiple biobanks."}, "http://arxiv.org/abs/2211.04370": {"title": "NESTER: An Adaptive Neurosymbolic Method for Causal Effect Estimation", "link": "http://arxiv.org/abs/2211.04370", "description": "Causal effect estimation from observational data is a central problem in\ncausal inference. Methods based on potential outcomes framework solve this\nproblem by exploiting inductive biases and heuristics from causal inference.\nEach of these methods addresses a specific aspect of causal effect estimation,\nsuch as controlling propensity score, enforcing randomization, etc., by\ndesigning neural network (NN) architectures and regularizers. In this paper, we\npropose an adaptive method called Neurosymbolic Causal Effect Estimator\n(NESTER), a generalized method for causal effect estimation. NESTER integrates\nthe ideas used in existing methods based on multi-head NNs for causal effect\nestimation into one framework. We design a Domain Specific Language (DSL)\ntailored for causal effect estimation based on causal inductive biases used in\nliterature. We conduct a theoretical analysis to investigate NESTER's efficacy\nin estimating causal effects. Our comprehensive empirical results show that\nNESTER performs better than state-of-the-art methods on benchmark datasets."}, "http://arxiv.org/abs/2211.13374": {"title": "A Multivariate Non-Gaussian Bayesian Filter Using Power Moments", "link": "http://arxiv.org/abs/2211.13374", "description": "In this paper, we extend our results on the univariate non-Gaussian Bayesian\nfilter using power moments to the multivariate systems, which can be either\nlinear or nonlinear. Doing this introduces several challenging problems, for\nexample a positive parametrization of the density surrogate, which is not only\na problem of filter design, but also one of the multiple dimensional Hamburger\nmoment problem. We propose a parametrization of the density surrogate with the\nproofs to its existence, Positivstellensatz and uniqueness. Based on it, we\nanalyze the errors of moments of the density estimates by the proposed density\nsurrogate. A discussion on continuous and discrete treatments to the\nnon-Gaussian Bayesian filtering problem is proposed to motivate the research on\ncontinuous parametrization of the system state. Simulation results on\nestimating different types of multivariate density functions are given to\nvalidate our proposed filter. To the best of our knowledge, the proposed filter\nis the first one implementing the multivariate Bayesian filter with the system\nstate parameterized as a continuous function, which only requires the true\nstates being Lebesgue integrable."}, "http://arxiv.org/abs/2303.02637": {"title": "A Semi-Bayesian Nonparametric Estimator of the Maximum Mean Discrepancy Measure: Applications in Goodness-of-Fit Testing and Generative Adversarial Networks", "link": "http://arxiv.org/abs/2303.02637", "description": "A classic inferential statistical problem is the goodness-of-fit (GOF) test.\nSuch a test can be challenging when the hypothesized parametric model has an\nintractable likelihood and its distributional form is not available. Bayesian\nmethods for GOF can be appealing due to their ability to incorporate expert\nknowledge through prior distributions.\n\nHowever, standard Bayesian methods for this test often require strong\ndistributional assumptions on the data and their relevant parameters. To\naddress this issue, we propose a semi-Bayesian nonparametric (semi-BNP)\nprocedure in the context of the maximum mean discrepancy (MMD) measure that can\nbe applied to the GOF test. Our method introduces a novel Bayesian estimator\nfor the MMD, enabling the development of a measure-based hypothesis test for\nintractable models. Through extensive experiments, we demonstrate that our\nproposed test outperforms frequentist MMD-based methods by achieving a lower\nfalse rejection and acceptance rate of the null hypothesis. Furthermore, we\nshowcase the versatility of our approach by embedding the proposed estimator\nwithin a generative adversarial network (GAN) framework. It facilitates a\nrobust BNP learning approach as another significant application of our method.\nWith our BNP procedure, this new GAN approach can enhance sample diversity and\nimprove inferential accuracy compared to traditional techniques."}, "http://arxiv.org/abs/2304.07113": {"title": "Causal inference with a functional outcome", "link": "http://arxiv.org/abs/2304.07113", "description": "This paper presents methods to study the causal effect of a binary treatment\non a functional outcome with observational data. We define a Functional Average\nTreatment Effect and develop an outcome regression estimator. We show how to\nobtain valid inference on the FATE using simultaneous confidence bands, which\ncover the FATE with a given probability over the entire domain. Simulation\nexperiments illustrate how the simultaneous confidence bands take the multiple\ncomparison problem into account. Finally, we use the methods to infer the\neffect of early adult location on subsequent income development for one Swedish\nbirth cohort."}, "http://arxiv.org/abs/2310.00233": {"title": "CausalImages: An R Package for Causal Inference with Earth Observation, Bio-medical, and Social Science Images", "link": "http://arxiv.org/abs/2310.00233", "description": "The causalimages R package enables causal inference with image and image\nsequence data, providing new tools for integrating novel data sources like\nsatellite and bio-medical imagery into the study of cause and effect. One set\nof functions enables image-based causal inference analyses. For example, one\nkey function decomposes treatment effect heterogeneity by images using an\ninterpretable Bayesian framework. This allows for determining which types of\nimages or image sequences are most responsive to interventions. A second\nmodeling function allows researchers to control for confounding using images.\nThe package also allows investigators to produce embeddings that serve as\nvector summaries of the image or video content. Finally, infrastructural\nfunctions are also provided, such as tools for writing large-scale image and\nimage sequence data as sequentialized byte strings for more rapid image\nanalysis. causalimages therefore opens new capabilities for causal inference in\nR, letting researchers use informative imagery in substantive analyses in a\nfast and accessible manner."}, "http://arxiv.org/abs/2311.06409": {"title": "Flexible joint models for multivariate longitudinal and time-to-event data using multivariate functional principal components", "link": "http://arxiv.org/abs/2311.06409", "description": "The joint modeling of multiple longitudinal biomarkers together with a\ntime-to-event outcome is a challenging modeling task of continued scientific\ninterest. In particular, the computational complexity of high dimensional\n(generalized) mixed effects models often restricts the flexibility of shared\nparameter joint models, even when the subject-specific marker trajectories\nfollow highly nonlinear courses. We propose a parsimonious multivariate\nfunctional principal components representation of the shared random effects.\nThis allows better scalability, as the dimension of the random effects does not\ndirectly increase with the number of markers, only with the chosen number of\nprincipal component basis functions used in the approximation of the random\neffects. The functional principal component representation additionally allows\nto estimate highly flexible subject-specific random trajectories without\nparametric assumptions. The modeled trajectories can thus be distinctly\ndifferent for each biomarker. We build on the framework of flexible Bayesian\nadditive joint models implemented in the R-package 'bamlss', which also\nsupports estimation of nonlinear covariate effects via Bayesian P-splines. The\nflexible yet parsimonious functional principal components basis used in the\nestimation of the joint model is first estimated in a preliminary step. We\nvalidate our approach in a simulation study and illustrate its advantages by\nanalyzing a study on primary biliary cholangitis."}, "http://arxiv.org/abs/2311.06412": {"title": "Online multiple testing with e-values", "link": "http://arxiv.org/abs/2311.06412", "description": "A scientist tests a continuous stream of hypotheses over time in the course\nof her investigation -- she does not test a predetermined, fixed number of\nhypotheses. The scientist wishes to make as many discoveries as possible while\nensuring the number of false discoveries is controlled -- a well recognized way\nfor accomplishing this is to control the false discovery rate (FDR). Prior\nmethods for FDR control in the online setting have focused on formulating\nalgorithms when specific dependency structures are assumed to exist between the\ntest statistics of each hypothesis. However, in practice, these dependencies\noften cannot be known beforehand or tested after the fact. Our algorithm,\ne-LOND, provides FDR control under arbitrary, possibly unknown, dependence. We\nshow that our method is more powerful than existing approaches to this problem\nthrough simulations. We also formulate extensions of this algorithm to utilize\nrandomization for increased power, and for constructing confidence intervals in\nonline selective inference."}, "http://arxiv.org/abs/2311.06415": {"title": "Long-Term Dagum-PVF Frailty Regression Model: Application in Health Studies", "link": "http://arxiv.org/abs/2311.06415", "description": "Survival models incorporating cure fractions, commonly known as cure fraction\nmodels or long-term survival models, are widely employed in epidemiological\nstudies to account for both immune and susceptible patients in relation to the\nfailure event of interest under investigation. In such studies, there is also a\nneed to estimate the unobservable heterogeneity caused by prognostic factors\nthat cannot be observed. Moreover, the hazard function may exhibit a\nnon-monotonic form, specifically, an unimodal hazard function. In this article,\nwe propose a long-term survival model based on the defective version of the\nDagum distribution, with a power variance function (PVF) frailty term\nintroduced in the hazard function to control for unobservable heterogeneity in\npatient populations, which is useful for accommodating survival data in the\npresence of a cure fraction and with a non-monotone hazard function. The\ndistribution is conveniently reparameterized in terms of the cure fraction, and\nthen associated with the covariates via a logit link function, enabling direct\ninterpretation of the covariate effects on the cure fraction, which is not\nusual in the defective approach. It is also proven a result that generates\ndefective models induced by PVF frailty distribution. We discuss maximum\nlikelihood estimation for model parameters and evaluate its performance through\nMonte Carlo simulation studies. Finally, the practicality and benefits of our\nmodel are demonstrated through two health-related datasets, focusing on severe\ncases of COVID-19 in pregnant and postpartum women and on patients with\nmalignant skin neoplasms."}, "http://arxiv.org/abs/2311.06458": {"title": "Conditional Adjustment in a Markov Equivalence Class", "link": "http://arxiv.org/abs/2311.06458", "description": "We consider the problem of identifying a conditional causal effect through\ncovariate adjustment. We focus on the setting where the causal graph is known\nup to one of two types of graphs: a maximally oriented partially directed\nacyclic graph (MPDAG) or a partial ancestral graph (PAG). Both MPDAGs and PAGs\nrepresent equivalence classes of possible underlying causal models. After\ndefining adjustment sets in this setting, we provide a necessary and sufficient\ngraphical criterion -- the conditional adjustment criterion -- for finding\nthese sets under conditioning on variables unaffected by treatment. We further\nprovide explicit sets from the graph that satisfy the conditional adjustment\ncriterion, and therefore, can be used as adjustment sets for conditional causal\neffect identification."}, "http://arxiv.org/abs/2311.06537": {"title": "Is Machine Learning Unsafe and Irresponsible in Social Sciences? Paradoxes and Reconsidering from Recidivism Prediction Tasks", "link": "http://arxiv.org/abs/2311.06537", "description": "The paper addresses some fundamental and hotly debated issues for high-stakes\nevent predictions underpinning the computational approach to social sciences.\nWe question several prevalent views against machine learning and outline a new\nparadigm that highlights the promises and promotes the infusion of\ncomputational methods and conventional social science approaches."}, "http://arxiv.org/abs/2311.06590": {"title": "Optimal resource allocation: Convex quantile regression approach", "link": "http://arxiv.org/abs/2311.06590", "description": "Optimal allocation of resources across sub-units in the context of\ncentralized decision-making systems such as bank branches or supermarket chains\nis a classical application of operations research and management science. In\nthis paper, we develop quantile allocation models to examine how much the\noutput and productivity could potentially increase if the resources were\nefficiently allocated between units. We increase robustness to random noise and\nheteroscedasticity by utilizing the local estimation of multiple production\nfunctions using convex quantile regression. The quantile allocation models then\nrely on the estimated shadow prices instead of detailed data of units and allow\nthe entry and exit of units. Our empirical results on Finland's business sector\nreveal a large potential for productivity gains through better allocation,\nkeeping the current technology and resources fixed."}, "http://arxiv.org/abs/2311.06681": {"title": "SpICE: An interpretable method for spatial data", "link": "http://arxiv.org/abs/2311.06681", "description": "Statistical learning methods are widely utilized in tackling complex problems\ndue to their flexibility, good predictive performance and its ability to\ncapture complex relationships among variables. Additionally, recently developed\nautomatic workflows have provided a standardized approach to implementing\nstatistical learning methods across various applications. However these tools\nhighlight a main drawbacks of statistical learning: its lack of interpretation\nin their results. In the past few years an important amount of research has\nbeen focused on methods for interpreting black box models. Having interpretable\nstatistical learning methods is relevant to have a deeper understanding of the\nmodel. In problems were spatial information is relevant, combined interpretable\nmethods with spatial data can help to get better understanding of the problem\nand interpretation of the results.\n\nThis paper is focused in the individual conditional expectation (ICE-plot), a\nmodel agnostic methods for interpreting statistical learning models and\ncombined them with spatial information. ICE-plot extension is proposed where\nspatial information is used as restriction to define Spatial ICE curves\n(SpICE). Spatial ICE curves are estimated using real data in the context of an\neconomic problem concerning property valuation in Montevideo, Uruguay.\nUnderstanding the key factors that influence property valuation is essential\nfor decision-making, and spatial data plays a relevant role in this regard."}, "http://arxiv.org/abs/2311.06719": {"title": "Efficient Multiple-Robust Estimation for Nonresponse Data Under Informative Sampling", "link": "http://arxiv.org/abs/2311.06719", "description": "Nonresponse after probability sampling is a universal challenge in survey\nsampling, often necessitating adjustments to mitigate sampling and selection\nbias simultaneously. This study explored the removal of bias and effective\nutilization of available information, not just in nonresponse but also in the\nscenario of data integration, where summary statistics from other data sources\nare accessible. We reformulate these settings within a two-step monotone\nmissing data framework, where the first step of missingness arises from\nsampling and the second originates from nonresponse. Subsequently, we derive\nthe semiparametric efficiency bound for the target parameter. We also propose\nadaptive estimators utilizing methods of moments and empirical likelihood\napproaches to attain the lower bound. The proposed estimator exhibits both\nefficiency and double robustness. However, attaining efficiency with an\nadaptive estimator requires the correct specification of certain working\nmodels. To reinforce robustness against the misspecification of working models,\nwe extend the property of double robustness to multiple robustness by proposing\na two-step empirical likelihood method that effectively leverages empirical\nweights. A numerical study is undertaken to investigate the finite-sample\nperformance of the proposed methods. We further applied our methods to a\ndataset from the National Health and Nutrition Examination Survey data by\nefficiently incorporating summary statistics from the National Health Interview\nSurvey data."}, "http://arxiv.org/abs/2311.06831": {"title": "Quasi-Bayes in Latent Variable Models", "link": "http://arxiv.org/abs/2311.06831", "description": "Latent variable models are widely used to account for unobserved determinants\nof economic behavior. Traditional nonparametric methods to estimate latent\nheterogeneity do not scale well into multidimensional settings. Distributional\nrestrictions alleviate tractability concerns but may impart non-trivial\nmisspecification bias. Motivated by these concerns, this paper introduces a\nquasi-Bayes approach to estimate a large class of multidimensional latent\nvariable models. Our approach to quasi-Bayes is novel in that we center it\naround relating the characteristic function of observables to the distribution\nof unobservables. We propose a computationally attractive class of priors that\nare supported on Gaussian mixtures and derive contraction rates for a variety\nof latent variable models."}, "http://arxiv.org/abs/2311.06840": {"title": "Distribution Re-weighting and Voting Paradoxes", "link": "http://arxiv.org/abs/2311.06840", "description": "We explore a specific type of distribution shift called domain expertise, in\nwhich training is limited to a subset of all possible labels. This setting is\ncommon among specialized human experts, or specific focused studies. We show\nhow the standard approach to distribution shift, which involves re-weighting\ndata, can result in paradoxical disagreements among differing domain expertise.\nWe also demonstrate how standard adjustments for causal inference lead to the\nsame paradox. We prove that the characteristics of these paradoxes exactly\nmimic another set of paradoxes which arise among sets of voter preferences."}, "http://arxiv.org/abs/2311.06928": {"title": "Attention for Causal Relationship Discovery from Biological Neural Dynamics", "link": "http://arxiv.org/abs/2311.06928", "description": "This paper explores the potential of the transformer models for learning\nGranger causality in networks with complex nonlinear dynamics at every node, as\nin neurobiological and biophysical networks. Our study primarily focuses on a\nproof-of-concept investigation based on simulated neural dynamics, for which\nthe ground-truth causality is known through the underlying connectivity matrix.\nFor transformer models trained to forecast neuronal population dynamics, we\nshow that the cross attention module effectively captures the causal\nrelationship among neurons, with an accuracy equal or superior to that for the\nmost popular Granger causality analysis method. While we acknowledge that\nreal-world neurobiology data will bring further challenges, including dynamic\nconnectivity and unobserved variability, this research offers an encouraging\npreliminary glimpse into the utility of the transformer model for causal\nrepresentation learning in neuroscience."}, "http://arxiv.org/abs/2311.06945": {"title": "An Efficient Approach for Identifying Important Biomarkers for Biomedical Diagnosis", "link": "http://arxiv.org/abs/2311.06945", "description": "In this paper, we explore the challenges associated with biomarker\nidentification for diagnosis purpose in biomedical experiments, and propose a\nnovel approach to handle the above challenging scenario via the generalization\nof the Dantzig selector. To improve the efficiency of the regularization\nmethod, we introduce a transformation from an inherent nonlinear programming\ndue to its nonlinear link function into a linear programming framework. We\nillustrate the use of of our method on an experiment with binary response,\nshowing superior performance on biomarker identification studies when compared\nto their conventional analysis. Our proposed method does not merely serve as a\nvariable/biomarker selection tool, its ranking of variable importance provides\nvaluable reference information for practitioners to reach informed decisions\nregarding the prioritization of factors for further investigations."}, "http://arxiv.org/abs/2311.07034": {"title": "Regularized Halfspace Depth for Functional Data", "link": "http://arxiv.org/abs/2311.07034", "description": "Data depth is a powerful nonparametric tool originally proposed to rank\nmultivariate data from center outward. In this context, one of the most\narchetypical depth notions is Tukey's halfspace depth. In the last few decades\nnotions of depth have also been proposed for functional data. However, Tukey's\ndepth cannot be extended to handle functional data because of its degeneracy.\nHere, we propose a new halfspace depth for functional data which avoids\ndegeneracy by regularization. The halfspace projection directions are\nconstrained to have a small reproducing kernel Hilbert space norm. Desirable\ntheoretical properties of the proposed depth, such as isometry invariance,\nmaximality at center, monotonicity relative to a deepest point, upper\nsemi-continuity, and consistency are established. Moreover, the regularized\nhalfspace depth can rank functional data with varying emphasis in shape or\nmagnitude, depending on the regularization. A new outlier detection approach is\nalso proposed, which is capable of detecting both shape and magnitude outliers.\nIt is applicable to trajectories in L2, a very general space of functions that\ninclude non-smooth trajectories. Based on extensive numerical studies, our\nmethods are shown to perform well in terms of detecting outliers of different\ntypes. Three real data examples showcase the proposed depth notion."}, "http://arxiv.org/abs/2311.07156": {"title": "Deep mixture of linear mixed models for complex longitudinal data", "link": "http://arxiv.org/abs/2311.07156", "description": "Mixtures of linear mixed models are widely used for modelling longitudinal\ndata for which observation times differ between subjects. In typical\napplications, temporal trends are described using a basis expansion, with basis\ncoefficients treated as random effects varying by subject. Additional random\neffects can describe variation between mixture components, or other known\nsources of variation in complex experimental designs. A key advantage of these\nmodels is that they provide a natural mechanism for clustering, which can be\nhelpful for interpretation in many applications. Current versions of mixtures\nof linear mixed models are not specifically designed for the case where there\nare many observations per subject and a complex temporal trend, which requires\na large number of basis functions to capture. In this case, the\nsubject-specific basis coefficients are a high-dimensional random effects\nvector, for which the covariance matrix is hard to specify and estimate,\nespecially if it varies between mixture components. To address this issue, we\nconsider the use of recently-developed deep mixture of factor analyzers models\nas the prior for the random effects. The resulting deep mixture of linear mixed\nmodels is well-suited to high-dimensional settings, and we describe an\nefficient variational inference approach to posterior computation. The efficacy\nof the method is demonstrated on both real and simulated data."}, "http://arxiv.org/abs/2311.07243": {"title": "Optimal Estimation of Large-Dimensional Nonlinear Factor Models", "link": "http://arxiv.org/abs/2311.07243", "description": "This paper studies optimal estimation of large-dimensional nonlinear factor\nmodels. The key challenge is that the observed variables are possibly nonlinear\nfunctions of some latent variables where the functional forms are left\nunspecified. A local principal component analysis method is proposed to\nestimate the factor structure and recover information on latent variables and\nlatent functions, which combines $K$-nearest neighbors matching and principal\ncomponent analysis. Large-sample properties are established, including a sharp\nbound on the matching discrepancy of nearest neighbors, sup-norm error bounds\nfor estimated local factors and factor loadings, and the uniform convergence\nrate of the factor structure estimator. Under mild conditions our estimator of\nthe latent factor structure can achieve the optimal rate of uniform convergence\nfor nonparametric regression. The method is illustrated with a Monte Carlo\nexperiment and an empirical application studying the effect of tax cuts on\neconomic growth."}, "http://arxiv.org/abs/2311.07371": {"title": "Scalable Estimation for Structured Additive Distributional Regression Through Variational Inference", "link": "http://arxiv.org/abs/2311.07371", "description": "Structured additive distributional regression models offer a versatile\nframework for estimating complete conditional distributions by relating all\nparameters of a parametric distribution to covariates. Although these models\nefficiently leverage information in vast and intricate data sets, they often\nresult in highly-parameterized models with many unknowns. Standard estimation\nmethods, like Bayesian approaches based on Markov chain Monte Carlo methods,\nface challenges in estimating these models due to their complexity and\ncostliness. To overcome these issues, we suggest a fast and scalable\nalternative based on variational inference. Our approach combines a\nparsimonious parametric approximation for the posteriors of regression\ncoefficients, with the exact conditional posterior for hyperparameters. For\noptimization, we use a stochastic gradient ascent method combined with an\nefficient strategy to reduce the variance of estimators. We provide theoretical\nproperties and investigate global and local annealing to enhance robustness,\nparticularly against data outliers. Our implementation is very general,\nallowing us to include various functional effects like penalized splines or\ncomplex tensor product interactions. In a simulation study, we demonstrate the\nefficacy of our approach in terms of accuracy and computation time. Lastly, we\npresent two real examples illustrating the modeling of infectious COVID-19\noutbreaks and outlier detection in brain activity."}, "http://arxiv.org/abs/2311.07474": {"title": "A Federated Data Fusion-Based Prognostic Model for Applications with Multi-Stream Incomplete Signals", "link": "http://arxiv.org/abs/2311.07474", "description": "Most prognostic methods require a decent amount of data for model training.\nIn reality, however, the amount of historical data owned by a single\norganization might be small or not large enough to train a reliable prognostic\nmodel. To address this challenge, this article proposes a federated prognostic\nmodel that allows multiple users to jointly construct a failure time prediction\nmodel using their multi-stream, high-dimensional, and incomplete data while\nkeeping each user's data local and confidential. The prognostic model first\nemploys multivariate functional principal component analysis to fuse the\nmulti-stream degradation signals. Then, the fused features coupled with the\ntimes-to-failure are utilized to build a (log)-location-scale regression model\nfor failure prediction. To estimate parameters using distributed datasets and\nkeep the data privacy of all participants, we propose a new federated algorithm\nfor feature extraction. Numerical studies indicate that the performance of the\nproposed model is the same as that of classic non-federated prognostic models\nand is better than that of the models constructed by each user itself."}, "http://arxiv.org/abs/2311.07511": {"title": "Machine learning for uncertainty estimation in fusing precipitation observations from satellites and ground-based gauges", "link": "http://arxiv.org/abs/2311.07511", "description": "To form precipitation datasets that are accurate and, at the same time, have\nhigh spatial densities, data from satellites and gauges are often merged in the\nliterature. However, uncertainty estimates for the data acquired in this manner\nare scarcely provided, although the importance of uncertainty quantification in\npredictive modelling is widely recognized. Furthermore, the benefits that\nmachine learning can bring to the task of providing such estimates have not\nbeen broadly realized and properly explored through benchmark experiments. The\npresent study aims at filling in this specific gap by conducting the first\nbenchmark tests on the topic. On a large dataset that comprises 15-year-long\nmonthly data spanning across the contiguous United States, we extensively\ncompared six learners that are, by their construction, appropriate for\npredictive uncertainty quantification. These are the quantile regression (QR),\nquantile regression forests (QRF), generalized random forests (GRF), gradient\nboosting machines (GBM), light gradient boosting machines (LightGBM) and\nquantile regression neural networks (QRNN). The comparison referred to the\ncompetence of the learners in issuing predictive quantiles at nine levels that\nfacilitate a good approximation of the entire predictive probability\ndistribution, and was primarily based on the quantile and continuous ranked\nprobability skill scores. Three types of predictor variables (i.e., satellite\nprecipitation variables, distances between a point of interest and satellite\ngrid points, and elevation at a point of interest) were used in the comparison\nand were additionally compared with each other. This additional comparison was\nbased on the explainable machine learning concept of feature importance. The\nresults suggest that the order from the best to the worst of the learners for\nthe task investigated is the following: LightGBM, QRF, GRF, GBM, QRNN and QR..."}, "http://arxiv.org/abs/2311.07524": {"title": "The Link Between Health Insurance Coverage and Citizenship Among Immigrants: Bayesian Unit-Level Regression Modeling of Categorical Survey Data Observed with Measurement Error", "link": "http://arxiv.org/abs/2311.07524", "description": "Social scientists are interested in studying the impact that citizenship\nstatus has on health insurance coverage among immigrants in the United States.\nThis can be done using data from the Survey of Income and Program Participation\n(SIPP); however, two primary challenges emerge. First, statistical models must\naccount for the survey design in some fashion to reduce the risk of bias due to\ninformative sampling. Second, it has been observed that survey respondents\nmisreport citizenship status at nontrivial rates. This too can induce bias\nwithin a statistical model. Thus, we propose the use of a weighted\npseudo-likelihood mixture of categorical distributions, where the mixture\ncomponent is determined by the latent true response variable, in order to model\nthe misreported data. We illustrate through an empirical simulation study that\nthis approach can mitigate the two sources of bias attributable to the sample\ndesign and misreporting. Importantly, our misreporting model can be further\nused as a component in a deeper hierarchical model. With this in mind, we\nconduct an analysis of the relationship between health insurance coverage and\ncitizenship status using data from the SIPP."}, "http://arxiv.org/abs/1902.09608": {"title": "On Binscatter", "link": "http://arxiv.org/abs/1902.09608", "description": "Binscatter is a popular method for visualizing bivariate relationships and\nconducting informal specification testing. We study the properties of this\nmethod formally and develop enhanced visualization and econometric binscatter\ntools. These include estimating conditional means with optimal binning and\nquantifying uncertainty. We also highlight a methodological problem related to\ncovariate adjustment that can yield incorrect conclusions. We revisit two\napplications using our methodology and find substantially different results\nrelative to those obtained using prior informal binscatter methods. General\npurpose software in Python, R, and Stata is provided. Our technical work is of\nindependent interest for the nonparametric partition-based estimation\nliterature."}, "http://arxiv.org/abs/2110.10195": {"title": "Operator-induced structural variable selection for identifying materials genes", "link": "http://arxiv.org/abs/2110.10195", "description": "In the emerging field of materials informatics, a fundamental task is to\nidentify physicochemically meaningful descriptors, or materials genes, which\nare engineered from primary features and a set of elementary algebraic\noperators through compositions. Standard practice directly analyzes the\nhigh-dimensional candidate predictor space in a linear model; statistical\nanalyses are then substantially hampered by the daunting challenge posed by the\nastronomically large number of correlated predictors with limited sample size.\nWe formulate this problem as variable selection with operator-induced structure\n(OIS) and propose a new method to achieve unconventional dimension reduction by\nutilizing the geometry embedded in OIS. Although the model remains linear, we\niterate nonparametric variable selection for effective dimension reduction.\nThis enables variable selection based on ab initio primary features, leading to\na method that is orders of magnitude faster than existing methods, with\nimproved accuracy. To select the nonparametric module, we discuss a desired\nperformance criterion that is uniquely induced by variable selection with OIS;\nin particular, we propose to employ a Bayesian Additive Regression Trees\n(BART)-based variable selection method. Numerical studies show superiority of\nthe proposed method, which continues to exhibit robust performance when the\ninput dimension is out of reach of existing methods. Our analysis of\nsingle-atom catalysis identifies physical descriptors that explain the binding\nenergy of metal-support pairs with high explanatory power, leading to\ninterpretable insights to guide the prevention of a notorious problem called\nsintering and aid catalysis design."}, "http://arxiv.org/abs/2204.12699": {"title": "Randomness of Shapes and Statistical Inference on Shapes via the Smooth Euler Characteristic Transform", "link": "http://arxiv.org/abs/2204.12699", "description": "In this article, we establish the mathematical foundations for modeling the\nrandomness of shapes and conducting statistical inference on shapes using the\nsmooth Euler characteristic transform. Based on these foundations, we propose\ntwo parametric algorithms for testing hypotheses on random shapes. Simulation\nstudies are presented to validate our mathematical derivations and to compare\nour algorithms with state-of-the-art methods to demonstrate the utility of our\nproposed framework. As real applications, we analyze a data set of mandibular\nmolars from four genera of primates and show that our algorithms have the power\nto detect significant shape differences that recapitulate known morphological\nvariation across suborders. Altogether, our discussions bridge the following\nfields: algebraic and computational topology, probability theory and stochastic\nprocesses, Sobolev spaces and functional analysis, statistical inference, and\ngeometric morphometrics."}, "http://arxiv.org/abs/2207.05019": {"title": "Covariate-adaptive randomization inference in matched designs", "link": "http://arxiv.org/abs/2207.05019", "description": "It is common to conduct causal inference in matched observational studies by\nproceeding as though treatment assignments within matched sets are assigned\nuniformly at random and using this distribution as the basis for inference.\nThis approach ignores observed discrepancies in matched sets that may be\nconsequential for the distribution of treatment, which are succinctly captured\nby within-set differences in the propensity score. We address this problem via\ncovariate-adaptive randomization inference, which modifies the permutation\nprobabilities to vary with estimated propensity score discrepancies and avoids\nrequirements to exclude matched pairs or model an outcome variable. We show\nthat the test achieves type I error control arbitrarily close to the nominal\nlevel when large samples are available for propensity score estimation. We\ncharacterize the large-sample behavior of the new randomization test for a\ndifference-in-means estimator of a constant additive effect. We also show that\nexisting methods of sensitivity analysis generalize effectively to\ncovariate-adaptive randomization inference. Finally, we evaluate the empirical\nvalue of covariate-adaptive randomization procedures via comparisons to\ntraditional uniform inference in matched designs with and without propensity\nscore calipers and regression adjustment using simulations and analyses of\ngenetic damage among welders and right-heart catheterization in surgical\npatients."}, "http://arxiv.org/abs/2209.07091": {"title": "A new Kernel Regression approach for Robustified $L_2$ Boosting", "link": "http://arxiv.org/abs/2209.07091", "description": "We investigate $L_2$ boosting in the context of kernel regression. Kernel\nsmoothers, in general, lack appealing traits like symmetry and positive\ndefiniteness, which are critical not only for understanding theoretical aspects\nbut also for achieving good practical performance. We consider a\nprojection-based smoother (Huang and Chen, 2008) that is symmetric, positive\ndefinite, and shrinking. Theoretical results based on the orthonormal\ndecomposition of the smoother reveal additional insights into the boosting\nalgorithm. In our asymptotic framework, we may replace the full-rank smoother\nwith a low-rank approximation. We demonstrate that the smoother's low-rank\n($d(n)$) is bounded above by $O(h^{-1})$, where $h$ is the bandwidth. Our\nnumerical findings show that, in terms of prediction accuracy, low-rank\nsmoothers may outperform full-rank smoothers. Furthermore, we show that the\nboosting estimator with low-rank smoother achieves the optimal convergence\nrate. Finally, to improve the performance of the boosting algorithm in the\npresence of outliers, we propose a novel robustified boosting algorithm which\ncan be used with any smoother discussed in the study. We investigate the\nnumerical performance of the proposed approaches using simulations and a\nreal-world case."}, "http://arxiv.org/abs/2210.01757": {"title": "Transportability of model-based estimands in evidence synthesis", "link": "http://arxiv.org/abs/2210.01757", "description": "In evidence synthesis, effect modifiers are typically described as variables\nthat induce treatment effect heterogeneity at the individual level, through\ntreatment-covariate interactions in an outcome model parametrized at such\nlevel. As such, effect modification is defined with respect to a conditional\nmeasure, but marginal effect estimates are required for population-level\ndecisions in health technology assessment. For non-collapsible measures, purely\nprognostic variables that are not determinants of treatment response at the\nindividual level may modify marginal effects, even where there is\nindividual-level treatment effect homogeneity. With heterogeneity, marginal\neffects for measures that are not directly collapsible cannot be expressed in\nterms of marginal covariate moments, and generally depend on the joint\ndistribution of conditional effect measure modifiers and purely prognostic\nvariables. There are implications for recommended practices in evidence\nsynthesis. Unadjusted anchored indirect comparisons can be biased in the\nabsence of individual-level treatment effect heterogeneity, or when marginal\ncovariate moments are balanced across studies. Covariate adjustment may be\nnecessary to account for cross-study imbalances in joint covariate\ndistributions involving purely prognostic variables. In the absence of\nindividual patient data for the target, covariate adjustment approaches are\ninherently limited in their ability to remove bias for measures that are not\ndirectly collapsible. Directly collapsible measures would facilitate the\ntransportability of marginal effects between studies by: (1) reducing\ndependence on model-based covariate adjustment where there is individual-level\ntreatment effect homogeneity and marginal covariate moments are balanced; and\n(2) facilitating the selection of baseline covariates for adjustment where\nthere is individual-level treatment effect heterogeneity."}, "http://arxiv.org/abs/2212.01900": {"title": "Bayesian survival analysis with INLA", "link": "http://arxiv.org/abs/2212.01900", "description": "This tutorial shows how various Bayesian survival models can be fitted using\nthe integrated nested Laplace approximation in a clear, legible, and\ncomprehensible manner using the INLA and INLAjoint R-packages. Such models\ninclude accelerated failure time, proportional hazards, mixture cure, competing\nrisks, multi-state, frailty, and joint models of longitudinal and survival\ndata, originally presented in the article \"Bayesian survival analysis with\nBUGS\" (Alvares et al., 2021). In addition, we illustrate the implementation of\na new joint model for a longitudinal semicontinuous marker, recurrent events,\nand a terminal event. Our proposal aims to provide the reader with syntax\nexamples for implementing survival models using a fast and accurate approximate\nBayesian inferential approach."}, "http://arxiv.org/abs/2301.03661": {"title": "Generative Quantile Regression with Variability Penalty", "link": "http://arxiv.org/abs/2301.03661", "description": "Quantile regression and conditional density estimation can reveal structure\nthat is missed by mean regression, such as multimodality and skewness. In this\npaper, we introduce a deep learning generative model for joint quantile\nestimation called Penalized Generative Quantile Regression (PGQR). Our approach\nsimultaneously generates samples from many random quantile levels, allowing us\nto infer the conditional distribution of a response variable given a set of\ncovariates. Our method employs a novel variability penalty to avoid the problem\nof vanishing variability, or memorization, in deep generative models. Further,\nwe introduce a new family of partial monotonic neural networks (PMNN) to\ncircumvent the problem of crossing quantile curves. A major benefit of PGQR is\nthat it can be fit using a single optimization, thus bypassing the need to\nrepeatedly train the model at multiple quantile levels or use computationally\nexpensive cross-validation to tune the penalty parameter. We illustrate the\nefficacy of PGQR through extensive simulation studies and analysis of real\ndatasets. Code to implement our method is available at\nhttps://github.com/shijiew97/PGQR."}, "http://arxiv.org/abs/2302.09526": {"title": "Mixed Semi-Supervised Generalized-Linear-Regression with applications to Deep-Learning and Interpolators", "link": "http://arxiv.org/abs/2302.09526", "description": "We present a methodology for using unlabeled data to design semi supervised\nlearning (SSL) methods that improve the prediction performance of supervised\nlearning for regression tasks. The main idea is to design different mechanisms\nfor integrating the unlabeled data, and include in each of them a mixing\nparameter $\\alpha$, controlling the weight given to the unlabeled data.\nFocusing on Generalized Linear Models (GLM) and linear interpolators classes of\nmodels, we analyze the characteristics of different mixing mechanisms, and\nprove that in all cases, it is invariably beneficial to integrate the unlabeled\ndata with some nonzero mixing ratio $\\alpha&gt;0$, in terms of predictive\nperformance. Moreover, we provide a rigorous framework to estimate the best\nmixing ratio $\\alpha^*$ where mixed SSL delivers the best predictive\nperformance, while using the labeled and unlabeled data on hand.\n\nThe effectiveness of our methodology in delivering substantial improvement\ncompared to the standard supervised models, in a variety of settings, is\ndemonstrated empirically through extensive simulation, in a manner that\nsupports the theoretical analysis. We also demonstrate the applicability of our\nmethodology (with some intuitive modifications) to improve more complex models,\nsuch as deep neural networks, in real-world regression tasks."}, "http://arxiv.org/abs/2305.13421": {"title": "Sequential Estimation using Hierarchically Stratified Domains with Latin Hypercube Sampling", "link": "http://arxiv.org/abs/2305.13421", "description": "Quantifying the effect of uncertainties in systems where only point\nevaluations in the stochastic domain but no regularity conditions are available\nis limited to sampling-based techniques. This work presents an adaptive\nsequential stratification estimation method that uses Latin Hypercube Sampling\nwithin each stratum. The adaptation is achieved through a sequential\nhierarchical refinement of the stratification, guided by previous estimators\nusing local (i.e., stratum-dependent) variability indicators based on\ngeneralized polynomial chaos expansions and Sobol decompositions. For a given\ntotal number of samples $N$, the corresponding hierarchically constructed\nsequence of Stratified Sampling estimators combined with Latin Hypercube\nsampling is adequately averaged to provide a final estimator with reduced\nvariance. Numerical experiments illustrate the procedure's efficiency,\nindicating that it can offer a variance decay proportional to $N^{-2}$ in some\ncases."}, "http://arxiv.org/abs/2306.08794": {"title": "Quantile autoregressive conditional heteroscedasticity", "link": "http://arxiv.org/abs/2306.08794", "description": "This paper proposes a novel conditional heteroscedastic time series model by\napplying the framework of quantile regression processes to the ARCH(\\infty)\nform of the GARCH model. This model can provide varying structures for\nconditional quantiles of the time series across different quantile levels,\nwhile including the commonly used GARCH model as a special case. The strict\nstationarity of the model is discussed. For robustness against heavy-tailed\ndistributions, a self-weighted quantile regression (QR) estimator is proposed.\nWhile QR performs satisfactorily at intermediate quantile levels, its accuracy\ndeteriorates at high quantile levels due to data scarcity. As a remedy, a\nself-weighted composite quantile regression (CQR) estimator is further\nintroduced and, based on an approximate GARCH model with a flexible\nTukey-lambda distribution for the innovations, we can extrapolate the high\nquantile levels by borrowing information from intermediate ones. Asymptotic\nproperties for the proposed estimators are established. Simulation experiments\nare carried out to access the finite sample performance of the proposed\nmethods, and an empirical example is presented to illustrate the usefulness of\nthe new model."}, "http://arxiv.org/abs/2309.00948": {"title": "Marginalised Normal Regression: Unbiased curve fitting in the presence of x-errors", "link": "http://arxiv.org/abs/2309.00948", "description": "The history of the seemingly simple problem of straight line fitting in the\npresence of both $x$ and $y$ errors has been fraught with misadventure, with\nstatistically ad hoc and poorly tested methods abounding in the literature. The\nproblem stems from the emergence of latent variables describing the \"true\"\nvalues of the independent variables, the priors on which have a significant\nimpact on the regression result. By analytic calculation of maximum a\nposteriori values and biases, and comprehensive numerical mock tests, we assess\nthe quality of possible priors. In the presence of intrinsic scatter, the only\nprior that we find to give reliably unbiased results in general is a mixture of\none or more Gaussians with means and variances determined as part of the\ninference. We find that a single Gaussian is typically sufficient and dub this\nmodel Marginalised Normal Regression (MNR). We illustrate the necessity for MNR\nby comparing it to alternative methods on an important linear relation in\ncosmology, and extend it to nonlinear regression and an arbitrary covariance\nmatrix linking $x$ and $y$. We publicly release a Python/Jax implementation of\nMNR and its Gaussian mixture model extension that is coupled to Hamiltonian\nMonte Carlo for efficient sampling, which we call ROXY (Regression and\nOptimisation with X and Y errors)."}, "http://arxiv.org/abs/2311.07733": {"title": "Credible Intervals for Probability of Failure with Gaussian Processes", "link": "http://arxiv.org/abs/2311.07733", "description": "Efficiently approximating the probability of system failure has gained\nincreasing importance as expensive simulations begin to play a larger role in\nreliability quantification tasks in areas such as structural design, power grid\ndesign, and safety certification among others. This work derives credible\nintervals on the probability of failure for a simulation which we assume is a\nrealizations of a Gaussian process. We connect these intervals to binary\nclassification error and comment on their applicability to a broad class of\niterative schemes proposed throughout the literature. A novel iterative\nsampling scheme is proposed which can suggest multiple samples per batch for\nsimulations with parallel implementations. We empirically test our scalable,\nopen-source implementation on a variety simulations including a Tsunami model\nwhere failure is quantified in terms of maximum wave hight."}, "http://arxiv.org/abs/2311.07736": {"title": "Use of Equivalent Relative Utility (ERU) to Evaluate Artificial Intelligence-Enabled Rule-Out Devices", "link": "http://arxiv.org/abs/2311.07736", "description": "We investigated the use of equivalent relative utility (ERU) to evaluate the\neffectiveness of artificial intelligence (AI)-enabled rule-out devices that use\nAI to identify and autonomously remove non-cancer patient images from\nradiologist review in screening mammography.We reviewed two performance metrics\nthat can be used to compare the diagnostic performance between the\nradiologist-with-rule-out-device and radiologist-without-device workflows:\npositive/negative predictive values (PPV/NPV) and equivalent relative utility\n(ERU). To demonstrate the use of the two evaluation metrics, we applied both\nmethods to a recent US-based study that reported an improved performance of the\nradiologist-with-device workflow compared to the one without the device by\nretrospectively applying their AI algorithm to a large mammography dataset. We\nfurther applied the ERU method to a European study utilizing their reported\nrecall rates and cancer detection rates at different thresholds of their AI\nalgorithm to compare the potential utility among different thresholds. For the\nstudy using US data, neither the PPV/NPV nor the ERU method can conclude a\nsignificant improvement in diagnostic performance for any of the algorithm\nthresholds reported. For the study using European data, ERU values at lower AI\nthresholds are found to be higher than that at a higher threshold because more\nfalse-negative cases would be ruled-out at higher threshold, reducing the\noverall diagnostic performance. Both PPV/NPV and ERU methods can be used to\ncompare the diagnostic performance between the radiologist-with-device workflow\nand that without. One limitation of the ERU method is the need to measure the\nbaseline, standard-of-care relative utility (RU) value for mammography\nscreening in the US. Once the baseline value is known, the ERU method can be\napplied to large US datasets without knowing the true prevalence of the\ndataset."}, "http://arxiv.org/abs/2311.07752": {"title": "Doubly Robust Estimation under Possibly Misspecified Marginal Structural Cox Model", "link": "http://arxiv.org/abs/2311.07752", "description": "In this paper we address the challenges posed by non-proportional hazards and\ninformative censoring, offering a path toward more meaningful causal inference\nconclusions. We start from the marginal structural Cox model, which has been\nwidely used for analyzing observational studies with survival outcomes, and\ntypically relies on the inverse probability weighting method. The latter hinges\nupon a propensity score model for the treatment assignment, and a censoring\nmodel which incorporates both the treatment and the covariates. In such\nsettings, model misspecification can occur quite effortlessly, and the Cox\nregression model's non-collapsibility has historically posed challenges when\nstriving to guard against model misspecification through augmentation. We\nintroduce an augmented inverse probability weighted estimator which, enriched\nwith doubly robust properties, paves the way for integrating machine learning\nand a plethora of nonparametric methods, effectively overcoming the challenges\nof non-collapsibility. The estimator extends naturally to estimating a\ntime-average treatment effect when the proportional hazards assumption fails.\nWe closely examine its theoretical and practical performance, showing that it\nsatisfies both the assumption-lean and the well-specification criteria\ndiscussed in the recent literature. Finally, its application to a dataset\nreveals insights into the impact of mid-life alcohol consumption on mortality\nin later life."}, "http://arxiv.org/abs/2311.07762": {"title": "Finite Mixtures of Multivariate Poisson-Log Normal Factor Analyzers for Clustering Count Data", "link": "http://arxiv.org/abs/2311.07762", "description": "A mixture of multivariate Poisson-log normal factor analyzers is introduced\nby imposing constraints on the covariance matrix, which resulted in flexible\nmodels for clustering purposes. In particular, a class of eight parsimonious\nmixture models based on the mixtures of factor analyzers model are introduced.\nVariational Gaussian approximation is used for parameter estimation, and\ninformation criteria are used for model selection. The proposed models are\nexplored in the context of clustering discrete data arising from RNA sequencing\nstudies. Using real and simulated data, the models are shown to give favourable\nclustering performance. The GitHub R package for this work is available at\nhttps://github.com/anjalisilva/mixMPLNFA and is released under the open-source\nMIT license."}, "http://arxiv.org/abs/2311.07793": {"title": "The brain uses renewal points to model random sequences of stimuli", "link": "http://arxiv.org/abs/2311.07793", "description": "It has been classically conjectured that the brain assigns probabilistic\nmodels to sequences of stimuli. An important issue associated with this\nconjecture is the identification of the classes of models used by the brain to\nperform this task. We address this issue by using a new clustering procedure\nfor sets of electroencephalographic (EEG) data recorded from participants\nexposed to a sequence of auditory stimuli generated by a stochastic chain. This\nclustering procedure indicates that the brain uses renewal points in the\nstochastic sequence of auditory stimuli in order to build a model."}, "http://arxiv.org/abs/2311.07906": {"title": "Mixture Conditional Regression with Ultrahigh Dimensional Text Data for Estimating Extralegal Factor Effects", "link": "http://arxiv.org/abs/2311.07906", "description": "Testing judicial impartiality is a problem of fundamental importance in\nempirical legal studies, for which standard regression methods have been\npopularly used to estimate the extralegal factor effects. However, those\nmethods cannot handle control variables with ultrahigh dimensionality, such as\nfound in judgment documents recorded in text format. To solve this problem, we\ndevelop a novel mixture conditional regression (MCR) approach, assuming that\nthe whole sample can be classified into a number of latent classes. Within each\nlatent class, a standard linear regression model can be used to model the\nrelationship between the response and a key feature vector, which is assumed to\nbe of a fixed dimension. Meanwhile, ultrahigh dimensional control variables are\nthen used to determine the latent class membership, where a Na\\\"ive Bayes type\nmodel is used to describe the relationship. Hence, the dimension of control\nvariables is allowed to be arbitrarily high. A novel expectation-maximization\nalgorithm is developed for model estimation. Therefore, we are able to estimate\nthe interested key parameters as efficiently as if the true class membership\nwere known in advance. Simulation studies are presented to demonstrate the\nproposed MCR method. A real dataset of Chinese burglary offenses is analyzed\nfor illustration purpose."}, "http://arxiv.org/abs/2311.07951": {"title": "A Fast and Simple Algorithm for computing the MLE of Amplitude Density Function Parameters", "link": "http://arxiv.org/abs/2311.07951", "description": "Over the last decades, the family of $\\alpha$-stale distributions has proven\nto be useful for modelling in telecommunication systems. Particularly, in the\ncase of radar applications, finding a fast and accurate estimation for the\namplitude density function parameters appears to be very important. In this\nwork, the maximum likelihood estimator (MLE) is proposed for parameters of the\namplitude distribution. To do this, the amplitude data are \\emph{projected} on\nthe horizontal and vertical axes using two simple transformations. It is proved\nthat the \\emph{projected} data follow a zero-location symmetric $\\alpha$-stale\ndistribution for which the MLE can be computed quite fast. The average of\ncomputed MLEs based on two \\emph{projections} is considered as estimator for\nparameters of the amplitude distribution. Performance of the proposed\n\\emph{projection} method is demonstrated through simulation study and analysis\nof two sets of real radar data."}, "http://arxiv.org/abs/2311.07972": {"title": "Residual Importance Weighted Transfer Learning For High-dimensional Linear Regression", "link": "http://arxiv.org/abs/2311.07972", "description": "Transfer learning is an emerging paradigm for leveraging multiple sources to\nimprove the statistical inference on a single target. In this paper, we propose\na novel approach named residual importance weighted transfer learning (RIW-TL)\nfor high-dimensional linear models built on penalized likelihood. Compared to\nexisting methods such as Trans-Lasso that selects sources in an all-in-all-out\nmanner, RIW-TL includes samples via importance weighting and thus may permit\nmore effective sample use. To determine the weights, remarkably RIW-TL only\nrequires the knowledge of one-dimensional densities dependent on residuals,\nthus overcoming the curse of dimensionality of having to estimate\nhigh-dimensional densities in naive importance weighting. We show that the\noracle RIW-TL provides a faster rate than its competitors and develop a\ncross-fitting procedure to estimate this oracle. We discuss variants of RIW-TL\nby adopting different choices for residual weighting. The theoretical\nproperties of RIW-TL and its variants are established and compared with those\nof LASSO and Trans-Lasso. Extensive simulation and a real data analysis confirm\nits advantages."}, "http://arxiv.org/abs/2311.08004": {"title": "Nonlinear blind source separation exploiting spatial nonstationarity", "link": "http://arxiv.org/abs/2311.08004", "description": "In spatial blind source separation the observed multivariate random fields\nare assumed to be mixtures of latent spatially dependent random fields. The\nobjective is to recover latent random fields by estimating the unmixing\ntransformation. Currently, the algorithms for spatial blind source separation\ncan only estimate linear unmixing transformations. Nonlinear blind source\nseparation methods for spatial data are scarce. In this paper we extend an\nidentifiable variational autoencoder that can estimate nonlinear unmixing\ntransformations to spatially dependent data and demonstrate its performance for\nboth stationary and nonstationary spatial data using simulations. In addition,\nwe introduce scaled mean absolute Shapley additive explanations for\ninterpreting the latent components through nonlinear mixing transformation. The\nspatial identifiable variational autoencoder is applied to a geochemical\ndataset to find the latent random fields, which are then interpreted by using\nthe scaled mean absolute Shapley additive explanations."}, "http://arxiv.org/abs/2311.08050": {"title": "INLA+ -- Approximate Bayesian inference for non-sparse models using HPC", "link": "http://arxiv.org/abs/2311.08050", "description": "The integrated nested Laplace approximations (INLA) method has become a\nwidely utilized tool for researchers and practitioners seeking to perform\napproximate Bayesian inference across various fields of application. To address\nthe growing demand for incorporating more complex models and enhancing the\nmethod's capabilities, this paper introduces a novel framework that leverages\ndense matrices for performing approximate Bayesian inference based on INLA\nacross multiple computing nodes using HPC. When dealing with non-sparse\nprecision or covariance matrices, this new approach scales better compared to\nthe current INLA method, capitalizing on the computational power offered by\nmultiprocessors in shared and distributed memory architectures available in\ncontemporary computing resources and specialized dense matrix algebra. To\nvalidate the efficacy of this approach, we conduct a simulation study then\napply it to analyze cancer mortality data in Spain, employing a three-way\nspatio-temporal interaction model."}, "http://arxiv.org/abs/2311.08139": {"title": "Feedforward neural networks as statistical models: Improving interpretability through uncertainty quantification", "link": "http://arxiv.org/abs/2311.08139", "description": "Feedforward neural networks (FNNs) are typically viewed as pure prediction\nalgorithms, and their strong predictive performance has led to their use in\nmany machine-learning applications. However, their flexibility comes with an\ninterpretability trade-off; thus, FNNs have been historically less popular\namong statisticians. Nevertheless, classical statistical theory, such as\nsignificance testing and uncertainty quantification, is still relevant.\nSupplementing FNNs with methods of statistical inference, and covariate-effect\nvisualisations, can shift the focus away from black-box prediction and make\nFNNs more akin to traditional statistical models. This can allow for more\ninferential analysis, and, hence, make FNNs more accessible within the\nstatistical-modelling context."}, "http://arxiv.org/abs/2311.08168": {"title": "Time-Uniform Confidence Spheres for Means of Random Vectors", "link": "http://arxiv.org/abs/2311.08168", "description": "We derive and study time-uniform confidence spheres - termed confidence\nsphere sequences (CSSs) - which contain the mean of random vectors with high\nprobability simultaneously across all sample sizes. Inspired by the original\nwork of Catoni and Giulini, we unify and extend their analysis to cover both\nthe sequential setting and to handle a variety of distributional assumptions.\nMore concretely, our results include an empirical-Bernstein CSS for bounded\nrandom vectors (resulting in a novel empirical-Bernstein confidence interval),\na CSS for sub-$\\psi$ random vectors, and a CSS for heavy-tailed random vectors\nbased on a sequentially valid Catoni-Giulini estimator. Finally, we provide a\nversion of our empirical-Bernstein CSS that is robust to contamination by Huber\nnoise."}, "http://arxiv.org/abs/2311.08181": {"title": "Frame to frame interpolation for high-dimensional data visualisation using the woylier package", "link": "http://arxiv.org/abs/2311.08181", "description": "The woylier package implements tour interpolation paths between frames using\nGivens rotations. This provides an alternative to the geodesic interpolation\nbetween planes currently available in the tourr package. Tours are used to\nvisualise high-dimensional data and models, to detect clustering, anomalies and\nnon-linear relationships. Frame-to-frame interpolation can be useful for\nprojection pursuit guided tours when the index is not rotationally invariant.\nIt also provides a way to specifically reach a given target frame. We\ndemonstrate the method for exploring non-linear relationships between currency\ncross-rates."}, "http://arxiv.org/abs/2311.08254": {"title": "Identifiable and interpretable nonparametric factor analysis", "link": "http://arxiv.org/abs/2311.08254", "description": "Factor models have been widely used to summarize the variability of\nhigh-dimensional data through a set of factors with much lower dimensionality.\nGaussian linear factor models have been particularly popular due to their\ninterpretability and ease of computation. However, in practice, data often\nviolate the multivariate Gaussian assumption. To characterize higher-order\ndependence and nonlinearity, models that include factors as predictors in\nflexible multivariate regression are popular, with GP-LVMs using Gaussian\nprocess (GP) priors for the regression function and VAEs using deep neural\nnetworks. Unfortunately, such approaches lack identifiability and\ninterpretability and tend to produce brittle and non-reproducible results. To\naddress these problems by simplifying the nonparametric factor model while\nmaintaining flexibility, we propose the NIFTY framework, which parsimoniously\ntransforms uniform latent variables using one-dimensional nonlinear mappings\nand then applies a linear generative model. The induced multivariate\ndistribution falls into a flexible class while maintaining simple computation\nand interpretation. We prove that this model is identifiable and empirically\nstudy NIFTY using simulated data, observing good performance in density\nestimation and data visualization. We then apply NIFTY to bird song data in an\nenvironmental monitoring application."}, "http://arxiv.org/abs/2311.08315": {"title": "Total Empiricism: Learning from Data", "link": "http://arxiv.org/abs/2311.08315", "description": "Statistical analysis is an important tool to distinguish systematic from\nchance findings. Current statistical analyses rely on distributional\nassumptions reflecting the structure of some underlying model, which if not met\nlead to problems in the analysis and interpretation of the results. Instead of\ntrying to fix the model or \"correct\" the data, we here describe a totally\nempirical statistical approach that does not rely on ad hoc distributional\nassumptions in order to overcome many problems in contemporary statistics.\nStarting from elementary combinatorics, we motivate an information-guided\nformalism to quantify knowledge extracted from the given data. Subsequently, we\nderive model-agnostic methods to identify patterns that are solely evidenced by\nthe data based on our prior knowledge. The data-centric character of empiricism\nallows for its universal applicability, particularly as sample size grows\nlarger. In this comprehensive framework, we re-interpret and extend model\ndistributions, scores and statistical tests used in different schools of\nstatistics."}, "http://arxiv.org/abs/2311.08335": {"title": "Distinguishing immunological and behavioral effects of vaccination", "link": "http://arxiv.org/abs/2311.08335", "description": "The interpretation of vaccine efficacy estimands is subtle, even in\nrandomized trials designed to quantify immunological effects of vaccination. In\nthis article, we introduce terminology to distinguish between different vaccine\nefficacy estimands and clarify their interpretations. This allows us to\nexplicitly consider immunological and behavioural effects of vaccination, and\nestablish that policy-relevant estimands can differ substantially from those\ncommonly reported in vaccine trials. We further show that a conventional\nvaccine trial allows identification and estimation of different vaccine\nestimands under plausible conditions, if one additional post-treatment variable\nis measured. Specifically, we utilize a ``belief variable'' that indicates the\ntreatment an individual believed they had received. The belief variable is\nsimilar to ``blinding assessment'' variables that are occasionally collected in\nplacebo-controlled trials in other fields. We illustrate the relations between\nthe different estimands, and their practical relevance, in numerical examples\nbased on an influenza vaccine trial."}, "http://arxiv.org/abs/2311.08340": {"title": "Causal Message Passing: A Method for Experiments with Unknown and General Network Interference", "link": "http://arxiv.org/abs/2311.08340", "description": "Randomized experiments are a powerful methodology for data-driven evaluation\nof decisions or interventions. Yet, their validity may be undermined by network\ninterference. This occurs when the treatment of one unit impacts not only its\noutcome but also that of connected units, biasing traditional treatment effect\nestimations. Our study introduces a new framework to accommodate complex and\nunknown network interference, moving beyond specialized models in the existing\nliterature. Our framework, which we term causal message-passing, is grounded in\na high-dimensional approximate message passing methodology and is specifically\ntailored to experimental design settings with prevalent network interference.\nUtilizing causal message-passing, we present a practical algorithm for\nestimating the total treatment effect and demonstrate its efficacy in four\nnumerical scenarios, each with its unique interference structure."}, "http://arxiv.org/abs/2104.14987": {"title": "Emulating complex dynamical simulators with random Fourier features", "link": "http://arxiv.org/abs/2104.14987", "description": "A Gaussian process (GP)-based methodology is proposed to emulate complex\ndynamical computer models (or simulators). The method relies on emulating the\nnumerical flow map of the system over an initial (short) time step, where the\nflow map is a function that describes the evolution of the system from an\ninitial condition to a subsequent value at the next time step. This yields a\nprobabilistic distribution over the entire flow map function, with each draw\noffering an approximation to the flow map. The model output times series is\nthen predicted (under the Markov assumption) by drawing a sample from the\nemulated flow map (i.e., its posterior distribution) and using it to iterate\nfrom the initial condition ahead in time. Repeating this procedure with\nmultiple such draws creates a distribution over the time series. The mean and\nvariance of this distribution at a specific time point serve as the model\noutput prediction and the associated uncertainty, respectively. However,\ndrawing a GP posterior sample that represents the underlying function across\nits entire domain is computationally infeasible, given the infinite-dimensional\nnature of this object. To overcome this limitation, one can generate such a\nsample in an approximate manner using random Fourier features (RFF). RFF is an\nefficient technique for approximating the kernel and generating GP samples,\noffering both computational efficiency and theoretical guarantees. The proposed\nmethod is applied to emulate several dynamic nonlinear simulators including the\nwell-known Lorenz and van der Pol models. The results suggest that our approach\nhas a high predictive performance and the associated uncertainty can capture\nthe dynamics of the system accurately."}, "http://arxiv.org/abs/2111.12945": {"title": "Low-rank variational Bayes correction to the Laplace method", "link": "http://arxiv.org/abs/2111.12945", "description": "Approximate inference methods like the Laplace method, Laplace approximations\nand variational methods, amongst others, are popular methods when exact\ninference is not feasible due to the complexity of the model or the abundance\nof data. In this paper we propose a hybrid approximate method called Low-Rank\nVariational Bayes correction (VBC), that uses the Laplace method and\nsubsequently a Variational Bayes correction in a lower dimension, to the joint\nposterior mean. The cost is essentially that of the Laplace method which\nensures scalability of the method, in both model complexity and data size.\nModels with fixed and unknown hyperparameters are considered, for simulated and\nreal examples, for small and large datasets."}, "http://arxiv.org/abs/2202.13961": {"title": "Spatio-Causal Patterns of Sample Growth", "link": "http://arxiv.org/abs/2202.13961", "description": "Different statistical samples (e.g., from different locations) offer\npopulations and learning systems observations with distinct statistical\nproperties. Samples under (1) 'Unconfounded' growth preserve systems' ability\nto determine the independent effects of their individual variables on any\noutcome-of-interest (and lead, therefore, to fair and interpretable black-box\npredictions). Samples under (2) 'Externally-Valid' growth preserve their\nability to make predictions that generalize across out-of-sample variation. The\nfirst promotes predictions that generalize over populations, the second over\ntheir shared uncontrolled factors. We illustrate these theoretic patterns in\nthe full American census from 1840 to 1940, and samples ranging from the\nstreet-level all the way to the national. This reveals sample requirements for\ngeneralizability over space and time, and new connections among the Shapley\nvalue, counterfactual statistics, and hyperbolic geometry."}, "http://arxiv.org/abs/2211.04958": {"title": "Black-Box Model Confidence Sets Using Cross-Validation with High-Dimensional Gaussian Comparison", "link": "http://arxiv.org/abs/2211.04958", "description": "We derive high-dimensional Gaussian comparison results for the standard\n$V$-fold cross-validated risk estimates. Our results combine a recent\nstability-based argument for the low-dimensional central limit theorem of\ncross-validation with the high-dimensional Gaussian comparison framework for\nsums of independent random variables. These results give new insights into the\njoint sampling distribution of cross-validated risks in the context of model\ncomparison and tuning parameter selection, where the number of candidate models\nand tuning parameters can be larger than the fitting sample size. As a\nconsequence, our results provide theoretical support for a recent\nmethodological development that constructs model confidence sets using\ncross-validation."}, "http://arxiv.org/abs/2311.08427": {"title": "Towards a Transportable Causal Network Model Based on Observational Healthcare Data", "link": "http://arxiv.org/abs/2311.08427", "description": "Over the last decades, many prognostic models based on artificial\nintelligence techniques have been used to provide detailed predictions in\nhealthcare. Unfortunately, the real-world observational data used to train and\nvalidate these models are almost always affected by biases that can strongly\nimpact the outcomes validity: two examples are values missing not-at-random and\nselection bias. Addressing them is a key element in achieving transportability\nand in studying the causal relationships that are critical in clinical decision\nmaking, going beyond simpler statistical approaches based on probabilistic\nassociation.\n\nIn this context, we propose a novel approach that combines selection\ndiagrams, missingness graphs, causal discovery and prior knowledge into a\nsingle graphical model to estimate the cardiovascular risk of adolescent and\nyoung females who survived breast cancer. We learn this model from data\ncomprising two different cohorts of patients. The resulting causal network\nmodel is validated by expert clinicians in terms of risk assessment, accuracy\nand explainability, and provides a prognostic model that outperforms competing\nmachine learning methods."}, "http://arxiv.org/abs/2311.08484": {"title": "Covariance Assisted Multivariate Penalized Additive Regression (CoMPAdRe)", "link": "http://arxiv.org/abs/2311.08484", "description": "We propose a new method for the simultaneous selection and estimation of\nmultivariate sparse additive models with correlated errors. Our method called\nCovariance Assisted Multivariate Penalized Additive Regression (CoMPAdRe)\nsimultaneously selects among null, linear, and smooth non-linear effects for\neach predictor while incorporating joint estimation of the sparse residual\nstructure among responses, with the motivation that accounting for\ninter-response correlation structure can lead to improved accuracy in variable\nselection and estimation efficiency. CoMPAdRe is constructed in a\ncomputationally efficient way that allows the selection and estimation of\nlinear and non-linear covariates to be conducted in parallel across responses.\nCompared to single-response approaches that marginally select linear and\nnon-linear covariate effects, we demonstrate in simulation studies that the\njoint multivariate modeling leads to gains in both estimation efficiency and\nselection accuracy, of greater magnitude in settings where signal is moderate\nrelative to the level of noise. We apply our approach to protein-mRNA\nexpression levels from multiple breast cancer pathways obtained from The Cancer\nProteome Atlas and characterize both mRNA-protein associations and\nprotein-protein subnetworks for each pathway. We find non-linear mRNA-protein\nassociations for the Core Reactive, EMT, PIK-AKT, and RTK pathways."}, "http://arxiv.org/abs/2311.08527": {"title": "Inferring the Long-Term Causal Effects of Long-Term Treatments from Short-Term Experiments", "link": "http://arxiv.org/abs/2311.08527", "description": "We study inference on the long-term causal effect of a continual exposure to\na novel intervention, which we term a long-term treatment, based on an\nexperiment involving only short-term observations. Key examples include the\nlong-term health effects of regularly-taken medicine or of environmental\nhazards and the long-term effects on users of changes to an online platform.\nThis stands in contrast to short-term treatments or \"shocks,\" whose long-term\neffect can reasonably be mediated by short-term observations, enabling the use\nof surrogate methods. Long-term treatments by definition have direct effects on\nlong-term outcomes via continual exposure so surrogacy cannot reasonably hold.\n\nOur approach instead learns long-term temporal dynamics directly from\nshort-term experimental data, assuming that the initial dynamics observed\npersist but avoiding the need for both surrogacy assumptions and auxiliary data\nwith long-term observations. We connect the problem with offline reinforcement\nlearning, leveraging doubly-robust estimators to estimate long-term causal\neffects for long-term treatments and construct confidence intervals. Finally,\nwe demonstrate the method in simulated experiments."}, "http://arxiv.org/abs/2311.08561": {"title": "Measuring association with recursive rank binning", "link": "http://arxiv.org/abs/2311.08561", "description": "Pairwise measures of dependence are a common tool to map data in the early\nstages of analysis with several modern examples based on maximized partitions\nof the pairwise sample space. Following a short survey of modern measures of\ndependence, we introduce a new measure which recursively splits the ranks of a\npair of variables to partition the sample space and computes the $\\chi^2$\nstatistic on the resulting bins. Splitting logic is detailed for splits\nmaximizing a score function and randomly selected splits. Simulations indicate\nthat random splitting produces a statistic conservatively approximated by the\n$\\chi^2$ distribution without a loss of power to detect numerous different data\npatterns compared to maximized binning. Though it seems to add no power to\ndetect dependence, maximized recursive binning is shown to produce a natural\nvisualization of the data and the measure. Applying maximized recursive rank\nbinning to S&amp;P 500 constituent data suggests the automatic detection of tail\ndependence."}, "http://arxiv.org/abs/2311.08604": {"title": "Incremental Cost-Effectiveness Statistical Inference: Calculations and Communications", "link": "http://arxiv.org/abs/2311.08604", "description": "We illustrate use of nonparametric statistical methods to compare alternative\ntreatments for a particular disease or condition on both their relative\neffectiveness and their relative cost. These Incremental Cost Effectiveness\n(ICE) methods are based upon Bootstrapping, i.e. Resampling with Replacement\nfrom observational or clinical-trial data on individual patients. We first show\nhow a reasonable numerical value for the \"Shadow Price of Health\" can be chosen\nusing functions within the ICEinfer R-package when effectiveness is not\nmeasured in \"QALY\"s. We also argue that simple histograms are ideal for\ncommunicating key findings to regulators, while our more detailed graphics may\nwell be more informative and compelling for other health-care stakeholders."}, "http://arxiv.org/abs/2311.08658": {"title": "Structured Estimation of Heterogeneous Time Series", "link": "http://arxiv.org/abs/2311.08658", "description": "How best to model structurally heterogeneous processes is a foundational\nquestion in the social, health and behavioral sciences. Recently, Fisher et\nal., (2022) introduced the multi-VAR approach for simultaneously estimating\nmultiple-subject multivariate time series characterized by common and\nindividualizing features using penalized estimation. This approach differs from\nmany popular modeling approaches for multiple-subject time series in that\nqualitative and quantitative differences in a large number of individual\ndynamics are well-accommodated. The current work extends the multi-VAR\nframework to include new adaptive weighting schemes that greatly improve\nestimation performance. In a small set of simulation studies we compare\nadaptive multi-VAR with these new penalty weights to common alternative\nestimators in terms of path recovery and bias. Furthermore, we provide toy\nexamples and code demonstrating the utility of multi-VAR under different\nheterogeneity regimes using the multivar package for R (Fisher, 2022)."}, "http://arxiv.org/abs/2311.08690": {"title": "Enabling CMF Estimation in Data-Constrained Scenarios: A Semantic-Encoding Knowledge Mining Model", "link": "http://arxiv.org/abs/2311.08690", "description": "Precise estimation of Crash Modification Factors (CMFs) is central to\nevaluating the effectiveness of various road safety treatments and prioritizing\ninfrastructure investment accordingly. While customized study for each\ncountermeasure scenario is desired, the conventional CMF estimation approaches\nrely heavily on the availability of crash data at given sites. This not only\nmakes the estimation costly, but the results are also less transferable, since\nthe intrinsic similarities between different safety countermeasure scenarios\nare not fully explored. Aiming to fill this gap, this study introduces a novel\nknowledge-mining framework for CMF prediction. This framework delves into the\nconnections of existing countermeasures and reduces the reliance of CMF\nestimation on crash data availability and manual data collection. Specifically,\nit draws inspiration from human comprehension processes and introduces advanced\nNatural Language Processing (NLP) techniques to extract intricate variations\nand patterns from existing CMF knowledge. It effectively encodes unstructured\ncountermeasure scenarios into machine-readable representations and models the\ncomplex relationships between scenarios and CMF values. This new data-driven\nframework provides a cost-effective and adaptable solution that complements the\ncase-specific approaches for CMF estimation, which is particularly beneficial\nwhen availability of crash data or time imposes constraints. Experimental\nvalidation using real-world CMF Clearinghouse data demonstrates the\neffectiveness of this new approach, which shows significant accuracy\nimprovements compared to baseline methods. This approach provides insights into\nnew possibilities of harnessing accumulated transportation knowledge in various\napplications."}, "http://arxiv.org/abs/2311.08691": {"title": "On Doubly Robust Estimation with Nonignorable Missing Data Using Instrumental Variables", "link": "http://arxiv.org/abs/2311.08691", "description": "Suppose we are interested in the mean of an outcome that is subject to\nnonignorable nonresponse. This paper develops new semiparametric estimation\nmethods with instrumental variables which affect nonresponse, but not the\noutcome. The proposed estimators remain consistent and asymptotically normal\neven under partial model misspecifications for two variation independent\nnuisance functions. We evaluate the performance of the proposed estimators via\na simulation study, and apply them in adjusting for missing data induced by HIV\ntesting refusal in the evaluation of HIV seroprevalence in Mochudi, Botswana,\nusing interviewer experience as an instrumental variable."}, "http://arxiv.org/abs/2311.08743": {"title": "Kernel-based independence tests for causal structure learning on functional data", "link": "http://arxiv.org/abs/2311.08743", "description": "Measurements of systems taken along a continuous functional dimension, such\nas time or space, are ubiquitous in many fields, from the physical and\nbiological sciences to economics and engineering.Such measurements can be\nviewed as realisations of an underlying smooth process sampled over the\ncontinuum. However, traditional methods for independence testing and causal\nlearning are not directly applicable to such data, as they do not take into\naccount the dependence along the functional dimension. By using specifically\ndesigned kernels, we introduce statistical tests for bivariate, joint, and\nconditional independence for functional variables. Our method not only extends\nthe applicability to functional data of the HSIC and its d-variate version\n(d-HSIC), but also allows us to introduce a test for conditional independence\nby defining a novel statistic for the CPT based on the HSCIC, with optimised\nregularisation strength estimated through an evaluation rejection rate. Our\nempirical results of the size and power of these tests on synthetic functional\ndata show good performance, and we then exemplify their application to several\nconstraint- and regression-based causal structure learning problems, including\nboth synthetic examples and real socio-economic data."}, "http://arxiv.org/abs/2311.08752": {"title": "ProSpar-GP: scalable Gaussian process modeling with massive non-stationary datasets", "link": "http://arxiv.org/abs/2311.08752", "description": "Gaussian processes (GPs) are a popular class of Bayesian nonparametric\nmodels, but its training can be computationally burdensome for massive training\ndatasets. While there has been notable work on scaling up these models for big\ndata, existing methods typically rely on a stationary GP assumption for\napproximation, and can thus perform poorly when the underlying response surface\nis non-stationary, i.e., it has some regions of rapid change and other regions\nwith little change. Such non-stationarity is, however, ubiquitous in real-world\nproblems, including our motivating application for surrogate modeling of\ncomputer experiments. We thus propose a new Product of Sparse GP (ProSpar-GP)\nmethod for scalable GP modeling with massive non-stationary data. The\nProSpar-GP makes use of a carefully-constructed product-of-experts formulation\nof sparse GP experts, where different experts are placed within local regions\nof non-stationarity. These GP experts are fit via a novel variational inference\napproach, which capitalizes on mini-batching and GPU acceleration for efficient\noptimization of inducing points and length-scale parameters for each expert. We\nfurther show that the ProSpar-GP is Kolmogorov-consistent, in that its\ngenerative distribution defines a valid stochastic process over the prediction\nspace; such a property provides essential stability for variational inference,\nparticularly in the presence of non-stationarity. We then demonstrate the\nimproved performance of the ProSpar-GP over the state-of-the-art, in a suite of\nnumerical experiments and an application for surrogate modeling of a satellite\ndrag simulator."}, "http://arxiv.org/abs/2311.08812": {"title": "Optimal subsampling algorithm for the marginal model with large longitudinal data", "link": "http://arxiv.org/abs/2311.08812", "description": "Big data is ubiquitous in practices, and it has also led to heavy computation\nburden. To reduce the calculation cost and ensure the effectiveness of\nparameter estimators, an optimal subset sampling method is proposed to estimate\nthe parameters in marginal models with massive longitudinal data. The optimal\nsubsampling probabilities are derived, and the corresponding asymptotic\nproperties are established to ensure the consistency and asymptotic normality\nof the estimator. Extensive simulation studies are carried out to evaluate the\nperformance of the proposed method for continuous, binary and count data and\nwith four different working correlation matrices. A depression data is used to\nillustrate the proposed method."}, "http://arxiv.org/abs/2311.08845": {"title": "Statistical learning by sparse deep neural networks", "link": "http://arxiv.org/abs/2311.08845", "description": "We consider a deep neural network estimator based on empirical risk\nminimization with l_1-regularization. We derive a general bound for its excess\nrisk in regression and classification (including multiclass), and prove that it\nis adaptively nearly-minimax (up to log-factors) simultaneously across the\nentire range of various function classes."}, "http://arxiv.org/abs/2311.08908": {"title": "Robust Brain MRI Image Classification with SIBOW-SVM", "link": "http://arxiv.org/abs/2311.08908", "description": "The majority of primary Central Nervous System (CNS) tumors in the brain are\namong the most aggressive diseases affecting humans. Early detection of brain\ntumor types, whether benign or malignant, glial or non-glial, is critical for\ncancer prevention and treatment, ultimately improving human life expectancy.\nMagnetic Resonance Imaging (MRI) stands as the most effective technique to\ndetect brain tumors by generating comprehensive brain images through scans.\nHowever, human examination can be error-prone and inefficient due to the\ncomplexity, size, and location variability of brain tumors. Recently, automated\nclassification techniques using machine learning (ML) methods, such as\nConvolutional Neural Network (CNN), have demonstrated significantly higher\naccuracy than manual screening, while maintaining low computational costs.\nNonetheless, deep learning-based image classification methods, including CNN,\nface challenges in estimating class probabilities without proper model\ncalibration. In this paper, we propose a novel brain tumor image classification\nmethod, called SIBOW-SVM, which integrates the Bag-of-Features (BoF) model with\nSIFT feature extraction and weighted Support Vector Machines (wSVMs). This new\napproach effectively captures hidden image features, enabling the\ndifferentiation of various tumor types and accurate label predictions.\nAdditionally, the SIBOW-SVM is able to estimate the probabilities of images\nbelonging to each class, thereby providing high-confidence classification\ndecisions. We have also developed scalable and parallelable algorithms to\nfacilitate the practical implementation of SIBOW-SVM for massive images. As a\nbenchmark, we apply the SIBOW-SVM to a public data set of brain tumor MRI\nimages containing four classes: glioma, meningioma, pituitary, and normal. Our\nresults show that the new method outperforms state-of-the-art methods,\nincluding CNN."}, "http://arxiv.org/abs/2311.09015": {"title": "Identification and Estimation for Nonignorable Missing Data: A Data Fusion Approach", "link": "http://arxiv.org/abs/2311.09015", "description": "We consider the task of identifying and estimating a parameter of interest in\nsettings where data is missing not at random (MNAR). In general, such\nparameters are not identified without strong assumptions on the missing data\nmodel. In this paper, we take an alternative approach and introduce a method\ninspired by data fusion, where information in an MNAR dataset is augmented by\ninformation in an auxiliary dataset subject to missingness at random (MAR). We\nshow that even if the parameter of interest cannot be identified given either\ndataset alone, it can be identified given pooled data, under two complementary\nsets of assumptions. We derive an inverse probability weighted (IPW) estimator\nfor identified parameters, and evaluate the performance of our estimation\nstrategies via simulation studies."}, "http://arxiv.org/abs/2311.09081": {"title": "Posterior accuracy and calibration under misspecification in Bayesian generalized linear models", "link": "http://arxiv.org/abs/2311.09081", "description": "Generalized linear models (GLMs) are popular for data-analysis in almost all\nquantitative sciences, but the choice of likelihood family and link function is\noften difficult. This motivates the search for likelihoods and links that\nminimize the impact of potential misspecification. We perform a large-scale\nsimulation study on double-bounded and lower-bounded response data where we\nsystematically vary both true and assumed likelihoods and links. In contrast to\nprevious studies, we also study posterior calibration and uncertainty metrics\nin addition to point-estimate accuracy. Our results indicate that certain\nlikelihoods and links can be remarkably robust to misspecification, performing\nalmost on par with their respective true counterparts. Additionally, normal\nlikelihood models with identity link (i.e., linear regression) often achieve\ncalibration comparable to the more structurally faithful alternatives, at least\nin the studied scenarios. On the basis of our findings, we provide practical\nsuggestions for robust likelihood and link choices in GLMs."}, "http://arxiv.org/abs/2311.09107": {"title": "Illness-death model with renewal", "link": "http://arxiv.org/abs/2311.09107", "description": "The illness-death model for chronic conditions is combined with a renewal\nequation for the number of newborns taking into account possibly different\nfertility rates in the healthy and diseased parts of the population. The\nresulting boundary value problem consists of a system of partial differential\nequations with an integral boundary condition. As an application, the boundary\nvalue problem is applied to an example about type 2 diabetes."}, "http://arxiv.org/abs/2311.09137": {"title": "Causal prediction models for medication safety monitoring: The diagnosis of vancomycin-induced acute kidney injury", "link": "http://arxiv.org/abs/2311.09137", "description": "The current best practice approach for the retrospective diagnosis of adverse\ndrug events (ADEs) in hospitalized patients relies on a full patient chart\nreview and a formal causality assessment by multiple medical experts. This\nevaluation serves to qualitatively estimate the probability of causation (PC);\nthe probability that a drug was a necessary cause of an adverse event. This\npractice is manual, resource intensive and prone to human biases, and may thus\nbenefit from data-driven decision support. Here, we pioneer a causal modeling\napproach using observational data to estimate a lower bound of the PC\n(PC$_{low}$). This method includes two key causal inference components: (1) the\ntarget trial emulation framework and (2) estimation of individualized treatment\neffects using machine learning. We apply our method to the clinically relevant\nuse-case of vancomycin-induced acute kidney injury in intensive care patients,\nand compare our causal model-based PC$_{low}$ estimates to qualitative\nestimates of the PC provided by a medical expert. Important limitations and\npotential improvements are discussed, and we conclude that future improved\ncausal models could provide essential data-driven support for medication safety\nmonitoring in hospitalized patients."}, "http://arxiv.org/abs/1911.03071": {"title": "Balancing Covariates in Randomized Experiments with the Gram-Schmidt Walk Design", "link": "http://arxiv.org/abs/1911.03071", "description": "The design of experiments involves a compromise between covariate balance and\nrobustness. This paper provides a formalization of this trade-off and describes\nan experimental design that allows experimenters to navigate it. The design is\nspecified by a robustness parameter that bounds the worst-case mean squared\nerror of an estimator of the average treatment effect. Subject to the\nexperimenter's desired level of robustness, the design aims to simultaneously\nbalance all linear functions of potentially many covariates. Less robustness\nallows for more balance. We show that the mean squared error of the estimator\nis bounded in finite samples by the minimum of the loss function of an implicit\nridge regression of the potential outcomes on the covariates. Asymptotically,\nthe design perfectly balances all linear functions of a growing number of\ncovariates with a diminishing reduction in robustness, effectively allowing\nexperimenters to escape the compromise between balance and robustness in large\nsamples. Finally, we describe conditions that ensure asymptotic normality and\nprovide a conservative variance estimator, which facilitate the construction of\nasymptotically valid confidence intervals."}, "http://arxiv.org/abs/2007.10432": {"title": "Treatment Effects with Targeting Instruments", "link": "http://arxiv.org/abs/2007.10432", "description": "Multivalued treatments are commonplace in applications. We explore the use of\ndiscrete-valued instruments to control for selection bias in this setting. Our\ndiscussion revolves around the concept of targeting instruments: which\ninstruments target which treatments. It allows us to establish conditions under\nwhich counterfactual averages and treatment effects are point- or\npartially-identified for composite complier groups. We illustrate the\nusefulness of our framework by applying it to data from the Head Start Impact\nStudy. Under a plausible positive selection assumption, we derive informative\nbounds that suggest less beneficial effects of Head Start expansions than the\nparametric estimates of Kline and Walters (2016)."}, "http://arxiv.org/abs/2102.07356": {"title": "Asymptotic properties of generalized closed-form maximum likelihood estimators", "link": "http://arxiv.org/abs/2102.07356", "description": "The maximum likelihood estimator (MLE) is pivotal in statistical inference,\nyet its application is often hindered by the absence of closed-form solutions\nfor many models. This poses challenges in real-time computation scenarios,\nparticularly within embedded systems technology, where numerical methods are\nimpractical. This study introduces a generalized form of the MLE that yields\nclosed-form estimators under certain conditions. We derive the asymptotic\nproperties of the proposed estimator and demonstrate that our approach retains\nkey properties such as invariance under one-to-one transformations, strong\nconsistency, and an asymptotic normal distribution. The effectiveness of the\ngeneralized MLE is exemplified through its application to the Gamma, Nakagami,\nand Beta distributions, showcasing improvements over the traditional MLE.\nAdditionally, we extend this methodology to a bivariate gamma distribution,\nsuccessfully deriving closed-form estimators. This advancement presents\nsignificant implications for real-time statistical analysis across various\napplications."}, "http://arxiv.org/abs/2207.13493": {"title": "The Cellwise Minimum Covariance Determinant Estimator", "link": "http://arxiv.org/abs/2207.13493", "description": "The usual Minimum Covariance Determinant (MCD) estimator of a covariance\nmatrix is robust against casewise outliers. These are cases (that is, rows of\nthe data matrix) that behave differently from the majority of cases, raising\nsuspicion that they might belong to a different population. On the other hand,\ncellwise outliers are individual cells in the data matrix. When a row contains\none or more outlying cells, the other cells in the same row still contain\nuseful information that we wish to preserve. We propose a cellwise robust\nversion of the MCD method, called cellMCD. Its main building blocks are\nobserved likelihood and a penalty term on the number of flagged cellwise\noutliers. It possesses good breakdown properties. We construct a fast algorithm\nfor cellMCD based on concentration steps (C-steps) that always lower the\nobjective. The method performs well in simulations with cellwise outliers, and\nhas high finite-sample efficiency on clean data. It is illustrated on real data\nwith visualizations of the results."}, "http://arxiv.org/abs/2208.07086": {"title": "Flexible Bayesian Multiple Comparison Adjustment Using Dirichlet Process and Beta-Binomial Model Priors", "link": "http://arxiv.org/abs/2208.07086", "description": "Researchers frequently wish to assess the equality or inequality of groups,\nbut this comes with the challenge of adequately adjusting for multiple\ncomparisons. Statistically, all possible configurations of equality and\ninequality constraints can be uniquely represented as partitions of the groups,\nwhere any number of groups are equal if they are in the same partition. In a\nBayesian framework, one can adjust for multiple comparisons by constructing a\nsuitable prior distribution over all possible partitions. Inspired by work on\nvariable selection in regression, we propose a class of flexible beta-binomial\npriors for Bayesian multiple comparison adjustment. We compare this prior setup\nto the Dirichlet process prior suggested by Gopalan and Berry (1998) and\nmultiple comparison adjustment methods that do not specify a prior over\npartitions directly. Our approach to multiple comparison adjustment not only\nallows researchers to assess all pairwise (in)equalities, but in fact all\npossible (in)equalities among all groups. As a consequence, the space of\npossible partitions grows quickly - for ten groups, there are already 115,975\npossible partitions - and we set up a stochastic search algorithm to\nefficiently explore the space. Our method is implemented in the Julia package\nEqualitySampler, and we illustrate it on examples related to the comparison of\nmeans, variances, and proportions."}, "http://arxiv.org/abs/2208.07959": {"title": "Variable Selection in Latent Regression IRT Models via Knockoffs: An Application to International Large-scale Assessment in Education", "link": "http://arxiv.org/abs/2208.07959", "description": "International large-scale assessments (ILSAs) play an important role in\neducational research and policy making. They collect valuable data on education\nquality and performance development across many education systems, giving\ncountries the opportunity to share techniques, organizational structures, and\npolicies that have proven efficient and successful. To gain insights from ILSA\ndata, we identify non-cognitive variables associated with students' academic\nperformance. This problem has three analytical challenges: 1) academic\nperformance is measured by cognitive items under a matrix sampling design; 2)\nthere are many missing values in the non-cognitive variables; and 3) multiple\ncomparisons due to a large number of non-cognitive variables. We consider an\napplication to the Programme for International Student Assessment (PISA),\naiming to identify non-cognitive variables associated with students'\nperformance in science. We formulate it as a variable selection problem under a\ngeneral latent variable model framework and further propose a knockoff method\nthat conducts variable selection with a controlled error rate for false\nselections."}, "http://arxiv.org/abs/2210.06927": {"title": "Prediction can be safely used as a proxy for explanation in causally consistent Bayesian generalized linear models", "link": "http://arxiv.org/abs/2210.06927", "description": "Bayesian modeling provides a principled approach to quantifying uncertainty\nin model parameters and model structure and has seen a surge of applications in\nrecent years. Within the context of a Bayesian workflow, we are concerned with\nmodel selection for the purpose of finding models that best explain the data,\nthat is, help us understand the underlying data generating process. Since we\nrarely have access to the true process, all we are left with during real-world\nanalyses is incomplete causal knowledge from sources outside of the current\ndata and model predictions of said data. This leads to the important question\nof when the use of prediction as a proxy for explanation for the purpose of\nmodel selection is valid. We approach this question by means of large-scale\nsimulations of Bayesian generalized linear models where we investigate various\ncausal and statistical misspecifications. Our results indicate that the use of\nprediction as proxy for explanation is valid and safe only when the models\nunder consideration are sufficiently consistent with the underlying causal\nstructure of the true data generating process."}, "http://arxiv.org/abs/2212.04550": {"title": "Modern Statistical Models and Methods for Estimating Fatigue-Life and Fatigue-Strength Distributions from Experimental Data", "link": "http://arxiv.org/abs/2212.04550", "description": "Engineers and scientists have been collecting and analyzing fatigue data\nsince the 1800s to ensure the reliability of life-critical structures.\nApplications include (but are not limited to) bridges, building structures,\naircraft and spacecraft components, ships, ground-based vehicles, and medical\ndevices. Engineers need to estimate S-N relationships (Stress or Strain versus\nNumber of cycles to failure), typically with a focus on estimating small\nquantiles of the fatigue-life distribution. Estimates from this kind of model\nare used as input to models (e.g., cumulative damage models) that predict\nfailure-time distributions under varying stress patterns. Also, design\nengineers need to estimate lower-tail quantiles of the closely related\nfatigue-strength distribution. The history of applying incorrect statistical\nmethods is nearly as long and such practices continue to the present. Examples\ninclude treating the applied stress (or strain) as the response and the number\nof cycles to failure as the explanatory variable in regression analyses\n(because of the need to estimate strength distributions) and ignoring or\notherwise mishandling censored observations (known as runouts in the fatigue\nliterature). The first part of the paper reviews the traditional modeling\napproach where a fatigue-life model is specified. We then show how this\nspecification induces a corresponding fatigue-strength model. The second part\nof the paper presents a novel alternative modeling approach where a\nfatigue-strength model is specified and a corresponding fatigue-life model is\ninduced. We explain and illustrate the important advantages of this new\nmodeling approach."}, "http://arxiv.org/abs/2303.01186": {"title": "Discrete-time Competing-Risks Regression with or without Penalization", "link": "http://arxiv.org/abs/2303.01186", "description": "Many studies employ the analysis of time-to-event data that incorporates\ncompeting risks and right censoring. Most methods and software packages are\ngeared towards analyzing data that comes from a continuous failure time\ndistribution. However, failure-time data may sometimes be discrete either\nbecause time is inherently discrete or due to imprecise measurement. This paper\nintroduces a novel estimation procedure for discrete-time survival analysis\nwith competing events. The proposed approach offers two key advantages over\nexisting procedures: first, it expedites the estimation process for a large\nnumber of unique failure time points; second, it allows for straightforward\nintegration and application of widely used regularized regression and screening\nmethods. We illustrate the benefits of our proposed approach by conducting a\ncomprehensive simulation study. Additionally, we showcase the utility of our\nprocedure by estimating a survival model for the length of stay of patients\nhospitalized in the intensive care unit, considering three competing events:\ndischarge to home, transfer to another medical facility, and in-hospital death."}, "http://arxiv.org/abs/2306.11281": {"title": "Towards Characterizing Domain Counterfactuals For Invertible Latent Causal Models", "link": "http://arxiv.org/abs/2306.11281", "description": "Answering counterfactual queries has many important applications such as\nknowledge discovery and explainability, but is challenging when causal\nvariables are unobserved and we only see a projection onto an observation\nspace, for instance, image pixels. One approach is to recover the latent\nStructural Causal Model (SCM), but this typically needs unrealistic\nassumptions, such as linearity of the causal mechanisms. Another approach is to\nuse na\\\"ive ML approximations, such as generative models, to generate\ncounterfactual samples; however, these lack guarantees of accuracy. In this\nwork, we strive to strike a balance between practicality and theoretical\nguarantees by focusing on a specific type of causal query called domain\ncounterfactuals, which hypothesizes what a sample would have looked like if it\nhad been generated in a different domain (or environment). Concretely, by only\nassuming invertibility, sparse domain interventions and access to observational\ndata from different domains, we aim to improve domain counterfactual estimation\nboth theoretically and practically with less restrictive assumptions. We define\ndomain counterfactually equivalent models and prove necessary and sufficient\nproperties for equivalent models that provide a tight characterization of the\ndomain counterfactual equivalence classes. Building upon this result, we prove\nthat every equivalence class contains a model where all intervened variables\nare at the end when topologically sorted by the causal DAG. This surprising\nresult suggests that a model design that only allows intervention in the last\n$k$ latent variables may improve model estimation for counterfactuals. We then\ntest this model design on extensive simulated and image-based experiments which\nshow the sparse canonical model indeed improves counterfactual estimation over\nbaseline non-sparse models."}, "http://arxiv.org/abs/2309.10378": {"title": "Group Spike and Slab Variational Bayes", "link": "http://arxiv.org/abs/2309.10378", "description": "We introduce Group Spike-and-slab Variational Bayes (GSVB), a scalable method\nfor group sparse regression. A fast co-ordinate ascent variational inference\n(CAVI) algorithm is developed for several common model families including\nGaussian, Binomial and Poisson. Theoretical guarantees for our proposed\napproach are provided by deriving contraction rates for the variational\nposterior in grouped linear regression. Through extensive numerical studies, we\ndemonstrate that GSVB provides state-of-the-art performance, offering a\ncomputationally inexpensive substitute to MCMC, whilst performing comparably or\nbetter than existing MAP methods. Additionally, we analyze three real world\ndatasets wherein we highlight the practical utility of our method,\ndemonstrating that GSVB provides parsimonious models with excellent predictive\nperformance, variable selection and uncertainty quantification."}, "http://arxiv.org/abs/2309.12632": {"title": "Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?", "link": "http://arxiv.org/abs/2309.12632", "description": "Following the great success of various deep learning methods in image and\nobject classification, the biomedical image processing society is also\noverwhelmed with their applications to various automatic diagnosis cases.\nUnfortunately, most of the deep learning-based classification attempts in the\nliterature solely focus on the aim of extreme accuracy scores, without\nconsidering interpretability, or patient-wise separation of training and test\ndata. For example, most lung nodule classification papers using deep learning\nrandomly shuffle data and split it into training, validation, and test sets,\ncausing certain images from the CT scan of a person to be in the training set,\nwhile other images of the exact same person to be in the validation or testing\nimage sets. This can result in reporting misleading accuracy rates and the\nlearning of irrelevant features, ultimately reducing the real-life usability of\nthese models. When the deep neural networks trained on the traditional, unfair\ndata shuffling method are challenged with new patient images, it is observed\nthat the trained models perform poorly. In contrast, deep neural networks\ntrained with strict patient-level separation maintain their accuracy rates even\nwhen new patient images are tested. Heat-map visualizations of the activations\nof the deep neural networks trained with strict patient-level separation\nindicate a higher degree of focus on the relevant nodules. We argue that the\nresearch question posed in the title has a positive answer only if the deep\nneural networks are trained with images of patients that are strictly isolated\nfrom the validation and testing patient sets."}}