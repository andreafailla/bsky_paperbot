{"http://arxiv.org/abs/2310.03114": {"title": "Bayesian Parameter Inference for Partially Observed Stochastic Volterra Equations", "link": "http://arxiv.org/abs/2310.03114", "description": "In this article we consider Bayesian parameter inference for a type of\npartially observed stochastic Volterra equation (SVE). SVEs are found in many\nareas such as physics and mathematical finance. In the latter field they can be\nused to represent long memory in unobserved volatility processes. In many cases\nof practical interest, SVEs must be time-discretized and then parameter\ninference is based upon the posterior associated to this time-discretized\nprocess. Based upon recent studies on time-discretization of SVEs (e.g. Richard\net al. 2021), we use Euler-Maruyama methods for the afore-mentioned\ndiscretization. We then show how multilevel Markov chain Monte Carlo (MCMC)\nmethods (Jasra et al. 2018) can be applied in this context. In the examples we\nstudy, we give a proof that shows that the cost to achieve a mean square error\n(MSE) of $\\mathcal{O}(\\epsilon^2)$, $\\epsilon&gt;0$, is\n$\\mathcal{O}(\\epsilon^{-20/9})$. If one uses a single level MCMC method then\nthe cost is $\\mathcal{O}(\\epsilon^{-38/9})$ to achieve the same MSE. We\nillustrate these results in the context of state-space and stochastic\nvolatility models, with the latter applied to real data."}, "http://arxiv.org/abs/2310.03164": {"title": "A Hierarchical Random Effects State-space Model for Modeling Brain Activities from Electroencephalogram Data", "link": "http://arxiv.org/abs/2310.03164", "description": "Mental disorders present challenges in diagnosis and treatment due to their\ncomplex and heterogeneous nature. Electroencephalogram (EEG) has shown promise\nas a potential biomarker for these disorders. However, existing methods for\nanalyzing EEG signals have limitations in addressing heterogeneity and\ncapturing complex brain activity patterns between regions. This paper proposes\na novel random effects state-space model (RESSM) for analyzing large-scale\nmulti-channel resting-state EEG signals, accounting for the heterogeneity of\nbrain connectivities between groups and individual subjects. We incorporate\nmulti-level random effects for temporal dynamical and spatial mapping matrices\nand address nonstationarity so that the brain connectivity patterns can vary\nover time. The model is fitted under a Bayesian hierarchical model framework\ncoupled with a Gibbs sampler. Compared to previous mixed-effects state-space\nmodels, we directly model high-dimensional random effects matrices without\nstructural constraints and tackle the challenge of identifiability. Through\nextensive simulation studies, we demonstrate that our approach yields valid\nestimation and inference. We apply RESSM to a multi-site clinical trial of\nMajor Depressive Disorder (MDD). Our analysis uncovers significant differences\nin resting-state brain temporal dynamics among MDD patients compared to healthy\nindividuals. In addition, we show the subject-level EEG features derived from\nRESSM exhibit a superior predictive value for the heterogeneous treatment\neffect compared to the EEG frequency band power, suggesting the potential of\nEEG as a valuable biomarker for MDD."}, "http://arxiv.org/abs/2310.03258": {"title": "Detecting Electricity Service Equity Issues with Transfer Counterfactual Learning on Large-Scale Outage Datasets", "link": "http://arxiv.org/abs/2310.03258", "description": "Energy justice is a growing area of interest in interdisciplinary energy\nresearch. However, identifying systematic biases in the energy sector remains\nchallenging due to confounding variables, intricate heterogeneity in treatment\neffects, and limited data availability. To address these challenges, we\nintroduce a novel approach for counterfactual causal analysis centered on\nenergy justice. We use subgroup analysis to manage diverse factors and leverage\nthe idea of transfer learning to mitigate data scarcity in each subgroup. In\nour numerical analysis, we apply our method to a large-scale customer-level\npower outage data set and investigate the counterfactual effect of demographic\nfactors, such as income and age of the population, on power outage durations.\nOur results indicate that low-income and elderly-populated areas consistently\nexperience longer power outages, regardless of weather conditions. This points\nto existing biases in the power system and highlights the need for focused\nimprovements in areas with economic challenges."}, "http://arxiv.org/abs/2310.03351": {"title": "Efficiently analyzing large patient registries with Bayesian joint models for longitudinal and time-to-event data", "link": "http://arxiv.org/abs/2310.03351", "description": "The joint modeling of longitudinal and time-to-event outcomes has become a\npopular tool in follow-up studies. However, fitting Bayesian joint models to\nlarge datasets, such as patient registries, can require extended computing\ntimes. To speed up sampling, we divided a patient registry dataset into\nsubsamples, analyzed them in parallel, and combined the resulting Markov chain\nMonte Carlo draws into a consensus distribution. We used a simulation study to\ninvestigate how different consensus strategies perform with joint models. In\nparticular, we compared grouping all draws together with using equal- and\nprecision-weighted averages. We considered scenarios reflecting different\nsample sizes, numbers of data splits, and processor characteristics.\nParallelization of the sampling process substantially decreased the time\nrequired to run the model. We found that the weighted-average consensus\ndistributions for large sample sizes were nearly identical to the target\nposterior distribution. The proposed algorithm has been made available in an R\npackage for joint models, JMbayes2. This work was motivated by the clinical\ninterest in investigating the association between ppFEV1, a commonly measured\nmarker of lung function, and the risk of lung transplant or death, using data\nfrom the US Cystic Fibrosis Foundation Patient Registry (35,153 individuals\nwith 372,366 years of cumulative follow-up). Splitting the registry into five\nsubsamples resulted in an 85\\% decrease in computing time, from 9.22 to 1.39\nhours. Splitting the data and finding a consensus distribution by\nprecision-weighted averaging proved to be a computationally efficient and\nrobust approach to handling large datasets under the joint modeling framework."}, "http://arxiv.org/abs/2310.03521": {"title": "Cutting Feedback in Misspecified Copula Models", "link": "http://arxiv.org/abs/2310.03521", "description": "In copula models the marginal distributions and copula function are specified\nseparately. We treat these as two modules in a modular Bayesian inference\nframework, and propose conducting modified Bayesian inference by ``cutting\nfeedback''. Cutting feedback limits the influence of potentially misspecified\nmodules in posterior inference. We consider two types of cuts. The first limits\nthe influence of a misspecified copula on inference for the marginals, which is\na Bayesian analogue of the popular Inference for Margins (IFM) estimator. The\nsecond limits the influence of misspecified marginals on inference for the\ncopula parameters by using a rank likelihood to define the cut model. We\nestablish that if only one of the modules is misspecified, then the appropriate\ncut posterior gives accurate uncertainty quantification asymptotically for the\nparameters in the other module. Computation of the cut posteriors is difficult,\nand new variational inference methods to do so are proposed. The efficacy of\nthe new methodology is demonstrated using both simulated data and a substantive\nmultivariate time series application from macroeconomic forecasting. In the\nlatter, cutting feedback from misspecified marginals to a 1096 dimension copula\nimproves posterior inference and predictive accuracy greatly, compared to\nconventional Bayesian inference."}, "http://arxiv.org/abs/2310.03630": {"title": "Model-based Clustering for Network Data via a Latent Shrinkage Position Cluster Model", "link": "http://arxiv.org/abs/2310.03630", "description": "Low-dimensional representation and clustering of network data are tasks of\ngreat interest across various fields. Latent position models are routinely used\nfor this purpose by assuming that each node has a location in a low-dimensional\nlatent space, and enabling node clustering. However, these models fall short in\nsimultaneously determining the optimal latent space dimension and the number of\nclusters. Here we introduce the latent shrinkage position cluster model\n(LSPCM), which addresses this limitation. The LSPCM posits a Bayesian\nnonparametric shrinkage prior on the latent positions' variance parameters\nresulting in higher dimensions having increasingly smaller variances, aiding in\nthe identification of dimensions with non-negligible variance. Further, the\nLSPCM assumes the latent positions follow a sparse finite Gaussian mixture\nmodel, allowing for automatic inference on the number of clusters related to\nnon-empty mixture components. As a result, the LSPCM simultaneously infers the\nlatent space dimensionality and the number of clusters, eliminating the need to\nfit and compare multiple models. The performance of the LSPCM is assessed via\nsimulation studies and demonstrated through application to two real Twitter\nnetwork datasets from sporting and political contexts. Open source software is\navailable to promote widespread use of the LSPCM."}, "http://arxiv.org/abs/2310.03722": {"title": "Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance", "link": "http://arxiv.org/abs/2310.03722", "description": "In 1976, Lai constructed a nontrivial confidence sequence for the mean $\\mu$\nof a Gaussian distribution with unknown variance $\\sigma$. Curiously, he\nemployed both an improper (right Haar) mixture over $\\sigma$ and an improper\n(flat) mixture over $\\mu$. Here, we elaborate carefully on the details of his\nconstruction, which use generalized nonintegrable martingales and an extended\nVille's inequality. While this does yield a sequential t-test, it does not\nyield an ``e-process'' (due to the nonintegrability of his martingale). In this\npaper, we develop two new e-processes and confidence sequences for the same\nsetting: one is a test martingale in a reduced filtration, while the other is\nan e-process in the canonical data filtration. These are respectively obtained\nby swapping Lai's flat mixture for a Gaussian mixture, and swapping the right\nHaar mixture over $\\sigma$ with the maximum likelihood estimate under the null,\nas done in universal inference. We also analyze the width of resulting\nconfidence sequences, which have a curious dependence on the error probability\n$\\alpha$. Numerical experiments are provided along the way to compare and\ncontrast the various approaches."}, "http://arxiv.org/abs/2103.10875": {"title": "Scalable Bayesian computation for crossed and nested hierarchical models", "link": "http://arxiv.org/abs/2103.10875", "description": "We develop sampling algorithms to fit Bayesian hierarchical models, the\ncomputational complexity of which scales linearly with the number of\nobservations and the number of parameters in the model. We focus on crossed\nrandom effect and nested multilevel models, which are used ubiquitously in\napplied sciences. The posterior dependence in both classes is sparse: in\ncrossed random effects models it resembles a random graph, whereas in nested\nmultilevel models it is tree-structured. For each class we identify a framework\nfor scalable computation, building on previous work. Methods for crossed models\nare based on extensions of appropriately designed collapsed Gibbs samplers,\nwhere we introduce the idea of local centering; while methods for nested models\nare based on sparse linear algebra and data augmentation. We provide a\ntheoretical analysis of the proposed algorithms in some simplified settings,\nincluding a comparison with previously proposed methodologies and an\naverage-case analysis based on random graph theory. Numerical experiments,\nincluding two challenging real data analyses on predicting electoral results\nand real estate prices, compare with off-the-shelf Hamiltonian Monte Carlo,\ndisplaying drastic improvement in performance."}, "http://arxiv.org/abs/2106.04106": {"title": "A Regression-based Approach to Robust Estimation and Inference for Genetic Covariance", "link": "http://arxiv.org/abs/2106.04106", "description": "Genome-wide association studies (GWAS) have identified thousands of genetic\nvariants associated with complex traits, and some variants are shown to be\nassociated with multiple complex traits. Genetic covariance between two traits\nis defined as the underlying covariance of genetic effects and can be used to\nmeasure the shared genetic architecture. The data used to estimate such a\ngenetic covariance can be from the same group or different groups of\nindividuals, and the traits can be of different types or collected based on\ndifferent study designs. This paper proposes a unified regression-based\napproach to robust estimation and inference for genetic covariance of general\ntraits that may be associated with genetic variants nonlinearly. The asymptotic\nproperties of the proposed estimator are provided and are shown to be robust\nunder certain model mis-specification. Our method under linear working models\nprovides a robust inference for the narrow-sense genetic covariance, even when\nboth linear models are mis-specified. Numerical experiments are performed to\nsupport the theoretical results. Our method is applied to an outbred mice GWAS\ndata set to study the overlapping genetic effects between the behavioral and\nphysiological phenotypes. The real data results reveal interesting genetic\ncovariance among different mice developmental traits."}, "http://arxiv.org/abs/2112.08417": {"title": "Characterization of causal ancestral graphs for time series with latent confounders", "link": "http://arxiv.org/abs/2112.08417", "description": "In this paper, we introduce a novel class of graphical models for\nrepresenting time lag specific causal relationships and independencies of\nmultivariate time series with unobserved confounders. We completely\ncharacterize these graphs and show that they constitute proper subsets of the\ncurrently employed model classes. As we show, from the novel graphs one can\nthus draw stronger causal inferences -- without additional assumptions. We\nfurther introduce a graphical representation of Markov equivalence classes of\nthe novel graphs. This graphical representation contains more causal knowledge\nthan what current state-of-the-art causal discovery algorithms learn."}, "http://arxiv.org/abs/2112.09313": {"title": "Federated Adaptive Causal Estimation (FACE) of Target Treatment Effects", "link": "http://arxiv.org/abs/2112.09313", "description": "Federated learning of causal estimands may greatly improve estimation\nefficiency by leveraging data from multiple study sites, but robustness to\nheterogeneity and model misspecifications is vital for ensuring validity. We\ndevelop a Federated Adaptive Causal Estimation (FACE) framework to incorporate\nheterogeneous data from multiple sites to provide treatment effect estimation\nand inference for a flexibly specified target population of interest. FACE\naccounts for site-level heterogeneity in the distribution of covariates through\ndensity ratio weighting. To safely incorporate source sites and avoid negative\ntransfer, we introduce an adaptive weighting procedure via a penalized\nregression, which achieves both consistency and optimal efficiency. Our\nstrategy is communication-efficient and privacy-preserving, allowing\nparticipating sites to share summary statistics only once with other sites. We\nconduct both theoretical and numerical evaluations of FACE and apply it to\nconduct a comparative effectiveness study of BNT162b2 (Pfizer) and mRNA-1273\n(Moderna) vaccines on COVID-19 outcomes in U.S. veterans using electronic\nhealth records from five VA regional sites. We show that compared to\ntraditional methods, FACE meaningfully increases the precision of treatment\neffect estimates, with reductions in standard errors ranging from $26\\%$ to\n$67\\%$."}, "http://arxiv.org/abs/2208.03246": {"title": "Non-Asymptotic Analysis of Ensemble Kalman Updates: Effective Dimension and Localization", "link": "http://arxiv.org/abs/2208.03246", "description": "Many modern algorithms for inverse problems and data assimilation rely on\nensemble Kalman updates to blend prior predictions with observed data. Ensemble\nKalman methods often perform well with a small ensemble size, which is\nessential in applications where generating each particle is costly. This paper\ndevelops a non-asymptotic analysis of ensemble Kalman updates that rigorously\nexplains why a small ensemble size suffices if the prior covariance has\nmoderate effective dimension due to fast spectrum decay or approximate\nsparsity. We present our theory in a unified framework, comparing several\nimplementations of ensemble Kalman updates that use perturbed observations,\nsquare root filtering, and localization. As part of our analysis, we develop\nnew dimension-free covariance estimation bounds for approximately sparse\nmatrices that may be of independent interest."}, "http://arxiv.org/abs/2307.10972": {"title": "Adaptively Weighted Audits of Instant-Runoff Voting Elections: AWAIRE", "link": "http://arxiv.org/abs/2307.10972", "description": "An election audit is risk-limiting if the audit limits (to a pre-specified\nthreshold) the chance that an erroneous electoral outcome will be certified.\nExtant methods for auditing instant-runoff voting (IRV) elections are either\nnot risk-limiting or require cast vote records (CVRs), the voting system's\nelectronic record of the votes on each ballot. CVRs are not always available,\nfor instance, in jurisdictions that tabulate IRV contests manually.\n\nWe develop an RLA method (AWAIRE) that uses adaptively weighted averages of\ntest supermartingales to efficiently audit IRV elections when CVRs are not\navailable. The adaptive weighting 'learns' an efficient set of hypotheses to\ntest to confirm the election outcome. When accurate CVRs are available, AWAIRE\ncan use them to increase the efficiency to match the performance of existing\nmethods that require CVRs.\n\nWe provide an open-source prototype implementation that can handle elections\nwith up to six candidates. Simulations using data from real elections show that\nAWAIRE is likely to be efficient in practice. We discuss how to extend the\ncomputational approach to handle elections with more candidates.\n\nAdaptively weighted averages of test supermartingales are a general tool,\nuseful beyond election audits to test collections of hypotheses sequentially\nwhile rigorously controlling the familywise error rate."}, "http://arxiv.org/abs/2309.10514": {"title": "Partially Specified Causal Simulations", "link": "http://arxiv.org/abs/2309.10514", "description": "Simulation studies play a key role in the validation of causal inference\nmethods. The simulation results are reliable only if the study is designed\naccording to the promised operational conditions of the method-in-test. Still,\nmany causal inference literature tend to design over-restricted or misspecified\nstudies. In this paper, we elaborate on the problem of improper simulation\ndesign for causal methods and compile a list of desiderata for an effective\nsimulation framework. We then introduce partially randomized causal simulation\n(PARCS), a simulation framework that meets those desiderata. PARCS synthesizes\ndata based on graphical causal models and a wide range of adjustable\nparameters. There is a legible mapping from usual causal assumptions to the\nparameters, thus, users can identify and specify the subset of related\nparameters and randomize the remaining ones to generate a range of complying\ndata-generating processes for their causal method. The result is a more\ncomprehensive and inclusive empirical investigation for causal claims. Using\nPARCS, we reproduce and extend the simulation studies of two well-known causal\ndiscovery and missing data analysis papers to emphasize the necessity of a\nproper simulation design. Our results show that those papers would have\nimproved and extended the findings, had they used PARCS for simulation. The\nframework is implemented as a Python package, too. By discussing the\ncomprehensiveness and transparency of PARCS, we encourage causal inference\nresearchers to utilize it as a standard tool for future works."}, "http://arxiv.org/abs/2310.03776": {"title": "Significance of the negative binomial distribution in multiplicity phenomena", "link": "http://arxiv.org/abs/2310.03776", "description": "The negative binomial distribution (NBD) has been theorized to express a\nscale-invariant property of many-body systems and has been consistently shown\nto outperform other statistical models in both describing the multiplicity of\nquantum-scale events in particle collision experiments and predicting the\nprevalence of cosmological observables, such as the number of galaxies in a\nregion of space. Despite its widespread applicability and empirical success in\nthese contexts, a theoretical justification for the NBD from first principles\nhas remained elusive for fifty years. The accuracy of the NBD in modeling\nhadronic, leptonic, and semileptonic processes is suggestive of a highly\ngeneral principle, which is yet to be understood. This study demonstrates that\na statistical event of the NBD can in fact be derived in a general context via\nthe dynamical equations of a canonical ensemble of particles in Minkowski\nspace. These results describe a fundamental feature of many-body systems that\nis consistent with data from the ALICE and ATLAS experiments and provides an\nexplanation for the emergence of the NBD in these multiplicity observations.\nTwo methods are used to derive this correspondence: the Feynman path integral\nand a hypersurface parametrization of a propagating ensemble."}, "http://arxiv.org/abs/2310.04030": {"title": "Robust inference with GhostKnockoffs in genome-wide association studies", "link": "http://arxiv.org/abs/2310.04030", "description": "Genome-wide association studies (GWASs) have been extensively adopted to\ndepict the underlying genetic architecture of complex diseases. Motivated by\nGWASs' limitations in identifying small effect loci to understand complex\ntraits' polygenicity and fine-mapping putative causal variants from proxy ones,\nwe propose a knockoff-based method which only requires summary statistics from\nGWASs and demonstrate its validity in the presence of relatedness. We show that\nGhostKnockoffs inference is robust to its input Z-scores as long as they are\nfrom valid marginal association tests and their correlations are consistent\nwith the correlations among the corresponding genetic variants. The property\ngeneralizes GhostKnockoffs to other GWASs settings, such as the meta-analysis\nof multiple overlapping studies and studies based on association test\nstatistics deviated from score tests. We demonstrate GhostKnockoffs'\nperformance using empirical simulation and a meta-analysis of nine European\nancestral genome-wide association studies and whole exome/genome sequencing\nstudies. Both results demonstrate that GhostKnockoffs identify more putative\ncausal variants with weak genotype-phenotype associations that are missed by\nconventional GWASs."}, "http://arxiv.org/abs/2310.04082": {"title": "An energy-based model approach to rare event probability estimation", "link": "http://arxiv.org/abs/2310.04082", "description": "The estimation of rare event probabilities plays a pivotal role in diverse\nfields. Our aim is to determine the probability of a hazard or system failure\noccurring when a quantity of interest exceeds a critical value. In our\napproach, the distribution of the quantity of interest is represented by an\nenergy density, characterized by a free energy function. To efficiently\nestimate the free energy, a bias potential is introduced. Using concepts from\nenergy-based models (EBM), this bias potential is optimized such that the\ncorresponding probability density function approximates a pre-defined\ndistribution targeting the failure region of interest. Given the optimal bias\npotential, the free energy function and the rare event probability of interest\ncan be determined. The approach is applicable not just in traditional rare\nevent settings where the variable upon which the quantity of interest relies\nhas a known distribution, but also in inversion settings where the variable\nfollows a posterior distribution. By combining the EBM approach with a Stein\ndiscrepancy-based stopping criterion, we aim for a balanced accuracy-efficiency\ntrade-off. Furthermore, we explore both parametric and non-parametric\napproaches for the bias potential, with the latter eliminating the need for\nchoosing a particular parameterization, but depending strongly on the accuracy\nof the kernel density estimate used in the optimization process. Through three\nillustrative test cases encompassing both traditional and inversion settings,\nwe show that the proposed EBM approach, when properly configured, (i) allows\nstable and efficient estimation of rare event probabilities and (ii) compares\nfavorably against subset sampling approaches."}, "http://arxiv.org/abs/2310.04165": {"title": "When Composite Likelihood Meets Stochastic Approximation", "link": "http://arxiv.org/abs/2310.04165", "description": "A composite likelihood is an inference function derived by multiplying a set\nof likelihood components. This approach provides a flexible framework for\ndrawing inference when the likelihood function of a statistical model is\ncomputationally intractable. While composite likelihood has computational\nadvantages, it can still be demanding when dealing with numerous likelihood\ncomponents and a large sample size. This paper tackles this challenge by\nemploying an approximation of the conventional composite likelihood estimator,\nwhich is derived from an optimization procedure relying on stochastic\ngradients. This novel estimator is shown to be asymptotically normally\ndistributed around the true parameter. In particular, based on the relative\ndivergent rate of the sample size and the number of iterations of the\noptimization, the variance of the limiting distribution is shown to compound\nfor two sources of uncertainty: the sampling variability of the data and the\noptimization noise, with the latter depending on the sampling distribution used\nto construct the stochastic gradients. The advantages of the proposed framework\nare illustrated through simulation studies on two working examples: an Ising\nmodel for binary data and a gamma frailty model for count data. Finally, a\nreal-data application is presented, showing its effectiveness in a large-scale\nmental health survey."}, "http://arxiv.org/abs/1904.06340": {"title": "A Composite Likelihood-based Approach for Change-point Detection in Spatio-temporal Processes", "link": "http://arxiv.org/abs/1904.06340", "description": "This paper develops a unified and computationally efficient method for\nchange-point estimation along the time dimension in a non-stationary\nspatio-temporal process. By modeling a non-stationary spatio-temporal process\nas a piecewise stationary spatio-temporal process, we consider simultaneous\nestimation of the number and locations of change-points, and model parameters\nin each segment. A composite likelihood-based criterion is developed for\nchange-point and parameters estimation. Under the framework of increasing\ndomain asymptotics, theoretical results including consistency and distribution\nof the estimators are derived under mild conditions. In contrast to classical\nresults in fixed dimensional time series that the localization error of\nchange-point estimator is $O_{p}(1)$, exact recovery of true change-points can\nbe achieved in the spatio-temporal setting. More surprisingly, the consistency\nof change-point estimation can be achieved without any penalty term in the\ncriterion function. In addition, we further establish consistency of the number\nand locations of the change-point estimator under the infill asymptotics\nframework where the time domain is increasing while the spatial sampling domain\nis fixed. A computationally efficient pruned dynamic programming algorithm is\ndeveloped for the challenging criterion optimization problem. Extensive\nsimulation studies and an application to U.S. precipitation data are provided\nto demonstrate the effectiveness and practicality of the proposed method."}, "http://arxiv.org/abs/2201.12936": {"title": "Pigeonhole Design: Balancing Sequential Experiments from an Online Matching Perspective", "link": "http://arxiv.org/abs/2201.12936", "description": "Practitioners and academics have long appreciated the benefits of covariate\nbalancing when they conduct randomized experiments. For web-facing firms\nrunning online A/B tests, however, it still remains challenging in balancing\ncovariate information when experimental subjects arrive sequentially. In this\npaper, we study an online experimental design problem, which we refer to as the\n\"Online Blocking Problem.\" In this problem, experimental subjects with\nheterogeneous covariate information arrive sequentially and must be immediately\nassigned into either the control or the treated group. The objective is to\nminimize the total discrepancy, which is defined as the minimum weight perfect\nmatching between the two groups. To solve this problem, we propose a randomized\ndesign of experiment, which we refer to as the \"Pigeonhole Design.\" The\npigeonhole design first partitions the covariate space into smaller spaces,\nwhich we refer to as pigeonholes, and then, when the experimental subjects\narrive at each pigeonhole, balances the number of control and treated subjects\nfor each pigeonhole. We analyze the theoretical performance of the pigeonhole\ndesign and show its effectiveness by comparing against two well-known benchmark\ndesigns: the match-pair design and the completely randomized design. We\nidentify scenarios when the pigeonhole design demonstrates more benefits over\nthe benchmark design. To conclude, we conduct extensive simulations using\nYahoo! data to show a 10.2% reduction in variance if we use the pigeonhole\ndesign to estimate the average treatment effect."}, "http://arxiv.org/abs/2208.00137": {"title": "Efficient estimation and inference for the signed $\\beta$-model in directed signed networks", "link": "http://arxiv.org/abs/2208.00137", "description": "This paper proposes a novel signed $\\beta$-model for directed signed network,\nwhich is frequently encountered in application domains but largely neglected in\nliterature. The proposed signed $\\beta$-model decomposes a directed signed\nnetwork as the difference of two unsigned networks and embeds each node with\ntwo latent factors for in-status and out-status. The presence of negative edges\nleads to a non-concave log-likelihood, and a one-step estimation algorithm is\ndeveloped to facilitate parameter estimation, which is efficient both\ntheoretically and computationally. We also develop an inferential procedure for\npairwise and multiple node comparisons under the signed $\\beta$-model, which\nfills the void of lacking uncertainty quantification for node ranking.\nTheoretical results are established for the coverage probability of confidence\ninterval, as well as the false discovery rate (FDR) control for multiple node\ncomparison. The finite sample performance of the signed $\\beta$-model is also\nexamined through extensive numerical experiments on both synthetic and\nreal-life networks."}, "http://arxiv.org/abs/2208.08401": {"title": "Conformal Inference for Online Prediction with Arbitrary Distribution Shifts", "link": "http://arxiv.org/abs/2208.08401", "description": "We consider the problem of forming prediction sets in an online setting where\nthe distribution generating the data is allowed to vary over time. Previous\napproaches to this problem suffer from over-weighting historical data and thus\nmay fail to quickly react to the underlying dynamics. Here we correct this\nissue and develop a novel procedure with provably small regret over all local\ntime intervals of a given width. We achieve this by modifying the adaptive\nconformal inference (ACI) algorithm of Gibbs and Cand\\`{e}s (2021) to contain\nan additional step in which the step-size parameter of ACI's gradient descent\nupdate is tuned over time. Crucially, this means that unlike ACI, which\nrequires knowledge of the rate of change of the data-generating mechanism, our\nnew procedure is adaptive to both the size and type of the distribution shift.\nOur methods are highly flexible and can be used in combination with any\nbaseline predictive algorithm that produces point estimates or estimated\nquantiles of the target without the need for distributional assumptions. We\ntest our techniques on two real-world datasets aimed at predicting stock market\nvolatility and COVID-19 case counts and find that they are robust and adaptive\nto real-world distribution shifts."}, "http://arxiv.org/abs/2303.01031": {"title": "Identifiability and Consistent Estimation of the Gaussian Chain Graph Model", "link": "http://arxiv.org/abs/2303.01031", "description": "The chain graph model admits both undirected and directed edges in one graph,\nwhere symmetric conditional dependencies are encoded via undirected edges and\nasymmetric causal relations are encoded via directed edges. Though frequently\nencountered in practice, the chain graph model has been largely under\ninvestigated in literature, possibly due to the lack of identifiability\nconditions between undirected and directed edges. In this paper, we first\nestablish a set of novel identifiability conditions for the Gaussian chain\ngraph model, exploiting a low rank plus sparse decomposition of the precision\nmatrix. Further, an efficient learning algorithm is built upon the\nidentifiability conditions to fully recover the chain graph structure.\nTheoretical analysis on the proposed method is conducted, assuring its\nasymptotic consistency in recovering the exact chain graph structure. The\nadvantage of the proposed method is also supported by numerical experiments on\nboth simulated examples and a real application on the Standard &amp; Poor 500 index\ndata."}, "http://arxiv.org/abs/2305.10817": {"title": "Robust inference of causality in high-dimensional dynamical processes from the Information Imbalance of distance ranks", "link": "http://arxiv.org/abs/2305.10817", "description": "We introduce an approach which allows detecting causal relationships between\nvariables for which the time evolution is available. Causality is assessed by a\nvariational scheme based on the Information Imbalance of distance ranks, a\nstatistical test capable of inferring the relative information content of\ndifferent distance measures. We test whether the predictability of a putative\ndriven system Y can be improved by incorporating information from a potential\ndriver system X, without making assumptions on the underlying dynamics and\nwithout the need to compute probability densities of the dynamic variables.\nThis framework makes causality detection possible even for high-dimensional\nsystems where only few of the variables are known or measured. Benchmark tests\non coupled chaotic dynamical systems demonstrate that our approach outperforms\nother model-free causality detection methods, successfully handling both\nunidirectional and bidirectional couplings. We also show that the method can be\nused to robustly detect causality in human electroencephalography data."}, "http://arxiv.org/abs/2309.06264": {"title": "Spectral clustering algorithm for the allometric extension model", "link": "http://arxiv.org/abs/2309.06264", "description": "The spectral clustering algorithm is often used as a binary clustering method\nfor unclassified data by applying the principal component analysis. To study\ntheoretical properties of the algorithm, the assumption of conditional\nhomoscedasticity is often supposed in existing studies. However, this\nassumption is restrictive and often unrealistic in practice. Therefore, in this\npaper, we consider the allometric extension model, that is, the directions of\nthe first eigenvectors of two covariance matrices and the direction of the\ndifference of two mean vectors coincide, and we provide a non-asymptotic bound\nof the error probability of the spectral clustering algorithm for the\nallometric extension model. As a byproduct of the result, we obtain the\nconsistency of the clustering method in high-dimensional settings."}, "http://arxiv.org/abs/2309.12833": {"title": "Model-based causal feature selection for general response types", "link": "http://arxiv.org/abs/2309.12833", "description": "Discovering causal relationships from observational data is a fundamental yet\nchallenging task. Invariant causal prediction (ICP, Peters et al., 2016) is a\nmethod for causal feature selection which requires data from heterogeneous\nsettings and exploits that causal models are invariant. ICP has been extended\nto general additive noise models and to nonparametric settings using\nconditional independence tests. However, the latter often suffer from low power\n(or poor type I error control) and additive noise models are not suitable for\napplications in which the response is not measured on a continuous scale, but\nreflects categories or counts. Here, we develop transformation-model (TRAM)\nbased ICP, allowing for continuous, categorical, count-type, and\nuninformatively censored responses (these model classes, generally, do not\nallow for identifiability when there is no exogenous heterogeneity). As an\ninvariance test, we propose TRAM-GCM based on the expected conditional\ncovariance between environments and score residuals with uniform asymptotic\nlevel guarantees. For the special case of linear shift TRAMs, we also consider\nTRAM-Wald, which tests invariance based on the Wald statistic. We provide an\nopen-source R package 'tramicp' and evaluate our approach on simulated data and\nin a case study investigating causal features of survival in critically ill\npatients."}, "http://arxiv.org/abs/2310.04452": {"title": "Short text classification with machine learning in the social sciences: The case of climate change on Twitter", "link": "http://arxiv.org/abs/2310.04452", "description": "To analyse large numbers of texts, social science researchers are\nincreasingly confronting the challenge of text classification. When manual\nlabeling is not possible and researchers have to find automatized ways to\nclassify texts, computer science provides a useful toolbox of machine-learning\nmethods whose performance remains understudied in the social sciences. In this\narticle, we compare the performance of the most widely used text classifiers by\napplying them to a typical research scenario in social science research: a\nrelatively small labeled dataset with infrequent occurrence of categories of\ninterest, which is a part of a large unlabeled dataset. As an example case, we\nlook at Twitter communication regarding climate change, a topic of increasing\nscholarly interest in interdisciplinary social science research. Using a novel\ndataset including 5,750 tweets from various international organizations\nregarding the highly ambiguous concept of climate change, we evaluate the\nperformance of methods in automatically classifying tweets based on whether\nthey are about climate change or not. In this context, we highlight two main\nfindings. First, supervised machine-learning methods perform better than\nstate-of-the-art lexicons, in particular as class balance increases. Second,\ntraditional machine-learning methods, such as logistic regression and random\nforest, perform similarly to sophisticated deep-learning methods, whilst\nrequiring much less training time and computational resources. The results have\nimportant implications for the analysis of short texts in social science\nresearch."}, "http://arxiv.org/abs/2310.04563": {"title": "Modeling the Risk of In-Person Instruction during the COVID-19 Pandemic", "link": "http://arxiv.org/abs/2310.04563", "description": "During the COVID-19 pandemic, implementing in-person indoor instruction in a\nsafe manner was a high priority for universities nationwide. To support this\neffort at the University, we developed a mathematical model for estimating the\nrisk of SARS-CoV-2 transmission in university classrooms. This model was used\nto design a safe classroom environment at the University during the COVID-19\npandemic that supported the higher occupancy levels needed to match\npre-pandemic numbers of in-person courses, despite a limited number of large\nclassrooms. A retrospective analysis at the end of the semester confirmed the\nmodel's assessment that the proposed classroom configuration would be safe. Our\nframework is generalizable and was also used to support reopening decisions at\nStanford University. In addition, our methods are flexible; our modeling\nframework was repurposed to plan for large university events and gatherings. We\nfound that our approach and methods work in a wide range of indoor settings and\ncould be used to support reopening planning across various industries, from\nsecondary schools to movie theaters and restaurants."}, "http://arxiv.org/abs/2310.04578": {"title": "TNDDR: Efficient and doubly robust estimation of COVID-19 vaccine effectiveness under the test-negative design", "link": "http://arxiv.org/abs/2310.04578", "description": "While the test-negative design (TND), which is routinely used for monitoring\nseasonal flu vaccine effectiveness (VE), has recently become integral to\nCOVID-19 vaccine surveillance, it is susceptible to selection bias due to\noutcome-dependent sampling. Some studies have addressed the identifiability and\nestimation of causal parameters under the TND, but efficiency bounds for\nnonparametric estimators of the target parameter under the unconfoundedness\nassumption have not yet been investigated. We propose a one-step doubly robust\nand locally efficient estimator called TNDDR (TND doubly robust), which\nutilizes sample splitting and can incorporate machine learning techniques to\nestimate the nuisance functions. We derive the efficient influence function\n(EIF) for the marginal expectation of the outcome under a vaccination\nintervention, explore the von Mises expansion, and establish the conditions for\n$\\sqrt{n}-$consistency, asymptotic normality and double robustness of TNDDR.\nThe proposed TNDDR is supported by both theoretical and empirical\njustifications, and we apply it to estimate COVID-19 VE in an administrative\ndataset of community-dwelling older people (aged $\\geq 60$y) in the province of\nQu\\'ebec, Canada."}, "http://arxiv.org/abs/2310.04660": {"title": "Balancing Weights for Causal Inference in Observational Factorial Studies", "link": "http://arxiv.org/abs/2310.04660", "description": "Many scientific questions in biomedical, environmental, and psychological\nresearch involve understanding the impact of multiple factors on outcomes.\nWhile randomized factorial experiments are ideal for this purpose,\nrandomization is infeasible in many empirical studies. Therefore, investigators\noften rely on observational data, where drawing reliable causal inferences for\nmultiple factors remains challenging. As the number of treatment combinations\ngrows exponentially with the number of factors, some treatment combinations can\nbe rare or even missing by chance in observed data, further complicating\nfactorial effects estimation. To address these challenges, we propose a novel\nweighting method tailored to observational studies with multiple factors. Our\napproach uses weighted observational data to emulate a randomized factorial\nexperiment, enabling simultaneous estimation of the effects of multiple factors\nand their interactions. Our investigations reveal a crucial nuance: achieving\nbalance among covariates, as in single-factor scenarios, is necessary but\ninsufficient for unbiasedly estimating factorial effects. Our findings suggest\nthat balancing the factors is also essential in multi-factor settings.\nMoreover, we extend our weighting method to handle missing treatment\ncombinations in observed data. Finally, we study the asymptotic behavior of the\nnew weighting estimators and propose a consistent variance estimator, providing\nreliable inferences on factorial effects in observational studies."}, "http://arxiv.org/abs/2310.04709": {"title": "Time-dependent mediators in survival analysis: Graphical representation of causal assumptions", "link": "http://arxiv.org/abs/2310.04709", "description": "We study time-dependent mediators in survival analysis using a treatment\nseparation approach due to Didelez [2019] and based on earlier work by Robins\nand Richardson [2011]. This approach avoids nested counterfactuals and\ncrossworld assumptions which are otherwise common in mediation analysis. The\ncausal model of treatment, mediators, covariates, confounders and outcome is\nrepresented by causal directed acyclic graphs (DAGs). However, the DAGs tend to\nbe very complex when we have measurements at a large number of time points. We\ntherefore suggest using so-called rolled graphs in which a node represents an\nentire coordinate process instead of a single random variable, leading us to\nfar simpler graphical representations. The rolled graphs are not necessarily\nacyclic; they can be analyzed by $\\delta$-separation which is the appropriate\ngraphical separation criterion in this class of graphs and analogous to\n$d$-separation. In particular, $\\delta$-separation is a graphical tool for\nevaluating if the conditions of the mediation analysis are met or if unmeasured\nconfounders influence the estimated effects. We also state a mediational\ng-formula. This is similar to the approach in Vansteelandt et al. [2019]\nalthough that paper has a different conceptual basis. Finally, we apply this\nframework to a statistical model based on a Cox model with an added treatment\neffect.survival analysis; mediation; causal inference; graphical models; local\nindependence graphs"}, "http://arxiv.org/abs/2310.04853": {"title": "On changepoint detection in functional data using empirical energy distance", "link": "http://arxiv.org/abs/2310.04853", "description": "We propose a novel family of test statistics to detect the presence of\nchangepoints in a sequence of dependent, possibly multivariate,\nfunctional-valued observations. Our approach allows to test for a very general\nclass of changepoints, including the \"classical\" case of changes in the mean,\nand even changes in the whole distribution. Our statistics are based on a\ngeneralisation of the empirical energy distance; we propose weighted\nfunctionals of the energy distance process, which are designed in order to\nenhance the ability to detect breaks occurring at sample endpoints. The\nlimiting distribution of the maximally selected version of our statistics\nrequires only the computation of the eigenvalues of the covariance function,\nthus being readily implementable in the most commonly employed packages, e.g.\nR. We show that, under the alternative, our statistics are able to detect\nchangepoints occurring even very close to the beginning/end of the sample. In\nthe presence of multiple changepoints, we propose a binary segmentation\nalgorithm to estimate the number of breaks and the locations thereof.\nSimulations show that our procedures work very well in finite samples. We\ncomplement our theory with applications to financial and temperature data."}, "http://arxiv.org/abs/2310.04919": {"title": "The Conditional Prediction Function: A Novel Technique to Control False Discovery Rate for Complex Models", "link": "http://arxiv.org/abs/2310.04919", "description": "In modern scientific research, the objective is often to identify which\nvariables are associated with an outcome among a large class of potential\npredictors. This goal can be achieved by selecting variables in a manner that\ncontrols the the false discovery rate (FDR), the proportion of irrelevant\npredictors among the selections. Knockoff filtering is a cutting-edge approach\nto variable selection that provides FDR control. Existing knockoff statistics\nfrequently employ linear models to assess relationships between features and\nthe response, but the linearity assumption is often violated in real world\napplications. This may result in poor power to detect truly prognostic\nvariables. We introduce a knockoff statistic based on the conditional\nprediction function (CPF), which can pair with state-of-art machine learning\npredictive models, such as deep neural networks. The CPF statistics can capture\nthe nonlinear relationships between predictors and outcomes while also\naccounting for correlation between features. We illustrate the capability of\nthe CPF statistics to provide superior power over common knockoff statistics\nwith continuous, categorical, and survival outcomes using repeated simulations.\nKnockoff filtering with the CPF statistics is demonstrated using (1) a\nresidential building dataset to select predictors for the actual sales prices\nand (2) the TCGA dataset to select genes that are correlated with disease\nstaging in lung cancer patients."}, "http://arxiv.org/abs/2310.04924": {"title": "Markov Chain Monte Carlo Significance Tests", "link": "http://arxiv.org/abs/2310.04924", "description": "Markov chain Monte Carlo significance tests were first introduced by Besag\nand Clifford in [4]. These methods produce statistical valid p-values in\nproblems where sampling from the null hypotheses is intractable. We give an\noverview of the methods of Besag and Clifford and some recent developments. A\nrange of examples and applications are discussed."}, "http://arxiv.org/abs/2310.04934": {"title": "UBSea: A Unified Community Detection Framework", "link": "http://arxiv.org/abs/2310.04934", "description": "Detecting communities in networks and graphs is an important task across many\ndisciplines such as statistics, social science and engineering. There are\ngenerally three different kinds of mixing patterns for the case of two\ncommunities: assortative mixing, disassortative mixing and core-periphery\nstructure. Modularity optimization is a classical way for fitting network\nmodels with communities. However, it can only deal with assortative mixing and\ndisassortative mixing when the mixing pattern is known and fails to discover\nthe core-periphery structure. In this paper, we extend modularity in a\nstrategic way and propose a new framework based on Unified Bigroups Standadized\nEdge-count Analysis (UBSea). It can address all the formerly mentioned\ncommunity mixing structures. In addition, this new framework is able to\nautomatically choose the mixing type to fit the networks. Simulation studies\nshow that the new framework has superb performance in a wide range of settings\nunder the stochastic block model and the degree-corrected stochastic block\nmodel. We show that the new approach produces consistent estimate of the\ncommunities under a suitable signal-to-noise-ratio condition, for the case of a\nblock model with two communities, for both undirected and directed networks.\nThe new method is illustrated through applications to several real-world\ndatasets."}, "http://arxiv.org/abs/2310.05049": {"title": "On Estimation of Optimal Dynamic Treatment Regimes with Multiple Treatments for Survival Data-With Application to Colorectal Cancer Study", "link": "http://arxiv.org/abs/2310.05049", "description": "Dynamic treatment regimes (DTR) are sequential decision rules corresponding\nto several stages of intervention. Each rule maps patients' covariates to\noptional treatments. The optimal dynamic treatment regime is the one that\nmaximizes the mean outcome of interest if followed by the overall population.\nMotivated by a clinical study on advanced colorectal cancer with traditional\nChinese medicine, we propose a censored C-learning (CC-learning) method to\nestimate the dynamic treatment regime with multiple treatments using survival\ndata. To address the challenges of multiple stages with right censoring, we\nmodify the backward recursion algorithm in order to adapt to the flexible\nnumber and timing of treatments. For handling the problem of multiple\ntreatments, we propose a framework from the classification perspective by\ntransferring the problem of optimization with multiple treatment comparisons\ninto an example-dependent cost-sensitive classification problem. With\nclassification and regression tree (CART) as the classifier, the CC-learning\nmethod can produce an estimated optimal DTR with good interpretability. We\ntheoretically prove the optimality of our method and numerically evaluate its\nfinite sample performances through simulation. With the proposed method, we\nidentify the interpretable tree treatment regimes at each stage for the\nadvanced colorectal cancer treatment data from Xiyuan Hospital."}, "http://arxiv.org/abs/2310.05151": {"title": "Sequential linear regression for conditional mean imputation of longitudinal continuous outcomes under reference-based assumptions", "link": "http://arxiv.org/abs/2310.05151", "description": "In clinical trials of longitudinal continuous outcomes, reference based\nimputation (RBI) has commonly been applied to handle missing outcome data in\nsettings where the estimand incorporates the effects of intercurrent events,\ne.g. treatment discontinuation. RBI was originally developed in the multiple\nimputation framework, however recently conditional mean imputation (CMI)\ncombined with the jackknife estimator of the standard error was proposed as a\nway to obtain deterministic treatment effect estimates and correct frequentist\ninference. For both multiple and CMI, a mixed model for repeated measures\n(MMRM) is often used for the imputation model, but this can be computationally\nintensive to fit to multiple data sets (e.g. the jackknife samples) and lead to\nconvergence issues with complex MMRM models with many parameters. Therefore, a\nstep-wise approach based on sequential linear regression (SLR) of the outcomes\nat each visit was developed for the imputation model in the multiple imputation\nframework, but similar developments in the CMI framework are lacking. In this\narticle, we fill this gap in the literature by proposing a SLR approach to\nimplement RBI in the CMI framework, and justify its validity using theoretical\nresults and simulations. We also illustrate our proposal on a real data\napplication."}, "http://arxiv.org/abs/2310.05398": {"title": "Statistical Inference for Modulation Index in Phase-Amplitude Coupling", "link": "http://arxiv.org/abs/2310.05398", "description": "Phase-amplitude coupling is a phenomenon observed in several neurological\nprocesses, where the phase of one signal modulates the amplitude of another\nsignal with a distinct frequency. The modulation index (MI) is a common\ntechnique used to quantify this interaction by assessing the Kullback-Leibler\ndivergence between a uniform distribution and the empirical conditional\ndistribution of amplitudes with respect to the phases of the observed signals.\nThe uniform distribution is an ideal representation that is expected to appear\nunder the absence of coupling. However, it does not reflect the statistical\nproperties of coupling values caused by random chance. In this paper, we\npropose a statistical framework for evaluating the significance of an observed\nMI value based on a null hypothesis that a MI value can be entirely explained\nby chance. Significance is obtained by comparing the value with a reference\ndistribution derived under the null hypothesis of independence (i.e., no\ncoupling) between signals. We derived a closed-form distribution of this null\nmodel, resulting in a scaled beta distribution. To validate the efficacy of our\nproposed framework, we conducted comprehensive Monte Carlo simulations,\nassessing the significance of MI values under various experimental scenarios,\nincluding amplitude modulation, trains of spikes, and sequences of\nhigh-frequency oscillations. Furthermore, we corroborated the reliability of\nour model by comparing its statistical significance thresholds with reported\nvalues from other research studies conducted under different experimental\nsettings. Our method offers several advantages such as meta-analysis\nreliability, simplicity and computational efficiency, as it provides p-values\nand significance levels without resorting to generating surrogate data through\nsampling procedures."}, "http://arxiv.org/abs/2310.05526": {"title": "Projecting infinite time series graphs to finite marginal graphs using number theory", "link": "http://arxiv.org/abs/2310.05526", "description": "In recent years, a growing number of method and application works have\nadapted and applied the causal-graphical-model framework to time series data.\nMany of these works employ time-resolved causal graphs that extend infinitely\ninto the past and future and whose edges are repetitive in time, thereby\nreflecting the assumption of stationary causal relationships. However, most\nresults and algorithms from the causal-graphical-model framework are not\ndesigned for infinite graphs. In this work, we develop a method for projecting\ninfinite time series graphs with repetitive edges to marginal graphical models\non a finite time window. These finite marginal graphs provide the answers to\n$m$-separation queries with respect to the infinite graph, a task that was\npreviously unresolved. Moreover, we argue that these marginal graphs are useful\nfor causal discovery and causal effect estimation in time series, effectively\nenabling to apply results developed for finite graphs to the infinite graphs.\nThe projection procedure relies on finding common ancestors in the\nto-be-projected graph and is, by itself, not new. However, the projection\nprocedure has not yet been algorithmically implemented for time series graphs\nsince in these infinite graphs there can be infinite sets of paths that might\ngive rise to common ancestors. We solve the search over these possibly infinite\nsets of paths by an intriguing combination of path-finding techniques for\nfinite directed graphs and solution theory for linear Diophantine equations. By\nproviding an algorithm that carries out the projection, our paper makes an\nimportant step towards a theoretically-grounded and method-agnostic\ngeneralization of a range of causal inference methods and results to time\nseries."}, "http://arxiv.org/abs/2310.05539": {"title": "Testing High-Dimensional Mediation Effect with Arbitrary Exposure-Mediator Coefficients", "link": "http://arxiv.org/abs/2310.05539", "description": "In response to the unique challenge created by high-dimensional mediators in\nmediation analysis, this paper presents a novel procedure for testing the\nnullity of the mediation effect in the presence of high-dimensional mediators.\nThe procedure incorporates two distinct features. Firstly, the test remains\nvalid under all cases of the composite null hypothesis, including the\nchallenging scenario where both exposure-mediator and mediator-outcome\ncoefficients are zero. Secondly, it does not impose structural assumptions on\nthe exposure-mediator coefficients, thereby allowing for an arbitrarily strong\nexposure-mediator relationship. To the best of our knowledge, the proposed test\nis the first of its kind to provably possess these two features in\nhigh-dimensional mediation analysis. The validity and consistency of the\nproposed test are established, and its numerical performance is showcased\nthrough simulation studies. The application of the proposed test is\ndemonstrated by examining the mediation effect of DNA methylation between\nsmoking status and lung cancer development."}, "http://arxiv.org/abs/2310.05548": {"title": "Cokrig-and-Regress for Spatially Misaligned Environmental Data", "link": "http://arxiv.org/abs/2310.05548", "description": "Spatially misaligned data, where the response and covariates are observed at\ndifferent spatial locations, commonly arise in many environmental studies. Much\nof the statistical literature on handling spatially misaligned data has been\ndevoted to the case of a single covariate and a linear relationship between the\nresponse and this covariate. Motivated by spatially misaligned data collected\non air pollution and weather in China, we propose a cokrig-and-regress (CNR)\nmethod to estimate spatial regression models involving multiple covariates and\npotentially non-linear associations. The CNR estimator is constructed by\nreplacing the unobserved covariates (at the response locations) by their\ncokriging predictor derived from the observed but misaligned covariates under a\nmultivariate Gaussian assumption, where a generalized Kronecker product\ncovariance is used to account for spatial correlations within and between\ncovariates. A parametric bootstrap approach is employed to bias-correct the CNR\nestimates of the spatial covariance parameters and for uncertainty\nquantification. Simulation studies demonstrate that CNR outperforms several\nexisting methods for handling spatially misaligned data, such as\nnearest-neighbor interpolation. Applying CNR to the spatially misaligned air\npollution and weather data in China reveals a number of non-linear\nrelationships between PM$_{2.5}$ concentration and several meteorological\ncovariates."}, "http://arxiv.org/abs/2310.05622": {"title": "A neutral comparison of statistical methods for time-to-event analyses under non-proportional hazards", "link": "http://arxiv.org/abs/2310.05622", "description": "While well-established methods for time-to-event data are available when the\nproportional hazards assumption holds, there is no consensus on the best\ninferential approach under non-proportional hazards (NPH). However, a wide\nrange of parametric and non-parametric methods for testing and estimation in\nthis scenario have been proposed. To provide recommendations on the statistical\nanalysis of clinical trials where non proportional hazards are expected, we\nconducted a comprehensive simulation study under different scenarios of\nnon-proportional hazards, including delayed onset of treatment effect, crossing\nhazard curves, subgroups with different treatment effect and changing hazards\nafter disease progression. We assessed type I error rate control, power and\nconfidence interval coverage, where applicable, for a wide range of methods\nincluding weighted log-rank tests, the MaxCombo test, summary measures such as\nthe restricted mean survival time (RMST), average hazard ratios, and milestone\nsurvival probabilities as well as accelerated failure time regression models.\nWe found a trade-off between interpretability and power when choosing an\nanalysis strategy under NPH scenarios. While analysis methods based on weighted\nlogrank tests typically were favorable in terms of power, they do not provide\nan easily interpretable treatment effect estimate. Also, depending on the\nweight function, they test a narrow null hypothesis of equal hazard functions\nand rejection of this null hypothesis may not allow for a direct conclusion of\ntreatment benefit in terms of the survival function. In contrast,\nnon-parametric procedures based on well interpretable measures as the RMST\ndifference had lower power in most scenarios. Model based methods based on\nspecific survival distributions had larger power, however often gave biased\nestimates and lower than nominal confidence interval coverage."}, "http://arxiv.org/abs/2310.05646": {"title": "Transfer learning for piecewise-constant mean estimation: Optimality, $\\ell_1$- and $\\ell_0$-penalisation", "link": "http://arxiv.org/abs/2310.05646", "description": "We study transfer learning in the context of estimating piecewise-constant\nsignals when source data, which may be relevant but disparate, are available in\naddition to the target data. We initially investigate transfer learning\nestimators that respectively employ $\\ell_1$- and $\\ell_0$-penalties for\nunisource data scenarios and then generalise these estimators to accommodate\nmultisource data. To further reduce estimation errors, especially in scenarios\nwhere some sources significantly differ from the target, we introduce an\ninformative source selection algorithm. We then examine these estimators with\nmultisource selection and establish their minimax optimality under specific\nregularity conditions. It is worth emphasising that, unlike the prevalent\nnarrative in the transfer learning literature that the performance is enhanced\nthrough large source sample sizes, our approaches leverage higher observation\nfrequencies and accommodate diverse frequencies across multiple sources. Our\ntheoretical findings are empirically validated through extensive numerical\nexperiments, with the code available online, see\nhttps://github.com/chrisfanwang/transferlearning"}, "http://arxiv.org/abs/2310.05685": {"title": "Post-Selection Inference for Sparse Estimation", "link": "http://arxiv.org/abs/2310.05685", "description": "When the model is not known and parameter testing or interval estimation is\nconducted after model selection, it is necessary to consider selective\ninference. This paper discusses this issue in the context of sparse estimation.\nFirstly, we describe selective inference related to Lasso as per \\cite{lee},\nand then present polyhedra and truncated distributions when applying it to\nmethods such as Forward Stepwise and LARS. Lastly, we discuss the Significance\nTest for Lasso by \\cite{significant} and the Spacing Test for LARS by\n\\cite{ryan_exact}. This paper serves as a review article.\n\nKeywords: post-selective inference, polyhedron, LARS, lasso, forward\nstepwise, significance test, spacing test."}, "http://arxiv.org/abs/2310.05921": {"title": "Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions", "link": "http://arxiv.org/abs/2310.05921", "description": "We introduce Conformal Decision Theory, a framework for producing safe\nautonomous decisions despite imperfect machine learning predictions. Examples\nof such decisions are ubiquitous, from robot planning algorithms that rely on\npedestrian predictions, to calibrating autonomous manufacturing to exhibit high\nthroughput and low error, to the choice of trusting a nominal policy versus\nswitching to a safe backup policy at run-time. The decisions produced by our\nalgorithms are safe in the sense that they come with provable statistical\nguarantees of having low risk without any assumptions on the world model\nwhatsoever; the observations need not be I.I.D. and can even be adversarial.\nThe theory extends results from conformal prediction to calibrate decisions\ndirectly, without requiring the construction of prediction sets. Experiments\ndemonstrate the utility of our approach in robot motion planning around humans,\nautomated stock trading, and robot manufacturin"}, "http://arxiv.org/abs/2101.06950": {"title": "Learning and scoring Gaussian latent variable causal models with unknown additive interventions", "link": "http://arxiv.org/abs/2101.06950", "description": "With observational data alone, causal structure learning is a challenging\nproblem. The task becomes easier when having access to data collected from\nperturbations of the underlying system, even when the nature of these is\nunknown. Existing methods either do not allow for the presence of latent\nvariables or assume that these remain unperturbed. However, these assumptions\nare hard to justify if the nature of the perturbations is unknown. We provide\nresults that enable scoring causal structures in the setting with additive, but\nunknown interventions. Specifically, we propose a maximum-likelihood estimator\nin a structural equation model that exploits system-wide invariances to output\nan equivalence class of causal structures from perturbation data. Furthermore,\nunder certain structural assumptions on the population model, we provide a\nsimple graphical characterization of all the DAGs in the interventional\nequivalence class. We illustrate the utility of our framework on synthetic data\nas well as real data involving California reservoirs and protein expressions.\nThe software implementation is available as the Python package \\emph{utlvce}."}, "http://arxiv.org/abs/2107.14151": {"title": "Modern Non-Linear Function-on-Function Regression", "link": "http://arxiv.org/abs/2107.14151", "description": "We introduce a new class of non-linear function-on-function regression models\nfor functional data using neural networks. We propose a framework using a\nhidden layer consisting of continuous neurons, called a continuous hidden\nlayer, for functional response modeling and give two model fitting strategies,\nFunctional Direct Neural Network (FDNN) and Functional Basis Neural Network\n(FBNN). Both are designed explicitly to exploit the structure inherent in\nfunctional data and capture the complex relations existing between the\nfunctional predictors and the functional response. We fit these models by\nderiving functional gradients and implement regularization techniques for more\nparsimonious results. We demonstrate the power and flexibility of our proposed\nmethod in handling complex functional models through extensive simulation\nstudies as well as real data examples."}, "http://arxiv.org/abs/2112.00832": {"title": "On the mixed-model analysis of covariance in cluster-randomized trials", "link": "http://arxiv.org/abs/2112.00832", "description": "In the analyses of cluster-randomized trials, mixed-model analysis of\ncovariance (ANCOVA) is a standard approach for covariate adjustment and\nhandling within-cluster correlations. However, when the normality, linearity,\nor the random-intercept assumption is violated, the validity and efficiency of\nthe mixed-model ANCOVA estimators for estimating the average treatment effect\nremain unclear. Under the potential outcomes framework, we prove that the\nmixed-model ANCOVA estimators for the average treatment effect are consistent\nand asymptotically normal under arbitrary misspecification of its working\nmodel. If the probability of receiving treatment is 0.5 for each cluster, we\nfurther show that the model-based variance estimator under mixed-model ANCOVA1\n(ANCOVA without treatment-covariate interactions) remains consistent,\nclarifying that the confidence interval given by standard software is\nasymptotically valid even under model misspecification. Beyond robustness, we\ndiscuss several insights on precision among classical methods for analyzing\ncluster-randomized trials, including the mixed-model ANCOVA, individual-level\nANCOVA, and cluster-level ANCOVA estimators. These insights may inform the\nchoice of methods in practice. Our analytical results and insights are\nillustrated via simulation studies and analyses of three cluster-randomized\ntrials."}, "http://arxiv.org/abs/2201.10770": {"title": "Confidence intervals for the Cox model test error from cross-validation", "link": "http://arxiv.org/abs/2201.10770", "description": "Cross-validation (CV) is one of the most widely used techniques in\nstatistical learning for estimating the test error of a model, but its behavior\nis not yet fully understood. It has been shown that standard confidence\nintervals for test error using estimates from CV may have coverage below\nnominal levels. This phenomenon occurs because each sample is used in both the\ntraining and testing procedures during CV and as a result, the CV estimates of\nthe errors become correlated. Without accounting for this correlation, the\nestimate of the variance is smaller than it should be. One way to mitigate this\nissue is by estimating the mean squared error of the prediction error instead\nusing nested CV. This approach has been shown to achieve superior coverage\ncompared to intervals derived from standard CV. In this work, we generalize the\nnested CV idea to the Cox proportional hazards model and explore various\nchoices of test error for this setting."}, "http://arxiv.org/abs/2202.08419": {"title": "High-Dimensional Time-Varying Coefficient Estimation", "link": "http://arxiv.org/abs/2202.08419", "description": "In this paper, we develop a novel high-dimensional time-varying coefficient\nestimation method, based on high-dimensional Ito diffusion processes. To\naccount for high-dimensional time-varying coefficients, we first estimate local\n(or instantaneous) coefficients using a time-localized Dantzig selection scheme\nunder a sparsity condition, which results in biased local coefficient\nestimators due to the regularization. To handle the bias, we propose a\ndebiasing scheme, which provides well-performing unbiased local coefficient\nestimators. With the unbiased local coefficient estimators, we estimate the\nintegrated coefficient, and to further account for the sparsity of the\ncoefficient process, we apply thresholding schemes. We call this Thresholding\ndEbiased Dantzig (TED). We establish asymptotic properties of the proposed TED\nestimator. In the empirical analysis, we apply the TED procedure to analyzing\nhigh-dimensional factor models using high-frequency data."}, "http://arxiv.org/abs/2206.12525": {"title": "Causality of Functional Longitudinal Data", "link": "http://arxiv.org/abs/2206.12525", "description": "\"Treatment-confounder feedback\" is the central complication to resolve in\nlongitudinal studies, to infer causality. The existing frameworks for\nidentifying causal effects for longitudinal studies with discrete repeated\nmeasures hinge heavily on assuming that time advances in discrete time steps or\ntreatment changes as a jumping process, rendering the number of \"feedbacks\"\nfinite. However, medical studies nowadays with real-time monitoring involve\nfunctional time-varying outcomes, treatment, and confounders, which leads to an\nuncountably infinite number of feedbacks between treatment and confounders.\nTherefore more general and advanced theory is needed. We generalize the\ndefinition of causal effects under user-specified stochastic treatment regimes\nto longitudinal studies with continuous monitoring and develop an\nidentification framework, allowing right censoring and truncation by death. We\nprovide sufficient identification assumptions including a generalized\nconsistency assumption, a sequential randomization assumption, a positivity\nassumption, and a novel \"intervenable\" assumption designed for the\ncontinuous-time case. Under these assumptions, we propose a g-computation\nprocess and an inverse probability weighting process, which suggest a\ng-computation formula and an inverse probability weighting formula for\nidentification. For practical purposes, we also construct two classes of\npopulation estimating equations to identify these two processes, respectively,\nwhich further suggest a doubly robust identification formula with extra\nrobustness against process misspecification. We prove that our framework fully\ngeneralize the existing frameworks and is nonparametric."}, "http://arxiv.org/abs/2209.08139": {"title": "Sparse high-dimensional linear regression with a partitioned empirical Bayes ECM algorithm", "link": "http://arxiv.org/abs/2209.08139", "description": "Bayesian variable selection methods are powerful techniques for fitting and\ninferring on sparse high-dimensional linear regression models. However, many\nare computationally intensive or require restrictive prior distributions on\nmodel parameters. In this paper, we proposed a computationally efficient and\npowerful Bayesian approach for sparse high-dimensional linear regression.\nMinimal prior assumptions on the parameters are required through the use of\nplug-in empirical Bayes estimates of hyperparameters. Efficient maximum a\nposteriori (MAP) estimation is completed through a Parameter-Expanded\nExpectation-Conditional-Maximization (PX-ECM) algorithm. The PX-ECM results in\na robust computationally efficient coordinate-wise optimization which -- when\nupdating the coefficient for a particular predictor -- adjusts for the impact\nof other predictor variables. The completion of the E-step uses an approach\nmotivated by the popular two-group approach to multiple testing. The result is\na PaRtitiOned empirical Bayes Ecm (PROBE) algorithm applied to sparse\nhigh-dimensional linear regression, which can be completed using one-at-a-time\nor all-at-once type optimization. We compare the empirical properties of PROBE\nto comparable approaches with numerous simulation studies and analyses of\ncancer cell drug responses. The proposed approach is implemented in the R\npackage probe."}, "http://arxiv.org/abs/2212.02709": {"title": "SURE-tuned Bridge Regression", "link": "http://arxiv.org/abs/2212.02709", "description": "Consider the {$\\ell_{\\alpha}$} regularized linear regression, also termed\nBridge regression. For $\\alpha\\in (0,1)$, Bridge regression enjoys several\nstatistical properties of interest such as sparsity and near-unbiasedness of\nthe estimates (Fan and Li, 2001). However, the main difficulty lies in the\nnon-convex nature of the penalty for these values of $\\alpha$, which makes an\noptimization procedure challenging and usually it is only possible to find a\nlocal optimum. To address this issue, Polson et al. (2013) took a sampling\nbased fully Bayesian approach to this problem, using the correspondence between\nthe Bridge penalty and a power exponential prior on the regression\ncoefficients. However, their sampling procedure relies on Markov chain Monte\nCarlo (MCMC) techniques, which are inherently sequential and not scalable to\nlarge problem dimensions. Cross validation approaches are similarly\ncomputation-intensive. To this end, our contribution is a novel\n\\emph{non-iterative} method to fit a Bridge regression model. The main\ncontribution lies in an explicit formula for Stein's unbiased risk estimate for\nthe out of sample prediction risk of Bridge regression, which can then be\noptimized to select the desired tuning parameters, allowing us to completely\nbypass MCMC as well as computation-intensive cross validation approaches. Our\nprocedure yields results in a fraction of computational times compared to\niterative schemes, without any appreciable loss in statistical performance. An\nR implementation is publicly available online at:\nhttps://github.com/loriaJ/Sure-tuned_BridgeRegression ."}, "http://arxiv.org/abs/2212.03122": {"title": "Robust convex biclustering with a tuning-free method", "link": "http://arxiv.org/abs/2212.03122", "description": "Biclustering is widely used in different kinds of fields including gene\ninformation analysis, text mining, and recommendation system by effectively\ndiscovering the local correlation between samples and features. However, many\nbiclustering algorithms will collapse when facing heavy-tailed data. In this\npaper, we propose a robust version of convex biclustering algorithm with Huber\nloss. Yet, the newly introduced robustification parameter brings an extra\nburden to selecting the optimal parameters. Therefore, we propose a tuning-free\nmethod for automatically selecting the optimal robustification parameter with\nhigh efficiency. The simulation study demonstrates the more fabulous\nperformance of our proposed method than traditional biclustering methods when\nencountering heavy-tailed noise. A real-life biomedical application is also\npresented. The R package RcvxBiclustr is available at\nhttps://github.com/YifanChen3/RcvxBiclustr."}, "http://arxiv.org/abs/2301.09661": {"title": "Estimating marginal treatment effects from observational studies and indirect treatment comparisons: When are standardization-based methods preferable to those based on propensity score weighting?", "link": "http://arxiv.org/abs/2301.09661", "description": "In light of newly developed standardization methods, we evaluate, via\nsimulation study, how propensity score weighting and standardization -based\napproaches compare for obtaining estimates of the marginal odds ratio and the\nmarginal hazard ratio. Specifically, we consider how the two approaches compare\nin two different scenarios: (1) in a single observational study, and (2) in an\nanchored indirect treatment comparison (ITC) of randomized controlled trials.\nWe present the material in such a way so that the matching-adjusted indirect\ncomparison (MAIC) and the (novel) simulated treatment comparison (STC) methods\nin the ITC setting may be viewed as analogous to the propensity score weighting\nand standardization methods in the single observational study setting. Our\nresults suggest that current recommendations for conducting ITCs can be\nimproved and underscore the importance of adjusting for purely prognostic\nfactors."}, "http://arxiv.org/abs/2302.11746": {"title": "Logistic Regression and Classification with non-Euclidean Covariates", "link": "http://arxiv.org/abs/2302.11746", "description": "We introduce a logistic regression model for data pairs consisting of a\nbinary response and a covariate residing in a non-Euclidean metric space\nwithout vector structures. Based on the proposed model we also develop a binary\nclassifier for non-Euclidean objects. We propose a maximum likelihood estimator\nfor the non-Euclidean regression coefficient in the model, and provide upper\nbounds on the estimation error under various metric entropy conditions that\nquantify complexity of the underlying metric space. Matching lower bounds are\nderived for the important metric spaces commonly seen in statistics,\nestablishing optimality of the proposed estimator in such spaces. Similarly, an\nupper bound on the excess risk of the developed classifier is provided for\ngeneral metric spaces. A finer upper bound and a matching lower bound, and thus\noptimality of the proposed classifier, are established for Riemannian\nmanifolds. We investigate the numerical performance of the proposed estimator\nand classifier via simulation studies, and illustrate their practical merits\nvia an application to task-related fMRI data."}, "http://arxiv.org/abs/2302.13658": {"title": "Robust High-Dimensional Time-Varying Coefficient Estimation", "link": "http://arxiv.org/abs/2302.13658", "description": "In this paper, we develop a novel high-dimensional coefficient estimation\nprocedure based on high-frequency data. Unlike usual high-dimensional\nregression procedure such as LASSO, we additionally handle the heavy-tailedness\nof high-frequency observations as well as time variations of coefficient\nprocesses. Specifically, we employ Huber loss and truncation scheme to handle\nheavy-tailed observations, while $\\ell_{1}$-regularization is adopted to\novercome the curse of dimensionality. To account for the time-varying\ncoefficient, we estimate local coefficients which are biased due to the\n$\\ell_{1}$-regularization. Thus, when estimating integrated coefficients, we\npropose a debiasing scheme to enjoy the law of large number property and employ\na thresholding scheme to further accommodate the sparsity of the coefficients.\nWe call this Robust thrEsholding Debiased LASSO (RED-LASSO) estimator. We show\nthat the RED-LASSO estimator can achieve a near-optimal convergence rate. In\nthe empirical study, we apply the RED-LASSO procedure to the high-dimensional\nintegrated coefficient estimation using high-frequency trading data."}, "http://arxiv.org/abs/2307.04754": {"title": "Action-State Dependent Dynamic Model Selection", "link": "http://arxiv.org/abs/2307.04754", "description": "A model among many may only be best under certain states of the world.\nSwitching from a model to another can also be costly. Finding a procedure to\ndynamically choose a model in these circumstances requires to solve a complex\nestimation procedure and a dynamic programming problem. A Reinforcement\nlearning algorithm is used to approximate and estimate from the data the\noptimal solution to this dynamic programming problem. The algorithm is shown to\nconsistently estimate the optimal policy that may choose different models based\non a set of covariates. A typical example is the one of switching between\ndifferent portfolio models under rebalancing costs, using macroeconomic\ninformation. Using a set of macroeconomic variables and price data, an\nempirical application to the aforementioned portfolio problem shows superior\nperformance to choosing the best portfolio model with hindsight."}, "http://arxiv.org/abs/2307.14828": {"title": "Identifying regime switches through Bayesian wavelet estimation: evidence from flood detection in the Taquari River Valley", "link": "http://arxiv.org/abs/2307.14828", "description": "Two-component mixture models have proved to be a powerful tool for modeling\nheterogeneity in several cluster analysis contexts. However, most methods based\non these models assume a constant behavior for the mixture weights, which can\nbe restrictive and unsuitable for some applications. In this paper, we relax\nthis assumption and allow the mixture weights to vary according to the index\n(e.g., time) to make the model more adaptive to a broader range of data sets.\nWe propose an efficient MCMC algorithm to jointly estimate both component\nparameters and dynamic weights from their posterior samples. We evaluate the\nmethod's performance by running Monte Carlo simulation studies under different\nscenarios for the dynamic weights. In addition, we apply the algorithm to a\ntime series that records the level reached by a river in southern Brazil. The\nTaquari River is a water body whose frequent flood inundations have caused\nvarious damage to riverside communities. Implementing a dynamic mixture model\nallows us to properly describe the flood regimes for the areas most affected by\nthese phenomena."}, "http://arxiv.org/abs/2310.06130": {"title": "Statistical inference for radially-stable generalized Pareto distributions and return level-sets in geometric extremes", "link": "http://arxiv.org/abs/2310.06130", "description": "We obtain a functional analogue of the quantile function for probability\nmeasures admitting a continuous Lebesgue density on $\\mathbb{R}^d$, and use it\nto characterize the class of non-trivial limit distributions of radially\nrecentered and rescaled multivariate exceedances in geometric extremes. A new\nclass of multivariate distributions is identified, termed radially stable\ngeneralized Pareto distributions, and is shown to admit certain stability\nproperties that permit extrapolation to extremal sets along any direction in\n$\\mathbb{R}^d$. Based on the limit Poisson point process likelihood of the\nradially renormalized point process of exceedances, we develop parsimonious\nstatistical models that exploit theoretical links between structural\nstar-bodies and are amenable to Bayesian inference. The star-bodies determine\nthe mean measure of the limit Poisson process through a hierarchical structure.\nOur framework sharpens statistical inference by suitably including additional\ninformation from the angular directions of the geometric exceedances and\nfacilitates efficient computations in dimensions $d=2$ and $d=3$. Additionally,\nit naturally leads to the notion of the return level-set, which is a canonical\nquantile set expressed in terms of its average recurrence interval, and a\ngeometric analogue of the uni-dimensional return level. We illustrate our\nmethods with a simulation study showing superior predictive performance of\nprobabilities of rare events, and with two case studies, one associated with\nriver flow extremes, and the other with oceanographic extremes."}, "http://arxiv.org/abs/2310.06242": {"title": "Treatment Choice, Mean Square Regret and Partial Identification", "link": "http://arxiv.org/abs/2310.06242", "description": "We consider a decision maker who faces a binary treatment choice when their\nwelfare is only partially identified from data. We contribute to the literature\nby anchoring our finite-sample analysis on mean square regret, a decision\ncriterion advocated by Kitagawa, Lee, and Qiu (2022). We find that optimal\nrules are always fractional, irrespective of the width of the identified set\nand precision of its estimate. The optimal treatment fraction is a simple\nlogistic transformation of the commonly used t-statistic multiplied by a factor\ncalculated by a simple constrained optimization. This treatment fraction gets\ncloser to 0.5 as the width of the identified set becomes wider, implying the\ndecision maker becomes more cautious against the adversarial Nature."}, "http://arxiv.org/abs/2310.06252": {"title": "Power and sample size calculation of two-sample projection-based testing for sparsely observed functional data", "link": "http://arxiv.org/abs/2310.06252", "description": "Projection-based testing for mean trajectory differences in two groups of\nirregularly and sparsely observed functional data has garnered significant\nattention in the literature because it accommodates a wide spectrum of group\ndifferences and (non-stationary) covariance structures. This article presents\nthe derivation of the theoretical power function and the introduction of a\ncomprehensive power and sample size (PASS) calculation toolkit tailored to the\nprojection-based testing method developed by Wang (2021). Our approach\naccommodates a wide spectrum of group difference scenarios and a broad class of\ncovariance structures governing the underlying processes. Through extensive\nnumerical simulation, we demonstrate the robustness of this testing method by\nshowcasing that its statistical power remains nearly unaffected even when a\ncertain percentage of observations are missing, rendering it 'missing-immune'.\nFurthermore, we illustrate the practical utility of this test through analysis\nof two randomized controlled trials of Parkinson's disease. To facilitate\nimplementation, we provide a user-friendly R package fPASS, complete with a\ndetailed vignette to guide users through its practical application. We\nanticipate that this article will significantly enhance the usability of this\npotent statistical tool across a range of biostatistical applications, with a\nparticular focus on its relevance in the design of clinical trials."}, "http://arxiv.org/abs/2310.06315": {"title": "Ultra-high dimensional confounder selection algorithms comparison with application to radiomics data", "link": "http://arxiv.org/abs/2310.06315", "description": "Radiomics is an emerging area of medical imaging data analysis particularly\nfor cancer. It involves the conversion of digital medical images into mineable\nultra-high dimensional data. Machine learning algorithms are widely used in\nradiomics data analysis to develop powerful decision support model to improve\nprecision in diagnosis, assessment of prognosis and prediction of therapy\nresponse. However, machine learning algorithms for causal inference have not\nbeen previously employed in radiomics analysis. In this paper, we evaluate the\nvalue of machine learning algorithms for causal inference in radiomics. We\nselect three recent competitive variable selection algorithms for causal\ninference: outcome-adaptive lasso (OAL), generalized outcome-adaptive lasso\n(GOAL) and causal ball screening (CBS). We used a sure independence screening\nprocedure to propose an extension of GOAL and OAL for ultra-high dimensional\ndata, SIS + GOAL and SIS + OAL. We compared SIS + GOAL, SIS + OAL and CBS using\nsimulation study and two radiomics datasets in cancer, osteosarcoma and\ngliosarcoma. The two radiomics studies and the simulation study identified SIS\n+ GOAL as the optimal variable selection algorithm."}, "http://arxiv.org/abs/2310.06330": {"title": "Multivariate moment least-squares estimators for reversible Markov chains", "link": "http://arxiv.org/abs/2310.06330", "description": "Markov chain Monte Carlo (MCMC) is a commonly used method for approximating\nexpectations with respect to probability distributions. Uncertainty assessment\nfor MCMC estimators is essential in practical applications. Moreover, for\nmultivariate functions of a Markov chain, it is important to estimate not only\nthe auto-correlation for each component but also to estimate\ncross-correlations, in order to better assess sample quality, improve estimates\nof effective sample size, and use more effective stopping rules. Berg and Song\n[2022] introduced the moment least squares (momentLS) estimator, a\nshape-constrained estimator for the autocovariance sequence from a reversible\nMarkov chain, for univariate functions of the Markov chain. Based on this\nsequence estimator, they proposed an estimator of the asymptotic variance of\nthe sample mean from MCMC samples. In this study, we propose novel\nautocovariance sequence and asymptotic variance estimators for Markov chain\nfunctions with multiple components, based on the univariate momentLS estimators\nfrom Berg and Song [2022]. We demonstrate strong consistency of the proposed\nauto(cross)-covariance sequence and asymptotic variance matrix estimators. We\nconduct empirical comparisons of our method with other state-of-the-art\napproaches on simulated and real-data examples, using popular samplers\nincluding the random-walk Metropolis sampler and the No-U-Turn sampler from\nSTAN."}, "http://arxiv.org/abs/2310.06357": {"title": "Adaptive Storey's null proportion estimator", "link": "http://arxiv.org/abs/2310.06357", "description": "False discovery rate (FDR) is a commonly used criterion in multiple testing\nand the Benjamini-Hochberg (BH) procedure is arguably the most popular approach\nwith FDR guarantee. To improve power, the adaptive BH procedure has been\nproposed by incorporating various null proportion estimators, among which\nStorey's estimator has gained substantial popularity. The performance of\nStorey's estimator hinges on a critical hyper-parameter, where a pre-fixed\nconfiguration lacks power and existing data-driven hyper-parameters compromise\nthe FDR control. In this work, we propose a novel class of adaptive\nhyper-parameters and establish the FDR control of the associated BH procedure\nusing a martingale argument. Within this class of data-driven hyper-parameters,\nwe present a specific configuration designed to maximize the number of\nrejections and characterize the convergence of this proposal to the optimal\nhyper-parameter under a commonly-used mixture model. We evaluate our adaptive\nStorey's null proportion estimator and the associated BH procedure on extensive\nsimulated data and a motivating protein dataset. Our proposal exhibits\nsignificant power gains when dealing with a considerable proportion of weak\nnon-nulls or a conservative null distribution."}, "http://arxiv.org/abs/2310.06467": {"title": "Advances in Kth nearest-neighbour clutter removal", "link": "http://arxiv.org/abs/2310.06467", "description": "We consider the problem of feature detection in the presence of clutter in\nspatial point processes. Classification methods have been developed in previous\nstudies. Among these, Byers and Raftery (1998) models the observed Kth nearest\nneighbour distances as a mixture distribution and classifies the clutter and\nfeature points consequently. In this paper, we enhance such approach in two\nmanners. First, we propose an automatic procedure for selecting the number of\nnearest neighbours to consider in the classification method by means of\nsegmented regression models. Secondly, with the aim of applying the procedure\nmultiple times to get a ``better\" end result, we propose a stopping criterion\nthat minimizes the overall entropy measure of cluster separation between\nclutter and feature points. The proposed procedures are suitable for a feature\nwith clutter as two superimposed Poisson processes on any space, including\nlinear networks. We present simulations and two case studies of environmental\ndata to illustrate the method."}, "http://arxiv.org/abs/2310.06533": {"title": "Multilevel Monte Carlo for a class of Partially Observed Processes in Neuroscience", "link": "http://arxiv.org/abs/2310.06533", "description": "In this paper we consider Bayesian parameter inference associated to a class\nof partially observed stochastic differential equations (SDE) driven by jump\nprocesses. Such type of models can be routinely found in applications, of which\nwe focus upon the case of neuroscience. The data are assumed to be observed\nregularly in time and driven by the SDE model with unknown parameters. In\npractice the SDE may not have an analytically tractable solution and this leads\nnaturally to a time-discretization. We adapt the multilevel Markov chain Monte\nCarlo method of [11], which works with a hierarchy of time discretizations and\nshow empirically and theoretically that this is preferable to using one single\ntime discretization. The improvement is in terms of the computational cost\nneeded to obtain a pre-specified numerical error. Our approach is illustrated\non models that are found in neuroscience."}, "http://arxiv.org/abs/2310.06653": {"title": "Evaluating causal effects on time-to-event outcomes in an RCT in Oncology with treatment discontinuation due to adverse events", "link": "http://arxiv.org/abs/2310.06653", "description": "In clinical trials, patients sometimes discontinue study treatments\nprematurely due to reasons such as adverse events. Treatment discontinuation\noccurs after the randomisation as an intercurrent event, making causal\ninference more challenging. The Intention-To-Treat (ITT) analysis provides\nvalid causal estimates of the effect of treatment assignment; still, it does\nnot take into account whether or not patients had to discontinue the treatment\nprematurely. We propose to deal with the problem of treatment discontinuation\nusing principal stratification, recognised in the ICH E9(R1) addendum as a\nstrategy for handling intercurrent events. Under this approach, we can\ndecompose the overall ITT effect into principal causal effects for groups of\npatients defined by their potential discontinuation behaviour in continuous\ntime. In this framework, we must consider that discontinuation happening in\ncontinuous time generates an infinite number of principal strata and that\ndiscontinuation time is not defined for patients who would never discontinue.\nAn additional complication is that discontinuation time and time-to-event\noutcomes are subject to administrative censoring. We employ a flexible\nmodel-based Bayesian approach to deal with such complications. We apply the\nBayesian principal stratification framework to analyse synthetic data based on\na recent RCT in Oncology, aiming to assess the causal effects of a new\ninvestigational drug combined with standard of care vs. standard of care alone\non progression-free survival. We simulate data under different assumptions that\nreflect real situations where patients' behaviour depends on critical baseline\ncovariates. Finally, we highlight how such an approach makes it straightforward\nto characterise patients' discontinuation behaviour with respect to the\navailable covariates with the help of a simulation study."}, "http://arxiv.org/abs/2310.06673": {"title": "Assurance Methods for designing a clinical trial with a delayed treatment effect", "link": "http://arxiv.org/abs/2310.06673", "description": "An assurance calculation is a Bayesian alternative to a power calculation.\nOne may be performed to aid the planning of a clinical trial, specifically\nsetting the sample size or to support decisions about whether or not to perform\na study. Immuno-oncology (IO) is a rapidly evolving area in the development of\nanticancer drugs. A common phenomenon that arises from IO trials is one of\ndelayed treatment effects, that is, there is a delay in the separation of the\nsurvival curves. To calculate assurance for a trial in which a delayed\ntreatment effect is likely to be present, uncertainty about key parameters\nneeds to be considered. If uncertainty is not considered, then the number of\npatients recruited may not be enough to ensure we have adequate statistical\npower to detect a clinically relevant treatment effect. We present a new\nelicitation technique for when a delayed treatment effect is likely to be\npresent and show how to compute assurance using these elicited prior\ndistributions. We provide an example to illustrate how this could be used in\npractice. Open-source software is provided for implementing our methods. Our\nmethodology makes the benefits of assurance methods available for the planning\nof IO trials (and others where a delayed treatment expect is likely to occur)."}, "http://arxiv.org/abs/2310.06696": {"title": "Variable selection with FDR control for noisy data -- an application to screening metabolites that are associated with breast and colorectal cancer", "link": "http://arxiv.org/abs/2310.06696", "description": "The rapidly expanding field of metabolomics presents an invaluable resource\nfor understanding the associations between metabolites and various diseases.\nHowever, the high dimensionality, presence of missing values, and measurement\nerrors associated with metabolomics data can present challenges in developing\nreliable and reproducible methodologies for disease association studies.\nTherefore, there is a compelling need to develop robust statistical methods\nthat can navigate these complexities to achieve reliable and reproducible\ndisease association studies. In this paper, we focus on developing such a\nmethodology with an emphasis on controlling the False Discovery Rate during the\nscreening of mutual metabolomic signals for multiple disease outcomes. We\nillustrate the versatility and performance of this procedure in a variety of\nscenarios, dealing with missing data and measurement errors. As a specific\napplication of this novel methodology, we target two of the most prevalent\ncancers among US women: breast cancer and colorectal cancer. By applying our\nmethod to the Wome's Health Initiative data, we successfully identify\nmetabolites that are associated with either or both of these cancers,\ndemonstrating the practical utility and potential of our method in identifying\nconsistent risk factors and understanding shared mechanisms between diseases."}, "http://arxiv.org/abs/2310.06708": {"title": "Adjustment with Three Continuous Variables", "link": "http://arxiv.org/abs/2310.06708", "description": "Spurious association between X and Y may be due to a confounding variable W.\nStatisticians may adjust for W using a variety of techniques. This paper\npresents the results of simulations conducted to assess the performance of\nthose techniques under various, elementary, data-generating processes. The\nresults indicate that no technique is best overall and that specific techniques\nshould be selected based on the particulars of the data-generating process.\nHere we show how causal graphs can guide the selection or design of techniques\nfor statistical adjustment. R programs are provided for researchers interested\nin generalization."}, "http://arxiv.org/abs/2310.06720": {"title": "Asymptotic theory for Bayesian inference and prediction: from the ordinary to a conditional Peaks-Over-Threshold method", "link": "http://arxiv.org/abs/2310.06720", "description": "The Peaks Over Threshold (POT) method is the most popular statistical method\nfor the analysis of univariate extremes. Even though there is a rich applied\nliterature on Bayesian inference for the POT method there is no asymptotic\ntheory for such proposals. Even more importantly, the ambitious and challenging\nproblem of predicting future extreme events according to a proper probabilistic\nforecasting approach has received no attention to date. In this paper we\ndevelop the asymptotic theory (consistency, contraction rates, asymptotic\nnormality and asymptotic coverage of credible intervals) for the Bayesian\ninference based on the POT method. We extend such an asymptotic theory to cover\nthe Bayesian inference on the tail properties of the conditional distribution\nof a response random variable conditionally to a vector of random covariates.\nWith the aim to make accurate predictions of severer extreme events than those\noccurred in the past, we specify the posterior predictive distribution of a\nfuture unobservable excess variable in the unconditional and conditional\napproach and we prove that is Wasserstein consistent and derive its contraction\nrates. Simulations show the good performances of the proposed Bayesian\ninferential methods. The analysis of the change in the frequency of financial\ncrises over time shows the utility of our methodology."}, "http://arxiv.org/abs/2310.06730": {"title": "Sparse topic modeling via spectral decomposition and thresholding", "link": "http://arxiv.org/abs/2310.06730", "description": "The probabilistic Latent Semantic Indexing model assumes that the expectation\nof the corpus matrix is low-rank and can be written as the product of a\ntopic-word matrix and a word-document matrix. In this paper, we study the\nestimation of the topic-word matrix under the additional assumption that the\nordered entries of its columns rapidly decay to zero. This sparsity assumption\nis motivated by the empirical observation that the word frequencies in a text\noften adhere to Zipf's law. We introduce a new spectral procedure for\nestimating the topic-word matrix that thresholds words based on their corpus\nfrequencies, and show that its $\\ell_1$-error rate under our sparsity\nassumption depends on the vocabulary size $p$ only via a logarithmic term. Our\nerror bound is valid for all parameter regimes and in particular for the\nsetting where $p$ is extremely large; this high-dimensional setting is commonly\nencountered but has not been adequately addressed in prior literature.\nFurthermore, our procedure also accommodates datasets that violate the\nseparability assumption, which is necessary for most prior approaches in topic\nmodeling. Experiments with synthetic data confirm that our procedure is\ncomputationally fast and allows for consistent estimation of the topic-word\nmatrix in a wide variety of parameter regimes. Our procedure also performs well\nrelative to well-established methods when applied to a large corpus of research\npaper abstracts, as well as the analysis of single-cell and microbiome data\nwhere the same statistical model is relevant but the parameter regimes are\nvastly different."}, "http://arxiv.org/abs/2310.06746": {"title": "Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules", "link": "http://arxiv.org/abs/2310.06746", "description": "Interpretability is a key concern in estimating heterogeneous treatment\neffects using machine learning methods, especially for healthcare applications\nwhere high-stake decisions are often made. Inspired by the Predictive,\nDescriptive, Relevant framework of interpretability, we propose causal rule\nlearning which finds a refined set of causal rules characterizing potential\nsubgroups to estimate and enhance our understanding of heterogeneous treatment\neffects. Causal rule learning involves three phases: rule discovery, rule\nselection, and rule analysis. In the rule discovery phase, we utilize a causal\nforest to generate a pool of causal rules with corresponding subgroup average\ntreatment effects. The selection phase then employs a D-learning method to\nselect a subset of these rules to deconstruct individual-level treatment\neffects as a linear combination of the subgroup-level effects. This helps to\nanswer an ignored question by previous literature: what if an individual\nsimultaneously belongs to multiple groups with different average treatment\neffects? The rule analysis phase outlines a detailed procedure to further\nanalyze each rule in the subset from multiple perspectives, revealing the most\npromising rules for further validation. The rules themselves, their\ncorresponding subgroup treatment effects, and their weights in the linear\ncombination give us more insights into heterogeneous treatment effects.\nSimulation and real-world data analysis demonstrate the superior performance of\ncausal rule learning on the interpretable estimation of heterogeneous treatment\neffect when the ground truth is complex and the sample size is sufficient."}, "http://arxiv.org/abs/2310.06808": {"title": "Odds are the sign is right", "link": "http://arxiv.org/abs/2310.06808", "description": "This article introduces a new condition based on odds ratios for sensitivity\nanalysis. The analysis involves the average effect of a treatment or exposure\non a response or outcome with estimates adjusted for and conditional on a\nsingle, unmeasured, dichotomous covariate. Results of statistical simulations\nare displayed to show that the odds ratio condition is as reliable as other\ncommonly used conditions for sensitivity analysis. Other conditions utilize\nquantities reflective of a mediating covariate. The odds ratio condition can be\napplied when the covariate is a confounding variable. As an example application\nwe use the odds ratio condition to analyze and interpret a positive association\nobserved between Zika virus infection and birth defects."}, "http://arxiv.org/abs/2009.07551": {"title": "Manipulation-Robust Regression Discontinuity Designs", "link": "http://arxiv.org/abs/2009.07551", "description": "We present a new identification condition for regression discontinuity\ndesigns. We replace the local randomization of Lee (2008) with two restrictions\non its threat, namely, the manipulation of the running variable. Furthermore,\nwe provide the first auxiliary assumption of McCrary's (2008) diagnostic test\nto detect manipulation. Based on our auxiliary assumption, we derive a novel\nexpression of moments that immediately implies the worst-case bounds of Gerard,\nRokkanen, and Rothe (2020) and an enhanced interpretation of their target\nparameters. We highlight two issues: an overlooked source of identification\nfailure, and a missing auxiliary assumption to detect manipulation. In the case\nstudies, we illustrate our solution to these issues using institutional details\nand economic theories."}, "http://arxiv.org/abs/2204.06030": {"title": "Variable importance measures for heterogeneous causal effects", "link": "http://arxiv.org/abs/2204.06030", "description": "The recognition that personalised treatment decisions lead to better clinical\noutcomes has sparked recent research activity in the following two domains.\nPolicy learning focuses on finding optimal treatment rules (OTRs), which\nexpress whether an individual would be better off with or without treatment,\ngiven their measured characteristics. OTRs optimize a pre-set population\ncriterion, but do not provide insight into the extent to which treatment\nbenefits or harms individual subjects. Estimates of conditional average\ntreatment effects (CATEs) do offer such insights, but valid inference is\ncurrently difficult to obtain when data-adaptive methods are used. Moreover,\nclinicians are (rightly) hesitant to blindly adopt OTR or CATE estimates, not\nleast since both may represent complicated functions of patient characteristics\nthat provide little insight into the key drivers of heterogeneity. To address\nthese limitations, we introduce novel nonparametric treatment effect variable\nimportance measures (TE-VIMs). TE-VIMs extend recent regression-VIMs, viewed as\nnonparametric analogues to ANOVA statistics. By not being tied to a particular\nmodel, they are amenable to data-adaptive (machine learning) estimation of the\nCATE, itself an active area of research. Estimators for the proposed statistics\nare derived from their efficient influence curves and these are illustrated\nthrough a simulation study and an applied example."}, "http://arxiv.org/abs/2204.07907": {"title": "Just Identified Indirect Inference Estimator: Accurate Inference through Bias Correction", "link": "http://arxiv.org/abs/2204.07907", "description": "An important challenge in statistical analysis lies in controlling the\nestimation bias when handling the ever-increasing data size and model\ncomplexity of modern data settings. In this paper, we propose a reliable\nestimation and inference approach for parametric models based on the Just\nIdentified iNdirect Inference estimator (JINI). The key advantage of our\napproach is that it allows to construct a consistent estimator in a simple\nmanner, while providing strong bias correction guarantees that lead to accurate\ninference. Our approach is particularly useful for complex parametric models,\nas it allows to bypass the analytical and computational difficulties (e.g., due\nto intractable estimating equation) typically encountered in standard\nprocedures. The properties of JINI (including consistency, asymptotic\nnormality, and its bias correction property) are also studied when the\nparameter dimension is allowed to diverge, which provide the theoretical\nfoundation to explain the advantageous performance of JINI in increasing\ndimensional covariates settings. Our simulations and an alcohol consumption\ndata analysis highlight the practical usefulness and excellent performance of\nJINI when data present features (e.g., misclassification, rounding) as well as\nin robust estimation."}, "http://arxiv.org/abs/2209.05598": {"title": "Learning domain-specific causal discovery from time series", "link": "http://arxiv.org/abs/2209.05598", "description": "Causal discovery (CD) from time-varying data is important in neuroscience,\nmedicine, and machine learning. Techniques for CD encompass randomized\nexperiments, which are generally unbiased but expensive, and algorithms such as\nGranger causality, conditional-independence-based, structural-equation-based,\nand score-based methods that are only accurate under strong assumptions made by\nhuman designers. However, as demonstrated in other areas of machine learning,\nhuman expertise is often not entirely accurate and tends to be outperformed in\ndomains with abundant data. In this study, we examine whether we can enhance\ndomain-specific causal discovery for time series using a data-driven approach.\nOur findings indicate that this procedure significantly outperforms\nhuman-designed, domain-agnostic causal discovery methods, such as Mutual\nInformation, VAR-LiNGAM, and Granger Causality on the MOS 6502 microprocessor,\nthe NetSim fMRI dataset, and the Dream3 gene dataset. We argue that, when\nfeasible, the causality field should consider a supervised approach in which\ndomain-specific CD procedures are learned from extensive datasets with known\ncausal relationships, rather than being designed by human specialists. Our\nfindings promise a new approach toward improving CD in neural and medical data\nand for the broader machine learning community."}, "http://arxiv.org/abs/2209.05795": {"title": "Joint modelling of the body and tail of bivariate data", "link": "http://arxiv.org/abs/2209.05795", "description": "In situations where both extreme and non-extreme data are of interest,\nmodelling the whole data set accurately is important. In a univariate\nframework, modelling the bulk and tail of a distribution has been extensively\nstudied before. However, when more than one variable is of concern, models that\naim specifically at capturing both regions correctly are scarce in the\nliterature. A dependence model that blends two copulas with different\ncharacteristics over the whole range of the data support is proposed. One\ncopula is tailored to the bulk and the other to the tail, with a dynamic\nweighting function employed to transition smoothly between them. Tail\ndependence properties are investigated numerically and simulation is used to\nconfirm that the blended model is sufficiently flexible to capture a wide\nvariety of structures. The model is applied to study the dependence between\ntemperature and ozone concentration at two sites in the UK and compared with a\nsingle copula fit. The proposed model provides a better, more flexible, fit to\nthe data, and is also capable of capturing complex dependence structures."}, "http://arxiv.org/abs/2212.14650": {"title": "Two-step estimators of high dimensional correlation matrices", "link": "http://arxiv.org/abs/2212.14650", "description": "We investigate block diagonal and hierarchical nested stochastic multivariate\nGaussian models by studying their sample cross-correlation matrix on high\ndimensions. By performing numerical simulations, we compare a filtered sample\ncross-correlation with the population cross-correlation matrices by using\nseveral rotationally invariant estimators (RIE) and hierarchical clustering\nestimators (HCE) under several loss functions. We show that at large but finite\nsample size, sample cross-correlation filtered by RIE estimators are often\noutperformed by HCE estimators for several of the loss functions. We also show\nthat for block models and for hierarchically nested block models the best\ndetermination of the filtered sample cross-correlation is achieved by\nintroducing two-step estimators combining state-of-the-art non-linear shrinkage\nmodels with hierarchical clustering estimators."}, "http://arxiv.org/abs/2302.02457": {"title": "Scalable inference in functional linear regression with streaming data", "link": "http://arxiv.org/abs/2302.02457", "description": "Traditional static functional data analysis is facing new challenges due to\nstreaming data, where data constantly flow in. A major challenge is that\nstoring such an ever-increasing amount of data in memory is nearly impossible.\nIn addition, existing inferential tools in online learning are mainly developed\nfor finite-dimensional problems, while inference methods for functional data\nare focused on the batch learning setting. In this paper, we tackle these\nissues by developing functional stochastic gradient descent algorithms and\nproposing an online bootstrap resampling procedure to systematically study the\ninference problem for functional linear regression. In particular, the proposed\nestimation and inference procedures use only one pass over the data; thus they\nare easy to implement and suitable to the situation where data arrive in a\nstreaming manner. Furthermore, we establish the convergence rate as well as the\nasymptotic distribution of the proposed estimator. Meanwhile, the proposed\nperturbed estimator from the bootstrap procedure is shown to enjoy the same\ntheoretical properties, which provide the theoretical justification for our\nonline inference tool. As far as we know, this is the first inference result on\nthe functional linear regression model with streaming data. Simulation studies\nare conducted to investigate the finite-sample performance of the proposed\nprocedure. An application is illustrated with the Beijing multi-site\nair-quality data."}, "http://arxiv.org/abs/2303.09598": {"title": "Variational Bayesian analysis of survival data using a log-logistic accelerated failure time model", "link": "http://arxiv.org/abs/2303.09598", "description": "The log-logistic regression model is one of the most commonly used\naccelerated failure time (AFT) models in survival analysis, for which\nstatistical inference methods are mainly established under the frequentist\nframework. Recently, Bayesian inference for log-logistic AFT models using\nMarkov chain Monte Carlo (MCMC) techniques has also been widely developed. In\nthis work, we develop an alternative approach to MCMC methods and infer the\nparameters of the log-logistic AFT model via a mean-field variational Bayes\n(VB) algorithm. A piecewise approximation technique is embedded in deriving the\nVB algorithm to achieve conjugacy. The proposed VB algorithm is evaluated and\ncompared with typical frequentist inferences and MCMC inference using simulated\ndata under various scenarios. A publicly available dataset is employed for\nillustration. We demonstrate that the proposed VB algorithm can achieve good\nestimation accuracy and has a lower computational cost compared with MCMC\nmethods."}, "http://arxiv.org/abs/2304.03853": {"title": "StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables", "link": "http://arxiv.org/abs/2304.03853", "description": "StepMix is an open-source Python package for the pseudo-likelihood estimation\n(one-, two- and three-step approaches) of generalized finite mixture models\n(latent profile and latent class analysis) with external variables (covariates\nand distal outcomes). In many applications in social sciences, the main\nobjective is not only to cluster individuals into latent classes, but also to\nuse these classes to develop more complex statistical models. These models\ngenerally divide into a measurement model that relates the latent classes to\nobserved indicators, and a structural model that relates covariates and outcome\nvariables to the latent classes. The measurement and structural models can be\nestimated jointly using the so-called one-step approach or sequentially using\nstepwise methods, which present significant advantages for practitioners\nregarding the interpretability of the estimated latent classes. In addition to\nthe one-step approach, StepMix implements the most important stepwise\nestimation methods from the literature, including the bias-adjusted three-step\nmethods with Bolk-Croon-Hagenaars and maximum likelihood corrections and the\nmore recent two-step approach. These pseudo-likelihood estimators are presented\nin this paper under a unified framework as specific expectation-maximization\nsubroutines. To facilitate and promote their adoption among the data science\ncommunity, StepMix follows the object-oriented design of the scikit-learn\nlibrary and provides an additional R wrapper."}, "http://arxiv.org/abs/2310.06926": {"title": "Bayesian inference and cure rate modeling for event history data", "link": "http://arxiv.org/abs/2310.06926", "description": "Estimating model parameters of a general family of cure models is always a\nchallenging task mainly due to flatness and multimodality of the likelihood\nfunction. In this work, we propose a fully Bayesian approach in order to\novercome these issues. Posterior inference is carried out by constructing a\nMetropolis-coupled Markov chain Monte Carlo (MCMC) sampler, which combines\nGibbs sampling for the latent cure indicators and Metropolis-Hastings steps\nwith Langevin diffusion dynamics for parameter updates. The main MCMC algorithm\nis embedded within a parallel tempering scheme by considering heated versions\nof the target posterior distribution. It is demonstrated via simulations that\nthe proposed algorithm freely explores the multimodal posterior distribution\nand produces robust point estimates, while it outperforms maximum likelihood\nestimation via the Expectation-Maximization algorithm. A by-product of our\nBayesian implementation is to control the False Discovery Rate when classifying\nitems as cured or not. Finally, the proposed method is illustrated in a real\ndataset which refers to recidivism for offenders released from prison; the\nevent of interest is whether the offender was re-incarcerated after probation\nor not."}, "http://arxiv.org/abs/2310.06969": {"title": "Positivity-free Policy Learning with Observational Data", "link": "http://arxiv.org/abs/2310.06969", "description": "Policy learning utilizing observational data is pivotal across various\ndomains, with the objective of learning the optimal treatment assignment policy\nwhile adhering to specific constraints such as fairness, budget, and\nsimplicity. This study introduces a novel positivity-free (stochastic) policy\nlearning framework designed to address the challenges posed by the\nimpracticality of the positivity assumption in real-world scenarios. This\nframework leverages incremental propensity score policies to adjust propensity\nscore values instead of assigning fixed values to treatments. We characterize\nthese incremental propensity score policies and establish identification\nconditions, employing semiparametric efficiency theory to propose efficient\nestimators capable of achieving rapid convergence rates, even when integrated\nwith advanced machine learning algorithms. This paper provides a thorough\nexploration of the theoretical guarantees associated with policy learning and\nvalidates the proposed framework's finite-sample performance through\ncomprehensive numerical experiments, ensuring the identification of causal\neffects from observational data is both robust and reliable."}, "http://arxiv.org/abs/2310.07002": {"title": "Bayesian cross-validation by parallel Markov Chain Monte Carlo", "link": "http://arxiv.org/abs/2310.07002", "description": "Brute force cross-validation (CV) is a method for predictive assessment and\nmodel selection that is general and applicable to a wide range of Bayesian\nmodels. However, in many cases brute force CV is too computationally burdensome\nto form part of interactive modeling workflows, especially when inference\nrelies on Markov chain Monte Carlo (MCMC). In this paper we present a method\nfor conducting fast Bayesian CV by massively parallel MCMC. On suitable\naccelerator hardware, for many applications our approach is about as fast (in\nwall clock time) as a single full-data model fit.\n\nParallel CV is more flexible than existing fast CV approximation methods\nbecause it can easily exploit a wide range of scoring rules and data\npartitioning schemes. This is particularly useful for CV methods designed for\nnon-exchangeable data. Our approach also delivers accurate estimates of Monte\nCarlo and CV uncertainty. In addition to parallelizing computations, parallel\nCV speeds up inference by reusing information from earlier MCMC adaptation and\ninference obtained during initial model fitting and checking of the full-data\nmodel.\n\nWe propose MCMC diagnostics for parallel CV applications, including a summary\nof MCMC mixing based on the popular potential scale reduction factor\n($\\hat{R}$) and MCMC effective sample size ($\\widehat{ESS}$) measures.\nFurthermore, we describe a method for determining whether an $\\hat{R}$\ndiagnostic indicates approximate stationarity of the chains, that may be of\nmore general interest for applications beyond parallel CV.\n\nFor parallel CV to work on memory-constrained computing accelerators, we show\nthat parallel CV and associated diagnostics can be implemented using online\n(streaming) algorithms ideal for parallel computing environments with limited\nmemory. Constant memory algorithms allow parallel CV to scale up to very large\nblocking designs."}, "http://arxiv.org/abs/2310.07016": {"title": "Discovering the Unknowns: A First Step", "link": "http://arxiv.org/abs/2310.07016", "description": "This article aims at discovering the unknown variables in the system through\ndata analysis. The main idea is to use the time of data collection as a\nsurrogate variable and try to identify the unknown variables by modeling\ngradual and sudden changes in the data. We use Gaussian process modeling and a\nsparse representation of the sudden changes to efficiently estimate the large\nnumber of parameters in the proposed statistical model. The method is tested on\na realistic dataset generated using a one-dimensional implementation of a\nMagnetized Liner Inertial Fusion (MagLIF) simulation model and encouraging\nresults are obtained."}, "http://arxiv.org/abs/2310.07107": {"title": "Root n consistent extremile regression and its supervised and semi-supervised learning", "link": "http://arxiv.org/abs/2310.07107", "description": "Extremile (Daouia, Gijbels and Stupfler,2019) is a novel and coherent measure\nof risk, determined by weighted expectations rather than tail probabilities. It\nfinds application in risk management, and, in contrast to quantiles, it\nfulfills the axioms of consistency, taking into account the severity of tail\nlosses. However, existing studies (Daouia, Gijbels and Stupfler,2019,2022) on\nextremile involve unknown distribution functions, making it challenging to\nobtain a root n-consistent estimator for unknown parameters in linear extremile\nregression. This article introduces a new definition of linear extremile\nregression and its estimation method, where the estimator is root n-consistent.\nAdditionally, while the analysis of unlabeled data for extremes presents a\nsignificant challenge and is currently a topic of great interest in machine\nlearning for various classification problems, we have developed a\nsemi-supervised framework for the proposed extremile regression using unlabeled\ndata. This framework can also enhance estimation accuracy under model\nmisspecification. Both simulations and real data analyses have been conducted\nto illustrate the finite sample performance of the proposed methods."}, "http://arxiv.org/abs/2310.07124": {"title": "Systematic simulation of age-period-cohort analysis: Demonstrating bias of Bayesian regularization", "link": "http://arxiv.org/abs/2310.07124", "description": "Age-period-cohort (APC) analysis is one of the fundamental time-series\nanalyses used in the social sciences. This paper evaluates APC analysis via\nsystematic simulation in term of how well the artificial parameters are\nrecovered. We consider three models of Bayesian regularization using normal\nprior distributions: the random effects model with reference to multilevel\nanalysis, the ridge regression model equivalent to the intrinsic estimator, and\nthe random walk model referred to as the Bayesian cohort model. The proposed\nsimulation generates artificial data through combinations of the linear\ncomponents, focusing on the fact that the identification problem affects the\nlinear components of the three effects. Among the 13 cases of artificial data,\nthe random walk model recovered the artificial parameters well in 10 cases,\nwhile the random effects model and the ridge regression model did so in 4\ncases. The cases in which the models failed to recover the artificial\nparameters show the estimated linear component of the cohort effects as close\nto zero. In conclusion, the models of Bayesian regularization in APC analysis\nhave a bias: the index weights have a large influence on the cohort effects and\nthese constraints drive the linear component of the cohort effects close to\nzero. However, the random walk model mitigates underestimating the linear\ncomponent of the cohort effects."}, "http://arxiv.org/abs/2310.07330": {"title": "Functional Generalized Canonical Correlation Analysis for studying multiple longitudinal variables", "link": "http://arxiv.org/abs/2310.07330", "description": "In this paper, we introduce Functional Generalized Canonical Correlation\nAnalysis (FGCCA), a new framework for exploring associations between multiple\nrandom processes observed jointly. The framework is based on the multiblock\nRegularized Generalized Canonical Correlation Analysis (RGCCA) framework. It is\nrobust to sparsely and irregularly observed data, making it applicable in many\nsettings. We establish the monotonic property of the solving procedure and\nintroduce a Bayesian approach for estimating canonical components. We propose\nan extension of the framework that allows the integration of a univariate or\nmultivariate response into the analysis, paving the way for predictive\napplications. We evaluate the method's efficiency in simulation studies and\npresent a use case on a longitudinal dataset."}, "http://arxiv.org/abs/2310.07364": {"title": "Statistical inference of high-dimensional vector autoregressive time series with non-i", "link": "http://arxiv.org/abs/2310.07364", "description": "Independent or i.i.d. innovations is an essential assumption in the\nliterature for analyzing a vector time series. However, this assumption is\neither too restrictive for a real-life time series to satisfy or is hard to\nverify through a hypothesis test. This paper performs statistical inference on\na sparse high-dimensional vector autoregressive time series, allowing its white\nnoise innovations to be dependent, even non-stationary. To achieve this goal,\nit adopts a post-selection estimator to fit the vector autoregressive model and\nderives the asymptotic distribution of the post-selection estimator. The\ninnovations in the autoregressive time series are not assumed to be\nindependent, thus making the covariance matrices of the autoregressive\ncoefficient estimators complex and difficult to estimate. Our work develops a\nbootstrap algorithm to facilitate practitioners in performing statistical\ninference without having to engage in sophisticated calculations. Simulations\nand real-life data experiments reveal the validity of the proposed methods and\ntheoretical results.\n\nReal-life data is rarely considered to exactly satisfy an autoregressive\nmodel with independent or i.i.d. innovations, so our work should better reflect\nthe reality compared to the literature that assumes i.i.d. innovations."}, "http://arxiv.org/abs/2310.07399": {"title": "Randomized Runge-Kutta-Nystr\\\"om", "link": "http://arxiv.org/abs/2310.07399", "description": "We present 5/2- and 7/2-order $L^2$-accurate randomized Runge-Kutta-Nystr\\\"om\nmethods to approximate the Hamiltonian flow underlying various non-reversible\nMarkov chain Monte Carlo chains including unadjusted Hamiltonian Monte Carlo\nand unadjusted kinetic Langevin chains. Quantitative 5/2-order $L^2$-accuracy\nupper bounds are provided under gradient and Hessian Lipschitz assumptions on\nthe potential energy function. The superior complexity of the corresponding\nMarkov chains is numerically demonstrated for a selection of `well-behaved',\nhigh-dimensional target distributions."}, "http://arxiv.org/abs/2310.07456": {"title": "Hierarchical Bayesian Claim Count modeling with Overdispersed Outcome and Mismeasured Covariates in Actuarial Practice", "link": "http://arxiv.org/abs/2310.07456", "description": "The problem of overdispersed claim counts and mismeasured covariates is\ncommon in insurance. On the one hand, the presence of overdispersion in the\ncount data violates the homogeneity assumption, and on the other hand,\nmeasurement errors in covariates highlight the model risk issue in actuarial\npractice. The consequence can be inaccurate premium pricing which would\nnegatively affect business competitiveness. Our goal is to address these two\nmodelling problems simultaneously by capturing the unobservable correlations\nbetween observations that arise from overdispersed outcome and mismeasured\ncovariate in actuarial process. To this end, we establish novel connections\nbetween the count-based generalized linear mixed model (GLMM) and a popular\nerror-correction tool for non-linear modelling - Simulation Extrapolation\n(SIMEX). We consider a modelling framework based on the hierarchical Bayesian\nparadigm. To our knowledge, the approach of combining a hierarchical Bayes with\nSIMEX has not previously been discussed in the literature. We demonstrate the\napplicability of our approach on the workplace absenteeism data. Our results\nindicate that the hierarchical Bayesian GLMM incorporated with the SIMEX\noutperforms naive GLMM / SIMEX in terms of goodness of fit."}, "http://arxiv.org/abs/2310.07567": {"title": "Comparing the effectiveness of k-different treatments through the area under the ROC curve", "link": "http://arxiv.org/abs/2310.07567", "description": "The area under the receiver-operating characteristic curve (AUC) has become a\npopular index not only for measuring the overall prediction capacity of a\nmarker but also the association strength between continuous and binary\nvariables. In the current study, it has been used for comparing the association\nsize of four different interventions involving impulsive decision making,\nstudied through an animal model, in which each animal provides several negative\n(pre-treatment) and positive (post-treatment) measures. The problem of the full\ncomparison of the average AUCs arises therefore in a natural way. We construct\nan analysis of variance (ANOVA) type test for testing the equality of the\nimpact of these treatments measured through the respective AUCs, and\nconsidering the random-effect represented by the animal. The use (and\ndevelopment) of a post-hoc Tukey's HSD type test is also considered. We explore\nthe finite-sample behavior of our proposal via Monte Carlo simulations, and\nanalyze the data generated from the original problem. An R package implementing\nthe procedures is provided as supplementary material."}, "http://arxiv.org/abs/2310.07605": {"title": "Split Knockoffs for Multiple Comparisons: Controlling the Directional False Discovery Rate", "link": "http://arxiv.org/abs/2310.07605", "description": "Multiple comparisons in hypothesis testing often encounter structural\nconstraints in various applications. For instance, in structural Magnetic\nResonance Imaging for Alzheimer's Disease, the focus extends beyond examining\natrophic brain regions to include comparisons of anatomically adjacent regions.\nThese constraints can be modeled as linear transformations of parameters, where\nthe sign patterns play a crucial role in estimating directional effects. This\nclass of problems, encompassing total variations, wavelet transforms, fused\nLASSO, trend filtering, and more, presents an open challenge in effectively\ncontrolling the directional false discovery rate. In this paper, we propose an\nextended Split Knockoff method specifically designed to address the control of\ndirectional false discovery rate under linear transformations. Our proposed\napproach relaxes the stringent linear manifold constraint to its neighborhood,\nemploying a variable splitting technique commonly used in optimization. This\nmethodology yields an orthogonal design that benefits both power and\ndirectional false discovery rate control. By incorporating a sample splitting\nscheme, we achieve effective control of the directional false discovery rate,\nwith a notable reduction to zero as the relaxed neighborhood expands. To\ndemonstrate the efficacy of our method, we conduct simulation experiments and\napply it to two real-world scenarios: Alzheimer's Disease analysis and human\nage comparisons."}, "http://arxiv.org/abs/2310.07680": {"title": "Hamiltonian Dynamics of Bayesian Inference Formalised by Arc Hamiltonian Systems", "link": "http://arxiv.org/abs/2310.07680", "description": "This paper makes two theoretical contributions. First, we establish a novel\nclass of Hamiltonian systems, called arc Hamiltonian systems, for saddle\nHamiltonian functions over infinite-dimensional metric spaces. Arc Hamiltonian\nsystems generate a flow that satisfies the law of conservation of energy\neverywhere in a metric space. They are governed by an extension of Hamilton's\nequation formulated based on (i) the framework of arc fields and (ii) an\ninfinite-dimensional gradient, termed the arc gradient, of a Hamiltonian\nfunction. We derive conditions for the existence of a flow generated by an arc\nHamiltonian system, showing that they reduce to local Lipschitz continuity of\nthe arc gradient under sufficient regularity. Second, we present two\nHamiltonian functions, called the cumulant generating functional and the\ncentred cumulant generating functional, over a metric space of log-likelihoods\nand measures. The former characterises the posterior of Bayesian inference as a\npart of the arc gradient that induces a flow of log-likelihoods and\nnon-negative measures. The latter characterises the difference of the posterior\nand the prior as a part of the arc gradient that induces a flow of\nlog-likelihoods and probability measures. Our results reveal an implication of\nthe belief updating mechanism from the prior to the posterior as an\ninfinitesimal change of a measure in the infinite-dimensional Hamiltonian\nflows."}, "http://arxiv.org/abs/2009.12217": {"title": "Latent Causal Socioeconomic Health Index", "link": "http://arxiv.org/abs/2009.12217", "description": "This research develops a model-based LAtent Causal Socioeconomic Health\n(LACSH) index at the national level. Motivated by the need for a holistic\nnational well-being index, we build upon the latent health factor index (LHFI)\napproach that has been used to assess the unobservable ecological/ecosystem\nhealth. LHFI integratively models the relationship between metrics, latent\nhealth, and covariates that drive the notion of health. In this paper, the LHFI\nstructure is integrated with spatial modeling and statistical causal modeling.\nOur efforts are focused on developing the integrated framework to facilitate\nthe understanding of how an observational continuous variable might have\ncausally affected a latent trait that exhibits spatial correlation. A novel\nvisualization technique to evaluate covariate balance is also introduced for\nthe case of a continuous policy (treatment) variable. Our resulting LACSH\nframework and visualization tool are illustrated through two global case\nstudies on national socioeconomic health (latent trait), each with various\nmetrics and covariates pertaining to different aspects of societal health, and\nthe treatment variable being mandatory maternity leave days and government\nexpenditure on healthcare, respectively. We validate our model by two\nsimulation studies. All approaches are structured in a Bayesian hierarchical\nframework and results are obtained by Markov chain Monte Carlo techniques."}, "http://arxiv.org/abs/2201.02958": {"title": "Smooth Nested Simulation: Bridging Cubic and Square Root Convergence Rates in High Dimensions", "link": "http://arxiv.org/abs/2201.02958", "description": "Nested simulation concerns estimating functionals of a conditional\nexpectation via simulation. In this paper, we propose a new method based on\nkernel ridge regression to exploit the smoothness of the conditional\nexpectation as a function of the multidimensional conditioning variable.\nAsymptotic analysis shows that the proposed method can effectively alleviate\nthe curse of dimensionality on the convergence rate as the simulation budget\nincreases, provided that the conditional expectation is sufficiently smooth.\nThe smoothness bridges the gap between the cubic root convergence rate (that\nis, the optimal rate for the standard nested simulation) and the square root\nconvergence rate (that is, the canonical rate for the standard Monte Carlo\nsimulation). We demonstrate the performance of the proposed method via\nnumerical examples from portfolio risk management and input uncertainty\nquantification."}, "http://arxiv.org/abs/2204.12635": {"title": "Multivariate and regression models for directional data based on projected P\\'olya trees", "link": "http://arxiv.org/abs/2204.12635", "description": "Projected distributions have proved to be useful in the study of circular and\ndirectional data. Although any multivariate distribution can be used to produce\na projected model, these distributions are typically parametric. In this\narticle we consider a multivariate P\\'olya tree on $R^k$ and project it to the\nunit hypersphere $S^k$ to define a new Bayesian nonparametric model for\ndirectional data. We study the properties of the proposed model and in\nparticular, concentrate on the implied conditional distributions of some\ndirections given the others to define a directional-directional regression\nmodel. We also define a multivariate linear regression model with P\\'olya tree\nerror and project it to define a linear-directional regression model. We obtain\nthe posterior characterisation of all models and show their performance with\nsimulated and real datasets."}, "http://arxiv.org/abs/2207.13250": {"title": "Spatio-Temporal Wildfire Prediction using Multi-Modal Data", "link": "http://arxiv.org/abs/2207.13250", "description": "Due to severe societal and environmental impacts, wildfire prediction using\nmulti-modal sensing data has become a highly sought-after data-analytical tool\nby various stakeholders (such as state governments and power utility companies)\nto achieve a more informed understanding of wildfire activities and plan\npreventive measures. A desirable algorithm should precisely predict fire risk\nand magnitude for a location in real time. In this paper, we develop a flexible\nspatio-temporal wildfire prediction framework using multi-modal time series\ndata. We first predict the wildfire risk (the chance of a wildfire event) in\nreal-time, considering the historical events using discrete mutually exciting\npoint process models. Then we further develop a wildfire magnitude prediction\nset method based on the flexible distribution-free time-series conformal\nprediction (CP) approach. Theoretically, we prove a risk model parameter\nrecovery guarantee, as well as coverage and set size guarantees for the CP\nsets. Through extensive real-data experiments with wildfire data in California,\nwe demonstrate the effectiveness of our methods, as well as their flexibility\nand scalability in large regions."}, "http://arxiv.org/abs/2210.13550": {"title": "Regularized Nonlinear Regression with Dependent Errors and its Application to a Biomechanical Model", "link": "http://arxiv.org/abs/2210.13550", "description": "A biomechanical model often requires parameter estimation and selection in a\nknown but complicated nonlinear function. Motivated by observing that data from\na head-neck position tracking system, one of biomechanical models, show\nmultiplicative time dependent errors, we develop a modified penalized weighted\nleast squares estimator. The proposed method can be also applied to a model\nwith non-zero mean time dependent additive errors. Asymptotic properties of the\nproposed estimator are investigated under mild conditions on a weight matrix\nand the error process. A simulation study demonstrates that the proposed\nestimation works well in both parameter estimation and selection with time\ndependent error. The analysis and comparison with an existing method for\nhead-neck position tracking data show better performance of the proposed method\nin terms of the variance accounted for (VAF)."}, "http://arxiv.org/abs/2210.14965": {"title": "Topology-Driven Goodness-of-Fit Tests in Arbitrary Dimensions", "link": "http://arxiv.org/abs/2210.14965", "description": "This paper adopts a tool from computational topology, the Euler\ncharacteristic curve (ECC) of a sample, to perform one- and two-sample goodness\nof fit tests. We call our procedure TopoTests. The presented tests work for\nsamples of arbitrary dimension, having comparable power to the state-of-the-art\ntests in the one-dimensional case. It is demonstrated that the type I error of\nTopoTests can be controlled and their type II error vanishes exponentially with\nincreasing sample size. Extensive numerical simulations of TopoTests are\nconducted to demonstrate their power for samples of various sizes."}, "http://arxiv.org/abs/2211.03860": {"title": "Automatic Change-Point Detection in Time Series via Deep Learning", "link": "http://arxiv.org/abs/2211.03860", "description": "Detecting change-points in data is challenging because of the range of\npossible types of change and types of behaviour of data when there is no\nchange. Statistically efficient methods for detecting a change will depend on\nboth of these features, and it can be difficult for a practitioner to develop\nan appropriate detection method for their application of interest. We show how\nto automatically generate new offline detection methods based on training a\nneural network. Our approach is motivated by many existing tests for the\npresence of a change-point being representable by a simple neural network, and\nthus a neural network trained with sufficient data should have performance at\nleast as good as these methods. We present theory that quantifies the error\nrate for such an approach, and how it depends on the amount of training data.\nEmpirical results show that, even with limited training data, its performance\nis competitive with the standard CUSUM-based classifier for detecting a change\nin mean when the noise is independent and Gaussian, and can substantially\noutperform it in the presence of auto-correlated or heavy-tailed noise. Our\nmethod also shows strong results in detecting and localising changes in\nactivity based on accelerometer data."}, "http://arxiv.org/abs/2211.09099": {"title": "Selecting Subpopulations for Causal Inference in Regression Discontinuity Designs", "link": "http://arxiv.org/abs/2211.09099", "description": "The Brazil Bolsa Familia (BF) program is a conditional cash transfer program\naimed to reduce short-term poverty by direct cash transfers and to fight\nlong-term poverty by increasing human capital among poor Brazilian people.\nEligibility for Bolsa Familia benefits depends on a cutoff rule, which\nclassifies the BF study as a regression discontinuity (RD) design. Extracting\ncausal information from RD studies is challenging. Following Li et al (2015)\nand Branson and Mealli (2019), we formally describe the BF RD design as a local\nrandomized experiment within the potential outcome approach. Under this\nframework, causal effects can be identified and estimated on a subpopulation\nwhere a local overlap assumption, a local SUTVA and a local ignorability\nassumption hold. We first discuss the potential advantages of this framework\nover local regression methods based on continuity assumptions, which concern\nthe definition of the causal estimands, the design and the analysis of the\nstudy, and the interpretation and generalizability of the results. A critical\nissue of this local randomization approach is how to choose subpopulations for\nwhich we can draw valid causal inference. We propose a Bayesian model-based\nfinite mixture approach to clustering to classify observations into\nsubpopulations where the RD assumptions hold and do not hold. This approach has\nimportant advantages: a) it allows to account for the uncertainty in the\nsubpopulation membership, which is typically neglected; b) it does not impose\nany constraint on the shape of the subpopulation; c) it is scalable to\nhigh-dimensional settings; e) it allows to target alternative causal estimands\nthan the average treatment effect (ATE); and f) it is robust to a certain\ndegree of manipulation/selection of the running variable. We apply our proposed\napproach to assess causal effects of the Bolsa Familia program on leprosy\nincidence in 2009."}, "http://arxiv.org/abs/2301.08276": {"title": "Cross-validatory model selection for Bayesian autoregressions with exogenous regressors", "link": "http://arxiv.org/abs/2301.08276", "description": "Bayesian cross-validation (CV) is a popular method for predictive model\nassessment that is simple to implement and broadly applicable. A wide range of\nCV schemes is available for time series applications, including generic\nleave-one-out (LOO) and K-fold methods, as well as specialized approaches\nintended to deal with serial dependence such as leave-future-out (LFO),\nh-block, and hv-block.\n\nExisting large-sample results show that both specialized and generic methods\nare applicable to models of serially-dependent data. However, large sample\nconsistency results overlook the impact of sampling variability on accuracy in\nfinite samples. Moreover, the accuracy of a CV scheme depends on many aspects\nof the procedure. We show that poor design choices can lead to elevated rates\nof adverse selection.\n\nIn this paper, we consider the problem of identifying the regression\ncomponent of an important class of models of data with serial dependence,\nautoregressions of order p with q exogenous regressors (ARX(p,q)), under the\nlogarithmic scoring rule. We show that when serial dependence is present,\nscores computed using the joint (multivariate) density have lower variance and\nbetter model selection accuracy than the popular pointwise estimator. In\naddition, we present a detailed case study of the special case of ARX models\nwith fixed autoregressive structure and variance. For this class, we derive the\nfinite-sample distribution of the CV estimators and the model selection\nstatistic. We conclude with recommendations for practitioners."}, "http://arxiv.org/abs/2301.12026": {"title": "G-formula for causal inference via multiple imputation", "link": "http://arxiv.org/abs/2301.12026", "description": "G-formula is a popular approach for estimating treatment or exposure effects\nfrom longitudinal data that are subject to time-varying confounding. G-formula\nestimation is typically performed by Monte-Carlo simulation, with\nnon-parametric bootstrapping used for inference. We show that G-formula can be\nimplemented by exploiting existing methods for multiple imputation (MI) for\nsynthetic data. This involves using an existing modified version of Rubin's\nvariance estimator. In practice missing data is ubiquitous in longitudinal\ndatasets. We show that such missing data can be readily accommodated as part of\nthe MI procedure when using G-formula, and describe how MI software can be used\nto implement the approach. We explore its performance using a simulation study\nand an application from cystic fibrosis."}, "http://arxiv.org/abs/2306.01292": {"title": "Alternative Measures of Direct and Indirect Effects", "link": "http://arxiv.org/abs/2306.01292", "description": "There are a number of measures of direct and indirect effects in the\nliterature. They are suitable in some cases and unsuitable in others. We\ndescribe a case where the existing measures are unsuitable and propose new\nsuitable ones. We also show that the new measures can partially handle\nunmeasured treatment-outcome confounding, and bound long-term effects by\ncombining experimental and observational data."}, "http://arxiv.org/abs/2308.00913": {"title": "The Bayesian Context Trees State Space Model for time series modelling and forecasting", "link": "http://arxiv.org/abs/2308.00913", "description": "A hierarchical Bayesian framework is introduced for developing rich mixture\nmodels for real-valued time series, partly motivated by important applications\nin financial time series analysis. At the top level, meaningful discrete states\nare identified as appropriately quantised values of some of the most recent\nsamples. These observable states are described as a discrete context-tree\nmodel. At the bottom level, a different, arbitrary model for real-valued time\nseries -- a base model -- is associated with each state. This defines a very\ngeneral framework that can be used in conjunction with any existing model class\nto build flexible and interpretable mixture models. We call this the Bayesian\nContext Trees State Space Model, or the BCT-X framework. Efficient algorithms\nare introduced that allow for effective, exact Bayesian inference and learning\nin this setting; in particular, the maximum a posteriori probability (MAP)\ncontext-tree model can be identified. These algorithms can be updated\nsequentially, facilitating efficient online forecasting. The utility of the\ngeneral framework is illustrated in two particular instances: When\nautoregressive (AR) models are used as base models, resulting in a nonlinear AR\nmixture model, and when conditional heteroscedastic (ARCH) models are used,\nresulting in a mixture model that offers a powerful and systematic way of\nmodelling the well-known volatility asymmetries in financial data. In\nforecasting, the BCT-X methods are found to outperform state-of-the-art\ntechniques on simulated and real-world data, both in terms of accuracy and\ncomputational requirements. In modelling, the BCT-X structure finds natural\nstructure present in the data. In particular, the BCT-ARCH model reveals a\nnovel, important feature of stock market index data, in the form of an enhanced\nleverage effect."}, "http://arxiv.org/abs/2309.11942": {"title": "On the Probability of Immunity", "link": "http://arxiv.org/abs/2309.11942", "description": "This work is devoted to the study of the probability of immunity, i.e. the\neffect occurs whether exposed or not. We derive necessary and sufficient\nconditions for non-immunity and $\\epsilon$-bounded immunity, i.e. the\nprobability of immunity is zero and $\\epsilon$-bounded, respectively. The\nformer allows us to estimate the probability of benefit (i.e., the effect\noccurs if and only if exposed) from a randomized controlled trial, and the\nlatter allows us to produce bounds of the probability of benefit that are\ntighter than the existing ones. We also introduce the concept of indirect\nimmunity (i.e., through a mediator) and repeat our previous analysis for it.\nFinally, we propose a method for sensitivity analysis of the probability of\nimmunity under unmeasured confounding."}, "http://arxiv.org/abs/2309.13441": {"title": "Anytime valid and asymptotically optimal statistical inference driven by predictive recursion", "link": "http://arxiv.org/abs/2309.13441", "description": "Distinguishing two classes of candidate models is a fundamental and\npractically important problem in statistical inference. Error rate control is\ncrucial to the logic but, in complex nonparametric settings, such guarantees\ncan be difficult to achieve, especially when the stopping rule that determines\nthe data collection process is not available. In this paper we develop a novel\ne-process construction that leverages the so-called predictive recursion (PR)\nalgorithm designed to rapidly and recursively fit nonparametric mixture models.\nThe resulting PRe-process affords anytime valid inference uniformly over\nstopping rules and is shown to be efficient in the sense that it achieves the\nmaximal growth rate under the alternative relative to the mixture model being\nfit by PR. In the special case of testing for a log-concave density, the\nPRe-process test is computationally simpler and faster, more stable, and no\nless efficient compared to a recently proposed anytime valid test."}, "http://arxiv.org/abs/2309.16598": {"title": "Cross-Prediction-Powered Inference", "link": "http://arxiv.org/abs/2309.16598", "description": "While reliable data-driven decision-making hinges on high-quality labeled\ndata, the acquisition of quality labels often involves laborious human\nannotations or slow and expensive scientific measurements. Machine learning is\nbecoming an appealing alternative as sophisticated predictive techniques are\nbeing used to quickly and cheaply produce large amounts of predicted labels;\ne.g., predicted protein structures are used to supplement experimentally\nderived structures, predictions of socioeconomic indicators from satellite\nimagery are used to supplement accurate survey data, and so on. Since\npredictions are imperfect and potentially biased, this practice brings into\nquestion the validity of downstream inferences. We introduce cross-prediction:\na method for valid inference powered by machine learning. With a small labeled\ndataset and a large unlabeled dataset, cross-prediction imputes the missing\nlabels via machine learning and applies a form of debiasing to remedy the\nprediction inaccuracies. The resulting inferences achieve the desired error\nprobability and are more powerful than those that only leverage the labeled\ndata. Closely related is the recent proposal of prediction-powered inference,\nwhich assumes that a good pre-trained model is already available. We show that\ncross-prediction is consistently more powerful than an adaptation of\nprediction-powered inference in which a fraction of the labeled data is split\noff and used to train the model. Finally, we observe that cross-prediction\ngives more stable conclusions than its competitors; its confidence intervals\ntypically have significantly lower variability."}, "http://arxiv.org/abs/2310.07801": {"title": "Trajectory-aware Principal Manifold Framework for Data Augmentation and Image Generation", "link": "http://arxiv.org/abs/2310.07801", "description": "Data augmentation for deep learning benefits model training, image\ntransformation, medical imaging analysis and many other fields. Many existing\nmethods generate new samples from a parametric distribution, like the Gaussian,\nwith little attention to generate samples along the data manifold in either the\ninput or feature space. In this paper, we verify that there are theoretical and\npractical advantages of using the principal manifold hidden in the feature\nspace than the Gaussian distribution. We then propose a novel trajectory-aware\nprincipal manifold framework to restore the manifold backbone and generate\nsamples along a specific trajectory. On top of the autoencoder architecture, we\nfurther introduce an intrinsic dimension regularization term to make the\nmanifold more compact and enable few-shot image generation. Experimental\nresults show that the novel framework is able to extract more compact manifold\nrepresentation, improve classification accuracy and generate smooth\ntransformation among few samples."}, "http://arxiv.org/abs/2310.07817": {"title": "Nonlinear global Fr\\'echet regression for random objects via weak conditional expectation", "link": "http://arxiv.org/abs/2310.07817", "description": "Random objects are complex non-Euclidean data taking value in general metric\nspace, possibly devoid of any underlying vector space structure. Such data are\ngetting increasingly abundant with the rapid advancement in technology.\nExamples include probability distributions, positive semi-definite matrices,\nand data on Riemannian manifolds. However, except for regression for\nobject-valued response with Euclidean predictors and\ndistribution-on-distribution regression, there has been limited development of\na general framework for object-valued response with object-valued predictors in\nthe literature. To fill this gap, we introduce the notion of a weak conditional\nFr\\'echet mean based on Carleman operators and then propose a global nonlinear\nFr\\'echet regression model through the reproducing kernel Hilbert space (RKHS)\nembedding. Furthermore, we establish the relationships between the conditional\nFr\\'echet mean and the weak conditional Fr\\'echet mean for both Euclidean and\nobject-valued data. We also show that the state-of-the-art global Fr\\'echet\nregression developed by Petersen and Mueller, 2019 emerges as a special case of\nour method by choosing a linear kernel. We require that the metric space for\nthe predictor admits a reproducing kernel, while the intrinsic geometry of the\nmetric space for the response is utilized to study the asymptotic properties of\nthe proposed estimates. Numerical studies, including extensive simulations and\na real application, are conducted to investigate the performance of our\nestimator in a finite sample."}, "http://arxiv.org/abs/2310.07839": {"title": "Marital Sorting, Household Inequality and Selection", "link": "http://arxiv.org/abs/2310.07839", "description": "Using CPS data for 1976 to 2022 we explore how wage inequality has evolved\nfor married couples with both spouses working full time full year, and its\nimpact on household income inequality. We also investigate how marriage sorting\npatterns have changed over this period. To determine the factors driving income\ninequality we estimate a model explaining the joint distribution of wages which\naccounts for the spouses' employment decisions. We find that income inequality\nhas increased for these households and increased assortative matching of wages\nhas exacerbated the inequality resulting from individual wage growth. We find\nthat positive sorting partially reflects the correlation across unobservables\ninfluencing both members' of the marriage wages. We decompose the changes in\nsorting patterns over the 47 years comprising our sample into structural,\ncomposition and selection effects and find that the increase in positive\nsorting primarily reflects the increased skill premia for both observed and\nunobserved characteristics."}, "http://arxiv.org/abs/2310.07850": {"title": "Conformal prediction with local weights: randomization enables local guarantees", "link": "http://arxiv.org/abs/2310.07850", "description": "In this work, we consider the problem of building distribution-free\nprediction intervals with finite-sample conditional coverage guarantees.\nConformal prediction (CP) is an increasingly popular framework for building\nprediction intervals with distribution-free guarantees, but these guarantees\nonly ensure marginal coverage: the probability of coverage is averaged over a\nrandom draw of both the training and test data, meaning that there might be\nsubstantial undercoverage within certain subpopulations. Instead, ideally, we\nwould want to have local coverage guarantees that hold for each possible value\nof the test point's features. While the impossibility of achieving pointwise\nlocal coverage is well established in the literature, many variants of\nconformal prediction algorithm show favorable local coverage properties\nempirically. Relaxing the definition of local coverage can allow for a\ntheoretical understanding of this empirical phenomenon. We aim to bridge this\ngap between theoretical validation and empirical performance by proving\nachievable and interpretable guarantees for a relaxed notion of local coverage.\nBuilding on the localized CP method of Guan (2023) and the weighted CP\nframework of Tibshirani et al. (2019), we propose a new method,\nrandomly-localized conformal prediction (RLCP), which returns prediction\nintervals that are not only marginally valid but also achieve a relaxed local\ncoverage guarantee and guarantees under covariate shift. Through a series of\nsimulations and real data experiments, we validate these coverage guarantees of\nRLCP while comparing it with the other local conformal prediction methods."}, "http://arxiv.org/abs/2310.07852": {"title": "On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism", "link": "http://arxiv.org/abs/2310.07852", "description": "We consider the problem of model selection in a high-dimensional sparse\nlinear regression model under the differential privacy framework. In\nparticular, we consider the problem of differentially private best subset\nselection and study its utility guarantee. We adopt the well-known exponential\nmechanism for selecting the best model, and under a certain margin condition,\nwe establish its strong model recovery property. However, the exponential\nsearch space of the exponential mechanism poses a serious computational\nbottleneck. To overcome this challenge, we propose a Metropolis-Hastings\nalgorithm for the sampling step and establish its polynomial mixing time to its\nstationary distribution in the problem parameters $n,p$, and $s$. Furthermore,\nwe also establish approximate differential privacy for the final estimates of\nthe Metropolis-Hastings random walk using its mixing property. Finally, we also\nperform some illustrative simulations that echo the theoretical findings of our\nmain results."}, "http://arxiv.org/abs/2310.07935": {"title": "Estimating the Likelihood of Arrest from Police Records in Presence of Unreported Crimes", "link": "http://arxiv.org/abs/2310.07935", "description": "Many important policy decisions concerning policing hinge on our\nunderstanding of how likely various criminal offenses are to result in arrests.\nSince many crimes are never reported to law enforcement, estimates based on\npolice records alone must be adjusted to account for the likelihood that each\ncrime would have been reported to the police. In this paper, we present a\nmethodological framework for estimating the likelihood of arrest from police\ndata that incorporates estimates of crime reporting rates computed from a\nvictimization survey. We propose a parametric regression-based two-step\nestimator that (i) estimates the likelihood of crime reporting using logistic\nregression with survey weights; and then (ii) applies a second regression step\nto model the likelihood of arrest. Our empirical analysis focuses on racial\ndisparities in arrests for violent crimes (sex offenses, robbery, aggravated\nand simple assaults) from 2006--2015 police records from the National Incident\nBased Reporting System (NIBRS), with estimates of crime reporting obtained\nusing 2003--2020 data from the National Crime Victimization Survey (NCVS). We\nfind that, after adjusting for unreported crimes, the likelihood of arrest\ncomputed from police records decreases significantly. We also find that, while\nincidents with white offenders on average result in arrests more often than\nthose with black offenders, the disparities tend to be small after accounting\nfor crime characteristics and unreported crimes."}, "http://arxiv.org/abs/2310.07953": {"title": "Enhancing Sample Quality through Minimum Energy Importance Weights", "link": "http://arxiv.org/abs/2310.07953", "description": "Importance sampling is a powerful tool for correcting the distributional\nmismatch in many statistical and machine learning problems, but in practice its\nperformance is limited by the usage of simple proposals whose importance\nweights can be computed analytically. To address this limitation, Liu and Lee\n(2017) proposed a Black-Box Importance Sampling (BBIS) algorithm that computes\nthe importance weights for arbitrary simulated samples by minimizing the\nkernelized Stein discrepancy. However, this requires knowing the score function\nof the target distribution, which is not easy to compute for many Bayesian\nproblems. Hence, in this paper we propose another novel BBIS algorithm using\nminimum energy design, BBIS-MED, that requires only the unnormalized density\nfunction, which can be utilized as a post-processing step to improve the\nquality of Markov Chain Monte Carlo samples. We demonstrate the effectiveness\nand wide applicability of our proposed BBIS-MED algorithm on extensive\nsimulations and a real-world Bayesian model calibration problem where the score\nfunction cannot be derived analytically."}, "http://arxiv.org/abs/2310.07958": {"title": "Towards Causal Deep Learning for Vulnerability Detection", "link": "http://arxiv.org/abs/2310.07958", "description": "Deep learning vulnerability detection has shown promising results in recent\nyears. However, an important challenge that still blocks it from being very\nuseful in practice is that the model is not robust under perturbation and it\ncannot generalize well over the out-of-distribution (OOD) data, e.g., applying\na trained model to unseen projects in real world. We hypothesize that this is\nbecause the model learned non-robust features, e.g., variable names, that have\nspurious correlations with labels. When the perturbed and OOD datasets no\nlonger have the same spurious features, the model prediction fails. To address\nthe challenge, in this paper, we introduced causality into deep learning\nvulnerability detection. Our approach CausalVul consists of two phases. First,\nwe designed novel perturbations to discover spurious features that the model\nmay use to make predictions. Second, we applied the causal learning algorithms,\nspecifically, do-calculus, on top of existing deep learning models to\nsystematically remove the use of spurious features and thus promote causal\nbased prediction. Our results show that CausalVul consistently improved the\nmodel accuracy, robustness and OOD performance for all the state-of-the-art\nmodels and datasets we experimented. To the best of our knowledge, this is the\nfirst work that introduces do calculus based causal learning to software\nengineering models and shows it's indeed useful for improving the model\naccuracy, robustness and generalization. Our replication package is located at\nhttps://figshare.com/s/0ffda320dcb96c249ef2."}, "http://arxiv.org/abs/2310.07973": {"title": "Statistical Performance Guarantee for Selecting Those Predicted to Benefit Most from Treatment", "link": "http://arxiv.org/abs/2310.07973", "description": "Across a wide array of disciplines, many researchers use machine learning\n(ML) algorithms to identify a subgroup of individuals, called exceptional\nresponders, who are likely to be helped by a treatment the most. A common\napproach consists of two steps. One first estimates the conditional average\ntreatment effect or its proxy using an ML algorithm. They then determine the\ncutoff of the resulting treatment prioritization score to select those\npredicted to benefit most from the treatment. Unfortunately, these estimated\ntreatment prioritization scores are often biased and noisy. Furthermore,\nutilizing the same data to both choose a cutoff value and estimate the average\ntreatment effect among the selected individuals suffer from a multiple testing\nproblem. To address these challenges, we develop a uniform confidence band for\nexperimentally evaluating the sorted average treatment effect (GATES) among the\nindividuals whose treatment prioritization score is at least as high as any\ngiven quantile value, regardless of how the quantile is chosen. This provides a\nstatistical guarantee that the GATES for the selected subgroup exceeds a\ncertain threshold. The validity of the proposed methodology depends solely on\nrandomization of treatment and random sampling of units without requiring\nmodeling assumptions or resampling methods. This widens its applicability\nincluding a wide range of other causal quantities. A simulation study shows\nthat the empirical coverage of the proposed uniform confidence bands is close\nto the nominal coverage when the sample is as small as 100. We analyze a\nclinical trial of late-stage prostate cancer and find a relatively large\nproportion of exceptional responders with a statistical performance guarantee."}, "http://arxiv.org/abs/2310.08020": {"title": "Assessing Copula Models for Mixed Continuous-Ordinal Variables", "link": "http://arxiv.org/abs/2310.08020", "description": "Vine pair-copula constructions exist for a mix of continuous and ordinal\nvariables. In some steps, this can involve estimating a bivariate copula for a\npair of mixed continuous-ordinal variables. To assess the adequacy of copula\nfits for such a pair, diagnostic and visualization methods based on normal\nscore plots and conditional Q-Q plots are proposed. The former utilizes a\nlatent continuous variable for the ordinal variable. Using the Kullback-Leibler\ndivergence, existing probability models for mixed continuous-ordinal variable\npair are assessed for the adequacy of fit with simple parametric copula\nfamilies. The effectiveness of the proposed visualization and diagnostic\nmethods is illustrated on simulated and real datasets."}, "http://arxiv.org/abs/2310.08115": {"title": "Model-Agnostic Covariate-Assisted Inference on Partially Identified Causal Effects", "link": "http://arxiv.org/abs/2310.08115", "description": "Many causal estimands are only partially identifiable since they depend on\nthe unobservable joint distribution between potential outcomes. Stratification\non pretreatment covariates can yield sharper partial identification bounds;\nhowever, unless the covariates are discrete with relatively small support, this\napproach typically requires consistent estimation of the conditional\ndistributions of the potential outcomes given the covariates. Thus, existing\napproaches may fail under model misspecification or if consistency assumptions\nare violated. In this study, we propose a unified and model-agnostic\ninferential approach for a wide class of partially identified estimands, based\non duality theory for optimal transport problems. In randomized experiments,\nour approach can wrap around any estimates of the conditional distributions and\nprovide uniformly valid inference, even if the initial estimates are\narbitrarily inaccurate. Also, our approach is doubly robust in observational\nstudies. Notably, this property allows analysts to use the multiplier bootstrap\nto select covariates and models without sacrificing validity even if the true\nmodel is not included. Furthermore, if the conditional distributions are\nestimated at semiparametric rates, our approach matches the performance of an\noracle with perfect knowledge of the outcome model. Finally, we propose an\nefficient computational framework, enabling implementation on many practical\nproblems in causal inference."}, "http://arxiv.org/abs/2310.08193": {"title": "Are sanctions for losers? A network study of trade sanctions", "link": "http://arxiv.org/abs/2310.08193", "description": "Studies built on dependency and world-system theory using network approaches\nhave shown that international trade is structured into clusters of 'core' and\n'peripheral' countries performing distinct functions. However, few have used\nthese methods to investigate how sanctions affect the position of the countries\ninvolved in the capitalist world-economy. Yet, this topic has acquired pressing\nrelevance due to the emergence of economic warfare as a key geopolitical weapon\nsince the 1950s. And even more so in light of the preeminent role that\nsanctions have played in the US and their allies' response to the\nRussian-Ukrainian war. Applying several clustering techniques designed for\ncomplex and temporal networks, this paper shows that a shift in the pattern of\ncommerce away from sanctioning countries and towards neutral or friendly ones.\nAdditionally, there are suggestions that these shifts may lead to the creation\nof an alternative 'core' that interacts with the world-economy's periphery\nbypassing traditional 'core' countries such as EU member States and the US."}, "http://arxiv.org/abs/2310.08268": {"title": "Change point detection in dynamic heterogeneous networks via subspace tracking", "link": "http://arxiv.org/abs/2310.08268", "description": "Dynamic networks consist of a sequence of time-varying networks, and it is of\ngreat importance to detect the network change points. Most existing methods\nfocus on detecting abrupt change points, necessitating the assumption that the\nunderlying network probability matrix remains constant between adjacent change\npoints. This paper introduces a new model that allows the network probability\nmatrix to undergo continuous shifting, while the latent network structure,\nrepresented via the embedding subspace, only changes at certain time points.\nTwo novel statistics are proposed to jointly detect these network subspace\nchange points, followed by a carefully refined detection procedure.\nTheoretically, we show that the proposed method is asymptotically consistent in\nterms of change point detection, and also establish the impossibility region\nfor detecting these network subspace change points. The advantage of the\nproposed method is also supported by extensive numerical experiments on both\nsynthetic networks and a UK politician social network."}, "http://arxiv.org/abs/2310.08397": {"title": "Assessing Marine Mammal Abundance: A Novel Data Fusion", "link": "http://arxiv.org/abs/2310.08397", "description": "Marine mammals are increasingly vulnerable to human disturbance and climate\nchange. Their diving behavior leads to limited visual access during data\ncollection, making studying the abundance and distribution of marine mammals\nchallenging. In theory, using data from more than one observation modality\nshould lead to better informed predictions of abundance and distribution. With\nfocus on North Atlantic right whales, we consider the fusion of two data\nsources to inform about their abundance and distribution. The first source is\naerial distance sampling which provides the spatial locations of whales\ndetected in the region. The second source is passive acoustic monitoring (PAM),\nreturning calls received at hydrophones placed on the ocean floor. Due to\nlimited time on the surface and detection limitations arising from sampling\neffort, aerial distance sampling only provides a partial realization of\nlocations. With PAM, we never observe numbers or locations of individuals. To\naddress these challenges, we develop a novel thinned point pattern data fusion.\nOur approach leads to improved inference regarding abundance and distribution\nof North Atlantic right whales throughout Cape Cod Bay, Massachusetts in the\nUS. We demonstrate performance gains of our approach compared to that from a\nsingle source through both simulation and real data."}, "http://arxiv.org/abs/2310.08410": {"title": "Evaluation of ChatGPT-Generated Medical Responses: A Systematic Review and Meta-Analysis", "link": "http://arxiv.org/abs/2310.08410", "description": "Large language models such as ChatGPT are increasingly explored in medical\ndomains. However, the absence of standard guidelines for performance evaluation\nhas led to methodological inconsistencies. This study aims to summarize the\navailable evidence on evaluating ChatGPT's performance in medicine and provide\ndirection for future research. We searched ten medical literature databases on\nJune 15, 2023, using the keyword \"ChatGPT\". A total of 3520 articles were\nidentified, of which 60 were reviewed and summarized in this paper and 17 were\nincluded in the meta-analysis. The analysis showed that ChatGPT displayed an\noverall integrated accuracy of 56% (95% CI: 51%-60%, I2 = 87%) in addressing\nmedical queries. However, the studies varied in question resource,\nquestion-asking process, and evaluation metrics. Moreover, many studies failed\nto report methodological details, including the version of ChatGPT and whether\neach question was used independently or repeatedly. Our findings revealed that\nalthough ChatGPT demonstrated considerable potential for application in\nhealthcare, the heterogeneity of the studies and insufficient reporting may\naffect the reliability of these results. Further well-designed studies with\ncomprehensive and transparent reporting are needed to evaluate ChatGPT's\nperformance in medicine."}, "http://arxiv.org/abs/2310.08414": {"title": "Confidence bounds for the true discovery proportion based on the exact distribution of the number of rejections", "link": "http://arxiv.org/abs/2310.08414", "description": "In multiple hypotheses testing it has become widely popular to make inference\non the true discovery proportion (TDP) of a set $\\mathcal{M}$ of null\nhypotheses. This approach is useful for several application fields, such as\nneuroimaging and genomics. Several procedures to compute simultaneous lower\nconfidence bounds for the TDP have been suggested in prior literature.\nSimultaneity allows for post-hoc selection of $\\mathcal{M}$. If sets of\ninterest are specified a priori, it is possible to gain power by removing the\nsimultaneity requirement. We present an approach to compute lower confidence\nbounds for the TDP if the set of null hypotheses is defined a priori. The\nproposed method determines the bounds using the exact distribution of the\nnumber of rejections based on a step-up multiple testing procedure under\nindependence assumptions. We assess robustness properties of our procedure and\napply it to real data from the field of functional magnetic resonance imaging."}, "http://arxiv.org/abs/2310.08426": {"title": "Extensions of Heterogeneity in Integration and Prediction (HIP) with R Shiny Application", "link": "http://arxiv.org/abs/2310.08426", "description": "Multiple data views measured on the same set of participants is becoming more\ncommon and has the potential to deepen our understanding of many complex\ndiseases by analyzing these different views simultaneously. Equally important,\nmany of these complex diseases show evidence of subgroup heterogeneity (e.g.,\nby sex or race). HIP (Heterogeneity in Integration and Prediction) is among the\nfirst methods proposed to integrate multiple data views while also accounting\nfor subgroup heterogeneity to identify common and subgroup-specific markers of\na particular disease. However, HIP is applicable to continuous outcomes and\nrequires programming expertise by the user. Here we propose extensions to HIP\nthat accommodate multi-class, Poisson, and Zero-Inflated Poisson outcomes while\nretaining the benefits of HIP. Additionally, we introduce an R Shiny\napplication, accessible on shinyapps.io at\nhttps://multi-viewlearn.shinyapps.io/HIP_ShinyApp/, that provides an interface\nwith the Python implementation of HIP to allow more researchers to use the\nmethod anywhere and on any device. We applied HIP to identify genes and\nproteins common and specific to males and females that are associated with\nexacerbation frequency. Although some of the identified genes and proteins show\nevidence of a relationship with chronic obstructive pulmonary disease (COPD) in\nexisting literature, others may be candidates for future research investigating\ntheir relationship with COPD. We demonstrate the use of the Shiny application\nwith a publicly available data. An R-package for HIP would be made available at\nhttps://github.com/lasandrall/HIP."}, "http://arxiv.org/abs/2310.08479": {"title": "Personalised dynamic super learning: an application in predicting hemodiafiltration's convection volumes", "link": "http://arxiv.org/abs/2310.08479", "description": "Obtaining continuously updated predictions is a major challenge for\npersonalised medicine. Leveraging combinations of parametric regressions and\nmachine learning approaches, the personalised online super learner (POSL) can\nachieve such dynamic and personalised predictions. We adapt POSL to predict a\nrepeated continuous outcome dynamically and propose a new way to validate such\npersonalised or dynamic prediction models. We illustrate its performance by\npredicting the convection volume of patients undergoing hemodiafiltration. POSL\noutperformed its candidate learners with respect to median absolute error,\ncalibration-in-the-large, discrimination, and net benefit. We finally discuss\nthe choices and challenges underlying the use of POSL."}, "http://arxiv.org/abs/1903.00037": {"title": "Distance-Based Independence Screening for Canonical Analysis", "link": "http://arxiv.org/abs/1903.00037", "description": "This paper introduces a novel method called Distance-Based Independence\nScreening for Canonical Analysis (DISCA) that performs simultaneous dimension\nreduction for a pair of random variables by optimizing the distance covariance\n(dCov). dCov is a statistic first proposed by Sz\\'ekely et al. [2009] for\nindependence testing. Compared with sufficient dimension reduction (SDR) and\ncanonical correlation analysis (CCA)-based approaches, DISCA is a model-free\napproach that does not impose dimensional or distributional restrictions on\nvariables and is more sensitive to nonlinear relationships. Theoretically, we\nestablish a non-asymptotic error bound to provide a guarantee of our method's\nperformance. Numerically, DISCA performs comparable to or better than other\nstate-of-the-art algorithms and is computationally faster. All codes of our\nDISCA method can be found on GitHub https : //github.com/Yijin911/DISCA.git,\nincluding an R package named DISCA."}, "http://arxiv.org/abs/2105.13952": {"title": "Generalized Permutation Framework for Testing Model Variable Significance", "link": "http://arxiv.org/abs/2105.13952", "description": "A common problem in machine learning is determining if a variable\nsignificantly contributes to a model's prediction performance. This problem is\naggravated for datasets, such as gene expression datasets, that suffer the\nworst case of dimensionality: a low number of observations along with a high\nnumber of possible explanatory variables. In such scenarios, traditional\nmethods for testing variable statistical significance or constructing variable\nconfidence intervals do not apply. To address these problems, we developed a\nnovel permutation framework for testing the significance of variables in\nsupervised models. Our permutation framework has three main advantages. First,\nit is non-parametric and does not rely on distributional assumptions or\nasymptotic results. Second, it not only ranks model variables in terms of\nrelative importance, but also tests for statistical significance of each\nvariable. Third, it can test for the significance of the interaction between\nmodel variables. We applied this permutation framework to multi-class\nclassification of the Iris flower dataset and of brain regions in RNA\nexpression data, and using this framework showed variable-level statistical\nsignificance and interactions."}, "http://arxiv.org/abs/2210.02002": {"title": "Factor Augmented Sparse Throughput Deep ReLU Neural Networks for High Dimensional Regression", "link": "http://arxiv.org/abs/2210.02002", "description": "This paper introduces a Factor Augmented Sparse Throughput (FAST) model that\nutilizes both latent factors and sparse idiosyncratic components for\nnonparametric regression. The FAST model bridges factor models on one end and\nsparse nonparametric models on the other end. It encompasses structured\nnonparametric models such as factor augmented additive models and sparse\nlow-dimensional nonparametric interaction models and covers the cases where the\ncovariates do not admit factor structures. Via diversified projections as\nestimation of latent factor space, we employ truncated deep ReLU networks to\nnonparametric factor regression without regularization and to a more general\nFAST model using nonconvex regularization, resulting in factor augmented\nregression using neural network (FAR-NN) and FAST-NN estimators respectively.\nWe show that FAR-NN and FAST-NN estimators adapt to the unknown low-dimensional\nstructure using hierarchical composition models in nonasymptotic minimax rates.\nWe also study statistical learning for the factor augmented sparse additive\nmodel using a more specific neural network architecture. Our results are\napplicable to the weak dependent cases without factor structures. In proving\nthe main technical result for FAST-NN, we establish a new deep ReLU network\napproximation result that contributes to the foundation of neural network\ntheory. Our theory and methods are further supported by simulation studies and\nan application to macroeconomic data."}, "http://arxiv.org/abs/2210.04482": {"title": "Leave-group-out cross-validation for latent Gaussian models", "link": "http://arxiv.org/abs/2210.04482", "description": "Evaluating the predictive performance of a statistical model is commonly done\nusing cross-validation. Although the leave-one-out method is frequently\nemployed, its application is justified primarily for independent and\nidentically distributed observations. However, this method tends to mimic\ninterpolation rather than prediction when dealing with dependent observations.\nThis paper proposes a modified cross-validation for dependent observations.\nThis is achieved by excluding an automatically determined set of observations\nfrom the training set to mimic a more reasonable prediction scenario. Also,\nwithin the framework of latent Gaussian models, we illustrate a method to\nadjust the joint posterior for this modified cross-validation to avoid model\nrefitting. This new approach is accessible in the R-INLA package\n(www.r-inla.org)."}, "http://arxiv.org/abs/2210.11355": {"title": "Network Synthetic Interventions: A Causal Framework for Panel Data Under Network Interference", "link": "http://arxiv.org/abs/2210.11355", "description": "We propose a generalization of the synthetic controls and synthetic\ninterventions methodology to incorporate network interference. We consider the\nestimation of unit-specific potential outcomes from panel data in the presence\nof spillover across units and unobserved confounding. Key to our approach is a\nnovel latent factor model that takes into account network interference and\ngeneralizes the factor models typically used in panel data settings. We propose\nan estimator, Network Synthetic Interventions (NSI), and show that it\nconsistently estimates the mean outcomes for a unit under an arbitrary set of\ncounterfactual treatments for the network. We further establish that the\nestimator is asymptotically normal. We furnish two validity tests for whether\nthe NSI estimator reliably generalizes to produce accurate counterfactual\nestimates. We provide a novel graph-based experiment design that guarantees the\nNSI estimator produces accurate counterfactual estimates, and also analyze the\nsample complexity of the proposed design. We conclude with simulations that\ncorroborate our theoretical findings."}, "http://arxiv.org/abs/2212.01179": {"title": "Feasibility of using survey data and semi-variogram kriging to obtain bespoke indices of neighbourhood characteristics: a simulation and a case study", "link": "http://arxiv.org/abs/2212.01179", "description": "Data on neighbourhood characteristics are not typically collected in\nepidemiological studies. They are however useful in the study of small-area\nhealth inequalities. Neighbourhood characteristics are collected in some\nsurveys and could be linked to the data of other studies. We propose to use\nkriging based on semi-variogram models to predict values at non-observed\nlocations with the aim of constructing bespoke indices of neighbourhood\ncharacteristics to be linked to data from epidemiological studies. We perform a\nsimulation study to assess the feasibility of the method as well as a case\nstudy using data from the RECORD study. Apart from having enough observed data\nat small distances to the non-observed locations, a good fitting\nsemi-variogram, a larger range and the absence of nugget effects for the\nsemi-variogram models are factors leading to a higher reliability."}, "http://arxiv.org/abs/2303.17823": {"title": "An interpretable neural network-based non-proportional odds model for ordinal regression", "link": "http://arxiv.org/abs/2303.17823", "description": "This study proposes an interpretable neural network-based non-proportional\nodds model (N$^3$POM) for ordinal regression. N$^3$POM is different from\nconventional approaches to ordinal regression with non-proportional models in\nseveral ways: (1) N$^3$POM is designed to directly handle continuous responses,\nwhereas standard methods typically treat de facto ordered continuous variables\nas discrete, (2) instead of estimating response-dependent finite coefficients\nof linear models from discrete responses as is done in conventional approaches,\nwe train a non-linear neural network to serve as a coefficient function. Thanks\nto the neural network, N$^3$POM offers flexibility while preserving the\ninterpretability of conventional ordinal regression. We establish a sufficient\ncondition under which the predicted conditional cumulative probability locally\nsatisfies the monotonicity constraint over a user-specified region in the\ncovariate space. Additionally, we provide a monotonicity-preserving stochastic\n(MPS) algorithm for effectively training the neural network. We apply N$^3$POM\nto several real-world datasets."}, "http://arxiv.org/abs/2306.16335": {"title": "Emulating the dynamics of complex systems using autoregressive models on manifolds (mNARX)", "link": "http://arxiv.org/abs/2306.16335", "description": "We propose a novel surrogate modelling approach to efficiently and accurately\napproximate the response of complex dynamical systems driven by time-varying\nexogenous excitations over extended time periods. Our approach, namely manifold\nnonlinear autoregressive modelling with exogenous input (mNARX), involves\nconstructing a problem-specific exogenous input manifold that is optimal for\nconstructing autoregressive surrogates. The manifold, which forms the core of\nmNARX, is constructed incrementally by incorporating the physics of the system,\nas well as prior expert- and domain- knowledge. Because mNARX decomposes the\nfull problem into a series of smaller sub-problems, each with a lower\ncomplexity than the original, it scales well with the complexity of the\nproblem, both in terms of training and evaluation costs of the final surrogate.\nFurthermore, mNARX synergizes well with traditional dimensionality reduction\ntechniques, making it highly suitable for modelling dynamical systems with\nhigh-dimensional exogenous inputs, a class of problems that is typically\nchallenging to solve. Since domain knowledge is particularly abundant in\nphysical systems, such as those found in civil and mechanical engineering,\nmNARX is well suited for these applications. We demonstrate that mNARX\noutperforms traditional autoregressive surrogates in predicting the response of\na classical coupled spring-mass system excited by a one-dimensional random\nexcitation. Additionally, we show that mNARX is well suited for emulating very\nhigh-dimensional time- and state-dependent systems, even when affected by\nactive controllers, by surrogating the dynamics of a realistic\naero-servo-elastic onshore wind turbine simulator. In general, our results\ndemonstrate that mNARX offers promising prospects for modelling complex\ndynamical systems, in terms of accuracy and efficiency."}, "http://arxiv.org/abs/2307.02236": {"title": "D-optimal Subsampling Design for Massive Data Linear Regression", "link": "http://arxiv.org/abs/2307.02236", "description": "Data reduction is a fundamental challenge of modern technology, where\nclassical statistical methods are not applicable because of computational\nlimitations. We consider linear regression for an extraordinarily large number\nof observations, but only a few covariates. Subsampling aims at the selection\nof a given percentage of the existing original data. Under distributional\nassumptions on the covariates, we derive D-optimal subsampling designs and\nstudy their theoretical properties. We make use of fundamental concepts of\noptimal design theory and an equivalence theorem from constrained convex\noptimization. The thus obtained subsampling designs provide simple rules for\nwhether to accept or reject a data point, allowing for an easy algorithmic\nimplementation. In addition, we propose a simplified subsampling method that\ndiffers from the D-optimal design but requires lower computing time. We present\na simulation study, comparing both subsampling schemes with the IBOSS method."}, "http://arxiv.org/abs/2310.08672": {"title": "Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal", "link": "http://arxiv.org/abs/2310.08672", "description": "In many settings, interventions may be more effective for some individuals\nthan others, so that targeting interventions may be beneficial. We analyze the\nvalue of targeting in the context of a large-scale field experiment with over\n53,000 college students, where the goal was to use \"nudges\" to encourage\nstudents to renew their financial-aid applications before a non-binding\ndeadline. We begin with baseline approaches to targeting. First, we target\nbased on a causal forest that estimates heterogeneous treatment effects and\nthen assigns students to treatment according to those estimated to have the\nhighest treatment effects. Next, we evaluate two alternative targeting\npolicies, one targeting students with low predicted probability of renewing\nfinancial aid in the absence of the treatment, the other targeting those with\nhigh probability. The predicted baseline outcome is not the ideal criterion for\ntargeting, nor is it a priori clear whether to prioritize low, high, or\nintermediate predicted probability. Nonetheless, targeting on low baseline\noutcomes is common in practice, for example because the relationship between\nindividual characteristics and treatment effects is often difficult or\nimpossible to estimate with historical data. We propose hybrid approaches that\nincorporate the strengths of both predictive approaches (accurate estimation)\nand causal approaches (correct criterion); we show that targeting intermediate\nbaseline outcomes is most effective, while targeting based on low baseline\noutcomes is detrimental. In one year of the experiment, nudging all students\nimproved early filing by an average of 6.4 percentage points over a baseline\naverage of 37% filing, and we estimate that targeting half of the students\nusing our preferred policy attains around 75% of this benefit."}, "http://arxiv.org/abs/2310.08726": {"title": "Design-Based RCT Estimators and Central Limit Theorems for Baseline Subgroup and Related Analyses", "link": "http://arxiv.org/abs/2310.08726", "description": "There is a growing literature on design-based methods to estimate average\ntreatment effects (ATEs) for randomized controlled trials (RCTs) for full\nsample analyses. This article extends these methods to estimate ATEs for\ndiscrete subgroups defined by pre-treatment variables, with an application to\nan RCT testing subgroup effects for a school voucher experiment in New York\nCity. We consider ratio estimators for subgroup effects using regression\nmethods, allowing for model covariates to improve precision, and prove a finite\npopulation central limit theorem. We discuss extensions to blocked and\nclustered RCT designs, and to other common estimators with random\ntreatment-control sample sizes (or weights): post-stratification estimators,\nweighted estimators that adjust for data nonresponse, and estimators for\nBernoulli trials. We also develop simple variance estimators that share\nfeatures with robust estimators. Simulations show that the design-based\nsubgroup estimators yield confidence interval coverage near nominal levels,\neven for small subgroups."}, "http://arxiv.org/abs/2310.08798": {"title": "Alteration Detection of Tensor Dependence Structure via Sparsity-Exploited Reranking Algorithm", "link": "http://arxiv.org/abs/2310.08798", "description": "Tensor-valued data arise frequently from a wide variety of scientific\napplications, and many among them can be translated into an alteration\ndetection problem of tensor dependence structures. In this article, we\nformulate the problem under the popularly adopted tensor-normal distributions\nand aim at two-sample correlation/partial correlation comparisons of\ntensor-valued observations. Through decorrelation and centralization, a\nseparable covariance structure is employed to pool sample information from\ndifferent tensor modes to enhance the power of the test. Additionally, we\npropose a novel Sparsity-Exploited Reranking Algorithm (SERA) to further\nimprove the multiple testing efficiency. The algorithm is approached through\nreranking of the p-values derived from the primary test statistics, by\nincorporating a carefully constructed auxiliary tensor sequence. Besides the\ntensor framework, SERA is also generally applicable to a wide range of\ntwo-sample large-scale inference problems with sparsity structures, and is of\nindependent interest. The asymptotic properties of the proposed test are\nderived and the algorithm is shown to control the false discovery at the\npre-specified level. We demonstrate the efficacy of the proposed method through\nintensive simulations and two scientific applications."}, "http://arxiv.org/abs/2310.08812": {"title": "A Nonlinear Method for time series forecasting using VMD-GARCH-LSTM model", "link": "http://arxiv.org/abs/2310.08812", "description": "Time series forecasting represents a significant and challenging task across\nvarious fields. Recently, methods based on mode decomposition have dominated\nthe forecasting of complex time series because of the advantages of capturing\nlocal characteristics and extracting intrinsic modes from data. Unfortunately,\nmost models fail to capture the implied volatilities that contain significant\ninformation. To enhance the forecasting of current, rapidly evolving, and\nvolatile time series, we propose a novel decomposition-ensemble paradigm, the\nVMD-LSTM-GARCH model. The Variational Mode Decomposition algorithm is employed\nto decompose the time series into K sub-modes. Subsequently, the GARCH model\nextracts the volatility information from these sub-modes, which serve as the\ninput for the LSTM. The numerical and volatility information of each sub-mode\nis utilized to train a Long Short-Term Memory network. This network predicts\nthe sub-mode, and then we aggregate the predictions from all sub-modes to\nproduce the output. By integrating econometric and artificial intelligence\nmethods, and taking into account both the numerical and volatility information\nof the time series, our proposed model demonstrates superior performance in\ntime series forecasting, as evidenced by the significant decrease in MSE, RMSE,\nand MAPE in our comparative experimental results."}, "http://arxiv.org/abs/2310.08867": {"title": "A Survey of Methods for Handling Disk Data Imbalance", "link": "http://arxiv.org/abs/2310.08867", "description": "Class imbalance exists in many classification problems, and since the data is\ndesigned for accuracy, imbalance in data classes can lead to classification\nchallenges with a few classes having higher misclassification costs. The\nBackblaze dataset, a widely used dataset related to hard discs, has a small\namount of failure data and a large amount of health data, which exhibits a\nserious class imbalance. This paper provides a comprehensive overview of\nresearch in the field of imbalanced data classification. The discussion is\norganized into three main aspects: data-level methods, algorithmic-level\nmethods, and hybrid methods. For each type of method, we summarize and analyze\nthe existing problems, algorithmic ideas, strengths, and weaknesses.\nAdditionally, the challenges of unbalanced data classification are discussed,\nalong with strategies to address them. It is convenient for researchers to\nchoose the appropriate method according to their needs."}, "http://arxiv.org/abs/2310.08939": {"title": "Fast Screening Rules for Optimal Design via Quadratic Lasso Reformulation", "link": "http://arxiv.org/abs/2310.08939", "description": "The problems of Lasso regression and optimal design of experiments share a\ncritical property: their optimal solutions are typically \\emph{sparse}, i.e.,\nonly a small fraction of the optimal variables are non-zero. Therefore, the\nidentification of the support of an optimal solution reduces the dimensionality\nof the problem and can yield a substantial simplification of the calculations.\nIt has recently been shown that linear regression with a \\emph{squared}\n$\\ell_1$-norm sparsity-inducing penalty is equivalent to an optimal\nexperimental design problem. In this work, we use this equivalence to derive\nsafe screening rules that can be used to discard inessential samples. Compared\nto previously existing rules, the new tests are much faster to compute,\nespecially for problems involving a parameter space of high dimension, and can\nbe used dynamically within any iterative solver, with negligible computational\noverhead. Moreover, we show how an existing homotopy algorithm to compute the\nregularization path of the lasso method can be reparametrized with respect to\nthe squared $\\ell_1$-penalty. This allows the computation of a Bayes\n$c$-optimal design in a finite number of steps and can be several orders of\nmagnitude faster than standard first-order algorithms. The efficiency of the\nnew screening rules and of the homotopy algorithm are demonstrated on different\nexamples based on real data."}, "http://arxiv.org/abs/2310.09013": {"title": "Smoothed instrumental variables quantile regression", "link": "http://arxiv.org/abs/2310.09013", "description": "In this article, I introduce the sivqr command, which estimates the\ncoefficients of the instrumental variables (IV) quantile regression model\nintroduced by Chernozhukov and Hansen (2005). The sivqr command offers several\nadvantages over the existing ivqreg and ivqreg2 commands for estimating this IV\nquantile regression model, which complements the alternative \"triangular model\"\nbehind cqiv and the \"local quantile treatment effect\" model of ivqte.\nComputationally, sivqr implements the smoothed estimator of Kaplan and Sun\n(2017), who show that smoothing improves both computation time and statistical\naccuracy. Standard errors are computed analytically or by Bayesian bootstrap;\nfor non-iid sampling, sivqr is compatible with bootstrap. I discuss syntax and\nthe underlying methodology, and I compare sivqr with other commands in an\nexample."}, "http://arxiv.org/abs/2310.09100": {"title": "Time-Uniform Self-Normalized Concentration for Vector-Valued Processes", "link": "http://arxiv.org/abs/2310.09100", "description": "Self-normalized processes arise naturally in many statistical tasks. While\nself-normalized concentration has been extensively studied for scalar-valued\nprocesses, there is less work on multidimensional processes outside of the\nsub-Gaussian setting. In this work, we construct a general, self-normalized\ninequality for $\\mathbb{R}^d$-valued processes that satisfy a simple yet broad\n\"sub-$\\psi$\" tail condition, which generalizes assumptions based on cumulant\ngenerating functions. From this general inequality, we derive an upper law of\nthe iterated logarithm for sub-$\\psi$ vector-valued processes, which is tight\nup to small constants. We demonstrate applications in prototypical statistical\ntasks, such as parameter estimation in online linear regression and\nauto-regressive modeling, and bounded mean estimation via a new (multivariate)\nempirical Bernstein concentration inequality."}, "http://arxiv.org/abs/2310.09185": {"title": "Mediation Analysis using Semi-parametric Shape-Restricted Regression with Applications", "link": "http://arxiv.org/abs/2310.09185", "description": "Often linear regression is used to perform mediation analysis. However, in\nmany instances, the underlying relationships may not be linear, as in the case\nof placental-fetal hormones and fetal development. Although, the exact\nfunctional form of the relationship may be unknown, one may hypothesize the\ngeneral shape of the relationship. For these reasons, we develop a novel\nshape-restricted inference-based methodology for conducting mediation analysis.\nThis work is motivated by an application in fetal endocrinology where\nresearchers are interested in understanding the effects of pesticide\napplication on birth weight, with human chorionic gonadotropin (hCG) as the\nmediator. We assume a practically plausible set of nonlinear effects of hCG on\nthe birth weight and a linear relationship between pesticide exposure and hCG,\nwith both exposure-outcome and exposure-mediator models being linear in the\nconfounding factors. Using the proposed methodology on a population-level\nprenatal screening program data, with hCG as the mediator, we discovered that,\nwhile the natural direct effects suggest a positive association between\npesticide application and birth weight, the natural indirect effects were\nnegative."}, "http://arxiv.org/abs/2310.09214": {"title": "An Introduction to the Calibration of Computer Models", "link": "http://arxiv.org/abs/2310.09214", "description": "In the context of computer models, calibration is the process of estimating\nunknown simulator parameters from observational data. Calibration is variously\nreferred to as model fitting, parameter estimation/inference, an inverse\nproblem, and model tuning. The need for calibration occurs in most areas of\nscience and engineering, and has been used to estimate hard to measure\nparameters in climate, cardiology, drug therapy response, hydrology, and many\nother disciplines. Although the statistical method used for calibration can\nvary substantially, the underlying approach is essentially the same and can be\nconsidered abstractly. In this survey, we review the decisions that need to be\ntaken when calibrating a model, and discuss a range of computational methods\nthat can be used to compute Bayesian posterior distributions."}, "http://arxiv.org/abs/2310.09239": {"title": "Estimating weighted quantile treatment effects with missing outcome data by double sampling", "link": "http://arxiv.org/abs/2310.09239", "description": "Causal weighted quantile treatment effects (WQTE) are a useful compliment to\nstandard causal contrasts that focus on the mean when interest lies at the\ntails of the counterfactual distribution. To-date, however, methods for\nestimation and inference regarding causal WQTEs have assumed complete data on\nall relevant factors. Missing or incomplete data, however, is a widespread\nchallenge in practical settings, particularly when the data are not collected\nfor research purposes such as electronic health records and disease registries.\nFurthermore, in such settings may be particularly susceptible to the outcome\ndata being missing-not-at-random (MNAR). In this paper, we consider the use of\ndouble-sampling, through which the otherwise missing data is ascertained on a\nsub-sample of study units, as a strategy to mitigate bias due to MNAR data in\nthe estimation of causal WQTEs. With the additional data in-hand, we present\nidentifying conditions that do not require assumptions regarding missingness in\nthe original data. We then propose a novel inverse-probability weighted\nestimator and derive its' asymptotic properties, both pointwise at specific\nquantiles and uniform across a range of quantiles in (0,1), when the propensity\nscore and double-sampling probabilities are estimated. For practical inference,\nwe develop a bootstrap method that can be used for both pointwise and uniform\ninference. A simulation study is conducted to examine the finite sample\nperformance of the proposed estimators."}, "http://arxiv.org/abs/2310.09257": {"title": "A SIMPLE Approach to Provably Reconstruct Ising Model with Global Optimality", "link": "http://arxiv.org/abs/2310.09257", "description": "Reconstruction of interaction network between random events is a critical\nproblem arising from statistical physics and politics to sociology, biology,\nand psychology, and beyond. The Ising model lays the foundation for this\nreconstruction process, but finding the underlying Ising model from the least\namount of observed samples in a computationally efficient manner has been\nhistorically challenging for half a century. By using the idea of sparsity\nlearning, we present a approach named SIMPLE that has a dominant sample\ncomplexity from theoretical limit. Furthermore, a tuning-free algorithm is\ndeveloped to give a statistically consistent solution of SIMPLE in polynomial\ntime with high probability. On extensive benchmarked cases, the SIMPLE approach\nprovably reconstructs underlying Ising models with global optimality. The\napplication on the U.S. senators voting in the last six congresses reveals that\nboth the Republicans and Democrats noticeably assemble in each congresses;\ninterestingly, the assembling of Democrats is particularly pronounced in the\nlatest congress."}, "http://arxiv.org/abs/2208.09817": {"title": "High-Dimensional Composite Quantile Regression: Optimal Statistical Guarantees and Fast Algorithms", "link": "http://arxiv.org/abs/2208.09817", "description": "The composite quantile regression (CQR) was introduced by Zou and Yuan [Ann.\nStatist. 36 (2008) 1108--1126] as a robust regression method for linear models\nwith heavy-tailed errors while achieving high efficiency. Its penalized\ncounterpart for high-dimensional sparse models was recently studied in Gu and\nZou [IEEE Trans. Inf. Theory 66 (2020) 7132--7154], along with a specialized\noptimization algorithm based on the alternating direct method of multipliers\n(ADMM). Compared to the various first-order algorithms for penalized least\nsquares, ADMM-based algorithms are not well-adapted to large-scale problems. To\novercome this computational hardness, in this paper we employ a\nconvolution-smoothed technique to CQR, complemented with iteratively reweighted\n$\\ell_1$-regularization. The smoothed composite loss function is convex, twice\ncontinuously differentiable, and locally strong convex with high probability.\nWe propose a gradient-based algorithm for penalized smoothed CQR via a variant\nof the majorize-minimization principal, which gains substantial computational\nefficiency over ADMM. Theoretically, we show that the iteratively reweighted\n$\\ell_1$-penalized smoothed CQR estimator achieves near-minimax optimal\nconvergence rate under heavy-tailed errors without any moment constraint, and\nfurther achieves near-oracle convergence rate under a weaker minimum signal\nstrength condition than needed in Gu and Zou (2020). Numerical studies\ndemonstrate that the proposed method exhibits significant computational\nadvantages without compromising statistical performance compared to two\nstate-of-the-art methods that achieve robustness and high efficiency\nsimultaneously."}, "http://arxiv.org/abs/2210.14292": {"title": "Statistical Inference for H\\\"usler-Reiss Graphical Models Through Matrix Completions", "link": "http://arxiv.org/abs/2210.14292", "description": "The severity of multivariate extreme events is driven by the dependence\nbetween the largest marginal observations. The H\\\"usler-Reiss distribution is a\nversatile model for this extremal dependence, and it is usually parameterized\nby a variogram matrix. In order to represent conditional independence relations\nand obtain sparse parameterizations, we introduce the novel H\\\"usler-Reiss\nprecision matrix. Similarly to the Gaussian case, this matrix appears naturally\nin density representations of the H\\\"usler-Reiss Pareto distribution and\nencodes the extremal graphical structure through its zero pattern. For a given,\narbitrary graph we prove the existence and uniqueness of the completion of a\npartially specified H\\\"usler-Reiss variogram matrix so that its precision\nmatrix has zeros on non-edges in the graph. Using suitable estimators for the\nparameters on the edges, our theory provides the first consistent estimator of\ngraph structured H\\\"usler-Reiss distributions. If the graph is unknown, our\nmethod can be combined with recent structure learning algorithms to jointly\ninfer the graph and the corresponding parameter matrix. Based on our\nmethodology, we propose new tools for statistical inference of sparse\nH\\\"usler-Reiss models and illustrate them on large flight delay data in the\nU.S., as well as Danube river flow data."}, "http://arxiv.org/abs/2302.02288": {"title": "Efficient Adaptive Joint Significance Tests and Sobel-Type Confidence Intervals for Mediation Effects", "link": "http://arxiv.org/abs/2302.02288", "description": "Mediation analysis is an important statistical tool in many research fields.\nIts aim is to investigate the mechanism along the causal pathway between an\nexposure and an outcome. The joint significance test is widely utilized as a\nprominent statistical approach for examining mediation effects in practical\napplications. Nevertheless, the limitation of this mediation testing method\nstems from its conservative Type I error, which reduces its statistical power\nand imposes certain constraints on its popularity and utility. The proposed\nsolution to address this gap is the adaptive joint significance test for one\nmediator, a novel data-adaptive test for mediation effect that exhibits\nsignificant advancements compared to traditional joint significance test. The\nproposed method is designed to be user-friendly, eliminating the need for\ncomplicated procedures. We have derived explicit expressions for size and\npower, ensuring the theoretical validity of our approach. Furthermore, we\nextend the proposed adaptive joint significance tests for small-scale mediation\nhypotheses with family-wise error rate (FWER) control. Additionally, a novel\nadaptive Sobel-type approach is proposed for the estimation of confidence\nintervals for the mediation effects, demonstrating significant advancements\nover conventional Sobel's confidence intervals in terms of achieving desirable\ncoverage probabilities. Our mediation testing and confidence intervals\nprocedure is evaluated through comprehensive simulations, and compared with\nnumerous existing approaches. Finally, we illustrate the usefulness of our\nmethod by analysing three real-world datasets with continuous, binary and\ntime-to-event outcomes, respectively."}, "http://arxiv.org/abs/2302.02747": {"title": "Testing Quantile Forecast Optimality", "link": "http://arxiv.org/abs/2302.02747", "description": "Quantile forecasts made across multiple horizons have become an important\noutput of many financial institutions, central banks and international\norganisations. This paper proposes misspecification tests for such quantile\nforecasts that assess optimality over a set of multiple forecast horizons\nand/or quantiles. The tests build on multiple Mincer-Zarnowitz quantile\nregressions cast in a moment equality framework. Our main test is for the null\nhypothesis of autocalibration, a concept which assesses optimality with respect\nto the information contained in the forecasts themselves. We provide an\nextension that allows to test for optimality with respect to larger information\nsets and a multivariate extension. Importantly, our tests do not just inform\nabout general violations of optimality, but may also provide useful insights\ninto specific forms of sub-optimality. A simulation study investigates the\nfinite sample performance of our tests, and two empirical applications to\nfinancial returns and U.S. macroeconomic series illustrate that our tests can\nyield interesting insights into quantile forecast sub-optimality and its\ncauses."}, "http://arxiv.org/abs/2305.00700": {"title": "Double and Single Descent in Causal Inference with an Application to High-Dimensional Synthetic Control", "link": "http://arxiv.org/abs/2305.00700", "description": "Motivated by a recent literature on the double-descent phenomenon in machine\nlearning, we consider highly over-parameterized models in causal inference,\nincluding synthetic control with many control units. In such models, there may\nbe so many free parameters that the model fits the training data perfectly. We\nfirst investigate high-dimensional linear regression for imputing wage data and\nestimating average treatment effects, where we find that models with many more\ncovariates than sample size can outperform simple ones. We then document the\nperformance of high-dimensional synthetic control estimators with many control\nunits. We find that adding control units can help improve imputation\nperformance even beyond the point where the pre-treatment fit is perfect. We\nprovide a unified theoretical perspective on the performance of these\nhigh-dimensional models. Specifically, we show that more complex models can be\ninterpreted as model-averaging estimators over simpler ones, which we link to\nan improvement in average performance. This perspective yields concrete\ninsights into the use of synthetic control when control units are many relative\nto the number of pre-treatment periods."}, "http://arxiv.org/abs/2305.15742": {"title": "Counterfactual Generative Models for Time-Varying Treatments", "link": "http://arxiv.org/abs/2305.15742", "description": "Estimating the counterfactual outcome of treatment is essential for\ndecision-making in public health and clinical science, among others. Often,\ntreatments are administered in a sequential, time-varying manner, leading to an\nexponentially increased number of possible counterfactual outcomes.\nFurthermore, in modern applications, the outcomes are high-dimensional and\nconventional average treatment effect estimation fails to capture disparities\nin individuals. To tackle these challenges, we propose a novel conditional\ngenerative framework capable of producing counterfactual samples under\ntime-varying treatment, without the need for explicit density estimation. Our\nmethod carefully addresses the distribution mismatch between the observed and\ncounterfactual distributions via a loss function based on inverse probability\nweighting. We present a thorough evaluation of our method using both synthetic\nand real-world data. Our results demonstrate that our method is capable of\ngenerating high-quality counterfactual samples and outperforms the\nstate-of-the-art baselines."}, "http://arxiv.org/abs/2309.09115": {"title": "Fully Synthetic Data for Complex Surveys", "link": "http://arxiv.org/abs/2309.09115", "description": "When seeking to release public use files for confidential data, statistical\nagencies can generate fully synthetic data. We propose an approach for making\nfully synthetic data from surveys collected with complex sampling designs.\nSpecifically, we generate pseudo-populations by applying the weighted finite\npopulation Bayesian bootstrap to account for survey weights, take simple random\nsamples from those pseudo-populations, estimate synthesis models using these\nsimple random samples, and release simulated data drawn from the models as the\npublic use files. We use the framework of multiple imputation to enable\nvariance estimation using two data generation strategies. In the first, we\ngenerate multiple data sets from each simple random sample, whereas in the\nsecond, we generate a single synthetic data set from each simple random sample.\nWe present multiple imputation combining rules for each setting. We illustrate\neach approach and the repeated sampling properties of the combining rules using\nsimulation studies."}, "http://arxiv.org/abs/2309.09323": {"title": "Answering Layer 3 queries with DiscoSCMs", "link": "http://arxiv.org/abs/2309.09323", "description": "Addressing causal queries across the Pearl Causal Hierarchy (PCH) (i.e.,\nassociational, interventional and counterfactual), which is formalized as\n\\Layer{} Valuations, is a central task in contemporary causal inference\nresearch. Counterfactual questions, in particular, pose a significant challenge\nas they often necessitate a complete knowledge of structural equations. This\npaper identifies \\textbf{the degeneracy problem} caused by the consistency\nrule. To tackle this, the \\textit{Distribution-consistency Structural Causal\nModels} (DiscoSCMs) is introduced, which extends both the structural causal\nmodels (SCM) and the potential outcome framework. The correlation pattern of\npotential outcomes in personalized incentive scenarios, described by $P(y_x,\ny'_{x'})$, is used as a case study for elucidation. Although counterfactuals\nare no longer degenerate, they remain indeterminable. As a result, the\ncondition of independent potential noise is incorporated into DiscoSCM. It is\nfound that by adeptly using homogeneity, counterfactuals can be identified.\nFurthermore, more refined results are achieved in the unit problem scenario. In\nsimpler terms, when modeling counterfactuals, one should contemplate: \"Consider\na person with average ability who takes a test and, due to good luck, achieves\nan exceptionally high score. If this person were to retake the test under\nidentical external conditions, what score will he obtain? An exceptionally high\nscore or an average score?\" If your choose is predicting an average score, then\nyou are essentially choosing DiscoSCM over the traditional frameworks based on\nthe consistency rule."}, "http://arxiv.org/abs/2310.01748": {"title": "A generative approach to frame-level multi-competitor races", "link": "http://arxiv.org/abs/2310.01748", "description": "Multi-competitor races often feature complicated within-race strategies that\nare difficult to capture when training data on race outcome level data.\nFurther, models which do not account for such strategic effects may suffer from\nconfounded inferences and predictions. In this work we develop a general\ngenerative model for multi-competitor races which allows analysts to explicitly\nmodel certain strategic effects such as changing lanes or drafting and separate\nthese impacts from competitor ability. The generative model allows one to\nsimulate full races from any real or created starting position which opens new\navenues for attributing value to within-race actions and to perform\ncounter-factual analyses. This methodology is sufficiently general to apply to\nany track based multi-competitor races where both tracking data is available\nand competitor movement is well described by simultaneous forward and lateral\nmovements. We apply this methodology to one-mile horse races using data\nprovided by the New York Racing Association (NYRA) and the New York\nThoroughbred Horsemen's Association (NYTHA) for the Big Data Derby 2022 Kaggle\nCompetition. This data features granular tracking data for all horses at the\nframe-level (occurring at approximately 4hz). We demonstrate how this model can\nyield new inferences, such as the estimation of horse-specific speed profiles\nwhich vary over phases of the race, and examples of posterior predictive\ncounterfactual simulations to answer questions of interest such as starting\nlane impacts on race outcomes."}, "http://arxiv.org/abs/2310.09345": {"title": "A Unified Bayesian Framework for Modeling Measurement Error in Multinomial Data", "link": "http://arxiv.org/abs/2310.09345", "description": "Measurement error in multinomial data is a well-known and well-studied\ninferential problem that is encountered in many fields, including engineering,\nbiomedical and omics research, ecology, finance, and social sciences.\nSurprisingly, methods developed to accommodate measurement error in multinomial\ndata are typically equipped to handle false negatives or false positives, but\nnot both. We provide a unified framework for accommodating both forms of\nmeasurement error using a Bayesian hierarchical approach. We demonstrate the\nproposed method's performance on simulated data and apply it to acoustic bat\nmonitoring data."}, "http://arxiv.org/abs/2310.09384": {"title": "Modeling Missing at Random Neuropsychological Test Scores Using a Mixture of Binomial Product Experts", "link": "http://arxiv.org/abs/2310.09384", "description": "Multivariate bounded discrete data arises in many fields. In the setting of\nlongitudinal dementia studies, such data is collected when individuals complete\nneuropsychological tests. We outline a modeling and inference procedure that\ncan model the joint distribution conditional on baseline covariates, leveraging\nprevious work on mixtures of experts and latent class models. Furthermore, we\nillustrate how the work can be extended when the outcome data is missing at\nrandom using a nested EM algorithm. The proposed model can incorporate\ncovariate information, perform imputation and clustering, and infer latent\ntrajectories. We apply our model on simulated data and an Alzheimer's disease\ndata set."}, "http://arxiv.org/abs/2310.09398": {"title": "An In-Depth Examination of Requirements for Disclosure Risk Assessment", "link": "http://arxiv.org/abs/2310.09398", "description": "The use of formal privacy to protect the confidentiality of responses in the\n2020 Decennial Census of Population and Housing has triggered renewed interest\nand debate over how to measure the disclosure risks and societal benefits of\nthe published data products. Following long-established precedent in economics\nand statistics, we argue that any proposal for quantifying disclosure risk\nshould be based on pre-specified, objective criteria. Such criteria should be\nused to compare methodologies to identify those with the most desirable\nproperties. We illustrate this approach, using simple desiderata, to evaluate\nthe absolute disclosure risk framework, the counterfactual framework underlying\ndifferential privacy, and prior-to-posterior comparisons. We conclude that\nsatisfying all the desiderata is impossible, but counterfactual comparisons\nsatisfy the most while absolute disclosure risk satisfies the fewest.\nFurthermore, we explain that many of the criticisms levied against differential\nprivacy would be levied against any technology that is not equivalent to\ndirect, unrestricted access to confidential data. Thus, more research is\nneeded, but in the near-term, the counterfactual approach appears best-suited\nfor privacy-utility analysis."}, "http://arxiv.org/abs/2310.09428": {"title": "Sparse higher order partial least squares for simultaneous variable selection, dimension reduction, and tensor denoising", "link": "http://arxiv.org/abs/2310.09428", "description": "Partial Least Squares (PLS) regression emerged as an alternative to ordinary\nleast squares for addressing multicollinearity in a wide range of scientific\napplications. As multidimensional tensor data is becoming more widespread,\ntensor adaptations of PLS have been developed. Our investigations reveal that\nthe previously established asymptotic result of the PLS estimator for a tensor\nresponse breaks down as the tensor dimensions and the number of features\nincrease relative to the sample size. To address this, we propose Sparse Higher\nOrder Partial Least Squares (SHOPS) regression and an accompanying algorithm.\nSHOPS simultaneously accommodates variable selection, dimension reduction, and\ntensor association denoising. We establish the asymptotic accuracy of the SHOPS\nalgorithm under a high-dimensional regime and verify these results through\ncomprehensive simulation experiments, and applications to two contemporary\nhigh-dimensional biological data analysis."}, "http://arxiv.org/abs/2310.09493": {"title": "Summary Statistics Knockoffs Inference with Family-wise Error Rate Control", "link": "http://arxiv.org/abs/2310.09493", "description": "Testing multiple hypotheses of conditional independence with provable error\nrate control is a fundamental problem with various applications. To infer\nconditional independence with family-wise error rate (FWER) control when only\nsummary statistics of marginal dependence are accessible, we adopt\nGhostKnockoff to directly generate knockoff copies of summary statistics and\npropose a new filter to select features conditionally dependent to the response\nwith provable FWER control. In addition, we develop a computationally efficient\nalgorithm to greatly reduce the computational cost of knockoff copies\ngeneration without sacrificing power and FWER control. Experiments on simulated\ndata and a real dataset of Alzheimer's disease genetics demonstrate the\nadvantage of proposed method over the existing alternatives in both statistical\npower and computational efficiency."}, "http://arxiv.org/abs/2310.09545": {"title": "A Semiparametric Instrumented Difference-in-Differences Approach to Policy Learning", "link": "http://arxiv.org/abs/2310.09545", "description": "Recently, there has been a surge in methodological development for the\ndifference-in-differences (DiD) approach to evaluate causal effects. Standard\nmethods in the literature rely on the parallel trends assumption to identify\nthe average treatment effect on the treated. However, the parallel trends\nassumption may be violated in the presence of unmeasured confounding, and the\naverage treatment effect on the treated may not be useful in learning a\ntreatment assignment policy for the entire population. In this article, we\npropose a general instrumented DiD approach for learning the optimal treatment\npolicy. Specifically, we establish identification results using a binary\ninstrumental variable (IV) when the parallel trends assumption fails to hold.\nAdditionally, we construct a Wald estimator, novel inverse probability\nweighting (IPW) estimators, and a class of semiparametric efficient and\nmultiply robust estimators, with theoretical guarantees on consistency and\nasymptotic normality, even when relying on flexible machine learning algorithms\nfor nuisance parameters estimation. Furthermore, we extend the instrumented DiD\nto the panel data setting. We evaluate our methods in extensive simulations and\na real data application."}, "http://arxiv.org/abs/2310.09646": {"title": "Jackknife empirical likelihood confidence intervals for the categorical Gini correlation", "link": "http://arxiv.org/abs/2310.09646", "description": "The categorical Gini correlation, $\\rho_g$, was proposed by Dang et al. to\nmeasure the dependence between a categorical variable, $Y$ , and a numerical\nvariable, $X$. It has been shown that $\\rho_g$ has more appealing properties\nthan current existing dependence measurements. In this paper, we develop the\njackknife empirical likelihood (JEL) method for $\\rho_g$. Confidence intervals\nfor the Gini correlation are constructed without estimating the asymptotic\nvariance. Adjusted and weighted JEL are explored to improve the performance of\nthe standard JEL. Simulation studies show that our methods are competitive to\nexisting methods in terms of coverage accuracy and shortness of confidence\nintervals. The proposed methods are illustrated in an application on two real\ndatasets."}, "http://arxiv.org/abs/2310.09673": {"title": "Robust Quickest Change Detection in Non-Stationary Processes", "link": "http://arxiv.org/abs/2310.09673", "description": "Optimal algorithms are developed for robust detection of changes in\nnon-stationary processes. These are processes in which the distribution of the\ndata after change varies with time. The decision-maker does not have access to\nprecise information on the post-change distribution. It is shown that if the\npost-change non-stationary family has a distribution that is least favorable in\na well-defined sense, then the algorithms designed using the least favorable\ndistributions are robust and optimal. Non-stationary processes are encountered\nin public health monitoring and space and military applications. The robust\nalgorithms are applied to real and simulated data to show their effectiveness."}, "http://arxiv.org/abs/2310.09701": {"title": "A powerful empirical Bayes approach for high dimensional replicability analysis", "link": "http://arxiv.org/abs/2310.09701", "description": "Identifying replicable signals across different studies provides stronger\nscientific evidence and more powerful inference. Existing literature on high\ndimensional applicability analysis either imposes strong modeling assumptions\nor has low power. We develop a powerful and robust empirical Bayes approach for\nhigh dimensional replicability analysis. Our method effectively borrows\ninformation from different features and studies while accounting for\nheterogeneity. We show that the proposed method has better power than competing\nmethods while controlling the false discovery rate, both empirically and\ntheoretically. Analyzing datasets from the genome-wide association studies\nreveals new biological insights that otherwise cannot be obtained by using\nexisting methods."}, "http://arxiv.org/abs/2310.09702": {"title": "Inference with Mondrian Random Forests", "link": "http://arxiv.org/abs/2310.09702", "description": "Random forests are popular methods for classification and regression, and\nmany different variants have been proposed in recent years. One interesting\nexample is the Mondrian random forest, in which the underlying trees are\nconstructed according to a Mondrian process. In this paper we give a central\nlimit theorem for the estimates made by a Mondrian random forest in the\nregression setting. When combined with a bias characterization and a consistent\nvariance estimator, this allows one to perform asymptotically valid statistical\ninference, such as constructing confidence intervals, on the unknown regression\nfunction. We also provide a debiasing procedure for Mondrian random forests\nwhich allows them to achieve minimax-optimal estimation rates with\n$\\beta$-H\\\"older regression functions, for all $\\beta$ and in arbitrary\ndimension, assuming appropriate parameter tuning."}, "http://arxiv.org/abs/2310.09818": {"title": "MCMC for Bayesian nonparametric mixture modeling under differential privacy", "link": "http://arxiv.org/abs/2310.09818", "description": "Estimating the probability density of a population while preserving the\nprivacy of individuals in that population is an important and challenging\nproblem that has received considerable attention in recent years. While the\nprevious literature focused on frequentist approaches, in this paper, we\npropose a Bayesian nonparametric mixture model under differential privacy (DP)\nand present two Markov chain Monte Carlo (MCMC) algorithms for posterior\ninference. One is a marginal approach, resembling Neal's algorithm 5 with a\npseudo-marginal Metropolis-Hastings move, and the other is a conditional\napproach. Although our focus is primarily on local DP, we show that our MCMC\nalgorithms can be easily extended to deal with global differential privacy\nmechanisms. Moreover, for certain classes of mechanisms and mixture kernels, we\nshow how standard algorithms can be employed, resulting in substantial\nefficiency gains. Our approach is general and applicable to any mixture model\nand privacy mechanism. In several simulations and a real case study, we discuss\nthe performance of our algorithms and evaluate different privacy mechanisms\nproposed in the frequentist literature."}, "http://arxiv.org/abs/2310.09955": {"title": "On the Statistical Foundations of H-likelihood for Unobserved Random Variables", "link": "http://arxiv.org/abs/2310.09955", "description": "The maximum likelihood estimation is widely used for statistical inferences.\nIn this study, we reformulate the h-likelihood proposed by Lee and Nelder in\n1996, whose maximization yields maximum likelihood estimators for fixed\nparameters and asymptotically best unbiased predictors for random parameters.\nWe establish the statistical foundations for h-likelihood theories, which\nextend classical likelihood theories to embrace broad classes of statistical\nmodels with random parameters. The maximum h-likelihood estimators\nasymptotically achieve the generalized Cramer-Rao lower bound. Furthermore, we\nexplore asymptotic theory when the consistency of either fixed parameter\nestimation or random parameter prediction is violated. The introduction of this\nnew h-likelihood framework enables likelihood theories to cover inferences for\na much broader class of models, while also providing computationally efficient\nfitting algorithms to give asymptotically optimal estimators for fixed\nparameters and predictors for random parameters."}, "http://arxiv.org/abs/2310.09960": {"title": "Point Mass in the Confidence Distribution: Is it a Drawback or an Advantage?", "link": "http://arxiv.org/abs/2310.09960", "description": "Stein's (1959) problem highlights the phenomenon called the probability\ndilution in high dimensional cases, which is known as a fundamental deficiency\nin probabilistic inference. The satellite conjunction problem also suffers from\nprobability dilution that poor-quality data can lead to a dilution of collision\nprobability. Though various methods have been proposed, such as generalized\nfiducial distribution and the reference posterior, they could not maintain the\ncoverage probability of confidence intervals (CIs) in both problems. On the\nother hand, the confidence distribution (CD) has a point mass at zero, which\nhas been interpreted paradoxical. However, we show that this point mass is an\nadvantage rather than a drawback, because it gives a way to maintain the\ncoverage probability of CIs. More recently, `false confidence theorem' was\npresented as another deficiency in probabilistic inferences, called the false\nconfidence. It was further claimed that the use of consonant belief can\nmitigate this deficiency. However, we show that the false confidence theorem\ncannot be applied to the CD in both Stein's and satellite conjunction problems.\nIt is crucial that a confidence feature, not a consonant one, is the key to\novercome the deficiencies in probabilistic inferences. Our findings reveal that\nthe CD outperforms the other existing methods, including the consonant belief,\nin the context of Stein's and satellite conjunction problems. Additionally, we\ndemonstrate the ambiguity of coverage probability in an observed CI from the\nfrequentist CI procedure, and show that the CD provides valuable information\nregarding this ambiguity."}, "http://arxiv.org/abs/2310.09961": {"title": "Theoretical Evaluation of Asymmetric Shapley Values for Root-Cause Analysis", "link": "http://arxiv.org/abs/2310.09961", "description": "In this work, we examine Asymmetric Shapley Values (ASV), a variant of the\npopular SHAP additive local explanation method. ASV proposes a way to improve\nmodel explanations incorporating known causal relations between variables, and\nis also considered as a way to test for unfair discrimination in model\npredictions. Unexplored in previous literature, relaxing symmetry in Shapley\nvalues can have counter-intuitive consequences for model explanation. To better\nunderstand the method, we first show how local contributions correspond to\nglobal contributions of variance reduction. Using variance, we demonstrate\nmultiple cases where ASV yields counter-intuitive attributions, arguably\nproducing incorrect results for root-cause analysis. Second, we identify\ngeneralized additive models (GAM) as a restricted class for which ASV exhibits\ndesirable properties. We support our arguments by proving multiple theoretical\nresults about the method. Finally, we demonstrate the use of asymmetric\nattributions on multiple real-world datasets, comparing the results with and\nwithout restricted model families using gradient boosting and deep learning\nmodels."}, "http://arxiv.org/abs/2310.10003": {"title": "Conformal Contextual Robust Optimization", "link": "http://arxiv.org/abs/2310.10003", "description": "Data-driven approaches to predict-then-optimize decision-making problems seek\nto mitigate the risk of uncertainty region misspecification in safety-critical\nsettings. Current approaches, however, suffer from considering overly\nconservative uncertainty regions, often resulting in suboptimal decisionmaking.\nTo this end, we propose Conformal-Predict-Then-Optimize (CPO), a framework for\nleveraging highly informative, nonconvex conformal prediction regions over\nhigh-dimensional spaces based on conditional generative models, which have the\ndesired distribution-free coverage guarantees. Despite guaranteeing robustness,\nsuch black-box optimization procedures alone inspire little confidence owing to\nthe lack of explanation of why a particular decision was found to be optimal.\nWe, therefore, augment CPO to additionally provide semantically meaningful\nvisual summaries of the uncertainty regions to give qualitative intuition for\nthe optimal decision. We highlight the CPO framework by demonstrating results\non a suite of simulation-based inference benchmark tasks and a vehicle routing\ntask based on probabilistic weather prediction."}, "http://arxiv.org/abs/2310.10048": {"title": "Evaluation of transplant benefits with the U", "link": "http://arxiv.org/abs/2310.10048", "description": "Kidney transplantation is the most effective renal replacement therapy for\nend stage renal disease patients. With the severe shortage of kidney supplies\nand for the clinical effectiveness of transplantation, patient's life\nexpectancy post transplantation is used to prioritize patients for\ntransplantation; however, severe comorbidity conditions and old age are the\nmost dominant factors that negatively impact post-transplantation life\nexpectancy, effectively precluding sick or old patients from receiving\ntransplants. It would be crucial to design objective measures to quantify the\ntransplantation benefit by comparing the mean residual life with and without a\ntransplant, after adjusting for comorbidity and demographic conditions. To\naddress this urgent need, we propose a new class of semiparametric\ncovariate-dependent mean residual life models. Our method estimates covariate\neffects semiparametrically efficiently and the mean residual life function\nnonparametrically, enabling us to predict the residual life increment potential\nfor any given patient. Our method potentially leads to a more fair system that\nprioritizes patients who would have the largest residual life gains. Our\nanalysis of the kidney transplant data from the U.S. Scientific Registry of\nTransplant Recipients also suggests that a single index of covariates summarize\nwell the impacts of multiple covariates, which may facilitate interpretations\nof each covariate's effect. Our subgroup analysis further disclosed\ninequalities in survival gains across groups defined by race, gender and\ninsurance type (reflecting socioeconomic status)."}, "http://arxiv.org/abs/2310.10052": {"title": "Group-Orthogonal Subsampling for Hierarchical Data Based on Linear Mixed Models", "link": "http://arxiv.org/abs/2310.10052", "description": "Hierarchical data analysis is crucial in various fields for making\ndiscoveries. The linear mixed model is often used for training hierarchical\ndata, but its parameter estimation is computationally expensive, especially\nwith big data. Subsampling techniques have been developed to address this\nchallenge. However, most existing subsampling methods assume homogeneous data\nand do not consider the possible heterogeneity in hierarchical data. To address\nthis limitation, we develop a new approach called group-orthogonal subsampling\n(GOSS) for selecting informative subsets of hierarchical data that may exhibit\nheterogeneity. GOSS selects subdata with balanced data size among groups and\ncombinatorial orthogonality within each group, resulting in subdata that are\n$D$- and $A$-optimal for building linear mixed models. Estimators of parameters\ntrained on GOSS subdata are consistent and asymptotically normal. GOSS is shown\nto be numerically appealing via simulations and a real data application.\nTheoretical proofs, R codes, and supplementary numerical results are accessible\nonline as Supplementary Materials."}, "http://arxiv.org/abs/2310.10239": {"title": "Structural transfer learning of non-Gaussian DAG", "link": "http://arxiv.org/abs/2310.10239", "description": "Directed acyclic graph (DAG) has been widely employed to represent\ndirectional relationships among a set of collected nodes. Yet, the available\ndata in one single study is often limited for accurate DAG reconstruction,\nwhereas heterogeneous data may be collected from multiple relevant studies. It\nremains an open question how to pool the heterogeneous data together for better\nDAG structure reconstruction in the target study. In this paper, we first\nintroduce a novel set of structural similarity measures for DAG and then\npresent a transfer DAG learning framework by effectively leveraging information\nfrom auxiliary DAGs of different levels of similarities. Our theoretical\nanalysis shows substantial improvement in terms of DAG reconstruction in the\ntarget study, even when no auxiliary DAG is overall similar to the target DAG,\nwhich is in sharp contrast to most existing transfer learning methods. The\nadvantage of the proposed transfer DAG learning is also supported by extensive\nnumerical experiments on both synthetic data and multi-site brain functional\nconnectivity network data."}, "http://arxiv.org/abs/2310.10271": {"title": "A geometric power analysis for general log-linear models", "link": "http://arxiv.org/abs/2310.10271", "description": "General log-linear models are widely used to express the association in\nmultivariate frequency data on contingency tables. The paper focuses on the\npower analysis for testing the goodness-of-fit hypothesis for these models.\nConventionally, for the power-related sample size calculations a deviation from\nthe null hypothesis, aka effect size, is specified by means of the chi-square\ngoodness-of-fit index. It is argued that the odds ratio is a more natural\nmeasure of effect size, with the advantage of having a data-relevant\ninterpretation. Therefore, a class of log-affine models that are specified by\nodds ratios whose values deviate from those of the null by a small amount can\nbe chosen as an alternative. Being expressed as sets of constraints on odds\nratios, both hypotheses are represented by smooth surfaces in the probability\nsimplex, and thus, the power analysis can be given a geometric interpretation\nas well. A concept of geometric power is introduced and a Monte-Carlo algorithm\nfor its estimation is proposed. The framework is applied to the power analysis\nof goodness-of-fit in the context of multinomial sampling. An iterative scaling\nprocedure for generating distributions from a log-affine model is described and\nits convergence is proved. To illustrate, the geometric power analysis is\ncarried out for data from a clinical study."}, "http://arxiv.org/abs/2310.10324": {"title": "Assessing univariate and bivariate risks of late-frost and drought using vine copulas: A historical study for Bavaria", "link": "http://arxiv.org/abs/2310.10324", "description": "In light of climate change's impacts on forests, including extreme drought\nand late-frost, leading to vitality decline and regional forest die-back, we\nassess univariate drought and late-frost risks and perform a joint risk\nanalysis in Bavaria, Germany, from 1952 to 2020. Utilizing a vast dataset with\n26 bioclimatic and topographic variables, we employ vine copula models due to\nthe data's non-Gaussian and asymmetric dependencies. We use D-vine regression\nfor univariate and Y-vine regression for bivariate analysis, and propose\ncorresponding univariate and bivariate conditional probability risk measures.\nWe identify \"at-risk\" regions, emphasizing the need for forest adaptation due\nto climate change."}, "http://arxiv.org/abs/2310.10329": {"title": "Towards Data-Conditional Simulation for ABC Inference in Stochastic Differential Equations", "link": "http://arxiv.org/abs/2310.10329", "description": "We develop a Bayesian inference method for discretely-observed stochastic\ndifferential equations (SDEs). Inference is challenging for most SDEs, due to\nthe analytical intractability of the likelihood function. Nevertheless, forward\nsimulation via numerical methods is straightforward, motivating the use of\napproximate Bayesian computation (ABC). We propose a conditional simulation\nscheme for SDEs that is based on lookahead strategies for sequential Monte\nCarlo (SMC) and particle smoothing using backward simulation. This leads to the\nsimulation of trajectories that are consistent with the observed trajectory,\nthereby increasing the ABC acceptance rate. We additionally employ an invariant\nneural network, previously developed for Markov processes, to learn the summary\nstatistics function required in ABC. The neural network is incrementally\nretrained by exploiting an ABC-SMC sampler, which provides new training data at\neach round. Since the SDE simulation scheme differs from standard forward\nsimulation, we propose a suitable importance sampling correction, which has the\nadded advantage of guiding the parameters towards regions of high posterior\ndensity, especially in the first ABC-SMC round. Our approach achieves accurate\ninference and is about three times faster than standard (forward-only) ABC-SMC.\nWe illustrate our method in four simulation studies, including three examples\nfrom the Chan-Karaolyi-Longstaff-Sanders SDE family."}, "http://arxiv.org/abs/2310.10331": {"title": "Specifications tests for count time series models with covariates", "link": "http://arxiv.org/abs/2310.10331", "description": "We propose a goodness-of-fit test for a class of count time series models\nwith covariates which includes the Poisson autoregressive model with covariates\n(PARX) as a special case. The test criteria are derived from a specific\ncharacterization for the conditional probability generating function and the\ntest statistic is formulated as a $L_2$ weighting norm of the corresponding\nsample counterpart. The asymptotic properties of the proposed test statistic\nare provided under the null hypothesis as well as under specific alternatives.\nA bootstrap version of the test is explored in a Monte--Carlo study and\nillustrated on a real data set on road safety."}, "http://arxiv.org/abs/2310.10373": {"title": "False Discovery Proportion control for aggregated Knockoffs", "link": "http://arxiv.org/abs/2310.10373", "description": "Controlled variable selection is an important analytical step in various\nscientific fields, such as brain imaging or genomics. In these high-dimensional\ndata settings, considering too many variables leads to poor models and high\ncosts, hence the need for statistical guarantees on false positives. Knockoffs\nare a popular statistical tool for conditional variable selection in high\ndimension. However, they control for the expected proportion of false\ndiscoveries (FDR) and not their actual proportion (FDP). We present a new\nmethod, KOPI, that controls the proportion of false discoveries for\nKnockoff-based inference. The proposed method also relies on a new type of\naggregation to address the undesirable randomness associated with classical\nKnockoff inference. We demonstrate FDP control and substantial power gains over\nexisting Knockoff-based methods in various simulation settings and achieve good\nsensitivity/specificity tradeoffs on brain imaging and genomic data."}, "http://arxiv.org/abs/2310.10393": {"title": "Statistical and Causal Robustness for Causal Null Hypothesis Tests", "link": "http://arxiv.org/abs/2310.10393", "description": "Prior work applying semiparametric theory to causal inference has primarily\nfocused on deriving estimators that exhibit statistical robustness under a\nprespecified causal model that permits identification of a desired causal\nparameter. However, a fundamental challenge is correct specification of such a\nmodel, which usually involves making untestable assumptions. Evidence factors\nis an approach to combining hypothesis tests of a common causal null hypothesis\nunder two or more candidate causal models. Under certain conditions, this\nyields a test that is valid if at least one of the underlying models is\ncorrect, which is a form of causal robustness. We propose a method of combining\nsemiparametric theory with evidence factors. We develop a causal null\nhypothesis test based on joint asymptotic normality of K asymptotically linear\nsemiparametric estimators, where each estimator is based on a distinct\nidentifying functional derived from each of K candidate causal models. We show\nthat this test provides both statistical and causal robustness in the sense\nthat it is valid if at least one of the K proposed causal models is correct,\nwhile also allowing for slower than parametric rates of convergence in\nestimating nuisance functions. We demonstrate the efficacy of our method via\nsimulations and an application to the Framingham Heart Study."}, "http://arxiv.org/abs/2310.10407": {"title": "Ensemble methods for testing a global null", "link": "http://arxiv.org/abs/2310.10407", "description": "Testing a global null is a canonical problem in statistics and has a wide\nrange of applications. In view of the fact that no uniformly most powerful test\nexists, prior and/or domain knowledge are commonly used to focus on a certain\nclass of alternatives to improve the testing power. However, it is generally\nchallenging to develop tests that are particularly powerful against a certain\nclass of alternatives. In this paper, motivated by the success of ensemble\nlearning methods for prediction or classification, we propose an ensemble\nframework for testing that mimics the spirit of random forests to deal with the\nchallenges. Our ensemble testing framework aggregates a collection of weak base\ntests to form a final ensemble test that maintains strong and robust power for\nglobal nulls. We apply the framework to four problems about global testing in\ndifferent classes of alternatives arising from Whole Genome Sequencing (WGS)\nassociation studies. Specific ensemble tests are proposed for each of these\nproblems, and their theoretical optimality is established in terms of Bahadur\nefficiency. Extensive simulations and an analysis of a real WGS dataset are\nconducted to demonstrate the type I error control and/or power gain of the\nproposed ensemble tests."}, "http://arxiv.org/abs/2310.10422": {"title": "A Neural Network-Based Approach to Normality Testing for Dependent Data", "link": "http://arxiv.org/abs/2310.10422", "description": "There is a wide availability of methods for testing normality under the\nassumption of independent and identically distributed data. When data are\ndependent in space and/or time, however, assessing and testing the marginal\nbehavior is considerably more challenging, as the marginal behavior is impacted\nby the degree of dependence. We propose a new approach to assess normality for\ndependent data by non-linearly incorporating existing statistics from normality\ntests as well as sample moments such as skewness and kurtosis through a neural\nnetwork. We calibrate (deep) neural networks by simulated normal and non-normal\ndata with a wide range of dependence structures and we determine the\nprobability of rejecting the null hypothesis. We compare several approaches for\nnormality tests and demonstrate the superiority of our method in terms of\nstatistical power through an extensive simulation study. A real world\napplication to global temperature data further demonstrates how the degree of\nspatio-temporal aggregation affects the marginal normality in the data."}, "http://arxiv.org/abs/2310.10494": {"title": "Multivariate Scalar on Multidimensional Distribution Regression", "link": "http://arxiv.org/abs/2310.10494", "description": "We develop a new method for multivariate scalar on multidimensional\ndistribution regression. Traditional approaches typically analyze isolated\nunivariate scalar outcomes or consider unidimensional distributional\nrepresentations as predictors. However, these approaches are sub-optimal\nbecause: i) they fail to utilize the dependence between the distributional\npredictors: ii) neglect the correlation structure of the response. To overcome\nthese limitations, we propose a multivariate distributional analysis framework\nthat harnesses the power of multivariate density functions and multitask\nlearning. We develop a computationally efficient semiparametric estimation\nmethod for modelling the effect of the latent joint density on multivariate\nresponse of interest. Additionally, we introduce a new conformal algorithm for\nquantifying the uncertainty of regression models with multivariate responses\nand distributional predictors, providing valuable insights into the conditional\ndistribution of the response. We have validated the effectiveness of our\nproposed method through comprehensive numerical simulations, clearly\ndemonstrating its superior performance compared to traditional methods. The\napplication of the proposed method is demonstrated on tri-axial accelerometer\ndata from the National Health and Nutrition Examination Survey (NHANES)\n2011-2014 for modelling the association between cognitive scores across various\ndomains and distributional representation of physical activity among older\nadult population. Our results highlight the advantages of the proposed\napproach, emphasizing the significance of incorporating complete spatial\ninformation derived from the accelerometer device."}, "http://arxiv.org/abs/2310.10588": {"title": "Max-convolution processes with random shape indicator kernels", "link": "http://arxiv.org/abs/2310.10588", "description": "In this paper, we introduce a new class of models for spatial data obtained\nfrom max-convolution processes based on indicator kernels with random shape. We\nshow that this class of models have appealing dependence properties including\ntail dependence at short distances and independence at long distances. We\nfurther consider max-convolutions between such processes and processes with\ntail independence, in order to separately control the bulk and tail dependence\nbehaviors, and to increase flexibility of the model at longer distances, in\nparticular, to capture intermediate tail dependence. We show how parameters can\nbe estimated using a weighted pairwise likelihood approach, and we conduct an\nextensive simulation study to show that the proposed inference approach is\nfeasible in high dimensions and it yields accurate parameter estimates in most\ncases. We apply the proposed methodology to analyse daily temperature maxima\nmeasured at 100 monitoring stations in the state of Oklahoma, US. Our results\nindicate that our proposed model provides a good fit to the data, and that it\ncaptures both the bulk and the tail dependence structures accurately."}, "http://arxiv.org/abs/1805.07301": {"title": "Enhanced Pricing and Management of Bundled Insurance Risks with Dependence-aware Prediction using Pair Copula Construction", "link": "http://arxiv.org/abs/1805.07301", "description": "We propose a dependence-aware predictive modeling framework for multivariate\nrisks stemmed from an insurance contract with bundling features - an important\ntype of policy increasingly offered by major insurance companies. The bundling\nfeature naturally leads to longitudinal measurements of multiple insurance\nrisks, and correct pricing and management of such risks is of fundamental\ninterest to financial stability of the macroeconomy. We build a novel\npredictive model that fully captures the dependence among the multivariate\nrepeated risk measurements. Specifically, the longitudinal measurement of each\nindividual risk is first modeled using pair copula construction with a D-vine\nstructure, and the multiple D-vines are then integrated by a flexible copula.\nThe proposed model provides a unified modeling framework for multivariate\nlongitudinal data that can accommodate different scales of measurements,\nincluding continuous, discrete, and mixed observations, and thus can be\npotentially useful for various economic studies. A computationally efficient\nsequential method is proposed for model estimation and inference, and its\nperformance is investigated both theoretically and via simulation studies. In\nthe application, we examine multivariate bundled risks in multi-peril property\ninsurance using proprietary data from a commercial property insurance provider.\nThe proposed model is found to provide improved decision making for several key\ninsurance operations. For underwriting, we show that the experience rate priced\nby the proposed model leads to a 9% lift in the insurer's net revenue. For\nreinsurance, we show that the insurer underestimates the risk of the retained\ninsurance portfolio by 10% when ignoring the dependence among bundled insurance\nrisks."}, "http://arxiv.org/abs/2005.04721": {"title": "Decision Making in Drug Development via Inference on Power", "link": "http://arxiv.org/abs/2005.04721", "description": "A typical power calculation is performed by replacing unknown\npopulation-level quantities in the power function with what is observed in\nexternal studies. Many authors and practitioners view this as an assumed value\nof power and offer the Bayesian quantity probability of success or assurance as\nan alternative. The claim is by averaging over a prior or posterior\ndistribution, probability of success transcends power by capturing the\nuncertainty around the unknown true treatment effect and any other\npopulation-level parameters. We use p-value functions to frame both the\nprobability of success calculation and the typical power calculation as merely\nproducing two different point estimates of power. We demonstrate that Go/No-Go\ndecisions based on either point estimate of power do not adequately quantify\nand control the risk involved, and instead we argue for Go/No-Go decisions that\nutilize inference on power for better risk management and decision making."}, "http://arxiv.org/abs/2103.00674": {"title": "BEAUTY Powered BEAST", "link": "http://arxiv.org/abs/2103.00674", "description": "We study distribution-free goodness-of-fit tests with the proposed Binary\nExpansion Approximation of UniformiTY (BEAUTY) approach. This method\ngeneralizes the renowned Euler's formula, and approximates the characteristic\nfunction of any copula through a linear combination of expectations of binary\ninteractions from marginal binary expansions. This novel theory enables a\nunification of many important tests of independence via approximations from\nspecific quadratic forms of symmetry statistics, where the deterministic weight\nmatrix characterizes the power properties of each test. To achieve a robust\npower, we examine test statistics with data-adaptive weights, referred to as\nthe Binary Expansion Adaptive Symmetry Test (BEAST). Using properties of the\nbinary expansion filtration, we demonstrate that the Neyman-Pearson test of\nuniformity can be approximated by an oracle weighted sum of symmetry\nstatistics. The BEAST with this oracle provides a useful benchmark of feasible\npower. To approach this oracle power, we devise the BEAST through a regularized\nresampling approximation of the oracle test. The BEAST improves the empirical\npower of many existing tests against a wide spectrum of common alternatives and\ndelivers a clear interpretation of dependency forms when significant."}, "http://arxiv.org/abs/2103.16159": {"title": "Controlling the False Discovery Rate in Transformational Sparsity: Split Knockoffs", "link": "http://arxiv.org/abs/2103.16159", "description": "Controlling the False Discovery Rate (FDR) in a variable selection procedure\nis critical for reproducible discoveries, and it has been extensively studied\nin sparse linear models. However, it remains largely open in scenarios where\nthe sparsity constraint is not directly imposed on the parameters but on a\nlinear transformation of the parameters to be estimated. Examples of such\nscenarios include total variations, wavelet transforms, fused LASSO, and trend\nfiltering. In this paper, we propose a data-adaptive FDR control method, called\nthe Split Knockoff method, for this transformational sparsity setting. The\nproposed method exploits both variable and data splitting. The linear\ntransformation constraint is relaxed to its Euclidean proximity in a lifted\nparameter space, which yields an orthogonal design that enables the orthogonal\nSplit Knockoff construction. To overcome the challenge that exchangeability\nfails due to the heterogeneous noise brought by the transformation, new inverse\nsupermartingale structures are developed via data splitting for provable FDR\ncontrol without sacrificing power. Simulation experiments demonstrate that the\nproposed methodology achieves the desired FDR and power. We also provide an\napplication to Alzheimer's Disease study, where atrophy brain regions and their\nabnormal connections can be discovered based on a structural Magnetic Resonance\nImaging dataset (ADNI)."}, "http://arxiv.org/abs/2201.05967": {"title": "Uniform Inference for Kernel Density Estimators with Dyadic Data", "link": "http://arxiv.org/abs/2201.05967", "description": "Dyadic data is often encountered when quantities of interest are associated\nwith the edges of a network. As such it plays an important role in statistics,\neconometrics and many other data science disciplines. We consider the problem\nof uniformly estimating a dyadic Lebesgue density function, focusing on\nnonparametric kernel-based estimators taking the form of dyadic empirical\nprocesses. Our main contributions include the minimax-optimal uniform\nconvergence rate of the dyadic kernel density estimator, along with strong\napproximation results for the associated standardized and Studentized\n$t$-processes. A consistent variance estimator enables the construction of\nvalid and feasible uniform confidence bands for the unknown density function.\nWe showcase the broad applicability of our results by developing novel\ncounterfactual density estimation and inference methodology for dyadic data,\nwhich can be used for causal inference and program evaluation. A crucial\nfeature of dyadic distributions is that they may be \"degenerate\" at certain\npoints in the support of the data, a property making our analysis somewhat\ndelicate. Nonetheless our methods for uniform inference remain robust to the\npotential presence of such points. For implementation purposes, we discuss\ninference procedures based on positive semi-definite covariance estimators,\nmean squared error optimal bandwidth selectors and robust bias correction\ntechniques. We illustrate the empirical finite-sample performance of our\nmethods both in simulations and with real-world trade data, for which we make\ncomparisons between observed and counterfactual trade distributions in\ndifferent years. Our technical results concerning strong approximations and\nmaximal inequalities are of potential independent interest."}, "http://arxiv.org/abs/2206.01076": {"title": "Likelihood-based Inference for Random Networks with Changepoints", "link": "http://arxiv.org/abs/2206.01076", "description": "Generative, temporal network models play an important role in analyzing the\ndependence structure and evolution patterns of complex networks. Due to the\ncomplicated nature of real network data, it is often naive to assume that the\nunderlying data-generative mechanism itself is invariant with time. Such\nobservation leads to the study of changepoints or sudden shifts in the\ndistributional structure of the evolving network. In this paper, we propose a\nlikelihood-based methodology to detect changepoints in undirected, affine\npreferential attachment networks, and establish a hypothesis testing framework\nto detect a single changepoint, together with a consistent estimator for the\nchangepoint. Such results require establishing consistency and asymptotic\nnormality of the MLE under the changepoint regime, which suffers from long\nrange dependence. The methodology is then extended to the multiple changepoint\nsetting via both a sliding window method and a more computationally efficient\nscore statistic. We also compare the proposed methodology with previously\ndeveloped non-parametric estimators of the changepoint via simulation, and the\nmethods developed herein are applied to modeling the popularity of a topic in a\nTwitter network over time."}, "http://arxiv.org/abs/2301.01616": {"title": "Locally Private Causal Inference for Randomized Experiments", "link": "http://arxiv.org/abs/2301.01616", "description": "Local differential privacy is a differential privacy paradigm in which\nindividuals first apply a privacy mechanism to their data (often by adding\nnoise) before transmitting the result to a curator. The noise for privacy\nresults in additional bias and variance in their analyses. Thus it is of great\nimportance for analysts to incorporate the privacy noise into valid inference.\nIn this article, we develop methodologies to infer causal effects from locally\nprivatized data under randomized experiments. First, we present frequentist\nestimators under various privacy scenarios with their variance estimators and\nplug-in confidence intervals. We show a na\\\"ive debiased estimator results in\ninferior mean-squared error (MSE) compared to minimax lower bounds. In\ncontrast, we show that using a customized privacy mechanism, we can match the\nlower bound, giving minimax optimal inference. We also develop a Bayesian\nnonparametric methodology along with a blocked Gibbs sampling algorithm, which\ncan be applied to any of our proposed privacy mechanisms, and which performs\nespecially well in terms of MSE for tight privacy budgets. Finally, we present\nsimulation studies to evaluate the performance of our proposed frequentist and\nBayesian methodologies for various privacy budgets, resulting in useful\nsuggestions for performing causal inference for privatized data."}, "http://arxiv.org/abs/2303.03215": {"title": "Quantile-Quantile Methodology -- Detailed Results", "link": "http://arxiv.org/abs/2303.03215", "description": "The linear quantile-quantile relationship provides an easy-to-implement yet\neffective tool for transformation to and testing for normality. Its good\nperformance is verified in this report."}, "http://arxiv.org/abs/2305.06645": {"title": "Causal Inference for Continuous Multiple Time Point Interventions", "link": "http://arxiv.org/abs/2305.06645", "description": "There are limited options to estimate the treatment effects of variables\nwhich are continuous and measured at multiple time points, particularly if the\ntrue dose-response curve should be estimated as closely as possible. However,\nthese situations may be of relevance: in pharmacology, one may be interested in\nhow outcomes of people living with -- and treated for -- HIV, such as viral\nfailure, would vary for time-varying interventions such as different drug\nconcentration trajectories. A challenge for doing causal inference with\ncontinuous interventions is that the positivity assumption is typically\nviolated. To address positivity violations, we develop projection functions,\nwhich reweigh and redefine the estimand of interest based on functions of the\nconditional support for the respective interventions. With these functions, we\nobtain the desired dose-response curve in areas of enough support, and\notherwise a meaningful estimand that does not require the positivity\nassumption. We develop $g$-computation type plug-in estimators for this case.\nThose are contrasted with g-computation estimators which are applied to\ncontinuous interventions without specifically addressing positivity violations,\nwhich we propose to be presented with diagnostics. The ideas are illustrated\nwith longitudinal data from HIV positive children treated with an\nefavirenz-based regimen as part of the CHAPAS-3 trial, which enrolled children\n$&lt;13$ years in Zambia/Uganda. Simulations show in which situations a standard\n$g$-computation approach is appropriate, and in which it leads to bias and how\nthe proposed weighted estimation approach then recovers the alternative\nestimand of interest."}, "http://arxiv.org/abs/2305.14275": {"title": "Amortized Variational Inference with Coverage Guarantees", "link": "http://arxiv.org/abs/2305.14275", "description": "Amortized variational inference produces a posterior approximation that can\nbe rapidly computed given any new observation. Unfortunately, there are few\nguarantees about the quality of these approximate posteriors. We propose\nConformalized Amortized Neural Variational Inference (CANVI), a procedure that\nis scalable, easily implemented, and provides guaranteed marginal coverage.\nGiven a collection of candidate amortized posterior approximators, CANVI\nconstructs conformalized predictors based on each candidate, compares the\npredictors using a metric known as predictive efficiency, and returns the most\nefficient predictor. CANVI ensures that the resulting predictor constructs\nregions that contain the truth with a user-specified level of probability.\nCANVI is agnostic to design decisions in formulating the candidate\napproximators and only requires access to samples from the forward model,\npermitting its use in likelihood-free settings. We prove lower bounds on the\npredictive efficiency of the regions produced by CANVI and explore how the\nquality of a posterior approximation relates to the predictive efficiency of\nprediction regions based on that approximation. Finally, we demonstrate the\naccurate calibration and high predictive efficiency of CANVI on a suite of\nsimulation-based inference benchmark tasks and an important scientific task:\nanalyzing galaxy emission spectra."}, "http://arxiv.org/abs/2305.17187": {"title": "Clip-OGD: An Experimental Design for Adaptive Neyman Allocation in Sequential Experiments", "link": "http://arxiv.org/abs/2305.17187", "description": "From clinical development of cancer therapies to investigations into partisan\nbias, adaptive sequential designs have become increasingly popular method for\ncausal inference, as they offer the possibility of improved precision over\ntheir non-adaptive counterparts. However, even in simple settings (e.g. two\ntreatments) the extent to which adaptive designs can improve precision is not\nsufficiently well understood. In this work, we study the problem of Adaptive\nNeyman Allocation in a design-based potential outcomes framework, where the\nexperimenter seeks to construct an adaptive design which is nearly as efficient\nas the optimal (but infeasible) non-adaptive Neyman design, which has access to\nall potential outcomes. Motivated by connections to online optimization, we\npropose Neyman Ratio and Neyman Regret as two (equivalent) performance measures\nof adaptive designs for this problem. We present Clip-OGD, an adaptive design\nwhich achieves $\\widetilde{O}(\\sqrt{T})$ expected Neyman regret and thereby\nrecovers the optimal Neyman variance in large samples. Finally, we construct a\nconservative variance estimator which facilitates the development of\nasymptotically valid confidence intervals. To complement our theoretical\nresults, we conduct simulations using data from a microeconomic experiment."}, "http://arxiv.org/abs/2306.15622": {"title": "Biclustering random matrix partitions with an application to classification of forensic body fluids", "link": "http://arxiv.org/abs/2306.15622", "description": "Classification of unlabeled data is usually achieved by supervised learning\nfrom labeled samples. Although there exist many sophisticated supervised\nmachine learning methods that can predict the missing labels with a high level\nof accuracy, they often lack the required transparency in situations where it\nis important to provide interpretable results and meaningful measures of\nconfidence. Body fluid classification of forensic casework data is the case in\npoint. We develop a new Biclustering Dirichlet Process for Class-assignment\nwith Random Matrices (BDP-CaRMa), with a three-level hierarchy of clustering,\nand a model-based approach to classification that adapts to block structure in\nthe data matrix. As the class labels of some observations are missing, the\nnumber of rows in the data matrix for each class is unknown. BDP-CaRMa handles\nthis and extends existing biclustering methods by simultaneously biclustering\nmultiple matrices each having a randomly variable number of rows. We\ndemonstrate our method by applying it to the motivating problem, which is the\nclassification of body fluids based on mRNA profiles taken from crime scenes.\nThe analyses of casework-like data show that our method is interpretable and\nproduces well-calibrated posterior probabilities. Our model can be more\ngenerally applied to other types of data with a similar structure to the\nforensic data."}, "http://arxiv.org/abs/2307.05644": {"title": "Lambert W random variables and their applications in loss modelling", "link": "http://arxiv.org/abs/2307.05644", "description": "Several distributions and families of distributions are proposed to model\nskewed data, think, e.g., of skew-normal and related distributions. Lambert W\nrandom variables offer an alternative approach where, instead of constructing a\nnew distribution, a certain transform is proposed (Goerg, 2011). Such an\napproach allows the construction of a Lambert W skewed version from any\ndistribution. We choose Lambert W normal distribution as a natural starting\npoint and also include Lambert W exponential distribution due to the simplicity\nand shape of the exponential distribution, which, after skewing, may produce a\nreasonably heavy tail for loss models. In the theoretical part, we focus on the\nmathematical properties of obtained distributions, including the range of\nskewness. In the practical part, the suitability of corresponding Lambert W\ntransformed distributions is evaluated on real insurance data. The results are\ncompared with those obtained using common loss distributions."}, "http://arxiv.org/abs/2307.06840": {"title": "Ensemble learning for blending gridded satellite and gauge-measured precipitation data", "link": "http://arxiv.org/abs/2307.06840", "description": "Regression algorithms are regularly used for improving the accuracy of\nsatellite precipitation products. In this context, satellite precipitation and\ntopography data are the predictor variables, and gauged-measured precipitation\ndata are the dependent variables. Alongside this, it is increasingly recognised\nin many fields that combinations of algorithms through ensemble learning can\nlead to substantial predictive performance improvements. Still, a sufficient\nnumber of ensemble learners for improving the accuracy of satellite\nprecipitation products and their large-scale comparison are currently missing\nfrom the literature. In this study, we work towards filling in this specific\ngap by proposing 11 new ensemble learners in the field and by extensively\ncomparing them. We apply the ensemble learners to monthly data from the\nPERSIANN (Precipitation Estimation from Remotely Sensed Information using\nArtificial Neural Networks) and IMERG (Integrated Multi-satellitE Retrievals\nfor GPM) gridded datasets that span over a 15-year period and over the entire\nthe contiguous United States (CONUS). We also use gauge-measured precipitation\ndata from the Global Historical Climatology Network monthly database, version 2\n(GHCNm). The ensemble learners combine the predictions of six machine learning\nregression algorithms (base learners), namely the multivariate adaptive\nregression splines (MARS), multivariate adaptive polynomial splines\n(poly-MARS), random forests (RF), gradient boosting machines (GBM), extreme\ngradient boosting (XGBoost) and Bayesian regularized neural networks (BRNN),\nand each of them is based on a different combiner. The combiners include the\nequal-weight combiner, the median combiner, two best learners and seven\nvariants of a sophisticated stacking method. The latter stacks a regression\nalgorithm on top of the base learners to combine their independent\npredictions..."}, "http://arxiv.org/abs/2309.12819": {"title": "Doubly Robust Proximal Causal Learning for Continuous Treatments", "link": "http://arxiv.org/abs/2309.12819", "description": "Proximal causal learning is a promising framework for identifying the causal\neffect under the existence of unmeasured confounders. Within this framework,\nthe doubly robust (DR) estimator was derived and has shown its effectiveness in\nestimation, especially when the model assumption is violated. However, the\ncurrent form of the DR estimator is restricted to binary treatments, while the\ntreatment can be continuous in many real-world applications. The primary\nobstacle to continuous treatments resides in the delta function present in the\noriginal DR estimator, making it infeasible in causal effect estimation and\nintroducing a heavy computational burden in nuisance function estimation. To\naddress these challenges, we propose a kernel-based DR estimator that can well\nhandle continuous treatments. Equipped with its smoothness, we show that its\noracle form is a consistent approximation of the influence function. Further,\nwe propose a new approach to efficiently solve the nuisance functions. We then\nprovide a comprehensive convergence analysis in terms of the mean square error.\nWe demonstrate the utility of our estimator on synthetic datasets and\nreal-world applications."}, "http://arxiv.org/abs/2309.17283": {"title": "The Blessings of Multiple Treatments and Outcomes in Treatment Effect Estimation", "link": "http://arxiv.org/abs/2309.17283", "description": "Assessing causal effects in the presence of unobserved confounding is a\nchallenging problem. Existing studies leveraged proxy variables or multiple\ntreatments to adjust for the confounding bias. In particular, the latter\napproach attributes the impact on a single outcome to multiple treatments,\nallowing estimating latent variables for confounding control. Nevertheless,\nthese methods primarily focus on a single outcome, whereas in many real-world\nscenarios, there is greater interest in studying the effects on multiple\noutcomes. Besides, these outcomes are often coupled with multiple treatments.\nExamples include the intensive care unit (ICU), where health providers evaluate\nthe effectiveness of therapies on multiple health indicators. To accommodate\nthese scenarios, we consider a new setting dubbed as multiple treatments and\nmultiple outcomes. We then show that parallel studies of multiple outcomes\ninvolved in this setting can assist each other in causal identification, in the\nsense that we can exploit other treatments and outcomes as proxies for each\ntreatment effect under study. We proceed with a causal discovery method that\ncan effectively identify such proxies for causal estimation. The utility of our\nmethod is demonstrated in synthetic data and sepsis disease."}, "http://arxiv.org/abs/2310.10740": {"title": "Unbiased Estimation of Structured Prediction Error", "link": "http://arxiv.org/abs/2310.10740", "description": "Many modern datasets, such as those in ecology and geology, are composed of\nsamples with spatial structure and dependence. With such data violating the\nusual independent and identically distributed (IID) assumption in machine\nlearning and classical statistics, it is unclear a priori how one should\nmeasure the performance and generalization of models. Several authors have\nempirically investigated cross-validation (CV) methods in this setting,\nreaching mixed conclusions. We provide a class of unbiased estimation methods\nfor general quadratic errors, correlated Gaussian response, and arbitrary\nprediction function $g$, for a noise-elevated version of the error. Our\napproach generalizes the coupled bootstrap (CB) from the normal means problem\nto general normal data, allowing correlation both within and between the\ntraining and test sets. CB relies on creating bootstrap samples that are\nintelligently decoupled, in the sense of being statistically independent.\nSpecifically, the key to CB lies in generating two independent \"views\" of our\ndata and using them as stand-ins for the usual independent training and test\nsamples. Beginning with Mallows' $C_p$, we generalize the estimator to develop\nour generalized $C_p$ estimators (GC). We show at under only a moment condition\non $g$, this noise-elevated error estimate converges smoothly to the noiseless\nerror estimate. We show that when Stein's unbiased risk estimator (SURE)\napplies, GC converges to SURE as in the normal means problem. Further, we use\nthese same tools to analyze CV and provide some theoretical analysis to help\nunderstand when CV will provide good estimates of error. Simulations align with\nour theoretical results, demonstrating the effectiveness of GC and illustrating\nthe behavior of CV methods. Lastly, we apply our estimator to a model selection\ntask on geothermal data in Nevada."}, "http://arxiv.org/abs/2310.10761": {"title": "Simulation Based Composite Likelihood", "link": "http://arxiv.org/abs/2310.10761", "description": "Inference for high-dimensional hidden Markov models is challenging due to the\nexponential-in-dimension computational cost of the forward algorithm. To\naddress this issue, we introduce an innovative composite likelihood approach\ncalled \"Simulation Based Composite Likelihood\" (SimBa-CL). With SimBa-CL, we\napproximate the likelihood by the product of its marginals, which we estimate\nusing Monte Carlo sampling. In a similar vein to approximate Bayesian\ncomputation (ABC), SimBa-CL requires multiple simulations from the model, but,\nin contrast to ABC, it provides a likelihood approximation that guides the\noptimization of the parameters. Leveraging automatic differentiation libraries,\nit is simple to calculate gradients and Hessians to not only speed-up\noptimization, but also to build approximate confidence sets. We conclude with\nan extensive experimental section, where we empirically validate our\ntheoretical results, conduct a comparative analysis with SMC, and apply\nSimBa-CL to real-world Aphtovirus data."}, "http://arxiv.org/abs/2310.10798": {"title": "Poisson Count Time Series", "link": "http://arxiv.org/abs/2310.10798", "description": "This paper reviews and compares popular methods, some old and some very\nrecent, that produce time series having Poisson marginal distributions. The\npaper begins by narrating ways where time series with Poisson marginal\ndistributions can be produced. Modeling nonstationary series with covariates\nmotivates consideration of methods where the Poisson parameter depends on time.\nHere, estimation methods are developed for some of the more flexible methods.\nThe results are used in the analysis of 1) a count sequence of tropical\ncyclones occurring in the North Atlantic Basin since 1970, and 2) the number of\nno-hitter games pitched in major league baseball since 1893. Tests for whether\nthe Poisson marginal distribution is appropriate are included."}, "http://arxiv.org/abs/2310.10915": {"title": "Identifiability of the Multinomial Processing Tree-IRT model for the Philadelphia Naming Test", "link": "http://arxiv.org/abs/2310.10915", "description": "For persons with aphasia, naming tests are used to evaluate the severity of\nthe disease and observing progress toward recovery. The Philadelphia Naming\nTest (PNT) is a leading naming test composed of 175 items. The items are common\nnouns which are one to four syllables in length and with low, medium, and high\nfrequency. Since the target word is known to the administrator, the response\nfrom the patient can be classified as correct or an error. If the patient\ncommits an error, the PNT provides procedures for classifying the type of error\nin the response. Item response theory can be applied to PNT data to provide\nestimates of item difficulty and subject naming ability. Walker et al. (2018)\ndeveloped a IRT multinomial processing tree (IRT-MPT) model to attempt to\nunderstand the pathways through which the different errors are made by patients\nwhen responding to an item. The MPT model expands on existing models by\nconsidering items to be heterogeneous and estimating multiple latent parameters\nfor patients to more precisely determine at which step of word of production a\npatient's ability has been affected. These latent parameters represent the\ntheoretical cognitive steps taken in responding to an item. Given the\ncomplexity of the model proposed in Walker et al. (2018), here we investigate\nthe identifiability of the parameters included in the IRT-MPT model."}, "http://arxiv.org/abs/2310.10976": {"title": "Exact nonlinear state estimation", "link": "http://arxiv.org/abs/2310.10976", "description": "The majority of data assimilation (DA) methods in the geosciences are based\non Gaussian assumptions. While these assumptions facilitate efficient\nalgorithms, they cause analysis biases and subsequent forecast degradations.\nNon-parametric, particle-based DA algorithms have superior accuracy, but their\napplication to high-dimensional models still poses operational challenges.\nDrawing inspiration from recent advances in the field of generative artificial\nintelligence (AI), this article introduces a new nonlinear estimation theory\nwhich attempts to bridge the existing gap in DA methodology. Specifically, a\nConjugate Transform Filter (CTF) is derived and shown to generalize the\ncelebrated Kalman filter to arbitrarily non-Gaussian distributions. The new\nfilter has several desirable properties, such as its ability to preserve\nstatistical relationships in the prior state and convergence to highly accurate\nobservations. An ensemble approximation of the new theory (ECTF) is also\npresented and validated using idealized statistical experiments that feature\nbounded quantities with non-Gaussian distributions, a prevalent challenge in\nEarth system models. Results from these experiments indicate that the greatest\nbenefits from ECTF occur when observation errors are small relative to the\nforecast uncertainty and when state variables exhibit strong nonlinear\ndependencies. Ultimately, the new filtering theory offers exciting avenues for\nimproving conventional DA algorithms through their principled integration with\nAI techniques."}, "http://arxiv.org/abs/2310.11122": {"title": "Sensitivity-Aware Amortized Bayesian Inference", "link": "http://arxiv.org/abs/2310.11122", "description": "Bayesian inference is a powerful framework for making probabilistic\ninferences and decisions under uncertainty. Fundamental choices in modern\nBayesian workflows concern the specification of the likelihood function and\nprior distributions, the posterior approximator, and the data. Each choice can\nsignificantly influence model-based inference and subsequent decisions, thereby\nnecessitating sensitivity analysis. In this work, we propose a multifaceted\napproach to integrate sensitivity analyses into amortized Bayesian inference\n(ABI, i.e., simulation-based inference with neural networks). First, we utilize\nweight sharing to encode the structural similarities between alternative\nlikelihood and prior specifications in the training process with minimal\ncomputational overhead. Second, we leverage the rapid inference of neural\nnetworks to assess sensitivity to various data perturbations or pre-processing\nprocedures. In contrast to most other Bayesian approaches, both steps\ncircumvent the costly bottleneck of refitting the model(s) for each choice of\nlikelihood, prior, or dataset. Finally, we propose to use neural network\nensembles to evaluate variation in results induced by unreliable approximation\non unseen data. We demonstrate the effectiveness of our method in applied\nmodeling problems, ranging from the estimation of disease outbreak dynamics and\nglobal warming thresholds to the comparison of human decision-making models.\nOur experiments showcase how our approach enables practitioners to effectively\nunveil hidden relationships between modeling choices and inferential\nconclusions."}, "http://arxiv.org/abs/2310.11357": {"title": "A Pseudo-likelihood Approach to Under-5 Mortality Estimation", "link": "http://arxiv.org/abs/2310.11357", "description": "Accurate and precise estimates of under-5 mortality rates (U5MR) are an\nimportant health summary for countries. Full survival curves are additionally\nof interest to better understand the pattern of mortality in children under 5.\nModern demographic methods for estimating a full mortality schedule for\nchildren have been developed for countries with good vital registration and\nreliable census data, but perform poorly in many low- and middle-income\ncountries. In these countries, the need to utilize nationally representative\nsurveys to estimate U5MR requires additional statistical care to mitigate\npotential biases in survey data, acknowledge the survey design, and handle\naspects of survival data (i.e., censoring and truncation). In this paper, we\ndevelop parametric and non-parametric pseudo-likelihood approaches to\nestimating under-5 mortality across time from complex survey data. We argue\nthat the parametric approach is particularly useful in scenarios where data are\nsparse and estimation may require stronger assumptions. The nonparametric\napproach provides an aid to model validation. We compare a variety of\nparametric models to three existing methods for obtaining a full survival curve\nfor children under the age of 5, and argue that a parametric pseudo-likelihood\napproach is advantageous in low- and middle-income countries. We apply our\nproposed approaches to survey data from Burkina Faso, Malawi, Senegal, and\nNamibia. All code for fitting the models described in this paper is available\nin the R package pssst."}, "http://arxiv.org/abs/2006.00767": {"title": "Generative Multiple-purpose Sampler for Weighted M-estimation", "link": "http://arxiv.org/abs/2006.00767", "description": "To overcome the computational bottleneck of various data perturbation\nprocedures such as the bootstrap and cross validations, we propose the\nGenerative Multiple-purpose Sampler (GMS), which constructs a generator\nfunction to produce solutions of weighted M-estimators from a set of given\nweights and tuning parameters. The GMS is implemented by a single optimization\nwithout having to repeatedly evaluate the minimizers of weighted losses, and is\nthus capable of significantly reducing the computational time. We demonstrate\nthat the GMS framework enables the implementation of various statistical\nprocedures that would be unfeasible in a conventional framework, such as the\niterated bootstrap, bootstrapped cross-validation for penalized likelihood,\nbootstrapped empirical Bayes with nonparametric maximum likelihood, etc. To\nconstruct a computationally efficient generator function, we also propose a\nnovel form of neural network called the \\emph{weight multiplicative multilayer\nperceptron} to achieve fast convergence. Our numerical results demonstrate that\nthe new neural network structure enjoys a few orders of magnitude speed\nadvantage in comparison to the conventional one. An R package called GMS is\nprovided, which runs under Pytorch to implement the proposed methods and allows\nthe user to provide a customized loss function to tailor to their own models of\ninterest."}, "http://arxiv.org/abs/2012.03593": {"title": "Algebraic geometry of discrete interventional models", "link": "http://arxiv.org/abs/2012.03593", "description": "We investigate the algebra and geometry of general interventions in discrete\nDAG models. To this end, we introduce a theory for modeling soft interventions\nin the more general family of staged tree models and develop the formalism to\nstudy these models as parametrized subvarieties of a product of probability\nsimplices. We then consider the problem of finding their defining equations,\nand we derive a combinatorial criterion for identifying interventional staged\ntree models for which the defining ideal is toric. We apply these results to\nthe class of discrete interventional DAG models and establish a criteria to\ndetermine when these models are toric varieties."}, "http://arxiv.org/abs/2105.12720": {"title": "Marginal structural models with Latent Class Growth Modeling of Treatment Trajectories", "link": "http://arxiv.org/abs/2105.12720", "description": "In a real-life setting, little is known regarding the effectiveness of\nstatins for primary prevention among older adults, and analysis of\nobservational data can add crucial information on the benefits of actual\npatterns of use. Latent class growth models (LCGM) are increasingly proposed as\na solution to summarize the observed longitudinal treatment in a few distinct\ngroups. When combined with standard approaches like Cox proportional hazards\nmodels, LCGM can fail to control time-dependent confounding bias because of\ntime-varying covariates that have a double role of confounders and mediators.\nWe propose to use LCGM to classify individuals into a few latent classes based\non their medication adherence pattern, then choose a working marginal\nstructural model (MSM) that relates the outcome to these groups. The parameter\nof interest is nonparametrically defined as the projection of the true MSM onto\nthe chosen working model. The combination of LCGM with MSM is a convenient way\nto describe treatment adherence and can effectively control time-dependent\nconfounding. Simulation studies were used to illustrate our approach and\ncompare it with unadjusted, baseline covariates-adjusted, time-varying\ncovariates adjusted and inverse probability of trajectory groups weighting\nadjusted models. We found that our proposed approach yielded estimators with\nlittle or no bias."}, "http://arxiv.org/abs/2208.07610": {"title": "E-Statistics, Group Invariance and Anytime Valid Testing", "link": "http://arxiv.org/abs/2208.07610", "description": "We study worst-case-growth-rate-optimal (GROW) e-statistics for hypothesis\ntesting between two group models. It is known that under a mild condition on\nthe action of the underlying group G on the data, there exists a maximally\ninvariant statistic. We show that among all e-statistics, invariant or not, the\nlikelihood ratio of the maximally invariant statistic is GROW, both in the\nabsolute and in the relative sense, and that an anytime-valid test can be based\non it. The GROW e-statistic is equal to a Bayes factor with a right Haar prior\non G. Our treatment avoids nonuniqueness issues that sometimes arise for such\npriors in Bayesian contexts. A crucial assumption on the group G is its\namenability, a well-known group-theoretical condition, which holds, for\ninstance, in scale-location families. Our results also apply to\nfinite-dimensional linear regression."}, "http://arxiv.org/abs/2302.03246": {"title": "CDANs: Temporal Causal Discovery from Autocorrelated and Non-Stationary Time Series Data", "link": "http://arxiv.org/abs/2302.03246", "description": "Time series data are found in many areas of healthcare such as medical time\nseries, electronic health records (EHR), measurements of vitals, and wearable\ndevices. Causal discovery, which involves estimating causal relationships from\nobservational data, holds the potential to play a significant role in\nextracting actionable insights about human health. In this study, we present a\nnovel constraint-based causal discovery approach for autocorrelated and\nnon-stationary time series data (CDANs). Our proposed method addresses several\nlimitations of existing causal discovery methods for autocorrelated and\nnon-stationary time series data, such as high dimensionality, the inability to\nidentify lagged causal relationships, and overlooking changing modules. Our\napproach identifies lagged and instantaneous/contemporaneous causal\nrelationships along with changing modules that vary over time. The method\noptimizes the conditioning sets in a constraint-based search by considering\nlagged parents instead of conditioning on the entire past that addresses high\ndimensionality. The changing modules are detected by considering both\ncontemporaneous and lagged parents. The approach first detects the lagged\nadjacencies, then identifies the changing modules and contemporaneous\nadjacencies, and finally determines the causal direction. We extensively\nevaluated our proposed method on synthetic and real-world clinical datasets,\nand compared its performance with several baseline approaches. The experimental\nresults demonstrate the effectiveness of the proposed method in detecting\ncausal relationships and changing modules for autocorrelated and non-stationary\ntime series data."}, "http://arxiv.org/abs/2305.07089": {"title": "Hierarchically Coherent Multivariate Mixture Networks", "link": "http://arxiv.org/abs/2305.07089", "description": "Large collections of time series data are often organized into hierarchies\nwith different levels of aggregation; examples include product and geographical\ngroupings. Probabilistic coherent forecasting is tasked to produce forecasts\nconsistent across levels of aggregation. In this study, we propose to augment\nneural forecasting architectures with a coherent multivariate mixture output.\nWe optimize the networks with a composite likelihood objective, allowing us to\ncapture time series' relationships while maintaining high computational\nefficiency. Our approach demonstrates 13.2% average accuracy improvements on\nmost datasets compared to state-of-the-art baselines. We conduct ablation\nstudies of the framework components and provide theoretical foundations for\nthem. To assist related work, the code is available at this\nhttps://github.com/Nixtla/neuralforecast."}, "http://arxiv.org/abs/2307.16720": {"title": "The epigraph and the hypograph indexes as useful tools for clustering multivariate functional data", "link": "http://arxiv.org/abs/2307.16720", "description": "The proliferation of data generation has spurred advancements in functional\ndata analysis. With the ability to analyze multiple variables simultaneously,\nthe demand for working with multivariate functional data has increased. This\nstudy proposes a novel formulation of the epigraph and hypograph indexes, as\nwell as their generalized expressions, specifically tailored for the\nmultivariate functional context. These definitions take into account the\ninterrelations between components. Furthermore, the proposed indexes are\nemployed to cluster multivariate functional data. In the clustering process,\nthe indexes are applied to both the data and their first and second\nderivatives. This generates a reduced-dimension dataset from the original\nmultivariate functional data, enabling the application of well-established\nmultivariate clustering techniques which have been extensively studied in the\nliterature. This methodology has been tested through simulated and real\ndatasets, performing comparative analyses against state-of-the-art to assess\nits performance."}, "http://arxiv.org/abs/2309.07810": {"title": "Spectrum-Aware Adjustment: A New Debiasing Framework with Applications to Principal Component Regression", "link": "http://arxiv.org/abs/2309.07810", "description": "We introduce a new debiasing framework for high-dimensional linear regression\nthat bypasses the restrictions on covariate distributions imposed by modern\ndebiasing technology. We study the prevalent setting where the number of\nfeatures and samples are both large and comparable. In this context,\nstate-of-the-art debiasing technology uses a degrees-of-freedom correction to\nremove the shrinkage bias of regularized estimators and conduct inference.\nHowever, this method requires that the observed samples are i.i.d., the\ncovariates follow a mean zero Gaussian distribution, and reliable covariance\nmatrix estimates for observed features are available. This approach struggles\nwhen (i) covariates are non-Gaussian with heavy tails or asymmetric\ndistributions, (ii) rows of the design exhibit heterogeneity or dependencies,\nand (iii) reliable feature covariance estimates are lacking.\n\nTo address these, we develop a new strategy where the debiasing correction is\na rescaled gradient descent step (suitably initialized) with step size\ndetermined by the spectrum of the sample covariance matrix. Unlike prior work,\nwe assume that eigenvectors of this matrix are uniform draws from the\northogonal group. We show this assumption remains valid in diverse situations\nwhere traditional debiasing fails, including designs with complex row-column\ndependencies, heavy tails, asymmetric properties, and latent low-rank\nstructures. We establish asymptotic normality of our proposed estimator\n(centered and scaled) under various convergence notions. Moreover, we develop a\nconsistent estimator for its asymptotic variance. Lastly, we introduce a\ndebiased Principal Components Regression (PCR) technique using our\nSpectrum-Aware approach. In varied simulations and real data experiments, we\nobserve that our method outperforms degrees-of-freedom debiasing by a margin."}, "http://arxiv.org/abs/2310.11471": {"title": "Modeling lower-truncated and right-censored insurance claims with an extension of the MBBEFD class", "link": "http://arxiv.org/abs/2310.11471", "description": "In general insurance, claims are often lower-truncated and right-censored\nbecause insurance contracts may involve deductibles and maximal covers. Most\nclassical statistical models are not (directly) suited to model lower-truncated\nand right-censored claims. A surprisingly flexible family of distributions that\ncan cope with lower-truncated and right-censored claims is the class of MBBEFD\ndistributions that originally has been introduced by Bernegger (1997) for\nreinsurance pricing, but which has not gained much attention outside the\nreinsurance literature. We derive properties of the class of MBBEFD\ndistributions, and we extend it to a bigger family of distribution functions\nsuitable for modeling lower-truncated and right-censored claims. Interestingly,\nin general insurance, we mainly rely on unimodal skewed densities, whereas the\nreinsurance literature typically proposes monotonically decreasing densities\nwithin the MBBEFD class."}, "http://arxiv.org/abs/2310.11603": {"title": "Group sequential two-stage preference designs", "link": "http://arxiv.org/abs/2310.11603", "description": "The two-stage preference design (TSPD) enables the inference for treatment\nefficacy while allowing for incorporation of patient preference to treatment.\nIt can provide unbiased estimates for selection and preference effects, where a\nselection effect occurs when patients who prefer one treatment respond\ndifferently than those who prefer another, and a preference effect is the\ndifference in response caused by an interaction between the patient's\npreference and the actual treatment they receive. One potential barrier to\nadopting TSPD in practice, however, is the relatively large sample size\nrequired to estimate selection and preference effects with sufficient power. To\naddress this concern, we propose a group sequential two-stage preference design\n(GS-TSPD), which combines TSPD with sequential monitoring for early stopping.\nIn the GS-TSPD, pre-planned sequential monitoring allows investigators to\nconduct repeated hypothesis tests on accumulated data prior to full enrollment\nto assess study eligibility for early trial termination without inflating type\nI error rates. Thus, the procedure allows investigators to terminate the study\nwhen there is sufficient evidence of treatment, selection, or preference\neffects during an interim analysis, thereby reducing the design resource in\nexpectation. To formalize such a procedure, we verify the independent\nincrements assumption for testing the selection and preference effects and\napply group sequential stopping boundaries from the approximate sequential\ndensity functions. Simulations are then conducted to investigate the operating\ncharacteristics of our proposed GS-TSPD compared to the traditional TSPD. We\ndemonstrate the applicability of the design using a study of Hepatitis C\ntreatment modality."}, "http://arxiv.org/abs/2310.11620": {"title": "Enhancing modified treatment policy effect estimation with weighted energy distance", "link": "http://arxiv.org/abs/2310.11620", "description": "The effects of continuous treatments are often characterized through the\naverage dose response function, which is challenging to estimate from\nobservational data due to confounding and positivity violations. Modified\ntreatment policies (MTPs) are an alternative approach that aim to assess the\neffect of a modification to observed treatment values and work under relaxed\nassumptions. Estimators for MTPs generally focus on estimating the conditional\ndensity of treatment given covariates and using it to construct weights.\nHowever, weighting using conditional density models has well-documented\nchallenges. Further, MTPs with larger treatment modifications have stronger\nconfounding and no tools exist to help choose an appropriate modification\nmagnitude. This paper investigates the role of weights for MTPs showing that to\ncontrol confounding, weights should balance the weighted data to an unobserved\nhypothetical target population, that can be characterized with observed data.\nLeveraging this insight, we present a versatile set of tools to enhance\nestimation for MTPs. We introduce a distance that measures imbalance of\ncovariate distributions under the MTP and use it to develop new weighting\nmethods and tools to aid in the estimation of MTPs. We illustrate our methods\nthrough an example studying the effect of mechanical power of ventilation on\nin-hospital mortality."}, "http://arxiv.org/abs/2310.11630": {"title": "Adaptive Bootstrap Tests for Composite Null Hypotheses in the Mediation Pathway Analysis", "link": "http://arxiv.org/abs/2310.11630", "description": "Mediation analysis aims to assess if, and how, a certain exposure influences\nan outcome of interest through intermediate variables. This problem has\nrecently gained a surge of attention due to the tremendous need for such\nanalyses in scientific fields. Testing for the mediation effect is greatly\nchallenged by the fact that the underlying null hypothesis (i.e. the absence of\nmediation effects) is composite. Most existing mediation tests are overly\nconservative and thus underpowered. To overcome this significant methodological\nhurdle, we develop an adaptive bootstrap testing framework that can accommodate\ndifferent types of composite null hypotheses in the mediation pathway analysis.\nApplied to the product of coefficients (PoC) test and the joint significance\n(JS) test, our adaptive testing procedures provide type I error control under\nthe composite null, resulting in much improved statistical power compared to\nexisting tests. Both theoretical properties and numerical examples of the\nproposed methodology are discussed."}, "http://arxiv.org/abs/2310.11683": {"title": "Are we bootstrapping the right thing? A new approach to quantify uncertainty of Average Treatment Effect Estimate", "link": "http://arxiv.org/abs/2310.11683", "description": "Existing approaches of using the bootstrap method to derive standard error\nand confidence interval of average treatment effect estimate has one potential\nissue, which is that they are actually bootstrapping the wrong thing, resulting\nin unvalid statistical inference. In this paper, we discuss this important\nissue and propose a new non-parametric bootstrap method that can more precisely\nquantify the uncertainty associated with average treatment effect estimates. We\ndemonstrate the validity of this approach through a simulation study and a\nreal-world example, and highlight the importance of deriving standard error and\nconfidence interval of average treatment effect estimates that both remove\nextra undesired noise and are easy to interpret when applied in real world\nscenarios."}, "http://arxiv.org/abs/2310.11724": {"title": "Simultaneous Nonparametric Inference of M-regression under Complex Temporal Dynamics", "link": "http://arxiv.org/abs/2310.11724", "description": "The paper considers simultaneous nonparametric inference for a wide class of\nM-regression models with time-varying coefficients. The covariates and errors\nof the regression model are tackled as a general class of piece-wise locally\nstationary time series and are allowed to be cross-dependent. We introduce an\nintegration technique to study the M-estimators, whose limiting properties are\ndisclosed using Bahadur representation and Gaussian approximation theory.\nFacilitated by a self-convolved bootstrap proposed in this paper, we introduce\na unified framework to conduct general classes of Exact Function Tests,\nLack-of-fit Tests, and Qualitative Tests for the time-varying coefficient\nM-regression under complex temporal dynamics. As an application, our method is\napplied to studying the anthropogenic warming trend and time-varying structures\nof the ENSO effect using global climate data from 1882 to 2005."}, "http://arxiv.org/abs/2310.11741": {"title": "Graph Sphere: From Nodes to Supernodes in Graphical Models", "link": "http://arxiv.org/abs/2310.11741", "description": "High-dimensional data analysis typically focuses on low-dimensional\nstructure, often to aid interpretation and computational efficiency. Graphical\nmodels provide a powerful methodology for learning the conditional independence\nstructure in multivariate data by representing variables as nodes and\ndependencies as edges. Inference is often focused on individual edges in the\nlatent graph. Nonetheless, there is increasing interest in determining more\ncomplex structures, such as communities of nodes, for multiple reasons,\nincluding more effective information retrieval and better interpretability. In\nthis work, we propose a multilayer graphical model where we first cluster nodes\nand then, at the second layer, investigate the relationships among groups of\nnodes. Specifically, nodes are partitioned into \"supernodes\" with a\ndata-coherent size-biased tessellation prior which combines ideas from Bayesian\nnonparametrics and Voronoi tessellations. This construct allows accounting also\nfor dependence of nodes within supernodes. At the second layer, dependence\nstructure among supernodes is modelled through a Gaussian graphical model,\nwhere the focus of inference is on \"superedges\". We provide theoretical\njustification for our modelling choices. We design tailored Markov chain Monte\nCarlo schemes, which also enable parallel computations. We demonstrate the\neffectiveness of our approach for large-scale structure learning in simulations\nand a transcriptomics application."}, "http://arxiv.org/abs/2310.11779": {"title": "A Multivariate Skew-Normal-Tukey-h Distribution", "link": "http://arxiv.org/abs/2310.11779", "description": "We introduce a new family of multivariate distributions by taking the\ncomponent-wise Tukey-h transformation of a random vector following a\nskew-normal distribution. The proposed distribution is named the\nskew-normal-Tukey-h distribution and is an extension of the skew-normal\ndistribution for handling heavy-tailed data. We compare this proposed\ndistribution to the skew-t distribution, which is another extension of the\nskew-normal distribution for modeling tail-thickness, and demonstrate that when\nthere are substantial differences in marginal kurtosis, the proposed\ndistribution is more appropriate. Moreover, we derive many appealing stochastic\nproperties of the proposed distribution and provide a methodology for the\nestimation of the parameters in which the computational requirement increases\nlinearly with the dimension. Using simulations, as well as a wine and a wind\nspeed data application, we illustrate how to draw inferences based on the\nmultivariate skew-normal-Tukey-h distribution."}, "http://arxiv.org/abs/2310.11799": {"title": "Testing for patterns and structures in covariance and correlation matrices", "link": "http://arxiv.org/abs/2310.11799", "description": "Covariance matrices of random vectors contain information that is crucial for\nmodelling. Certain structures and patterns of the covariances (or correlations)\nmay be used to justify parametric models, e.g., autoregressive models. Until\nnow, there have been only few approaches for testing such covariance structures\nsystematically and in a unified way. In the present paper, we propose such a\nunified testing procedure, and we will exemplify the approach with a large\nvariety of covariance structure models. This includes common structures such as\ndiagonal matrices, Toeplitz matrices, and compound symmetry but also the more\ninvolved autoregressive matrices. We propose hypothesis tests for these\nstructures, and we use bootstrap techniques for better small-sample\napproximation. The structures of the proposed tests invite for adaptations to\nother covariance patterns by choosing the hypothesis matrix appropriately. We\nprove their correctness for large sample sizes. The proposed methods require\nonly weak assumptions.\n\nWith the help of a simulation study, we assess the small sample properties of\nthe tests.\n\nWe also analyze a real data set to illustrate the application of the\nprocedure."}, "http://arxiv.org/abs/2310.11822": {"title": "Post-clustering Inference under Dependency", "link": "http://arxiv.org/abs/2310.11822", "description": "Recent work by Gao et al. has laid the foundations for post-clustering\ninference. For the first time, the authors established a theoretical framework\nallowing to test for differences between means of estimated clusters.\nAdditionally, they studied the estimation of unknown parameters while\ncontrolling the selective type I error. However, their theory was developed for\nindependent observations identically distributed as $p$-dimensional Gaussian\nvariables with a spherical covariance matrix. Here, we aim at extending this\nframework to a more convenient scenario for practical applications, where\narbitrary dependence structures between observations and features are allowed.\nWe show that a $p$-value for post-clustering inference under general dependency\ncan be defined, and we assess the theoretical conditions allowing the\ncompatible estimation of a covariance matrix. The theory is developed for\nhierarchical agglomerative clustering algorithms with several types of\nlinkages, and for the $k$-means algorithm. We illustrate our method with\nsynthetic data and real data of protein structures."}, "http://arxiv.org/abs/2310.11969": {"title": "Survey calibration for causal inference: a simple method to balance covariate distributions", "link": "http://arxiv.org/abs/2310.11969", "description": "This paper proposes a simple method for balancing distributions of covariates\nfor causal inference based on observational studies. The method makes it\npossible to balance an arbitrary number of quantiles (e.g., medians, quartiles,\nor deciles) together with means if necessary. The proposed approach is based on\nthe theory of calibration estimators (Deville and S\\\"arndal 1992), in\nparticular, calibration estimators for quantiles, proposed by Harms and\nDuchesne (2006). By modifying the entropy balancing method and the covariate\nbalancing propensity score method, it is possible to balance the distributions\nof the treatment and control groups. The method does not require numerical\nintegration, kernel density estimation or assumptions about the distributions;\nvalid estimates can be obtained by drawing on existing asymptotic theory.\nResults of a simulation study indicate that the method efficiently estimates\naverage treatment effects on the treated (ATT), the average treatment effect\n(ATE), the quantile treatment effect on the treated (QTT) and the quantile\ntreatment effect (QTE), especially in the presence of non-linearity and\nmis-specification of the models. The proposed methods are implemented in an\nopen source R package jointCalib."}, "http://arxiv.org/abs/2310.12000": {"title": "Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models", "link": "http://arxiv.org/abs/2310.12000", "description": "Latent Gaussian process (GP) models are flexible probabilistic non-parametric\nfunction models. Vecchia approximations are accurate approximations for GPs to\novercome computational bottlenecks for large data, and the Laplace\napproximation is a fast method with asymptotic convergence guarantees to\napproximate marginal likelihoods and posterior predictive distributions for\nnon-Gaussian likelihoods. Unfortunately, the computational complexity of\ncombined Vecchia-Laplace approximations grows faster than linearly in the\nsample size when used in combination with direct solver methods such as the\nCholesky decomposition. Computations with Vecchia-Laplace approximations thus\nbecome prohibitively slow precisely when the approximations are usually the\nmost accurate, i.e., on large data sets. In this article, we present several\niterative methods for inference with Vecchia-Laplace approximations which make\ncomputations considerably faster compared to Cholesky-based calculations. We\nanalyze our proposed methods theoretically and in experiments with simulated\nand real-world data. In particular, we obtain a speed-up of an order of\nmagnitude compared to Cholesky-based inference and a threefold increase in\nprediction accuracy in terms of the continuous ranked probability score\ncompared to a state-of-the-art method on a large satellite data set. All\nmethods are implemented in a free C++ software library with high-level Python\nand R packages."}, "http://arxiv.org/abs/2310.12010": {"title": "A Note on Improving Variational Estimation for Multidimensional Item Response Theory", "link": "http://arxiv.org/abs/2310.12010", "description": "Survey instruments and assessments are frequently used in many domains of\nsocial science. When the constructs that these assessments try to measure\nbecome multifaceted, multidimensional item response theory (MIRT) provides a\nunified framework and convenient statistical tool for item analysis,\ncalibration, and scoring. However, the computational challenge of estimating\nMIRT models prohibits its wide use because many of the extant methods can\nhardly provide results in a realistic time frame when the number of dimensions,\nsample size, and test length are large. Instead, variational estimation\nmethods, such as Gaussian Variational Expectation Maximization (GVEM)\nalgorithm, have been recently proposed to solve the estimation challenge by\nproviding a fast and accurate solution. However, results have shown that\nvariational estimation methods may produce some bias on discrimination\nparameters during confirmatory model estimation, and this note proposes an\nimportance weighted version of GVEM (i.e., IW-GVEM) to correct for such bias\nunder MIRT models. We also use the adaptive moment estimation method to update\nthe learning rate for gradient descent automatically. Our simulations show that\nIW-GVEM can effectively correct bias with modest increase of computation time,\ncompared with GVEM. The proposed method may also shed light on improving the\nvariational estimation for other psychometrics models."}, "http://arxiv.org/abs/2310.12115": {"title": "MMD-based Variable Importance for Distributional Random Forest", "link": "http://arxiv.org/abs/2310.12115", "description": "Distributional Random Forest (DRF) is a flexible forest-based method to\nestimate the full conditional distribution of a multivariate output of interest\ngiven input variables. In this article, we introduce a variable importance\nalgorithm for DRFs, based on the well-established drop and relearn principle\nand MMD distance. While traditional importance measures only detect variables\nwith an influence on the output mean, our algorithm detects variables impacting\nthe output distribution more generally. We show that the introduced importance\nmeasure is consistent, exhibits high empirical performance on both real and\nsimulated data, and outperforms competitors. In particular, our algorithm is\nhighly efficient to select variables through recursive feature elimination, and\ncan therefore provide small sets of variables to build accurate estimates of\nconditional output distributions."}, "http://arxiv.org/abs/2310.12140": {"title": "Online Estimation with Rolling Validation: Adaptive Nonparametric Estimation with Stream Data", "link": "http://arxiv.org/abs/2310.12140", "description": "Online nonparametric estimators are gaining popularity due to their efficient\ncomputation and competitive generalization abilities. An important example\nincludes variants of stochastic gradient descent. These algorithms often take\none sample point at a time and instantly update the parameter estimate of\ninterest. In this work we consider model selection and hyperparameter tuning\nfor such online algorithms. We propose a weighted rolling-validation procedure,\nan online variant of leave-one-out cross-validation, that costs minimal extra\ncomputation for many typical stochastic gradient descent estimators. Similar to\nbatch cross-validation, it can boost base estimators to achieve a better,\nadaptive convergence rate. Our theoretical analysis is straightforward, relying\nmainly on some general statistical stability assumptions. The simulation study\nunderscores the significance of diverging weights in rolling validation in\npractice and demonstrates its sensitivity even when there is only a slim\ndifference between candidate estimators."}, "http://arxiv.org/abs/2010.02968": {"title": "Modelling of functional profiles and explainable shape shifts detection: An approach combining the notion of the Fr\\'echet mean with the shape invariant model", "link": "http://arxiv.org/abs/2010.02968", "description": "A modelling framework suitable for detecting shape shifts in functional\nprofiles combining the notion of Fr\\'echet mean and the concept of deformation\nmodels is developed and proposed. The generalized mean sense offered by the\nFr\\'echet mean notion is employed to capture the typical pattern of the\nprofiles under study, while the concept of deformation models, and in\nparticular of the shape invariant model, allows for interpretable\nparameterizations of profile's deviations from the typical shape. EWMA-type\ncontrol charts compatible with the functional nature of data and the employed\ndeformation model are built and proposed, exploiting certain shape\ncharacteristics of the profiles under study with respect to the generalized\nmean sense, allowing for the identification of potential shifts concerning the\nshape and/or the deformation process. Potential shifts in the shape deformation\nprocess, are further distinguished to significant shifts with respect to\namplitude and/or the phase of the profile under study. The proposed modelling\nand shift detection framework is implemented to a real world case study, where\ndaily concentration profiles concerning air pollutants from an area in the city\nof Athens are modelled, while profiles indicating hazardous concentration\nlevels are successfully identified in most of the cases."}, "http://arxiv.org/abs/2207.07218": {"title": "On the Selection of Tuning Parameters for Patch-Stitching Embedding Methods", "link": "http://arxiv.org/abs/2207.07218", "description": "While classical scaling, just like principal component analysis, is\nparameter-free, other methods for embedding multivariate data require the\nselection of one or several tuning parameters. This tuning can be difficult due\nto the unsupervised nature of the situation. We propose a simple, almost\nobvious, approach to supervise the choice of tuning parameter(s): minimize a\nnotion of stress. We apply this approach to the selection of the patch size in\na prototypical patch-stitching embedding method, both in the multidimensional\nscaling (aka network localization) setting and in the dimensionality reduction\n(aka manifold learning) setting. In our study, we uncover a new bias--variance\ntradeoff phenomenon."}, "http://arxiv.org/abs/2303.17856": {"title": "Bootstrapping multiple systems estimates to account for model selection", "link": "http://arxiv.org/abs/2303.17856", "description": "Multiple systems estimation using a Poisson loglinear model is a standard\napproach to quantifying hidden populations where data sources are based on\nlists of known cases. Information criteria are often used for selecting between\nthe large number of possible models. Confidence intervals are often reported\nconditional on the model selected, providing an over-optimistic impression of\nestimation accuracy. A bootstrap approach is a natural way to account for the\nmodel selection. However, because the model selection step has to be carried\nout for every bootstrap replication, there may be a high or even prohibitive\ncomputational burden. We explore the merit of modifying the model selection\nprocedure in the bootstrap to look only among a subset of models, chosen on the\nbasis of their information criterion score on the original data. This provides\nlarge computational gains with little apparent effect on inference. We also\nincorporate rigorous and economical ways of approaching issues of the existence\nof estimators when applying the method to sparse data tables."}, "http://arxiv.org/abs/2308.07319": {"title": "Partial identification for discrete data with nonignorable missing outcomes", "link": "http://arxiv.org/abs/2308.07319", "description": "Nonignorable missing outcomes are common in real world datasets and often\nrequire strong parametric assumptions to achieve identification. These\nassumptions can be implausible or untestable, and so we may forgo them in\nfavour of partially identified models that narrow the set of a priori possible\nvalues to an identification region. Here we propose a new nonparametric Bayes\nmethod that allows for the incorporation of multiple clinically relevant\nrestrictions of the parameter space simultaneously. We focus on two common\nrestrictions, instrumental variables and the direction of missing data bias,\nand investigate how these restrictions narrow the identification region for\nparameters of interest. Additionally, we propose a rejection sampling algorithm\nthat allows us to quantify the evidence for these assumptions in the data. We\ncompare our method to a standard Heckman selection model in both simulation\nstudies and in an applied problem examining the effectiveness of cash-transfers\nfor people experiencing homelessness."}}