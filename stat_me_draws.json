{"http://arxiv.org/abs/2310.03114": {"title": "Bayesian Parameter Inference for Partially Observed Stochastic Volterra Equations", "link": "http://arxiv.org/abs/2310.03114", "description": "In this article we consider Bayesian parameter inference for a type of\npartially observed stochastic Volterra equation (SVE). SVEs are found in many\nareas such as physics and mathematical finance. In the latter field they can be\nused to represent long memory in unobserved volatility processes. In many cases\nof practical interest, SVEs must be time-discretized and then parameter\ninference is based upon the posterior associated to this time-discretized\nprocess. Based upon recent studies on time-discretization of SVEs (e.g. Richard\net al. 2021), we use Euler-Maruyama methods for the afore-mentioned\ndiscretization. We then show how multilevel Markov chain Monte Carlo (MCMC)\nmethods (Jasra et al. 2018) can be applied in this context. In the examples we\nstudy, we give a proof that shows that the cost to achieve a mean square error\n(MSE) of $\\mathcal{O}(\\epsilon^2)$, $\\epsilon&gt;0$, is\n$\\mathcal{O}(\\epsilon^{-20/9})$. If one uses a single level MCMC method then\nthe cost is $\\mathcal{O}(\\epsilon^{-38/9})$ to achieve the same MSE. We\nillustrate these results in the context of state-space and stochastic\nvolatility models, with the latter applied to real data."}, "http://arxiv.org/abs/2310.03164": {"title": "A Hierarchical Random Effects State-space Model for Modeling Brain Activities from Electroencephalogram Data", "link": "http://arxiv.org/abs/2310.03164", "description": "Mental disorders present challenges in diagnosis and treatment due to their\ncomplex and heterogeneous nature. Electroencephalogram (EEG) has shown promise\nas a potential biomarker for these disorders. However, existing methods for\nanalyzing EEG signals have limitations in addressing heterogeneity and\ncapturing complex brain activity patterns between regions. This paper proposes\na novel random effects state-space model (RESSM) for analyzing large-scale\nmulti-channel resting-state EEG signals, accounting for the heterogeneity of\nbrain connectivities between groups and individual subjects. We incorporate\nmulti-level random effects for temporal dynamical and spatial mapping matrices\nand address nonstationarity so that the brain connectivity patterns can vary\nover time. The model is fitted under a Bayesian hierarchical model framework\ncoupled with a Gibbs sampler. Compared to previous mixed-effects state-space\nmodels, we directly model high-dimensional random effects matrices without\nstructural constraints and tackle the challenge of identifiability. Through\nextensive simulation studies, we demonstrate that our approach yields valid\nestimation and inference. We apply RESSM to a multi-site clinical trial of\nMajor Depressive Disorder (MDD). Our analysis uncovers significant differences\nin resting-state brain temporal dynamics among MDD patients compared to healthy\nindividuals. In addition, we show the subject-level EEG features derived from\nRESSM exhibit a superior predictive value for the heterogeneous treatment\neffect compared to the EEG frequency band power, suggesting the potential of\nEEG as a valuable biomarker for MDD."}, "http://arxiv.org/abs/2310.03258": {"title": "Detecting Electricity Service Equity Issues with Transfer Counterfactual Learning on Large-Scale Outage Datasets", "link": "http://arxiv.org/abs/2310.03258", "description": "Energy justice is a growing area of interest in interdisciplinary energy\nresearch. However, identifying systematic biases in the energy sector remains\nchallenging due to confounding variables, intricate heterogeneity in treatment\neffects, and limited data availability. To address these challenges, we\nintroduce a novel approach for counterfactual causal analysis centered on\nenergy justice. We use subgroup analysis to manage diverse factors and leverage\nthe idea of transfer learning to mitigate data scarcity in each subgroup. In\nour numerical analysis, we apply our method to a large-scale customer-level\npower outage data set and investigate the counterfactual effect of demographic\nfactors, such as income and age of the population, on power outage durations.\nOur results indicate that low-income and elderly-populated areas consistently\nexperience longer power outages, regardless of weather conditions. This points\nto existing biases in the power system and highlights the need for focused\nimprovements in areas with economic challenges."}, "http://arxiv.org/abs/2310.03351": {"title": "Efficiently analyzing large patient registries with Bayesian joint models for longitudinal and time-to-event data", "link": "http://arxiv.org/abs/2310.03351", "description": "The joint modeling of longitudinal and time-to-event outcomes has become a\npopular tool in follow-up studies. However, fitting Bayesian joint models to\nlarge datasets, such as patient registries, can require extended computing\ntimes. To speed up sampling, we divided a patient registry dataset into\nsubsamples, analyzed them in parallel, and combined the resulting Markov chain\nMonte Carlo draws into a consensus distribution. We used a simulation study to\ninvestigate how different consensus strategies perform with joint models. In\nparticular, we compared grouping all draws together with using equal- and\nprecision-weighted averages. We considered scenarios reflecting different\nsample sizes, numbers of data splits, and processor characteristics.\nParallelization of the sampling process substantially decreased the time\nrequired to run the model. We found that the weighted-average consensus\ndistributions for large sample sizes were nearly identical to the target\nposterior distribution. The proposed algorithm has been made available in an R\npackage for joint models, JMbayes2. This work was motivated by the clinical\ninterest in investigating the association between ppFEV1, a commonly measured\nmarker of lung function, and the risk of lung transplant or death, using data\nfrom the US Cystic Fibrosis Foundation Patient Registry (35,153 individuals\nwith 372,366 years of cumulative follow-up). Splitting the registry into five\nsubsamples resulted in an 85\\% decrease in computing time, from 9.22 to 1.39\nhours. Splitting the data and finding a consensus distribution by\nprecision-weighted averaging proved to be a computationally efficient and\nrobust approach to handling large datasets under the joint modeling framework."}, "http://arxiv.org/abs/2310.03521": {"title": "Cutting Feedback in Misspecified Copula Models", "link": "http://arxiv.org/abs/2310.03521", "description": "In copula models the marginal distributions and copula function are specified\nseparately. We treat these as two modules in a modular Bayesian inference\nframework, and propose conducting modified Bayesian inference by ``cutting\nfeedback''. Cutting feedback limits the influence of potentially misspecified\nmodules in posterior inference. We consider two types of cuts. The first limits\nthe influence of a misspecified copula on inference for the marginals, which is\na Bayesian analogue of the popular Inference for Margins (IFM) estimator. The\nsecond limits the influence of misspecified marginals on inference for the\ncopula parameters by using a rank likelihood to define the cut model. We\nestablish that if only one of the modules is misspecified, then the appropriate\ncut posterior gives accurate uncertainty quantification asymptotically for the\nparameters in the other module. Computation of the cut posteriors is difficult,\nand new variational inference methods to do so are proposed. The efficacy of\nthe new methodology is demonstrated using both simulated data and a substantive\nmultivariate time series application from macroeconomic forecasting. In the\nlatter, cutting feedback from misspecified marginals to a 1096 dimension copula\nimproves posterior inference and predictive accuracy greatly, compared to\nconventional Bayesian inference."}, "http://arxiv.org/abs/2310.03630": {"title": "Model-based Clustering for Network Data via a Latent Shrinkage Position Cluster Model", "link": "http://arxiv.org/abs/2310.03630", "description": "Low-dimensional representation and clustering of network data are tasks of\ngreat interest across various fields. Latent position models are routinely used\nfor this purpose by assuming that each node has a location in a low-dimensional\nlatent space, and enabling node clustering. However, these models fall short in\nsimultaneously determining the optimal latent space dimension and the number of\nclusters. Here we introduce the latent shrinkage position cluster model\n(LSPCM), which addresses this limitation. The LSPCM posits a Bayesian\nnonparametric shrinkage prior on the latent positions' variance parameters\nresulting in higher dimensions having increasingly smaller variances, aiding in\nthe identification of dimensions with non-negligible variance. Further, the\nLSPCM assumes the latent positions follow a sparse finite Gaussian mixture\nmodel, allowing for automatic inference on the number of clusters related to\nnon-empty mixture components. As a result, the LSPCM simultaneously infers the\nlatent space dimensionality and the number of clusters, eliminating the need to\nfit and compare multiple models. The performance of the LSPCM is assessed via\nsimulation studies and demonstrated through application to two real Twitter\nnetwork datasets from sporting and political contexts. Open source software is\navailable to promote widespread use of the LSPCM."}, "http://arxiv.org/abs/2310.03722": {"title": "Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance", "link": "http://arxiv.org/abs/2310.03722", "description": "In 1976, Lai constructed a nontrivial confidence sequence for the mean $\\mu$\nof a Gaussian distribution with unknown variance $\\sigma$. Curiously, he\nemployed both an improper (right Haar) mixture over $\\sigma$ and an improper\n(flat) mixture over $\\mu$. Here, we elaborate carefully on the details of his\nconstruction, which use generalized nonintegrable martingales and an extended\nVille's inequality. While this does yield a sequential t-test, it does not\nyield an ``e-process'' (due to the nonintegrability of his martingale). In this\npaper, we develop two new e-processes and confidence sequences for the same\nsetting: one is a test martingale in a reduced filtration, while the other is\nan e-process in the canonical data filtration. These are respectively obtained\nby swapping Lai's flat mixture for a Gaussian mixture, and swapping the right\nHaar mixture over $\\sigma$ with the maximum likelihood estimate under the null,\nas done in universal inference. We also analyze the width of resulting\nconfidence sequences, which have a curious dependence on the error probability\n$\\alpha$. Numerical experiments are provided along the way to compare and\ncontrast the various approaches."}, "http://arxiv.org/abs/2103.10875": {"title": "Scalable Bayesian computation for crossed and nested hierarchical models", "link": "http://arxiv.org/abs/2103.10875", "description": "We develop sampling algorithms to fit Bayesian hierarchical models, the\ncomputational complexity of which scales linearly with the number of\nobservations and the number of parameters in the model. We focus on crossed\nrandom effect and nested multilevel models, which are used ubiquitously in\napplied sciences. The posterior dependence in both classes is sparse: in\ncrossed random effects models it resembles a random graph, whereas in nested\nmultilevel models it is tree-structured. For each class we identify a framework\nfor scalable computation, building on previous work. Methods for crossed models\nare based on extensions of appropriately designed collapsed Gibbs samplers,\nwhere we introduce the idea of local centering; while methods for nested models\nare based on sparse linear algebra and data augmentation. We provide a\ntheoretical analysis of the proposed algorithms in some simplified settings,\nincluding a comparison with previously proposed methodologies and an\naverage-case analysis based on random graph theory. Numerical experiments,\nincluding two challenging real data analyses on predicting electoral results\nand real estate prices, compare with off-the-shelf Hamiltonian Monte Carlo,\ndisplaying drastic improvement in performance."}, "http://arxiv.org/abs/2106.04106": {"title": "A Regression-based Approach to Robust Estimation and Inference for Genetic Covariance", "link": "http://arxiv.org/abs/2106.04106", "description": "Genome-wide association studies (GWAS) have identified thousands of genetic\nvariants associated with complex traits, and some variants are shown to be\nassociated with multiple complex traits. Genetic covariance between two traits\nis defined as the underlying covariance of genetic effects and can be used to\nmeasure the shared genetic architecture. The data used to estimate such a\ngenetic covariance can be from the same group or different groups of\nindividuals, and the traits can be of different types or collected based on\ndifferent study designs. This paper proposes a unified regression-based\napproach to robust estimation and inference for genetic covariance of general\ntraits that may be associated with genetic variants nonlinearly. The asymptotic\nproperties of the proposed estimator are provided and are shown to be robust\nunder certain model mis-specification. Our method under linear working models\nprovides a robust inference for the narrow-sense genetic covariance, even when\nboth linear models are mis-specified. Numerical experiments are performed to\nsupport the theoretical results. Our method is applied to an outbred mice GWAS\ndata set to study the overlapping genetic effects between the behavioral and\nphysiological phenotypes. The real data results reveal interesting genetic\ncovariance among different mice developmental traits."}, "http://arxiv.org/abs/2112.08417": {"title": "Characterization of causal ancestral graphs for time series with latent confounders", "link": "http://arxiv.org/abs/2112.08417", "description": "In this paper, we introduce a novel class of graphical models for\nrepresenting time lag specific causal relationships and independencies of\nmultivariate time series with unobserved confounders. We completely\ncharacterize these graphs and show that they constitute proper subsets of the\ncurrently employed model classes. As we show, from the novel graphs one can\nthus draw stronger causal inferences -- without additional assumptions. We\nfurther introduce a graphical representation of Markov equivalence classes of\nthe novel graphs. This graphical representation contains more causal knowledge\nthan what current state-of-the-art causal discovery algorithms learn."}, "http://arxiv.org/abs/2112.09313": {"title": "Federated Adaptive Causal Estimation (FACE) of Target Treatment Effects", "link": "http://arxiv.org/abs/2112.09313", "description": "Federated learning of causal estimands may greatly improve estimation\nefficiency by leveraging data from multiple study sites, but robustness to\nheterogeneity and model misspecifications is vital for ensuring validity. We\ndevelop a Federated Adaptive Causal Estimation (FACE) framework to incorporate\nheterogeneous data from multiple sites to provide treatment effect estimation\nand inference for a flexibly specified target population of interest. FACE\naccounts for site-level heterogeneity in the distribution of covariates through\ndensity ratio weighting. To safely incorporate source sites and avoid negative\ntransfer, we introduce an adaptive weighting procedure via a penalized\nregression, which achieves both consistency and optimal efficiency. Our\nstrategy is communication-efficient and privacy-preserving, allowing\nparticipating sites to share summary statistics only once with other sites. We\nconduct both theoretical and numerical evaluations of FACE and apply it to\nconduct a comparative effectiveness study of BNT162b2 (Pfizer) and mRNA-1273\n(Moderna) vaccines on COVID-19 outcomes in U.S. veterans using electronic\nhealth records from five VA regional sites. We show that compared to\ntraditional methods, FACE meaningfully increases the precision of treatment\neffect estimates, with reductions in standard errors ranging from $26\\%$ to\n$67\\%$."}, "http://arxiv.org/abs/2208.03246": {"title": "Non-Asymptotic Analysis of Ensemble Kalman Updates: Effective Dimension and Localization", "link": "http://arxiv.org/abs/2208.03246", "description": "Many modern algorithms for inverse problems and data assimilation rely on\nensemble Kalman updates to blend prior predictions with observed data. Ensemble\nKalman methods often perform well with a small ensemble size, which is\nessential in applications where generating each particle is costly. This paper\ndevelops a non-asymptotic analysis of ensemble Kalman updates that rigorously\nexplains why a small ensemble size suffices if the prior covariance has\nmoderate effective dimension due to fast spectrum decay or approximate\nsparsity. We present our theory in a unified framework, comparing several\nimplementations of ensemble Kalman updates that use perturbed observations,\nsquare root filtering, and localization. As part of our analysis, we develop\nnew dimension-free covariance estimation bounds for approximately sparse\nmatrices that may be of independent interest."}, "http://arxiv.org/abs/2307.10972": {"title": "Adaptively Weighted Audits of Instant-Runoff Voting Elections: AWAIRE", "link": "http://arxiv.org/abs/2307.10972", "description": "An election audit is risk-limiting if the audit limits (to a pre-specified\nthreshold) the chance that an erroneous electoral outcome will be certified.\nExtant methods for auditing instant-runoff voting (IRV) elections are either\nnot risk-limiting or require cast vote records (CVRs), the voting system's\nelectronic record of the votes on each ballot. CVRs are not always available,\nfor instance, in jurisdictions that tabulate IRV contests manually.\n\nWe develop an RLA method (AWAIRE) that uses adaptively weighted averages of\ntest supermartingales to efficiently audit IRV elections when CVRs are not\navailable. The adaptive weighting 'learns' an efficient set of hypotheses to\ntest to confirm the election outcome. When accurate CVRs are available, AWAIRE\ncan use them to increase the efficiency to match the performance of existing\nmethods that require CVRs.\n\nWe provide an open-source prototype implementation that can handle elections\nwith up to six candidates. Simulations using data from real elections show that\nAWAIRE is likely to be efficient in practice. We discuss how to extend the\ncomputational approach to handle elections with more candidates.\n\nAdaptively weighted averages of test supermartingales are a general tool,\nuseful beyond election audits to test collections of hypotheses sequentially\nwhile rigorously controlling the familywise error rate."}, "http://arxiv.org/abs/2309.10514": {"title": "Partially Specified Causal Simulations", "link": "http://arxiv.org/abs/2309.10514", "description": "Simulation studies play a key role in the validation of causal inference\nmethods. The simulation results are reliable only if the study is designed\naccording to the promised operational conditions of the method-in-test. Still,\nmany causal inference literature tend to design over-restricted or misspecified\nstudies. In this paper, we elaborate on the problem of improper simulation\ndesign for causal methods and compile a list of desiderata for an effective\nsimulation framework. We then introduce partially randomized causal simulation\n(PARCS), a simulation framework that meets those desiderata. PARCS synthesizes\ndata based on graphical causal models and a wide range of adjustable\nparameters. There is a legible mapping from usual causal assumptions to the\nparameters, thus, users can identify and specify the subset of related\nparameters and randomize the remaining ones to generate a range of complying\ndata-generating processes for their causal method. The result is a more\ncomprehensive and inclusive empirical investigation for causal claims. Using\nPARCS, we reproduce and extend the simulation studies of two well-known causal\ndiscovery and missing data analysis papers to emphasize the necessity of a\nproper simulation design. Our results show that those papers would have\nimproved and extended the findings, had they used PARCS for simulation. The\nframework is implemented as a Python package, too. By discussing the\ncomprehensiveness and transparency of PARCS, we encourage causal inference\nresearchers to utilize it as a standard tool for future works."}, "http://arxiv.org/abs/2310.03776": {"title": "Significance of the negative binomial distribution in multiplicity phenomena", "link": "http://arxiv.org/abs/2310.03776", "description": "The negative binomial distribution (NBD) has been theorized to express a\nscale-invariant property of many-body systems and has been consistently shown\nto outperform other statistical models in both describing the multiplicity of\nquantum-scale events in particle collision experiments and predicting the\nprevalence of cosmological observables, such as the number of galaxies in a\nregion of space. Despite its widespread applicability and empirical success in\nthese contexts, a theoretical justification for the NBD from first principles\nhas remained elusive for fifty years. The accuracy of the NBD in modeling\nhadronic, leptonic, and semileptonic processes is suggestive of a highly\ngeneral principle, which is yet to be understood. This study demonstrates that\na statistical event of the NBD can in fact be derived in a general context via\nthe dynamical equations of a canonical ensemble of particles in Minkowski\nspace. These results describe a fundamental feature of many-body systems that\nis consistent with data from the ALICE and ATLAS experiments and provides an\nexplanation for the emergence of the NBD in these multiplicity observations.\nTwo methods are used to derive this correspondence: the Feynman path integral\nand a hypersurface parametrization of a propagating ensemble."}, "http://arxiv.org/abs/2310.04030": {"title": "Robust inference with GhostKnockoffs in genome-wide association studies", "link": "http://arxiv.org/abs/2310.04030", "description": "Genome-wide association studies (GWASs) have been extensively adopted to\ndepict the underlying genetic architecture of complex diseases. Motivated by\nGWASs' limitations in identifying small effect loci to understand complex\ntraits' polygenicity and fine-mapping putative causal variants from proxy ones,\nwe propose a knockoff-based method which only requires summary statistics from\nGWASs and demonstrate its validity in the presence of relatedness. We show that\nGhostKnockoffs inference is robust to its input Z-scores as long as they are\nfrom valid marginal association tests and their correlations are consistent\nwith the correlations among the corresponding genetic variants. The property\ngeneralizes GhostKnockoffs to other GWASs settings, such as the meta-analysis\nof multiple overlapping studies and studies based on association test\nstatistics deviated from score tests. We demonstrate GhostKnockoffs'\nperformance using empirical simulation and a meta-analysis of nine European\nancestral genome-wide association studies and whole exome/genome sequencing\nstudies. Both results demonstrate that GhostKnockoffs identify more putative\ncausal variants with weak genotype-phenotype associations that are missed by\nconventional GWASs."}, "http://arxiv.org/abs/2310.04082": {"title": "An energy-based model approach to rare event probability estimation", "link": "http://arxiv.org/abs/2310.04082", "description": "The estimation of rare event probabilities plays a pivotal role in diverse\nfields. Our aim is to determine the probability of a hazard or system failure\noccurring when a quantity of interest exceeds a critical value. In our\napproach, the distribution of the quantity of interest is represented by an\nenergy density, characterized by a free energy function. To efficiently\nestimate the free energy, a bias potential is introduced. Using concepts from\nenergy-based models (EBM), this bias potential is optimized such that the\ncorresponding probability density function approximates a pre-defined\ndistribution targeting the failure region of interest. Given the optimal bias\npotential, the free energy function and the rare event probability of interest\ncan be determined. The approach is applicable not just in traditional rare\nevent settings where the variable upon which the quantity of interest relies\nhas a known distribution, but also in inversion settings where the variable\nfollows a posterior distribution. By combining the EBM approach with a Stein\ndiscrepancy-based stopping criterion, we aim for a balanced accuracy-efficiency\ntrade-off. Furthermore, we explore both parametric and non-parametric\napproaches for the bias potential, with the latter eliminating the need for\nchoosing a particular parameterization, but depending strongly on the accuracy\nof the kernel density estimate used in the optimization process. Through three\nillustrative test cases encompassing both traditional and inversion settings,\nwe show that the proposed EBM approach, when properly configured, (i) allows\nstable and efficient estimation of rare event probabilities and (ii) compares\nfavorably against subset sampling approaches."}, "http://arxiv.org/abs/2310.04165": {"title": "When Composite Likelihood Meets Stochastic Approximation", "link": "http://arxiv.org/abs/2310.04165", "description": "A composite likelihood is an inference function derived by multiplying a set\nof likelihood components. This approach provides a flexible framework for\ndrawing inference when the likelihood function of a statistical model is\ncomputationally intractable. While composite likelihood has computational\nadvantages, it can still be demanding when dealing with numerous likelihood\ncomponents and a large sample size. This paper tackles this challenge by\nemploying an approximation of the conventional composite likelihood estimator,\nwhich is derived from an optimization procedure relying on stochastic\ngradients. This novel estimator is shown to be asymptotically normally\ndistributed around the true parameter. In particular, based on the relative\ndivergent rate of the sample size and the number of iterations of the\noptimization, the variance of the limiting distribution is shown to compound\nfor two sources of uncertainty: the sampling variability of the data and the\noptimization noise, with the latter depending on the sampling distribution used\nto construct the stochastic gradients. The advantages of the proposed framework\nare illustrated through simulation studies on two working examples: an Ising\nmodel for binary data and a gamma frailty model for count data. Finally, a\nreal-data application is presented, showing its effectiveness in a large-scale\nmental health survey."}, "http://arxiv.org/abs/1904.06340": {"title": "A Composite Likelihood-based Approach for Change-point Detection in Spatio-temporal Processes", "link": "http://arxiv.org/abs/1904.06340", "description": "This paper develops a unified and computationally efficient method for\nchange-point estimation along the time dimension in a non-stationary\nspatio-temporal process. By modeling a non-stationary spatio-temporal process\nas a piecewise stationary spatio-temporal process, we consider simultaneous\nestimation of the number and locations of change-points, and model parameters\nin each segment. A composite likelihood-based criterion is developed for\nchange-point and parameters estimation. Under the framework of increasing\ndomain asymptotics, theoretical results including consistency and distribution\nof the estimators are derived under mild conditions. In contrast to classical\nresults in fixed dimensional time series that the localization error of\nchange-point estimator is $O_{p}(1)$, exact recovery of true change-points can\nbe achieved in the spatio-temporal setting. More surprisingly, the consistency\nof change-point estimation can be achieved without any penalty term in the\ncriterion function. In addition, we further establish consistency of the number\nand locations of the change-point estimator under the infill asymptotics\nframework where the time domain is increasing while the spatial sampling domain\nis fixed. A computationally efficient pruned dynamic programming algorithm is\ndeveloped for the challenging criterion optimization problem. Extensive\nsimulation studies and an application to U.S. precipitation data are provided\nto demonstrate the effectiveness and practicality of the proposed method."}, "http://arxiv.org/abs/2201.12936": {"title": "Pigeonhole Design: Balancing Sequential Experiments from an Online Matching Perspective", "link": "http://arxiv.org/abs/2201.12936", "description": "Practitioners and academics have long appreciated the benefits of covariate\nbalancing when they conduct randomized experiments. For web-facing firms\nrunning online A/B tests, however, it still remains challenging in balancing\ncovariate information when experimental subjects arrive sequentially. In this\npaper, we study an online experimental design problem, which we refer to as the\n\"Online Blocking Problem.\" In this problem, experimental subjects with\nheterogeneous covariate information arrive sequentially and must be immediately\nassigned into either the control or the treated group. The objective is to\nminimize the total discrepancy, which is defined as the minimum weight perfect\nmatching between the two groups. To solve this problem, we propose a randomized\ndesign of experiment, which we refer to as the \"Pigeonhole Design.\" The\npigeonhole design first partitions the covariate space into smaller spaces,\nwhich we refer to as pigeonholes, and then, when the experimental subjects\narrive at each pigeonhole, balances the number of control and treated subjects\nfor each pigeonhole. We analyze the theoretical performance of the pigeonhole\ndesign and show its effectiveness by comparing against two well-known benchmark\ndesigns: the match-pair design and the completely randomized design. We\nidentify scenarios when the pigeonhole design demonstrates more benefits over\nthe benchmark design. To conclude, we conduct extensive simulations using\nYahoo! data to show a 10.2% reduction in variance if we use the pigeonhole\ndesign to estimate the average treatment effect."}, "http://arxiv.org/abs/2208.00137": {"title": "Efficient estimation and inference for the signed $\\beta$-model in directed signed networks", "link": "http://arxiv.org/abs/2208.00137", "description": "This paper proposes a novel signed $\\beta$-model for directed signed network,\nwhich is frequently encountered in application domains but largely neglected in\nliterature. The proposed signed $\\beta$-model decomposes a directed signed\nnetwork as the difference of two unsigned networks and embeds each node with\ntwo latent factors for in-status and out-status. The presence of negative edges\nleads to a non-concave log-likelihood, and a one-step estimation algorithm is\ndeveloped to facilitate parameter estimation, which is efficient both\ntheoretically and computationally. We also develop an inferential procedure for\npairwise and multiple node comparisons under the signed $\\beta$-model, which\nfills the void of lacking uncertainty quantification for node ranking.\nTheoretical results are established for the coverage probability of confidence\ninterval, as well as the false discovery rate (FDR) control for multiple node\ncomparison. The finite sample performance of the signed $\\beta$-model is also\nexamined through extensive numerical experiments on both synthetic and\nreal-life networks."}, "http://arxiv.org/abs/2208.08401": {"title": "Conformal Inference for Online Prediction with Arbitrary Distribution Shifts", "link": "http://arxiv.org/abs/2208.08401", "description": "We consider the problem of forming prediction sets in an online setting where\nthe distribution generating the data is allowed to vary over time. Previous\napproaches to this problem suffer from over-weighting historical data and thus\nmay fail to quickly react to the underlying dynamics. Here we correct this\nissue and develop a novel procedure with provably small regret over all local\ntime intervals of a given width. We achieve this by modifying the adaptive\nconformal inference (ACI) algorithm of Gibbs and Cand\\`{e}s (2021) to contain\nan additional step in which the step-size parameter of ACI's gradient descent\nupdate is tuned over time. Crucially, this means that unlike ACI, which\nrequires knowledge of the rate of change of the data-generating mechanism, our\nnew procedure is adaptive to both the size and type of the distribution shift.\nOur methods are highly flexible and can be used in combination with any\nbaseline predictive algorithm that produces point estimates or estimated\nquantiles of the target without the need for distributional assumptions. We\ntest our techniques on two real-world datasets aimed at predicting stock market\nvolatility and COVID-19 case counts and find that they are robust and adaptive\nto real-world distribution shifts."}, "http://arxiv.org/abs/2303.01031": {"title": "Identifiability and Consistent Estimation of the Gaussian Chain Graph Model", "link": "http://arxiv.org/abs/2303.01031", "description": "The chain graph model admits both undirected and directed edges in one graph,\nwhere symmetric conditional dependencies are encoded via undirected edges and\nasymmetric causal relations are encoded via directed edges. Though frequently\nencountered in practice, the chain graph model has been largely under\ninvestigated in literature, possibly due to the lack of identifiability\nconditions between undirected and directed edges. In this paper, we first\nestablish a set of novel identifiability conditions for the Gaussian chain\ngraph model, exploiting a low rank plus sparse decomposition of the precision\nmatrix. Further, an efficient learning algorithm is built upon the\nidentifiability conditions to fully recover the chain graph structure.\nTheoretical analysis on the proposed method is conducted, assuring its\nasymptotic consistency in recovering the exact chain graph structure. The\nadvantage of the proposed method is also supported by numerical experiments on\nboth simulated examples and a real application on the Standard &amp; Poor 500 index\ndata."}, "http://arxiv.org/abs/2305.10817": {"title": "Robust inference of causality in high-dimensional dynamical processes from the Information Imbalance of distance ranks", "link": "http://arxiv.org/abs/2305.10817", "description": "We introduce an approach which allows detecting causal relationships between\nvariables for which the time evolution is available. Causality is assessed by a\nvariational scheme based on the Information Imbalance of distance ranks, a\nstatistical test capable of inferring the relative information content of\ndifferent distance measures. We test whether the predictability of a putative\ndriven system Y can be improved by incorporating information from a potential\ndriver system X, without making assumptions on the underlying dynamics and\nwithout the need to compute probability densities of the dynamic variables.\nThis framework makes causality detection possible even for high-dimensional\nsystems where only few of the variables are known or measured. Benchmark tests\non coupled chaotic dynamical systems demonstrate that our approach outperforms\nother model-free causality detection methods, successfully handling both\nunidirectional and bidirectional couplings. We also show that the method can be\nused to robustly detect causality in human electroencephalography data."}, "http://arxiv.org/abs/2309.06264": {"title": "Spectral clustering algorithm for the allometric extension model", "link": "http://arxiv.org/abs/2309.06264", "description": "The spectral clustering algorithm is often used as a binary clustering method\nfor unclassified data by applying the principal component analysis. To study\ntheoretical properties of the algorithm, the assumption of conditional\nhomoscedasticity is often supposed in existing studies. However, this\nassumption is restrictive and often unrealistic in practice. Therefore, in this\npaper, we consider the allometric extension model, that is, the directions of\nthe first eigenvectors of two covariance matrices and the direction of the\ndifference of two mean vectors coincide, and we provide a non-asymptotic bound\nof the error probability of the spectral clustering algorithm for the\nallometric extension model. As a byproduct of the result, we obtain the\nconsistency of the clustering method in high-dimensional settings."}, "http://arxiv.org/abs/2309.12833": {"title": "Model-based causal feature selection for general response types", "link": "http://arxiv.org/abs/2309.12833", "description": "Discovering causal relationships from observational data is a fundamental yet\nchallenging task. Invariant causal prediction (ICP, Peters et al., 2016) is a\nmethod for causal feature selection which requires data from heterogeneous\nsettings and exploits that causal models are invariant. ICP has been extended\nto general additive noise models and to nonparametric settings using\nconditional independence tests. However, the latter often suffer from low power\n(or poor type I error control) and additive noise models are not suitable for\napplications in which the response is not measured on a continuous scale, but\nreflects categories or counts. Here, we develop transformation-model (TRAM)\nbased ICP, allowing for continuous, categorical, count-type, and\nuninformatively censored responses (these model classes, generally, do not\nallow for identifiability when there is no exogenous heterogeneity). As an\ninvariance test, we propose TRAM-GCM based on the expected conditional\ncovariance between environments and score residuals with uniform asymptotic\nlevel guarantees. For the special case of linear shift TRAMs, we also consider\nTRAM-Wald, which tests invariance based on the Wald statistic. We provide an\nopen-source R package 'tramicp' and evaluate our approach on simulated data and\nin a case study investigating causal features of survival in critically ill\npatients."}, "http://arxiv.org/abs/2310.04452": {"title": "Short text classification with machine learning in the social sciences: The case of climate change on Twitter", "link": "http://arxiv.org/abs/2310.04452", "description": "To analyse large numbers of texts, social science researchers are\nincreasingly confronting the challenge of text classification. When manual\nlabeling is not possible and researchers have to find automatized ways to\nclassify texts, computer science provides a useful toolbox of machine-learning\nmethods whose performance remains understudied in the social sciences. In this\narticle, we compare the performance of the most widely used text classifiers by\napplying them to a typical research scenario in social science research: a\nrelatively small labeled dataset with infrequent occurrence of categories of\ninterest, which is a part of a large unlabeled dataset. As an example case, we\nlook at Twitter communication regarding climate change, a topic of increasing\nscholarly interest in interdisciplinary social science research. Using a novel\ndataset including 5,750 tweets from various international organizations\nregarding the highly ambiguous concept of climate change, we evaluate the\nperformance of methods in automatically classifying tweets based on whether\nthey are about climate change or not. In this context, we highlight two main\nfindings. First, supervised machine-learning methods perform better than\nstate-of-the-art lexicons, in particular as class balance increases. Second,\ntraditional machine-learning methods, such as logistic regression and random\nforest, perform similarly to sophisticated deep-learning methods, whilst\nrequiring much less training time and computational resources. The results have\nimportant implications for the analysis of short texts in social science\nresearch."}, "http://arxiv.org/abs/2310.04563": {"title": "Modeling the Risk of In-Person Instruction during the COVID-19 Pandemic", "link": "http://arxiv.org/abs/2310.04563", "description": "During the COVID-19 pandemic, implementing in-person indoor instruction in a\nsafe manner was a high priority for universities nationwide. To support this\neffort at the University, we developed a mathematical model for estimating the\nrisk of SARS-CoV-2 transmission in university classrooms. This model was used\nto design a safe classroom environment at the University during the COVID-19\npandemic that supported the higher occupancy levels needed to match\npre-pandemic numbers of in-person courses, despite a limited number of large\nclassrooms. A retrospective analysis at the end of the semester confirmed the\nmodel's assessment that the proposed classroom configuration would be safe. Our\nframework is generalizable and was also used to support reopening decisions at\nStanford University. In addition, our methods are flexible; our modeling\nframework was repurposed to plan for large university events and gatherings. We\nfound that our approach and methods work in a wide range of indoor settings and\ncould be used to support reopening planning across various industries, from\nsecondary schools to movie theaters and restaurants."}, "http://arxiv.org/abs/2310.04578": {"title": "TNDDR: Efficient and doubly robust estimation of COVID-19 vaccine effectiveness under the test-negative design", "link": "http://arxiv.org/abs/2310.04578", "description": "While the test-negative design (TND), which is routinely used for monitoring\nseasonal flu vaccine effectiveness (VE), has recently become integral to\nCOVID-19 vaccine surveillance, it is susceptible to selection bias due to\noutcome-dependent sampling. Some studies have addressed the identifiability and\nestimation of causal parameters under the TND, but efficiency bounds for\nnonparametric estimators of the target parameter under the unconfoundedness\nassumption have not yet been investigated. We propose a one-step doubly robust\nand locally efficient estimator called TNDDR (TND doubly robust), which\nutilizes sample splitting and can incorporate machine learning techniques to\nestimate the nuisance functions. We derive the efficient influence function\n(EIF) for the marginal expectation of the outcome under a vaccination\nintervention, explore the von Mises expansion, and establish the conditions for\n$\\sqrt{n}-$consistency, asymptotic normality and double robustness of TNDDR.\nThe proposed TNDDR is supported by both theoretical and empirical\njustifications, and we apply it to estimate COVID-19 VE in an administrative\ndataset of community-dwelling older people (aged $\\geq 60$y) in the province of\nQu\\'ebec, Canada."}, "http://arxiv.org/abs/2310.04660": {"title": "Balancing Weights for Causal Inference in Observational Factorial Studies", "link": "http://arxiv.org/abs/2310.04660", "description": "Many scientific questions in biomedical, environmental, and psychological\nresearch involve understanding the impact of multiple factors on outcomes.\nWhile randomized factorial experiments are ideal for this purpose,\nrandomization is infeasible in many empirical studies. Therefore, investigators\noften rely on observational data, where drawing reliable causal inferences for\nmultiple factors remains challenging. As the number of treatment combinations\ngrows exponentially with the number of factors, some treatment combinations can\nbe rare or even missing by chance in observed data, further complicating\nfactorial effects estimation. To address these challenges, we propose a novel\nweighting method tailored to observational studies with multiple factors. Our\napproach uses weighted observational data to emulate a randomized factorial\nexperiment, enabling simultaneous estimation of the effects of multiple factors\nand their interactions. Our investigations reveal a crucial nuance: achieving\nbalance among covariates, as in single-factor scenarios, is necessary but\ninsufficient for unbiasedly estimating factorial effects. Our findings suggest\nthat balancing the factors is also essential in multi-factor settings.\nMoreover, we extend our weighting method to handle missing treatment\ncombinations in observed data. Finally, we study the asymptotic behavior of the\nnew weighting estimators and propose a consistent variance estimator, providing\nreliable inferences on factorial effects in observational studies."}, "http://arxiv.org/abs/2310.04709": {"title": "Time-dependent mediators in survival analysis: Graphical representation of causal assumptions", "link": "http://arxiv.org/abs/2310.04709", "description": "We study time-dependent mediators in survival analysis using a treatment\nseparation approach due to Didelez [2019] and based on earlier work by Robins\nand Richardson [2011]. This approach avoids nested counterfactuals and\ncrossworld assumptions which are otherwise common in mediation analysis. The\ncausal model of treatment, mediators, covariates, confounders and outcome is\nrepresented by causal directed acyclic graphs (DAGs). However, the DAGs tend to\nbe very complex when we have measurements at a large number of time points. We\ntherefore suggest using so-called rolled graphs in which a node represents an\nentire coordinate process instead of a single random variable, leading us to\nfar simpler graphical representations. The rolled graphs are not necessarily\nacyclic; they can be analyzed by $\\delta$-separation which is the appropriate\ngraphical separation criterion in this class of graphs and analogous to\n$d$-separation. In particular, $\\delta$-separation is a graphical tool for\nevaluating if the conditions of the mediation analysis are met or if unmeasured\nconfounders influence the estimated effects. We also state a mediational\ng-formula. This is similar to the approach in Vansteelandt et al. [2019]\nalthough that paper has a different conceptual basis. Finally, we apply this\nframework to a statistical model based on a Cox model with an added treatment\neffect.survival analysis; mediation; causal inference; graphical models; local\nindependence graphs"}, "http://arxiv.org/abs/2310.04853": {"title": "On changepoint detection in functional data using empirical energy distance", "link": "http://arxiv.org/abs/2310.04853", "description": "We propose a novel family of test statistics to detect the presence of\nchangepoints in a sequence of dependent, possibly multivariate,\nfunctional-valued observations. Our approach allows to test for a very general\nclass of changepoints, including the \"classical\" case of changes in the mean,\nand even changes in the whole distribution. Our statistics are based on a\ngeneralisation of the empirical energy distance; we propose weighted\nfunctionals of the energy distance process, which are designed in order to\nenhance the ability to detect breaks occurring at sample endpoints. The\nlimiting distribution of the maximally selected version of our statistics\nrequires only the computation of the eigenvalues of the covariance function,\nthus being readily implementable in the most commonly employed packages, e.g.\nR. We show that, under the alternative, our statistics are able to detect\nchangepoints occurring even very close to the beginning/end of the sample. In\nthe presence of multiple changepoints, we propose a binary segmentation\nalgorithm to estimate the number of breaks and the locations thereof.\nSimulations show that our procedures work very well in finite samples. We\ncomplement our theory with applications to financial and temperature data."}, "http://arxiv.org/abs/2310.04919": {"title": "The Conditional Prediction Function: A Novel Technique to Control False Discovery Rate for Complex Models", "link": "http://arxiv.org/abs/2310.04919", "description": "In modern scientific research, the objective is often to identify which\nvariables are associated with an outcome among a large class of potential\npredictors. This goal can be achieved by selecting variables in a manner that\ncontrols the the false discovery rate (FDR), the proportion of irrelevant\npredictors among the selections. Knockoff filtering is a cutting-edge approach\nto variable selection that provides FDR control. Existing knockoff statistics\nfrequently employ linear models to assess relationships between features and\nthe response, but the linearity assumption is often violated in real world\napplications. This may result in poor power to detect truly prognostic\nvariables. We introduce a knockoff statistic based on the conditional\nprediction function (CPF), which can pair with state-of-art machine learning\npredictive models, such as deep neural networks. The CPF statistics can capture\nthe nonlinear relationships between predictors and outcomes while also\naccounting for correlation between features. We illustrate the capability of\nthe CPF statistics to provide superior power over common knockoff statistics\nwith continuous, categorical, and survival outcomes using repeated simulations.\nKnockoff filtering with the CPF statistics is demonstrated using (1) a\nresidential building dataset to select predictors for the actual sales prices\nand (2) the TCGA dataset to select genes that are correlated with disease\nstaging in lung cancer patients."}, "http://arxiv.org/abs/2310.04924": {"title": "Markov Chain Monte Carlo Significance Tests", "link": "http://arxiv.org/abs/2310.04924", "description": "Markov chain Monte Carlo significance tests were first introduced by Besag\nand Clifford in [4]. These methods produce statistical valid p-values in\nproblems where sampling from the null hypotheses is intractable. We give an\noverview of the methods of Besag and Clifford and some recent developments. A\nrange of examples and applications are discussed."}, "http://arxiv.org/abs/2310.04934": {"title": "UBSea: A Unified Community Detection Framework", "link": "http://arxiv.org/abs/2310.04934", "description": "Detecting communities in networks and graphs is an important task across many\ndisciplines such as statistics, social science and engineering. There are\ngenerally three different kinds of mixing patterns for the case of two\ncommunities: assortative mixing, disassortative mixing and core-periphery\nstructure. Modularity optimization is a classical way for fitting network\nmodels with communities. However, it can only deal with assortative mixing and\ndisassortative mixing when the mixing pattern is known and fails to discover\nthe core-periphery structure. In this paper, we extend modularity in a\nstrategic way and propose a new framework based on Unified Bigroups Standadized\nEdge-count Analysis (UBSea). It can address all the formerly mentioned\ncommunity mixing structures. In addition, this new framework is able to\nautomatically choose the mixing type to fit the networks. Simulation studies\nshow that the new framework has superb performance in a wide range of settings\nunder the stochastic block model and the degree-corrected stochastic block\nmodel. We show that the new approach produces consistent estimate of the\ncommunities under a suitable signal-to-noise-ratio condition, for the case of a\nblock model with two communities, for both undirected and directed networks.\nThe new method is illustrated through applications to several real-world\ndatasets."}, "http://arxiv.org/abs/2310.05049": {"title": "On Estimation of Optimal Dynamic Treatment Regimes with Multiple Treatments for Survival Data-With Application to Colorectal Cancer Study", "link": "http://arxiv.org/abs/2310.05049", "description": "Dynamic treatment regimes (DTR) are sequential decision rules corresponding\nto several stages of intervention. Each rule maps patients' covariates to\noptional treatments. The optimal dynamic treatment regime is the one that\nmaximizes the mean outcome of interest if followed by the overall population.\nMotivated by a clinical study on advanced colorectal cancer with traditional\nChinese medicine, we propose a censored C-learning (CC-learning) method to\nestimate the dynamic treatment regime with multiple treatments using survival\ndata. To address the challenges of multiple stages with right censoring, we\nmodify the backward recursion algorithm in order to adapt to the flexible\nnumber and timing of treatments. For handling the problem of multiple\ntreatments, we propose a framework from the classification perspective by\ntransferring the problem of optimization with multiple treatment comparisons\ninto an example-dependent cost-sensitive classification problem. With\nclassification and regression tree (CART) as the classifier, the CC-learning\nmethod can produce an estimated optimal DTR with good interpretability. We\ntheoretically prove the optimality of our method and numerically evaluate its\nfinite sample performances through simulation. With the proposed method, we\nidentify the interpretable tree treatment regimes at each stage for the\nadvanced colorectal cancer treatment data from Xiyuan Hospital."}, "http://arxiv.org/abs/2310.05151": {"title": "Sequential linear regression for conditional mean imputation of longitudinal continuous outcomes under reference-based assumptions", "link": "http://arxiv.org/abs/2310.05151", "description": "In clinical trials of longitudinal continuous outcomes, reference based\nimputation (RBI) has commonly been applied to handle missing outcome data in\nsettings where the estimand incorporates the effects of intercurrent events,\ne.g. treatment discontinuation. RBI was originally developed in the multiple\nimputation framework, however recently conditional mean imputation (CMI)\ncombined with the jackknife estimator of the standard error was proposed as a\nway to obtain deterministic treatment effect estimates and correct frequentist\ninference. For both multiple and CMI, a mixed model for repeated measures\n(MMRM) is often used for the imputation model, but this can be computationally\nintensive to fit to multiple data sets (e.g. the jackknife samples) and lead to\nconvergence issues with complex MMRM models with many parameters. Therefore, a\nstep-wise approach based on sequential linear regression (SLR) of the outcomes\nat each visit was developed for the imputation model in the multiple imputation\nframework, but similar developments in the CMI framework are lacking. In this\narticle, we fill this gap in the literature by proposing a SLR approach to\nimplement RBI in the CMI framework, and justify its validity using theoretical\nresults and simulations. We also illustrate our proposal on a real data\napplication."}, "http://arxiv.org/abs/2310.05398": {"title": "Statistical Inference for Modulation Index in Phase-Amplitude Coupling", "link": "http://arxiv.org/abs/2310.05398", "description": "Phase-amplitude coupling is a phenomenon observed in several neurological\nprocesses, where the phase of one signal modulates the amplitude of another\nsignal with a distinct frequency. The modulation index (MI) is a common\ntechnique used to quantify this interaction by assessing the Kullback-Leibler\ndivergence between a uniform distribution and the empirical conditional\ndistribution of amplitudes with respect to the phases of the observed signals.\nThe uniform distribution is an ideal representation that is expected to appear\nunder the absence of coupling. However, it does not reflect the statistical\nproperties of coupling values caused by random chance. In this paper, we\npropose a statistical framework for evaluating the significance of an observed\nMI value based on a null hypothesis that a MI value can be entirely explained\nby chance. Significance is obtained by comparing the value with a reference\ndistribution derived under the null hypothesis of independence (i.e., no\ncoupling) between signals. We derived a closed-form distribution of this null\nmodel, resulting in a scaled beta distribution. To validate the efficacy of our\nproposed framework, we conducted comprehensive Monte Carlo simulations,\nassessing the significance of MI values under various experimental scenarios,\nincluding amplitude modulation, trains of spikes, and sequences of\nhigh-frequency oscillations. Furthermore, we corroborated the reliability of\nour model by comparing its statistical significance thresholds with reported\nvalues from other research studies conducted under different experimental\nsettings. Our method offers several advantages such as meta-analysis\nreliability, simplicity and computational efficiency, as it provides p-values\nand significance levels without resorting to generating surrogate data through\nsampling procedures."}, "http://arxiv.org/abs/2310.05526": {"title": "Projecting infinite time series graphs to finite marginal graphs using number theory", "link": "http://arxiv.org/abs/2310.05526", "description": "In recent years, a growing number of method and application works have\nadapted and applied the causal-graphical-model framework to time series data.\nMany of these works employ time-resolved causal graphs that extend infinitely\ninto the past and future and whose edges are repetitive in time, thereby\nreflecting the assumption of stationary causal relationships. However, most\nresults and algorithms from the causal-graphical-model framework are not\ndesigned for infinite graphs. In this work, we develop a method for projecting\ninfinite time series graphs with repetitive edges to marginal graphical models\non a finite time window. These finite marginal graphs provide the answers to\n$m$-separation queries with respect to the infinite graph, a task that was\npreviously unresolved. Moreover, we argue that these marginal graphs are useful\nfor causal discovery and causal effect estimation in time series, effectively\nenabling to apply results developed for finite graphs to the infinite graphs.\nThe projection procedure relies on finding common ancestors in the\nto-be-projected graph and is, by itself, not new. However, the projection\nprocedure has not yet been algorithmically implemented for time series graphs\nsince in these infinite graphs there can be infinite sets of paths that might\ngive rise to common ancestors. We solve the search over these possibly infinite\nsets of paths by an intriguing combination of path-finding techniques for\nfinite directed graphs and solution theory for linear Diophantine equations. By\nproviding an algorithm that carries out the projection, our paper makes an\nimportant step towards a theoretically-grounded and method-agnostic\ngeneralization of a range of causal inference methods and results to time\nseries."}, "http://arxiv.org/abs/2310.05539": {"title": "Testing High-Dimensional Mediation Effect with Arbitrary Exposure-Mediator Coefficients", "link": "http://arxiv.org/abs/2310.05539", "description": "In response to the unique challenge created by high-dimensional mediators in\nmediation analysis, this paper presents a novel procedure for testing the\nnullity of the mediation effect in the presence of high-dimensional mediators.\nThe procedure incorporates two distinct features. Firstly, the test remains\nvalid under all cases of the composite null hypothesis, including the\nchallenging scenario where both exposure-mediator and mediator-outcome\ncoefficients are zero. Secondly, it does not impose structural assumptions on\nthe exposure-mediator coefficients, thereby allowing for an arbitrarily strong\nexposure-mediator relationship. To the best of our knowledge, the proposed test\nis the first of its kind to provably possess these two features in\nhigh-dimensional mediation analysis. The validity and consistency of the\nproposed test are established, and its numerical performance is showcased\nthrough simulation studies. The application of the proposed test is\ndemonstrated by examining the mediation effect of DNA methylation between\nsmoking status and lung cancer development."}, "http://arxiv.org/abs/2310.05548": {"title": "Cokrig-and-Regress for Spatially Misaligned Environmental Data", "link": "http://arxiv.org/abs/2310.05548", "description": "Spatially misaligned data, where the response and covariates are observed at\ndifferent spatial locations, commonly arise in many environmental studies. Much\nof the statistical literature on handling spatially misaligned data has been\ndevoted to the case of a single covariate and a linear relationship between the\nresponse and this covariate. Motivated by spatially misaligned data collected\non air pollution and weather in China, we propose a cokrig-and-regress (CNR)\nmethod to estimate spatial regression models involving multiple covariates and\npotentially non-linear associations. The CNR estimator is constructed by\nreplacing the unobserved covariates (at the response locations) by their\ncokriging predictor derived from the observed but misaligned covariates under a\nmultivariate Gaussian assumption, where a generalized Kronecker product\ncovariance is used to account for spatial correlations within and between\ncovariates. A parametric bootstrap approach is employed to bias-correct the CNR\nestimates of the spatial covariance parameters and for uncertainty\nquantification. Simulation studies demonstrate that CNR outperforms several\nexisting methods for handling spatially misaligned data, such as\nnearest-neighbor interpolation. Applying CNR to the spatially misaligned air\npollution and weather data in China reveals a number of non-linear\nrelationships between PM$_{2.5}$ concentration and several meteorological\ncovariates."}, "http://arxiv.org/abs/2310.05622": {"title": "A neutral comparison of statistical methods for time-to-event analyses under non-proportional hazards", "link": "http://arxiv.org/abs/2310.05622", "description": "While well-established methods for time-to-event data are available when the\nproportional hazards assumption holds, there is no consensus on the best\ninferential approach under non-proportional hazards (NPH). However, a wide\nrange of parametric and non-parametric methods for testing and estimation in\nthis scenario have been proposed. To provide recommendations on the statistical\nanalysis of clinical trials where non proportional hazards are expected, we\nconducted a comprehensive simulation study under different scenarios of\nnon-proportional hazards, including delayed onset of treatment effect, crossing\nhazard curves, subgroups with different treatment effect and changing hazards\nafter disease progression. We assessed type I error rate control, power and\nconfidence interval coverage, where applicable, for a wide range of methods\nincluding weighted log-rank tests, the MaxCombo test, summary measures such as\nthe restricted mean survival time (RMST), average hazard ratios, and milestone\nsurvival probabilities as well as accelerated failure time regression models.\nWe found a trade-off between interpretability and power when choosing an\nanalysis strategy under NPH scenarios. While analysis methods based on weighted\nlogrank tests typically were favorable in terms of power, they do not provide\nan easily interpretable treatment effect estimate. Also, depending on the\nweight function, they test a narrow null hypothesis of equal hazard functions\nand rejection of this null hypothesis may not allow for a direct conclusion of\ntreatment benefit in terms of the survival function. In contrast,\nnon-parametric procedures based on well interpretable measures as the RMST\ndifference had lower power in most scenarios. Model based methods based on\nspecific survival distributions had larger power, however often gave biased\nestimates and lower than nominal confidence interval coverage."}, "http://arxiv.org/abs/2310.05646": {"title": "Transfer learning for piecewise-constant mean estimation: Optimality, $\\ell_1$- and $\\ell_0$-penalisation", "link": "http://arxiv.org/abs/2310.05646", "description": "We study transfer learning in the context of estimating piecewise-constant\nsignals when source data, which may be relevant but disparate, are available in\naddition to the target data. We initially investigate transfer learning\nestimators that respectively employ $\\ell_1$- and $\\ell_0$-penalties for\nunisource data scenarios and then generalise these estimators to accommodate\nmultisource data. To further reduce estimation errors, especially in scenarios\nwhere some sources significantly differ from the target, we introduce an\ninformative source selection algorithm. We then examine these estimators with\nmultisource selection and establish their minimax optimality under specific\nregularity conditions. It is worth emphasising that, unlike the prevalent\nnarrative in the transfer learning literature that the performance is enhanced\nthrough large source sample sizes, our approaches leverage higher observation\nfrequencies and accommodate diverse frequencies across multiple sources. Our\ntheoretical findings are empirically validated through extensive numerical\nexperiments, with the code available online, see\nhttps://github.com/chrisfanwang/transferlearning"}, "http://arxiv.org/abs/2310.05685": {"title": "Post-Selection Inference for Sparse Estimation", "link": "http://arxiv.org/abs/2310.05685", "description": "When the model is not known and parameter testing or interval estimation is\nconducted after model selection, it is necessary to consider selective\ninference. This paper discusses this issue in the context of sparse estimation.\nFirstly, we describe selective inference related to Lasso as per \\cite{lee},\nand then present polyhedra and truncated distributions when applying it to\nmethods such as Forward Stepwise and LARS. Lastly, we discuss the Significance\nTest for Lasso by \\cite{significant} and the Spacing Test for LARS by\n\\cite{ryan_exact}. This paper serves as a review article.\n\nKeywords: post-selective inference, polyhedron, LARS, lasso, forward\nstepwise, significance test, spacing test."}, "http://arxiv.org/abs/2310.05921": {"title": "Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions", "link": "http://arxiv.org/abs/2310.05921", "description": "We introduce Conformal Decision Theory, a framework for producing safe\nautonomous decisions despite imperfect machine learning predictions. Examples\nof such decisions are ubiquitous, from robot planning algorithms that rely on\npedestrian predictions, to calibrating autonomous manufacturing to exhibit high\nthroughput and low error, to the choice of trusting a nominal policy versus\nswitching to a safe backup policy at run-time. The decisions produced by our\nalgorithms are safe in the sense that they come with provable statistical\nguarantees of having low risk without any assumptions on the world model\nwhatsoever; the observations need not be I.I.D. and can even be adversarial.\nThe theory extends results from conformal prediction to calibrate decisions\ndirectly, without requiring the construction of prediction sets. Experiments\ndemonstrate the utility of our approach in robot motion planning around humans,\nautomated stock trading, and robot manufacturin"}, "http://arxiv.org/abs/2101.06950": {"title": "Learning and scoring Gaussian latent variable causal models with unknown additive interventions", "link": "http://arxiv.org/abs/2101.06950", "description": "With observational data alone, causal structure learning is a challenging\nproblem. The task becomes easier when having access to data collected from\nperturbations of the underlying system, even when the nature of these is\nunknown. Existing methods either do not allow for the presence of latent\nvariables or assume that these remain unperturbed. However, these assumptions\nare hard to justify if the nature of the perturbations is unknown. We provide\nresults that enable scoring causal structures in the setting with additive, but\nunknown interventions. Specifically, we propose a maximum-likelihood estimator\nin a structural equation model that exploits system-wide invariances to output\nan equivalence class of causal structures from perturbation data. Furthermore,\nunder certain structural assumptions on the population model, we provide a\nsimple graphical characterization of all the DAGs in the interventional\nequivalence class. We illustrate the utility of our framework on synthetic data\nas well as real data involving California reservoirs and protein expressions.\nThe software implementation is available as the Python package \\emph{utlvce}."}, "http://arxiv.org/abs/2107.14151": {"title": "Modern Non-Linear Function-on-Function Regression", "link": "http://arxiv.org/abs/2107.14151", "description": "We introduce a new class of non-linear function-on-function regression models\nfor functional data using neural networks. We propose a framework using a\nhidden layer consisting of continuous neurons, called a continuous hidden\nlayer, for functional response modeling and give two model fitting strategies,\nFunctional Direct Neural Network (FDNN) and Functional Basis Neural Network\n(FBNN). Both are designed explicitly to exploit the structure inherent in\nfunctional data and capture the complex relations existing between the\nfunctional predictors and the functional response. We fit these models by\nderiving functional gradients and implement regularization techniques for more\nparsimonious results. We demonstrate the power and flexibility of our proposed\nmethod in handling complex functional models through extensive simulation\nstudies as well as real data examples."}, "http://arxiv.org/abs/2112.00832": {"title": "On the mixed-model analysis of covariance in cluster-randomized trials", "link": "http://arxiv.org/abs/2112.00832", "description": "In the analyses of cluster-randomized trials, mixed-model analysis of\ncovariance (ANCOVA) is a standard approach for covariate adjustment and\nhandling within-cluster correlations. However, when the normality, linearity,\nor the random-intercept assumption is violated, the validity and efficiency of\nthe mixed-model ANCOVA estimators for estimating the average treatment effect\nremain unclear. Under the potential outcomes framework, we prove that the\nmixed-model ANCOVA estimators for the average treatment effect are consistent\nand asymptotically normal under arbitrary misspecification of its working\nmodel. If the probability of receiving treatment is 0.5 for each cluster, we\nfurther show that the model-based variance estimator under mixed-model ANCOVA1\n(ANCOVA without treatment-covariate interactions) remains consistent,\nclarifying that the confidence interval given by standard software is\nasymptotically valid even under model misspecification. Beyond robustness, we\ndiscuss several insights on precision among classical methods for analyzing\ncluster-randomized trials, including the mixed-model ANCOVA, individual-level\nANCOVA, and cluster-level ANCOVA estimators. These insights may inform the\nchoice of methods in practice. Our analytical results and insights are\nillustrated via simulation studies and analyses of three cluster-randomized\ntrials."}, "http://arxiv.org/abs/2201.10770": {"title": "Confidence intervals for the Cox model test error from cross-validation", "link": "http://arxiv.org/abs/2201.10770", "description": "Cross-validation (CV) is one of the most widely used techniques in\nstatistical learning for estimating the test error of a model, but its behavior\nis not yet fully understood. It has been shown that standard confidence\nintervals for test error using estimates from CV may have coverage below\nnominal levels. This phenomenon occurs because each sample is used in both the\ntraining and testing procedures during CV and as a result, the CV estimates of\nthe errors become correlated. Without accounting for this correlation, the\nestimate of the variance is smaller than it should be. One way to mitigate this\nissue is by estimating the mean squared error of the prediction error instead\nusing nested CV. This approach has been shown to achieve superior coverage\ncompared to intervals derived from standard CV. In this work, we generalize the\nnested CV idea to the Cox proportional hazards model and explore various\nchoices of test error for this setting."}, "http://arxiv.org/abs/2202.08419": {"title": "High-Dimensional Time-Varying Coefficient Estimation", "link": "http://arxiv.org/abs/2202.08419", "description": "In this paper, we develop a novel high-dimensional time-varying coefficient\nestimation method, based on high-dimensional Ito diffusion processes. To\naccount for high-dimensional time-varying coefficients, we first estimate local\n(or instantaneous) coefficients using a time-localized Dantzig selection scheme\nunder a sparsity condition, which results in biased local coefficient\nestimators due to the regularization. To handle the bias, we propose a\ndebiasing scheme, which provides well-performing unbiased local coefficient\nestimators. With the unbiased local coefficient estimators, we estimate the\nintegrated coefficient, and to further account for the sparsity of the\ncoefficient process, we apply thresholding schemes. We call this Thresholding\ndEbiased Dantzig (TED). We establish asymptotic properties of the proposed TED\nestimator. In the empirical analysis, we apply the TED procedure to analyzing\nhigh-dimensional factor models using high-frequency data."}, "http://arxiv.org/abs/2206.12525": {"title": "Causality of Functional Longitudinal Data", "link": "http://arxiv.org/abs/2206.12525", "description": "\"Treatment-confounder feedback\" is the central complication to resolve in\nlongitudinal studies, to infer causality. The existing frameworks for\nidentifying causal effects for longitudinal studies with discrete repeated\nmeasures hinge heavily on assuming that time advances in discrete time steps or\ntreatment changes as a jumping process, rendering the number of \"feedbacks\"\nfinite. However, medical studies nowadays with real-time monitoring involve\nfunctional time-varying outcomes, treatment, and confounders, which leads to an\nuncountably infinite number of feedbacks between treatment and confounders.\nTherefore more general and advanced theory is needed. We generalize the\ndefinition of causal effects under user-specified stochastic treatment regimes\nto longitudinal studies with continuous monitoring and develop an\nidentification framework, allowing right censoring and truncation by death. We\nprovide sufficient identification assumptions including a generalized\nconsistency assumption, a sequential randomization assumption, a positivity\nassumption, and a novel \"intervenable\" assumption designed for the\ncontinuous-time case. Under these assumptions, we propose a g-computation\nprocess and an inverse probability weighting process, which suggest a\ng-computation formula and an inverse probability weighting formula for\nidentification. For practical purposes, we also construct two classes of\npopulation estimating equations to identify these two processes, respectively,\nwhich further suggest a doubly robust identification formula with extra\nrobustness against process misspecification. We prove that our framework fully\ngeneralize the existing frameworks and is nonparametric."}, "http://arxiv.org/abs/2209.08139": {"title": "Sparse high-dimensional linear regression with a partitioned empirical Bayes ECM algorithm", "link": "http://arxiv.org/abs/2209.08139", "description": "Bayesian variable selection methods are powerful techniques for fitting and\ninferring on sparse high-dimensional linear regression models. However, many\nare computationally intensive or require restrictive prior distributions on\nmodel parameters. In this paper, we proposed a computationally efficient and\npowerful Bayesian approach for sparse high-dimensional linear regression.\nMinimal prior assumptions on the parameters are required through the use of\nplug-in empirical Bayes estimates of hyperparameters. Efficient maximum a\nposteriori (MAP) estimation is completed through a Parameter-Expanded\nExpectation-Conditional-Maximization (PX-ECM) algorithm. The PX-ECM results in\na robust computationally efficient coordinate-wise optimization which -- when\nupdating the coefficient for a particular predictor -- adjusts for the impact\nof other predictor variables. The completion of the E-step uses an approach\nmotivated by the popular two-group approach to multiple testing. The result is\na PaRtitiOned empirical Bayes Ecm (PROBE) algorithm applied to sparse\nhigh-dimensional linear regression, which can be completed using one-at-a-time\nor all-at-once type optimization. We compare the empirical properties of PROBE\nto comparable approaches with numerous simulation studies and analyses of\ncancer cell drug responses. The proposed approach is implemented in the R\npackage probe."}, "http://arxiv.org/abs/2212.02709": {"title": "SURE-tuned Bridge Regression", "link": "http://arxiv.org/abs/2212.02709", "description": "Consider the {$\\ell_{\\alpha}$} regularized linear regression, also termed\nBridge regression. For $\\alpha\\in (0,1)$, Bridge regression enjoys several\nstatistical properties of interest such as sparsity and near-unbiasedness of\nthe estimates (Fan and Li, 2001). However, the main difficulty lies in the\nnon-convex nature of the penalty for these values of $\\alpha$, which makes an\noptimization procedure challenging and usually it is only possible to find a\nlocal optimum. To address this issue, Polson et al. (2013) took a sampling\nbased fully Bayesian approach to this problem, using the correspondence between\nthe Bridge penalty and a power exponential prior on the regression\ncoefficients. However, their sampling procedure relies on Markov chain Monte\nCarlo (MCMC) techniques, which are inherently sequential and not scalable to\nlarge problem dimensions. Cross validation approaches are similarly\ncomputation-intensive. To this end, our contribution is a novel\n\\emph{non-iterative} method to fit a Bridge regression model. The main\ncontribution lies in an explicit formula for Stein's unbiased risk estimate for\nthe out of sample prediction risk of Bridge regression, which can then be\noptimized to select the desired tuning parameters, allowing us to completely\nbypass MCMC as well as computation-intensive cross validation approaches. Our\nprocedure yields results in a fraction of computational times compared to\niterative schemes, without any appreciable loss in statistical performance. An\nR implementation is publicly available online at:\nhttps://github.com/loriaJ/Sure-tuned_BridgeRegression ."}, "http://arxiv.org/abs/2212.03122": {"title": "Robust convex biclustering with a tuning-free method", "link": "http://arxiv.org/abs/2212.03122", "description": "Biclustering is widely used in different kinds of fields including gene\ninformation analysis, text mining, and recommendation system by effectively\ndiscovering the local correlation between samples and features. However, many\nbiclustering algorithms will collapse when facing heavy-tailed data. In this\npaper, we propose a robust version of convex biclustering algorithm with Huber\nloss. Yet, the newly introduced robustification parameter brings an extra\nburden to selecting the optimal parameters. Therefore, we propose a tuning-free\nmethod for automatically selecting the optimal robustification parameter with\nhigh efficiency. The simulation study demonstrates the more fabulous\nperformance of our proposed method than traditional biclustering methods when\nencountering heavy-tailed noise. A real-life biomedical application is also\npresented. The R package RcvxBiclustr is available at\nhttps://github.com/YifanChen3/RcvxBiclustr."}, "http://arxiv.org/abs/2301.09661": {"title": "Estimating marginal treatment effects from observational studies and indirect treatment comparisons: When are standardization-based methods preferable to those based on propensity score weighting?", "link": "http://arxiv.org/abs/2301.09661", "description": "In light of newly developed standardization methods, we evaluate, via\nsimulation study, how propensity score weighting and standardization -based\napproaches compare for obtaining estimates of the marginal odds ratio and the\nmarginal hazard ratio. Specifically, we consider how the two approaches compare\nin two different scenarios: (1) in a single observational study, and (2) in an\nanchored indirect treatment comparison (ITC) of randomized controlled trials.\nWe present the material in such a way so that the matching-adjusted indirect\ncomparison (MAIC) and the (novel) simulated treatment comparison (STC) methods\nin the ITC setting may be viewed as analogous to the propensity score weighting\nand standardization methods in the single observational study setting. Our\nresults suggest that current recommendations for conducting ITCs can be\nimproved and underscore the importance of adjusting for purely prognostic\nfactors."}, "http://arxiv.org/abs/2302.11746": {"title": "Logistic Regression and Classification with non-Euclidean Covariates", "link": "http://arxiv.org/abs/2302.11746", "description": "We introduce a logistic regression model for data pairs consisting of a\nbinary response and a covariate residing in a non-Euclidean metric space\nwithout vector structures. Based on the proposed model we also develop a binary\nclassifier for non-Euclidean objects. We propose a maximum likelihood estimator\nfor the non-Euclidean regression coefficient in the model, and provide upper\nbounds on the estimation error under various metric entropy conditions that\nquantify complexity of the underlying metric space. Matching lower bounds are\nderived for the important metric spaces commonly seen in statistics,\nestablishing optimality of the proposed estimator in such spaces. Similarly, an\nupper bound on the excess risk of the developed classifier is provided for\ngeneral metric spaces. A finer upper bound and a matching lower bound, and thus\noptimality of the proposed classifier, are established for Riemannian\nmanifolds. We investigate the numerical performance of the proposed estimator\nand classifier via simulation studies, and illustrate their practical merits\nvia an application to task-related fMRI data."}, "http://arxiv.org/abs/2302.13658": {"title": "Robust High-Dimensional Time-Varying Coefficient Estimation", "link": "http://arxiv.org/abs/2302.13658", "description": "In this paper, we develop a novel high-dimensional coefficient estimation\nprocedure based on high-frequency data. Unlike usual high-dimensional\nregression procedure such as LASSO, we additionally handle the heavy-tailedness\nof high-frequency observations as well as time variations of coefficient\nprocesses. Specifically, we employ Huber loss and truncation scheme to handle\nheavy-tailed observations, while $\\ell_{1}$-regularization is adopted to\novercome the curse of dimensionality. To account for the time-varying\ncoefficient, we estimate local coefficients which are biased due to the\n$\\ell_{1}$-regularization. Thus, when estimating integrated coefficients, we\npropose a debiasing scheme to enjoy the law of large number property and employ\na thresholding scheme to further accommodate the sparsity of the coefficients.\nWe call this Robust thrEsholding Debiased LASSO (RED-LASSO) estimator. We show\nthat the RED-LASSO estimator can achieve a near-optimal convergence rate. In\nthe empirical study, we apply the RED-LASSO procedure to the high-dimensional\nintegrated coefficient estimation using high-frequency trading data."}, "http://arxiv.org/abs/2307.04754": {"title": "Action-State Dependent Dynamic Model Selection", "link": "http://arxiv.org/abs/2307.04754", "description": "A model among many may only be best under certain states of the world.\nSwitching from a model to another can also be costly. Finding a procedure to\ndynamically choose a model in these circumstances requires to solve a complex\nestimation procedure and a dynamic programming problem. A Reinforcement\nlearning algorithm is used to approximate and estimate from the data the\noptimal solution to this dynamic programming problem. The algorithm is shown to\nconsistently estimate the optimal policy that may choose different models based\non a set of covariates. A typical example is the one of switching between\ndifferent portfolio models under rebalancing costs, using macroeconomic\ninformation. Using a set of macroeconomic variables and price data, an\nempirical application to the aforementioned portfolio problem shows superior\nperformance to choosing the best portfolio model with hindsight."}, "http://arxiv.org/abs/2307.14828": {"title": "Identifying regime switches through Bayesian wavelet estimation: evidence from flood detection in the Taquari River Valley", "link": "http://arxiv.org/abs/2307.14828", "description": "Two-component mixture models have proved to be a powerful tool for modeling\nheterogeneity in several cluster analysis contexts. However, most methods based\non these models assume a constant behavior for the mixture weights, which can\nbe restrictive and unsuitable for some applications. In this paper, we relax\nthis assumption and allow the mixture weights to vary according to the index\n(e.g., time) to make the model more adaptive to a broader range of data sets.\nWe propose an efficient MCMC algorithm to jointly estimate both component\nparameters and dynamic weights from their posterior samples. We evaluate the\nmethod's performance by running Monte Carlo simulation studies under different\nscenarios for the dynamic weights. In addition, we apply the algorithm to a\ntime series that records the level reached by a river in southern Brazil. The\nTaquari River is a water body whose frequent flood inundations have caused\nvarious damage to riverside communities. Implementing a dynamic mixture model\nallows us to properly describe the flood regimes for the areas most affected by\nthese phenomena."}, "http://arxiv.org/abs/2310.06130": {"title": "Statistical inference for radially-stable generalized Pareto distributions and return level-sets in geometric extremes", "link": "http://arxiv.org/abs/2310.06130", "description": "We obtain a functional analogue of the quantile function for probability\nmeasures admitting a continuous Lebesgue density on $\\mathbb{R}^d$, and use it\nto characterize the class of non-trivial limit distributions of radially\nrecentered and rescaled multivariate exceedances in geometric extremes. A new\nclass of multivariate distributions is identified, termed radially stable\ngeneralized Pareto distributions, and is shown to admit certain stability\nproperties that permit extrapolation to extremal sets along any direction in\n$\\mathbb{R}^d$. Based on the limit Poisson point process likelihood of the\nradially renormalized point process of exceedances, we develop parsimonious\nstatistical models that exploit theoretical links between structural\nstar-bodies and are amenable to Bayesian inference. The star-bodies determine\nthe mean measure of the limit Poisson process through a hierarchical structure.\nOur framework sharpens statistical inference by suitably including additional\ninformation from the angular directions of the geometric exceedances and\nfacilitates efficient computations in dimensions $d=2$ and $d=3$. Additionally,\nit naturally leads to the notion of the return level-set, which is a canonical\nquantile set expressed in terms of its average recurrence interval, and a\ngeometric analogue of the uni-dimensional return level. We illustrate our\nmethods with a simulation study showing superior predictive performance of\nprobabilities of rare events, and with two case studies, one associated with\nriver flow extremes, and the other with oceanographic extremes."}, "http://arxiv.org/abs/2310.06242": {"title": "Treatment Choice, Mean Square Regret and Partial Identification", "link": "http://arxiv.org/abs/2310.06242", "description": "We consider a decision maker who faces a binary treatment choice when their\nwelfare is only partially identified from data. We contribute to the literature\nby anchoring our finite-sample analysis on mean square regret, a decision\ncriterion advocated by Kitagawa, Lee, and Qiu (2022). We find that optimal\nrules are always fractional, irrespective of the width of the identified set\nand precision of its estimate. The optimal treatment fraction is a simple\nlogistic transformation of the commonly used t-statistic multiplied by a factor\ncalculated by a simple constrained optimization. This treatment fraction gets\ncloser to 0.5 as the width of the identified set becomes wider, implying the\ndecision maker becomes more cautious against the adversarial Nature."}, "http://arxiv.org/abs/2310.06252": {"title": "Power and sample size calculation of two-sample projection-based testing for sparsely observed functional data", "link": "http://arxiv.org/abs/2310.06252", "description": "Projection-based testing for mean trajectory differences in two groups of\nirregularly and sparsely observed functional data has garnered significant\nattention in the literature because it accommodates a wide spectrum of group\ndifferences and (non-stationary) covariance structures. This article presents\nthe derivation of the theoretical power function and the introduction of a\ncomprehensive power and sample size (PASS) calculation toolkit tailored to the\nprojection-based testing method developed by Wang (2021). Our approach\naccommodates a wide spectrum of group difference scenarios and a broad class of\ncovariance structures governing the underlying processes. Through extensive\nnumerical simulation, we demonstrate the robustness of this testing method by\nshowcasing that its statistical power remains nearly unaffected even when a\ncertain percentage of observations are missing, rendering it 'missing-immune'.\nFurthermore, we illustrate the practical utility of this test through analysis\nof two randomized controlled trials of Parkinson's disease. To facilitate\nimplementation, we provide a user-friendly R package fPASS, complete with a\ndetailed vignette to guide users through its practical application. We\nanticipate that this article will significantly enhance the usability of this\npotent statistical tool across a range of biostatistical applications, with a\nparticular focus on its relevance in the design of clinical trials."}, "http://arxiv.org/abs/2310.06315": {"title": "Ultra-high dimensional confounder selection algorithms comparison with application to radiomics data", "link": "http://arxiv.org/abs/2310.06315", "description": "Radiomics is an emerging area of medical imaging data analysis particularly\nfor cancer. It involves the conversion of digital medical images into mineable\nultra-high dimensional data. Machine learning algorithms are widely used in\nradiomics data analysis to develop powerful decision support model to improve\nprecision in diagnosis, assessment of prognosis and prediction of therapy\nresponse. However, machine learning algorithms for causal inference have not\nbeen previously employed in radiomics analysis. In this paper, we evaluate the\nvalue of machine learning algorithms for causal inference in radiomics. We\nselect three recent competitive variable selection algorithms for causal\ninference: outcome-adaptive lasso (OAL), generalized outcome-adaptive lasso\n(GOAL) and causal ball screening (CBS). We used a sure independence screening\nprocedure to propose an extension of GOAL and OAL for ultra-high dimensional\ndata, SIS + GOAL and SIS + OAL. We compared SIS + GOAL, SIS + OAL and CBS using\nsimulation study and two radiomics datasets in cancer, osteosarcoma and\ngliosarcoma. The two radiomics studies and the simulation study identified SIS\n+ GOAL as the optimal variable selection algorithm."}, "http://arxiv.org/abs/2310.06330": {"title": "Multivariate moment least-squares estimators for reversible Markov chains", "link": "http://arxiv.org/abs/2310.06330", "description": "Markov chain Monte Carlo (MCMC) is a commonly used method for approximating\nexpectations with respect to probability distributions. Uncertainty assessment\nfor MCMC estimators is essential in practical applications. Moreover, for\nmultivariate functions of a Markov chain, it is important to estimate not only\nthe auto-correlation for each component but also to estimate\ncross-correlations, in order to better assess sample quality, improve estimates\nof effective sample size, and use more effective stopping rules. Berg and Song\n[2022] introduced the moment least squares (momentLS) estimator, a\nshape-constrained estimator for the autocovariance sequence from a reversible\nMarkov chain, for univariate functions of the Markov chain. Based on this\nsequence estimator, they proposed an estimator of the asymptotic variance of\nthe sample mean from MCMC samples. In this study, we propose novel\nautocovariance sequence and asymptotic variance estimators for Markov chain\nfunctions with multiple components, based on the univariate momentLS estimators\nfrom Berg and Song [2022]. We demonstrate strong consistency of the proposed\nauto(cross)-covariance sequence and asymptotic variance matrix estimators. We\nconduct empirical comparisons of our method with other state-of-the-art\napproaches on simulated and real-data examples, using popular samplers\nincluding the random-walk Metropolis sampler and the No-U-Turn sampler from\nSTAN."}, "http://arxiv.org/abs/2310.06357": {"title": "Adaptive Storey's null proportion estimator", "link": "http://arxiv.org/abs/2310.06357", "description": "False discovery rate (FDR) is a commonly used criterion in multiple testing\nand the Benjamini-Hochberg (BH) procedure is arguably the most popular approach\nwith FDR guarantee. To improve power, the adaptive BH procedure has been\nproposed by incorporating various null proportion estimators, among which\nStorey's estimator has gained substantial popularity. The performance of\nStorey's estimator hinges on a critical hyper-parameter, where a pre-fixed\nconfiguration lacks power and existing data-driven hyper-parameters compromise\nthe FDR control. In this work, we propose a novel class of adaptive\nhyper-parameters and establish the FDR control of the associated BH procedure\nusing a martingale argument. Within this class of data-driven hyper-parameters,\nwe present a specific configuration designed to maximize the number of\nrejections and characterize the convergence of this proposal to the optimal\nhyper-parameter under a commonly-used mixture model. We evaluate our adaptive\nStorey's null proportion estimator and the associated BH procedure on extensive\nsimulated data and a motivating protein dataset. Our proposal exhibits\nsignificant power gains when dealing with a considerable proportion of weak\nnon-nulls or a conservative null distribution."}, "http://arxiv.org/abs/2310.06467": {"title": "Advances in Kth nearest-neighbour clutter removal", "link": "http://arxiv.org/abs/2310.06467", "description": "We consider the problem of feature detection in the presence of clutter in\nspatial point processes. Classification methods have been developed in previous\nstudies. Among these, Byers and Raftery (1998) models the observed Kth nearest\nneighbour distances as a mixture distribution and classifies the clutter and\nfeature points consequently. In this paper, we enhance such approach in two\nmanners. First, we propose an automatic procedure for selecting the number of\nnearest neighbours to consider in the classification method by means of\nsegmented regression models. Secondly, with the aim of applying the procedure\nmultiple times to get a ``better\" end result, we propose a stopping criterion\nthat minimizes the overall entropy measure of cluster separation between\nclutter and feature points. The proposed procedures are suitable for a feature\nwith clutter as two superimposed Poisson processes on any space, including\nlinear networks. We present simulations and two case studies of environmental\ndata to illustrate the method."}, "http://arxiv.org/abs/2310.06533": {"title": "Multilevel Monte Carlo for a class of Partially Observed Processes in Neuroscience", "link": "http://arxiv.org/abs/2310.06533", "description": "In this paper we consider Bayesian parameter inference associated to a class\nof partially observed stochastic differential equations (SDE) driven by jump\nprocesses. Such type of models can be routinely found in applications, of which\nwe focus upon the case of neuroscience. The data are assumed to be observed\nregularly in time and driven by the SDE model with unknown parameters. In\npractice the SDE may not have an analytically tractable solution and this leads\nnaturally to a time-discretization. We adapt the multilevel Markov chain Monte\nCarlo method of [11], which works with a hierarchy of time discretizations and\nshow empirically and theoretically that this is preferable to using one single\ntime discretization. The improvement is in terms of the computational cost\nneeded to obtain a pre-specified numerical error. Our approach is illustrated\non models that are found in neuroscience."}, "http://arxiv.org/abs/2310.06653": {"title": "Evaluating causal effects on time-to-event outcomes in an RCT in Oncology with treatment discontinuation due to adverse events", "link": "http://arxiv.org/abs/2310.06653", "description": "In clinical trials, patients sometimes discontinue study treatments\nprematurely due to reasons such as adverse events. Treatment discontinuation\noccurs after the randomisation as an intercurrent event, making causal\ninference more challenging. The Intention-To-Treat (ITT) analysis provides\nvalid causal estimates of the effect of treatment assignment; still, it does\nnot take into account whether or not patients had to discontinue the treatment\nprematurely. We propose to deal with the problem of treatment discontinuation\nusing principal stratification, recognised in the ICH E9(R1) addendum as a\nstrategy for handling intercurrent events. Under this approach, we can\ndecompose the overall ITT effect into principal causal effects for groups of\npatients defined by their potential discontinuation behaviour in continuous\ntime. In this framework, we must consider that discontinuation happening in\ncontinuous time generates an infinite number of principal strata and that\ndiscontinuation time is not defined for patients who would never discontinue.\nAn additional complication is that discontinuation time and time-to-event\noutcomes are subject to administrative censoring. We employ a flexible\nmodel-based Bayesian approach to deal with such complications. We apply the\nBayesian principal stratification framework to analyse synthetic data based on\na recent RCT in Oncology, aiming to assess the causal effects of a new\ninvestigational drug combined with standard of care vs. standard of care alone\non progression-free survival. We simulate data under different assumptions that\nreflect real situations where patients' behaviour depends on critical baseline\ncovariates. Finally, we highlight how such an approach makes it straightforward\nto characterise patients' discontinuation behaviour with respect to the\navailable covariates with the help of a simulation study."}, "http://arxiv.org/abs/2310.06673": {"title": "Assurance Methods for designing a clinical trial with a delayed treatment effect", "link": "http://arxiv.org/abs/2310.06673", "description": "An assurance calculation is a Bayesian alternative to a power calculation.\nOne may be performed to aid the planning of a clinical trial, specifically\nsetting the sample size or to support decisions about whether or not to perform\na study. Immuno-oncology (IO) is a rapidly evolving area in the development of\nanticancer drugs. A common phenomenon that arises from IO trials is one of\ndelayed treatment effects, that is, there is a delay in the separation of the\nsurvival curves. To calculate assurance for a trial in which a delayed\ntreatment effect is likely to be present, uncertainty about key parameters\nneeds to be considered. If uncertainty is not considered, then the number of\npatients recruited may not be enough to ensure we have adequate statistical\npower to detect a clinically relevant treatment effect. We present a new\nelicitation technique for when a delayed treatment effect is likely to be\npresent and show how to compute assurance using these elicited prior\ndistributions. We provide an example to illustrate how this could be used in\npractice. Open-source software is provided for implementing our methods. Our\nmethodology makes the benefits of assurance methods available for the planning\nof IO trials (and others where a delayed treatment expect is likely to occur)."}, "http://arxiv.org/abs/2310.06696": {"title": "Variable selection with FDR control for noisy data -- an application to screening metabolites that are associated with breast and colorectal cancer", "link": "http://arxiv.org/abs/2310.06696", "description": "The rapidly expanding field of metabolomics presents an invaluable resource\nfor understanding the associations between metabolites and various diseases.\nHowever, the high dimensionality, presence of missing values, and measurement\nerrors associated with metabolomics data can present challenges in developing\nreliable and reproducible methodologies for disease association studies.\nTherefore, there is a compelling need to develop robust statistical methods\nthat can navigate these complexities to achieve reliable and reproducible\ndisease association studies. In this paper, we focus on developing such a\nmethodology with an emphasis on controlling the False Discovery Rate during the\nscreening of mutual metabolomic signals for multiple disease outcomes. We\nillustrate the versatility and performance of this procedure in a variety of\nscenarios, dealing with missing data and measurement errors. As a specific\napplication of this novel methodology, we target two of the most prevalent\ncancers among US women: breast cancer and colorectal cancer. By applying our\nmethod to the Wome's Health Initiative data, we successfully identify\nmetabolites that are associated with either or both of these cancers,\ndemonstrating the practical utility and potential of our method in identifying\nconsistent risk factors and understanding shared mechanisms between diseases."}, "http://arxiv.org/abs/2310.06708": {"title": "Adjustment with Three Continuous Variables", "link": "http://arxiv.org/abs/2310.06708", "description": "Spurious association between X and Y may be due to a confounding variable W.\nStatisticians may adjust for W using a variety of techniques. This paper\npresents the results of simulations conducted to assess the performance of\nthose techniques under various, elementary, data-generating processes. The\nresults indicate that no technique is best overall and that specific techniques\nshould be selected based on the particulars of the data-generating process.\nHere we show how causal graphs can guide the selection or design of techniques\nfor statistical adjustment. R programs are provided for researchers interested\nin generalization."}, "http://arxiv.org/abs/2310.06720": {"title": "Asymptotic theory for Bayesian inference and prediction: from the ordinary to a conditional Peaks-Over-Threshold method", "link": "http://arxiv.org/abs/2310.06720", "description": "The Peaks Over Threshold (POT) method is the most popular statistical method\nfor the analysis of univariate extremes. Even though there is a rich applied\nliterature on Bayesian inference for the POT method there is no asymptotic\ntheory for such proposals. Even more importantly, the ambitious and challenging\nproblem of predicting future extreme events according to a proper probabilistic\nforecasting approach has received no attention to date. In this paper we\ndevelop the asymptotic theory (consistency, contraction rates, asymptotic\nnormality and asymptotic coverage of credible intervals) for the Bayesian\ninference based on the POT method. We extend such an asymptotic theory to cover\nthe Bayesian inference on the tail properties of the conditional distribution\nof a response random variable conditionally to a vector of random covariates.\nWith the aim to make accurate predictions of severer extreme events than those\noccurred in the past, we specify the posterior predictive distribution of a\nfuture unobservable excess variable in the unconditional and conditional\napproach and we prove that is Wasserstein consistent and derive its contraction\nrates. Simulations show the good performances of the proposed Bayesian\ninferential methods. The analysis of the change in the frequency of financial\ncrises over time shows the utility of our methodology."}, "http://arxiv.org/abs/2310.06730": {"title": "Sparse topic modeling via spectral decomposition and thresholding", "link": "http://arxiv.org/abs/2310.06730", "description": "The probabilistic Latent Semantic Indexing model assumes that the expectation\nof the corpus matrix is low-rank and can be written as the product of a\ntopic-word matrix and a word-document matrix. In this paper, we study the\nestimation of the topic-word matrix under the additional assumption that the\nordered entries of its columns rapidly decay to zero. This sparsity assumption\nis motivated by the empirical observation that the word frequencies in a text\noften adhere to Zipf's law. We introduce a new spectral procedure for\nestimating the topic-word matrix that thresholds words based on their corpus\nfrequencies, and show that its $\\ell_1$-error rate under our sparsity\nassumption depends on the vocabulary size $p$ only via a logarithmic term. Our\nerror bound is valid for all parameter regimes and in particular for the\nsetting where $p$ is extremely large; this high-dimensional setting is commonly\nencountered but has not been adequately addressed in prior literature.\nFurthermore, our procedure also accommodates datasets that violate the\nseparability assumption, which is necessary for most prior approaches in topic\nmodeling. Experiments with synthetic data confirm that our procedure is\ncomputationally fast and allows for consistent estimation of the topic-word\nmatrix in a wide variety of parameter regimes. Our procedure also performs well\nrelative to well-established methods when applied to a large corpus of research\npaper abstracts, as well as the analysis of single-cell and microbiome data\nwhere the same statistical model is relevant but the parameter regimes are\nvastly different."}, "http://arxiv.org/abs/2310.06746": {"title": "Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules", "link": "http://arxiv.org/abs/2310.06746", "description": "Interpretability is a key concern in estimating heterogeneous treatment\neffects using machine learning methods, especially for healthcare applications\nwhere high-stake decisions are often made. Inspired by the Predictive,\nDescriptive, Relevant framework of interpretability, we propose causal rule\nlearning which finds a refined set of causal rules characterizing potential\nsubgroups to estimate and enhance our understanding of heterogeneous treatment\neffects. Causal rule learning involves three phases: rule discovery, rule\nselection, and rule analysis. In the rule discovery phase, we utilize a causal\nforest to generate a pool of causal rules with corresponding subgroup average\ntreatment effects. The selection phase then employs a D-learning method to\nselect a subset of these rules to deconstruct individual-level treatment\neffects as a linear combination of the subgroup-level effects. This helps to\nanswer an ignored question by previous literature: what if an individual\nsimultaneously belongs to multiple groups with different average treatment\neffects? The rule analysis phase outlines a detailed procedure to further\nanalyze each rule in the subset from multiple perspectives, revealing the most\npromising rules for further validation. The rules themselves, their\ncorresponding subgroup treatment effects, and their weights in the linear\ncombination give us more insights into heterogeneous treatment effects.\nSimulation and real-world data analysis demonstrate the superior performance of\ncausal rule learning on the interpretable estimation of heterogeneous treatment\neffect when the ground truth is complex and the sample size is sufficient."}, "http://arxiv.org/abs/2310.06808": {"title": "Odds are the sign is right", "link": "http://arxiv.org/abs/2310.06808", "description": "This article introduces a new condition based on odds ratios for sensitivity\nanalysis. The analysis involves the average effect of a treatment or exposure\non a response or outcome with estimates adjusted for and conditional on a\nsingle, unmeasured, dichotomous covariate. Results of statistical simulations\nare displayed to show that the odds ratio condition is as reliable as other\ncommonly used conditions for sensitivity analysis. Other conditions utilize\nquantities reflective of a mediating covariate. The odds ratio condition can be\napplied when the covariate is a confounding variable. As an example application\nwe use the odds ratio condition to analyze and interpret a positive association\nobserved between Zika virus infection and birth defects."}, "http://arxiv.org/abs/2009.07551": {"title": "Manipulation-Robust Regression Discontinuity Designs", "link": "http://arxiv.org/abs/2009.07551", "description": "We present a new identification condition for regression discontinuity\ndesigns. We replace the local randomization of Lee (2008) with two restrictions\non its threat, namely, the manipulation of the running variable. Furthermore,\nwe provide the first auxiliary assumption of McCrary's (2008) diagnostic test\nto detect manipulation. Based on our auxiliary assumption, we derive a novel\nexpression of moments that immediately implies the worst-case bounds of Gerard,\nRokkanen, and Rothe (2020) and an enhanced interpretation of their target\nparameters. We highlight two issues: an overlooked source of identification\nfailure, and a missing auxiliary assumption to detect manipulation. In the case\nstudies, we illustrate our solution to these issues using institutional details\nand economic theories."}, "http://arxiv.org/abs/2204.06030": {"title": "Variable importance measures for heterogeneous causal effects", "link": "http://arxiv.org/abs/2204.06030", "description": "The recognition that personalised treatment decisions lead to better clinical\noutcomes has sparked recent research activity in the following two domains.\nPolicy learning focuses on finding optimal treatment rules (OTRs), which\nexpress whether an individual would be better off with or without treatment,\ngiven their measured characteristics. OTRs optimize a pre-set population\ncriterion, but do not provide insight into the extent to which treatment\nbenefits or harms individual subjects. Estimates of conditional average\ntreatment effects (CATEs) do offer such insights, but valid inference is\ncurrently difficult to obtain when data-adaptive methods are used. Moreover,\nclinicians are (rightly) hesitant to blindly adopt OTR or CATE estimates, not\nleast since both may represent complicated functions of patient characteristics\nthat provide little insight into the key drivers of heterogeneity. To address\nthese limitations, we introduce novel nonparametric treatment effect variable\nimportance measures (TE-VIMs). TE-VIMs extend recent regression-VIMs, viewed as\nnonparametric analogues to ANOVA statistics. By not being tied to a particular\nmodel, they are amenable to data-adaptive (machine learning) estimation of the\nCATE, itself an active area of research. Estimators for the proposed statistics\nare derived from their efficient influence curves and these are illustrated\nthrough a simulation study and an applied example."}, "http://arxiv.org/abs/2204.07907": {"title": "Just Identified Indirect Inference Estimator: Accurate Inference through Bias Correction", "link": "http://arxiv.org/abs/2204.07907", "description": "An important challenge in statistical analysis lies in controlling the\nestimation bias when handling the ever-increasing data size and model\ncomplexity of modern data settings. In this paper, we propose a reliable\nestimation and inference approach for parametric models based on the Just\nIdentified iNdirect Inference estimator (JINI). The key advantage of our\napproach is that it allows to construct a consistent estimator in a simple\nmanner, while providing strong bias correction guarantees that lead to accurate\ninference. Our approach is particularly useful for complex parametric models,\nas it allows to bypass the analytical and computational difficulties (e.g., due\nto intractable estimating equation) typically encountered in standard\nprocedures. The properties of JINI (including consistency, asymptotic\nnormality, and its bias correction property) are also studied when the\nparameter dimension is allowed to diverge, which provide the theoretical\nfoundation to explain the advantageous performance of JINI in increasing\ndimensional covariates settings. Our simulations and an alcohol consumption\ndata analysis highlight the practical usefulness and excellent performance of\nJINI when data present features (e.g., misclassification, rounding) as well as\nin robust estimation."}, "http://arxiv.org/abs/2209.05598": {"title": "Learning domain-specific causal discovery from time series", "link": "http://arxiv.org/abs/2209.05598", "description": "Causal discovery (CD) from time-varying data is important in neuroscience,\nmedicine, and machine learning. Techniques for CD encompass randomized\nexperiments, which are generally unbiased but expensive, and algorithms such as\nGranger causality, conditional-independence-based, structural-equation-based,\nand score-based methods that are only accurate under strong assumptions made by\nhuman designers. However, as demonstrated in other areas of machine learning,\nhuman expertise is often not entirely accurate and tends to be outperformed in\ndomains with abundant data. In this study, we examine whether we can enhance\ndomain-specific causal discovery for time series using a data-driven approach.\nOur findings indicate that this procedure significantly outperforms\nhuman-designed, domain-agnostic causal discovery methods, such as Mutual\nInformation, VAR-LiNGAM, and Granger Causality on the MOS 6502 microprocessor,\nthe NetSim fMRI dataset, and the Dream3 gene dataset. We argue that, when\nfeasible, the causality field should consider a supervised approach in which\ndomain-specific CD procedures are learned from extensive datasets with known\ncausal relationships, rather than being designed by human specialists. Our\nfindings promise a new approach toward improving CD in neural and medical data\nand for the broader machine learning community."}, "http://arxiv.org/abs/2209.05795": {"title": "Joint modelling of the body and tail of bivariate data", "link": "http://arxiv.org/abs/2209.05795", "description": "In situations where both extreme and non-extreme data are of interest,\nmodelling the whole data set accurately is important. In a univariate\nframework, modelling the bulk and tail of a distribution has been extensively\nstudied before. However, when more than one variable is of concern, models that\naim specifically at capturing both regions correctly are scarce in the\nliterature. A dependence model that blends two copulas with different\ncharacteristics over the whole range of the data support is proposed. One\ncopula is tailored to the bulk and the other to the tail, with a dynamic\nweighting function employed to transition smoothly between them. Tail\ndependence properties are investigated numerically and simulation is used to\nconfirm that the blended model is sufficiently flexible to capture a wide\nvariety of structures. The model is applied to study the dependence between\ntemperature and ozone concentration at two sites in the UK and compared with a\nsingle copula fit. The proposed model provides a better, more flexible, fit to\nthe data, and is also capable of capturing complex dependence structures."}, "http://arxiv.org/abs/2212.14650": {"title": "Two-step estimators of high dimensional correlation matrices", "link": "http://arxiv.org/abs/2212.14650", "description": "We investigate block diagonal and hierarchical nested stochastic multivariate\nGaussian models by studying their sample cross-correlation matrix on high\ndimensions. By performing numerical simulations, we compare a filtered sample\ncross-correlation with the population cross-correlation matrices by using\nseveral rotationally invariant estimators (RIE) and hierarchical clustering\nestimators (HCE) under several loss functions. We show that at large but finite\nsample size, sample cross-correlation filtered by RIE estimators are often\noutperformed by HCE estimators for several of the loss functions. We also show\nthat for block models and for hierarchically nested block models the best\ndetermination of the filtered sample cross-correlation is achieved by\nintroducing two-step estimators combining state-of-the-art non-linear shrinkage\nmodels with hierarchical clustering estimators."}, "http://arxiv.org/abs/2302.02457": {"title": "Scalable inference in functional linear regression with streaming data", "link": "http://arxiv.org/abs/2302.02457", "description": "Traditional static functional data analysis is facing new challenges due to\nstreaming data, where data constantly flow in. A major challenge is that\nstoring such an ever-increasing amount of data in memory is nearly impossible.\nIn addition, existing inferential tools in online learning are mainly developed\nfor finite-dimensional problems, while inference methods for functional data\nare focused on the batch learning setting. In this paper, we tackle these\nissues by developing functional stochastic gradient descent algorithms and\nproposing an online bootstrap resampling procedure to systematically study the\ninference problem for functional linear regression. In particular, the proposed\nestimation and inference procedures use only one pass over the data; thus they\nare easy to implement and suitable to the situation where data arrive in a\nstreaming manner. Furthermore, we establish the convergence rate as well as the\nasymptotic distribution of the proposed estimator. Meanwhile, the proposed\nperturbed estimator from the bootstrap procedure is shown to enjoy the same\ntheoretical properties, which provide the theoretical justification for our\nonline inference tool. As far as we know, this is the first inference result on\nthe functional linear regression model with streaming data. Simulation studies\nare conducted to investigate the finite-sample performance of the proposed\nprocedure. An application is illustrated with the Beijing multi-site\nair-quality data."}, "http://arxiv.org/abs/2303.09598": {"title": "Variational Bayesian analysis of survival data using a log-logistic accelerated failure time model", "link": "http://arxiv.org/abs/2303.09598", "description": "The log-logistic regression model is one of the most commonly used\naccelerated failure time (AFT) models in survival analysis, for which\nstatistical inference methods are mainly established under the frequentist\nframework. Recently, Bayesian inference for log-logistic AFT models using\nMarkov chain Monte Carlo (MCMC) techniques has also been widely developed. In\nthis work, we develop an alternative approach to MCMC methods and infer the\nparameters of the log-logistic AFT model via a mean-field variational Bayes\n(VB) algorithm. A piecewise approximation technique is embedded in deriving the\nVB algorithm to achieve conjugacy. The proposed VB algorithm is evaluated and\ncompared with typical frequentist inferences and MCMC inference using simulated\ndata under various scenarios. A publicly available dataset is employed for\nillustration. We demonstrate that the proposed VB algorithm can achieve good\nestimation accuracy and has a lower computational cost compared with MCMC\nmethods."}, "http://arxiv.org/abs/2304.03853": {"title": "StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables", "link": "http://arxiv.org/abs/2304.03853", "description": "StepMix is an open-source Python package for the pseudo-likelihood estimation\n(one-, two- and three-step approaches) of generalized finite mixture models\n(latent profile and latent class analysis) with external variables (covariates\nand distal outcomes). In many applications in social sciences, the main\nobjective is not only to cluster individuals into latent classes, but also to\nuse these classes to develop more complex statistical models. These models\ngenerally divide into a measurement model that relates the latent classes to\nobserved indicators, and a structural model that relates covariates and outcome\nvariables to the latent classes. The measurement and structural models can be\nestimated jointly using the so-called one-step approach or sequentially using\nstepwise methods, which present significant advantages for practitioners\nregarding the interpretability of the estimated latent classes. In addition to\nthe one-step approach, StepMix implements the most important stepwise\nestimation methods from the literature, including the bias-adjusted three-step\nmethods with Bolk-Croon-Hagenaars and maximum likelihood corrections and the\nmore recent two-step approach. These pseudo-likelihood estimators are presented\nin this paper under a unified framework as specific expectation-maximization\nsubroutines. To facilitate and promote their adoption among the data science\ncommunity, StepMix follows the object-oriented design of the scikit-learn\nlibrary and provides an additional R wrapper."}, "http://arxiv.org/abs/2310.06926": {"title": "Bayesian inference and cure rate modeling for event history data", "link": "http://arxiv.org/abs/2310.06926", "description": "Estimating model parameters of a general family of cure models is always a\nchallenging task mainly due to flatness and multimodality of the likelihood\nfunction. In this work, we propose a fully Bayesian approach in order to\novercome these issues. Posterior inference is carried out by constructing a\nMetropolis-coupled Markov chain Monte Carlo (MCMC) sampler, which combines\nGibbs sampling for the latent cure indicators and Metropolis-Hastings steps\nwith Langevin diffusion dynamics for parameter updates. The main MCMC algorithm\nis embedded within a parallel tempering scheme by considering heated versions\nof the target posterior distribution. It is demonstrated via simulations that\nthe proposed algorithm freely explores the multimodal posterior distribution\nand produces robust point estimates, while it outperforms maximum likelihood\nestimation via the Expectation-Maximization algorithm. A by-product of our\nBayesian implementation is to control the False Discovery Rate when classifying\nitems as cured or not. Finally, the proposed method is illustrated in a real\ndataset which refers to recidivism for offenders released from prison; the\nevent of interest is whether the offender was re-incarcerated after probation\nor not."}, "http://arxiv.org/abs/2310.06969": {"title": "Positivity-free Policy Learning with Observational Data", "link": "http://arxiv.org/abs/2310.06969", "description": "Policy learning utilizing observational data is pivotal across various\ndomains, with the objective of learning the optimal treatment assignment policy\nwhile adhering to specific constraints such as fairness, budget, and\nsimplicity. This study introduces a novel positivity-free (stochastic) policy\nlearning framework designed to address the challenges posed by the\nimpracticality of the positivity assumption in real-world scenarios. This\nframework leverages incremental propensity score policies to adjust propensity\nscore values instead of assigning fixed values to treatments. We characterize\nthese incremental propensity score policies and establish identification\nconditions, employing semiparametric efficiency theory to propose efficient\nestimators capable of achieving rapid convergence rates, even when integrated\nwith advanced machine learning algorithms. This paper provides a thorough\nexploration of the theoretical guarantees associated with policy learning and\nvalidates the proposed framework's finite-sample performance through\ncomprehensive numerical experiments, ensuring the identification of causal\neffects from observational data is both robust and reliable."}, "http://arxiv.org/abs/2310.07002": {"title": "Bayesian cross-validation by parallel Markov Chain Monte Carlo", "link": "http://arxiv.org/abs/2310.07002", "description": "Brute force cross-validation (CV) is a method for predictive assessment and\nmodel selection that is general and applicable to a wide range of Bayesian\nmodels. However, in many cases brute force CV is too computationally burdensome\nto form part of interactive modeling workflows, especially when inference\nrelies on Markov chain Monte Carlo (MCMC). In this paper we present a method\nfor conducting fast Bayesian CV by massively parallel MCMC. On suitable\naccelerator hardware, for many applications our approach is about as fast (in\nwall clock time) as a single full-data model fit.\n\nParallel CV is more flexible than existing fast CV approximation methods\nbecause it can easily exploit a wide range of scoring rules and data\npartitioning schemes. This is particularly useful for CV methods designed for\nnon-exchangeable data. Our approach also delivers accurate estimates of Monte\nCarlo and CV uncertainty. In addition to parallelizing computations, parallel\nCV speeds up inference by reusing information from earlier MCMC adaptation and\ninference obtained during initial model fitting and checking of the full-data\nmodel.\n\nWe propose MCMC diagnostics for parallel CV applications, including a summary\nof MCMC mixing based on the popular potential scale reduction factor\n($\\hat{R}$) and MCMC effective sample size ($\\widehat{ESS}$) measures.\nFurthermore, we describe a method for determining whether an $\\hat{R}$\ndiagnostic indicates approximate stationarity of the chains, that may be of\nmore general interest for applications beyond parallel CV.\n\nFor parallel CV to work on memory-constrained computing accelerators, we show\nthat parallel CV and associated diagnostics can be implemented using online\n(streaming) algorithms ideal for parallel computing environments with limited\nmemory. Constant memory algorithms allow parallel CV to scale up to very large\nblocking designs."}, "http://arxiv.org/abs/2310.07016": {"title": "Discovering the Unknowns: A First Step", "link": "http://arxiv.org/abs/2310.07016", "description": "This article aims at discovering the unknown variables in the system through\ndata analysis. The main idea is to use the time of data collection as a\nsurrogate variable and try to identify the unknown variables by modeling\ngradual and sudden changes in the data. We use Gaussian process modeling and a\nsparse representation of the sudden changes to efficiently estimate the large\nnumber of parameters in the proposed statistical model. The method is tested on\na realistic dataset generated using a one-dimensional implementation of a\nMagnetized Liner Inertial Fusion (MagLIF) simulation model and encouraging\nresults are obtained."}, "http://arxiv.org/abs/2310.07107": {"title": "Root n consistent extremile regression and its supervised and semi-supervised learning", "link": "http://arxiv.org/abs/2310.07107", "description": "Extremile (Daouia, Gijbels and Stupfler,2019) is a novel and coherent measure\nof risk, determined by weighted expectations rather than tail probabilities. It\nfinds application in risk management, and, in contrast to quantiles, it\nfulfills the axioms of consistency, taking into account the severity of tail\nlosses. However, existing studies (Daouia, Gijbels and Stupfler,2019,2022) on\nextremile involve unknown distribution functions, making it challenging to\nobtain a root n-consistent estimator for unknown parameters in linear extremile\nregression. This article introduces a new definition of linear extremile\nregression and its estimation method, where the estimator is root n-consistent.\nAdditionally, while the analysis of unlabeled data for extremes presents a\nsignificant challenge and is currently a topic of great interest in machine\nlearning for various classification problems, we have developed a\nsemi-supervised framework for the proposed extremile regression using unlabeled\ndata. This framework can also enhance estimation accuracy under model\nmisspecification. Both simulations and real data analyses have been conducted\nto illustrate the finite sample performance of the proposed methods."}, "http://arxiv.org/abs/2310.07124": {"title": "Systematic simulation of age-period-cohort analysis: Demonstrating bias of Bayesian regularization", "link": "http://arxiv.org/abs/2310.07124", "description": "Age-period-cohort (APC) analysis is one of the fundamental time-series\nanalyses used in the social sciences. This paper evaluates APC analysis via\nsystematic simulation in term of how well the artificial parameters are\nrecovered. We consider three models of Bayesian regularization using normal\nprior distributions: the random effects model with reference to multilevel\nanalysis, the ridge regression model equivalent to the intrinsic estimator, and\nthe random walk model referred to as the Bayesian cohort model. The proposed\nsimulation generates artificial data through combinations of the linear\ncomponents, focusing on the fact that the identification problem affects the\nlinear components of the three effects. Among the 13 cases of artificial data,\nthe random walk model recovered the artificial parameters well in 10 cases,\nwhile the random effects model and the ridge regression model did so in 4\ncases. The cases in which the models failed to recover the artificial\nparameters show the estimated linear component of the cohort effects as close\nto zero. In conclusion, the models of Bayesian regularization in APC analysis\nhave a bias: the index weights have a large influence on the cohort effects and\nthese constraints drive the linear component of the cohort effects close to\nzero. However, the random walk model mitigates underestimating the linear\ncomponent of the cohort effects."}, "http://arxiv.org/abs/2310.07330": {"title": "Functional Generalized Canonical Correlation Analysis for studying multiple longitudinal variables", "link": "http://arxiv.org/abs/2310.07330", "description": "In this paper, we introduce Functional Generalized Canonical Correlation\nAnalysis (FGCCA), a new framework for exploring associations between multiple\nrandom processes observed jointly. The framework is based on the multiblock\nRegularized Generalized Canonical Correlation Analysis (RGCCA) framework. It is\nrobust to sparsely and irregularly observed data, making it applicable in many\nsettings. We establish the monotonic property of the solving procedure and\nintroduce a Bayesian approach for estimating canonical components. We propose\nan extension of the framework that allows the integration of a univariate or\nmultivariate response into the analysis, paving the way for predictive\napplications. We evaluate the method's efficiency in simulation studies and\npresent a use case on a longitudinal dataset."}, "http://arxiv.org/abs/2310.07364": {"title": "Statistical inference of high-dimensional vector autoregressive time series with non-i", "link": "http://arxiv.org/abs/2310.07364", "description": "Independent or i.i.d. innovations is an essential assumption in the\nliterature for analyzing a vector time series. However, this assumption is\neither too restrictive for a real-life time series to satisfy or is hard to\nverify through a hypothesis test. This paper performs statistical inference on\na sparse high-dimensional vector autoregressive time series, allowing its white\nnoise innovations to be dependent, even non-stationary. To achieve this goal,\nit adopts a post-selection estimator to fit the vector autoregressive model and\nderives the asymptotic distribution of the post-selection estimator. The\ninnovations in the autoregressive time series are not assumed to be\nindependent, thus making the covariance matrices of the autoregressive\ncoefficient estimators complex and difficult to estimate. Our work develops a\nbootstrap algorithm to facilitate practitioners in performing statistical\ninference without having to engage in sophisticated calculations. Simulations\nand real-life data experiments reveal the validity of the proposed methods and\ntheoretical results.\n\nReal-life data is rarely considered to exactly satisfy an autoregressive\nmodel with independent or i.i.d. innovations, so our work should better reflect\nthe reality compared to the literature that assumes i.i.d. innovations."}, "http://arxiv.org/abs/2310.07399": {"title": "Randomized Runge-Kutta-Nystr\\\"om", "link": "http://arxiv.org/abs/2310.07399", "description": "We present 5/2- and 7/2-order $L^2$-accurate randomized Runge-Kutta-Nystr\\\"om\nmethods to approximate the Hamiltonian flow underlying various non-reversible\nMarkov chain Monte Carlo chains including unadjusted Hamiltonian Monte Carlo\nand unadjusted kinetic Langevin chains. Quantitative 5/2-order $L^2$-accuracy\nupper bounds are provided under gradient and Hessian Lipschitz assumptions on\nthe potential energy function. The superior complexity of the corresponding\nMarkov chains is numerically demonstrated for a selection of `well-behaved',\nhigh-dimensional target distributions."}, "http://arxiv.org/abs/2310.07456": {"title": "Hierarchical Bayesian Claim Count modeling with Overdispersed Outcome and Mismeasured Covariates in Actuarial Practice", "link": "http://arxiv.org/abs/2310.07456", "description": "The problem of overdispersed claim counts and mismeasured covariates is\ncommon in insurance. On the one hand, the presence of overdispersion in the\ncount data violates the homogeneity assumption, and on the other hand,\nmeasurement errors in covariates highlight the model risk issue in actuarial\npractice. The consequence can be inaccurate premium pricing which would\nnegatively affect business competitiveness. Our goal is to address these two\nmodelling problems simultaneously by capturing the unobservable correlations\nbetween observations that arise from overdispersed outcome and mismeasured\ncovariate in actuarial process. To this end, we establish novel connections\nbetween the count-based generalized linear mixed model (GLMM) and a popular\nerror-correction tool for non-linear modelling - Simulation Extrapolation\n(SIMEX). We consider a modelling framework based on the hierarchical Bayesian\nparadigm. To our knowledge, the approach of combining a hierarchical Bayes with\nSIMEX has not previously been discussed in the literature. We demonstrate the\napplicability of our approach on the workplace absenteeism data. Our results\nindicate that the hierarchical Bayesian GLMM incorporated with the SIMEX\noutperforms naive GLMM / SIMEX in terms of goodness of fit."}, "http://arxiv.org/abs/2310.07567": {"title": "Comparing the effectiveness of k-different treatments through the area under the ROC curve", "link": "http://arxiv.org/abs/2310.07567", "description": "The area under the receiver-operating characteristic curve (AUC) has become a\npopular index not only for measuring the overall prediction capacity of a\nmarker but also the association strength between continuous and binary\nvariables. In the current study, it has been used for comparing the association\nsize of four different interventions involving impulsive decision making,\nstudied through an animal model, in which each animal provides several negative\n(pre-treatment) and positive (post-treatment) measures. The problem of the full\ncomparison of the average AUCs arises therefore in a natural way. We construct\nan analysis of variance (ANOVA) type test for testing the equality of the\nimpact of these treatments measured through the respective AUCs, and\nconsidering the random-effect represented by the animal. The use (and\ndevelopment) of a post-hoc Tukey's HSD type test is also considered. We explore\nthe finite-sample behavior of our proposal via Monte Carlo simulations, and\nanalyze the data generated from the original problem. An R package implementing\nthe procedures is provided as supplementary material."}, "http://arxiv.org/abs/2310.07605": {"title": "Split Knockoffs for Multiple Comparisons: Controlling the Directional False Discovery Rate", "link": "http://arxiv.org/abs/2310.07605", "description": "Multiple comparisons in hypothesis testing often encounter structural\nconstraints in various applications. For instance, in structural Magnetic\nResonance Imaging for Alzheimer's Disease, the focus extends beyond examining\natrophic brain regions to include comparisons of anatomically adjacent regions.\nThese constraints can be modeled as linear transformations of parameters, where\nthe sign patterns play a crucial role in estimating directional effects. This\nclass of problems, encompassing total variations, wavelet transforms, fused\nLASSO, trend filtering, and more, presents an open challenge in effectively\ncontrolling the directional false discovery rate. In this paper, we propose an\nextended Split Knockoff method specifically designed to address the control of\ndirectional false discovery rate under linear transformations. Our proposed\napproach relaxes the stringent linear manifold constraint to its neighborhood,\nemploying a variable splitting technique commonly used in optimization. This\nmethodology yields an orthogonal design that benefits both power and\ndirectional false discovery rate control. By incorporating a sample splitting\nscheme, we achieve effective control of the directional false discovery rate,\nwith a notable reduction to zero as the relaxed neighborhood expands. To\ndemonstrate the efficacy of our method, we conduct simulation experiments and\napply it to two real-world scenarios: Alzheimer's Disease analysis and human\nage comparisons."}, "http://arxiv.org/abs/2310.07680": {"title": "Hamiltonian Dynamics of Bayesian Inference Formalised by Arc Hamiltonian Systems", "link": "http://arxiv.org/abs/2310.07680", "description": "This paper makes two theoretical contributions. First, we establish a novel\nclass of Hamiltonian systems, called arc Hamiltonian systems, for saddle\nHamiltonian functions over infinite-dimensional metric spaces. Arc Hamiltonian\nsystems generate a flow that satisfies the law of conservation of energy\neverywhere in a metric space. They are governed by an extension of Hamilton's\nequation formulated based on (i) the framework of arc fields and (ii) an\ninfinite-dimensional gradient, termed the arc gradient, of a Hamiltonian\nfunction. We derive conditions for the existence of a flow generated by an arc\nHamiltonian system, showing that they reduce to local Lipschitz continuity of\nthe arc gradient under sufficient regularity. Second, we present two\nHamiltonian functions, called the cumulant generating functional and the\ncentred cumulant generating functional, over a metric space of log-likelihoods\nand measures. The former characterises the posterior of Bayesian inference as a\npart of the arc gradient that induces a flow of log-likelihoods and\nnon-negative measures. The latter characterises the difference of the posterior\nand the prior as a part of the arc gradient that induces a flow of\nlog-likelihoods and probability measures. Our results reveal an implication of\nthe belief updating mechanism from the prior to the posterior as an\ninfinitesimal change of a measure in the infinite-dimensional Hamiltonian\nflows."}, "http://arxiv.org/abs/2009.12217": {"title": "Latent Causal Socioeconomic Health Index", "link": "http://arxiv.org/abs/2009.12217", "description": "This research develops a model-based LAtent Causal Socioeconomic Health\n(LACSH) index at the national level. Motivated by the need for a holistic\nnational well-being index, we build upon the latent health factor index (LHFI)\napproach that has been used to assess the unobservable ecological/ecosystem\nhealth. LHFI integratively models the relationship between metrics, latent\nhealth, and covariates that drive the notion of health. In this paper, the LHFI\nstructure is integrated with spatial modeling and statistical causal modeling.\nOur efforts are focused on developing the integrated framework to facilitate\nthe understanding of how an observational continuous variable might have\ncausally affected a latent trait that exhibits spatial correlation. A novel\nvisualization technique to evaluate covariate balance is also introduced for\nthe case of a continuous policy (treatment) variable. Our resulting LACSH\nframework and visualization tool are illustrated through two global case\nstudies on national socioeconomic health (latent trait), each with various\nmetrics and covariates pertaining to different aspects of societal health, and\nthe treatment variable being mandatory maternity leave days and government\nexpenditure on healthcare, respectively. We validate our model by two\nsimulation studies. All approaches are structured in a Bayesian hierarchical\nframework and results are obtained by Markov chain Monte Carlo techniques."}, "http://arxiv.org/abs/2201.02958": {"title": "Smooth Nested Simulation: Bridging Cubic and Square Root Convergence Rates in High Dimensions", "link": "http://arxiv.org/abs/2201.02958", "description": "Nested simulation concerns estimating functionals of a conditional\nexpectation via simulation. In this paper, we propose a new method based on\nkernel ridge regression to exploit the smoothness of the conditional\nexpectation as a function of the multidimensional conditioning variable.\nAsymptotic analysis shows that the proposed method can effectively alleviate\nthe curse of dimensionality on the convergence rate as the simulation budget\nincreases, provided that the conditional expectation is sufficiently smooth.\nThe smoothness bridges the gap between the cubic root convergence rate (that\nis, the optimal rate for the standard nested simulation) and the square root\nconvergence rate (that is, the canonical rate for the standard Monte Carlo\nsimulation). We demonstrate the performance of the proposed method via\nnumerical examples from portfolio risk management and input uncertainty\nquantification."}, "http://arxiv.org/abs/2204.12635": {"title": "Multivariate and regression models for directional data based on projected P\\'olya trees", "link": "http://arxiv.org/abs/2204.12635", "description": "Projected distributions have proved to be useful in the study of circular and\ndirectional data. Although any multivariate distribution can be used to produce\na projected model, these distributions are typically parametric. In this\narticle we consider a multivariate P\\'olya tree on $R^k$ and project it to the\nunit hypersphere $S^k$ to define a new Bayesian nonparametric model for\ndirectional data. We study the properties of the proposed model and in\nparticular, concentrate on the implied conditional distributions of some\ndirections given the others to define a directional-directional regression\nmodel. We also define a multivariate linear regression model with P\\'olya tree\nerror and project it to define a linear-directional regression model. We obtain\nthe posterior characterisation of all models and show their performance with\nsimulated and real datasets."}, "http://arxiv.org/abs/2207.13250": {"title": "Spatio-Temporal Wildfire Prediction using Multi-Modal Data", "link": "http://arxiv.org/abs/2207.13250", "description": "Due to severe societal and environmental impacts, wildfire prediction using\nmulti-modal sensing data has become a highly sought-after data-analytical tool\nby various stakeholders (such as state governments and power utility companies)\nto achieve a more informed understanding of wildfire activities and plan\npreventive measures. A desirable algorithm should precisely predict fire risk\nand magnitude for a location in real time. In this paper, we develop a flexible\nspatio-temporal wildfire prediction framework using multi-modal time series\ndata. We first predict the wildfire risk (the chance of a wildfire event) in\nreal-time, considering the historical events using discrete mutually exciting\npoint process models. Then we further develop a wildfire magnitude prediction\nset method based on the flexible distribution-free time-series conformal\nprediction (CP) approach. Theoretically, we prove a risk model parameter\nrecovery guarantee, as well as coverage and set size guarantees for the CP\nsets. Through extensive real-data experiments with wildfire data in California,\nwe demonstrate the effectiveness of our methods, as well as their flexibility\nand scalability in large regions."}, "http://arxiv.org/abs/2210.13550": {"title": "Regularized Nonlinear Regression with Dependent Errors and its Application to a Biomechanical Model", "link": "http://arxiv.org/abs/2210.13550", "description": "A biomechanical model often requires parameter estimation and selection in a\nknown but complicated nonlinear function. Motivated by observing that data from\na head-neck position tracking system, one of biomechanical models, show\nmultiplicative time dependent errors, we develop a modified penalized weighted\nleast squares estimator. The proposed method can be also applied to a model\nwith non-zero mean time dependent additive errors. Asymptotic properties of the\nproposed estimator are investigated under mild conditions on a weight matrix\nand the error process. A simulation study demonstrates that the proposed\nestimation works well in both parameter estimation and selection with time\ndependent error. The analysis and comparison with an existing method for\nhead-neck position tracking data show better performance of the proposed method\nin terms of the variance accounted for (VAF)."}, "http://arxiv.org/abs/2210.14965": {"title": "Topology-Driven Goodness-of-Fit Tests in Arbitrary Dimensions", "link": "http://arxiv.org/abs/2210.14965", "description": "This paper adopts a tool from computational topology, the Euler\ncharacteristic curve (ECC) of a sample, to perform one- and two-sample goodness\nof fit tests. We call our procedure TopoTests. The presented tests work for\nsamples of arbitrary dimension, having comparable power to the state-of-the-art\ntests in the one-dimensional case. It is demonstrated that the type I error of\nTopoTests can be controlled and their type II error vanishes exponentially with\nincreasing sample size. Extensive numerical simulations of TopoTests are\nconducted to demonstrate their power for samples of various sizes."}, "http://arxiv.org/abs/2211.03860": {"title": "Automatic Change-Point Detection in Time Series via Deep Learning", "link": "http://arxiv.org/abs/2211.03860", "description": "Detecting change-points in data is challenging because of the range of\npossible types of change and types of behaviour of data when there is no\nchange. Statistically efficient methods for detecting a change will depend on\nboth of these features, and it can be difficult for a practitioner to develop\nan appropriate detection method for their application of interest. We show how\nto automatically generate new offline detection methods based on training a\nneural network. Our approach is motivated by many existing tests for the\npresence of a change-point being representable by a simple neural network, and\nthus a neural network trained with sufficient data should have performance at\nleast as good as these methods. We present theory that quantifies the error\nrate for such an approach, and how it depends on the amount of training data.\nEmpirical results show that, even with limited training data, its performance\nis competitive with the standard CUSUM-based classifier for detecting a change\nin mean when the noise is independent and Gaussian, and can substantially\noutperform it in the presence of auto-correlated or heavy-tailed noise. Our\nmethod also shows strong results in detecting and localising changes in\nactivity based on accelerometer data."}, "http://arxiv.org/abs/2211.09099": {"title": "Selecting Subpopulations for Causal Inference in Regression Discontinuity Designs", "link": "http://arxiv.org/abs/2211.09099", "description": "The Brazil Bolsa Familia (BF) program is a conditional cash transfer program\naimed to reduce short-term poverty by direct cash transfers and to fight\nlong-term poverty by increasing human capital among poor Brazilian people.\nEligibility for Bolsa Familia benefits depends on a cutoff rule, which\nclassifies the BF study as a regression discontinuity (RD) design. Extracting\ncausal information from RD studies is challenging. Following Li et al (2015)\nand Branson and Mealli (2019), we formally describe the BF RD design as a local\nrandomized experiment within the potential outcome approach. Under this\nframework, causal effects can be identified and estimated on a subpopulation\nwhere a local overlap assumption, a local SUTVA and a local ignorability\nassumption hold. We first discuss the potential advantages of this framework\nover local regression methods based on continuity assumptions, which concern\nthe definition of the causal estimands, the design and the analysis of the\nstudy, and the interpretation and generalizability of the results. A critical\nissue of this local randomization approach is how to choose subpopulations for\nwhich we can draw valid causal inference. We propose a Bayesian model-based\nfinite mixture approach to clustering to classify observations into\nsubpopulations where the RD assumptions hold and do not hold. This approach has\nimportant advantages: a) it allows to account for the uncertainty in the\nsubpopulation membership, which is typically neglected; b) it does not impose\nany constraint on the shape of the subpopulation; c) it is scalable to\nhigh-dimensional settings; e) it allows to target alternative causal estimands\nthan the average treatment effect (ATE); and f) it is robust to a certain\ndegree of manipulation/selection of the running variable. We apply our proposed\napproach to assess causal effects of the Bolsa Familia program on leprosy\nincidence in 2009."}, "http://arxiv.org/abs/2301.08276": {"title": "Cross-validatory model selection for Bayesian autoregressions with exogenous regressors", "link": "http://arxiv.org/abs/2301.08276", "description": "Bayesian cross-validation (CV) is a popular method for predictive model\nassessment that is simple to implement and broadly applicable. A wide range of\nCV schemes is available for time series applications, including generic\nleave-one-out (LOO) and K-fold methods, as well as specialized approaches\nintended to deal with serial dependence such as leave-future-out (LFO),\nh-block, and hv-block.\n\nExisting large-sample results show that both specialized and generic methods\nare applicable to models of serially-dependent data. However, large sample\nconsistency results overlook the impact of sampling variability on accuracy in\nfinite samples. Moreover, the accuracy of a CV scheme depends on many aspects\nof the procedure. We show that poor design choices can lead to elevated rates\nof adverse selection.\n\nIn this paper, we consider the problem of identifying the regression\ncomponent of an important class of models of data with serial dependence,\nautoregressions of order p with q exogenous regressors (ARX(p,q)), under the\nlogarithmic scoring rule. We show that when serial dependence is present,\nscores computed using the joint (multivariate) density have lower variance and\nbetter model selection accuracy than the popular pointwise estimator. In\naddition, we present a detailed case study of the special case of ARX models\nwith fixed autoregressive structure and variance. For this class, we derive the\nfinite-sample distribution of the CV estimators and the model selection\nstatistic. We conclude with recommendations for practitioners."}, "http://arxiv.org/abs/2301.12026": {"title": "G-formula for causal inference via multiple imputation", "link": "http://arxiv.org/abs/2301.12026", "description": "G-formula is a popular approach for estimating treatment or exposure effects\nfrom longitudinal data that are subject to time-varying confounding. G-formula\nestimation is typically performed by Monte-Carlo simulation, with\nnon-parametric bootstrapping used for inference. We show that G-formula can be\nimplemented by exploiting existing methods for multiple imputation (MI) for\nsynthetic data. This involves using an existing modified version of Rubin's\nvariance estimator. In practice missing data is ubiquitous in longitudinal\ndatasets. We show that such missing data can be readily accommodated as part of\nthe MI procedure when using G-formula, and describe how MI software can be used\nto implement the approach. We explore its performance using a simulation study\nand an application from cystic fibrosis."}, "http://arxiv.org/abs/2306.01292": {"title": "Alternative Measures of Direct and Indirect Effects", "link": "http://arxiv.org/abs/2306.01292", "description": "There are a number of measures of direct and indirect effects in the\nliterature. They are suitable in some cases and unsuitable in others. We\ndescribe a case where the existing measures are unsuitable and propose new\nsuitable ones. We also show that the new measures can partially handle\nunmeasured treatment-outcome confounding, and bound long-term effects by\ncombining experimental and observational data."}, "http://arxiv.org/abs/2308.00913": {"title": "The Bayesian Context Trees State Space Model for time series modelling and forecasting", "link": "http://arxiv.org/abs/2308.00913", "description": "A hierarchical Bayesian framework is introduced for developing rich mixture\nmodels for real-valued time series, partly motivated by important applications\nin financial time series analysis. At the top level, meaningful discrete states\nare identified as appropriately quantised values of some of the most recent\nsamples. These observable states are described as a discrete context-tree\nmodel. At the bottom level, a different, arbitrary model for real-valued time\nseries -- a base model -- is associated with each state. This defines a very\ngeneral framework that can be used in conjunction with any existing model class\nto build flexible and interpretable mixture models. We call this the Bayesian\nContext Trees State Space Model, or the BCT-X framework. Efficient algorithms\nare introduced that allow for effective, exact Bayesian inference and learning\nin this setting; in particular, the maximum a posteriori probability (MAP)\ncontext-tree model can be identified. These algorithms can be updated\nsequentially, facilitating efficient online forecasting. The utility of the\ngeneral framework is illustrated in two particular instances: When\nautoregressive (AR) models are used as base models, resulting in a nonlinear AR\nmixture model, and when conditional heteroscedastic (ARCH) models are used,\nresulting in a mixture model that offers a powerful and systematic way of\nmodelling the well-known volatility asymmetries in financial data. In\nforecasting, the BCT-X methods are found to outperform state-of-the-art\ntechniques on simulated and real-world data, both in terms of accuracy and\ncomputational requirements. In modelling, the BCT-X structure finds natural\nstructure present in the data. In particular, the BCT-ARCH model reveals a\nnovel, important feature of stock market index data, in the form of an enhanced\nleverage effect."}, "http://arxiv.org/abs/2309.11942": {"title": "On the Probability of Immunity", "link": "http://arxiv.org/abs/2309.11942", "description": "This work is devoted to the study of the probability of immunity, i.e. the\neffect occurs whether exposed or not. We derive necessary and sufficient\nconditions for non-immunity and $\\epsilon$-bounded immunity, i.e. the\nprobability of immunity is zero and $\\epsilon$-bounded, respectively. The\nformer allows us to estimate the probability of benefit (i.e., the effect\noccurs if and only if exposed) from a randomized controlled trial, and the\nlatter allows us to produce bounds of the probability of benefit that are\ntighter than the existing ones. We also introduce the concept of indirect\nimmunity (i.e., through a mediator) and repeat our previous analysis for it.\nFinally, we propose a method for sensitivity analysis of the probability of\nimmunity under unmeasured confounding."}, "http://arxiv.org/abs/2309.13441": {"title": "Anytime valid and asymptotically optimal statistical inference driven by predictive recursion", "link": "http://arxiv.org/abs/2309.13441", "description": "Distinguishing two classes of candidate models is a fundamental and\npractically important problem in statistical inference. Error rate control is\ncrucial to the logic but, in complex nonparametric settings, such guarantees\ncan be difficult to achieve, especially when the stopping rule that determines\nthe data collection process is not available. In this paper we develop a novel\ne-process construction that leverages the so-called predictive recursion (PR)\nalgorithm designed to rapidly and recursively fit nonparametric mixture models.\nThe resulting PRe-process affords anytime valid inference uniformly over\nstopping rules and is shown to be efficient in the sense that it achieves the\nmaximal growth rate under the alternative relative to the mixture model being\nfit by PR. In the special case of testing for a log-concave density, the\nPRe-process test is computationally simpler and faster, more stable, and no\nless efficient compared to a recently proposed anytime valid test."}, "http://arxiv.org/abs/2309.16598": {"title": "Cross-Prediction-Powered Inference", "link": "http://arxiv.org/abs/2309.16598", "description": "While reliable data-driven decision-making hinges on high-quality labeled\ndata, the acquisition of quality labels often involves laborious human\nannotations or slow and expensive scientific measurements. Machine learning is\nbecoming an appealing alternative as sophisticated predictive techniques are\nbeing used to quickly and cheaply produce large amounts of predicted labels;\ne.g., predicted protein structures are used to supplement experimentally\nderived structures, predictions of socioeconomic indicators from satellite\nimagery are used to supplement accurate survey data, and so on. Since\npredictions are imperfect and potentially biased, this practice brings into\nquestion the validity of downstream inferences. We introduce cross-prediction:\na method for valid inference powered by machine learning. With a small labeled\ndataset and a large unlabeled dataset, cross-prediction imputes the missing\nlabels via machine learning and applies a form of debiasing to remedy the\nprediction inaccuracies. The resulting inferences achieve the desired error\nprobability and are more powerful than those that only leverage the labeled\ndata. Closely related is the recent proposal of prediction-powered inference,\nwhich assumes that a good pre-trained model is already available. We show that\ncross-prediction is consistently more powerful than an adaptation of\nprediction-powered inference in which a fraction of the labeled data is split\noff and used to train the model. Finally, we observe that cross-prediction\ngives more stable conclusions than its competitors; its confidence intervals\ntypically have significantly lower variability."}, "http://arxiv.org/abs/2310.07801": {"title": "Trajectory-aware Principal Manifold Framework for Data Augmentation and Image Generation", "link": "http://arxiv.org/abs/2310.07801", "description": "Data augmentation for deep learning benefits model training, image\ntransformation, medical imaging analysis and many other fields. Many existing\nmethods generate new samples from a parametric distribution, like the Gaussian,\nwith little attention to generate samples along the data manifold in either the\ninput or feature space. In this paper, we verify that there are theoretical and\npractical advantages of using the principal manifold hidden in the feature\nspace than the Gaussian distribution. We then propose a novel trajectory-aware\nprincipal manifold framework to restore the manifold backbone and generate\nsamples along a specific trajectory. On top of the autoencoder architecture, we\nfurther introduce an intrinsic dimension regularization term to make the\nmanifold more compact and enable few-shot image generation. Experimental\nresults show that the novel framework is able to extract more compact manifold\nrepresentation, improve classification accuracy and generate smooth\ntransformation among few samples."}, "http://arxiv.org/abs/2310.07817": {"title": "Nonlinear global Fr\\'echet regression for random objects via weak conditional expectation", "link": "http://arxiv.org/abs/2310.07817", "description": "Random objects are complex non-Euclidean data taking value in general metric\nspace, possibly devoid of any underlying vector space structure. Such data are\ngetting increasingly abundant with the rapid advancement in technology.\nExamples include probability distributions, positive semi-definite matrices,\nand data on Riemannian manifolds. However, except for regression for\nobject-valued response with Euclidean predictors and\ndistribution-on-distribution regression, there has been limited development of\na general framework for object-valued response with object-valued predictors in\nthe literature. To fill this gap, we introduce the notion of a weak conditional\nFr\\'echet mean based on Carleman operators and then propose a global nonlinear\nFr\\'echet regression model through the reproducing kernel Hilbert space (RKHS)\nembedding. Furthermore, we establish the relationships between the conditional\nFr\\'echet mean and the weak conditional Fr\\'echet mean for both Euclidean and\nobject-valued data. We also show that the state-of-the-art global Fr\\'echet\nregression developed by Petersen and Mueller, 2019 emerges as a special case of\nour method by choosing a linear kernel. We require that the metric space for\nthe predictor admits a reproducing kernel, while the intrinsic geometry of the\nmetric space for the response is utilized to study the asymptotic properties of\nthe proposed estimates. Numerical studies, including extensive simulations and\na real application, are conducted to investigate the performance of our\nestimator in a finite sample."}, "http://arxiv.org/abs/2310.07839": {"title": "Marital Sorting, Household Inequality and Selection", "link": "http://arxiv.org/abs/2310.07839", "description": "Using CPS data for 1976 to 2022 we explore how wage inequality has evolved\nfor married couples with both spouses working full time full year, and its\nimpact on household income inequality. We also investigate how marriage sorting\npatterns have changed over this period. To determine the factors driving income\ninequality we estimate a model explaining the joint distribution of wages which\naccounts for the spouses' employment decisions. We find that income inequality\nhas increased for these households and increased assortative matching of wages\nhas exacerbated the inequality resulting from individual wage growth. We find\nthat positive sorting partially reflects the correlation across unobservables\ninfluencing both members' of the marriage wages. We decompose the changes in\nsorting patterns over the 47 years comprising our sample into structural,\ncomposition and selection effects and find that the increase in positive\nsorting primarily reflects the increased skill premia for both observed and\nunobserved characteristics."}, "http://arxiv.org/abs/2310.07850": {"title": "Conformal prediction with local weights: randomization enables local guarantees", "link": "http://arxiv.org/abs/2310.07850", "description": "In this work, we consider the problem of building distribution-free\nprediction intervals with finite-sample conditional coverage guarantees.\nConformal prediction (CP) is an increasingly popular framework for building\nprediction intervals with distribution-free guarantees, but these guarantees\nonly ensure marginal coverage: the probability of coverage is averaged over a\nrandom draw of both the training and test data, meaning that there might be\nsubstantial undercoverage within certain subpopulations. Instead, ideally, we\nwould want to have local coverage guarantees that hold for each possible value\nof the test point's features. While the impossibility of achieving pointwise\nlocal coverage is well established in the literature, many variants of\nconformal prediction algorithm show favorable local coverage properties\nempirically. Relaxing the definition of local coverage can allow for a\ntheoretical understanding of this empirical phenomenon. We aim to bridge this\ngap between theoretical validation and empirical performance by proving\nachievable and interpretable guarantees for a relaxed notion of local coverage.\nBuilding on the localized CP method of Guan (2023) and the weighted CP\nframework of Tibshirani et al. (2019), we propose a new method,\nrandomly-localized conformal prediction (RLCP), which returns prediction\nintervals that are not only marginally valid but also achieve a relaxed local\ncoverage guarantee and guarantees under covariate shift. Through a series of\nsimulations and real data experiments, we validate these coverage guarantees of\nRLCP while comparing it with the other local conformal prediction methods."}, "http://arxiv.org/abs/2310.07852": {"title": "On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism", "link": "http://arxiv.org/abs/2310.07852", "description": "We consider the problem of model selection in a high-dimensional sparse\nlinear regression model under the differential privacy framework. In\nparticular, we consider the problem of differentially private best subset\nselection and study its utility guarantee. We adopt the well-known exponential\nmechanism for selecting the best model, and under a certain margin condition,\nwe establish its strong model recovery property. However, the exponential\nsearch space of the exponential mechanism poses a serious computational\nbottleneck. To overcome this challenge, we propose a Metropolis-Hastings\nalgorithm for the sampling step and establish its polynomial mixing time to its\nstationary distribution in the problem parameters $n,p$, and $s$. Furthermore,\nwe also establish approximate differential privacy for the final estimates of\nthe Metropolis-Hastings random walk using its mixing property. Finally, we also\nperform some illustrative simulations that echo the theoretical findings of our\nmain results."}, "http://arxiv.org/abs/2310.07935": {"title": "Estimating the Likelihood of Arrest from Police Records in Presence of Unreported Crimes", "link": "http://arxiv.org/abs/2310.07935", "description": "Many important policy decisions concerning policing hinge on our\nunderstanding of how likely various criminal offenses are to result in arrests.\nSince many crimes are never reported to law enforcement, estimates based on\npolice records alone must be adjusted to account for the likelihood that each\ncrime would have been reported to the police. In this paper, we present a\nmethodological framework for estimating the likelihood of arrest from police\ndata that incorporates estimates of crime reporting rates computed from a\nvictimization survey. We propose a parametric regression-based two-step\nestimator that (i) estimates the likelihood of crime reporting using logistic\nregression with survey weights; and then (ii) applies a second regression step\nto model the likelihood of arrest. Our empirical analysis focuses on racial\ndisparities in arrests for violent crimes (sex offenses, robbery, aggravated\nand simple assaults) from 2006--2015 police records from the National Incident\nBased Reporting System (NIBRS), with estimates of crime reporting obtained\nusing 2003--2020 data from the National Crime Victimization Survey (NCVS). We\nfind that, after adjusting for unreported crimes, the likelihood of arrest\ncomputed from police records decreases significantly. We also find that, while\nincidents with white offenders on average result in arrests more often than\nthose with black offenders, the disparities tend to be small after accounting\nfor crime characteristics and unreported crimes."}, "http://arxiv.org/abs/2310.07953": {"title": "Enhancing Sample Quality through Minimum Energy Importance Weights", "link": "http://arxiv.org/abs/2310.07953", "description": "Importance sampling is a powerful tool for correcting the distributional\nmismatch in many statistical and machine learning problems, but in practice its\nperformance is limited by the usage of simple proposals whose importance\nweights can be computed analytically. To address this limitation, Liu and Lee\n(2017) proposed a Black-Box Importance Sampling (BBIS) algorithm that computes\nthe importance weights for arbitrary simulated samples by minimizing the\nkernelized Stein discrepancy. However, this requires knowing the score function\nof the target distribution, which is not easy to compute for many Bayesian\nproblems. Hence, in this paper we propose another novel BBIS algorithm using\nminimum energy design, BBIS-MED, that requires only the unnormalized density\nfunction, which can be utilized as a post-processing step to improve the\nquality of Markov Chain Monte Carlo samples. We demonstrate the effectiveness\nand wide applicability of our proposed BBIS-MED algorithm on extensive\nsimulations and a real-world Bayesian model calibration problem where the score\nfunction cannot be derived analytically."}, "http://arxiv.org/abs/2310.07958": {"title": "Towards Causal Deep Learning for Vulnerability Detection", "link": "http://arxiv.org/abs/2310.07958", "description": "Deep learning vulnerability detection has shown promising results in recent\nyears. However, an important challenge that still blocks it from being very\nuseful in practice is that the model is not robust under perturbation and it\ncannot generalize well over the out-of-distribution (OOD) data, e.g., applying\na trained model to unseen projects in real world. We hypothesize that this is\nbecause the model learned non-robust features, e.g., variable names, that have\nspurious correlations with labels. When the perturbed and OOD datasets no\nlonger have the same spurious features, the model prediction fails. To address\nthe challenge, in this paper, we introduced causality into deep learning\nvulnerability detection. Our approach CausalVul consists of two phases. First,\nwe designed novel perturbations to discover spurious features that the model\nmay use to make predictions. Second, we applied the causal learning algorithms,\nspecifically, do-calculus, on top of existing deep learning models to\nsystematically remove the use of spurious features and thus promote causal\nbased prediction. Our results show that CausalVul consistently improved the\nmodel accuracy, robustness and OOD performance for all the state-of-the-art\nmodels and datasets we experimented. To the best of our knowledge, this is the\nfirst work that introduces do calculus based causal learning to software\nengineering models and shows it's indeed useful for improving the model\naccuracy, robustness and generalization. Our replication package is located at\nhttps://figshare.com/s/0ffda320dcb96c249ef2."}, "http://arxiv.org/abs/2310.07973": {"title": "Statistical Performance Guarantee for Selecting Those Predicted to Benefit Most from Treatment", "link": "http://arxiv.org/abs/2310.07973", "description": "Across a wide array of disciplines, many researchers use machine learning\n(ML) algorithms to identify a subgroup of individuals, called exceptional\nresponders, who are likely to be helped by a treatment the most. A common\napproach consists of two steps. One first estimates the conditional average\ntreatment effect or its proxy using an ML algorithm. They then determine the\ncutoff of the resulting treatment prioritization score to select those\npredicted to benefit most from the treatment. Unfortunately, these estimated\ntreatment prioritization scores are often biased and noisy. Furthermore,\nutilizing the same data to both choose a cutoff value and estimate the average\ntreatment effect among the selected individuals suffer from a multiple testing\nproblem. To address these challenges, we develop a uniform confidence band for\nexperimentally evaluating the sorted average treatment effect (GATES) among the\nindividuals whose treatment prioritization score is at least as high as any\ngiven quantile value, regardless of how the quantile is chosen. This provides a\nstatistical guarantee that the GATES for the selected subgroup exceeds a\ncertain threshold. The validity of the proposed methodology depends solely on\nrandomization of treatment and random sampling of units without requiring\nmodeling assumptions or resampling methods. This widens its applicability\nincluding a wide range of other causal quantities. A simulation study shows\nthat the empirical coverage of the proposed uniform confidence bands is close\nto the nominal coverage when the sample is as small as 100. We analyze a\nclinical trial of late-stage prostate cancer and find a relatively large\nproportion of exceptional responders with a statistical performance guarantee."}, "http://arxiv.org/abs/2310.08020": {"title": "Assessing Copula Models for Mixed Continuous-Ordinal Variables", "link": "http://arxiv.org/abs/2310.08020", "description": "Vine pair-copula constructions exist for a mix of continuous and ordinal\nvariables. In some steps, this can involve estimating a bivariate copula for a\npair of mixed continuous-ordinal variables. To assess the adequacy of copula\nfits for such a pair, diagnostic and visualization methods based on normal\nscore plots and conditional Q-Q plots are proposed. The former utilizes a\nlatent continuous variable for the ordinal variable. Using the Kullback-Leibler\ndivergence, existing probability models for mixed continuous-ordinal variable\npair are assessed for the adequacy of fit with simple parametric copula\nfamilies. The effectiveness of the proposed visualization and diagnostic\nmethods is illustrated on simulated and real datasets."}, "http://arxiv.org/abs/2310.08115": {"title": "Model-Agnostic Covariate-Assisted Inference on Partially Identified Causal Effects", "link": "http://arxiv.org/abs/2310.08115", "description": "Many causal estimands are only partially identifiable since they depend on\nthe unobservable joint distribution between potential outcomes. Stratification\non pretreatment covariates can yield sharper partial identification bounds;\nhowever, unless the covariates are discrete with relatively small support, this\napproach typically requires consistent estimation of the conditional\ndistributions of the potential outcomes given the covariates. Thus, existing\napproaches may fail under model misspecification or if consistency assumptions\nare violated. In this study, we propose a unified and model-agnostic\ninferential approach for a wide class of partially identified estimands, based\non duality theory for optimal transport problems. In randomized experiments,\nour approach can wrap around any estimates of the conditional distributions and\nprovide uniformly valid inference, even if the initial estimates are\narbitrarily inaccurate. Also, our approach is doubly robust in observational\nstudies. Notably, this property allows analysts to use the multiplier bootstrap\nto select covariates and models without sacrificing validity even if the true\nmodel is not included. Furthermore, if the conditional distributions are\nestimated at semiparametric rates, our approach matches the performance of an\noracle with perfect knowledge of the outcome model. Finally, we propose an\nefficient computational framework, enabling implementation on many practical\nproblems in causal inference."}, "http://arxiv.org/abs/2310.08193": {"title": "Are sanctions for losers? A network study of trade sanctions", "link": "http://arxiv.org/abs/2310.08193", "description": "Studies built on dependency and world-system theory using network approaches\nhave shown that international trade is structured into clusters of 'core' and\n'peripheral' countries performing distinct functions. However, few have used\nthese methods to investigate how sanctions affect the position of the countries\ninvolved in the capitalist world-economy. Yet, this topic has acquired pressing\nrelevance due to the emergence of economic warfare as a key geopolitical weapon\nsince the 1950s. And even more so in light of the preeminent role that\nsanctions have played in the US and their allies' response to the\nRussian-Ukrainian war. Applying several clustering techniques designed for\ncomplex and temporal networks, this paper shows that a shift in the pattern of\ncommerce away from sanctioning countries and towards neutral or friendly ones.\nAdditionally, there are suggestions that these shifts may lead to the creation\nof an alternative 'core' that interacts with the world-economy's periphery\nbypassing traditional 'core' countries such as EU member States and the US."}, "http://arxiv.org/abs/2310.08268": {"title": "Change point detection in dynamic heterogeneous networks via subspace tracking", "link": "http://arxiv.org/abs/2310.08268", "description": "Dynamic networks consist of a sequence of time-varying networks, and it is of\ngreat importance to detect the network change points. Most existing methods\nfocus on detecting abrupt change points, necessitating the assumption that the\nunderlying network probability matrix remains constant between adjacent change\npoints. This paper introduces a new model that allows the network probability\nmatrix to undergo continuous shifting, while the latent network structure,\nrepresented via the embedding subspace, only changes at certain time points.\nTwo novel statistics are proposed to jointly detect these network subspace\nchange points, followed by a carefully refined detection procedure.\nTheoretically, we show that the proposed method is asymptotically consistent in\nterms of change point detection, and also establish the impossibility region\nfor detecting these network subspace change points. The advantage of the\nproposed method is also supported by extensive numerical experiments on both\nsynthetic networks and a UK politician social network."}, "http://arxiv.org/abs/2310.08397": {"title": "Assessing Marine Mammal Abundance: A Novel Data Fusion", "link": "http://arxiv.org/abs/2310.08397", "description": "Marine mammals are increasingly vulnerable to human disturbance and climate\nchange. Their diving behavior leads to limited visual access during data\ncollection, making studying the abundance and distribution of marine mammals\nchallenging. In theory, using data from more than one observation modality\nshould lead to better informed predictions of abundance and distribution. With\nfocus on North Atlantic right whales, we consider the fusion of two data\nsources to inform about their abundance and distribution. The first source is\naerial distance sampling which provides the spatial locations of whales\ndetected in the region. The second source is passive acoustic monitoring (PAM),\nreturning calls received at hydrophones placed on the ocean floor. Due to\nlimited time on the surface and detection limitations arising from sampling\neffort, aerial distance sampling only provides a partial realization of\nlocations. With PAM, we never observe numbers or locations of individuals. To\naddress these challenges, we develop a novel thinned point pattern data fusion.\nOur approach leads to improved inference regarding abundance and distribution\nof North Atlantic right whales throughout Cape Cod Bay, Massachusetts in the\nUS. We demonstrate performance gains of our approach compared to that from a\nsingle source through both simulation and real data."}, "http://arxiv.org/abs/2310.08410": {"title": "Evaluation of ChatGPT-Generated Medical Responses: A Systematic Review and Meta-Analysis", "link": "http://arxiv.org/abs/2310.08410", "description": "Large language models such as ChatGPT are increasingly explored in medical\ndomains. However, the absence of standard guidelines for performance evaluation\nhas led to methodological inconsistencies. This study aims to summarize the\navailable evidence on evaluating ChatGPT's performance in medicine and provide\ndirection for future research. We searched ten medical literature databases on\nJune 15, 2023, using the keyword \"ChatGPT\". A total of 3520 articles were\nidentified, of which 60 were reviewed and summarized in this paper and 17 were\nincluded in the meta-analysis. The analysis showed that ChatGPT displayed an\noverall integrated accuracy of 56% (95% CI: 51%-60%, I2 = 87%) in addressing\nmedical queries. However, the studies varied in question resource,\nquestion-asking process, and evaluation metrics. Moreover, many studies failed\nto report methodological details, including the version of ChatGPT and whether\neach question was used independently or repeatedly. Our findings revealed that\nalthough ChatGPT demonstrated considerable potential for application in\nhealthcare, the heterogeneity of the studies and insufficient reporting may\naffect the reliability of these results. Further well-designed studies with\ncomprehensive and transparent reporting are needed to evaluate ChatGPT's\nperformance in medicine."}, "http://arxiv.org/abs/2310.08414": {"title": "Confidence bounds for the true discovery proportion based on the exact distribution of the number of rejections", "link": "http://arxiv.org/abs/2310.08414", "description": "In multiple hypotheses testing it has become widely popular to make inference\non the true discovery proportion (TDP) of a set $\\mathcal{M}$ of null\nhypotheses. This approach is useful for several application fields, such as\nneuroimaging and genomics. Several procedures to compute simultaneous lower\nconfidence bounds for the TDP have been suggested in prior literature.\nSimultaneity allows for post-hoc selection of $\\mathcal{M}$. If sets of\ninterest are specified a priori, it is possible to gain power by removing the\nsimultaneity requirement. We present an approach to compute lower confidence\nbounds for the TDP if the set of null hypotheses is defined a priori. The\nproposed method determines the bounds using the exact distribution of the\nnumber of rejections based on a step-up multiple testing procedure under\nindependence assumptions. We assess robustness properties of our procedure and\napply it to real data from the field of functional magnetic resonance imaging."}, "http://arxiv.org/abs/2310.08426": {"title": "Extensions of Heterogeneity in Integration and Prediction (HIP) with R Shiny Application", "link": "http://arxiv.org/abs/2310.08426", "description": "Multiple data views measured on the same set of participants is becoming more\ncommon and has the potential to deepen our understanding of many complex\ndiseases by analyzing these different views simultaneously. Equally important,\nmany of these complex diseases show evidence of subgroup heterogeneity (e.g.,\nby sex or race). HIP (Heterogeneity in Integration and Prediction) is among the\nfirst methods proposed to integrate multiple data views while also accounting\nfor subgroup heterogeneity to identify common and subgroup-specific markers of\na particular disease. However, HIP is applicable to continuous outcomes and\nrequires programming expertise by the user. Here we propose extensions to HIP\nthat accommodate multi-class, Poisson, and Zero-Inflated Poisson outcomes while\nretaining the benefits of HIP. Additionally, we introduce an R Shiny\napplication, accessible on shinyapps.io at\nhttps://multi-viewlearn.shinyapps.io/HIP_ShinyApp/, that provides an interface\nwith the Python implementation of HIP to allow more researchers to use the\nmethod anywhere and on any device. We applied HIP to identify genes and\nproteins common and specific to males and females that are associated with\nexacerbation frequency. Although some of the identified genes and proteins show\nevidence of a relationship with chronic obstructive pulmonary disease (COPD) in\nexisting literature, others may be candidates for future research investigating\ntheir relationship with COPD. We demonstrate the use of the Shiny application\nwith a publicly available data. An R-package for HIP would be made available at\nhttps://github.com/lasandrall/HIP."}, "http://arxiv.org/abs/2310.08479": {"title": "Personalised dynamic super learning: an application in predicting hemodiafiltration's convection volumes", "link": "http://arxiv.org/abs/2310.08479", "description": "Obtaining continuously updated predictions is a major challenge for\npersonalised medicine. Leveraging combinations of parametric regressions and\nmachine learning approaches, the personalised online super learner (POSL) can\nachieve such dynamic and personalised predictions. We adapt POSL to predict a\nrepeated continuous outcome dynamically and propose a new way to validate such\npersonalised or dynamic prediction models. We illustrate its performance by\npredicting the convection volume of patients undergoing hemodiafiltration. POSL\noutperformed its candidate learners with respect to median absolute error,\ncalibration-in-the-large, discrimination, and net benefit. We finally discuss\nthe choices and challenges underlying the use of POSL."}, "http://arxiv.org/abs/1903.00037": {"title": "Distance-Based Independence Screening for Canonical Analysis", "link": "http://arxiv.org/abs/1903.00037", "description": "This paper introduces a novel method called Distance-Based Independence\nScreening for Canonical Analysis (DISCA) that performs simultaneous dimension\nreduction for a pair of random variables by optimizing the distance covariance\n(dCov). dCov is a statistic first proposed by Sz\\'ekely et al. [2009] for\nindependence testing. Compared with sufficient dimension reduction (SDR) and\ncanonical correlation analysis (CCA)-based approaches, DISCA is a model-free\napproach that does not impose dimensional or distributional restrictions on\nvariables and is more sensitive to nonlinear relationships. Theoretically, we\nestablish a non-asymptotic error bound to provide a guarantee of our method's\nperformance. Numerically, DISCA performs comparable to or better than other\nstate-of-the-art algorithms and is computationally faster. All codes of our\nDISCA method can be found on GitHub https : //github.com/Yijin911/DISCA.git,\nincluding an R package named DISCA."}, "http://arxiv.org/abs/2105.13952": {"title": "Generalized Permutation Framework for Testing Model Variable Significance", "link": "http://arxiv.org/abs/2105.13952", "description": "A common problem in machine learning is determining if a variable\nsignificantly contributes to a model's prediction performance. This problem is\naggravated for datasets, such as gene expression datasets, that suffer the\nworst case of dimensionality: a low number of observations along with a high\nnumber of possible explanatory variables. In such scenarios, traditional\nmethods for testing variable statistical significance or constructing variable\nconfidence intervals do not apply. To address these problems, we developed a\nnovel permutation framework for testing the significance of variables in\nsupervised models. Our permutation framework has three main advantages. First,\nit is non-parametric and does not rely on distributional assumptions or\nasymptotic results. Second, it not only ranks model variables in terms of\nrelative importance, but also tests for statistical significance of each\nvariable. Third, it can test for the significance of the interaction between\nmodel variables. We applied this permutation framework to multi-class\nclassification of the Iris flower dataset and of brain regions in RNA\nexpression data, and using this framework showed variable-level statistical\nsignificance and interactions."}, "http://arxiv.org/abs/2210.02002": {"title": "Factor Augmented Sparse Throughput Deep ReLU Neural Networks for High Dimensional Regression", "link": "http://arxiv.org/abs/2210.02002", "description": "This paper introduces a Factor Augmented Sparse Throughput (FAST) model that\nutilizes both latent factors and sparse idiosyncratic components for\nnonparametric regression. The FAST model bridges factor models on one end and\nsparse nonparametric models on the other end. It encompasses structured\nnonparametric models such as factor augmented additive models and sparse\nlow-dimensional nonparametric interaction models and covers the cases where the\ncovariates do not admit factor structures. Via diversified projections as\nestimation of latent factor space, we employ truncated deep ReLU networks to\nnonparametric factor regression without regularization and to a more general\nFAST model using nonconvex regularization, resulting in factor augmented\nregression using neural network (FAR-NN) and FAST-NN estimators respectively.\nWe show that FAR-NN and FAST-NN estimators adapt to the unknown low-dimensional\nstructure using hierarchical composition models in nonasymptotic minimax rates.\nWe also study statistical learning for the factor augmented sparse additive\nmodel using a more specific neural network architecture. Our results are\napplicable to the weak dependent cases without factor structures. In proving\nthe main technical result for FAST-NN, we establish a new deep ReLU network\napproximation result that contributes to the foundation of neural network\ntheory. Our theory and methods are further supported by simulation studies and\nan application to macroeconomic data."}, "http://arxiv.org/abs/2210.04482": {"title": "Leave-group-out cross-validation for latent Gaussian models", "link": "http://arxiv.org/abs/2210.04482", "description": "Evaluating the predictive performance of a statistical model is commonly done\nusing cross-validation. Although the leave-one-out method is frequently\nemployed, its application is justified primarily for independent and\nidentically distributed observations. However, this method tends to mimic\ninterpolation rather than prediction when dealing with dependent observations.\nThis paper proposes a modified cross-validation for dependent observations.\nThis is achieved by excluding an automatically determined set of observations\nfrom the training set to mimic a more reasonable prediction scenario. Also,\nwithin the framework of latent Gaussian models, we illustrate a method to\nadjust the joint posterior for this modified cross-validation to avoid model\nrefitting. This new approach is accessible in the R-INLA package\n(www.r-inla.org)."}, "http://arxiv.org/abs/2210.11355": {"title": "Network Synthetic Interventions: A Causal Framework for Panel Data Under Network Interference", "link": "http://arxiv.org/abs/2210.11355", "description": "We propose a generalization of the synthetic controls and synthetic\ninterventions methodology to incorporate network interference. We consider the\nestimation of unit-specific potential outcomes from panel data in the presence\nof spillover across units and unobserved confounding. Key to our approach is a\nnovel latent factor model that takes into account network interference and\ngeneralizes the factor models typically used in panel data settings. We propose\nan estimator, Network Synthetic Interventions (NSI), and show that it\nconsistently estimates the mean outcomes for a unit under an arbitrary set of\ncounterfactual treatments for the network. We further establish that the\nestimator is asymptotically normal. We furnish two validity tests for whether\nthe NSI estimator reliably generalizes to produce accurate counterfactual\nestimates. We provide a novel graph-based experiment design that guarantees the\nNSI estimator produces accurate counterfactual estimates, and also analyze the\nsample complexity of the proposed design. We conclude with simulations that\ncorroborate our theoretical findings."}, "http://arxiv.org/abs/2212.01179": {"title": "Feasibility of using survey data and semi-variogram kriging to obtain bespoke indices of neighbourhood characteristics: a simulation and a case study", "link": "http://arxiv.org/abs/2212.01179", "description": "Data on neighbourhood characteristics are not typically collected in\nepidemiological studies. They are however useful in the study of small-area\nhealth inequalities. Neighbourhood characteristics are collected in some\nsurveys and could be linked to the data of other studies. We propose to use\nkriging based on semi-variogram models to predict values at non-observed\nlocations with the aim of constructing bespoke indices of neighbourhood\ncharacteristics to be linked to data from epidemiological studies. We perform a\nsimulation study to assess the feasibility of the method as well as a case\nstudy using data from the RECORD study. Apart from having enough observed data\nat small distances to the non-observed locations, a good fitting\nsemi-variogram, a larger range and the absence of nugget effects for the\nsemi-variogram models are factors leading to a higher reliability."}, "http://arxiv.org/abs/2303.17823": {"title": "An interpretable neural network-based non-proportional odds model for ordinal regression", "link": "http://arxiv.org/abs/2303.17823", "description": "This study proposes an interpretable neural network-based non-proportional\nodds model (N$^3$POM) for ordinal regression. N$^3$POM is different from\nconventional approaches to ordinal regression with non-proportional models in\nseveral ways: (1) N$^3$POM is designed to directly handle continuous responses,\nwhereas standard methods typically treat de facto ordered continuous variables\nas discrete, (2) instead of estimating response-dependent finite coefficients\nof linear models from discrete responses as is done in conventional approaches,\nwe train a non-linear neural network to serve as a coefficient function. Thanks\nto the neural network, N$^3$POM offers flexibility while preserving the\ninterpretability of conventional ordinal regression. We establish a sufficient\ncondition under which the predicted conditional cumulative probability locally\nsatisfies the monotonicity constraint over a user-specified region in the\ncovariate space. Additionally, we provide a monotonicity-preserving stochastic\n(MPS) algorithm for effectively training the neural network. We apply N$^3$POM\nto several real-world datasets."}, "http://arxiv.org/abs/2306.16335": {"title": "Emulating the dynamics of complex systems using autoregressive models on manifolds (mNARX)", "link": "http://arxiv.org/abs/2306.16335", "description": "We propose a novel surrogate modelling approach to efficiently and accurately\napproximate the response of complex dynamical systems driven by time-varying\nexogenous excitations over extended time periods. Our approach, namely manifold\nnonlinear autoregressive modelling with exogenous input (mNARX), involves\nconstructing a problem-specific exogenous input manifold that is optimal for\nconstructing autoregressive surrogates. The manifold, which forms the core of\nmNARX, is constructed incrementally by incorporating the physics of the system,\nas well as prior expert- and domain- knowledge. Because mNARX decomposes the\nfull problem into a series of smaller sub-problems, each with a lower\ncomplexity than the original, it scales well with the complexity of the\nproblem, both in terms of training and evaluation costs of the final surrogate.\nFurthermore, mNARX synergizes well with traditional dimensionality reduction\ntechniques, making it highly suitable for modelling dynamical systems with\nhigh-dimensional exogenous inputs, a class of problems that is typically\nchallenging to solve. Since domain knowledge is particularly abundant in\nphysical systems, such as those found in civil and mechanical engineering,\nmNARX is well suited for these applications. We demonstrate that mNARX\noutperforms traditional autoregressive surrogates in predicting the response of\na classical coupled spring-mass system excited by a one-dimensional random\nexcitation. Additionally, we show that mNARX is well suited for emulating very\nhigh-dimensional time- and state-dependent systems, even when affected by\nactive controllers, by surrogating the dynamics of a realistic\naero-servo-elastic onshore wind turbine simulator. In general, our results\ndemonstrate that mNARX offers promising prospects for modelling complex\ndynamical systems, in terms of accuracy and efficiency."}, "http://arxiv.org/abs/2307.02236": {"title": "D-optimal Subsampling Design for Massive Data Linear Regression", "link": "http://arxiv.org/abs/2307.02236", "description": "Data reduction is a fundamental challenge of modern technology, where\nclassical statistical methods are not applicable because of computational\nlimitations. We consider linear regression for an extraordinarily large number\nof observations, but only a few covariates. Subsampling aims at the selection\nof a given percentage of the existing original data. Under distributional\nassumptions on the covariates, we derive D-optimal subsampling designs and\nstudy their theoretical properties. We make use of fundamental concepts of\noptimal design theory and an equivalence theorem from constrained convex\noptimization. The thus obtained subsampling designs provide simple rules for\nwhether to accept or reject a data point, allowing for an easy algorithmic\nimplementation. In addition, we propose a simplified subsampling method that\ndiffers from the D-optimal design but requires lower computing time. We present\na simulation study, comparing both subsampling schemes with the IBOSS method."}, "http://arxiv.org/abs/2310.08672": {"title": "Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal", "link": "http://arxiv.org/abs/2310.08672", "description": "In many settings, interventions may be more effective for some individuals\nthan others, so that targeting interventions may be beneficial. We analyze the\nvalue of targeting in the context of a large-scale field experiment with over\n53,000 college students, where the goal was to use \"nudges\" to encourage\nstudents to renew their financial-aid applications before a non-binding\ndeadline. We begin with baseline approaches to targeting. First, we target\nbased on a causal forest that estimates heterogeneous treatment effects and\nthen assigns students to treatment according to those estimated to have the\nhighest treatment effects. Next, we evaluate two alternative targeting\npolicies, one targeting students with low predicted probability of renewing\nfinancial aid in the absence of the treatment, the other targeting those with\nhigh probability. The predicted baseline outcome is not the ideal criterion for\ntargeting, nor is it a priori clear whether to prioritize low, high, or\nintermediate predicted probability. Nonetheless, targeting on low baseline\noutcomes is common in practice, for example because the relationship between\nindividual characteristics and treatment effects is often difficult or\nimpossible to estimate with historical data. We propose hybrid approaches that\nincorporate the strengths of both predictive approaches (accurate estimation)\nand causal approaches (correct criterion); we show that targeting intermediate\nbaseline outcomes is most effective, while targeting based on low baseline\noutcomes is detrimental. In one year of the experiment, nudging all students\nimproved early filing by an average of 6.4 percentage points over a baseline\naverage of 37% filing, and we estimate that targeting half of the students\nusing our preferred policy attains around 75% of this benefit."}, "http://arxiv.org/abs/2310.08726": {"title": "Design-Based RCT Estimators and Central Limit Theorems for Baseline Subgroup and Related Analyses", "link": "http://arxiv.org/abs/2310.08726", "description": "There is a growing literature on design-based methods to estimate average\ntreatment effects (ATEs) for randomized controlled trials (RCTs) for full\nsample analyses. This article extends these methods to estimate ATEs for\ndiscrete subgroups defined by pre-treatment variables, with an application to\nan RCT testing subgroup effects for a school voucher experiment in New York\nCity. We consider ratio estimators for subgroup effects using regression\nmethods, allowing for model covariates to improve precision, and prove a finite\npopulation central limit theorem. We discuss extensions to blocked and\nclustered RCT designs, and to other common estimators with random\ntreatment-control sample sizes (or weights): post-stratification estimators,\nweighted estimators that adjust for data nonresponse, and estimators for\nBernoulli trials. We also develop simple variance estimators that share\nfeatures with robust estimators. Simulations show that the design-based\nsubgroup estimators yield confidence interval coverage near nominal levels,\neven for small subgroups."}, "http://arxiv.org/abs/2310.08798": {"title": "Alteration Detection of Tensor Dependence Structure via Sparsity-Exploited Reranking Algorithm", "link": "http://arxiv.org/abs/2310.08798", "description": "Tensor-valued data arise frequently from a wide variety of scientific\napplications, and many among them can be translated into an alteration\ndetection problem of tensor dependence structures. In this article, we\nformulate the problem under the popularly adopted tensor-normal distributions\nand aim at two-sample correlation/partial correlation comparisons of\ntensor-valued observations. Through decorrelation and centralization, a\nseparable covariance structure is employed to pool sample information from\ndifferent tensor modes to enhance the power of the test. Additionally, we\npropose a novel Sparsity-Exploited Reranking Algorithm (SERA) to further\nimprove the multiple testing efficiency. The algorithm is approached through\nreranking of the p-values derived from the primary test statistics, by\nincorporating a carefully constructed auxiliary tensor sequence. Besides the\ntensor framework, SERA is also generally applicable to a wide range of\ntwo-sample large-scale inference problems with sparsity structures, and is of\nindependent interest. The asymptotic properties of the proposed test are\nderived and the algorithm is shown to control the false discovery at the\npre-specified level. We demonstrate the efficacy of the proposed method through\nintensive simulations and two scientific applications."}, "http://arxiv.org/abs/2310.08812": {"title": "A Nonlinear Method for time series forecasting using VMD-GARCH-LSTM model", "link": "http://arxiv.org/abs/2310.08812", "description": "Time series forecasting represents a significant and challenging task across\nvarious fields. Recently, methods based on mode decomposition have dominated\nthe forecasting of complex time series because of the advantages of capturing\nlocal characteristics and extracting intrinsic modes from data. Unfortunately,\nmost models fail to capture the implied volatilities that contain significant\ninformation. To enhance the forecasting of current, rapidly evolving, and\nvolatile time series, we propose a novel decomposition-ensemble paradigm, the\nVMD-LSTM-GARCH model. The Variational Mode Decomposition algorithm is employed\nto decompose the time series into K sub-modes. Subsequently, the GARCH model\nextracts the volatility information from these sub-modes, which serve as the\ninput for the LSTM. The numerical and volatility information of each sub-mode\nis utilized to train a Long Short-Term Memory network. This network predicts\nthe sub-mode, and then we aggregate the predictions from all sub-modes to\nproduce the output. By integrating econometric and artificial intelligence\nmethods, and taking into account both the numerical and volatility information\nof the time series, our proposed model demonstrates superior performance in\ntime series forecasting, as evidenced by the significant decrease in MSE, RMSE,\nand MAPE in our comparative experimental results."}, "http://arxiv.org/abs/2310.08867": {"title": "A Survey of Methods for Handling Disk Data Imbalance", "link": "http://arxiv.org/abs/2310.08867", "description": "Class imbalance exists in many classification problems, and since the data is\ndesigned for accuracy, imbalance in data classes can lead to classification\nchallenges with a few classes having higher misclassification costs. The\nBackblaze dataset, a widely used dataset related to hard discs, has a small\namount of failure data and a large amount of health data, which exhibits a\nserious class imbalance. This paper provides a comprehensive overview of\nresearch in the field of imbalanced data classification. The discussion is\norganized into three main aspects: data-level methods, algorithmic-level\nmethods, and hybrid methods. For each type of method, we summarize and analyze\nthe existing problems, algorithmic ideas, strengths, and weaknesses.\nAdditionally, the challenges of unbalanced data classification are discussed,\nalong with strategies to address them. It is convenient for researchers to\nchoose the appropriate method according to their needs."}, "http://arxiv.org/abs/2310.08939": {"title": "Fast Screening Rules for Optimal Design via Quadratic Lasso Reformulation", "link": "http://arxiv.org/abs/2310.08939", "description": "The problems of Lasso regression and optimal design of experiments share a\ncritical property: their optimal solutions are typically \\emph{sparse}, i.e.,\nonly a small fraction of the optimal variables are non-zero. Therefore, the\nidentification of the support of an optimal solution reduces the dimensionality\nof the problem and can yield a substantial simplification of the calculations.\nIt has recently been shown that linear regression with a \\emph{squared}\n$\\ell_1$-norm sparsity-inducing penalty is equivalent to an optimal\nexperimental design problem. In this work, we use this equivalence to derive\nsafe screening rules that can be used to discard inessential samples. Compared\nto previously existing rules, the new tests are much faster to compute,\nespecially for problems involving a parameter space of high dimension, and can\nbe used dynamically within any iterative solver, with negligible computational\noverhead. Moreover, we show how an existing homotopy algorithm to compute the\nregularization path of the lasso method can be reparametrized with respect to\nthe squared $\\ell_1$-penalty. This allows the computation of a Bayes\n$c$-optimal design in a finite number of steps and can be several orders of\nmagnitude faster than standard first-order algorithms. The efficiency of the\nnew screening rules and of the homotopy algorithm are demonstrated on different\nexamples based on real data."}, "http://arxiv.org/abs/2310.09013": {"title": "Smoothed instrumental variables quantile regression", "link": "http://arxiv.org/abs/2310.09013", "description": "In this article, I introduce the sivqr command, which estimates the\ncoefficients of the instrumental variables (IV) quantile regression model\nintroduced by Chernozhukov and Hansen (2005). The sivqr command offers several\nadvantages over the existing ivqreg and ivqreg2 commands for estimating this IV\nquantile regression model, which complements the alternative \"triangular model\"\nbehind cqiv and the \"local quantile treatment effect\" model of ivqte.\nComputationally, sivqr implements the smoothed estimator of Kaplan and Sun\n(2017), who show that smoothing improves both computation time and statistical\naccuracy. Standard errors are computed analytically or by Bayesian bootstrap;\nfor non-iid sampling, sivqr is compatible with bootstrap. I discuss syntax and\nthe underlying methodology, and I compare sivqr with other commands in an\nexample."}, "http://arxiv.org/abs/2310.09100": {"title": "Time-Uniform Self-Normalized Concentration for Vector-Valued Processes", "link": "http://arxiv.org/abs/2310.09100", "description": "Self-normalized processes arise naturally in many statistical tasks. While\nself-normalized concentration has been extensively studied for scalar-valued\nprocesses, there is less work on multidimensional processes outside of the\nsub-Gaussian setting. In this work, we construct a general, self-normalized\ninequality for $\\mathbb{R}^d$-valued processes that satisfy a simple yet broad\n\"sub-$\\psi$\" tail condition, which generalizes assumptions based on cumulant\ngenerating functions. From this general inequality, we derive an upper law of\nthe iterated logarithm for sub-$\\psi$ vector-valued processes, which is tight\nup to small constants. We demonstrate applications in prototypical statistical\ntasks, such as parameter estimation in online linear regression and\nauto-regressive modeling, and bounded mean estimation via a new (multivariate)\nempirical Bernstein concentration inequality."}, "http://arxiv.org/abs/2310.09185": {"title": "Mediation Analysis using Semi-parametric Shape-Restricted Regression with Applications", "link": "http://arxiv.org/abs/2310.09185", "description": "Often linear regression is used to perform mediation analysis. However, in\nmany instances, the underlying relationships may not be linear, as in the case\nof placental-fetal hormones and fetal development. Although, the exact\nfunctional form of the relationship may be unknown, one may hypothesize the\ngeneral shape of the relationship. For these reasons, we develop a novel\nshape-restricted inference-based methodology for conducting mediation analysis.\nThis work is motivated by an application in fetal endocrinology where\nresearchers are interested in understanding the effects of pesticide\napplication on birth weight, with human chorionic gonadotropin (hCG) as the\nmediator. We assume a practically plausible set of nonlinear effects of hCG on\nthe birth weight and a linear relationship between pesticide exposure and hCG,\nwith both exposure-outcome and exposure-mediator models being linear in the\nconfounding factors. Using the proposed methodology on a population-level\nprenatal screening program data, with hCG as the mediator, we discovered that,\nwhile the natural direct effects suggest a positive association between\npesticide application and birth weight, the natural indirect effects were\nnegative."}, "http://arxiv.org/abs/2310.09214": {"title": "An Introduction to the Calibration of Computer Models", "link": "http://arxiv.org/abs/2310.09214", "description": "In the context of computer models, calibration is the process of estimating\nunknown simulator parameters from observational data. Calibration is variously\nreferred to as model fitting, parameter estimation/inference, an inverse\nproblem, and model tuning. The need for calibration occurs in most areas of\nscience and engineering, and has been used to estimate hard to measure\nparameters in climate, cardiology, drug therapy response, hydrology, and many\nother disciplines. Although the statistical method used for calibration can\nvary substantially, the underlying approach is essentially the same and can be\nconsidered abstractly. In this survey, we review the decisions that need to be\ntaken when calibrating a model, and discuss a range of computational methods\nthat can be used to compute Bayesian posterior distributions."}, "http://arxiv.org/abs/2310.09239": {"title": "Estimating weighted quantile treatment effects with missing outcome data by double sampling", "link": "http://arxiv.org/abs/2310.09239", "description": "Causal weighted quantile treatment effects (WQTE) are a useful compliment to\nstandard causal contrasts that focus on the mean when interest lies at the\ntails of the counterfactual distribution. To-date, however, methods for\nestimation and inference regarding causal WQTEs have assumed complete data on\nall relevant factors. Missing or incomplete data, however, is a widespread\nchallenge in practical settings, particularly when the data are not collected\nfor research purposes such as electronic health records and disease registries.\nFurthermore, in such settings may be particularly susceptible to the outcome\ndata being missing-not-at-random (MNAR). In this paper, we consider the use of\ndouble-sampling, through which the otherwise missing data is ascertained on a\nsub-sample of study units, as a strategy to mitigate bias due to MNAR data in\nthe estimation of causal WQTEs. With the additional data in-hand, we present\nidentifying conditions that do not require assumptions regarding missingness in\nthe original data. We then propose a novel inverse-probability weighted\nestimator and derive its' asymptotic properties, both pointwise at specific\nquantiles and uniform across a range of quantiles in (0,1), when the propensity\nscore and double-sampling probabilities are estimated. For practical inference,\nwe develop a bootstrap method that can be used for both pointwise and uniform\ninference. A simulation study is conducted to examine the finite sample\nperformance of the proposed estimators."}, "http://arxiv.org/abs/2310.09257": {"title": "A SIMPLE Approach to Provably Reconstruct Ising Model with Global Optimality", "link": "http://arxiv.org/abs/2310.09257", "description": "Reconstruction of interaction network between random events is a critical\nproblem arising from statistical physics and politics to sociology, biology,\nand psychology, and beyond. The Ising model lays the foundation for this\nreconstruction process, but finding the underlying Ising model from the least\namount of observed samples in a computationally efficient manner has been\nhistorically challenging for half a century. By using the idea of sparsity\nlearning, we present a approach named SIMPLE that has a dominant sample\ncomplexity from theoretical limit. Furthermore, a tuning-free algorithm is\ndeveloped to give a statistically consistent solution of SIMPLE in polynomial\ntime with high probability. On extensive benchmarked cases, the SIMPLE approach\nprovably reconstructs underlying Ising models with global optimality. The\napplication on the U.S. senators voting in the last six congresses reveals that\nboth the Republicans and Democrats noticeably assemble in each congresses;\ninterestingly, the assembling of Democrats is particularly pronounced in the\nlatest congress."}, "http://arxiv.org/abs/2208.09817": {"title": "High-Dimensional Composite Quantile Regression: Optimal Statistical Guarantees and Fast Algorithms", "link": "http://arxiv.org/abs/2208.09817", "description": "The composite quantile regression (CQR) was introduced by Zou and Yuan [Ann.\nStatist. 36 (2008) 1108--1126] as a robust regression method for linear models\nwith heavy-tailed errors while achieving high efficiency. Its penalized\ncounterpart for high-dimensional sparse models was recently studied in Gu and\nZou [IEEE Trans. Inf. Theory 66 (2020) 7132--7154], along with a specialized\noptimization algorithm based on the alternating direct method of multipliers\n(ADMM). Compared to the various first-order algorithms for penalized least\nsquares, ADMM-based algorithms are not well-adapted to large-scale problems. To\novercome this computational hardness, in this paper we employ a\nconvolution-smoothed technique to CQR, complemented with iteratively reweighted\n$\\ell_1$-regularization. The smoothed composite loss function is convex, twice\ncontinuously differentiable, and locally strong convex with high probability.\nWe propose a gradient-based algorithm for penalized smoothed CQR via a variant\nof the majorize-minimization principal, which gains substantial computational\nefficiency over ADMM. Theoretically, we show that the iteratively reweighted\n$\\ell_1$-penalized smoothed CQR estimator achieves near-minimax optimal\nconvergence rate under heavy-tailed errors without any moment constraint, and\nfurther achieves near-oracle convergence rate under a weaker minimum signal\nstrength condition than needed in Gu and Zou (2020). Numerical studies\ndemonstrate that the proposed method exhibits significant computational\nadvantages without compromising statistical performance compared to two\nstate-of-the-art methods that achieve robustness and high efficiency\nsimultaneously."}, "http://arxiv.org/abs/2210.14292": {"title": "Statistical Inference for H\\\"usler-Reiss Graphical Models Through Matrix Completions", "link": "http://arxiv.org/abs/2210.14292", "description": "The severity of multivariate extreme events is driven by the dependence\nbetween the largest marginal observations. The H\\\"usler-Reiss distribution is a\nversatile model for this extremal dependence, and it is usually parameterized\nby a variogram matrix. In order to represent conditional independence relations\nand obtain sparse parameterizations, we introduce the novel H\\\"usler-Reiss\nprecision matrix. Similarly to the Gaussian case, this matrix appears naturally\nin density representations of the H\\\"usler-Reiss Pareto distribution and\nencodes the extremal graphical structure through its zero pattern. For a given,\narbitrary graph we prove the existence and uniqueness of the completion of a\npartially specified H\\\"usler-Reiss variogram matrix so that its precision\nmatrix has zeros on non-edges in the graph. Using suitable estimators for the\nparameters on the edges, our theory provides the first consistent estimator of\ngraph structured H\\\"usler-Reiss distributions. If the graph is unknown, our\nmethod can be combined with recent structure learning algorithms to jointly\ninfer the graph and the corresponding parameter matrix. Based on our\nmethodology, we propose new tools for statistical inference of sparse\nH\\\"usler-Reiss models and illustrate them on large flight delay data in the\nU.S., as well as Danube river flow data."}, "http://arxiv.org/abs/2302.02288": {"title": "Efficient Adaptive Joint Significance Tests and Sobel-Type Confidence Intervals for Mediation Effects", "link": "http://arxiv.org/abs/2302.02288", "description": "Mediation analysis is an important statistical tool in many research fields.\nIts aim is to investigate the mechanism along the causal pathway between an\nexposure and an outcome. The joint significance test is widely utilized as a\nprominent statistical approach for examining mediation effects in practical\napplications. Nevertheless, the limitation of this mediation testing method\nstems from its conservative Type I error, which reduces its statistical power\nand imposes certain constraints on its popularity and utility. The proposed\nsolution to address this gap is the adaptive joint significance test for one\nmediator, a novel data-adaptive test for mediation effect that exhibits\nsignificant advancements compared to traditional joint significance test. The\nproposed method is designed to be user-friendly, eliminating the need for\ncomplicated procedures. We have derived explicit expressions for size and\npower, ensuring the theoretical validity of our approach. Furthermore, we\nextend the proposed adaptive joint significance tests for small-scale mediation\nhypotheses with family-wise error rate (FWER) control. Additionally, a novel\nadaptive Sobel-type approach is proposed for the estimation of confidence\nintervals for the mediation effects, demonstrating significant advancements\nover conventional Sobel's confidence intervals in terms of achieving desirable\ncoverage probabilities. Our mediation testing and confidence intervals\nprocedure is evaluated through comprehensive simulations, and compared with\nnumerous existing approaches. Finally, we illustrate the usefulness of our\nmethod by analysing three real-world datasets with continuous, binary and\ntime-to-event outcomes, respectively."}, "http://arxiv.org/abs/2302.02747": {"title": "Testing Quantile Forecast Optimality", "link": "http://arxiv.org/abs/2302.02747", "description": "Quantile forecasts made across multiple horizons have become an important\noutput of many financial institutions, central banks and international\norganisations. This paper proposes misspecification tests for such quantile\nforecasts that assess optimality over a set of multiple forecast horizons\nand/or quantiles. The tests build on multiple Mincer-Zarnowitz quantile\nregressions cast in a moment equality framework. Our main test is for the null\nhypothesis of autocalibration, a concept which assesses optimality with respect\nto the information contained in the forecasts themselves. We provide an\nextension that allows to test for optimality with respect to larger information\nsets and a multivariate extension. Importantly, our tests do not just inform\nabout general violations of optimality, but may also provide useful insights\ninto specific forms of sub-optimality. A simulation study investigates the\nfinite sample performance of our tests, and two empirical applications to\nfinancial returns and U.S. macroeconomic series illustrate that our tests can\nyield interesting insights into quantile forecast sub-optimality and its\ncauses."}, "http://arxiv.org/abs/2305.00700": {"title": "Double and Single Descent in Causal Inference with an Application to High-Dimensional Synthetic Control", "link": "http://arxiv.org/abs/2305.00700", "description": "Motivated by a recent literature on the double-descent phenomenon in machine\nlearning, we consider highly over-parameterized models in causal inference,\nincluding synthetic control with many control units. In such models, there may\nbe so many free parameters that the model fits the training data perfectly. We\nfirst investigate high-dimensional linear regression for imputing wage data and\nestimating average treatment effects, where we find that models with many more\ncovariates than sample size can outperform simple ones. We then document the\nperformance of high-dimensional synthetic control estimators with many control\nunits. We find that adding control units can help improve imputation\nperformance even beyond the point where the pre-treatment fit is perfect. We\nprovide a unified theoretical perspective on the performance of these\nhigh-dimensional models. Specifically, we show that more complex models can be\ninterpreted as model-averaging estimators over simpler ones, which we link to\nan improvement in average performance. This perspective yields concrete\ninsights into the use of synthetic control when control units are many relative\nto the number of pre-treatment periods."}, "http://arxiv.org/abs/2305.15742": {"title": "Counterfactual Generative Models for Time-Varying Treatments", "link": "http://arxiv.org/abs/2305.15742", "description": "Estimating the counterfactual outcome of treatment is essential for\ndecision-making in public health and clinical science, among others. Often,\ntreatments are administered in a sequential, time-varying manner, leading to an\nexponentially increased number of possible counterfactual outcomes.\nFurthermore, in modern applications, the outcomes are high-dimensional and\nconventional average treatment effect estimation fails to capture disparities\nin individuals. To tackle these challenges, we propose a novel conditional\ngenerative framework capable of producing counterfactual samples under\ntime-varying treatment, without the need for explicit density estimation. Our\nmethod carefully addresses the distribution mismatch between the observed and\ncounterfactual distributions via a loss function based on inverse probability\nweighting. We present a thorough evaluation of our method using both synthetic\nand real-world data. Our results demonstrate that our method is capable of\ngenerating high-quality counterfactual samples and outperforms the\nstate-of-the-art baselines."}, "http://arxiv.org/abs/2309.09115": {"title": "Fully Synthetic Data for Complex Surveys", "link": "http://arxiv.org/abs/2309.09115", "description": "When seeking to release public use files for confidential data, statistical\nagencies can generate fully synthetic data. We propose an approach for making\nfully synthetic data from surveys collected with complex sampling designs.\nSpecifically, we generate pseudo-populations by applying the weighted finite\npopulation Bayesian bootstrap to account for survey weights, take simple random\nsamples from those pseudo-populations, estimate synthesis models using these\nsimple random samples, and release simulated data drawn from the models as the\npublic use files. We use the framework of multiple imputation to enable\nvariance estimation using two data generation strategies. In the first, we\ngenerate multiple data sets from each simple random sample, whereas in the\nsecond, we generate a single synthetic data set from each simple random sample.\nWe present multiple imputation combining rules for each setting. We illustrate\neach approach and the repeated sampling properties of the combining rules using\nsimulation studies."}, "http://arxiv.org/abs/2309.09323": {"title": "Answering Layer 3 queries with DiscoSCMs", "link": "http://arxiv.org/abs/2309.09323", "description": "Addressing causal queries across the Pearl Causal Hierarchy (PCH) (i.e.,\nassociational, interventional and counterfactual), which is formalized as\n\\Layer{} Valuations, is a central task in contemporary causal inference\nresearch. Counterfactual questions, in particular, pose a significant challenge\nas they often necessitate a complete knowledge of structural equations. This\npaper identifies \\textbf{the degeneracy problem} caused by the consistency\nrule. To tackle this, the \\textit{Distribution-consistency Structural Causal\nModels} (DiscoSCMs) is introduced, which extends both the structural causal\nmodels (SCM) and the potential outcome framework. The correlation pattern of\npotential outcomes in personalized incentive scenarios, described by $P(y_x,\ny'_{x'})$, is used as a case study for elucidation. Although counterfactuals\nare no longer degenerate, they remain indeterminable. As a result, the\ncondition of independent potential noise is incorporated into DiscoSCM. It is\nfound that by adeptly using homogeneity, counterfactuals can be identified.\nFurthermore, more refined results are achieved in the unit problem scenario. In\nsimpler terms, when modeling counterfactuals, one should contemplate: \"Consider\na person with average ability who takes a test and, due to good luck, achieves\nan exceptionally high score. If this person were to retake the test under\nidentical external conditions, what score will he obtain? An exceptionally high\nscore or an average score?\" If your choose is predicting an average score, then\nyou are essentially choosing DiscoSCM over the traditional frameworks based on\nthe consistency rule."}, "http://arxiv.org/abs/2310.01748": {"title": "A generative approach to frame-level multi-competitor races", "link": "http://arxiv.org/abs/2310.01748", "description": "Multi-competitor races often feature complicated within-race strategies that\nare difficult to capture when training data on race outcome level data.\nFurther, models which do not account for such strategic effects may suffer from\nconfounded inferences and predictions. In this work we develop a general\ngenerative model for multi-competitor races which allows analysts to explicitly\nmodel certain strategic effects such as changing lanes or drafting and separate\nthese impacts from competitor ability. The generative model allows one to\nsimulate full races from any real or created starting position which opens new\navenues for attributing value to within-race actions and to perform\ncounter-factual analyses. This methodology is sufficiently general to apply to\nany track based multi-competitor races where both tracking data is available\nand competitor movement is well described by simultaneous forward and lateral\nmovements. We apply this methodology to one-mile horse races using data\nprovided by the New York Racing Association (NYRA) and the New York\nThoroughbred Horsemen's Association (NYTHA) for the Big Data Derby 2022 Kaggle\nCompetition. This data features granular tracking data for all horses at the\nframe-level (occurring at approximately 4hz). We demonstrate how this model can\nyield new inferences, such as the estimation of horse-specific speed profiles\nwhich vary over phases of the race, and examples of posterior predictive\ncounterfactual simulations to answer questions of interest such as starting\nlane impacts on race outcomes."}, "http://arxiv.org/abs/2310.09345": {"title": "A Unified Bayesian Framework for Modeling Measurement Error in Multinomial Data", "link": "http://arxiv.org/abs/2310.09345", "description": "Measurement error in multinomial data is a well-known and well-studied\ninferential problem that is encountered in many fields, including engineering,\nbiomedical and omics research, ecology, finance, and social sciences.\nSurprisingly, methods developed to accommodate measurement error in multinomial\ndata are typically equipped to handle false negatives or false positives, but\nnot both. We provide a unified framework for accommodating both forms of\nmeasurement error using a Bayesian hierarchical approach. We demonstrate the\nproposed method's performance on simulated data and apply it to acoustic bat\nmonitoring data."}, "http://arxiv.org/abs/2310.09384": {"title": "Modeling Missing at Random Neuropsychological Test Scores Using a Mixture of Binomial Product Experts", "link": "http://arxiv.org/abs/2310.09384", "description": "Multivariate bounded discrete data arises in many fields. In the setting of\nlongitudinal dementia studies, such data is collected when individuals complete\nneuropsychological tests. We outline a modeling and inference procedure that\ncan model the joint distribution conditional on baseline covariates, leveraging\nprevious work on mixtures of experts and latent class models. Furthermore, we\nillustrate how the work can be extended when the outcome data is missing at\nrandom using a nested EM algorithm. The proposed model can incorporate\ncovariate information, perform imputation and clustering, and infer latent\ntrajectories. We apply our model on simulated data and an Alzheimer's disease\ndata set."}, "http://arxiv.org/abs/2310.09398": {"title": "An In-Depth Examination of Requirements for Disclosure Risk Assessment", "link": "http://arxiv.org/abs/2310.09398", "description": "The use of formal privacy to protect the confidentiality of responses in the\n2020 Decennial Census of Population and Housing has triggered renewed interest\nand debate over how to measure the disclosure risks and societal benefits of\nthe published data products. Following long-established precedent in economics\nand statistics, we argue that any proposal for quantifying disclosure risk\nshould be based on pre-specified, objective criteria. Such criteria should be\nused to compare methodologies to identify those with the most desirable\nproperties. We illustrate this approach, using simple desiderata, to evaluate\nthe absolute disclosure risk framework, the counterfactual framework underlying\ndifferential privacy, and prior-to-posterior comparisons. We conclude that\nsatisfying all the desiderata is impossible, but counterfactual comparisons\nsatisfy the most while absolute disclosure risk satisfies the fewest.\nFurthermore, we explain that many of the criticisms levied against differential\nprivacy would be levied against any technology that is not equivalent to\ndirect, unrestricted access to confidential data. Thus, more research is\nneeded, but in the near-term, the counterfactual approach appears best-suited\nfor privacy-utility analysis."}, "http://arxiv.org/abs/2310.09428": {"title": "Sparse higher order partial least squares for simultaneous variable selection, dimension reduction, and tensor denoising", "link": "http://arxiv.org/abs/2310.09428", "description": "Partial Least Squares (PLS) regression emerged as an alternative to ordinary\nleast squares for addressing multicollinearity in a wide range of scientific\napplications. As multidimensional tensor data is becoming more widespread,\ntensor adaptations of PLS have been developed. Our investigations reveal that\nthe previously established asymptotic result of the PLS estimator for a tensor\nresponse breaks down as the tensor dimensions and the number of features\nincrease relative to the sample size. To address this, we propose Sparse Higher\nOrder Partial Least Squares (SHOPS) regression and an accompanying algorithm.\nSHOPS simultaneously accommodates variable selection, dimension reduction, and\ntensor association denoising. We establish the asymptotic accuracy of the SHOPS\nalgorithm under a high-dimensional regime and verify these results through\ncomprehensive simulation experiments, and applications to two contemporary\nhigh-dimensional biological data analysis."}, "http://arxiv.org/abs/2310.09493": {"title": "Summary Statistics Knockoffs Inference with Family-wise Error Rate Control", "link": "http://arxiv.org/abs/2310.09493", "description": "Testing multiple hypotheses of conditional independence with provable error\nrate control is a fundamental problem with various applications. To infer\nconditional independence with family-wise error rate (FWER) control when only\nsummary statistics of marginal dependence are accessible, we adopt\nGhostKnockoff to directly generate knockoff copies of summary statistics and\npropose a new filter to select features conditionally dependent to the response\nwith provable FWER control. In addition, we develop a computationally efficient\nalgorithm to greatly reduce the computational cost of knockoff copies\ngeneration without sacrificing power and FWER control. Experiments on simulated\ndata and a real dataset of Alzheimer's disease genetics demonstrate the\nadvantage of proposed method over the existing alternatives in both statistical\npower and computational efficiency."}, "http://arxiv.org/abs/2310.09545": {"title": "A Semiparametric Instrumented Difference-in-Differences Approach to Policy Learning", "link": "http://arxiv.org/abs/2310.09545", "description": "Recently, there has been a surge in methodological development for the\ndifference-in-differences (DiD) approach to evaluate causal effects. Standard\nmethods in the literature rely on the parallel trends assumption to identify\nthe average treatment effect on the treated. However, the parallel trends\nassumption may be violated in the presence of unmeasured confounding, and the\naverage treatment effect on the treated may not be useful in learning a\ntreatment assignment policy for the entire population. In this article, we\npropose a general instrumented DiD approach for learning the optimal treatment\npolicy. Specifically, we establish identification results using a binary\ninstrumental variable (IV) when the parallel trends assumption fails to hold.\nAdditionally, we construct a Wald estimator, novel inverse probability\nweighting (IPW) estimators, and a class of semiparametric efficient and\nmultiply robust estimators, with theoretical guarantees on consistency and\nasymptotic normality, even when relying on flexible machine learning algorithms\nfor nuisance parameters estimation. Furthermore, we extend the instrumented DiD\nto the panel data setting. We evaluate our methods in extensive simulations and\na real data application."}, "http://arxiv.org/abs/2310.09646": {"title": "Jackknife empirical likelihood confidence intervals for the categorical Gini correlation", "link": "http://arxiv.org/abs/2310.09646", "description": "The categorical Gini correlation, $\\rho_g$, was proposed by Dang et al. to\nmeasure the dependence between a categorical variable, $Y$ , and a numerical\nvariable, $X$. It has been shown that $\\rho_g$ has more appealing properties\nthan current existing dependence measurements. In this paper, we develop the\njackknife empirical likelihood (JEL) method for $\\rho_g$. Confidence intervals\nfor the Gini correlation are constructed without estimating the asymptotic\nvariance. Adjusted and weighted JEL are explored to improve the performance of\nthe standard JEL. Simulation studies show that our methods are competitive to\nexisting methods in terms of coverage accuracy and shortness of confidence\nintervals. The proposed methods are illustrated in an application on two real\ndatasets."}, "http://arxiv.org/abs/2310.09673": {"title": "Robust Quickest Change Detection in Non-Stationary Processes", "link": "http://arxiv.org/abs/2310.09673", "description": "Optimal algorithms are developed for robust detection of changes in\nnon-stationary processes. These are processes in which the distribution of the\ndata after change varies with time. The decision-maker does not have access to\nprecise information on the post-change distribution. It is shown that if the\npost-change non-stationary family has a distribution that is least favorable in\na well-defined sense, then the algorithms designed using the least favorable\ndistributions are robust and optimal. Non-stationary processes are encountered\nin public health monitoring and space and military applications. The robust\nalgorithms are applied to real and simulated data to show their effectiveness."}, "http://arxiv.org/abs/2310.09701": {"title": "A powerful empirical Bayes approach for high dimensional replicability analysis", "link": "http://arxiv.org/abs/2310.09701", "description": "Identifying replicable signals across different studies provides stronger\nscientific evidence and more powerful inference. Existing literature on high\ndimensional applicability analysis either imposes strong modeling assumptions\nor has low power. We develop a powerful and robust empirical Bayes approach for\nhigh dimensional replicability analysis. Our method effectively borrows\ninformation from different features and studies while accounting for\nheterogeneity. We show that the proposed method has better power than competing\nmethods while controlling the false discovery rate, both empirically and\ntheoretically. Analyzing datasets from the genome-wide association studies\nreveals new biological insights that otherwise cannot be obtained by using\nexisting methods."}, "http://arxiv.org/abs/2310.09702": {"title": "Inference with Mondrian Random Forests", "link": "http://arxiv.org/abs/2310.09702", "description": "Random forests are popular methods for classification and regression, and\nmany different variants have been proposed in recent years. One interesting\nexample is the Mondrian random forest, in which the underlying trees are\nconstructed according to a Mondrian process. In this paper we give a central\nlimit theorem for the estimates made by a Mondrian random forest in the\nregression setting. When combined with a bias characterization and a consistent\nvariance estimator, this allows one to perform asymptotically valid statistical\ninference, such as constructing confidence intervals, on the unknown regression\nfunction. We also provide a debiasing procedure for Mondrian random forests\nwhich allows them to achieve minimax-optimal estimation rates with\n$\\beta$-H\\\"older regression functions, for all $\\beta$ and in arbitrary\ndimension, assuming appropriate parameter tuning."}, "http://arxiv.org/abs/2310.09818": {"title": "MCMC for Bayesian nonparametric mixture modeling under differential privacy", "link": "http://arxiv.org/abs/2310.09818", "description": "Estimating the probability density of a population while preserving the\nprivacy of individuals in that population is an important and challenging\nproblem that has received considerable attention in recent years. While the\nprevious literature focused on frequentist approaches, in this paper, we\npropose a Bayesian nonparametric mixture model under differential privacy (DP)\nand present two Markov chain Monte Carlo (MCMC) algorithms for posterior\ninference. One is a marginal approach, resembling Neal's algorithm 5 with a\npseudo-marginal Metropolis-Hastings move, and the other is a conditional\napproach. Although our focus is primarily on local DP, we show that our MCMC\nalgorithms can be easily extended to deal with global differential privacy\nmechanisms. Moreover, for certain classes of mechanisms and mixture kernels, we\nshow how standard algorithms can be employed, resulting in substantial\nefficiency gains. Our approach is general and applicable to any mixture model\nand privacy mechanism. In several simulations and a real case study, we discuss\nthe performance of our algorithms and evaluate different privacy mechanisms\nproposed in the frequentist literature."}, "http://arxiv.org/abs/2310.09955": {"title": "On the Statistical Foundations of H-likelihood for Unobserved Random Variables", "link": "http://arxiv.org/abs/2310.09955", "description": "The maximum likelihood estimation is widely used for statistical inferences.\nIn this study, we reformulate the h-likelihood proposed by Lee and Nelder in\n1996, whose maximization yields maximum likelihood estimators for fixed\nparameters and asymptotically best unbiased predictors for random parameters.\nWe establish the statistical foundations for h-likelihood theories, which\nextend classical likelihood theories to embrace broad classes of statistical\nmodels with random parameters. The maximum h-likelihood estimators\nasymptotically achieve the generalized Cramer-Rao lower bound. Furthermore, we\nexplore asymptotic theory when the consistency of either fixed parameter\nestimation or random parameter prediction is violated. The introduction of this\nnew h-likelihood framework enables likelihood theories to cover inferences for\na much broader class of models, while also providing computationally efficient\nfitting algorithms to give asymptotically optimal estimators for fixed\nparameters and predictors for random parameters."}, "http://arxiv.org/abs/2310.09960": {"title": "Point Mass in the Confidence Distribution: Is it a Drawback or an Advantage?", "link": "http://arxiv.org/abs/2310.09960", "description": "Stein's (1959) problem highlights the phenomenon called the probability\ndilution in high dimensional cases, which is known as a fundamental deficiency\nin probabilistic inference. The satellite conjunction problem also suffers from\nprobability dilution that poor-quality data can lead to a dilution of collision\nprobability. Though various methods have been proposed, such as generalized\nfiducial distribution and the reference posterior, they could not maintain the\ncoverage probability of confidence intervals (CIs) in both problems. On the\nother hand, the confidence distribution (CD) has a point mass at zero, which\nhas been interpreted paradoxical. However, we show that this point mass is an\nadvantage rather than a drawback, because it gives a way to maintain the\ncoverage probability of CIs. More recently, `false confidence theorem' was\npresented as another deficiency in probabilistic inferences, called the false\nconfidence. It was further claimed that the use of consonant belief can\nmitigate this deficiency. However, we show that the false confidence theorem\ncannot be applied to the CD in both Stein's and satellite conjunction problems.\nIt is crucial that a confidence feature, not a consonant one, is the key to\novercome the deficiencies in probabilistic inferences. Our findings reveal that\nthe CD outperforms the other existing methods, including the consonant belief,\nin the context of Stein's and satellite conjunction problems. Additionally, we\ndemonstrate the ambiguity of coverage probability in an observed CI from the\nfrequentist CI procedure, and show that the CD provides valuable information\nregarding this ambiguity."}, "http://arxiv.org/abs/2310.09961": {"title": "Theoretical Evaluation of Asymmetric Shapley Values for Root-Cause Analysis", "link": "http://arxiv.org/abs/2310.09961", "description": "In this work, we examine Asymmetric Shapley Values (ASV), a variant of the\npopular SHAP additive local explanation method. ASV proposes a way to improve\nmodel explanations incorporating known causal relations between variables, and\nis also considered as a way to test for unfair discrimination in model\npredictions. Unexplored in previous literature, relaxing symmetry in Shapley\nvalues can have counter-intuitive consequences for model explanation. To better\nunderstand the method, we first show how local contributions correspond to\nglobal contributions of variance reduction. Using variance, we demonstrate\nmultiple cases where ASV yields counter-intuitive attributions, arguably\nproducing incorrect results for root-cause analysis. Second, we identify\ngeneralized additive models (GAM) as a restricted class for which ASV exhibits\ndesirable properties. We support our arguments by proving multiple theoretical\nresults about the method. Finally, we demonstrate the use of asymmetric\nattributions on multiple real-world datasets, comparing the results with and\nwithout restricted model families using gradient boosting and deep learning\nmodels."}, "http://arxiv.org/abs/2310.10003": {"title": "Conformal Contextual Robust Optimization", "link": "http://arxiv.org/abs/2310.10003", "description": "Data-driven approaches to predict-then-optimize decision-making problems seek\nto mitigate the risk of uncertainty region misspecification in safety-critical\nsettings. Current approaches, however, suffer from considering overly\nconservative uncertainty regions, often resulting in suboptimal decisionmaking.\nTo this end, we propose Conformal-Predict-Then-Optimize (CPO), a framework for\nleveraging highly informative, nonconvex conformal prediction regions over\nhigh-dimensional spaces based on conditional generative models, which have the\ndesired distribution-free coverage guarantees. Despite guaranteeing robustness,\nsuch black-box optimization procedures alone inspire little confidence owing to\nthe lack of explanation of why a particular decision was found to be optimal.\nWe, therefore, augment CPO to additionally provide semantically meaningful\nvisual summaries of the uncertainty regions to give qualitative intuition for\nthe optimal decision. We highlight the CPO framework by demonstrating results\non a suite of simulation-based inference benchmark tasks and a vehicle routing\ntask based on probabilistic weather prediction."}, "http://arxiv.org/abs/2310.10048": {"title": "Evaluation of transplant benefits with the U", "link": "http://arxiv.org/abs/2310.10048", "description": "Kidney transplantation is the most effective renal replacement therapy for\nend stage renal disease patients. With the severe shortage of kidney supplies\nand for the clinical effectiveness of transplantation, patient's life\nexpectancy post transplantation is used to prioritize patients for\ntransplantation; however, severe comorbidity conditions and old age are the\nmost dominant factors that negatively impact post-transplantation life\nexpectancy, effectively precluding sick or old patients from receiving\ntransplants. It would be crucial to design objective measures to quantify the\ntransplantation benefit by comparing the mean residual life with and without a\ntransplant, after adjusting for comorbidity and demographic conditions. To\naddress this urgent need, we propose a new class of semiparametric\ncovariate-dependent mean residual life models. Our method estimates covariate\neffects semiparametrically efficiently and the mean residual life function\nnonparametrically, enabling us to predict the residual life increment potential\nfor any given patient. Our method potentially leads to a more fair system that\nprioritizes patients who would have the largest residual life gains. Our\nanalysis of the kidney transplant data from the U.S. Scientific Registry of\nTransplant Recipients also suggests that a single index of covariates summarize\nwell the impacts of multiple covariates, which may facilitate interpretations\nof each covariate's effect. Our subgroup analysis further disclosed\ninequalities in survival gains across groups defined by race, gender and\ninsurance type (reflecting socioeconomic status)."}, "http://arxiv.org/abs/2310.10052": {"title": "Group-Orthogonal Subsampling for Hierarchical Data Based on Linear Mixed Models", "link": "http://arxiv.org/abs/2310.10052", "description": "Hierarchical data analysis is crucial in various fields for making\ndiscoveries. The linear mixed model is often used for training hierarchical\ndata, but its parameter estimation is computationally expensive, especially\nwith big data. Subsampling techniques have been developed to address this\nchallenge. However, most existing subsampling methods assume homogeneous data\nand do not consider the possible heterogeneity in hierarchical data. To address\nthis limitation, we develop a new approach called group-orthogonal subsampling\n(GOSS) for selecting informative subsets of hierarchical data that may exhibit\nheterogeneity. GOSS selects subdata with balanced data size among groups and\ncombinatorial orthogonality within each group, resulting in subdata that are\n$D$- and $A$-optimal for building linear mixed models. Estimators of parameters\ntrained on GOSS subdata are consistent and asymptotically normal. GOSS is shown\nto be numerically appealing via simulations and a real data application.\nTheoretical proofs, R codes, and supplementary numerical results are accessible\nonline as Supplementary Materials."}, "http://arxiv.org/abs/2310.10239": {"title": "Structural transfer learning of non-Gaussian DAG", "link": "http://arxiv.org/abs/2310.10239", "description": "Directed acyclic graph (DAG) has been widely employed to represent\ndirectional relationships among a set of collected nodes. Yet, the available\ndata in one single study is often limited for accurate DAG reconstruction,\nwhereas heterogeneous data may be collected from multiple relevant studies. It\nremains an open question how to pool the heterogeneous data together for better\nDAG structure reconstruction in the target study. In this paper, we first\nintroduce a novel set of structural similarity measures for DAG and then\npresent a transfer DAG learning framework by effectively leveraging information\nfrom auxiliary DAGs of different levels of similarities. Our theoretical\nanalysis shows substantial improvement in terms of DAG reconstruction in the\ntarget study, even when no auxiliary DAG is overall similar to the target DAG,\nwhich is in sharp contrast to most existing transfer learning methods. The\nadvantage of the proposed transfer DAG learning is also supported by extensive\nnumerical experiments on both synthetic data and multi-site brain functional\nconnectivity network data."}, "http://arxiv.org/abs/2310.10271": {"title": "A geometric power analysis for general log-linear models", "link": "http://arxiv.org/abs/2310.10271", "description": "General log-linear models are widely used to express the association in\nmultivariate frequency data on contingency tables. The paper focuses on the\npower analysis for testing the goodness-of-fit hypothesis for these models.\nConventionally, for the power-related sample size calculations a deviation from\nthe null hypothesis, aka effect size, is specified by means of the chi-square\ngoodness-of-fit index. It is argued that the odds ratio is a more natural\nmeasure of effect size, with the advantage of having a data-relevant\ninterpretation. Therefore, a class of log-affine models that are specified by\nodds ratios whose values deviate from those of the null by a small amount can\nbe chosen as an alternative. Being expressed as sets of constraints on odds\nratios, both hypotheses are represented by smooth surfaces in the probability\nsimplex, and thus, the power analysis can be given a geometric interpretation\nas well. A concept of geometric power is introduced and a Monte-Carlo algorithm\nfor its estimation is proposed. The framework is applied to the power analysis\nof goodness-of-fit in the context of multinomial sampling. An iterative scaling\nprocedure for generating distributions from a log-affine model is described and\nits convergence is proved. To illustrate, the geometric power analysis is\ncarried out for data from a clinical study."}, "http://arxiv.org/abs/2310.10324": {"title": "Assessing univariate and bivariate risks of late-frost and drought using vine copulas: A historical study for Bavaria", "link": "http://arxiv.org/abs/2310.10324", "description": "In light of climate change's impacts on forests, including extreme drought\nand late-frost, leading to vitality decline and regional forest die-back, we\nassess univariate drought and late-frost risks and perform a joint risk\nanalysis in Bavaria, Germany, from 1952 to 2020. Utilizing a vast dataset with\n26 bioclimatic and topographic variables, we employ vine copula models due to\nthe data's non-Gaussian and asymmetric dependencies. We use D-vine regression\nfor univariate and Y-vine regression for bivariate analysis, and propose\ncorresponding univariate and bivariate conditional probability risk measures.\nWe identify \"at-risk\" regions, emphasizing the need for forest adaptation due\nto climate change."}, "http://arxiv.org/abs/2310.10329": {"title": "Towards Data-Conditional Simulation for ABC Inference in Stochastic Differential Equations", "link": "http://arxiv.org/abs/2310.10329", "description": "We develop a Bayesian inference method for discretely-observed stochastic\ndifferential equations (SDEs). Inference is challenging for most SDEs, due to\nthe analytical intractability of the likelihood function. Nevertheless, forward\nsimulation via numerical methods is straightforward, motivating the use of\napproximate Bayesian computation (ABC). We propose a conditional simulation\nscheme for SDEs that is based on lookahead strategies for sequential Monte\nCarlo (SMC) and particle smoothing using backward simulation. This leads to the\nsimulation of trajectories that are consistent with the observed trajectory,\nthereby increasing the ABC acceptance rate. We additionally employ an invariant\nneural network, previously developed for Markov processes, to learn the summary\nstatistics function required in ABC. The neural network is incrementally\nretrained by exploiting an ABC-SMC sampler, which provides new training data at\neach round. Since the SDE simulation scheme differs from standard forward\nsimulation, we propose a suitable importance sampling correction, which has the\nadded advantage of guiding the parameters towards regions of high posterior\ndensity, especially in the first ABC-SMC round. Our approach achieves accurate\ninference and is about three times faster than standard (forward-only) ABC-SMC.\nWe illustrate our method in four simulation studies, including three examples\nfrom the Chan-Karaolyi-Longstaff-Sanders SDE family."}, "http://arxiv.org/abs/2310.10331": {"title": "Specifications tests for count time series models with covariates", "link": "http://arxiv.org/abs/2310.10331", "description": "We propose a goodness-of-fit test for a class of count time series models\nwith covariates which includes the Poisson autoregressive model with covariates\n(PARX) as a special case. The test criteria are derived from a specific\ncharacterization for the conditional probability generating function and the\ntest statistic is formulated as a $L_2$ weighting norm of the corresponding\nsample counterpart. The asymptotic properties of the proposed test statistic\nare provided under the null hypothesis as well as under specific alternatives.\nA bootstrap version of the test is explored in a Monte--Carlo study and\nillustrated on a real data set on road safety."}, "http://arxiv.org/abs/2310.10373": {"title": "False Discovery Proportion control for aggregated Knockoffs", "link": "http://arxiv.org/abs/2310.10373", "description": "Controlled variable selection is an important analytical step in various\nscientific fields, such as brain imaging or genomics. In these high-dimensional\ndata settings, considering too many variables leads to poor models and high\ncosts, hence the need for statistical guarantees on false positives. Knockoffs\nare a popular statistical tool for conditional variable selection in high\ndimension. However, they control for the expected proportion of false\ndiscoveries (FDR) and not their actual proportion (FDP). We present a new\nmethod, KOPI, that controls the proportion of false discoveries for\nKnockoff-based inference. The proposed method also relies on a new type of\naggregation to address the undesirable randomness associated with classical\nKnockoff inference. We demonstrate FDP control and substantial power gains over\nexisting Knockoff-based methods in various simulation settings and achieve good\nsensitivity/specificity tradeoffs on brain imaging and genomic data."}, "http://arxiv.org/abs/2310.10393": {"title": "Statistical and Causal Robustness for Causal Null Hypothesis Tests", "link": "http://arxiv.org/abs/2310.10393", "description": "Prior work applying semiparametric theory to causal inference has primarily\nfocused on deriving estimators that exhibit statistical robustness under a\nprespecified causal model that permits identification of a desired causal\nparameter. However, a fundamental challenge is correct specification of such a\nmodel, which usually involves making untestable assumptions. Evidence factors\nis an approach to combining hypothesis tests of a common causal null hypothesis\nunder two or more candidate causal models. Under certain conditions, this\nyields a test that is valid if at least one of the underlying models is\ncorrect, which is a form of causal robustness. We propose a method of combining\nsemiparametric theory with evidence factors. We develop a causal null\nhypothesis test based on joint asymptotic normality of K asymptotically linear\nsemiparametric estimators, where each estimator is based on a distinct\nidentifying functional derived from each of K candidate causal models. We show\nthat this test provides both statistical and causal robustness in the sense\nthat it is valid if at least one of the K proposed causal models is correct,\nwhile also allowing for slower than parametric rates of convergence in\nestimating nuisance functions. We demonstrate the efficacy of our method via\nsimulations and an application to the Framingham Heart Study."}, "http://arxiv.org/abs/2310.10407": {"title": "Ensemble methods for testing a global null", "link": "http://arxiv.org/abs/2310.10407", "description": "Testing a global null is a canonical problem in statistics and has a wide\nrange of applications. In view of the fact that no uniformly most powerful test\nexists, prior and/or domain knowledge are commonly used to focus on a certain\nclass of alternatives to improve the testing power. However, it is generally\nchallenging to develop tests that are particularly powerful against a certain\nclass of alternatives. In this paper, motivated by the success of ensemble\nlearning methods for prediction or classification, we propose an ensemble\nframework for testing that mimics the spirit of random forests to deal with the\nchallenges. Our ensemble testing framework aggregates a collection of weak base\ntests to form a final ensemble test that maintains strong and robust power for\nglobal nulls. We apply the framework to four problems about global testing in\ndifferent classes of alternatives arising from Whole Genome Sequencing (WGS)\nassociation studies. Specific ensemble tests are proposed for each of these\nproblems, and their theoretical optimality is established in terms of Bahadur\nefficiency. Extensive simulations and an analysis of a real WGS dataset are\nconducted to demonstrate the type I error control and/or power gain of the\nproposed ensemble tests."}, "http://arxiv.org/abs/2310.10422": {"title": "A Neural Network-Based Approach to Normality Testing for Dependent Data", "link": "http://arxiv.org/abs/2310.10422", "description": "There is a wide availability of methods for testing normality under the\nassumption of independent and identically distributed data. When data are\ndependent in space and/or time, however, assessing and testing the marginal\nbehavior is considerably more challenging, as the marginal behavior is impacted\nby the degree of dependence. We propose a new approach to assess normality for\ndependent data by non-linearly incorporating existing statistics from normality\ntests as well as sample moments such as skewness and kurtosis through a neural\nnetwork. We calibrate (deep) neural networks by simulated normal and non-normal\ndata with a wide range of dependence structures and we determine the\nprobability of rejecting the null hypothesis. We compare several approaches for\nnormality tests and demonstrate the superiority of our method in terms of\nstatistical power through an extensive simulation study. A real world\napplication to global temperature data further demonstrates how the degree of\nspatio-temporal aggregation affects the marginal normality in the data."}, "http://arxiv.org/abs/2310.10494": {"title": "Multivariate Scalar on Multidimensional Distribution Regression", "link": "http://arxiv.org/abs/2310.10494", "description": "We develop a new method for multivariate scalar on multidimensional\ndistribution regression. Traditional approaches typically analyze isolated\nunivariate scalar outcomes or consider unidimensional distributional\nrepresentations as predictors. However, these approaches are sub-optimal\nbecause: i) they fail to utilize the dependence between the distributional\npredictors: ii) neglect the correlation structure of the response. To overcome\nthese limitations, we propose a multivariate distributional analysis framework\nthat harnesses the power of multivariate density functions and multitask\nlearning. We develop a computationally efficient semiparametric estimation\nmethod for modelling the effect of the latent joint density on multivariate\nresponse of interest. Additionally, we introduce a new conformal algorithm for\nquantifying the uncertainty of regression models with multivariate responses\nand distributional predictors, providing valuable insights into the conditional\ndistribution of the response. We have validated the effectiveness of our\nproposed method through comprehensive numerical simulations, clearly\ndemonstrating its superior performance compared to traditional methods. The\napplication of the proposed method is demonstrated on tri-axial accelerometer\ndata from the National Health and Nutrition Examination Survey (NHANES)\n2011-2014 for modelling the association between cognitive scores across various\ndomains and distributional representation of physical activity among older\nadult population. Our results highlight the advantages of the proposed\napproach, emphasizing the significance of incorporating complete spatial\ninformation derived from the accelerometer device."}, "http://arxiv.org/abs/2310.10588": {"title": "Max-convolution processes with random shape indicator kernels", "link": "http://arxiv.org/abs/2310.10588", "description": "In this paper, we introduce a new class of models for spatial data obtained\nfrom max-convolution processes based on indicator kernels with random shape. We\nshow that this class of models have appealing dependence properties including\ntail dependence at short distances and independence at long distances. We\nfurther consider max-convolutions between such processes and processes with\ntail independence, in order to separately control the bulk and tail dependence\nbehaviors, and to increase flexibility of the model at longer distances, in\nparticular, to capture intermediate tail dependence. We show how parameters can\nbe estimated using a weighted pairwise likelihood approach, and we conduct an\nextensive simulation study to show that the proposed inference approach is\nfeasible in high dimensions and it yields accurate parameter estimates in most\ncases. We apply the proposed methodology to analyse daily temperature maxima\nmeasured at 100 monitoring stations in the state of Oklahoma, US. Our results\nindicate that our proposed model provides a good fit to the data, and that it\ncaptures both the bulk and the tail dependence structures accurately."}, "http://arxiv.org/abs/1805.07301": {"title": "Enhanced Pricing and Management of Bundled Insurance Risks with Dependence-aware Prediction using Pair Copula Construction", "link": "http://arxiv.org/abs/1805.07301", "description": "We propose a dependence-aware predictive modeling framework for multivariate\nrisks stemmed from an insurance contract with bundling features - an important\ntype of policy increasingly offered by major insurance companies. The bundling\nfeature naturally leads to longitudinal measurements of multiple insurance\nrisks, and correct pricing and management of such risks is of fundamental\ninterest to financial stability of the macroeconomy. We build a novel\npredictive model that fully captures the dependence among the multivariate\nrepeated risk measurements. Specifically, the longitudinal measurement of each\nindividual risk is first modeled using pair copula construction with a D-vine\nstructure, and the multiple D-vines are then integrated by a flexible copula.\nThe proposed model provides a unified modeling framework for multivariate\nlongitudinal data that can accommodate different scales of measurements,\nincluding continuous, discrete, and mixed observations, and thus can be\npotentially useful for various economic studies. A computationally efficient\nsequential method is proposed for model estimation and inference, and its\nperformance is investigated both theoretically and via simulation studies. In\nthe application, we examine multivariate bundled risks in multi-peril property\ninsurance using proprietary data from a commercial property insurance provider.\nThe proposed model is found to provide improved decision making for several key\ninsurance operations. For underwriting, we show that the experience rate priced\nby the proposed model leads to a 9% lift in the insurer's net revenue. For\nreinsurance, we show that the insurer underestimates the risk of the retained\ninsurance portfolio by 10% when ignoring the dependence among bundled insurance\nrisks."}, "http://arxiv.org/abs/2005.04721": {"title": "Decision Making in Drug Development via Inference on Power", "link": "http://arxiv.org/abs/2005.04721", "description": "A typical power calculation is performed by replacing unknown\npopulation-level quantities in the power function with what is observed in\nexternal studies. Many authors and practitioners view this as an assumed value\nof power and offer the Bayesian quantity probability of success or assurance as\nan alternative. The claim is by averaging over a prior or posterior\ndistribution, probability of success transcends power by capturing the\nuncertainty around the unknown true treatment effect and any other\npopulation-level parameters. We use p-value functions to frame both the\nprobability of success calculation and the typical power calculation as merely\nproducing two different point estimates of power. We demonstrate that Go/No-Go\ndecisions based on either point estimate of power do not adequately quantify\nand control the risk involved, and instead we argue for Go/No-Go decisions that\nutilize inference on power for better risk management and decision making."}, "http://arxiv.org/abs/2103.00674": {"title": "BEAUTY Powered BEAST", "link": "http://arxiv.org/abs/2103.00674", "description": "We study distribution-free goodness-of-fit tests with the proposed Binary\nExpansion Approximation of UniformiTY (BEAUTY) approach. This method\ngeneralizes the renowned Euler's formula, and approximates the characteristic\nfunction of any copula through a linear combination of expectations of binary\ninteractions from marginal binary expansions. This novel theory enables a\nunification of many important tests of independence via approximations from\nspecific quadratic forms of symmetry statistics, where the deterministic weight\nmatrix characterizes the power properties of each test. To achieve a robust\npower, we examine test statistics with data-adaptive weights, referred to as\nthe Binary Expansion Adaptive Symmetry Test (BEAST). Using properties of the\nbinary expansion filtration, we demonstrate that the Neyman-Pearson test of\nuniformity can be approximated by an oracle weighted sum of symmetry\nstatistics. The BEAST with this oracle provides a useful benchmark of feasible\npower. To approach this oracle power, we devise the BEAST through a regularized\nresampling approximation of the oracle test. The BEAST improves the empirical\npower of many existing tests against a wide spectrum of common alternatives and\ndelivers a clear interpretation of dependency forms when significant."}, "http://arxiv.org/abs/2103.16159": {"title": "Controlling the False Discovery Rate in Transformational Sparsity: Split Knockoffs", "link": "http://arxiv.org/abs/2103.16159", "description": "Controlling the False Discovery Rate (FDR) in a variable selection procedure\nis critical for reproducible discoveries, and it has been extensively studied\nin sparse linear models. However, it remains largely open in scenarios where\nthe sparsity constraint is not directly imposed on the parameters but on a\nlinear transformation of the parameters to be estimated. Examples of such\nscenarios include total variations, wavelet transforms, fused LASSO, and trend\nfiltering. In this paper, we propose a data-adaptive FDR control method, called\nthe Split Knockoff method, for this transformational sparsity setting. The\nproposed method exploits both variable and data splitting. The linear\ntransformation constraint is relaxed to its Euclidean proximity in a lifted\nparameter space, which yields an orthogonal design that enables the orthogonal\nSplit Knockoff construction. To overcome the challenge that exchangeability\nfails due to the heterogeneous noise brought by the transformation, new inverse\nsupermartingale structures are developed via data splitting for provable FDR\ncontrol without sacrificing power. Simulation experiments demonstrate that the\nproposed methodology achieves the desired FDR and power. We also provide an\napplication to Alzheimer's Disease study, where atrophy brain regions and their\nabnormal connections can be discovered based on a structural Magnetic Resonance\nImaging dataset (ADNI)."}, "http://arxiv.org/abs/2201.05967": {"title": "Uniform Inference for Kernel Density Estimators with Dyadic Data", "link": "http://arxiv.org/abs/2201.05967", "description": "Dyadic data is often encountered when quantities of interest are associated\nwith the edges of a network. As such it plays an important role in statistics,\neconometrics and many other data science disciplines. We consider the problem\nof uniformly estimating a dyadic Lebesgue density function, focusing on\nnonparametric kernel-based estimators taking the form of dyadic empirical\nprocesses. Our main contributions include the minimax-optimal uniform\nconvergence rate of the dyadic kernel density estimator, along with strong\napproximation results for the associated standardized and Studentized\n$t$-processes. A consistent variance estimator enables the construction of\nvalid and feasible uniform confidence bands for the unknown density function.\nWe showcase the broad applicability of our results by developing novel\ncounterfactual density estimation and inference methodology for dyadic data,\nwhich can be used for causal inference and program evaluation. A crucial\nfeature of dyadic distributions is that they may be \"degenerate\" at certain\npoints in the support of the data, a property making our analysis somewhat\ndelicate. Nonetheless our methods for uniform inference remain robust to the\npotential presence of such points. For implementation purposes, we discuss\ninference procedures based on positive semi-definite covariance estimators,\nmean squared error optimal bandwidth selectors and robust bias correction\ntechniques. We illustrate the empirical finite-sample performance of our\nmethods both in simulations and with real-world trade data, for which we make\ncomparisons between observed and counterfactual trade distributions in\ndifferent years. Our technical results concerning strong approximations and\nmaximal inequalities are of potential independent interest."}, "http://arxiv.org/abs/2206.01076": {"title": "Likelihood-based Inference for Random Networks with Changepoints", "link": "http://arxiv.org/abs/2206.01076", "description": "Generative, temporal network models play an important role in analyzing the\ndependence structure and evolution patterns of complex networks. Due to the\ncomplicated nature of real network data, it is often naive to assume that the\nunderlying data-generative mechanism itself is invariant with time. Such\nobservation leads to the study of changepoints or sudden shifts in the\ndistributional structure of the evolving network. In this paper, we propose a\nlikelihood-based methodology to detect changepoints in undirected, affine\npreferential attachment networks, and establish a hypothesis testing framework\nto detect a single changepoint, together with a consistent estimator for the\nchangepoint. Such results require establishing consistency and asymptotic\nnormality of the MLE under the changepoint regime, which suffers from long\nrange dependence. The methodology is then extended to the multiple changepoint\nsetting via both a sliding window method and a more computationally efficient\nscore statistic. We also compare the proposed methodology with previously\ndeveloped non-parametric estimators of the changepoint via simulation, and the\nmethods developed herein are applied to modeling the popularity of a topic in a\nTwitter network over time."}, "http://arxiv.org/abs/2301.01616": {"title": "Locally Private Causal Inference for Randomized Experiments", "link": "http://arxiv.org/abs/2301.01616", "description": "Local differential privacy is a differential privacy paradigm in which\nindividuals first apply a privacy mechanism to their data (often by adding\nnoise) before transmitting the result to a curator. The noise for privacy\nresults in additional bias and variance in their analyses. Thus it is of great\nimportance for analysts to incorporate the privacy noise into valid inference.\nIn this article, we develop methodologies to infer causal effects from locally\nprivatized data under randomized experiments. First, we present frequentist\nestimators under various privacy scenarios with their variance estimators and\nplug-in confidence intervals. We show a na\\\"ive debiased estimator results in\ninferior mean-squared error (MSE) compared to minimax lower bounds. In\ncontrast, we show that using a customized privacy mechanism, we can match the\nlower bound, giving minimax optimal inference. We also develop a Bayesian\nnonparametric methodology along with a blocked Gibbs sampling algorithm, which\ncan be applied to any of our proposed privacy mechanisms, and which performs\nespecially well in terms of MSE for tight privacy budgets. Finally, we present\nsimulation studies to evaluate the performance of our proposed frequentist and\nBayesian methodologies for various privacy budgets, resulting in useful\nsuggestions for performing causal inference for privatized data."}, "http://arxiv.org/abs/2303.03215": {"title": "Quantile-Quantile Methodology -- Detailed Results", "link": "http://arxiv.org/abs/2303.03215", "description": "The linear quantile-quantile relationship provides an easy-to-implement yet\neffective tool for transformation to and testing for normality. Its good\nperformance is verified in this report."}, "http://arxiv.org/abs/2305.06645": {"title": "Causal Inference for Continuous Multiple Time Point Interventions", "link": "http://arxiv.org/abs/2305.06645", "description": "There are limited options to estimate the treatment effects of variables\nwhich are continuous and measured at multiple time points, particularly if the\ntrue dose-response curve should be estimated as closely as possible. However,\nthese situations may be of relevance: in pharmacology, one may be interested in\nhow outcomes of people living with -- and treated for -- HIV, such as viral\nfailure, would vary for time-varying interventions such as different drug\nconcentration trajectories. A challenge for doing causal inference with\ncontinuous interventions is that the positivity assumption is typically\nviolated. To address positivity violations, we develop projection functions,\nwhich reweigh and redefine the estimand of interest based on functions of the\nconditional support for the respective interventions. With these functions, we\nobtain the desired dose-response curve in areas of enough support, and\notherwise a meaningful estimand that does not require the positivity\nassumption. We develop $g$-computation type plug-in estimators for this case.\nThose are contrasted with g-computation estimators which are applied to\ncontinuous interventions without specifically addressing positivity violations,\nwhich we propose to be presented with diagnostics. The ideas are illustrated\nwith longitudinal data from HIV positive children treated with an\nefavirenz-based regimen as part of the CHAPAS-3 trial, which enrolled children\n$&lt;13$ years in Zambia/Uganda. Simulations show in which situations a standard\n$g$-computation approach is appropriate, and in which it leads to bias and how\nthe proposed weighted estimation approach then recovers the alternative\nestimand of interest."}, "http://arxiv.org/abs/2305.14275": {"title": "Amortized Variational Inference with Coverage Guarantees", "link": "http://arxiv.org/abs/2305.14275", "description": "Amortized variational inference produces a posterior approximation that can\nbe rapidly computed given any new observation. Unfortunately, there are few\nguarantees about the quality of these approximate posteriors. We propose\nConformalized Amortized Neural Variational Inference (CANVI), a procedure that\nis scalable, easily implemented, and provides guaranteed marginal coverage.\nGiven a collection of candidate amortized posterior approximators, CANVI\nconstructs conformalized predictors based on each candidate, compares the\npredictors using a metric known as predictive efficiency, and returns the most\nefficient predictor. CANVI ensures that the resulting predictor constructs\nregions that contain the truth with a user-specified level of probability.\nCANVI is agnostic to design decisions in formulating the candidate\napproximators and only requires access to samples from the forward model,\npermitting its use in likelihood-free settings. We prove lower bounds on the\npredictive efficiency of the regions produced by CANVI and explore how the\nquality of a posterior approximation relates to the predictive efficiency of\nprediction regions based on that approximation. Finally, we demonstrate the\naccurate calibration and high predictive efficiency of CANVI on a suite of\nsimulation-based inference benchmark tasks and an important scientific task:\nanalyzing galaxy emission spectra."}, "http://arxiv.org/abs/2305.17187": {"title": "Clip-OGD: An Experimental Design for Adaptive Neyman Allocation in Sequential Experiments", "link": "http://arxiv.org/abs/2305.17187", "description": "From clinical development of cancer therapies to investigations into partisan\nbias, adaptive sequential designs have become increasingly popular method for\ncausal inference, as they offer the possibility of improved precision over\ntheir non-adaptive counterparts. However, even in simple settings (e.g. two\ntreatments) the extent to which adaptive designs can improve precision is not\nsufficiently well understood. In this work, we study the problem of Adaptive\nNeyman Allocation in a design-based potential outcomes framework, where the\nexperimenter seeks to construct an adaptive design which is nearly as efficient\nas the optimal (but infeasible) non-adaptive Neyman design, which has access to\nall potential outcomes. Motivated by connections to online optimization, we\npropose Neyman Ratio and Neyman Regret as two (equivalent) performance measures\nof adaptive designs for this problem. We present Clip-OGD, an adaptive design\nwhich achieves $\\widetilde{O}(\\sqrt{T})$ expected Neyman regret and thereby\nrecovers the optimal Neyman variance in large samples. Finally, we construct a\nconservative variance estimator which facilitates the development of\nasymptotically valid confidence intervals. To complement our theoretical\nresults, we conduct simulations using data from a microeconomic experiment."}, "http://arxiv.org/abs/2306.15622": {"title": "Biclustering random matrix partitions with an application to classification of forensic body fluids", "link": "http://arxiv.org/abs/2306.15622", "description": "Classification of unlabeled data is usually achieved by supervised learning\nfrom labeled samples. Although there exist many sophisticated supervised\nmachine learning methods that can predict the missing labels with a high level\nof accuracy, they often lack the required transparency in situations where it\nis important to provide interpretable results and meaningful measures of\nconfidence. Body fluid classification of forensic casework data is the case in\npoint. We develop a new Biclustering Dirichlet Process for Class-assignment\nwith Random Matrices (BDP-CaRMa), with a three-level hierarchy of clustering,\nand a model-based approach to classification that adapts to block structure in\nthe data matrix. As the class labels of some observations are missing, the\nnumber of rows in the data matrix for each class is unknown. BDP-CaRMa handles\nthis and extends existing biclustering methods by simultaneously biclustering\nmultiple matrices each having a randomly variable number of rows. We\ndemonstrate our method by applying it to the motivating problem, which is the\nclassification of body fluids based on mRNA profiles taken from crime scenes.\nThe analyses of casework-like data show that our method is interpretable and\nproduces well-calibrated posterior probabilities. Our model can be more\ngenerally applied to other types of data with a similar structure to the\nforensic data."}, "http://arxiv.org/abs/2307.05644": {"title": "Lambert W random variables and their applications in loss modelling", "link": "http://arxiv.org/abs/2307.05644", "description": "Several distributions and families of distributions are proposed to model\nskewed data, think, e.g., of skew-normal and related distributions. Lambert W\nrandom variables offer an alternative approach where, instead of constructing a\nnew distribution, a certain transform is proposed (Goerg, 2011). Such an\napproach allows the construction of a Lambert W skewed version from any\ndistribution. We choose Lambert W normal distribution as a natural starting\npoint and also include Lambert W exponential distribution due to the simplicity\nand shape of the exponential distribution, which, after skewing, may produce a\nreasonably heavy tail for loss models. In the theoretical part, we focus on the\nmathematical properties of obtained distributions, including the range of\nskewness. In the practical part, the suitability of corresponding Lambert W\ntransformed distributions is evaluated on real insurance data. The results are\ncompared with those obtained using common loss distributions."}, "http://arxiv.org/abs/2307.06840": {"title": "Ensemble learning for blending gridded satellite and gauge-measured precipitation data", "link": "http://arxiv.org/abs/2307.06840", "description": "Regression algorithms are regularly used for improving the accuracy of\nsatellite precipitation products. In this context, satellite precipitation and\ntopography data are the predictor variables, and gauged-measured precipitation\ndata are the dependent variables. Alongside this, it is increasingly recognised\nin many fields that combinations of algorithms through ensemble learning can\nlead to substantial predictive performance improvements. Still, a sufficient\nnumber of ensemble learners for improving the accuracy of satellite\nprecipitation products and their large-scale comparison are currently missing\nfrom the literature. In this study, we work towards filling in this specific\ngap by proposing 11 new ensemble learners in the field and by extensively\ncomparing them. We apply the ensemble learners to monthly data from the\nPERSIANN (Precipitation Estimation from Remotely Sensed Information using\nArtificial Neural Networks) and IMERG (Integrated Multi-satellitE Retrievals\nfor GPM) gridded datasets that span over a 15-year period and over the entire\nthe contiguous United States (CONUS). We also use gauge-measured precipitation\ndata from the Global Historical Climatology Network monthly database, version 2\n(GHCNm). The ensemble learners combine the predictions of six machine learning\nregression algorithms (base learners), namely the multivariate adaptive\nregression splines (MARS), multivariate adaptive polynomial splines\n(poly-MARS), random forests (RF), gradient boosting machines (GBM), extreme\ngradient boosting (XGBoost) and Bayesian regularized neural networks (BRNN),\nand each of them is based on a different combiner. The combiners include the\nequal-weight combiner, the median combiner, two best learners and seven\nvariants of a sophisticated stacking method. The latter stacks a regression\nalgorithm on top of the base learners to combine their independent\npredictions..."}, "http://arxiv.org/abs/2309.12819": {"title": "Doubly Robust Proximal Causal Learning for Continuous Treatments", "link": "http://arxiv.org/abs/2309.12819", "description": "Proximal causal learning is a promising framework for identifying the causal\neffect under the existence of unmeasured confounders. Within this framework,\nthe doubly robust (DR) estimator was derived and has shown its effectiveness in\nestimation, especially when the model assumption is violated. However, the\ncurrent form of the DR estimator is restricted to binary treatments, while the\ntreatment can be continuous in many real-world applications. The primary\nobstacle to continuous treatments resides in the delta function present in the\noriginal DR estimator, making it infeasible in causal effect estimation and\nintroducing a heavy computational burden in nuisance function estimation. To\naddress these challenges, we propose a kernel-based DR estimator that can well\nhandle continuous treatments. Equipped with its smoothness, we show that its\noracle form is a consistent approximation of the influence function. Further,\nwe propose a new approach to efficiently solve the nuisance functions. We then\nprovide a comprehensive convergence analysis in terms of the mean square error.\nWe demonstrate the utility of our estimator on synthetic datasets and\nreal-world applications."}, "http://arxiv.org/abs/2309.17283": {"title": "The Blessings of Multiple Treatments and Outcomes in Treatment Effect Estimation", "link": "http://arxiv.org/abs/2309.17283", "description": "Assessing causal effects in the presence of unobserved confounding is a\nchallenging problem. Existing studies leveraged proxy variables or multiple\ntreatments to adjust for the confounding bias. In particular, the latter\napproach attributes the impact on a single outcome to multiple treatments,\nallowing estimating latent variables for confounding control. Nevertheless,\nthese methods primarily focus on a single outcome, whereas in many real-world\nscenarios, there is greater interest in studying the effects on multiple\noutcomes. Besides, these outcomes are often coupled with multiple treatments.\nExamples include the intensive care unit (ICU), where health providers evaluate\nthe effectiveness of therapies on multiple health indicators. To accommodate\nthese scenarios, we consider a new setting dubbed as multiple treatments and\nmultiple outcomes. We then show that parallel studies of multiple outcomes\ninvolved in this setting can assist each other in causal identification, in the\nsense that we can exploit other treatments and outcomes as proxies for each\ntreatment effect under study. We proceed with a causal discovery method that\ncan effectively identify such proxies for causal estimation. The utility of our\nmethod is demonstrated in synthetic data and sepsis disease."}, "http://arxiv.org/abs/2310.10740": {"title": "Unbiased Estimation of Structured Prediction Error", "link": "http://arxiv.org/abs/2310.10740", "description": "Many modern datasets, such as those in ecology and geology, are composed of\nsamples with spatial structure and dependence. With such data violating the\nusual independent and identically distributed (IID) assumption in machine\nlearning and classical statistics, it is unclear a priori how one should\nmeasure the performance and generalization of models. Several authors have\nempirically investigated cross-validation (CV) methods in this setting,\nreaching mixed conclusions. We provide a class of unbiased estimation methods\nfor general quadratic errors, correlated Gaussian response, and arbitrary\nprediction function $g$, for a noise-elevated version of the error. Our\napproach generalizes the coupled bootstrap (CB) from the normal means problem\nto general normal data, allowing correlation both within and between the\ntraining and test sets. CB relies on creating bootstrap samples that are\nintelligently decoupled, in the sense of being statistically independent.\nSpecifically, the key to CB lies in generating two independent \"views\" of our\ndata and using them as stand-ins for the usual independent training and test\nsamples. Beginning with Mallows' $C_p$, we generalize the estimator to develop\nour generalized $C_p$ estimators (GC). We show at under only a moment condition\non $g$, this noise-elevated error estimate converges smoothly to the noiseless\nerror estimate. We show that when Stein's unbiased risk estimator (SURE)\napplies, GC converges to SURE as in the normal means problem. Further, we use\nthese same tools to analyze CV and provide some theoretical analysis to help\nunderstand when CV will provide good estimates of error. Simulations align with\nour theoretical results, demonstrating the effectiveness of GC and illustrating\nthe behavior of CV methods. Lastly, we apply our estimator to a model selection\ntask on geothermal data in Nevada."}, "http://arxiv.org/abs/2310.10761": {"title": "Simulation Based Composite Likelihood", "link": "http://arxiv.org/abs/2310.10761", "description": "Inference for high-dimensional hidden Markov models is challenging due to the\nexponential-in-dimension computational cost of the forward algorithm. To\naddress this issue, we introduce an innovative composite likelihood approach\ncalled \"Simulation Based Composite Likelihood\" (SimBa-CL). With SimBa-CL, we\napproximate the likelihood by the product of its marginals, which we estimate\nusing Monte Carlo sampling. In a similar vein to approximate Bayesian\ncomputation (ABC), SimBa-CL requires multiple simulations from the model, but,\nin contrast to ABC, it provides a likelihood approximation that guides the\noptimization of the parameters. Leveraging automatic differentiation libraries,\nit is simple to calculate gradients and Hessians to not only speed-up\noptimization, but also to build approximate confidence sets. We conclude with\nan extensive experimental section, where we empirically validate our\ntheoretical results, conduct a comparative analysis with SMC, and apply\nSimBa-CL to real-world Aphtovirus data."}, "http://arxiv.org/abs/2310.10798": {"title": "Poisson Count Time Series", "link": "http://arxiv.org/abs/2310.10798", "description": "This paper reviews and compares popular methods, some old and some very\nrecent, that produce time series having Poisson marginal distributions. The\npaper begins by narrating ways where time series with Poisson marginal\ndistributions can be produced. Modeling nonstationary series with covariates\nmotivates consideration of methods where the Poisson parameter depends on time.\nHere, estimation methods are developed for some of the more flexible methods.\nThe results are used in the analysis of 1) a count sequence of tropical\ncyclones occurring in the North Atlantic Basin since 1970, and 2) the number of\nno-hitter games pitched in major league baseball since 1893. Tests for whether\nthe Poisson marginal distribution is appropriate are included."}, "http://arxiv.org/abs/2310.10915": {"title": "Identifiability of the Multinomial Processing Tree-IRT model for the Philadelphia Naming Test", "link": "http://arxiv.org/abs/2310.10915", "description": "For persons with aphasia, naming tests are used to evaluate the severity of\nthe disease and observing progress toward recovery. The Philadelphia Naming\nTest (PNT) is a leading naming test composed of 175 items. The items are common\nnouns which are one to four syllables in length and with low, medium, and high\nfrequency. Since the target word is known to the administrator, the response\nfrom the patient can be classified as correct or an error. If the patient\ncommits an error, the PNT provides procedures for classifying the type of error\nin the response. Item response theory can be applied to PNT data to provide\nestimates of item difficulty and subject naming ability. Walker et al. (2018)\ndeveloped a IRT multinomial processing tree (IRT-MPT) model to attempt to\nunderstand the pathways through which the different errors are made by patients\nwhen responding to an item. The MPT model expands on existing models by\nconsidering items to be heterogeneous and estimating multiple latent parameters\nfor patients to more precisely determine at which step of word of production a\npatient's ability has been affected. These latent parameters represent the\ntheoretical cognitive steps taken in responding to an item. Given the\ncomplexity of the model proposed in Walker et al. (2018), here we investigate\nthe identifiability of the parameters included in the IRT-MPT model."}, "http://arxiv.org/abs/2310.10976": {"title": "Exact nonlinear state estimation", "link": "http://arxiv.org/abs/2310.10976", "description": "The majority of data assimilation (DA) methods in the geosciences are based\non Gaussian assumptions. While these assumptions facilitate efficient\nalgorithms, they cause analysis biases and subsequent forecast degradations.\nNon-parametric, particle-based DA algorithms have superior accuracy, but their\napplication to high-dimensional models still poses operational challenges.\nDrawing inspiration from recent advances in the field of generative artificial\nintelligence (AI), this article introduces a new nonlinear estimation theory\nwhich attempts to bridge the existing gap in DA methodology. Specifically, a\nConjugate Transform Filter (CTF) is derived and shown to generalize the\ncelebrated Kalman filter to arbitrarily non-Gaussian distributions. The new\nfilter has several desirable properties, such as its ability to preserve\nstatistical relationships in the prior state and convergence to highly accurate\nobservations. An ensemble approximation of the new theory (ECTF) is also\npresented and validated using idealized statistical experiments that feature\nbounded quantities with non-Gaussian distributions, a prevalent challenge in\nEarth system models. Results from these experiments indicate that the greatest\nbenefits from ECTF occur when observation errors are small relative to the\nforecast uncertainty and when state variables exhibit strong nonlinear\ndependencies. Ultimately, the new filtering theory offers exciting avenues for\nimproving conventional DA algorithms through their principled integration with\nAI techniques."}, "http://arxiv.org/abs/2310.11122": {"title": "Sensitivity-Aware Amortized Bayesian Inference", "link": "http://arxiv.org/abs/2310.11122", "description": "Bayesian inference is a powerful framework for making probabilistic\ninferences and decisions under uncertainty. Fundamental choices in modern\nBayesian workflows concern the specification of the likelihood function and\nprior distributions, the posterior approximator, and the data. Each choice can\nsignificantly influence model-based inference and subsequent decisions, thereby\nnecessitating sensitivity analysis. In this work, we propose a multifaceted\napproach to integrate sensitivity analyses into amortized Bayesian inference\n(ABI, i.e., simulation-based inference with neural networks). First, we utilize\nweight sharing to encode the structural similarities between alternative\nlikelihood and prior specifications in the training process with minimal\ncomputational overhead. Second, we leverage the rapid inference of neural\nnetworks to assess sensitivity to various data perturbations or pre-processing\nprocedures. In contrast to most other Bayesian approaches, both steps\ncircumvent the costly bottleneck of refitting the model(s) for each choice of\nlikelihood, prior, or dataset. Finally, we propose to use neural network\nensembles to evaluate variation in results induced by unreliable approximation\non unseen data. We demonstrate the effectiveness of our method in applied\nmodeling problems, ranging from the estimation of disease outbreak dynamics and\nglobal warming thresholds to the comparison of human decision-making models.\nOur experiments showcase how our approach enables practitioners to effectively\nunveil hidden relationships between modeling choices and inferential\nconclusions."}, "http://arxiv.org/abs/2310.11357": {"title": "A Pseudo-likelihood Approach to Under-5 Mortality Estimation", "link": "http://arxiv.org/abs/2310.11357", "description": "Accurate and precise estimates of under-5 mortality rates (U5MR) are an\nimportant health summary for countries. Full survival curves are additionally\nof interest to better understand the pattern of mortality in children under 5.\nModern demographic methods for estimating a full mortality schedule for\nchildren have been developed for countries with good vital registration and\nreliable census data, but perform poorly in many low- and middle-income\ncountries. In these countries, the need to utilize nationally representative\nsurveys to estimate U5MR requires additional statistical care to mitigate\npotential biases in survey data, acknowledge the survey design, and handle\naspects of survival data (i.e., censoring and truncation). In this paper, we\ndevelop parametric and non-parametric pseudo-likelihood approaches to\nestimating under-5 mortality across time from complex survey data. We argue\nthat the parametric approach is particularly useful in scenarios where data are\nsparse and estimation may require stronger assumptions. The nonparametric\napproach provides an aid to model validation. We compare a variety of\nparametric models to three existing methods for obtaining a full survival curve\nfor children under the age of 5, and argue that a parametric pseudo-likelihood\napproach is advantageous in low- and middle-income countries. We apply our\nproposed approaches to survey data from Burkina Faso, Malawi, Senegal, and\nNamibia. All code for fitting the models described in this paper is available\nin the R package pssst."}, "http://arxiv.org/abs/2006.00767": {"title": "Generative Multiple-purpose Sampler for Weighted M-estimation", "link": "http://arxiv.org/abs/2006.00767", "description": "To overcome the computational bottleneck of various data perturbation\nprocedures such as the bootstrap and cross validations, we propose the\nGenerative Multiple-purpose Sampler (GMS), which constructs a generator\nfunction to produce solutions of weighted M-estimators from a set of given\nweights and tuning parameters. The GMS is implemented by a single optimization\nwithout having to repeatedly evaluate the minimizers of weighted losses, and is\nthus capable of significantly reducing the computational time. We demonstrate\nthat the GMS framework enables the implementation of various statistical\nprocedures that would be unfeasible in a conventional framework, such as the\niterated bootstrap, bootstrapped cross-validation for penalized likelihood,\nbootstrapped empirical Bayes with nonparametric maximum likelihood, etc. To\nconstruct a computationally efficient generator function, we also propose a\nnovel form of neural network called the \\emph{weight multiplicative multilayer\nperceptron} to achieve fast convergence. Our numerical results demonstrate that\nthe new neural network structure enjoys a few orders of magnitude speed\nadvantage in comparison to the conventional one. An R package called GMS is\nprovided, which runs under Pytorch to implement the proposed methods and allows\nthe user to provide a customized loss function to tailor to their own models of\ninterest."}, "http://arxiv.org/abs/2012.03593": {"title": "Algebraic geometry of discrete interventional models", "link": "http://arxiv.org/abs/2012.03593", "description": "We investigate the algebra and geometry of general interventions in discrete\nDAG models. To this end, we introduce a theory for modeling soft interventions\nin the more general family of staged tree models and develop the formalism to\nstudy these models as parametrized subvarieties of a product of probability\nsimplices. We then consider the problem of finding their defining equations,\nand we derive a combinatorial criterion for identifying interventional staged\ntree models for which the defining ideal is toric. We apply these results to\nthe class of discrete interventional DAG models and establish a criteria to\ndetermine when these models are toric varieties."}, "http://arxiv.org/abs/2105.12720": {"title": "Marginal structural models with Latent Class Growth Modeling of Treatment Trajectories", "link": "http://arxiv.org/abs/2105.12720", "description": "In a real-life setting, little is known regarding the effectiveness of\nstatins for primary prevention among older adults, and analysis of\nobservational data can add crucial information on the benefits of actual\npatterns of use. Latent class growth models (LCGM) are increasingly proposed as\na solution to summarize the observed longitudinal treatment in a few distinct\ngroups. When combined with standard approaches like Cox proportional hazards\nmodels, LCGM can fail to control time-dependent confounding bias because of\ntime-varying covariates that have a double role of confounders and mediators.\nWe propose to use LCGM to classify individuals into a few latent classes based\non their medication adherence pattern, then choose a working marginal\nstructural model (MSM) that relates the outcome to these groups. The parameter\nof interest is nonparametrically defined as the projection of the true MSM onto\nthe chosen working model. The combination of LCGM with MSM is a convenient way\nto describe treatment adherence and can effectively control time-dependent\nconfounding. Simulation studies were used to illustrate our approach and\ncompare it with unadjusted, baseline covariates-adjusted, time-varying\ncovariates adjusted and inverse probability of trajectory groups weighting\nadjusted models. We found that our proposed approach yielded estimators with\nlittle or no bias."}, "http://arxiv.org/abs/2208.07610": {"title": "E-Statistics, Group Invariance and Anytime Valid Testing", "link": "http://arxiv.org/abs/2208.07610", "description": "We study worst-case-growth-rate-optimal (GROW) e-statistics for hypothesis\ntesting between two group models. It is known that under a mild condition on\nthe action of the underlying group G on the data, there exists a maximally\ninvariant statistic. We show that among all e-statistics, invariant or not, the\nlikelihood ratio of the maximally invariant statistic is GROW, both in the\nabsolute and in the relative sense, and that an anytime-valid test can be based\non it. The GROW e-statistic is equal to a Bayes factor with a right Haar prior\non G. Our treatment avoids nonuniqueness issues that sometimes arise for such\npriors in Bayesian contexts. A crucial assumption on the group G is its\namenability, a well-known group-theoretical condition, which holds, for\ninstance, in scale-location families. Our results also apply to\nfinite-dimensional linear regression."}, "http://arxiv.org/abs/2302.03246": {"title": "CDANs: Temporal Causal Discovery from Autocorrelated and Non-Stationary Time Series Data", "link": "http://arxiv.org/abs/2302.03246", "description": "Time series data are found in many areas of healthcare such as medical time\nseries, electronic health records (EHR), measurements of vitals, and wearable\ndevices. Causal discovery, which involves estimating causal relationships from\nobservational data, holds the potential to play a significant role in\nextracting actionable insights about human health. In this study, we present a\nnovel constraint-based causal discovery approach for autocorrelated and\nnon-stationary time series data (CDANs). Our proposed method addresses several\nlimitations of existing causal discovery methods for autocorrelated and\nnon-stationary time series data, such as high dimensionality, the inability to\nidentify lagged causal relationships, and overlooking changing modules. Our\napproach identifies lagged and instantaneous/contemporaneous causal\nrelationships along with changing modules that vary over time. The method\noptimizes the conditioning sets in a constraint-based search by considering\nlagged parents instead of conditioning on the entire past that addresses high\ndimensionality. The changing modules are detected by considering both\ncontemporaneous and lagged parents. The approach first detects the lagged\nadjacencies, then identifies the changing modules and contemporaneous\nadjacencies, and finally determines the causal direction. We extensively\nevaluated our proposed method on synthetic and real-world clinical datasets,\nand compared its performance with several baseline approaches. The experimental\nresults demonstrate the effectiveness of the proposed method in detecting\ncausal relationships and changing modules for autocorrelated and non-stationary\ntime series data."}, "http://arxiv.org/abs/2305.07089": {"title": "Hierarchically Coherent Multivariate Mixture Networks", "link": "http://arxiv.org/abs/2305.07089", "description": "Large collections of time series data are often organized into hierarchies\nwith different levels of aggregation; examples include product and geographical\ngroupings. Probabilistic coherent forecasting is tasked to produce forecasts\nconsistent across levels of aggregation. In this study, we propose to augment\nneural forecasting architectures with a coherent multivariate mixture output.\nWe optimize the networks with a composite likelihood objective, allowing us to\ncapture time series' relationships while maintaining high computational\nefficiency. Our approach demonstrates 13.2% average accuracy improvements on\nmost datasets compared to state-of-the-art baselines. We conduct ablation\nstudies of the framework components and provide theoretical foundations for\nthem. To assist related work, the code is available at this\nhttps://github.com/Nixtla/neuralforecast."}, "http://arxiv.org/abs/2307.16720": {"title": "The epigraph and the hypograph indexes as useful tools for clustering multivariate functional data", "link": "http://arxiv.org/abs/2307.16720", "description": "The proliferation of data generation has spurred advancements in functional\ndata analysis. With the ability to analyze multiple variables simultaneously,\nthe demand for working with multivariate functional data has increased. This\nstudy proposes a novel formulation of the epigraph and hypograph indexes, as\nwell as their generalized expressions, specifically tailored for the\nmultivariate functional context. These definitions take into account the\ninterrelations between components. Furthermore, the proposed indexes are\nemployed to cluster multivariate functional data. In the clustering process,\nthe indexes are applied to both the data and their first and second\nderivatives. This generates a reduced-dimension dataset from the original\nmultivariate functional data, enabling the application of well-established\nmultivariate clustering techniques which have been extensively studied in the\nliterature. This methodology has been tested through simulated and real\ndatasets, performing comparative analyses against state-of-the-art to assess\nits performance."}, "http://arxiv.org/abs/2309.07810": {"title": "Spectrum-Aware Adjustment: A New Debiasing Framework with Applications to Principal Component Regression", "link": "http://arxiv.org/abs/2309.07810", "description": "We introduce a new debiasing framework for high-dimensional linear regression\nthat bypasses the restrictions on covariate distributions imposed by modern\ndebiasing technology. We study the prevalent setting where the number of\nfeatures and samples are both large and comparable. In this context,\nstate-of-the-art debiasing technology uses a degrees-of-freedom correction to\nremove the shrinkage bias of regularized estimators and conduct inference.\nHowever, this method requires that the observed samples are i.i.d., the\ncovariates follow a mean zero Gaussian distribution, and reliable covariance\nmatrix estimates for observed features are available. This approach struggles\nwhen (i) covariates are non-Gaussian with heavy tails or asymmetric\ndistributions, (ii) rows of the design exhibit heterogeneity or dependencies,\nand (iii) reliable feature covariance estimates are lacking.\n\nTo address these, we develop a new strategy where the debiasing correction is\na rescaled gradient descent step (suitably initialized) with step size\ndetermined by the spectrum of the sample covariance matrix. Unlike prior work,\nwe assume that eigenvectors of this matrix are uniform draws from the\northogonal group. We show this assumption remains valid in diverse situations\nwhere traditional debiasing fails, including designs with complex row-column\ndependencies, heavy tails, asymmetric properties, and latent low-rank\nstructures. We establish asymptotic normality of our proposed estimator\n(centered and scaled) under various convergence notions. Moreover, we develop a\nconsistent estimator for its asymptotic variance. Lastly, we introduce a\ndebiased Principal Components Regression (PCR) technique using our\nSpectrum-Aware approach. In varied simulations and real data experiments, we\nobserve that our method outperforms degrees-of-freedom debiasing by a margin."}, "http://arxiv.org/abs/2310.11471": {"title": "Modeling lower-truncated and right-censored insurance claims with an extension of the MBBEFD class", "link": "http://arxiv.org/abs/2310.11471", "description": "In general insurance, claims are often lower-truncated and right-censored\nbecause insurance contracts may involve deductibles and maximal covers. Most\nclassical statistical models are not (directly) suited to model lower-truncated\nand right-censored claims. A surprisingly flexible family of distributions that\ncan cope with lower-truncated and right-censored claims is the class of MBBEFD\ndistributions that originally has been introduced by Bernegger (1997) for\nreinsurance pricing, but which has not gained much attention outside the\nreinsurance literature. We derive properties of the class of MBBEFD\ndistributions, and we extend it to a bigger family of distribution functions\nsuitable for modeling lower-truncated and right-censored claims. Interestingly,\nin general insurance, we mainly rely on unimodal skewed densities, whereas the\nreinsurance literature typically proposes monotonically decreasing densities\nwithin the MBBEFD class."}, "http://arxiv.org/abs/2310.11603": {"title": "Group sequential two-stage preference designs", "link": "http://arxiv.org/abs/2310.11603", "description": "The two-stage preference design (TSPD) enables the inference for treatment\nefficacy while allowing for incorporation of patient preference to treatment.\nIt can provide unbiased estimates for selection and preference effects, where a\nselection effect occurs when patients who prefer one treatment respond\ndifferently than those who prefer another, and a preference effect is the\ndifference in response caused by an interaction between the patient's\npreference and the actual treatment they receive. One potential barrier to\nadopting TSPD in practice, however, is the relatively large sample size\nrequired to estimate selection and preference effects with sufficient power. To\naddress this concern, we propose a group sequential two-stage preference design\n(GS-TSPD), which combines TSPD with sequential monitoring for early stopping.\nIn the GS-TSPD, pre-planned sequential monitoring allows investigators to\nconduct repeated hypothesis tests on accumulated data prior to full enrollment\nto assess study eligibility for early trial termination without inflating type\nI error rates. Thus, the procedure allows investigators to terminate the study\nwhen there is sufficient evidence of treatment, selection, or preference\neffects during an interim analysis, thereby reducing the design resource in\nexpectation. To formalize such a procedure, we verify the independent\nincrements assumption for testing the selection and preference effects and\napply group sequential stopping boundaries from the approximate sequential\ndensity functions. Simulations are then conducted to investigate the operating\ncharacteristics of our proposed GS-TSPD compared to the traditional TSPD. We\ndemonstrate the applicability of the design using a study of Hepatitis C\ntreatment modality."}, "http://arxiv.org/abs/2310.11620": {"title": "Enhancing modified treatment policy effect estimation with weighted energy distance", "link": "http://arxiv.org/abs/2310.11620", "description": "The effects of continuous treatments are often characterized through the\naverage dose response function, which is challenging to estimate from\nobservational data due to confounding and positivity violations. Modified\ntreatment policies (MTPs) are an alternative approach that aim to assess the\neffect of a modification to observed treatment values and work under relaxed\nassumptions. Estimators for MTPs generally focus on estimating the conditional\ndensity of treatment given covariates and using it to construct weights.\nHowever, weighting using conditional density models has well-documented\nchallenges. Further, MTPs with larger treatment modifications have stronger\nconfounding and no tools exist to help choose an appropriate modification\nmagnitude. This paper investigates the role of weights for MTPs showing that to\ncontrol confounding, weights should balance the weighted data to an unobserved\nhypothetical target population, that can be characterized with observed data.\nLeveraging this insight, we present a versatile set of tools to enhance\nestimation for MTPs. We introduce a distance that measures imbalance of\ncovariate distributions under the MTP and use it to develop new weighting\nmethods and tools to aid in the estimation of MTPs. We illustrate our methods\nthrough an example studying the effect of mechanical power of ventilation on\nin-hospital mortality."}, "http://arxiv.org/abs/2310.11630": {"title": "Adaptive Bootstrap Tests for Composite Null Hypotheses in the Mediation Pathway Analysis", "link": "http://arxiv.org/abs/2310.11630", "description": "Mediation analysis aims to assess if, and how, a certain exposure influences\nan outcome of interest through intermediate variables. This problem has\nrecently gained a surge of attention due to the tremendous need for such\nanalyses in scientific fields. Testing for the mediation effect is greatly\nchallenged by the fact that the underlying null hypothesis (i.e. the absence of\nmediation effects) is composite. Most existing mediation tests are overly\nconservative and thus underpowered. To overcome this significant methodological\nhurdle, we develop an adaptive bootstrap testing framework that can accommodate\ndifferent types of composite null hypotheses in the mediation pathway analysis.\nApplied to the product of coefficients (PoC) test and the joint significance\n(JS) test, our adaptive testing procedures provide type I error control under\nthe composite null, resulting in much improved statistical power compared to\nexisting tests. Both theoretical properties and numerical examples of the\nproposed methodology are discussed."}, "http://arxiv.org/abs/2310.11683": {"title": "Are we bootstrapping the right thing? A new approach to quantify uncertainty of Average Treatment Effect Estimate", "link": "http://arxiv.org/abs/2310.11683", "description": "Existing approaches of using the bootstrap method to derive standard error\nand confidence interval of average treatment effect estimate has one potential\nissue, which is that they are actually bootstrapping the wrong thing, resulting\nin unvalid statistical inference. In this paper, we discuss this important\nissue and propose a new non-parametric bootstrap method that can more precisely\nquantify the uncertainty associated with average treatment effect estimates. We\ndemonstrate the validity of this approach through a simulation study and a\nreal-world example, and highlight the importance of deriving standard error and\nconfidence interval of average treatment effect estimates that both remove\nextra undesired noise and are easy to interpret when applied in real world\nscenarios."}, "http://arxiv.org/abs/2310.11724": {"title": "Simultaneous Nonparametric Inference of M-regression under Complex Temporal Dynamics", "link": "http://arxiv.org/abs/2310.11724", "description": "The paper considers simultaneous nonparametric inference for a wide class of\nM-regression models with time-varying coefficients. The covariates and errors\nof the regression model are tackled as a general class of piece-wise locally\nstationary time series and are allowed to be cross-dependent. We introduce an\nintegration technique to study the M-estimators, whose limiting properties are\ndisclosed using Bahadur representation and Gaussian approximation theory.\nFacilitated by a self-convolved bootstrap proposed in this paper, we introduce\na unified framework to conduct general classes of Exact Function Tests,\nLack-of-fit Tests, and Qualitative Tests for the time-varying coefficient\nM-regression under complex temporal dynamics. As an application, our method is\napplied to studying the anthropogenic warming trend and time-varying structures\nof the ENSO effect using global climate data from 1882 to 2005."}, "http://arxiv.org/abs/2310.11741": {"title": "Graph Sphere: From Nodes to Supernodes in Graphical Models", "link": "http://arxiv.org/abs/2310.11741", "description": "High-dimensional data analysis typically focuses on low-dimensional\nstructure, often to aid interpretation and computational efficiency. Graphical\nmodels provide a powerful methodology for learning the conditional independence\nstructure in multivariate data by representing variables as nodes and\ndependencies as edges. Inference is often focused on individual edges in the\nlatent graph. Nonetheless, there is increasing interest in determining more\ncomplex structures, such as communities of nodes, for multiple reasons,\nincluding more effective information retrieval and better interpretability. In\nthis work, we propose a multilayer graphical model where we first cluster nodes\nand then, at the second layer, investigate the relationships among groups of\nnodes. Specifically, nodes are partitioned into \"supernodes\" with a\ndata-coherent size-biased tessellation prior which combines ideas from Bayesian\nnonparametrics and Voronoi tessellations. This construct allows accounting also\nfor dependence of nodes within supernodes. At the second layer, dependence\nstructure among supernodes is modelled through a Gaussian graphical model,\nwhere the focus of inference is on \"superedges\". We provide theoretical\njustification for our modelling choices. We design tailored Markov chain Monte\nCarlo schemes, which also enable parallel computations. We demonstrate the\neffectiveness of our approach for large-scale structure learning in simulations\nand a transcriptomics application."}, "http://arxiv.org/abs/2310.11779": {"title": "A Multivariate Skew-Normal-Tukey-h Distribution", "link": "http://arxiv.org/abs/2310.11779", "description": "We introduce a new family of multivariate distributions by taking the\ncomponent-wise Tukey-h transformation of a random vector following a\nskew-normal distribution. The proposed distribution is named the\nskew-normal-Tukey-h distribution and is an extension of the skew-normal\ndistribution for handling heavy-tailed data. We compare this proposed\ndistribution to the skew-t distribution, which is another extension of the\nskew-normal distribution for modeling tail-thickness, and demonstrate that when\nthere are substantial differences in marginal kurtosis, the proposed\ndistribution is more appropriate. Moreover, we derive many appealing stochastic\nproperties of the proposed distribution and provide a methodology for the\nestimation of the parameters in which the computational requirement increases\nlinearly with the dimension. Using simulations, as well as a wine and a wind\nspeed data application, we illustrate how to draw inferences based on the\nmultivariate skew-normal-Tukey-h distribution."}, "http://arxiv.org/abs/2310.11799": {"title": "Testing for patterns and structures in covariance and correlation matrices", "link": "http://arxiv.org/abs/2310.11799", "description": "Covariance matrices of random vectors contain information that is crucial for\nmodelling. Certain structures and patterns of the covariances (or correlations)\nmay be used to justify parametric models, e.g., autoregressive models. Until\nnow, there have been only few approaches for testing such covariance structures\nsystematically and in a unified way. In the present paper, we propose such a\nunified testing procedure, and we will exemplify the approach with a large\nvariety of covariance structure models. This includes common structures such as\ndiagonal matrices, Toeplitz matrices, and compound symmetry but also the more\ninvolved autoregressive matrices. We propose hypothesis tests for these\nstructures, and we use bootstrap techniques for better small-sample\napproximation. The structures of the proposed tests invite for adaptations to\nother covariance patterns by choosing the hypothesis matrix appropriately. We\nprove their correctness for large sample sizes. The proposed methods require\nonly weak assumptions.\n\nWith the help of a simulation study, we assess the small sample properties of\nthe tests.\n\nWe also analyze a real data set to illustrate the application of the\nprocedure."}, "http://arxiv.org/abs/2310.11822": {"title": "Post-clustering Inference under Dependency", "link": "http://arxiv.org/abs/2310.11822", "description": "Recent work by Gao et al. has laid the foundations for post-clustering\ninference. For the first time, the authors established a theoretical framework\nallowing to test for differences between means of estimated clusters.\nAdditionally, they studied the estimation of unknown parameters while\ncontrolling the selective type I error. However, their theory was developed for\nindependent observations identically distributed as $p$-dimensional Gaussian\nvariables with a spherical covariance matrix. Here, we aim at extending this\nframework to a more convenient scenario for practical applications, where\narbitrary dependence structures between observations and features are allowed.\nWe show that a $p$-value for post-clustering inference under general dependency\ncan be defined, and we assess the theoretical conditions allowing the\ncompatible estimation of a covariance matrix. The theory is developed for\nhierarchical agglomerative clustering algorithms with several types of\nlinkages, and for the $k$-means algorithm. We illustrate our method with\nsynthetic data and real data of protein structures."}, "http://arxiv.org/abs/2310.11969": {"title": "Survey calibration for causal inference: a simple method to balance covariate distributions", "link": "http://arxiv.org/abs/2310.11969", "description": "This paper proposes a simple method for balancing distributions of covariates\nfor causal inference based on observational studies. The method makes it\npossible to balance an arbitrary number of quantiles (e.g., medians, quartiles,\nor deciles) together with means if necessary. The proposed approach is based on\nthe theory of calibration estimators (Deville and S\\\"arndal 1992), in\nparticular, calibration estimators for quantiles, proposed by Harms and\nDuchesne (2006). By modifying the entropy balancing method and the covariate\nbalancing propensity score method, it is possible to balance the distributions\nof the treatment and control groups. The method does not require numerical\nintegration, kernel density estimation or assumptions about the distributions;\nvalid estimates can be obtained by drawing on existing asymptotic theory.\nResults of a simulation study indicate that the method efficiently estimates\naverage treatment effects on the treated (ATT), the average treatment effect\n(ATE), the quantile treatment effect on the treated (QTT) and the quantile\ntreatment effect (QTE), especially in the presence of non-linearity and\nmis-specification of the models. The proposed methods are implemented in an\nopen source R package jointCalib."}, "http://arxiv.org/abs/2310.12000": {"title": "Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models", "link": "http://arxiv.org/abs/2310.12000", "description": "Latent Gaussian process (GP) models are flexible probabilistic non-parametric\nfunction models. Vecchia approximations are accurate approximations for GPs to\novercome computational bottlenecks for large data, and the Laplace\napproximation is a fast method with asymptotic convergence guarantees to\napproximate marginal likelihoods and posterior predictive distributions for\nnon-Gaussian likelihoods. Unfortunately, the computational complexity of\ncombined Vecchia-Laplace approximations grows faster than linearly in the\nsample size when used in combination with direct solver methods such as the\nCholesky decomposition. Computations with Vecchia-Laplace approximations thus\nbecome prohibitively slow precisely when the approximations are usually the\nmost accurate, i.e., on large data sets. In this article, we present several\niterative methods for inference with Vecchia-Laplace approximations which make\ncomputations considerably faster compared to Cholesky-based calculations. We\nanalyze our proposed methods theoretically and in experiments with simulated\nand real-world data. In particular, we obtain a speed-up of an order of\nmagnitude compared to Cholesky-based inference and a threefold increase in\nprediction accuracy in terms of the continuous ranked probability score\ncompared to a state-of-the-art method on a large satellite data set. All\nmethods are implemented in a free C++ software library with high-level Python\nand R packages."}, "http://arxiv.org/abs/2310.12010": {"title": "A Note on Improving Variational Estimation for Multidimensional Item Response Theory", "link": "http://arxiv.org/abs/2310.12010", "description": "Survey instruments and assessments are frequently used in many domains of\nsocial science. When the constructs that these assessments try to measure\nbecome multifaceted, multidimensional item response theory (MIRT) provides a\nunified framework and convenient statistical tool for item analysis,\ncalibration, and scoring. However, the computational challenge of estimating\nMIRT models prohibits its wide use because many of the extant methods can\nhardly provide results in a realistic time frame when the number of dimensions,\nsample size, and test length are large. Instead, variational estimation\nmethods, such as Gaussian Variational Expectation Maximization (GVEM)\nalgorithm, have been recently proposed to solve the estimation challenge by\nproviding a fast and accurate solution. However, results have shown that\nvariational estimation methods may produce some bias on discrimination\nparameters during confirmatory model estimation, and this note proposes an\nimportance weighted version of GVEM (i.e., IW-GVEM) to correct for such bias\nunder MIRT models. We also use the adaptive moment estimation method to update\nthe learning rate for gradient descent automatically. Our simulations show that\nIW-GVEM can effectively correct bias with modest increase of computation time,\ncompared with GVEM. The proposed method may also shed light on improving the\nvariational estimation for other psychometrics models."}, "http://arxiv.org/abs/2310.12115": {"title": "MMD-based Variable Importance for Distributional Random Forest", "link": "http://arxiv.org/abs/2310.12115", "description": "Distributional Random Forest (DRF) is a flexible forest-based method to\nestimate the full conditional distribution of a multivariate output of interest\ngiven input variables. In this article, we introduce a variable importance\nalgorithm for DRFs, based on the well-established drop and relearn principle\nand MMD distance. While traditional importance measures only detect variables\nwith an influence on the output mean, our algorithm detects variables impacting\nthe output distribution more generally. We show that the introduced importance\nmeasure is consistent, exhibits high empirical performance on both real and\nsimulated data, and outperforms competitors. In particular, our algorithm is\nhighly efficient to select variables through recursive feature elimination, and\ncan therefore provide small sets of variables to build accurate estimates of\nconditional output distributions."}, "http://arxiv.org/abs/2310.12140": {"title": "Online Estimation with Rolling Validation: Adaptive Nonparametric Estimation with Stream Data", "link": "http://arxiv.org/abs/2310.12140", "description": "Online nonparametric estimators are gaining popularity due to their efficient\ncomputation and competitive generalization abilities. An important example\nincludes variants of stochastic gradient descent. These algorithms often take\none sample point at a time and instantly update the parameter estimate of\ninterest. In this work we consider model selection and hyperparameter tuning\nfor such online algorithms. We propose a weighted rolling-validation procedure,\nan online variant of leave-one-out cross-validation, that costs minimal extra\ncomputation for many typical stochastic gradient descent estimators. Similar to\nbatch cross-validation, it can boost base estimators to achieve a better,\nadaptive convergence rate. Our theoretical analysis is straightforward, relying\nmainly on some general statistical stability assumptions. The simulation study\nunderscores the significance of diverging weights in rolling validation in\npractice and demonstrates its sensitivity even when there is only a slim\ndifference between candidate estimators."}, "http://arxiv.org/abs/2010.02968": {"title": "Modelling of functional profiles and explainable shape shifts detection: An approach combining the notion of the Fr\\'echet mean with the shape invariant model", "link": "http://arxiv.org/abs/2010.02968", "description": "A modelling framework suitable for detecting shape shifts in functional\nprofiles combining the notion of Fr\\'echet mean and the concept of deformation\nmodels is developed and proposed. The generalized mean sense offered by the\nFr\\'echet mean notion is employed to capture the typical pattern of the\nprofiles under study, while the concept of deformation models, and in\nparticular of the shape invariant model, allows for interpretable\nparameterizations of profile's deviations from the typical shape. EWMA-type\ncontrol charts compatible with the functional nature of data and the employed\ndeformation model are built and proposed, exploiting certain shape\ncharacteristics of the profiles under study with respect to the generalized\nmean sense, allowing for the identification of potential shifts concerning the\nshape and/or the deformation process. Potential shifts in the shape deformation\nprocess, are further distinguished to significant shifts with respect to\namplitude and/or the phase of the profile under study. The proposed modelling\nand shift detection framework is implemented to a real world case study, where\ndaily concentration profiles concerning air pollutants from an area in the city\nof Athens are modelled, while profiles indicating hazardous concentration\nlevels are successfully identified in most of the cases."}, "http://arxiv.org/abs/2207.07218": {"title": "On the Selection of Tuning Parameters for Patch-Stitching Embedding Methods", "link": "http://arxiv.org/abs/2207.07218", "description": "While classical scaling, just like principal component analysis, is\nparameter-free, other methods for embedding multivariate data require the\nselection of one or several tuning parameters. This tuning can be difficult due\nto the unsupervised nature of the situation. We propose a simple, almost\nobvious, approach to supervise the choice of tuning parameter(s): minimize a\nnotion of stress. We apply this approach to the selection of the patch size in\na prototypical patch-stitching embedding method, both in the multidimensional\nscaling (aka network localization) setting and in the dimensionality reduction\n(aka manifold learning) setting. In our study, we uncover a new bias--variance\ntradeoff phenomenon."}, "http://arxiv.org/abs/2303.17856": {"title": "Bootstrapping multiple systems estimates to account for model selection", "link": "http://arxiv.org/abs/2303.17856", "description": "Multiple systems estimation using a Poisson loglinear model is a standard\napproach to quantifying hidden populations where data sources are based on\nlists of known cases. Information criteria are often used for selecting between\nthe large number of possible models. Confidence intervals are often reported\nconditional on the model selected, providing an over-optimistic impression of\nestimation accuracy. A bootstrap approach is a natural way to account for the\nmodel selection. However, because the model selection step has to be carried\nout for every bootstrap replication, there may be a high or even prohibitive\ncomputational burden. We explore the merit of modifying the model selection\nprocedure in the bootstrap to look only among a subset of models, chosen on the\nbasis of their information criterion score on the original data. This provides\nlarge computational gains with little apparent effect on inference. We also\nincorporate rigorous and economical ways of approaching issues of the existence\nof estimators when applying the method to sparse data tables."}, "http://arxiv.org/abs/2308.07319": {"title": "Partial identification for discrete data with nonignorable missing outcomes", "link": "http://arxiv.org/abs/2308.07319", "description": "Nonignorable missing outcomes are common in real world datasets and often\nrequire strong parametric assumptions to achieve identification. These\nassumptions can be implausible or untestable, and so we may forgo them in\nfavour of partially identified models that narrow the set of a priori possible\nvalues to an identification region. Here we propose a new nonparametric Bayes\nmethod that allows for the incorporation of multiple clinically relevant\nrestrictions of the parameter space simultaneously. We focus on two common\nrestrictions, instrumental variables and the direction of missing data bias,\nand investigate how these restrictions narrow the identification region for\nparameters of interest. Additionally, we propose a rejection sampling algorithm\nthat allows us to quantify the evidence for these assumptions in the data. We\ncompare our method to a standard Heckman selection model in both simulation\nstudies and in an applied problem examining the effectiveness of cash-transfers\nfor people experiencing homelessness."}, "http://arxiv.org/abs/2310.12285": {"title": "Sparse high-dimensional linear mixed modeling with a partitioned empirical Bayes ECM algorithm", "link": "http://arxiv.org/abs/2310.12285", "description": "High-dimensional longitudinal data is increasingly used in a wide range of\nscientific studies. However, there are few statistical methods for\nhigh-dimensional linear mixed models (LMMs), as most Bayesian variable\nselection or penalization methods are designed for independent observations.\nAdditionally, the few available software packages for high-dimensional LMMs\nsuffer from scalability issues. This work presents an efficient and accurate\nBayesian framework for high-dimensional LMMs. We use empirical Bayes estimators\nof hyperparameters for increased flexibility and an\nExpectation-Conditional-Minimization (ECM) algorithm for computationally\nefficient maximum a posteriori probability (MAP) estimation of parameters. The\nnovelty of the approach lies in its partitioning and parameter expansion as\nwell as its fast and scalable computation. We illustrate Linear Mixed Modeling\nwith PaRtitiOned empirical Bayes ECM (LMM-PROBE) in simulation studies\nevaluating fixed and random effects estimation along with computation time. A\nreal-world example is provided using data from a study of lupus in children,\nwhere we identify genes and clinical factors associated with a new lupus\nbiomarker and predict the biomarker over time."}, "http://arxiv.org/abs/2310.12348": {"title": "Goodness--of--Fit Tests Based on the Min--Characteristic Function", "link": "http://arxiv.org/abs/2310.12348", "description": "We propose tests of fit for classes of distributions that include the\nWeibull, the Pareto and the Fr\\'echet, distributions. The new tests employ the\nnovel tool of the min--characteristic function and are based on an L2--type\nweighted distance between this function and its empirical counterpart applied\non suitably standardized data. If data--standardization is performed using the\nMLE of the distributional parameters then the method reduces to testing for the\nstandard member of the family, with parameter values known and set equal to\none. We investigate asymptotic properties of the tests, while a Monte Carlo\nstudy is presented that includes the new procedure as well as competitors for\nthe purpose of specification testing with three extreme value distributions.\nThe new tests are also applied on a few real--data sets."}, "http://arxiv.org/abs/2310.12358": {"title": "causalBETA: An R Package for Bayesian Semiparametric Casual Inference with Event-Time Outcomes", "link": "http://arxiv.org/abs/2310.12358", "description": "Observational studies are often conducted to estimate causal effects of\ntreatments or exposures on event-time outcomes. Since treatments are not\nrandomized in observational studies, techniques from causal inference are\nrequired to adjust for confounding. Bayesian approaches to causal estimates are\ndesirable because they provide 1) prior smoothing provides useful\nregularization of causal effect estimates, 2) flexible models that are robust\nto misspecification, 3) full inference (i.e. both point and uncertainty\nestimates) for causal estimands. However, Bayesian causal inference is\ndifficult to implement manually and there is a lack of user-friendly software,\npresenting a significant barrier to wide-spread use. We address this gap by\ndeveloping causalBETA (Bayesian Event Time Analysis) - an open-source R package\nfor estimating causal effects on event-time outcomes using Bayesian\nsemiparametric models. The package provides a familiar front-end to users, with\nsyntax identical to existing survival analysis R packages such as survival. At\nthe same time, it back-ends to Stan - a popular platform for Bayesian modeling\nand high performance statistical computing - for efficient posterior\ncomputation. To improve user experience, the package is built using customized\nS3 class objects and methods to facilitate visualizations and summaries of\nresults using familiar generic functions like plot() and summary(). In this\npaper, we provide the methodological details of the package, a demonstration\nusing publicly-available data, and computational guidance."}, "http://arxiv.org/abs/2310.12391": {"title": "Real-time Semiparametric Regression via Sequential Monte Carlo", "link": "http://arxiv.org/abs/2310.12391", "description": "We develop and describe online algorithms for performing real-time\nsemiparametric regression analyses. Earlier work on this topic is in Luts,\nBroderick &amp; Wand (J. Comput. Graph. Statist., 2014) where online mean field\nvariational Bayes was employed. In this article we instead develop sequential\nMonte Carlo approaches to circumvent well-known inaccuracies inherent in\nvariational approaches. Even though sequential Monte Carlo is not as fast as\nonline mean field variational Bayes, it can be a viable alternative for\napplications where the data rate is not overly high. For Gaussian response\nsemiparametric regression models our new algorithms share the online mean field\nvariational Bayes property of only requiring updating and storage of sufficient\nstatistics quantities of streaming data. In the non-Gaussian case accurate\nreal-time semiparametric regression requires the full data to be kept in\nstorage. The new algorithms allow for new options concerning accuracy/speed\ntrade-offs for real-time semiparametric regression."}, "http://arxiv.org/abs/2310.12402": {"title": "Data visualization and dimension reduction for metric-valued response regression", "link": "http://arxiv.org/abs/2310.12402", "description": "As novel data collection becomes increasingly common, traditional dimension\nreduction and data visualization techniques are becoming inadequate to analyze\nthese complex data. A surrogate-assisted sufficient dimension reduction (SDR)\nmethod for regression with a general metric-valued response on Euclidean\npredictors is proposed. The response objects are mapped to a real-valued\ndistance matrix using an appropriate metric and then projected onto a large\nsample of random unit vectors to obtain scalar-valued surrogate responses. An\nensemble estimate of the subspaces for the regression of the surrogate\nresponses versus the predictor is used to estimate the original central space.\nUnder this framework, classical SDR methods such as ordinary least squares and\nsliced inverse regression are extended. The surrogate-assisted method applies\nto responses on compact metric spaces including but not limited to Euclidean,\ndistributional, and functional. An extensive simulation experiment demonstrates\nthe superior performance of the proposed surrogate-assisted method on synthetic\ndata compared to existing competing methods where applicable. The analysis of\nthe distributions and functional trajectories of county-level COVID-19\ntransmission rates in the U.S. as a function of demographic characteristics is\nalso provided. The theoretical justifications are included as well."}, "http://arxiv.org/abs/2310.12424": {"title": "Optimal heteroskedasticity testing in nonparametric regression", "link": "http://arxiv.org/abs/2310.12424", "description": "Heteroskedasticity testing in nonparametric regression is a classic\nstatistical problem with important practical applications, yet fundamental\nlimits are unknown. Adopting a minimax perspective, this article considers the\ntesting problem in the context of an $\\alpha$-H\\\"{o}lder mean and a\n$\\beta$-H\\\"{o}lder variance function. For $\\alpha &gt; 0$ and $\\beta \\in (0,\n\\frac{1}{2})$, the sharp minimax separation rate $n^{-4\\alpha} +\nn^{-\\frac{4\\beta}{4\\beta+1}} + n^{-2\\beta}$ is established. To achieve the\nminimax separation rate, a kernel-based statistic using first-order squared\ndifferences is developed. Notably, the statistic estimates a proxy rather than\na natural quadratic functional (the squared distance between the variance\nfunction and its best $L^2$ approximation by a constant) suggested in previous\nwork.\n\nThe setting where no smoothness is assumed on the variance function is also\nstudied; the variance profile across the design points can be arbitrary.\nDespite the lack of structure, consistent testing turns out to still be\npossible by using the Gaussian character of the noise, and the minimax rate is\nshown to be $n^{-4\\alpha} + n^{-1/2}$. Exploiting noise information happens to\nbe a fundamental necessity as consistent testing is impossible if nothing more\nthan zero mean and unit variance is known about the noise distribution.\nFurthermore, in the setting where $V$ is $\\beta$-H\\\"{o}lder but\nheteroskedasticity is measured only with respect to the design points, the\nminimax separation rate is shown to be $n^{-4\\alpha} + n^{-\\left(\\frac{1}{2}\n\\vee \\frac{4\\beta}{4\\beta+1}\\right)}$ when the noise is Gaussian and\n$n^{-4\\alpha} + n^{-\\frac{4\\beta}{4\\beta+1}} + n^{-2\\beta}$ when the noise\ndistribution is unknown."}, "http://arxiv.org/abs/2310.12427": {"title": "Fast Power Curve Approximation for Posterior Analyses", "link": "http://arxiv.org/abs/2310.12427", "description": "Bayesian hypothesis testing leverages posterior probabilities, Bayes factors,\nor credible intervals to assess characteristics that summarize data. We propose\na framework for power curve approximation with such hypothesis tests that\nassumes data are generated using statistical models with fixed parameters for\nthe purposes of sample size determination. We present a fast approach to\nexplore the sampling distribution of posterior probabilities when the\nconditions for the Bernstein-von Mises theorem are satisfied. We extend that\napproach to facilitate targeted sampling from the approximate sampling\ndistribution of posterior probabilities for each sample size explored. These\nsampling distributions are used to construct power curves for various types of\nposterior analyses. Our resulting method for power curve approximation is\norders of magnitude faster than conventional power curve estimation for\nBayesian hypothesis tests. We also prove the consistency of the corresponding\npower estimates and sample size recommendations under certain conditions."}, "http://arxiv.org/abs/2310.12428": {"title": "Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach", "link": "http://arxiv.org/abs/2310.12428", "description": "We initiate a novel approach to explain the out of sample performance of\nrandom forest (RF) models by exploiting the fact that any RF can be formulated\nas an adaptive weighted K nearest-neighbors model. Specifically, we use the\nproximity between points in the feature space learned by the RF to re-write\nrandom forest predictions exactly as a weighted average of the target labels of\ntraining data points. This linearity facilitates a local notion of\nexplainability of RF predictions that generates attributions for any model\nprediction across observations in the training set, and thereby complements\nestablished methods like SHAP, which instead generates attributions for a model\nprediction across dimensions of the feature space. We demonstrate this approach\nin the context of a bond pricing model trained on US corporate bond trades, and\ncompare our approach to various existing approaches to model explainability."}, "http://arxiv.org/abs/2310.12460": {"title": "Linear Source Apportionment using Generalized Least Squares", "link": "http://arxiv.org/abs/2310.12460", "description": "Motivated by applications to water quality monitoring using fluorescence\nspectroscopy, we develop the source apportionment model for high dimensional\nprofiles of dissolved organic matter (DOM). We describe simple methods to\nestimate the parameters of a linear source apportionment model, and show how\nthe estimates are related to those of ordinary and generalized least squares.\nUsing this least squares framework, we analyze the variability of the\nestimates, and we propose predictors for missing elements of a DOM profile. We\ndemonstrate the practical utility of our results on fluorescence spectroscopy\ndata collected from the Neuse River in North Carolina."}, "http://arxiv.org/abs/2310.12711": {"title": "Modelling multivariate extremes through angular-radial decomposition of the density function", "link": "http://arxiv.org/abs/2310.12711", "description": "We present a new framework for modelling multivariate extremes, based on an\nangular-radial representation of the probability density function. Under this\nrepresentation, the problem of modelling multivariate extremes is transformed\nto that of modelling an angular density and the tail of the radial variable,\nconditional on angle. Motivated by univariate theory, we assume that the tail\nof the conditional radial distribution converges to a generalised Pareto (GP)\ndistribution. To simplify inference, we also assume that the angular density is\ncontinuous and finite and the GP parameter functions are continuous with angle.\nWe refer to the resulting model as the semi-parametric angular-radial (SPAR)\nmodel for multivariate extremes. We consider the effect of the choice of polar\ncoordinate system and introduce generalised concepts of angular-radial\ncoordinate systems and generalised scalar angles in two dimensions. We show\nthat under certain conditions, the choice of polar coordinate system does not\naffect the validity of the SPAR assumptions. However, some choices of\ncoordinate system lead to simpler representations. In contrast, we show that\nthe choice of margin does affect whether the model assumptions are satisfied.\nIn particular, the use of Laplace margins results in a form of the density\nfunction for which the SPAR assumptions are satisfied for many common families\nof copula, with various dependence classes. We show that the SPAR model\nprovides a more versatile framework for characterising multivariate extremes\nthan provided by existing approaches, and that several commonly-used approaches\nare special cases of the SPAR model. Moreover, the SPAR framework provides a\nmeans of characterising all `extreme regions' of a joint distribution using a\nsingle inference. Applications in which this is useful are discussed."}, "http://arxiv.org/abs/2310.12757": {"title": "Conservative Inference for Counterfactuals", "link": "http://arxiv.org/abs/2310.12757", "description": "In causal inference, the joint law of a set of counterfactual random\nvariables is generally not identified. We show that a conservative version of\nthe joint law - corresponding to the smallest treatment effect - is identified.\nFinding this law uses recent results from optimal transport theory. Under this\nconservative law we can bound causal effects and we may construct inferences\nfor each individual's counterfactual dose-response curve. Intuitively, this is\nthe flattest counterfactual curve for each subject that is consistent with the\ndistribution of the observables. If the outcome is univariate then, under mild\nconditions, this curve is simply the quantile function of the counterfactual\ndistribution that passes through the observed point. This curve corresponds to\na nonparametric rank preserving structural model."}, "http://arxiv.org/abs/2310.12882": {"title": "Sequential Gibbs Posteriors with Applications to Principal Component Analysis", "link": "http://arxiv.org/abs/2310.12882", "description": "Gibbs posteriors are proportional to a prior distribution multiplied by an\nexponentiated loss function, with a key tuning parameter weighting information\nin the loss relative to the prior and providing a control of posterior\nuncertainty. Gibbs posteriors provide a principled framework for\nlikelihood-free Bayesian inference, but in many situations, including a single\ntuning parameter inevitably leads to poor uncertainty quantification. In\nparticular, regardless of the value of the parameter, credible regions have far\nfrom the nominal frequentist coverage even in large samples. We propose a\nsequential extension to Gibbs posteriors to address this problem. We prove the\nproposed sequential posterior exhibits concentration and a Bernstein-von Mises\ntheorem, which holds under easy to verify conditions in Euclidean space and on\nmanifolds. As a byproduct, we obtain the first Bernstein-von Mises theorem for\ntraditional likelihood-based Bayesian posteriors on manifolds. All methods are\nillustrated with an application to principal component analysis."}, "http://arxiv.org/abs/2207.06949": {"title": "Seeking the Truth Beyond the Data", "link": "http://arxiv.org/abs/2207.06949", "description": "Clustering is an unsupervised machine learning methodology where unlabeled\nelements/objects are grouped together aiming to the construction of\nwell-established clusters that their elements are classified according to their\nsimilarity. The goal of this process is to provide a useful aid to the\nresearcher that will help her/him to identify patterns among the data. Dealing\nwith large databases, such patterns may not be easily detectable without the\ncontribution of a clustering algorithm. This article provides a deep\ndescription of the most widely used clustering methodologies accompanied by\nuseful presentations concerning suitable parameter selection and\ninitializations. Simultaneously, this article not only represents a review\nhighlighting the major elements of examined clustering techniques but\nemphasizes the comparison of these algorithms' clustering efficiency based on 3\ndatasets, revealing their existing weaknesses and capabilities through accuracy\nand complexity, during the confrontation of discrete and continuous\nobservations. The produced results help us extract valuable conclusions about\nthe appropriateness of the examined clustering techniques in accordance with\nthe dataset's size."}, "http://arxiv.org/abs/2208.07831": {"title": "Structured prior distributions for the covariance matrix in latent factor models", "link": "http://arxiv.org/abs/2208.07831", "description": "Factor models are widely used for dimension reduction in the analysis of\nmultivariate data. This is achieved through decomposition of a p x p covariance\nmatrix into the sum of two components. Through a latent factor representation,\nthey can be interpreted as a diagonal matrix of idiosyncratic variances and a\nshared variation matrix, that is, the product of a p x k factor loadings matrix\nand its transpose. If k &lt;&lt; p, this defines a sparse factorisation of the\ncovariance matrix. Historically, little attention has been paid to\nincorporating prior information in Bayesian analyses using factor models where,\nat best, the prior for the factor loadings is order invariant. In this work, a\nclass of structured priors is developed that can encode ideas of dependence\nstructure about the shared variation matrix. The construction allows\ndata-informed shrinkage towards sensible parametric structures while also\nfacilitating inference over the number of factors. Using an unconstrained\nreparameterisation of stationary vector autoregressions, the methodology is\nextended to stationary dynamic factor models. For computational inference,\nparameter-expanded Markov chain Monte Carlo samplers are proposed, including an\nefficient adaptive Gibbs sampler. Two substantive applications showcase the\nscope of the methodology and its inferential benefits."}, "http://arxiv.org/abs/2209.11840": {"title": "Revisiting the Analysis of Matched-Pair and Stratified Experiments in the Presence of Attrition", "link": "http://arxiv.org/abs/2209.11840", "description": "In this paper we revisit some common recommendations regarding the analysis\nof matched-pair and stratified experimental designs in the presence of\nattrition. Our main objective is to clarify a number of well-known claims about\nthe practice of dropping pairs with an attrited unit when analyzing\nmatched-pair designs. Contradictory advice appears in the literature about\nwhether or not dropping pairs is beneficial or harmful, and stratifying into\nlarger groups has been recommended as a resolution to the issue. To address\nthese claims, we derive the estimands obtained from the difference-in-means\nestimator in a matched-pair design both when the observations from pairs with\nan attrited unit are retained and when they are dropped. We find limited\nevidence to support the claims that dropping pairs helps recover the average\ntreatment effect, but we find that it may potentially help in recovering a\nconvex weighted average of conditional average treatment effects. We report\nsimilar findings for stratified designs when studying the estimands obtained\nfrom a regression of outcomes on treatment with and without strata fixed\neffects."}, "http://arxiv.org/abs/2211.01746": {"title": "Log-density gradient covariance and automatic metric tensors for Riemann manifold Monte Carlo methods", "link": "http://arxiv.org/abs/2211.01746", "description": "A metric tensor for Riemann manifold Monte Carlo particularly suited for\nnon-linear Bayesian hierarchical models is proposed. The metric tensor is built\nfrom symmetric positive semidefinite log-density gradient covariance (LGC)\nmatrices, which are also proposed and further explored here. The LGCs\ngeneralize the Fisher information matrix by measuring the joint information\ncontent and dependence structure of both a random variable and the parameters\nof said variable. Consequently, positive definite Fisher/LGC-based metric\ntensors may be constructed not only from the observation likelihoods as is\ncurrent practice, but also from arbitrarily complicated non-linear prior/latent\nvariable structures, provided the LGC may be derived for each conditional\ndistribution used to construct said structures. The proposed methodology is\nhighly automatic and allows for exploitation of any sparsity associated with\nthe model in question. When implemented in conjunction with a Riemann manifold\nvariant of the recently proposed numerical generalized randomized Hamiltonian\nMonte Carlo processes, the proposed methodology is highly competitive, in\nparticular for the more challenging target distributions associated with\nBayesian hierarchical models."}, "http://arxiv.org/abs/2211.02383": {"title": "Simulation-Based Calibration Checking for Bayesian Computation: The Choice of Test Quantities Shapes Sensitivity", "link": "http://arxiv.org/abs/2211.02383", "description": "Simulation-based calibration checking (SBC) is a practical method to validate\ncomputationally-derived posterior distributions or their approximations. In\nthis paper, we introduce a new variant of SBC to alleviate several known\nproblems. Our variant allows the user to in principle detect any possible issue\nwith the posterior, while previously reported implementations could never\ndetect large classes of problems including when the posterior is equal to the\nprior. This is made possible by including additional data-dependent test\nquantities when running SBC. We argue and demonstrate that the joint likelihood\nof the data is an especially useful test quantity. Some other types of test\nquantities and their theoretical and practical benefits are also investigated.\nWe provide theoretical analysis of SBC, thereby providing a more complete\nunderstanding of the underlying statistical mechanisms. We also bring attention\nto a relatively common mistake in the literature and clarify the difference\nbetween SBC and checks based on the data-averaged posterior. We support our\nrecommendations with numerical case studies on a multivariate normal example\nand a case study in implementing an ordered simplex data type for use with\nHamiltonian Monte Carlo. The SBC variant introduced in this paper is\nimplemented in the $\\mathtt{SBC}$ R package."}, "http://arxiv.org/abs/2310.13081": {"title": "Metastable Hidden Markov Processes: a theory for modeling financial markets", "link": "http://arxiv.org/abs/2310.13081", "description": "The modeling of financial time series by hidden Markov models has been\nperformed successfully in the literature. In this paper, we propose a theory\nthat justifies such a modeling under the assumption that there exists a market\nformed by agents whose states evolve on time as an interacting Markov system\nthat has a metastable behavior described by the hidden Markov chain. This\ntheory is a rare application of metastability outside the modeling of physical\nsystems, and may inspire the development of new interacting Markov systems with\nfinancial constraints. In the context of financial economics and causal factor\ninvestment, the theory implies a new paradigm in which fluctuations in\ninvestment performance are primarily driven by the state of the market, rather\nthan being directly caused by other variables. Even though the usual approach\nto causal factor investment based on causal inference is not completely\ninconsistent with the proposed theory, the latter has the advantage of\naccounting for the non-stationary evolution of the time series through the\nchange between hidden market states. By accounting for this possibility, one\ncan more effectively assess risks and implement mitigation strategies."}, "http://arxiv.org/abs/2310.13162": {"title": "Network Meta-Analysis of Time-to-Event Endpoints with Individual Participant Data using Restricted Mean Survival Time Regression", "link": "http://arxiv.org/abs/2310.13162", "description": "Restricted mean survival time (RMST) models have gained popularity when\nanalyzing time-to-event outcomes because RMST models offer more straightforward\ninterpretations of treatment effects with fewer assumptions than hazard ratios\ncommonly estimated from Cox models. However, few network meta-analysis (NMA)\nmethods have been developed using RMST. In this paper, we propose advanced RMST\nNMA models when individual participant data are available. Our models allow us\nto study treatment effect moderation and provide comprehensive understanding\nabout comparative effectiveness of treatments and subgroup effects. An\nextensive simulation study and a real data example about treatments for\npatients with atrial fibrillation are presented."}, "http://arxiv.org/abs/2310.13178": {"title": "Exact Inference for Common Odds Ratio in Meta-Analysis with Zero-Total-Event Studies", "link": "http://arxiv.org/abs/2310.13178", "description": "Stemming from the high profile publication of Nissen and Wolski (2007) and\nsubsequent discussions with divergent views on how to handle observed\nzero-total-event studies, defined to be studies which observe zero events in\nboth treatment and control arms, the research topic concerning the common odds\nratio model with zero-total-event studies remains to be an unresolved problem\nin meta-analysis. In this article, we address this problem by proposing a novel\nrepro samples method to handle zero-total-event studies and make inference for\nthe parameter of common odds ratio. The development explicitly accounts for\nsampling scheme and does not rely on large sample approximation. It is\ntheoretically justified with a guaranteed finite sample performance. The\nempirical performance of the proposed method is demonstrated through simulation\nstudies. It shows that the proposed confidence set achieves the desired\nempirical coverage rate and also that the zero-total-event studies contains\ninformation and impacts the inference for the common odds ratio. The proposed\nmethod is applied to combine information in the Nissen and Wolski study."}, "http://arxiv.org/abs/2310.13232": {"title": "Interaction Screening and Pseudolikelihood Approaches for Tensor Learning in Ising Models", "link": "http://arxiv.org/abs/2310.13232", "description": "In this paper, we study two well known methods of Ising structure learning,\nnamely the pseudolikelihood approach and the interaction screening approach, in\nthe context of tensor recovery in $k$-spin Ising models. We show that both\nthese approaches, with proper regularization, retrieve the underlying\nhypernetwork structure using a sample size logarithmic in the number of network\nnodes, and exponential in the maximum interaction strength and maximum\nnode-degree. We also track down the exact dependence of the rate of tensor\nrecovery on the interaction order $k$, that is allowed to grow with the number\nof samples and nodes, for both the approaches. Finally, we provide a\ncomparative discussion of the performance of the two approaches based on\nsimulation studies, which also demonstrate the exponential dependence of the\ntensor recovery rate on the maximum coupling strength."}, "http://arxiv.org/abs/2310.13387": {"title": "Assumption violations in causal discovery and the robustness of score matching", "link": "http://arxiv.org/abs/2310.13387", "description": "When domain knowledge is limited and experimentation is restricted by\nethical, financial, or time constraints, practitioners turn to observational\ncausal discovery methods to recover the causal structure, exploiting the\nstatistical properties of their data. Because causal discovery without further\nassumptions is an ill-posed problem, each algorithm comes with its own set of\nusually untestable assumptions, some of which are hard to meet in real\ndatasets. Motivated by these considerations, this paper extensively benchmarks\nthe empirical performance of recent causal discovery methods on observational\ni.i.d. data generated under different background conditions, allowing for\nviolations of the critical assumptions required by each selected approach. Our\nexperimental findings show that score matching-based methods demonstrate\nsurprising performance in the false positive and false negative rate of the\ninferred graph in these challenging scenarios, and we provide theoretical\ninsights into their performance. This work is also the first effort to\nbenchmark the stability of causal discovery algorithms with respect to the\nvalues of their hyperparameters. Finally, we hope this paper will set a new\nstandard for the evaluation of causal discovery methods and can serve as an\naccessible entry point for practitioners interested in the field, highlighting\nthe empirical implications of different algorithm choices."}, "http://arxiv.org/abs/2310.13444": {"title": "Testing for the extent of instability in nearly unstable processes", "link": "http://arxiv.org/abs/2310.13444", "description": "This paper deals with unit root issues in time series analysis. It has been\nknown for a long time that unit root tests may be flawed when a series although\nstationary has a root close to unity. That motivated recent papers dedicated to\nautoregressive processes where the bridge between stability and instability is\nexpressed by means of time-varying coefficients. In this vein the process we\nconsider has a companion matrix $A_{n}$ with spectral radius $\\rho(A_{n}) &lt; 1$\nsatisfying $\\rho(A_{n}) \\rightarrow 1$, a situation that we describe as `nearly\nunstable'. The question we investigate is the following: given an observed path\nsupposed to come from a nearly-unstable process, is it possible to test for the\n`extent of instability', \\textit{i.e.} to test how close we are to the unit\nroot? In this regard, we develop a strategy to evaluate $\\alpha$ and to test\nfor $\\mathcal{H}_0 : \"\\alpha = \\alpha_0\"$ against $\\mathcal{H}_1 : \"\\alpha &gt;\n\\alpha_0\"$ when $\\rho(A_{n})$ lies in an inner $O(n^{-\\alpha})$-neighborhood of\nthe unity, for some $0 &lt; \\alpha &lt; 1$. Empirical evidence is given (on\nsimulations and real time series) about the advantages of the flexibility\ninduced by such a procedure compared to the usual unit root tests and their\nbinary responses. As a by-product, we also build a symmetric procedure for the\nusually left out situation where the dominant root lies around $-1$."}, "http://arxiv.org/abs/2310.13446": {"title": "Simple binning algorithm and SimDec visualization for comprehensive sensitivity analysis of complex computational models", "link": "http://arxiv.org/abs/2310.13446", "description": "Models of complex technological systems inherently contain interactions and\ndependencies among their input variables that affect their joint influence on\nthe output. Such models are often computationally expensive and few sensitivity\nanalysis methods can effectively process such complexities. Moreover, the\nsensitivity analysis field as a whole pays limited attention to the nature of\ninteraction effects, whose understanding can prove to be critical for the\ndesign of safe and reliable systems. In this paper, we introduce and\nextensively test a simple binning approach for computing sensitivity indices\nand demonstrate how complementing it with the smart visualization method,\nsimulation decomposition (SimDec), can permit important insights into the\nbehavior of complex engineering models. The simple binning approach computes\nfirst-, second-order effects, and a combined sensitivity index, and is\nconsiderably more computationally efficient than Sobol' indices. The totality\nof the sensitivity analysis framework provides an efficient and intuitive way\nto analyze the behavior of complex systems containing interactions and\ndependencies."}, "http://arxiv.org/abs/2310.13487": {"title": "Two-stage weighted least squares estimator of multivariate discrete-valued observation-driven models", "link": "http://arxiv.org/abs/2310.13487", "description": "In this work a general semi-parametric multivariate model where the first two\nconditional moments are assumed to be multivariate time series is introduced.\nThe focus of the estimation is the conditional mean parameter vector for\ndiscrete-valued distributions. Quasi-Maximum Likelihood Estimators (QMLEs)\nbased on the linear exponential family are typically employed for such\nestimation problems when the true multivariate conditional probability\ndistribution is unknown or too complex. Although QMLEs provide consistent\nestimates they may be inefficient. In this paper novel two-stage Multivariate\nWeighted Least Square Estimators (MWLSEs) are introduced which enjoy the same\nconsistency property as the QMLEs but can provide improved efficiency with\nsuitable choice of the covariance matrix of the observations. The proposed\nmethod allows for a more accurate estimation of model parameters in particular\nfor count and categorical data when maximum likelihood estimation is\nunfeasible. Moreover, consistency and asymptotic normality of MWLSEs are\nderived. The estimation performance of QMLEs and MWLSEs is compared through\nsimulation experiments and a real data application, showing superior accuracy\nof the proposed methodology."}, "http://arxiv.org/abs/2310.13511": {"title": "Dynamic Realized Minimum Variance Portfolio Models", "link": "http://arxiv.org/abs/2310.13511", "description": "This paper introduces a dynamic minimum variance portfolio (MVP) model using\nnonlinear volatility dynamic models, based on high-frequency financial data.\nSpecifically, we impose an autoregressive dynamic structure on MVP processes,\nwhich helps capture the MVP dynamics directly. To evaluate the dynamic MVP\nmodel, we estimate the inverse volatility matrix using the constrained\n$\\ell_1$-minimization for inverse matrix estimation (CLIME) and calculate daily\nrealized non-normalized MVP weights. Based on the realized non-normalized MVP\nweight estimator, we propose the dynamic MVP model, which we call the dynamic\nrealized minimum variance portfolio (DR-MVP) model. To estimate a large number\nof parameters, we employ the least absolute shrinkage and selection operator\n(LASSO) and predict the future MVP and establish its asymptotic properties.\nUsing high-frequency trading data, we apply the proposed method to MVP\nprediction."}, "http://arxiv.org/abs/2310.13580": {"title": "Bayesian Hierarchical Modeling for Bivariate Multiscale Spatial Data with Application to Blood Test Monitoring", "link": "http://arxiv.org/abs/2310.13580", "description": "In public health applications, spatial data collected are often recorded at\ndifferent spatial scales and over different correlated variables. Spatial\nchange of support is a key inferential problem in these applications and have\nbecome standard in univariate settings; however, it is less standard in\nmultivariate settings. There are several existing multivariate spatial models\nthat can be easily combined with multiscale spatial approach to analyze\nmultivariate multiscale spatial data. In this paper, we propose three new\nmodels from such combinations for bivariate multiscale spatial data in a\nBayesian context. In particular, we extend spatial random effects models,\nmultivariate conditional autoregressive models, and ordered hierarchical models\nthrough a multiscale spatial approach. We run simulation studies for the three\nmodels and compare them in terms of prediction performance and computational\nefficiency. We motivate our models through an analysis of 2015 Texas annual\naverage percentage receiving two blood tests from the Dartmouth Atlas Project."}, "http://arxiv.org/abs/2102.13209": {"title": "Wielding Occam's razor: Fast and frugal retail forecasting", "link": "http://arxiv.org/abs/2102.13209", "description": "The algorithms available for retail forecasting have increased in complexity.\nNewer methods, such as machine learning, are inherently complex. The more\ntraditional families of forecasting models, such as exponential smoothing and\nautoregressive integrated moving averages, have expanded to contain multiple\npossible forms and forecasting profiles. We question complexity in forecasting\nand the need to consider such large families of models. Our argument is that\nparsimoniously identifying suitable subsets of models will not decrease\nforecasting accuracy nor will it reduce the ability to estimate forecast\nuncertainty. We propose a framework that balances forecasting performance\nversus computational cost, resulting in the consideration of only a reduced set\nof models. We empirically demonstrate that a reduced set performs well.\nFinally, we translate computational benefits to monetary cost savings and\nenvironmental impact and discuss the implications of our results in the context\nof large retailers."}, "http://arxiv.org/abs/2211.04666": {"title": "Fast and Locally Adaptive Bayesian Quantile Smoothing using Calibrated Variational Approximations", "link": "http://arxiv.org/abs/2211.04666", "description": "Quantiles are useful characteristics of random variables that can provide\nsubstantial information on distributions compared with commonly used summary\nstatistics such as means. In this paper, we propose a Bayesian quantile trend\nfiltering method to estimate non-stationary trend of quantiles. We introduce\ngeneral shrinkage priors to induce locally adaptive Bayesian inference on\ntrends and mixture representation of the asymmetric Laplace likelihood. To\nquickly compute the posterior distribution, we develop calibrated mean-field\nvariational approximations to guarantee that the frequentist coverage of\ncredible intervals obtained from the approximated posterior is a specified\nnominal level. Simulation and empirical studies show that the proposed\nalgorithm is computationally much more efficient than the Gibbs sampler and\ntends to provide stable inference results, especially for high/low quantiles."}, "http://arxiv.org/abs/2305.17631": {"title": "A Bayesian Approach for Clustering Constant-wise Change-point Data", "link": "http://arxiv.org/abs/2305.17631", "description": "Change-point models deal with ordered data sequences. Their primary goal is\nto infer the locations where an aspect of the data sequence changes. In this\npaper, we propose and implement a nonparametric Bayesian model for clustering\nobservations based on their constant-wise change-point profiles via Gibbs\nsampler. Our model incorporates a Dirichlet Process on the constant-wise\nchange-point structures to cluster observations while simultaneously performing\nchange-point estimation. Additionally, our approach controls the number of\nclusters in the model, not requiring the specification of the number of\nclusters a priori. Our method's performance is evaluated on simulated data\nunder various scenarios and on a real dataset from single-cell genomic\nsequencing."}, "http://arxiv.org/abs/2306.06342": {"title": "Distribution-free inference with hierarchical data", "link": "http://arxiv.org/abs/2306.06342", "description": "This paper studies distribution-free inference in settings where the data set\nhas a hierarchical structure -- for example, groups of observations, or\nrepeated measurements. In such settings, standard notions of exchangeability\nmay not hold. To address this challenge, a hierarchical form of exchangeability\nis derived, facilitating extensions of distribution-free methods, including\nconformal prediction and jackknife+. While the standard theoretical guarantee\nobtained by the conformal prediction framework is a marginal predictive\ncoverage guarantee, in the special case of independent repeated measurements,\nit is possible to achieve a stronger form of coverage -- the \"second-moment\ncoverage\" property -- to provide better control of conditional miscoverage\nrates, and distribution-free prediction sets that achieve this property are\nconstructed. Simulations illustrate that this guarantee indeed leads to\nuniformly small conditional miscoverage rates. Empirically, this stronger\nguarantee comes at the cost of a larger width of the prediction set in\nscenarios where the fitted model is poorly calibrated, but this cost is very\nmild in cases where the fitted model is accurate."}, "http://arxiv.org/abs/2307.15205": {"title": "Robust graph-based methods for overcoming the curse of dimensionality", "link": "http://arxiv.org/abs/2307.15205", "description": "Graph-based two-sample tests and graph-based change-point detection that\nutilize a similarity graph provide a powerful tool for analyzing\nhigh-dimensional and non-Euclidean data as these methods do not impose\ndistributional assumptions on data and have good performance across various\nscenarios. Current graph-based tests that deliver efficacy across a broad\nspectrum of alternatives typically reply on the $K$-nearest neighbor graph or\nthe $K$-minimum spanning tree. However, these graphs can be vulnerable for\nhigh-dimensional data due to the curse of dimensionality. To mitigate this\nissue, we propose to use a robust graph that is considerably less influenced by\nthe curse of dimensionality. We also establish a theoretical foundation for\ngraph-based methods utilizing this proposed robust graph and demonstrate its\nconsistency under fixed alternatives for both low-dimensional and\nhigh-dimensional data."}, "http://arxiv.org/abs/2310.13764": {"title": "Random Flows of Covariance Operators and their Statistical Inference", "link": "http://arxiv.org/abs/2310.13764", "description": "We develop a statistical framework for conducting inference on collections of\ntime-varying covariance operators (covariance flows) over a general, possibly\ninfinite dimensional, Hilbert space. We model the intrinsically non-linear\nstructure of covariances by means of the Bures-Wasserstein metric geometry. We\nmake use of the Riemmanian-like structure induced by this metric to define a\nnotion of mean and covariance of a random flow, and develop an associated\nKarhunen-Lo\\`eve expansion. We then treat the problem of estimation and\nconstruction of functional principal components from a finite collection of\ncovariance flows. Our theoretical results are motivated by modern problems in\nfunctional data analysis, where one observes operator-valued random processes\n-- for instance when analysing dynamic functional connectivity and fMRI data,\nor when analysing multiple functional time series in the frequency domain.\n{Nevertheless, our framework is also novel in the finite-dimensions (matrix\ncase), and we demonstrate what simplifications can be afforded then}. We\nillustrate our methodology by means of simulations and a data analyses."}, "http://arxiv.org/abs/2310.13796": {"title": "Faithful graphical representations of local independence", "link": "http://arxiv.org/abs/2310.13796", "description": "Graphical models use graphs to represent conditional independence structure\nin the distribution of a random vector. In stochastic processes, graphs may\nrepresent so-called local independence or conditional Granger causality. Under\nsome regularity conditions, a local independence graph implies a set of\nindependences using a graphical criterion known as $\\delta$-separation, or\nusing its generalization, $\\mu$-separation. This is a stochastic process\nanalogue of $d$-separation in DAGs. However, there may be more independences\nthan implied by this graph and this is a violation of so-called faithfulness.\nWe characterize faithfulness in local independence graphs and give a method to\nconstruct a faithful graph from any local independence model such that the\noutput equals the true graph when Markov and faithfulness assumptions hold. We\ndiscuss various assumptions that are weaker than faithfulness, and we explore\ndifferent structure learning algorithms and their properties under varying\nassumptions."}, "http://arxiv.org/abs/2310.13826": {"title": "A p-value for Process Tracing and other N=1 Studies", "link": "http://arxiv.org/abs/2310.13826", "description": "The paper introduces a \\(p\\)-value that summarizes the evidence against a\nrival causal theory that explains an observed outcome in a single case. We show\nhow to represent the probability distribution characterizing a theorized rival\nhypothesis (the null) in the absence of randomization of treatment and when\ncounting on qualitative data, for instance when conducting process tracing. As\nin Fisher's \\autocite*{fisher1935design} original design, our \\(p\\)-value\nindicates how frequently one would find the same observations or even more\nfavorable observations under a theory that is compatible with our observations\nbut antagonistic to the working hypothesis. We also present an extension that\nallows researchers assess the sensitivity of their results to confirmation\nbias. Finally, we illustrate the application of our hypothesis test using the\nstudy by Snow \\autocite*{Snow1855} about the cause of Cholera in Soho, a\nclassic in Process Tracing, Epidemiology, and Microbiology. Our framework suits\nany type of case studies and evidence, such as data from interviews, archives,\nor participant observation."}, "http://arxiv.org/abs/2310.13858": {"title": "Likelihood-based surrogate dimension reduction", "link": "http://arxiv.org/abs/2310.13858", "description": "We consider the problem of surrogate sufficient dimension reduction, that is,\nestimating the central subspace of a regression model, when the covariates are\ncontaminated by measurement error. When no measurement error is present, a\nlikelihood-based dimension reduction method that relies on maximizing the\nlikelihood of a Gaussian inverse regression model on the Grassmann manifold is\nwell-known to have superior performance to traditional inverse moment methods.\nWe propose two likelihood-based estimators for the central subspace in\nmeasurement error settings, which make different adjustments to the observed\nsurrogates. Both estimators are computed based on maximizing objective\nfunctions on the Grassmann manifold and are shown to consistently recover the\ntrue central subspace. When the central subspace is assumed to depend on only a\nfew covariates, we further propose to augment the likelihood function with a\npenalty term that induces sparsity on the Grassmann manifold to obtain sparse\nestimators. The resulting objective function has a closed-form Riemann gradient\nwhich facilitates efficient computation of the penalized estimator. We leverage\nthe state-of-the-art trust region algorithm on the Grassmann manifold to\ncompute the proposed estimators efficiently. Simulation studies and a data\napplication demonstrate the proposed likelihood-based estimators perform better\nthan inverse moment-based estimators in terms of both estimation and variable\nselection accuracy."}, "http://arxiv.org/abs/2310.13874": {"title": "A Linear Errors-in-Variables Model with Unknown Heteroscedastic Measurement Errors", "link": "http://arxiv.org/abs/2310.13874", "description": "In the classic measurement error framework, covariates are contaminated by\nindependent additive noise. This paper considers parameter estimation in such a\nlinear errors-in-variables model where the unknown measurement error\ndistribution is heteroscedastic across observations. We propose a new\ngeneralized method of moment (GMM) estimator that combines a moment correction\napproach and a phase function-based approach. The former requires distributions\nto have four finite moments, while the latter relies on covariates having\nasymmetric distributions. The new estimator is shown to be consistent and\nasymptotically normal under appropriate regularity conditions. The asymptotic\ncovariance of the estimator is derived, and the estimated standard error is\ncomputed using a fast bootstrap procedure. The GMM estimator is demonstrated to\nhave strong finite sample performance in numerical studies, especially when the\nmeasurement errors follow non-Gaussian distributions."}, "http://arxiv.org/abs/2310.13911": {"title": "Multilevel Matrix Factor Model", "link": "http://arxiv.org/abs/2310.13911", "description": "Large-scale matrix data has been widely discovered and continuously studied\nin various fields recently. Considering the multi-level factor structure and\nutilizing the matrix structure, we propose a multilevel matrix factor model\nwith both global and local factors. The global factors can affect all matrix\ntimes series, whereas the local factors are only allow to affect within each\nspecific matrix time series. The estimation procedures can consistently\nestimate the factor loadings and determine the number of factors. We establish\nthe asymptotic properties of the estimators. The simulation is presented to\nillustrate the performance of the proposed estimation method. We utilize the\nmodel to analyze eight indicators across 200 stocks from ten distinct\nindustries, demonstrating the empirical utility of our proposed approach."}, "http://arxiv.org/abs/2310.13966": {"title": "Minimax Optimal Transfer Learning for Kernel-based Nonparametric Regression", "link": "http://arxiv.org/abs/2310.13966", "description": "In recent years, transfer learning has garnered significant attention in the\nmachine learning community. Its ability to leverage knowledge from related\nstudies to improve generalization performance in a target study has made it\nhighly appealing. This paper focuses on investigating the transfer learning\nproblem within the context of nonparametric regression over a reproducing\nkernel Hilbert space. The aim is to bridge the gap between practical\neffectiveness and theoretical guarantees. We specifically consider two\nscenarios: one where the transferable sources are known and another where they\nare unknown. For the known transferable source case, we propose a two-step\nkernel-based estimator by solely using kernel ridge regression. For the unknown\ncase, we develop a novel method based on an efficient aggregation algorithm,\nwhich can automatically detect and alleviate the effects of negative sources.\nThis paper provides the statistical properties of the desired estimators and\nestablishes the minimax optimal rate. Through extensive numerical experiments\non synthetic data and real examples, we validate our theoretical findings and\ndemonstrate the effectiveness of our proposed method."}, "http://arxiv.org/abs/2310.13973": {"title": "Estimation and convergence rates in the distributional single index model", "link": "http://arxiv.org/abs/2310.13973", "description": "The distributional single index model is a semiparametric regression model in\nwhich the conditional distribution functions $P(Y \\leq y | X = x) =\nF_0(\\theta_0(x), y)$ of a real-valued outcome variable $Y$ depend on\n$d$-dimensional covariates $X$ through a univariate, parametric index function\n$\\theta_0(x)$, and increase stochastically as $\\theta_0(x)$ increases. We\npropose least squares approaches for the joint estimation of $\\theta_0$ and\n$F_0$ in the important case where $\\theta_0(x) = \\alpha_0^{\\top}x$ and obtain\nconvergence rates of $n^{-1/3}$, thereby improving an existing result that\ngives a rate of $n^{-1/6}$. A simulation study indicates that the convergence\nrate for the estimation of $\\alpha_0$ might be faster. Furthermore, we\nillustrate our methods in a real data application that demonstrates the\nadvantages of shape restrictions in single index models."}, "http://arxiv.org/abs/2310.14068": {"title": "Unobserved Grouped Heteroskedasticity and Fixed Effects", "link": "http://arxiv.org/abs/2310.14068", "description": "This paper extends the linear grouped fixed effects (GFE) panel model to\nallow for heteroskedasticity from a discrete latent group variable. Key\nfeatures of GFE are preserved, such as individuals belonging to one of a finite\nnumber of groups and group membership is unrestricted and estimated. Ignoring\ngroup heteroskedasticity may lead to poor classification, which is detrimental\nto finite sample bias and standard errors of estimators. I introduce the\n\"weighted grouped fixed effects\" (WGFE) estimator that minimizes a weighted\naverage of group sum of squared residuals. I establish $\\sqrt{NT}$-consistency\nand normality under a concept of group separation based on second moments. A\ntest of group homoskedasticity is discussed. A fast computation procedure is\nprovided. Simulations show that WGFE outperforms alternatives that exclude\nsecond moment information. I demonstrate this approach by considering the link\nbetween income and democracy and the effect of unionization on earnings."}, "http://arxiv.org/abs/2310.14246": {"title": "Shortcuts for causal discovery of nonlinear models by score matching", "link": "http://arxiv.org/abs/2310.14246", "description": "The use of simulated data in the field of causal discovery is ubiquitous due\nto the scarcity of annotated real data. Recently, Reisach et al., 2021\nhighlighted the emergence of patterns in simulated linear data, which displays\nincreasing marginal variance in the casual direction. As an ablation in their\nexperiments, Montagna et al., 2023 found that similar patterns may emerge in\nnonlinear models for the variance of the score vector $\\nabla \\log\np_{\\mathbf{X}}$, and introduced the ScoreSort algorithm. In this work, we\nformally define and characterize this score-sortability pattern of nonlinear\nadditive noise models. We find that it defines a class of identifiable\n(bivariate) causal models overlapping with nonlinear additive noise models. We\ntheoretically demonstrate the advantages of ScoreSort in terms of statistical\nefficiency compared to prior state-of-the-art score matching-based methods and\nempirically show the score-sortability of the most common synthetic benchmarks\nin the literature. Our findings remark (1) the lack of diversity in the data as\nan important limitation in the evaluation of nonlinear causal discovery\napproaches, (2) the importance of thoroughly testing different settings within\na problem class, and (3) the importance of analyzing statistical properties in\ncausal discovery, where research is often limited to defining identifiability\nconditions of the model."}, "http://arxiv.org/abs/2310.14293": {"title": "Testing exchangeability by pairwise betting", "link": "http://arxiv.org/abs/2310.14293", "description": "In this paper, we address the problem of testing exchangeability of a\nsequence of random variables, $X_1, X_2,\\cdots$. This problem has been studied\nunder the recently popular framework of testing by betting. But the mapping of\ntesting problems to game is not one to one: many games can be designed for the\nsame test. Past work established that it is futile to play single game betting\non every observation: test martingales in the data filtration are powerless.\nTwo avenues have been explored to circumvent this impossibility: betting in a\nreduced filtration (wealth is a test martingale in a coarsened filtration), or\nplaying many games in parallel (wealth is an e-process in the data filtration).\nThe former has proved to be difficult to theoretically analyze, while the\nlatter only works for binary or discrete observation spaces. Here, we introduce\na different approach that circumvents both drawbacks. We design a new (yet\nsimple) game in which we observe the data sequence in pairs. Despite the fact\nthat betting on individual observations is futile, we show that betting on\npairs of observations is not. To elaborate, we prove that our game leads to a\nnontrivial test martingale, which is interesting because it has been obtained\nby shrinking the filtration very slightly. We show that our test controls\ntype-1 error despite continuous monitoring, and achieves power one for both\nbinary and continuous observations, under a broad class of alternatives. Due to\nthe shrunk filtration, optional stopping is only allowed at even stopping\ntimes, not at odd ones: a relatively minor price. We provide a wide array of\nsimulations that align with our theoretical findings."}, "http://arxiv.org/abs/2310.14399": {"title": "The role of randomization inference in unraveling individual treatment effects in clinical trials: Application to HIV vaccine trials", "link": "http://arxiv.org/abs/2310.14399", "description": "Randomization inference is a powerful tool in early phase vaccine trials to\nestimate the causal effect of a regimen against a placebo or another regimen.\nTraditionally, randomization-based inference often focuses on testing either\nFisher's sharp null hypothesis of no treatment effect for any unit or Neyman's\nweak null hypothesis of no sample average treatment effect. Many recent efforts\nhave explored conducting exact randomization-based inference for other\nsummaries of the treatment effect profile, for instance, quantiles of the\ntreatment effect distribution function. In this article, we systematically\nreview methods that conduct exact, randomization-based inference for quantiles\nof individual treatment effects (ITEs) and extend some results by incorporating\nauxiliary information often available in a vaccine trial. These methods are\nsuitable for four scenarios: (i) a randomized controlled trial (RCT) where the\npotential outcomes under one regimen are constant; (ii) an RCT with no\nrestriction on any potential outcomes; (iii) an RCT with some user-specified\nbounds on potential outcomes; and (iv) a matched study comparing two\nnon-randomized, possibly confounded treatment arms. We then conduct two\nextensive simulation studies, one comparing the performance of each method in\nmany practical clinical settings and the other evaluating the usefulness of the\nmethods in ranking and advancing experimental therapies. We apply these methods\nto an early-phase clinical trail, HIV Vaccine Trials Network Study 086 (HVTN\n086), to showcase the usefulness of the methods."}, "http://arxiv.org/abs/2310.14419": {"title": "An RKHS Approach for Variable Selection in High-dimensional Functional Linear Models", "link": "http://arxiv.org/abs/2310.14419", "description": "High-dimensional functional data has become increasingly prevalent in modern\napplications such as high-frequency financial data and neuroimaging data\nanalysis. We investigate a class of high-dimensional linear regression models,\nwhere each predictor is a random element in an infinite dimensional function\nspace, and the number of functional predictors p can potentially be much\ngreater than the sample size n. Assuming that each of the unknown coefficient\nfunctions belongs to some reproducing kernel Hilbert space (RKHS), we\nregularized the fitting of the model by imposing a group elastic-net type of\npenalty on the RKHS norms of the coefficient functions. We show that our loss\nfunction is Gateaux sub-differentiable, and our functional elastic-net\nestimator exists uniquely in the product RKHS. Under suitable sparsity\nassumptions and a functional version of the irrepresentible condition, we\nestablish the variable selection consistency property of our approach. The\nproposed method is illustrated through simulation studies and a real-data\napplication from the Human Connectome Project."}, "http://arxiv.org/abs/2310.14448": {"title": "Semiparametrically Efficient Score for the Survival Odds Ratio", "link": "http://arxiv.org/abs/2310.14448", "description": "We consider a general proportional odds model for survival data under binary\ntreatment, where the functional form of the covariates is left unspecified. We\nderive the efficient score for the conditional survival odds ratio given the\ncovariates using modern semiparametric theory. The efficient score may be\nuseful in the development of doubly robust estimators, although computational\nchallenges remain."}, "http://arxiv.org/abs/2310.14763": {"title": "Externally Valid Policy Evaluation Combining Trial and Observational Data", "link": "http://arxiv.org/abs/2310.14763", "description": "Randomized trials are widely considered as the gold standard for evaluating\nthe effects of decision policies. Trial data is, however, drawn from a\npopulation which may differ from the intended target population and this raises\na problem of external validity (aka. generalizability). In this paper we seek\nto use trial data to draw valid inferences about the outcome of a policy on the\ntarget population. Additional covariate data from the target population is used\nto model the sampling of individuals in the trial study. We develop a method\nthat yields certifiably valid trial-based policy evaluations under any\nspecified range of model miscalibrations. The method is nonparametric and the\nvalidity is assured even with finite samples. The certified policy evaluations\nare illustrated using both simulated and real data."}, "http://arxiv.org/abs/2310.14922": {"title": "The Complex Network Patterns of Human Migration at Different Geographical Scales: Network Science meets Regression Analysis", "link": "http://arxiv.org/abs/2310.14922", "description": "Migration's influence in shaping population dynamics in times of impending\nclimate and population crises exposes its crucial role in upholding societal\ncohesion. As migration impacts virtually all aspects of life, it continues to\nrequire attention across scientific disciplines. This study delves into two\ndistinctive substrates of Migration Studies: the \"why\" substrate, which deals\nwith identifying the factors driving migration relying primarily on regression\nmodeling, encompassing economic, demographic, geographic, cultural, political,\nand other variables; and the \"how\" substrate, which focuses on identifying\nmigration flows and patterns, drawing from Network Science tools and\nvisualization techniques to depict complex migration networks. Despite the\ngrowing percentage of Network Science studies in migration, the explanations of\nthe identified network traits remain very scarce, highlighting the detachment\nbetween the two research substrates. Our study includes real-world network\nanalyses of human migration across different geographical levels: city,\ncountry, and global. We examine inter-district migration in Vienna at the city\nlevel, review internal migration networks in Austria and Croatia at the country\nlevel, and analyze migration exchange between Croatia and the world at the\nglobal level. By comparing network structures, we demonstrate how distinct\nnetwork traits impact regression modeling. This work not only uncovers\nmigration network patterns in previously unexplored areas but also presents a\ncomprehensive overview of recent research, highlighting gaps in each field and\ntheir interconnectedness. Our contribution offers suggestions for integrating\nboth fields to enhance methodological rigor and support future research."}, "http://arxiv.org/abs/2310.14983": {"title": "Causal clustering: design of cluster experiments under network interference", "link": "http://arxiv.org/abs/2310.14983", "description": "This paper studies the design of cluster experiments to estimate the global\ntreatment effect in the presence of spillovers on a single network. We provide\nan econometric framework to choose the clustering that minimizes the worst-case\nmean-squared error of the estimated global treatment effect. We show that the\noptimal clustering can be approximated as the solution of a novel penalized\nmin-cut optimization problem computed via off-the-shelf semi-definite\nprogramming algorithms. Our analysis also characterizes easy-to-check\nconditions to choose between a cluster or individual-level randomization. We\nillustrate the method's properties using unique network data from the universe\nof Facebook's users and existing network data from a field experiment."}, "http://arxiv.org/abs/2310.15016": {"title": "Impact of Record-Linkage Errors in Covid-19 Vaccine-Safety Analyses using German Health-Care Data: A Simulation Study", "link": "http://arxiv.org/abs/2310.15016", "description": "With unprecedented speed, 192,248,678 doses of Covid-19 vaccines were\nadministered in Germany by July 11, 2023 to combat the pandemic. Limitations of\nclinical trials imply that the safety profile of these vaccines is not fully\nknown before marketing. However, routine health-care data can help address\nthese issues. Despite the high proportion of insured people, the analysis of\nvaccination-related data is challenging in Germany. Generally, the Covid-19\nvaccination status and other health-care data are stored in separate databases,\nwithout persistent and database-independent person identifiers. Error-prone\nrecord-linkage techniques must be used to merge these databases. Our aim was to\nquantify the impact of record-linkage errors on the power and bias of different\nanalysis methods designed to assess Covid-19 vaccine safety when using German\nhealth-care data with a Monte-Carlo simulation study. We used a discrete-time\nsimulation and empirical data to generate realistic data with varying amounts\nof record-linkage errors. Afterwards, we analysed this data using a Cox model\nand the self-controlled case series (SCCS) method. Realistic proportions of\nrandom linkage errors only had little effect on the power of either method. The\nSCCS method produced unbiased results even with a high percentage of linkage\nerrors, while the Cox model underestimated the true effect."}, "http://arxiv.org/abs/2310.15069": {"title": "Second-order group knockoffs with applications to GWAS", "link": "http://arxiv.org/abs/2310.15069", "description": "Conditional testing via the knockoff framework allows one to identify --\namong large number of possible explanatory variables -- those that carry unique\ninformation about an outcome of interest, and also provides a false discovery\nrate guarantee on the selection. This approach is particularly well suited to\nthe analysis of genome wide association studies (GWAS), which have the goal of\nidentifying genetic variants which influence traits of medical relevance.\n\nWhile conditional testing can be both more powerful and precise than\ntraditional GWAS analysis methods, its vanilla implementation encounters a\ndifficulty common to all multivariate analysis methods: it is challenging to\ndistinguish among multiple, highly correlated regressors. This impasse can be\novercome by shifting the object of inference from single variables to groups of\ncorrelated variables. To achieve this, it is necessary to construct \"group\nknockoffs.\" While successful examples are already documented in the literature,\nthis paper substantially expands the set of algorithms and software for group\nknockoffs. We focus in particular on second-order knockoffs, for which we\ndescribe correlation matrix approximations that are appropriate for GWAS data\nand that result in considerable computational savings. We illustrate the\neffectiveness of the proposed methods with simulations and with the analysis of\nalbuminuria data from the UK Biobank.\n\nThe described algorithms are implemented in an open-source Julia package\nKnockoffs.jl, for which both R and Python wrappers are available."}, "http://arxiv.org/abs/2310.15070": {"title": "Improving estimation efficiency of case-cohort study with interval-censored failure time data", "link": "http://arxiv.org/abs/2310.15070", "description": "The case-cohort design is a commonly used cost-effective sampling strategy\nfor large cohort studies, where some covariates are expensive to measure or\nobtain. In this paper, we consider regression analysis under a case-cohort\nstudy with interval-censored failure time data, where the failure time is only\nknown to fall within an interval instead of being exactly observed. A common\napproach to analyze data from a case-cohort study is the inverse probability\nweighting approach, where only subjects in the case-cohort sample are used in\nestimation, and the subjects are weighted based on the probability of inclusion\ninto the case-cohort sample. This approach, though consistent, is generally\ninefficient as it does not incorporate information outside the case-cohort\nsample. To improve efficiency, we first develop a sieve maximum weighted\nlikelihood estimator under the Cox model based on the case-cohort sample, and\nthen propose a procedure to update this estimator by using information in the\nfull cohort. We show that the update estimator is consistent, asymptotically\nnormal, and more efficient than the original estimator. The proposed method can\nflexibly incorporate auxiliary variables to further improve estimation\nefficiency. We employ a weighted bootstrap procedure for variance estimation.\nSimulation results indicate that the proposed method works well in practical\nsituations. A real study on diabetes is provided for illustration."}, "http://arxiv.org/abs/2310.15108": {"title": "Evaluating machine learning models in non-standard settings: An overview and new findings", "link": "http://arxiv.org/abs/2310.15108", "description": "Estimating the generalization error (GE) of machine learning models is\nfundamental, with resampling methods being the most common approach. However,\nin non-standard settings, particularly those where observations are not\nindependently and identically distributed, resampling using simple random data\ndivisions may lead to biased GE estimates. This paper strives to present\nwell-grounded guidelines for GE estimation in various such non-standard\nsettings: clustered data, spatial data, unequal sampling probabilities, concept\ndrift, and hierarchically structured outcomes. Our overview combines\nwell-established methodologies with other existing methods that, to our\nknowledge, have not been frequently considered in these particular settings. A\nunifying principle among these techniques is that the test data used in each\niteration of the resampling procedure should reflect the new observations to\nwhich the model will be applied, while the training data should be\nrepresentative of the entire data set used to obtain the final model. Beyond\nproviding an overview, we address literature gaps by conducting simulation\nstudies. These studies assess the necessity of using GE-estimation methods\ntailored to the respective setting. Our findings corroborate the concern that\nstandard resampling methods often yield biased GE estimates in non-standard\nsettings, underscoring the importance of tailored GE estimation."}, "http://arxiv.org/abs/2310.15124": {"title": "Mixed-Variable Global Sensitivity Analysis For Knowledge Discovery And Efficient Combinatorial Materials Design", "link": "http://arxiv.org/abs/2310.15124", "description": "Global Sensitivity Analysis (GSA) is the study of the influence of any given\ninputs on the outputs of a model. In the context of engineering design, GSA has\nbeen widely used to understand both individual and collective contributions of\ndesign variables on the design objectives. So far, global sensitivity studies\nhave often been limited to design spaces with only quantitative (numerical)\ndesign variables. However, many engineering systems also contain, if not only,\nqualitative (categorical) design variables in addition to quantitative design\nvariables. In this paper, we integrate Latent Variable Gaussian Process (LVGP)\nwith Sobol' analysis to develop the first metamodel-based mixed-variable GSA\nmethod. Through numerical case studies, we validate and demonstrate the\neffectiveness of our proposed method for mixed-variable problems. Furthermore,\nwhile the proposed GSA method is general enough to benefit various engineering\ndesign applications, we integrate it with multi-objective Bayesian optimization\n(BO) to create a sensitivity-aware design framework in accelerating the Pareto\nfront design exploration for metal-organic framework (MOF) materials with\nmany-level combinatorial design spaces. Although MOFs are constructed only from\nqualitative variables that are notoriously difficult to design, our method can\nutilize sensitivity analysis to navigate the optimization in the many-level\nlarge combinatorial design space, greatly expediting the exploration of novel\nMOF candidates."}, "http://arxiv.org/abs/2003.04433": {"title": "Least Squares Estimation of a Quasiconvex Regression Function", "link": "http://arxiv.org/abs/2003.04433", "description": "We develop a new approach for the estimation of a multivariate function based\non the economic axioms of quasiconvexity (and monotonicity). On the\ncomputational side, we prove the existence of the quasiconvex constrained least\nsquares estimator (LSE) and provide a characterization of the function space to\ncompute the LSE via a mixed integer quadratic programme. On the theoretical\nside, we provide finite sample risk bounds for the LSE via a sharp oracle\ninequality. Our results allow for errors to depend on the covariates and to\nhave only two finite moments. We illustrate the superior performance of the LSE\nagainst some competing estimators via simulation. Finally, we use the LSE to\nestimate the production function for the Japanese plywood industry and the cost\nfunction for hospitals across the US."}, "http://arxiv.org/abs/2004.08318": {"title": "Causal Inference under Outcome-Based Sampling with Monotonicity Assumptions", "link": "http://arxiv.org/abs/2004.08318", "description": "We study causal inference under case-control and case-population sampling.\nSpecifically, we focus on the binary-outcome and binary-treatment case, where\nthe parameters of interest are causal relative and attributable risks defined\nvia the potential outcome framework. It is shown that strong ignorability is\nnot always as powerful as it is under random sampling and that certain\nmonotonicity assumptions yield comparable results in terms of sharp identified\nintervals. Specifically, the usual odds ratio is shown to be a sharp identified\nupper bound on causal relative risk under the monotone treatment response and\nmonotone treatment selection assumptions. We offer algorithms for inference on\nthe causal parameters that are aggregated over the true population distribution\nof the covariates. We show the usefulness of our approach by studying three\nempirical examples: the benefit of attending private school for entering a\nprestigious university in Pakistan; the relationship between staying in school\nand getting involved with drug-trafficking gangs in Brazil; and the link\nbetween physicians' hours and size of the group practice in the United States."}, "http://arxiv.org/abs/2008.10296": {"title": "Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison", "link": "http://arxiv.org/abs/2008.10296", "description": "Leave-one-out cross-validation (LOO-CV) is a popular method for comparing\nBayesian models based on their estimated predictive performance on new, unseen,\ndata. As leave-one-out cross-validation is based on finite observed data, there\nis uncertainty about the expected predictive performance on new data. By\nmodeling this uncertainty when comparing two models, we can compute the\nprobability that one model has a better predictive performance than the other.\nModeling this uncertainty well is not trivial, and for example, it is known\nthat the commonly used standard error estimate is often too small. We study the\nproperties of the Bayesian LOO-CV estimator and the related uncertainty\nestimates when comparing two models. We provide new results of the properties\nboth theoretically in the linear regression case and empirically for multiple\ndifferent models and discuss the challenges of modeling the uncertainty. We\nshow that problematic cases include: comparing models with similar predictions,\nmisspecified models, and small data. In these cases, there is a weak connection\nin the skewness of the individual leave-one-out terms and the distribution of\nthe error of the Bayesian LOO-CV estimator. We show that it is possible that\nthe problematic skewness of the error distribution, which occurs when the\nmodels make similar predictions, does not fade away when the data size grows to\ninfinity in certain situations. Based on the results, we also provide practical\nrecommendations for the users of Bayesian LOO-CV for model comparison."}, "http://arxiv.org/abs/2105.04981": {"title": "Uncovering patterns for adverse pregnancy outcomes with a Bayesian spatial model: Evidence from Philadelphia", "link": "http://arxiv.org/abs/2105.04981", "description": "We introduce a Bayesian conditional autoregressive model for analyzing\npatient-specific and neighborhood risks of stillbirth and preterm birth within\na city. Our fully Bayesian approach automatically learns the amount of spatial\nheterogeneity and spatial dependence between neighborhoods. Our model provides\nmeaningful inferences and uncertainty quantification for both covariate effects\nand neighborhood risk probabilities through their posterior distributions. We\napply our methodology to data from the city of Philadelphia. Using electronic\nhealth records (45,919 deliveries at hospitals within the University of\nPennsylvania Health System) and United States Census Bureau data from 363\ncensus tracts in Philadelphia, we find that both patient-level characteristics\n(e.g. self-identified race/ethnicity) and neighborhood-level characteristics\n(e.g. violent crime) are highly associated with patients' odds of stillbirth or\npreterm birth. Our neighborhood risk analysis further reveals that census\ntracts in West Philadelphia and North Philadelphia are at highest risk of these\noutcomes. Specifically, neighborhoods with higher rates of women in poverty or\non public assistance have greater neighborhood risk for these outcomes, while\nneighborhoods with higher rates of college-educated women or women in the labor\nforce have lower risk. Our findings could be useful for targeted individual and\nneighborhood interventions."}, "http://arxiv.org/abs/2107.07317": {"title": "Nonparametric Statistical Inference via Metric Distribution Function in Metric Spaces", "link": "http://arxiv.org/abs/2107.07317", "description": "Distribution function is essential in statistical inference, and connected\nwith samples to form a directed closed loop by the correspondence theorem in\nmeasure theory and the Glivenko-Cantelli and Donsker properties. This\nconnection creates a paradigm for statistical inference. However, existing\ndistribution functions are defined in Euclidean spaces and no longer convenient\nto use in rapidly evolving data objects of complex nature. It is imperative to\ndevelop the concept of distribution function in a more general space to meet\nemerging needs. Note that the linearity allows us to use hypercubes to define\nthe distribution function in a Euclidean space, but without the linearity in a\nmetric space, we must work with the metric to investigate the probability\nmeasure. We introduce a class of metric distribution functions through the\nmetric between random objects and a fixed location in metric spaces. We\novercome this challenging step by proving the correspondence theorem and the\nGlivenko-Cantelli theorem for metric distribution functions in metric spaces\nthat lie the foundation for conducting rational statistical inference for\nmetric space-valued data. Then, we develop homogeneity test and mutual\nindependence test for non-Euclidean random objects, and present comprehensive\nempirical evidence to support the performance of our proposed methods."}, "http://arxiv.org/abs/2108.07455": {"title": "Causal Inference with Noncompliance and Unknown Interference", "link": "http://arxiv.org/abs/2108.07455", "description": "We consider a causal inference model in which individuals interact in a\nsocial network and they may not comply with the assigned treatments. In\nparticular, we suppose that the form of network interference is unknown to\nresearchers. To estimate meaningful causal parameters in this situation, we\nintroduce a new concept of exposure mapping, which summarizes potentially\ncomplicated spillover effects into a fixed dimensional statistic of\ninstrumental variables. We investigate identification conditions for the\nintention-to-treat effects and the average treatment effects for compliers,\nwhile explicitly considering the possibility of misspecification of exposure\nmapping. Based on our identification results, we develop nonparametric\nestimation procedures via inverse probability weighting. Their asymptotic\nproperties, including consistency and asymptotic normality, are investigated\nusing an approximate neighborhood interference framework. For an empirical\nillustration, we apply our method to experimental data on the anti-conflict\nintervention school program. The proposed methods are readily available with\nthe companion R package latenetwork."}, "http://arxiv.org/abs/2109.03694": {"title": "Parameterizing and Simulating from Causal Models", "link": "http://arxiv.org/abs/2109.03694", "description": "Many statistical problems in causal inference involve a probability\ndistribution other than the one from which data are actually observed; as an\nadditional complication, the object of interest is often a marginal quantity of\nthis other probability distribution. This creates many practical complications\nfor statistical inference, even where the problem is non-parametrically\nidentified. In particular, it is difficult to perform likelihood-based\ninference, or even to simulate from the model in a general way.\n\nWe introduce the `frugal parameterization', which places the causal effect of\ninterest at its centre, and then builds the rest of the model around it. We do\nthis in a way that provides a recipe for constructing a regular, non-redundant\nparameterization using causal quantities of interest. In the case of discrete\nvariables we can use odds ratios to complete the parameterization, while in the\ncontinuous case copulas are the natural choice; other possibilities are also\ndiscussed.\n\nOur methods allow us to construct and simulate from models with\nparametrically specified causal distributions, and fit them using\nlikelihood-based methods, including fully Bayesian approaches. Our proposal\nincludes parameterizations for the average causal effect and effect of\ntreatment on the treated, as well as other causal quantities of interest."}, "http://arxiv.org/abs/2112.03872": {"title": "Nonparametric Treatment Effect Identification in School Choice", "link": "http://arxiv.org/abs/2112.03872", "description": "This paper studies nonparametric identification and estimation of causal\neffects in centralized school assignment. In many centralized assignment\nsettings, students are subjected to both lottery-driven variation and\nregression discontinuity (RD) driven variation. We characterize the full set of\nidentified atomic treatment effects (aTEs), defined as the conditional average\ntreatment effect between a pair of schools, given student characteristics.\nAtomic treatment effects are the building blocks of more aggregated notions of\ntreatment contrasts, and common approaches estimating aggregations of aTEs can\nmask important heterogeneity. In particular, many aggregations of aTEs put zero\nweight on aTEs driven by RD variation, and estimators of such aggregations put\nasymptotically vanishing weight on the RD-driven aTEs. We develop a diagnostic\ntool for empirically assessing the weight put on aTEs driven by RD variation.\nLastly, we provide estimators and accompanying asymptotic results for inference\non aggregations of RD-driven aTEs."}, "http://arxiv.org/abs/2202.09534": {"title": "Locally Adaptive Spatial Quantile Smoothing: Application to Monitoring Crime Density in Tokyo", "link": "http://arxiv.org/abs/2202.09534", "description": "Spatial trend estimation under potential heterogeneity is an important\nproblem to extract spatial characteristics and hazards such as criminal\nactivity. By focusing on quantiles, which provide substantial information on\ndistributions compared with commonly used summary statistics such as means, it\nis often useful to estimate not only the average trend but also the high (low)\nrisk trend additionally. In this paper, we propose a Bayesian quantile trend\nfiltering method to estimate the non-stationary trend of quantiles on graphs\nand apply it to crime data in Tokyo between 2013 and 2017. By modeling multiple\nobservation cases, we can estimate the potential heterogeneity of spatial crime\ntrends over multiple years in the application. To induce locally adaptive\nBayesian inference on trends, we introduce general shrinkage priors for graph\ndifferences. Introducing so-called shadow priors with multivariate distribution\nfor local scale parameters and mixture representation of the asymmetric Laplace\ndistribution, we provide a simple Gibbs sampling algorithm to generate\nposterior samples. The numerical performance of the proposed method is\ndemonstrated through simulation studies."}, "http://arxiv.org/abs/2203.16710": {"title": "Detecting Treatment Interference under the K-Nearest-Neighbors Interference Model", "link": "http://arxiv.org/abs/2203.16710", "description": "We propose a model of treatment interference where the response of a unit\ndepends only on its treatment status and the statuses of units within its\nK-neighborhood. Current methods for detecting interference include carefully\ndesigned randomized experiments and conditional randomization tests on a set of\nfocal units. We give guidance on how to choose focal units under this model of\ninterference. We then conduct a simulation study to evaluate the efficacy of\nexisting methods for detecting network interference. We show that this choice\nof focal units leads to powerful tests of treatment interference which\noutperform current experimental methods."}, "http://arxiv.org/abs/2206.00646": {"title": "Importance sampling for stochastic reaction-diffusion equations in the moderate deviation regime", "link": "http://arxiv.org/abs/2206.00646", "description": "We develop a provably efficient importance sampling scheme that estimates\nexit probabilities of solutions to small-noise stochastic reaction-diffusion\nequations from scaled neighborhoods of a stable equilibrium. The moderate\ndeviation scaling allows for a local approximation of the nonlinear dynamics by\ntheir linearized version. In addition, we identify a finite-dimensional\nsubspace where exits take place with high probability. Using stochastic control\nand variational methods we show that our scheme performs well both in the zero\nnoise limit and pre-asymptotically. Simulation studies for stochastically\nperturbed bistable dynamics illustrate the theoretical results."}, "http://arxiv.org/abs/2206.12084": {"title": "Functional Mixed Membership Models", "link": "http://arxiv.org/abs/2206.12084", "description": "Mixed membership models, or partial membership models, are a flexible\nunsupervised learning method that allows each observation to belong to multiple\nclusters. In this paper, we propose a Bayesian mixed membership model for\nfunctional data. By using the multivariate Karhunen-Lo\\`eve theorem, we are\nable to derive a scalable representation of Gaussian processes that maintains\ndata-driven learning of the covariance structure. Within this framework, we\nestablish conditional posterior consistency given a known feature allocation\nmatrix. Compared to previous work on mixed membership models, our proposal\nallows for increased modeling flexibility, with the benefit of a directly\ninterpretable mean and covariance structure. Our work is motivated by studies\nin functional brain imaging through electroencephalography (EEG) of children\nwith autism spectrum disorder (ASD). In this context, our work formalizes the\nclinical notion of \"spectrum\" in terms of feature membership proportions."}, "http://arxiv.org/abs/2208.07614": {"title": "Reweighting the RCT for generalization: finite sample error and variable selection", "link": "http://arxiv.org/abs/2208.07614", "description": "Randomized Controlled Trials (RCTs) may suffer from limited scope. In\nparticular, samples may be unrepresentative: some RCTs over- or under- sample\nindividuals with certain characteristics compared to the target population, for\nwhich one wants conclusions on treatment effectiveness. Re-weighting trial\nindividuals to match the target population can improve the treatment effect\nestimation. In this work, we establish the exact expressions of the bias and\nvariance of such reweighting procedures -- also called Inverse Propensity of\nSampling Weighting (IPSW) -- in presence of categorical covariates for any\nsample size. Such results allow us to compare the theoretical performance of\ndifferent versions of IPSW estimates. Besides, our results show how the\nperformance (bias, variance, and quadratic risk) of IPSW estimates depends on\nthe two sample sizes (RCT and target population). A by-product of our work is\nthe proof of consistency of IPSW estimates. Results also reveal that IPSW\nperformances are improved when the trial probability to be treated is estimated\n(rather than using its oracle counterpart). In addition, we study choice of\nvariables: how including covariates that are not necessary for identifiability\nof the causal effect may impact the asymptotic variance. Including covariates\nthat are shifted between the two samples but not treatment effect modifiers\nincreases the variance while non-shifted but treatment effect modifiers do not.\nWe illustrate all the takeaways in a didactic example, and on a semi-synthetic\nsimulation inspired from critical care medicine."}, "http://arxiv.org/abs/2209.15448": {"title": "Blessing from Human-AI Interaction: Super Reinforcement Learning in Confounded Environments", "link": "http://arxiv.org/abs/2209.15448", "description": "As AI becomes more prevalent throughout society, effective methods of\nintegrating humans and AI systems that leverage their respective strengths and\nmitigate risk have become an important priority. In this paper, we introduce\nthe paradigm of super reinforcement learning that takes advantage of Human-AI\ninteraction for data driven sequential decision making. This approach utilizes\nthe observed action, either from AI or humans, as input for achieving a\nstronger oracle in policy learning for the decision maker (humans or AI). In\nthe decision process with unmeasured confounding, the actions taken by past\nagents can offer valuable insights into undisclosed information. By including\nthis information for the policy search in a novel and legitimate manner, the\nproposed super reinforcement learning will yield a super-policy that is\nguaranteed to outperform both the standard optimal policy and the behavior one\n(e.g., past agents' actions). We call this stronger oracle a blessing from\nhuman-AI interaction. Furthermore, to address the issue of unmeasured\nconfounding in finding super-policies using the batch data, a number of\nnonparametric and causal identifications are established. Building upon on\nthese novel identification results, we develop several super-policy learning\nalgorithms and systematically study their theoretical properties such as\nfinite-sample regret guarantee. Finally, we illustrate the effectiveness of our\nproposal through extensive simulations and real-world applications."}, "http://arxiv.org/abs/2212.06906": {"title": "Flexible Regularized Estimation in High-Dimensional Mixed Membership Models", "link": "http://arxiv.org/abs/2212.06906", "description": "Mixed membership models are an extension of finite mixture models, where each\nobservation can partially belong to more than one mixture component. A\nprobabilistic framework for mixed membership models of high-dimensional\ncontinuous data is proposed with a focus on scalability and interpretability.\nThe novel probabilistic representation of mixed membership is based on convex\ncombinations of dependent multivariate Gaussian random vectors. In this\nsetting, scalability is ensured through approximations of a tensor covariance\nstructure through multivariate eigen-approximations with adaptive\nregularization imposed through shrinkage priors. Conditional weak posterior\nconsistency is established on an unconstrained model, allowing for a simple\nposterior sampling scheme while keeping many of the desired theoretical\nproperties of our model. The model is motivated by two biomedical case studies:\na case study on functional brain imaging of children with autism spectrum\ndisorder (ASD) and a case study on gene expression data from breast cancer\ntissue. These applications highlight how the typical assumption made in cluster\nanalysis, that each observation comes from one homogeneous subgroup, may often\nbe restrictive in several applications, leading to unnatural interpretations of\ndata features."}, "http://arxiv.org/abs/2301.09020": {"title": "On the Role of Volterra Integral Equations in Self-Consistent, Product-Limit, Inverse Probability of Censoring Weighted, and Redistribution-to-the-Right Estimators for the Survival Function", "link": "http://arxiv.org/abs/2301.09020", "description": "This paper reconsiders several results of historical and current importance\nto nonparametric estimation of the survival distribution for failure in the\npresence of right-censored observation times, demonstrating in particular how\nVolterra integral equations of the first kind help inter-connect the resulting\nestimators. The paper begins by considering Efron's self-consistency equation,\nintroduced in a seminal 1967 Berkeley symposium paper. Novel insights provided\nin the current work include the observations that (i) the self-consistency\nequation leads directly to an anticipating Volterra integral equation of the\nfirst kind whose solution is given by a product-limit estimator for the\ncensoring survival function; (ii) a definition used in this argument\nimmediately establishes the familiar product-limit estimator for the failure\nsurvival function; (iii) the usual Volterra integral equation for the\nproduct-limit estimator of the failure survival function leads to an immediate\nand simple proof that it can be represented as an inverse probability of\ncensoring weighted estimator (i.e., under appropriate conditions). Finally, we\nshow that the resulting inverse probability of censoring weighted estimators,\nattributed to a highly influential 1992 paper of Robins and Rotnitzky, were\nimplicitly introduced in Efron's 1967 paper in its development of the\nredistribution-to-the-right algorithm. All results developed herein allow for\nties between failure and/or censored observations."}, "http://arxiv.org/abs/2302.01576": {"title": "ResMem: Learn what you can and memorize the rest", "link": "http://arxiv.org/abs/2302.01576", "description": "The impressive generalization performance of modern neural networks is\nattributed in part to their ability to implicitly memorize complex training\npatterns. Inspired by this, we explore a novel mechanism to improve model\ngeneralization via explicit memorization. Specifically, we propose the\nresidual-memorization (ResMem) algorithm, a new method that augments an\nexisting prediction model (e.g. a neural network) by fitting the model's\nresiduals with a $k$-nearest neighbor based regressor. The final prediction is\nthen the sum of the original model and the fitted residual regressor. By\nconstruction, ResMem can explicitly memorize the training labels. Empirically,\nwe show that ResMem consistently improves the test set generalization of the\noriginal prediction model across various standard vision and natural language\nprocessing benchmarks. Theoretically, we formulate a stylized linear regression\nproblem and rigorously show that ResMem results in a more favorable test risk\nover the base predictor."}, "http://arxiv.org/abs/2303.05032": {"title": "Sensitivity analysis for principal ignorability violation in estimating complier and noncomplier average causal effects", "link": "http://arxiv.org/abs/2303.05032", "description": "An important strategy for identifying principal causal effects, which are\noften used in settings with noncompliance, is to invoke the principal\nignorability (PI) assumption. As PI is untestable, it is important to gauge how\nsensitive effect estimates are to its violation. We focus on this task for the\ncommon one-sided noncompliance setting where there are two principal strata,\ncompliers and noncompliers. Under PI, compliers and noncompliers share the same\noutcome-mean-given-covariates function under the control condition. For\nsensitivity analysis, we allow this function to differ between compliers and\nnoncompliers in several ways, indexed by an odds ratio, a generalized odds\nratio, a mean ratio, or a standardized mean difference sensitivity parameter.\nWe tailor sensitivity analysis techniques (with any sensitivity parameter\nchoice) to several types of PI-based main analysis methods, including outcome\nregression, influence function (IF) based and weighting methods. We illustrate\nthe proposed sensitivity analyses using several outcome types from the JOBS II\nstudy. This application estimates nuisance functions parametrically -- for\nsimplicity and accessibility. In addition, we establish rate conditions on\nnonparametric nuisance estimation for IF-based estimators to be asymptotically\nnormal -- with a view to inform nonparametric inference."}, "http://arxiv.org/abs/2304.13307": {"title": "A Statistical Interpretation of the Maximum Subarray Problem", "link": "http://arxiv.org/abs/2304.13307", "description": "Maximum subarray is a classical problem in computer science that given an\narray of numbers aims to find a contiguous subarray with the largest sum. We\nfocus on its use for a noisy statistical problem of localizing an interval with\na mean different from background. While a naive application of maximum subarray\nfails at this task, both a penalized and a constrained version can succeed. We\nshow that the penalized version can be derived for common exponential family\ndistributions, in a manner similar to the change-point detection literature,\nand we interpret the resulting optimal penalty value. The failure of the naive\nformulation is then explained by an analysis of the estimated interval\nboundaries. Experiments further quantify the effect of deviating from the\noptimal penalty. We also relate the penalized and constrained formulations and\nshow that the solutions to the former lie on the convex hull of the solutions\nto the latter."}, "http://arxiv.org/abs/2305.10637": {"title": "Conformalized matrix completion", "link": "http://arxiv.org/abs/2305.10637", "description": "Matrix completion aims to estimate missing entries in a data matrix, using\nthe assumption of a low-complexity structure (e.g., low rank) so that\nimputation is possible. While many effective estimation algorithms exist in the\nliterature, uncertainty quantification for this problem has proved to be\nchallenging, and existing methods are extremely sensitive to model\nmisspecification. In this work, we propose a distribution-free method for\npredictive inference in the matrix completion problem. Our method adapts the\nframework of conformal prediction, which provides confidence intervals with\nguaranteed distribution-free validity in the setting of regression, to the\nproblem of matrix completion. Our resulting method, conformalized matrix\ncompletion (cmc), offers provable predictive coverage regardless of the\naccuracy of the low-rank model. Empirical results on simulated and real data\ndemonstrate that cmc is robust to model misspecification while matching the\nperformance of existing model-based methods when the model is correct."}, "http://arxiv.org/abs/2305.15027": {"title": "A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods", "link": "http://arxiv.org/abs/2305.15027", "description": "We establish the first mathematically rigorous link between Bayesian,\nvariational Bayesian, and ensemble methods. A key step towards this it to\nreformulate the non-convex optimisation problem typically encountered in deep\nlearning as a convex optimisation in the space of probability measures. On a\ntechnical level, our contribution amounts to studying generalised variational\ninference through the lense of Wasserstein gradient flows. The result is a\nunified theory of various seemingly disconnected approaches that are commonly\nused for uncertainty quantification in deep learning -- including deep\nensembles and (variational) Bayesian methods. This offers a fresh perspective\non the reasons behind the success of deep ensembles over procedures based on\nparameterised variational inference, and allows the derivation of new\nensembling schemes with convergence guarantees. We showcase this by proposing a\nfamily of interacting deep ensembles with direct parallels to the interactions\nof particle systems in thermodynamics, and use our theory to prove the\nconvergence of these algorithms to a well-defined global minimiser on the space\nof probability measures."}, "http://arxiv.org/abs/2306.02584": {"title": "Synthetic Regressing Control Method", "link": "http://arxiv.org/abs/2306.02584", "description": "Estimating weights in the synthetic control method, typically resulting in\nsparse weights where only a few control units have non-zero weights, involves\nan optimization procedure that simultaneously selects and aligns control units\nto closely match the treated unit. However, this simultaneous selection and\nalignment of control units may lead to a loss of efficiency. Another concern\narising from the aforementioned procedure is its susceptibility to\nunder-fitting due to imperfect pre-treatment fit. It is not uncommon for the\nlinear combination, using nonnegative weights, of pre-treatment period outcomes\nfor the control units to inadequately approximate the pre-treatment outcomes\nfor the treated unit. To address both of these issues, this paper proposes a\nsimple and effective method called Synthetic Regressing Control (SRC). The SRC\nmethod begins by performing the univariate linear regression to appropriately\nalign the pre-treatment periods of the control units with the treated unit.\nSubsequently, a SRC estimator is obtained by synthesizing (taking a weighted\naverage) the fitted controls. To determine the weights in the synthesis\nprocedure, we propose an approach that utilizes a criterion of unbiased risk\nestimator. Theoretically, we show that the synthesis way is asymptotically\noptimal in the sense of achieving the lowest possible squared error. Extensive\nnumerical experiments highlight the advantages of the SRC method."}, "http://arxiv.org/abs/2308.05858": {"title": "Inconsistency and Acausality of Model Selection in Bayesian Inverse Problems", "link": "http://arxiv.org/abs/2308.05858", "description": "Bayesian inference paradigms are regarded as powerful tools for solution of\ninverse problems. However, when applied to inverse problems in physical\nsciences, Bayesian formulations suffer from a number of inconsistencies that\nare often overlooked. A well known, but mostly neglected, difficulty is\nconnected to the notion of conditional probability densities. Borel, and later\nKolmogorov's (1933/1956), found that the traditional definition of conditional\ndensities is incomplete: In different parameterizations it leads to different\nresults. We will show an example where two apparently correct procedures\napplied to the same problem lead to two widely different results. Another type\nof inconsistency involves violation of causality. This problem is found in\nmodel selection strategies in Bayesian inversion, such as Hierarchical Bayes\nand Trans-Dimensional Inversion where so-called hyperparameters are included as\nvariables to control either the number (or type) of unknowns, or the prior\nuncertainties on data or model parameters. For Hierarchical Bayes we\ndemonstrate that the calculated 'prior' distributions of data or model\nparameters are not prior-, but posterior information. In fact, the calculated\n'standard deviations' of the data are a measure of the inability of the forward\nfunction to model the data, rather than uncertainties of the data. For\ntrans-dimensional inverse problems we show that the so-called evidence is, in\nfact, not a measure of the success of fitting the data for the given choice (or\nnumber) of parameters, as often claimed. We also find that the notion of\nNatural Parsimony is ill-defined, because of its dependence on the parameter\nprior. Based on this study, we find that careful rethinking of Bayesian\ninversion practices is required, with special emphasis on ways of avoiding the\nBorel-Kolmogorov inconsistency, and on the way we interpret model selection\nresults."}, "http://arxiv.org/abs/2308.12470": {"title": "Scalable Estimation of Multinomial Response Models with Uncertain Consideration Sets", "link": "http://arxiv.org/abs/2308.12470", "description": "A standard assumption in the fitting of unordered multinomial response models\nfor $J$ mutually exclusive nominal categories, on cross-sectional or\nlongitudinal data, is that the responses arise from the same set of $J$\ncategories between subjects. However, when responses measure a choice made by\nthe subject, it is more appropriate to assume that the distribution of\nmultinomial responses is conditioned on a subject-specific consideration set,\nwhere this consideration set is drawn from the power set of $\\{1,2,\\ldots,J\\}$.\nBecause the cardinality of this power set is exponential in $J$, estimation is\ninfeasible in general. In this paper, we provide an approach to overcoming this\nproblem. A key step in the approach is a probability model over consideration\nsets, based on a general representation of probability distributions on\ncontingency tables, which results in mixtures of independent consideration\nmodels. Although the support of this distribution is exponentially large, the\nposterior distribution over consideration sets given parameters is typically\nsparse, and is easily sampled in an MCMC scheme. We show posterior consistency\nof the parameters of the conditional response model and the distribution of\nconsideration sets. The effectiveness of the methodology is documented in\nsimulated longitudinal data sets with $J=100$ categories and real data from the\ncereal market with $J=68$ brands."}, "http://arxiv.org/abs/2310.15266": {"title": "Causal progress with imperfect placebo treatments and outcomes", "link": "http://arxiv.org/abs/2310.15266", "description": "In the quest to make defensible causal claims from observational data, it is\nsometimes possible to leverage information from \"placebo treatments\" and\n\"placebo outcomes\" (or \"negative outcome controls\"). Existing approaches\nemploying such information focus largely on point identification and assume (i)\n\"perfect placebos\", meaning placebo treatments have precisely zero effect on\nthe outcome and the real treatment has precisely zero effect on a placebo\noutcome; and (ii) \"equiconfounding\", meaning that the treatment-outcome\nrelationship where one is a placebo suffers the same amount of confounding as\ndoes the real treatment-outcome relationship, on some scale. We instead\nconsider an omitted variable bias framework, in which users can postulate\nnon-zero effects of placebo treatment on real outcomes or of real treatments on\nplacebo outcomes, and the relative strengths of confounding suffered by a\nplacebo treatment/outcome compared to the true treatment-outcome relationship.\nOnce postulated, these assumptions identify or bound the linear estimates of\ntreatment effects. While applicable in many settings, one ubiquitous use-case\nfor this approach is to employ pre-treatment outcomes as (perfect) placebo\noutcomes. In this setting, the parallel trends assumption of\ndifference-in-difference is in fact a strict equiconfounding assumption on a\nparticular scale, which can be relaxed in our framework. Finally, we\ndemonstrate the use of our framework with two applications, employing an R\npackage that implements these approaches."}, "http://arxiv.org/abs/2310.15333": {"title": "Estimating Trustworthy and Safe Optimal Treatment Regimes", "link": "http://arxiv.org/abs/2310.15333", "description": "Recent statistical and reinforcement learning methods have significantly\nadvanced patient care strategies. However, these approaches face substantial\nchallenges in high-stakes contexts, including missing data, inherent\nstochasticity, and the critical requirements for interpretability and patient\nsafety. Our work operationalizes a safe and interpretable framework to identify\noptimal treatment regimes. This approach involves matching patients with\nsimilar medical and pharmacological characteristics, allowing us to construct\nan optimal policy via interpolation. We perform a comprehensive simulation\nstudy to demonstrate the framework's ability to identify optimal policies even\nin complex settings. Ultimately, we operationalize our approach to study\nregimes for treating seizures in critically ill patients. Our findings strongly\nsupport personalized treatment strategies based on a patient's medical history\nand pharmacological features. Notably, we identify that reducing medication\ndoses for patients with mild and brief seizure episodes while adopting\naggressive treatment for patients in intensive care unit experiencing intense\nseizures leads to more favorable outcomes."}, "http://arxiv.org/abs/2310.15459": {"title": "Strategies to mitigate bias from time recording errors in pharmacokinetic studies", "link": "http://arxiv.org/abs/2310.15459", "description": "Opportunistic pharmacokinetic (PK) studies have sparse and imbalanced\nclinical measurement data, and the impact of sample time errors is an important\nconcern when seeking accurate estimates of treatment response. We evaluated an\napproximate Bayesian model for individualized pharmacokinetics in the presence\nof time recording errors (TREs), considering both a short and long infusion\ndosing pattern. We found that the long infusion schedule generally had lower\nbias in estimates of the pharmacodynamic (PD) endpoint relative to the short\ninfusion schedule. We investigated three different design strategies for their\nability to mitigate the impact of TREs: (i) shifting blood draws taken during\nan active infusion to the post-infusion period, (ii) identifying the best next\nsample time by minimizing bias in the presence of TREs, and (iii) collecting\nadditional information on a subset of patients based on estimate uncertainty or\nquadrature-estimated variance in the presence of TREs. Generally, the proposed\nstrategies led to a decrease in bias of the PD estimate for the short infusion\nschedule, but had a negligible impact for the long infusion schedule. Dosing\nregimens with periods of high non-linearity may benefit from design\nmodifications, while more stable concentration-time profiles are generally more\nrobust to TREs with no design modifications."}, "http://arxiv.org/abs/2310.15497": {"title": "Generalized Box-Cox method to estimate sample mean and standard deviation for Meta-analysis", "link": "http://arxiv.org/abs/2310.15497", "description": "Meta-analysis is the aggregation of data from multiple studies to find\npatterns across a broad range relating to a particular subject. It is becoming\nincreasingly useful to apply meta-analysis to summarize these studies being\ndone across various fields. In meta-analysis, it is common to use the mean and\nstandard deviation from each study to compare for analysis. While many studies\nreported mean and standard deviation for their summary statistics, some report\nother values including the minimum, maximum, median, and first and third\nquantiles. Often, the quantiles and median are reported when the data is skewed\nand does not follow a normal distribution. In order to correctly summarize the\ndata and draw conclusions from multiple studies, it is necessary to estimate\nthe mean and standard deviation from each study, considering variation and\nskewness within each study. In past literature, methods have been proposed to\nestimate the mean and standard deviation, but do not consider negative values.\nData that include negative values are common and would increase the accuracy\nand impact of the me-ta-analysis. We propose a method that implements a\ngeneralized Box-Cox transformation to estimate the mean and standard deviation\naccounting for such negative values while maintaining similar accuracy."}, "http://arxiv.org/abs/2310.15877": {"title": "Regression analysis of multiplicative hazards model with time-dependent coefficient for sparse longitudinal covariates", "link": "http://arxiv.org/abs/2310.15877", "description": "We study the multiplicative hazards model with intermittently observed\nlongitudinal covariates and time-varying coefficients. For such models, the\nexisting {\\it ad hoc} approach, such as the last value carried forward, is\nbiased. We propose a kernel weighting approach to get an unbiased estimation of\nthe non-parametric coefficient function and establish asymptotic normality for\nany fixed time point. Furthermore, we construct the simultaneous confidence\nband to examine the overall magnitude of the variation. Simulation studies\nsupport our theoretical predictions and show favorable performance of the\nproposed method. A data set from cerebral infarction is used to illustrate our\nmethodology."}, "http://arxiv.org/abs/2310.15956": {"title": "Likelihood-Based Inference for Semi-Parametric Transformation Cure Models with Interval Censored Data", "link": "http://arxiv.org/abs/2310.15956", "description": "A simple yet effective way of modeling survival data with cure fraction is by\nconsidering Box-Cox transformation cure model (BCTM) that unifies mixture and\npromotion time cure models. In this article, we numerically study the\nstatistical properties of the BCTM when applied to interval censored data.\nTime-to-events associated with susceptible subjects are modeled through\nproportional hazards structure that allows for non-homogeneity across subjects,\nwhere the baseline hazard function is estimated by distribution-free piecewise\nlinear function with varied degrees of non-parametricity. Due to missing cured\nstatuses for right censored subjects, maximum likelihood estimates of model\nparameters are obtained by developing an expectation-maximization (EM)\nalgorithm. Under the EM framework, the conditional expectation of the complete\ndata log-likelihood function is maximized by considering all parameters\n(including the Box-Cox transformation parameter $\\alpha$) simultaneously, in\ncontrast to conventional profile-likelihood technique of estimating $\\alpha$.\nThe robustness and accuracy of the model and estimation method are established\nthrough a detailed simulation study under various parameter settings, and an\nanalysis of real-life data obtained from a smoking cessation study."}, "http://arxiv.org/abs/1901.04916": {"title": "Pairwise accelerated failure time regression models for infectious disease transmission in close-contact groups with external sources of infection", "link": "http://arxiv.org/abs/1901.04916", "description": "Many important questions in infectious disease epidemiology involve the\neffects of covariates (e.g., age or vaccination status) on infectiousness and\nsusceptibility, which can be measured in studies of transmission in households\nor other close-contact groups. Because the transmission of disease produces\ndependent outcomes, these questions are difficult or impossible to address\nusing standard regression models from biostatistics. Pairwise survival analysis\nhandles dependent outcomes by calculating likelihoods in terms of contact\ninterval distributions in ordered pairs of individuals. The contact interval in\nthe ordered pair ij is the time from the onset of infectiousness in i to\ninfectious contact from i to j, where an infectious contact is sufficient to\ninfect j if they are susceptible. Here, we introduce a pairwise accelerated\nfailure time regression model for infectious disease transmission that allows\nthe rate parameter of the contact interval distribution to depend on\ninfectiousness covariates for i, susceptibility covariates for j, and pairwise\ncovariates. This model can simultaneously handle internal infections (caused by\ntransmission between individuals under observation) and external infections\n(caused by environmental or community sources of infection). In a simulation\nstudy, we show that these models produce valid point and interval estimates of\nparameters governing the contact interval distributions. We also explore the\nrole of epidemiologic study design and the consequences of model\nmisspecification. We use this regression model to analyze household data from\nLos Angeles County during the 2009 influenza A (H1N1) pandemic, where we find\nthat the ability to account for external sources of infection is critical to\nestimating the effect of antiviral prophylaxis."}, "http://arxiv.org/abs/2003.06416": {"title": "VCBART: Bayesian trees for varying coefficients", "link": "http://arxiv.org/abs/2003.06416", "description": "The linear varying coefficient models posits a linear relationship between an\noutcome and covariates in which the covariate effects are modeled as functions\nof additional effect modifiers. Despite a long history of study and use in\nstatistics and econometrics, state-of-the-art varying coefficient modeling\nmethods cannot accommodate multivariate effect modifiers without imposing\nrestrictive functional form assumptions or involving computationally intensive\nhyperparameter tuning. In response, we introduce VCBART, which flexibly\nestimates the covariate effect in a varying coefficient model using Bayesian\nAdditive Regression Trees. With simple default settings, VCBART outperforms\nexisting varying coefficient methods in terms of covariate effect estimation,\nuncertainty quantification, and outcome prediction. We illustrate the utility\nof VCBART with two case studies: one examining how the association between\nlater-life cognition and measures of socioeconomic position vary with respect\nto age and socio-demographics and another estimating how temporal trends in\nurban crime vary at the neighborhood level. An R package implementing VCBART is\navailable at https://github.com/skdeshpande91/VCBART"}, "http://arxiv.org/abs/2204.05870": {"title": "How much of the past matters? Using dynamic survival models for the monitoring of potassium in heart failure patients using electronic health records", "link": "http://arxiv.org/abs/2204.05870", "description": "Statistical methods to study the association between a longitudinal biomarker\nand the risk of death are very relevant for the long-term care of subjects\naffected by chronic illnesses, such as potassium in heart failure patients.\nParticularly in the presence of comorbidities or pharmacological treatments,\nsudden crises can cause potassium to undergo very abrupt yet transient changes.\nIn the context of the monitoring of potassium, there is a need for a dynamic\nmodel that can be used in clinical practice to assess the risk of death related\nto an observed patient's potassium trajectory. We considered different dynamic\nsurvival approaches, starting from the simple approach considering the most\nrecent measurement, to the joint model. We then propose a novel method based on\nwavelet filtering and landmarking to retrieve the prognostic role of past\nshort-term potassium shifts. We argue that while taking into account past\ninformation is important, not all past information is equally informative.\nState-of-the-art dynamic survival models are prone to give more importance to\nthe mean long-term value of potassium. However, our findings suggest that it is\nessential to take into account also recent potassium instability to capture all\nthe relevant prognostic information. The data used comes from over 2000\nsubjects, with a total of over 80 000 repeated potassium measurements collected\nthrough Administrative Health Records and Outpatient and Inpatient Clinic\nE-charts. A novel dynamic survival approach is proposed in this work for the\nmonitoring of potassium in heart failure. The proposed wavelet landmark method\nshows promising results revealing the prognostic role of past short-term\nchanges, according to their different duration, and achieving higher\nperformances in predicting the survival probability of individuals."}, "http://arxiv.org/abs/2212.09494": {"title": "Optimal Treatment Regimes for Proximal Causal Learning", "link": "http://arxiv.org/abs/2212.09494", "description": "A common concern when a policymaker draws causal inferences from and makes\ndecisions based on observational data is that the measured covariates are\ninsufficiently rich to account for all sources of confounding, i.e., the\nstandard no confoundedness assumption fails to hold. The recently proposed\nproximal causal inference framework shows that proxy variables that abound in\nreal-life scenarios can be leveraged to identify causal effects and therefore\nfacilitate decision-making. Building upon this line of work, we propose a novel\noptimal individualized treatment regime based on so-called outcome and\ntreatment confounding bridges. We then show that the value function of this new\noptimal treatment regime is superior to that of existing ones in the\nliterature. Theoretical guarantees, including identification, superiority,\nexcess value bound, and consistency of the estimated regime, are established.\nFurthermore, we demonstrate the proposed optimal regime via numerical\nexperiments and a real data application."}, "http://arxiv.org/abs/2301.09016": {"title": "Inference for Two-stage Experiments under Covariate-Adaptive Randomization", "link": "http://arxiv.org/abs/2301.09016", "description": "This paper studies inference in two-stage randomized experiments under\ncovariate-adaptive randomization. In the initial stage of this experimental\ndesign, clusters (e.g., households, schools, or graph partitions) are\nstratified and randomly assigned to control or treatment groups based on\ncluster-level covariates. Subsequently, an independent second-stage design is\ncarried out, wherein units within each treated cluster are further stratified\nand randomly assigned to either control or treatment groups, based on\nindividual-level covariates. Under the homogeneous partial interference\nassumption, I establish conditions under which the proposed\ndifference-in-\"average of averages\" estimators are consistent and\nasymptotically normal for the corresponding average primary and spillover\neffects and develop consistent estimators of their asymptotic variances.\nCombining these results establishes the asymptotic validity of tests based on\nthese estimators. My findings suggest that ignoring covariate information in\nthe design stage can result in efficiency loss, and commonly used inference\nmethods that ignore or improperly use covariate information can lead to either\nconservative or invalid inference. Finally, I apply these results to studying\noptimal use of covariate information under covariate-adaptive randomization in\nlarge samples, and demonstrate that a specific generalized matched-pair design\nachieves minimum asymptotic variance for each proposed estimator. The practical\nrelevance of the theoretical results is illustrated through a simulation study\nand an empirical application."}, "http://arxiv.org/abs/2302.07294": {"title": "Derandomized Novelty Detection with FDR Control via Conformal E-values", "link": "http://arxiv.org/abs/2302.07294", "description": "Conformal inference provides a general distribution-free method to rigorously\ncalibrate the output of any machine learning algorithm for novelty detection.\nWhile this approach has many strengths, it has the limitation of being\nrandomized, in the sense that it may lead to different results when analyzing\ntwice the same data, and this can hinder the interpretation of any findings. We\npropose to make conformal inferences more stable by leveraging suitable\nconformal e-values instead of p-values to quantify statistical significance.\nThis solution allows the evidence gathered from multiple analyses of the same\ndata to be aggregated effectively while provably controlling the false\ndiscovery rate. Further, we show that the proposed method can reduce randomness\nwithout much loss of power compared to standard conformal inference, partly\nthanks to an innovative way of weighting conformal e-values based on additional\nside information carefully extracted from the same data. Simulations with\nsynthetic and real data confirm this solution can be effective at eliminating\nrandom noise in the inferences obtained with state-of-the-art alternative\ntechniques, sometimes also leading to higher power."}, "http://arxiv.org/abs/2304.02127": {"title": "A Bayesian Collocation Integral Method for Parameter Estimation in Ordinary Differential Equations", "link": "http://arxiv.org/abs/2304.02127", "description": "Inferring the parameters of ordinary differential equations (ODEs) from noisy\nobservations is an important problem in many scientific fields. Currently, most\nparameter estimation methods that bypass numerical integration tend to rely on\nbasis functions or Gaussian processes to approximate the ODE solution and its\nderivatives. Due to the sensitivity of the ODE solution to its derivatives,\nthese methods can be hindered by estimation error, especially when only sparse\ntime-course observations are available. We present a Bayesian collocation\nframework that operates on the integrated form of the ODEs and also avoids the\nexpensive use of numerical solvers. Our methodology has the capability to\nhandle general nonlinear ODE systems. We demonstrate the accuracy of the\nproposed method through simulation studies, where the estimated parameters and\nrecovered system trajectories are compared with other recent methods. A real\ndata example is also provided."}, "http://arxiv.org/abs/2307.00127": {"title": "Large-scale Bayesian Structure Learning for Gaussian Graphical Models using Marginal Pseudo-likelihood", "link": "http://arxiv.org/abs/2307.00127", "description": "Bayesian methods for learning Gaussian graphical models offer a robust\nframework that addresses model uncertainty and incorporates prior knowledge.\nDespite their theoretical strengths, the applicability of Bayesian methods is\noften constrained by computational needs, especially in modern contexts\ninvolving thousands of variables. To overcome this issue, we introduce two\nnovel Markov chain Monte Carlo (MCMC) search algorithms that have a\nsignificantly lower computational cost than leading Bayesian approaches. Our\nproposed MCMC-based search algorithms use the marginal pseudo-likelihood\napproach to bypass the complexities of computing intractable normalizing\nconstants and iterative precision matrix sampling. These algorithms can deliver\nreliable results in mere minutes on standard computers, even for large-scale\nproblems with one thousand variables. Furthermore, our proposed method is\ncapable of addressing model uncertainty by efficiently exploring the full\nposterior graph space. Our simulation study indicates that the proposed\nalgorithms, particularly for large-scale sparse graphs, outperform the leading\nBayesian approaches in terms of computational efficiency and precision. The\nimplementation supporting the new approach is available through the R package\nBDgraph."}, "http://arxiv.org/abs/2307.09302": {"title": "Conformal prediction under ambiguous ground truth", "link": "http://arxiv.org/abs/2307.09302", "description": "Conformal Prediction (CP) allows to perform rigorous uncertainty\nquantification by constructing a prediction set $C(X)$ satisfying $\\mathbb{P}(Y\n\\in C(X))\\geq 1-\\alpha$ for a user-chosen $\\alpha \\in [0,1]$ by relying on\ncalibration data $(X_1,Y_1),...,(X_n,Y_n)$ from $\\mathbb{P}=\\mathbb{P}^{X}\n\\otimes \\mathbb{P}^{Y|X}$. It is typically implicitly assumed that\n$\\mathbb{P}^{Y|X}$ is the \"true\" posterior label distribution. However, in many\nreal-world scenarios, the labels $Y_1,...,Y_n$ are obtained by aggregating\nexpert opinions using a voting procedure, resulting in a one-hot distribution\n$\\mathbb{P}_{vote}^{Y|X}$. For such ``voted'' labels, CP guarantees are thus\nw.r.t. $\\mathbb{P}_{vote}=\\mathbb{P}^X \\otimes \\mathbb{P}_{vote}^{Y|X}$ rather\nthan the true distribution $\\mathbb{P}$. In cases with unambiguous ground truth\nlabels, the distinction between $\\mathbb{P}_{vote}$ and $\\mathbb{P}$ is\nirrelevant. However, when experts do not agree because of ambiguous labels,\napproximating $\\mathbb{P}^{Y|X}$ with a one-hot distribution\n$\\mathbb{P}_{vote}^{Y|X}$ ignores this uncertainty. In this paper, we propose\nto leverage expert opinions to approximate $\\mathbb{P}^{Y|X}$ using a\nnon-degenerate distribution $\\mathbb{P}_{agg}^{Y|X}$. We develop Monte Carlo CP\nprocedures which provide guarantees w.r.t. $\\mathbb{P}_{agg}=\\mathbb{P}^X\n\\otimes \\mathbb{P}_{agg}^{Y|X}$ by sampling multiple synthetic pseudo-labels\nfrom $\\mathbb{P}_{agg}^{Y|X}$ for each calibration example $X_1,...,X_n$. In a\ncase study of skin condition classification with significant disagreement among\nexpert annotators, we show that applying CP w.r.t. $\\mathbb{P}_{vote}$\nunder-covers expert annotations: calibrated for $72\\%$ coverage, it falls short\nby on average $10\\%$; our Monte Carlo CP closes this gap both empirically and\ntheoretically."}, "http://arxiv.org/abs/2310.16203": {"title": "Multivariate Dynamic Mediation Analysis under a Reinforcement Learning Framework", "link": "http://arxiv.org/abs/2310.16203", "description": "Mediation analysis is an important analytic tool commonly used in a broad\nrange of scientific applications. In this article, we study the problem of\nmediation analysis when there are multivariate and conditionally dependent\nmediators, and when the variables are observed over multiple time points. The\nproblem is challenging, because the effect of a mediator involves not only the\npath from the treatment to this mediator itself at the current time point, but\nalso all possible paths pointed to this mediator from its upstream mediators,\nas well as the carryover effects from all previous time points. We propose a\nnovel multivariate dynamic mediation analysis approach. Drawing inspiration\nfrom the Markov decision process model that is frequently employed in\nreinforcement learning, we introduce a Markov mediation process paired with a\nsystem of time-varying linear structural equation models to formulate the\nproblem. We then formally define the individual mediation effect, built upon\nthe idea of simultaneous interventions and intervention calculus. We next\nderive the closed-form expression and propose an iterative estimation procedure\nunder the Markov mediation process model. We study both the asymptotic property\nand the empirical performance of the proposed estimator, and further illustrate\nour method with a mobile health application."}, "http://arxiv.org/abs/2310.16207": {"title": "Propensity score weighting plus an adjusted proportional hazards model does not equal doubly robust away from the null", "link": "http://arxiv.org/abs/2310.16207", "description": "Recently it has become common for applied works to combine commonly used\nsurvival analysis modeling methods, such as the multivariable Cox model, and\npropensity score weighting with the intention of forming a doubly robust\nestimator that is unbiased in large samples when either the Cox model or the\npropensity score model is correctly specified. This combination does not, in\ngeneral, produce a doubly robust estimator, even after regression\nstandardization, when there is truly a causal effect. We demonstrate via\nsimulation this lack of double robustness for the semiparametric Cox model, the\nWeibull proportional hazards model, and a simple proportional hazards flexible\nparametric model, with both the latter models fit via maximum likelihood. We\nprovide a novel proof that the combination of propensity score weighting and a\nproportional hazards survival model, fit either via full or partial likelihood,\nis consistent under the null of no causal effect of the exposure on the outcome\nunder particular censoring mechanisms if either the propensity score or the\noutcome model is correctly specified and contains all confounders. Given our\nresults suggesting that double robustness only exists under the null, we\noutline two simple alternative estimators that are doubly robust for the\nsurvival difference at a given time point (in the above sense), provided the\ncensoring mechanism can be correctly modeled, and one doubly robust method of\nestimation for the full survival curve. We provide R code to use these\nestimators for estimation and inference in the supplementary materials."}, "http://arxiv.org/abs/2310.16213": {"title": "Bayes factor functions", "link": "http://arxiv.org/abs/2310.16213", "description": "We describe Bayes factors functions based on z, t, $\\chi^2$, and F statistics\nand the prior distributions used to define alternative hypotheses. The\nnon-local alternative prior distributions are centered on standardized effects,\nwhich index the Bayes factor function. The prior densities include a dispersion\nparameter that models the variation of effect sizes across replicated\nexperiments. We examine the convergence rates of Bayes factor functions under\ntrue null and true alternative hypotheses. Several examples illustrate the\napplication of the Bayes factor functions to replicated experimental designs\nand compare the conclusions from these analyses to other default Bayes factor\nmethods."}, "http://arxiv.org/abs/2310.16256": {"title": "A Causal Disentangled Multi-Granularity Graph Classification Method", "link": "http://arxiv.org/abs/2310.16256", "description": "Graph data widely exists in real life, with large amounts of data and complex\nstructures. It is necessary to map graph data to low-dimensional embedding.\nGraph classification, a critical graph task, mainly relies on identifying the\nimportant substructures within the graph. At present, some graph classification\nmethods do not combine the multi-granularity characteristics of graph data.\nThis lack of granularity distinction in modeling leads to a conflation of key\ninformation and false correlations within the model. So, achieving the desired\ngoal of a credible and interpretable model becomes challenging. This paper\nproposes a causal disentangled multi-granularity graph representation learning\nmethod (CDM-GNN) to solve this challenge. The CDM-GNN model disentangles the\nimportant substructures and bias parts within the graph from a\nmulti-granularity perspective. The disentanglement of the CDM-GNN model reveals\nimportant and bias parts, forming the foundation for its classification task,\nspecifically, model interpretations. The CDM-GNN model exhibits strong\nclassification performance and generates explanatory outcomes aligning with\nhuman cognitive patterns. In order to verify the effectiveness of the model,\nthis paper compares the three real-world datasets MUTAG, PTC, and IMDM-M. Six\nstate-of-the-art models, namely GCN, GAT, Top-k, ASAPool, SUGAR, and SAT are\nemployed for comparison purposes. Additionally, a qualitative analysis of the\ninterpretation results is conducted."}, "http://arxiv.org/abs/2310.16260": {"title": "Private Estimation and Inference in High-Dimensional Regression with FDR Control", "link": "http://arxiv.org/abs/2310.16260", "description": "This paper presents novel methodologies for conducting practical\ndifferentially private (DP) estimation and inference in high-dimensional linear\nregression. We start by proposing a differentially private Bayesian Information\nCriterion (BIC) for selecting the unknown sparsity parameter in DP-Lasso,\neliminating the need for prior knowledge of model sparsity, a requisite in the\nexisting literature. Then we propose a differentially private debiased LASSO\nalgorithm that enables privacy-preserving inference on regression parameters.\nOur proposed method enables accurate and private inference on the regression\nparameters by leveraging the inherent sparsity of high-dimensional linear\nregression models. Additionally, we address the issue of multiple testing in\nhigh-dimensional linear regression by introducing a differentially private\nmultiple testing procedure that controls the false discovery rate (FDR). This\nallows for accurate and privacy-preserving identification of significant\npredictors in the regression model. Through extensive simulations and real data\nanalysis, we demonstrate the efficacy of our proposed methods in conducting\ninference for high-dimensional linear models while safeguarding privacy and\ncontrolling the FDR."}, "http://arxiv.org/abs/2310.16284": {"title": "Bayesian Image Mediation Analysis", "link": "http://arxiv.org/abs/2310.16284", "description": "Mediation analysis aims to separate the indirect effect through mediators\nfrom the direct effect of the exposure on the outcome. It is challenging to\nperform mediation analysis with neuroimaging data which involves high\ndimensionality, complex spatial correlations, sparse activation patterns and\nrelatively low signal-to-noise ratio. To address these issues, we develop a new\nspatially varying coefficient structural equation model for Bayesian Image\nMediation Analysis (BIMA). We define spatially varying mediation effects within\nthe potential outcome framework, employing the soft-thresholded Gaussian\nprocess prior for functional parameters. We establish the posterior consistency\nfor spatially varying mediation effects along with selection consistency on\nimportant regions that contribute to the mediation effects. We develop an\nefficient posterior computation algorithm scalable to analysis of large-scale\nimaging data. Through extensive simulations, we show that BIMA can improve the\nestimation accuracy and computational efficiency for high-dimensional mediation\nanalysis over the existing methods. We apply BIMA to analyze the behavioral and\nfMRI data in the Adolescent Brain Cognitive Development (ABCD) study with a\nfocus on inferring the mediation effects of the parental education level on the\nchildren's general cognitive ability that are mediated through the working\nmemory brain activities."}, "http://arxiv.org/abs/2310.16290": {"title": "Fair Adaptive Experiments", "link": "http://arxiv.org/abs/2310.16290", "description": "Randomized experiments have been the gold standard for assessing the\neffectiveness of a treatment or policy. The classical complete randomization\napproach assigns treatments based on a prespecified probability and may lead to\ninefficient use of data. Adaptive experiments improve upon complete\nrandomization by sequentially learning and updating treatment assignment\nprobabilities. However, their application can also raise fairness and equity\nconcerns, as assignment probabilities may vary drastically across groups of\nparticipants. Furthermore, when treatment is expected to be extremely\nbeneficial to certain groups of participants, it is more appropriate to expose\nmany of these participants to favorable treatment. In response to these\nchallenges, we propose a fair adaptive experiment strategy that simultaneously\nenhances data use efficiency, achieves an envy-free treatment assignment\nguarantee, and improves the overall welfare of participants. An important\nfeature of our proposed strategy is that we do not impose parametric modeling\nassumptions on the outcome variables, making it more versatile and applicable\nto a wider array of applications. Through our theoretical investigation, we\ncharacterize the convergence rate of the estimated treatment effects and the\nassociated standard deviations at the group level and further prove that our\nadaptive treatment assignment algorithm, despite not having a closed-form\nexpression, approaches the optimal allocation rule asymptotically. Our proof\nstrategy takes into account the fact that the allocation decisions in our\ndesign depend on sequentially accumulated data, which poses a significant\nchallenge in characterizing the properties and conducting statistical inference\nof our method. We further provide simulation evidence to showcase the\nperformance of our fair adaptive experiment strategy."}, "http://arxiv.org/abs/2310.16294": {"title": "Producer-Side Experiments Based on Counterfactual Interleaving Designs for Online Recommender Systems", "link": "http://arxiv.org/abs/2310.16294", "description": "Recommender systems have become an integral part of online platforms,\nproviding personalized suggestions for purchasing items, consuming contents,\nand connecting with individuals. An online recommender system consists of two\nsides of components: the producer side comprises product sellers, content\ncreators, or service providers, etc., and the consumer side includes buyers,\nviewers, or guests, etc. To optimize an online recommender system, A/B tests\nserve as the golden standard for comparing different ranking models and\nevaluating their impact on both the consumers and producers. While\nconsumer-side experiments are relatively straightforward to design and commonly\nused to gauge the impact of ranking changes on the behavior of consumers\n(buyers, viewers, etc.), designing producer-side experiments presents a\nconsiderable challenge because producer items in the treatment and control\ngroups need to be ranked by different models and then merged into a single\nranking for the recommender to show to each consumer. In this paper, we review\nissues with the existing methods, propose new design principles for\nproducer-side experiments, and develop a rigorous solution based on\ncounterfactual interleaving designs for accurately measuring the effects of\nranking changes on the producers (sellers, creators, etc.)."}, "http://arxiv.org/abs/2310.16466": {"title": "Learning Continuous Network Emerging Dynamics from Scarce Observations via Data-Adaptive Stochastic Processes", "link": "http://arxiv.org/abs/2310.16466", "description": "Learning network dynamics from the empirical structure and spatio-temporal\nobservation data is crucial to revealing the interaction mechanisms of complex\nnetworks in a wide range of domains. However, most existing methods only aim at\nlearning network dynamic behaviors generated by a specific ordinary\ndifferential equation instance, resulting in ineffectiveness for new ones, and\ngenerally require dense observations. The observed data, especially from\nnetwork emerging dynamics, are usually difficult to obtain, which brings\ntrouble to model learning. Therefore, how to learn accurate network dynamics\nwith sparse, irregularly-sampled, partial, and noisy observations remains a\nfundamental challenge. We introduce Neural ODE Processes for Network Dynamics\n(NDP4ND), a new class of stochastic processes governed by stochastic\ndata-adaptive network dynamics, to overcome the challenge and learn continuous\nnetwork dynamics from scarce observations. Intensive experiments conducted on\nvarious network dynamics in ecological population evolution, phototaxis\nmovement, brain activity, epidemic spreading, and real-world empirical systems,\ndemonstrate that the proposed method has excellent data adaptability and\ncomputational efficiency, and can adapt to unseen network emerging dynamics,\nproducing accurate interpolation and extrapolation with reducing the ratio of\nrequired observation data to only about 6\\% and improving the learning speed\nfor new dynamics by three orders of magnitude."}, "http://arxiv.org/abs/2310.16489": {"title": "Latent event history models for quasi-reaction systems", "link": "http://arxiv.org/abs/2310.16489", "description": "Various processes can be modelled as quasi-reaction systems of stochastic\ndifferential equations, such as cell differentiation and disease spreading.\nSince the underlying data of particle interactions, such as reactions between\nproteins or contacts between people, are typically unobserved, statistical\ninference of the parameters driving these systems is developed from\nconcentration data measuring each unit in the system over time. While observing\nthe continuous time process at a time scale as fine as possible should in\ntheory help with parameter estimation, the existing Local Linear Approximation\n(LLA) methods fail in this case, due to numerical instability caused by small\nchanges of the system at successive time points. On the other hand, one may be\nable to reconstruct the underlying unobserved interactions from the observed\ncount data. Motivated by this, we first formalise the latent event history\nmodel underlying the observed count process. We then propose a computationally\nefficient Expectation-Maximation algorithm for parameter estimation, with an\nextended Kalman filtering procedure for the prediction of the latent states. A\nsimulation study shows the performance of the proposed method and highlights\nthe settings where it is particularly advantageous compared to the existing LLA\napproaches. Finally, we present an illustration of the methodology on the\nspreading of the COVID-19 pandemic in Italy."}, "http://arxiv.org/abs/2310.16502": {"title": "Assessing the overall and partial causal well-specification of nonlinear additive noise models", "link": "http://arxiv.org/abs/2310.16502", "description": "We propose a method to detect model misspecifications in nonlinear causal\nadditive and potentially heteroscedastic noise models. We aim to identify\npredictor variables for which we can infer the causal effect even in cases of\nsuch misspecification. We develop a general framework based on knowledge of the\nmultivariate observational data distribution and we then propose an algorithm\nfor finite sample data, discuss its asymptotic properties, and illustrate its\nperformance on simulated and real data."}, "http://arxiv.org/abs/2310.16600": {"title": "Balancing central and marginal rejection when combining independent significance tests", "link": "http://arxiv.org/abs/2310.16600", "description": "A common approach to evaluating the significance of a collection of\n$p$-values combines them with a pooling function, in particular when the\noriginal data are not available. These pooled $p$-values convert a sample of\n$p$-values into a single number which behaves like a univariate $p$-value. To\nclarify discussion of these functions, a telescoping series of alternative\nhypotheses are introduced that communicate the strength and prevalence of\nnon-null evidence in the $p$-values before general pooling formulae are\ndiscussed. A pattern noticed in the UMP pooled $p$-value for a particular\nalternative motivates the definition and discussion of central and marginal\nrejection levels at $\\alpha$. It is proven that central rejection is always\ngreater than or equal to marginal rejection, motivating a quotient to measure\nthe balance between the two for pooled $p$-values. A combining function based\non the $\\chi^2_{\\kappa}$ quantile transformation is proposed to control this\nquotient and shown to be robust to mis-specified parameters relative to the\nUMP. Different powers for different parameter settings motivate a map of\nplausible alternatives based on where this pooled $p$-value is minimized."}, "http://arxiv.org/abs/2310.16626": {"title": "Scalable Causal Structure Learning via Amortized Conditional Independence Testing", "link": "http://arxiv.org/abs/2310.16626", "description": "Controlling false positives (Type I errors) through statistical hypothesis\ntesting is a foundation of modern scientific data analysis. Existing causal\nstructure discovery algorithms either do not provide Type I error control or\ncannot scale to the size of modern scientific datasets. We consider a variant\nof the causal discovery problem with two sets of nodes, where the only edges of\ninterest form a bipartite causal subgraph between the sets. We develop Scalable\nCausal Structure Learning (SCSL), a method for causal structure discovery on\nbipartite subgraphs that provides Type I error control. SCSL recasts the\ndiscovery problem as a simultaneous hypothesis testing problem and uses\ndiscrete optimization over the set of possible confounders to obtain an upper\nbound on the test statistic for each edge. Semi-synthetic simulations\ndemonstrate that SCSL scales to handle graphs with hundreds of nodes while\nmaintaining error control and good power. We demonstrate the practical\napplicability of the method by applying it to a cancer dataset to reveal\nconnections between somatic gene mutations and metastases to different tissues."}, "http://arxiv.org/abs/2310.16638": {"title": "Covariate Shift Adaptation Robust to Density-Ratio Estimation", "link": "http://arxiv.org/abs/2310.16638", "description": "Consider a scenario where we have access to train data with both covariates\nand outcomes while test data only contains covariates. In this scenario, our\nprimary aim is to predict the missing outcomes of the test data. With this\nobjective in mind, we train parametric regression models under a covariate\nshift, where covariate distributions are different between the train and test\ndata. For this problem, existing studies have proposed covariate shift\nadaptation via importance weighting using the density ratio. This approach\naverages the train data losses, each weighted by an estimated ratio of the\ncovariate densities between the train and test data, to approximate the\ntest-data risk. Although it allows us to obtain a test-data risk minimizer, its\nperformance heavily relies on the accuracy of the density ratio estimation.\nMoreover, even if the density ratio can be consistently estimated, the\nestimation errors of the density ratio also yield bias in the estimators of the\nregression model's parameters of interest. To mitigate these challenges, we\nintroduce a doubly robust estimator for covariate shift adaptation via\nimportance weighting, which incorporates an additional estimator for the\nregression function. Leveraging double machine learning techniques, our\nestimator reduces the bias arising from the density ratio estimation errors. We\ndemonstrate the asymptotic distribution of the regression parameter estimator.\nNotably, our estimator remains consistent if either the density ratio estimator\nor the regression function is consistent, showcasing its robustness against\npotential errors in density ratio estimation. Finally, we confirm the soundness\nof our proposed method via simulation studies."}, "http://arxiv.org/abs/2310.16650": {"title": "Data-integration with pseudoweights and survey-calibration: application to developing US-representative lung cancer risk models for use in screening", "link": "http://arxiv.org/abs/2310.16650", "description": "Accurate cancer risk estimation is crucial to clinical decision-making, such\nas identifying high-risk people for screening. However, most existing cancer\nrisk models incorporate data from epidemiologic studies, which usually cannot\nrepresent the target population. While population-based health surveys are\nideal for making inference to the target population, they typically do not\ncollect time-to-cancer incidence data. Instead, time-to-cancer specific\nmortality is often readily available on surveys via linkage to vital\nstatistics. We develop calibrated pseudoweighting methods that integrate\nindividual-level data from a cohort and a survey, and summary statistics of\ncancer incidence from national cancer registries. By leveraging\nindividual-level cancer mortality data in the survey, the proposed methods\nimpute time-to-cancer incidence for survey sample individuals and use survey\ncalibration with auxiliary variables of influence functions generated from Cox\nregression to improve robustness and efficiency of the inverse-propensity\npseudoweighting method in estimating pure risks. We develop a lung cancer\nincidence pure risk model from the Prostate, Lung, Colorectal, and Ovarian\n(PLCO) Cancer Screening Trial using our proposed methods by integrating data\nfrom the National Health Interview Survey (NHIS) and cancer registries."}, "http://arxiv.org/abs/2310.16653": {"title": "Adaptive importance sampling for heavy-tailed distributions via $\\alpha$-divergence minimization", "link": "http://arxiv.org/abs/2310.16653", "description": "Adaptive importance sampling (AIS) algorithms are widely used to approximate\nexpectations with respect to complicated target probability distributions. When\nthe target has heavy tails, existing AIS algorithms can provide inconsistent\nestimators or exhibit slow convergence, as they often neglect the target's tail\nbehaviour. To avoid this pitfall, we propose an AIS algorithm that approximates\nthe target by Student-t proposal distributions. We adapt location and scale\nparameters by matching the escort moments - which are defined even for\nheavy-tailed distributions - of the target and the proposal. These updates\nminimize the $\\alpha$-divergence between the target and the proposal, thereby\nconnecting with variational inference. We then show that the\n$\\alpha$-divergence can be approximated by a generalized notion of effective\nsample size and leverage this new perspective to adapt the tail parameter with\nBayesian optimization. We demonstrate the efficacy of our approach through\napplications to synthetic targets and a Bayesian Student-t regression task on a\nreal example with clinical trial data."}, "http://arxiv.org/abs/2310.16690": {"title": "Dynamic treatment effect phenotyping through functional survival analysis", "link": "http://arxiv.org/abs/2310.16690", "description": "In recent years, research interest in personalised treatments has been\ngrowing. However, treatment effect heterogeneity and possibly time-varying\ntreatment effects are still often overlooked in clinical studies. Statistical\ntools are needed for the identification of treatment response patterns, taking\ninto account that treatment response is not constant over time. We aim to\nprovide an innovative method to obtain dynamic treatment effect phenotypes on a\ntime-to-event outcome, conditioned on a set of relevant effect modifiers. The\nproposed method does not require the assumption of proportional hazards for the\ntreatment effect, which is rarely realistic. We propose a spline-based survival\nneural network, inspired by the Royston-Parmar survival model, to estimate\ntime-varying conditional treatment effects. We then exploit the functional\nnature of the resulting estimates to apply a functional clustering of the\ntreatment effect curves in order to identify different patterns of treatment\neffects. The application that motivated this work is the discontinuation of\ntreatment with Mineralocorticoid receptor Antagonists (MRAs) in patients with\nheart failure, where there is no clear evidence as to which patients it is the\nsafest choice to discontinue treatment and, conversely, when it leads to a\nhigher risk of adverse events. The data come from an electronic health record\ndatabase. A simulation study was performed to assess the performance of the\nspline-based neural network and the stability of the treatment response\nphenotyping procedure. We provide a novel method to inform individualized\nmedical decisions by characterising subject-specific treatment responses over\ntime."}, "http://arxiv.org/abs/2310.16698": {"title": "Causal Discovery with Generalized Linear Models through Peeling Algorithms", "link": "http://arxiv.org/abs/2310.16698", "description": "This article presents a novel method for causal discovery with generalized\nstructural equation models suited for analyzing diverse types of outcomes,\nincluding discrete, continuous, and mixed data. Causal discovery often faces\nchallenges due to unmeasured confounders that hinder the identification of\ncausal relationships. The proposed approach addresses this issue by developing\ntwo peeling algorithms (bottom-up and top-down) to ascertain causal\nrelationships and valid instruments. This approach first reconstructs a\nsuper-graph to represent ancestral relationships between variables, using a\npeeling algorithm based on nodewise GLM regressions that exploit relationships\nbetween primary and instrumental variables. Then, it estimates parent-child\neffects from the ancestral relationships using another peeling algorithm while\ndeconfounding a child's model with information borrowed from its parents'\nmodels. The article offers a theoretical analysis of the proposed approach,\nwhich establishes conditions for model identifiability and provides statistical\nguarantees for accurately discovering parent-child relationships via the\npeeling algorithms. Furthermore, the article presents numerical experiments\nshowcasing the effectiveness of our approach in comparison to state-of-the-art\nstructure learning methods without confounders. Lastly, it demonstrates an\napplication to Alzheimer's disease (AD), highlighting the utility of the method\nin constructing gene-to-gene and gene-to-disease regulatory networks involving\nSingle Nucleotide Polymorphisms (SNPs) for healthy and AD subjects."}, "http://arxiv.org/abs/2310.16813": {"title": "Improving the Aggregation and Evaluation of NBA Mock Drafts", "link": "http://arxiv.org/abs/2310.16813", "description": "Many enthusiasts and experts publish forecasts of the order players are\ndrafted into professional sports leagues, known as mock drafts. Using a novel\ndataset of mock drafts for the National Basketball Association (NBA), we\nanalyze authors' mock draft accuracy over time and ask how we can reasonably\nuse information from multiple authors. To measure how accurate mock drafts are,\nwe assume that both mock drafts and the actual draft are ranked lists, and we\npropose that rank-biased distance (RBD) of Webber et al. (2010) is the\nappropriate error metric for mock draft accuracy. This is because RBD allows\nmock drafts to have a different length than the actual draft, accounts for\nplayers not appearing in both lists, and weights errors early in the draft more\nthan errors later on. We validate that mock drafts, as expected, improve in\naccuracy over the course of a season, and that accuracy of the mock drafts\nproduced right before their drafts is fairly stable across seasons. To be able\nto combine information from multiple mock drafts into a single consensus mock\ndraft, we also propose a ranked-list combination method based on the ideas of\nranked-choice voting. We show that our method provides improved forecasts over\nthe standard Borda count combination method used for most similar analyses in\nsports, and that either combination method provides a more accurate forecast\nover time than any single author."}, "http://arxiv.org/abs/2310.16819": {"title": "CATE Lasso: Conditional Average Treatment Effect Estimation with High-Dimensional Linear Regression", "link": "http://arxiv.org/abs/2310.16819", "description": "In causal inference about two treatments, Conditional Average Treatment\nEffects (CATEs) play an important role as a quantity representing an\nindividualized causal effect, defined as a difference between the expected\noutcomes of the two treatments conditioned on covariates. This study assumes\ntwo linear regression models between a potential outcome and covariates of the\ntwo treatments and defines CATEs as a difference between the linear regression\nmodels. Then, we propose a method for consistently estimating CATEs even under\nhigh-dimensional and non-sparse parameters. In our study, we demonstrate that\ndesirable theoretical properties, such as consistency, remain attainable even\nwithout assuming sparsity explicitly if we assume a weaker assumption called\nimplicit sparsity originating from the definition of CATEs. In this assumption,\nwe suppose that parameters of linear models in potential outcomes can be\ndivided into treatment-specific and common parameters, where the\ntreatment-specific parameters take difference values between each linear\nregression model, while the common parameters remain identical. Thus, in a\ndifference between two linear regression models, the common parameters\ndisappear, leaving only differences in the treatment-specific parameters.\nConsequently, the non-zero parameters in CATEs correspond to the differences in\nthe treatment-specific parameters. Leveraging this assumption, we develop a\nLasso regression method specialized for CATE estimation and present that the\nestimator is consistent. Finally, we confirm the soundness of the proposed\nmethod by simulation studies."}, "http://arxiv.org/abs/2310.16824": {"title": "Parametric model for post-processing visibility ensemble forecasts", "link": "http://arxiv.org/abs/2310.16824", "description": "Despite the continuous development of the different operational ensemble\nprediction systems over the past decades, ensemble forecasts still might suffer\nfrom lack of calibration and/or display systematic bias, thus require some\npost-processing to improve their forecast skill. Here we focus on visibility,\nwhich quantity plays a crucial role e.g. in aviation and road safety or in ship\nnavigation, and propose a parametric model where the predictive distribution is\na mixture of a gamma and a truncated normal distribution, both right censored\nat the maximal reported visibility value. The new model is evaluated in two\ncase studies based on visibility ensemble forecasts of the European Centre for\nMedium-Range Weather Forecasts covering two distinct domains in Central and\nWestern Europe and two different time periods. The results of the case studies\nindicate that climatology is substantially superior to the raw ensemble;\nnevertheless, the forecast skill can be further improved by post-processing, at\nleast for short lead times. Moreover, the proposed mixture model consistently\noutperforms the Bayesian model averaging approach used as reference\npost-processing technique."}, "http://arxiv.org/abs/2109.09339": {"title": "Improving the accuracy of estimating indexes in contingency tables using Bayesian estimators", "link": "http://arxiv.org/abs/2109.09339", "description": "In contingency table analysis, one is interested in testing whether a model\nof interest (e.g., the independent or symmetry model) holds using\ngoodness-of-fit tests. When the null hypothesis where the model is true is\nrejected, the interest turns to the degree to which the probability structure\nof the contingency table deviates from the model. Many indexes have been\nstudied to measure the degree of the departure, such as the Yule coefficient\nand Cram\\'er coefficient for the independence model, and Tomizawa's symmetry\nindex for the symmetry model. The inference of these indexes is performed using\nsample proportions, which are estimates of cell probabilities, but it is\nwell-known that the bias and mean square error (MSE) values become large\nwithout a sufficient number of samples. To address the problem, this study\nproposes a new estimator for indexes using Bayesian estimators of cell\nprobabilities. Assuming the Dirichlet distribution for the prior of cell\nprobabilities, we asymptotically evaluate the value of MSE when plugging the\nposterior means of cell probabilities into the index, and propose an estimator\nof the index using the Dirichlet hyperparameter that minimizes the value.\nNumerical experiments show that when the number of samples per cell is small,\nthe proposed method has smaller values of bias and MSE than other methods of\ncorrecting estimation accuracy. We also show that the values of bias and MSE\nare smaller than those obtained by using the uniform and Jeffreys priors."}, "http://arxiv.org/abs/2110.01031": {"title": "A general framework for formulating structured variable selection", "link": "http://arxiv.org/abs/2110.01031", "description": "In variable selection, a selection rule that prescribes the permissible sets\nof selected variables (called a \"selection dictionary\") is desirable due to the\ninherent structural constraints among the candidate variables. Such selection\nrules can be complex in real-world data analyses, and failing to incorporate\nsuch restrictions could not only compromise the interpretability of the model\nbut also lead to decreased prediction accuracy. However, no general framework\nhas been proposed to formalize selection rules and their applications, which\nposes a significant challenge for practitioners seeking to integrate these\nrules into their analyses. In this work, we establish a framework for\nstructured variable selection that can incorporate universal structural\nconstraints. We develop a mathematical language for constructing arbitrary\nselection rules, where the selection dictionary is formally defined. We\ndemonstrate that all selection rules can be expressed as combinations of\noperations on constructs, facilitating the identification of the corresponding\nselection dictionary. Once this selection dictionary is derived, practitioners\ncan apply their own user-defined criteria to select the optimal model.\nAdditionally, our framework enhances existing penalized regression methods for\nvariable selection by providing guidance on how to appropriately group\nvariables to achieve the desired selection rule. Furthermore, our innovative\nframework opens the door to establishing new l0 norm-based penalized regression\ntechniques that can be tailored to respect arbitrary selection rules, thereby\nexpanding the possibilities for more robust and tailored model development."}, "http://arxiv.org/abs/2203.14223": {"title": "Identifying Peer Influence in Therapeutic Communities", "link": "http://arxiv.org/abs/2203.14223", "description": "We investigate if there is a peer influence or role model effect on\nsuccessful graduation from Therapeutic Communities (TCs). We analyze anonymized\nindividual-level observational data from 3 TCs that kept records of written\nexchanges of affirmations and corrections among residents, and their precise\nentry and exit dates. The affirmations allow us to form peer networks, and the\nentry and exit dates allow us to define a causal effect of interest. We\nconceptualize the causal role model effect as measuring the difference in the\nexpected outcome of a resident (ego) who can observe one of their social\ncontacts (e.g., peers who gave affirmations), to be successful in graduating\nbefore the ego's exit vs not successfully graduating before the ego's exit.\nSince peer influence is usually confounded with unobserved homophily in\nobservational data, we model the network with a latent variable model to\nestimate homophily and include it in the outcome equation. We provide a\ntheoretical guarantee that the bias of our peer influence estimator decreases\nwith sample size. Our results indicate there is an effect of peers' graduation\non the graduation of residents. The magnitude of peer influence differs based\non gender, race, and the definition of the role model effect. A counterfactual\nexercise quantifies the potential benefits of intervention of assigning a buddy\nto \"at-risk\" individuals directly on the treated resident and indirectly on\ntheir peers through network propagation."}, "http://arxiv.org/abs/2207.03182": {"title": "Chilled Sampling for Uncertainty Quantification: A Motivation From A Meteorological Inverse Problem", "link": "http://arxiv.org/abs/2207.03182", "description": "Atmospheric motion vectors (AMVs) extracted from satellite imagery are the\nonly wind observations with good global coverage. They are important features\nfor feeding numerical weather prediction (NWP) models. Several Bayesian models\nhave been proposed to estimate AMVs. Although critical for correct assimilation\ninto NWP models, very few methods provide a thorough characterization of the\nestimation errors. The difficulty of estimating errors stems from the\nspecificity of the posterior distribution, which is both very high dimensional,\nand highly ill-conditioned due to a singular likelihood. Motivated by this\ndifficult inverse problem, this work studies the evaluation of the (expected)\nestimation errors using gradient-based Markov Chain Monte Carlo (MCMC)\nalgorithms. The main contribution is to propose a general strategy, called here\nchilling, which amounts to sampling a local approximation of the posterior\ndistribution in the neighborhood of a point estimate. From a theoretical point\nof view, we show that under regularity assumptions, the family of chilled\nposterior distributions converges in distribution as temperature decreases to\nan optimal Gaussian approximation at a point estimate given by the Maximum A\nPosteriori, also known as the Laplace approximation. Chilled sampling therefore\nprovides access to this approximation generally out of reach in such\nhigh-dimensional nonlinear contexts. From an empirical perspective, we evaluate\nthe proposed approach based on some quantitative Bayesian criteria. Our\nnumerical simulations are performed on synthetic and real meteorological data.\nThey reveal that not only the proposed chilling exhibits a significant gain in\nterms of accuracy of the point estimates and of their associated expected\nerrors, but also a substantial acceleration in the convergence speed of the\nMCMC algorithms."}, "http://arxiv.org/abs/2207.13612": {"title": "Robust Output Analysis with Monte-Carlo Methodology", "link": "http://arxiv.org/abs/2207.13612", "description": "In predictive modeling with simulation or machine learning, it is critical to\naccurately assess the quality of estimated values through output analysis. In\nrecent decades output analysis has become enriched with methods that quantify\nthe impact of input data uncertainty in the model outputs to increase\nrobustness. However, most developments are applicable assuming that the input\ndata adheres to a parametric family of distributions. We propose a unified\noutput analysis framework for simulation and machine learning outputs through\nthe lens of Monte Carlo sampling. This framework provides nonparametric\nquantification of the variance and bias induced in the outputs with\nhigher-order accuracy. Our new bias-corrected estimation from the model outputs\nleverages the extension of fast iterative bootstrap sampling and higher-order\ninfluence functions. For the scalability of the proposed estimation methods, we\ndevise budget-optimal rules and leverage control variates for variance\nreduction. Our theoretical and numerical results demonstrate a clear advantage\nin building more robust confidence intervals from the model outputs with higher\ncoverage probability."}, "http://arxiv.org/abs/2208.06685": {"title": "Adaptive novelty detection with false discovery rate guarantee", "link": "http://arxiv.org/abs/2208.06685", "description": "This paper studies the semi-supervised novelty detection problem where a set\nof \"typical\" measurements is available to the researcher. Motivated by recent\nadvances in multiple testing and conformal inference, we propose AdaDetect, a\nflexible method that is able to wrap around any probabilistic classification\nalgorithm and control the false discovery rate (FDR) on detected novelties in\nfinite samples without any distributional assumption other than\nexchangeability. In contrast to classical FDR-controlling procedures that are\noften committed to a pre-specified p-value function, AdaDetect learns the\ntransformation in a data-adaptive manner to focus the power on the directions\nthat distinguish between inliers and outliers. Inspired by the multiple testing\nliterature, we further propose variants of AdaDetect that are adaptive to the\nproportion of nulls while maintaining the finite-sample FDR control. The\nmethods are illustrated on synthetic datasets and real-world datasets,\nincluding an application in astrophysics."}, "http://arxiv.org/abs/2211.02582": {"title": "Inference for Network Count Time Series with the R Package PNAR", "link": "http://arxiv.org/abs/2211.02582", "description": "We introduce a new R package useful for inference about network count time\nseries. Such data are frequently encountered in statistics and they are usually\ntreated as multivariate time series. Their statistical analysis is based on\nlinear or log linear models. Nonlinear models, which have been applied\nsuccessfully in several research areas, have been neglected from such\napplications mainly because of their computational complexity. We provide R\nusers the flexibility to fit and study nonlinear network count time series\nmodels which include either a drift in the intercept or a regime switching\nmechanism. We develop several computational tools including estimation of\nvarious count Network Autoregressive models and fast computational algorithms\nfor testing linearity in standard cases and when non-identifiable parameters\nhamper the analysis. Finally, we introduce a copula Poisson algorithm for\nsimulating multivariate network count time series. We illustrate the\nmethodology by modeling weekly number of influenza cases in Germany."}, "http://arxiv.org/abs/2212.08642": {"title": "Estimating Higher-Order Mixed Memberships via the $\\ell_{2,\\infty}$ Tensor Perturbation Bound", "link": "http://arxiv.org/abs/2212.08642", "description": "Higher-order multiway data is ubiquitous in machine learning and statistics\nand often exhibits community-like structures, where each component (node) along\neach different mode has a community membership associated with it. In this\npaper we propose the tensor mixed-membership blockmodel, a generalization of\nthe tensor blockmodel positing that memberships need not be discrete, but\ninstead are convex combinations of latent communities. We establish the\nidentifiability of our model and propose a computationally efficient estimation\nprocedure based on the higher-order orthogonal iteration algorithm (HOOI) for\ntensor SVD composed with a simplex corner-finding algorithm. We then\ndemonstrate the consistency of our estimation procedure by providing a per-node\nerror bound, which showcases the effect of higher-order structures on\nestimation accuracy. To prove our consistency result, we develop the\n$\\ell_{2,\\infty}$ tensor perturbation bound for HOOI under independent,\npossibly heteroskedastic, subgaussian noise that may be of independent\ninterest. Our analysis uses a novel leave-one-out construction for the\niterates, and our bounds depend only on spectral properties of the underlying\nlow-rank tensor under nearly optimal signal-to-noise ratio conditions such that\ntensor SVD is computationally feasible. Whereas other leave-one-out analyses\ntypically focus on sequences constructed by analyzing the output of a given\nalgorithm with a small part of the noise removed, our leave-one-out analysis\nconstructions use both the previous iterates and the additional tensor\nstructure to eliminate a potential additional source of error. Finally, we\napply our methodology to real and simulated data, including applications to two\nflight datasets and a trade network dataset, demonstrating some effects not\nidentifiable from the model with discrete community memberships."}, "http://arxiv.org/abs/2304.10372": {"title": "Statistical inference for Gaussian Whittle-Mat\\'ern fields on metric graphs", "link": "http://arxiv.org/abs/2304.10372", "description": "Whittle-Mat\\'ern fields are a recently introduced class of Gaussian processes\non metric graphs, which are specified as solutions to a fractional-order\nstochastic differential equation. Unlike earlier covariance-based approaches\nfor specifying Gaussian fields on metric graphs, the Whittle-Mat\\'ern fields\nare well-defined for any compact metric graph and can provide Gaussian\nprocesses with differentiable sample paths. We derive the main statistical\nproperties of the model class, particularly the consistency and asymptotic\nnormality of maximum likelihood estimators of model parameters and the\nnecessary and sufficient conditions for asymptotic optimality properties of\nlinear prediction based on the model with misspecified parameters.\n\nThe covariance function of the Whittle-Mat\\'ern fields is generally\nunavailable in closed form, and they have therefore been challenging to use for\nstatistical inference. However, we show that for specific values of the\nfractional exponent, when the fields have Markov properties, likelihood-based\ninference and spatial prediction can be performed exactly and computationally\nefficiently. This facilitates using the Whittle-Mat\\'ern fields in statistical\napplications involving big datasets without the need for any approximations.\nThe methods are illustrated via an application to modeling of traffic data,\nwhere allowing for differentiable processes dramatically improves the results."}, "http://arxiv.org/abs/2305.09282": {"title": "Errors-in-variables Fr\\'echet Regression with Low-rank Covariate Approximation", "link": "http://arxiv.org/abs/2305.09282", "description": "Fr\\'echet regression has emerged as a promising approach for regression\nanalysis involving non-Euclidean response variables. However, its practical\napplicability has been hindered by its reliance on ideal scenarios with\nabundant and noiseless covariate data. In this paper, we present a novel\nestimation method that tackles these limitations by leveraging the low-rank\nstructure inherent in the covariate matrix. Our proposed framework combines the\nconcepts of global Fr\\'echet regression and principal component regression,\naiming to improve the efficiency and accuracy of the regression estimator. By\nincorporating the low-rank structure, our method enables more effective\nmodeling and estimation, particularly in high-dimensional and\nerrors-in-variables regression settings. We provide a theoretical analysis of\nthe proposed estimator's large-sample properties, including a comprehensive\nrate analysis of bias, variance, and additional variations due to measurement\nerrors. Furthermore, our numerical experiments provide empirical evidence that\nsupports the theoretical findings, demonstrating the superior performance of\nour approach. Overall, this work introduces a promising framework for\nregression analysis of non-Euclidean variables, effectively addressing the\nchallenges associated with limited and noisy covariate data, with potential\napplications in diverse fields."}, "http://arxiv.org/abs/2305.19417": {"title": "Model averaging approaches to data subset selection", "link": "http://arxiv.org/abs/2305.19417", "description": "Model averaging is a useful and robust method for dealing with model\nuncertainty in statistical analysis. Often, it is useful to consider data\nsubset selection at the same time, in which model selection criteria are used\nto compare models across different subsets of the data. Two different criteria\nhave been proposed in the literature for how the data subsets should be\nweighted. We compare the two criteria closely in a unified treatment based on\nthe Kullback-Leibler divergence, and conclude that one of them is subtly flawed\nand will tend to yield larger uncertainties due to loss of information.\nAnalytical and numerical examples are provided."}, "http://arxiv.org/abs/2309.06053": {"title": "Confounder selection via iterative graph expansion", "link": "http://arxiv.org/abs/2309.06053", "description": "Confounder selection, namely choosing a set of covariates to control for\nconfounding between a treatment and an outcome, is arguably the most important\nstep in the design of observational studies. Previous methods, such as Pearl's\ncelebrated back-door criterion, typically require pre-specifying a causal\ngraph, which can often be difficult in practice. We propose an interactive\nprocedure for confounder selection that does not require pre-specifying the\ngraph or the set of observed variables. This procedure iteratively expands the\ncausal graph by finding what we call \"primary adjustment sets\" for a pair of\npossibly confounded variables. This can be viewed as inverting a sequence of\nlatent projections of the underlying causal graph. Structural information in\nthe form of primary adjustment sets is elicited from the user, bit by bit,\nuntil either a set of covariates are found to control for confounding or it can\nbe determined that no such set exists. Other information, such as the causal\nrelations between confounders, is not required by the procedure. We show that\nif the user correctly specifies the primary adjustment sets in every step, our\nprocedure is both sound and complete."}, "http://arxiv.org/abs/2310.16989": {"title": "Randomization Inference When N Equals One", "link": "http://arxiv.org/abs/2310.16989", "description": "N-of-1 experiments, where a unit serves as its own control and treatment in\ndifferent time windows, have been used in certain medical contexts for decades.\nHowever, due to effects that accumulate over long time windows and\ninterventions that have complex evolution, a lack of robust inference tools has\nlimited the widespread applicability of such N-of-1 designs. This work combines\ntechniques from experiment design in causal inference and system identification\nfrom control theory to provide such an inference framework. We derive a model\nof the dynamic interference effect that arises in linear time-invariant\ndynamical systems. We show that a family of causal estimands analogous to those\nstudied in potential outcomes are estimable via a standard estimator derived\nfrom the method of moments. We derive formulae for higher moments of this\nestimator and describe conditions under which N-of-1 designs may provide faster\nways to estimate the effects of interventions in dynamical systems. We also\nprovide conditions under which our estimator is asymptotically normal and\nderive valid confidence intervals for this setting."}, "http://arxiv.org/abs/2310.17009": {"title": "Simulation based stacking", "link": "http://arxiv.org/abs/2310.17009", "description": "Simulation-based inference has been popular for amortized Bayesian\ncomputation. It is typical to have more than one posterior approximation, from\ndifferent inference algorithms, different architectures, or simply the\nrandomness of initialization and stochastic gradients. With a provable\nasymptotic guarantee, we present a general stacking framework to make use of\nall available posterior approximations. Our stacking method is able to combine\ndensities, simulation draws, confidence intervals, and moments, and address the\noverall precision, calibration, coverage, and bias at the same time. We\nillustrate our method on several benchmark simulations and a challenging\ncosmological inference task."}, "http://arxiv.org/abs/2310.17153": {"title": "Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration", "link": "http://arxiv.org/abs/2310.17153", "description": "Semi-implicit variational inference (SIVI) has been introduced to expand the\nanalytical variational families by defining expressive semi-implicit\ndistributions in a hierarchical manner. However, the single-layer architecture\ncommonly used in current SIVI methods can be insufficient when the target\nposterior has complicated structures. In this paper, we propose hierarchical\nsemi-implicit variational inference, called HSIVI, which generalizes SIVI to\nallow more expressive multi-layer construction of semi-implicit distributions.\nBy introducing auxiliary distributions that interpolate between a simple base\ndistribution and the target distribution, the conditional layers can be trained\nby progressively matching these auxiliary distributions one layer after\nanother. Moreover, given pre-trained score networks, HSIVI can be used to\naccelerate the sampling process of diffusion models with the score matching\nobjective. We show that HSIVI significantly enhances the expressiveness of SIVI\non several Bayesian inference problems with complicated target distributions.\nWhen used for diffusion model acceleration, we show that HSIVI can produce high\nquality samples comparable to or better than the existing fast diffusion model\nbased samplers with a small number of function evaluations on various datasets."}, "http://arxiv.org/abs/2310.17165": {"title": "Price Experimentation and Interference in Online Platforms", "link": "http://arxiv.org/abs/2310.17165", "description": "In this paper, we examine the biases arising in A/B tests where a firm\nmodifies a continuous parameter, such as price, to estimate the global\ntreatment effect associated to a given performance metric. Such biases emerge\nfrom canonical designs and estimators due to interference among market\nparticipants. We employ structural modeling and differential calculus to derive\nintuitive structural characterizations of this bias. We then specialize our\ngeneral model to a standard revenue management pricing problem. This setting\nhighlights a key potential pitfall in the use of pricing experiments to guide\nprofit maximization: notably, the canonical estimator for the change in profits\ncan have the {\\em wrong sign}. In other words, following the guidance of the\ncanonical estimator may lead the firm to move prices in the wrong direction,\nand thereby decrease profits relative to the status quo. We apply these results\nto a two-sided market model and show how this ``change of sign\" regime depends\non model parameters, and discuss structural and practical implications for\nplatform operators."}, "http://arxiv.org/abs/2310.17248": {"title": "The observed Fisher information attached to the EM algorithm, illustrated on Shepp and Vardi estimation procedure for positron emission tomography", "link": "http://arxiv.org/abs/2310.17248", "description": "The Shepp &amp; Vardi (1982) implementation of the EM algorithm for PET scan\ntumor estimation provides a point estimate of the tumor. The current study\npresents a closed-form formula of the observed Fisher information for Shepp &amp;\nVardi PET scan tumor estimation. Keywords: PET scan, EM algorithm, Fisher\ninformation matrix, standard errors."}, "http://arxiv.org/abs/2310.17308": {"title": "Wild Bootstrap for Counting Process-Based Statistics", "link": "http://arxiv.org/abs/2310.17308", "description": "The wild bootstrap is a popular resampling method in the context of\ntime-to-event data analyses. Previous works established the large sample\nproperties of it for applications to different estimators and test statistics.\nIt can be used to justify the accuracy of inference procedures such as\nhypothesis tests or time-simultaneous confidence bands. This paper consists of\ntwo parts: in Part~I, a general framework is developed in which the large\nsample properties are established in a unified way by using martingale\nstructures. The framework includes most of the well-known non- and\nsemiparametric statistical methods in time-to-event analysis and parametric\napproaches. In Part II, the Fine-Gray proportional sub-hazards model\nexemplifies the theory for inference on cumulative incidence functions given\nthe covariates. The model falls within the framework if the data are\ncensoring-complete. A simulation study demonstrates the reliability of the\nmethod and an application to a data set about hospital-acquired infections\nillustrates the statistical procedure."}, "http://arxiv.org/abs/2310.17334": {"title": "Bayesian Optimization for Personalized Dose-Finding Trials with Combination Therapies", "link": "http://arxiv.org/abs/2310.17334", "description": "Identification of optimal dose combinations in early phase dose-finding\ntrials is challenging, due to the trade-off between precisely estimating the\nmany parameters required to flexibly model the dose-response surface, and the\nsmall sample sizes in early phase trials. Existing methods often restrict the\nsearch to pre-defined dose combinations, which may fail to identify regions of\noptimality in the dose combination space. These difficulties are even more\npertinent in the context of personalized dose-finding, where patient\ncharacteristics are used to identify tailored optimal dose combinations. To\novercome these challenges, we propose the use of Bayesian optimization for\nfinding optimal dose combinations in standard (\"one size fits all\") and\npersonalized multi-agent dose-finding trials. Bayesian optimization is a method\nfor estimating the global optima of expensive-to-evaluate objective functions.\nThe objective function is approximated by a surrogate model, commonly a\nGaussian process, paired with a sequential design strategy to select the next\npoint via an acquisition function. This work is motivated by an\nindustry-sponsored problem, where focus is on optimizing a dual-agent therapy\nin a setting featuring minimal toxicity. To compare the performance of the\nstandard and personalized methods under this setting, simulation studies are\nperformed for a variety of scenarios. Our study concludes that taking a\npersonalized approach is highly beneficial in the presence of heterogeneity."}, "http://arxiv.org/abs/2310.17434": {"title": "The `Why' behind including `Y' in your imputation model", "link": "http://arxiv.org/abs/2310.17434", "description": "Missing data is a common challenge when analyzing epidemiological data, and\nimputation is often used to address this issue. Here, we investigate the\nscenario where a covariate used in an analysis has missingness and will be\nimputed. There are recommendations to include the outcome from the analysis\nmodel in the imputation model for missing covariates, but it is not necessarily\nclear if this recommmendation always holds and why this is sometimes true. We\nexamine deterministic imputation (i.e., single imputation where the imputed\nvalues are treated as fixed) and stochastic imputation (i.e., single imputation\nwith a random value or multiple imputation) methods and their implications for\nestimating the relationship between the imputed covariate and the outcome. We\nmathematically demonstrate that including the outcome variable in imputation\nmodels is not just a recommendation but a requirement to achieve unbiased\nresults when using stochastic imputation methods. Moreover, we dispel common\nmisconceptions about deterministic imputation models and demonstrate why the\noutcome should not be included in these models. This paper aims to bridge the\ngap between imputation in theory and in practice, providing mathematical\nderivations to explain common statistical recommendations. We offer a better\nunderstanding of the considerations involved in imputing missing covariates and\nemphasize when it is necessary to include the outcome variable in the\nimputation model."}, "http://arxiv.org/abs/2310.17440": {"title": "Gibbs optimal design of experiments", "link": "http://arxiv.org/abs/2310.17440", "description": "Bayesian optimal design of experiments is a well-established approach to\nplanning experiments. Briefly, a probability distribution, known as a\nstatistical model, for the responses is assumed which is dependent on a vector\nof unknown parameters. A utility function is then specified which gives the\ngain in information for estimating the true value of the parameters using the\nBayesian posterior distribution. A Bayesian optimal design is given by\nmaximising the expectation of the utility with respect to the joint\ndistribution given by the statistical model and prior distribution for the true\nparameter values. The approach takes account of the experimental aim via\nspecification of the utility and of all assumed sources of uncertainty via the\nexpected utility. However, it is predicated on the specification of the\nstatistical model. Recently, a new type of statistical inference, known as\nGibbs (or General Bayesian) inference, has been advanced. This is\nBayesian-like, in that uncertainty on unknown quantities is represented by a\nposterior distribution, but does not necessarily rely on specification of a\nstatistical model. Thus the resulting inference should be less sensitive to\nmisspecification of the statistical model. The purpose of this paper is to\npropose Gibbs optimal design: a framework for optimal design of experiments for\nGibbs inference. The concept behind the framework is introduced along with a\ncomputational approach to find Gibbs optimal designs in practice. The framework\nis demonstrated on exemplars including linear models, and experiments with\ncount and time-to-event responses."}, "http://arxiv.org/abs/2310.17496": {"title": "Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach", "link": "http://arxiv.org/abs/2310.17496", "description": "In modern recommendation systems, the standard pipeline involves training\nmachine learning models on historical data to predict user behaviors and\nimprove recommendations continuously. However, these data training loops can\nintroduce interference in A/B tests, where data generated by control and\ntreatment algorithms, potentially with different distributions, are combined.\nTo address these challenges, we introduce a novel approach called weighted\ntraining. This approach entails training a model to predict the probability of\neach data point appearing in either the treatment or control data and\nsubsequently applying weighted losses during model training. We demonstrate\nthat this approach achieves the least variance among all estimators without\ncausing shifts in the training distributions. Through simulation studies, we\ndemonstrate the lower bias and variance of our approach compared to other\nmethods."}, "http://arxiv.org/abs/2310.17546": {"title": "A changepoint approach to modelling non-stationary soil moisture dynamics", "link": "http://arxiv.org/abs/2310.17546", "description": "Soil moisture dynamics provide an indicator of soil health that scientists\nmodel via soil drydown curves. The typical modeling process requires the soil\nmoisture time series to be manually separated into drydown segments and then\nexponential decay models are fitted to them independently. Sensor development\nover recent years means that experiments that were previously conducted over a\nfew field campaigns can now be scaled to months or even years, often at a\nhigher sampling rate. Manual identification of drydown segments is no longer\npractical. To better meet the challenge of increasing data size, this paper\nproposes a novel changepoint-based approach to automatically identify\nstructural changes in the soil drying process, and estimate the parameters\ncharacterizing the drying processes simultaneously. A simulation study is\ncarried out to assess the performance of the method. The results demonstrate\nits ability to identify structural changes and retrieve key parameters of\ninterest to soil scientists. The method is applied to hourly soil moisture time\nseries from the NEON data portal to investigate the temporal dynamics of soil\nmoisture drydown. We recover known relationships previously identified\nmanually, alongside delivering new insights into the temporal variability\nacross soil types and locations."}, "http://arxiv.org/abs/2310.17629": {"title": "Approximate Leave-one-out Cross Validation for Regression with $\\ell_1$ Regularizers (extended version)", "link": "http://arxiv.org/abs/2310.17629", "description": "The out-of-sample error (OO) is the main quantity of interest in risk\nestimation and model selection. Leave-one-out cross validation (LO) offers a\n(nearly) distribution-free yet computationally demanding approach to estimate\nOO. Recent theoretical work showed that approximate leave-one-out cross\nvalidation (ALO) is a computationally efficient and statistically reliable\nestimate of LO (and OO) for generalized linear models with differentiable\nregularizers. For problems involving non-differentiable regularizers, despite\nsignificant empirical evidence, the theoretical understanding of ALO's error\nremains unknown. In this paper, we present a novel theory for a wide class of\nproblems in the generalized linear model family with non-differentiable\nregularizers. We bound the error |ALO - LO| in terms of intuitive metrics such\nas the size of leave-i-out perturbations in active sets, sample size n, number\nof features p and regularization parameters. As a consequence, for the\n$\\ell_1$-regularized problems, we show that |ALO - LO| goes to zero as p goes\nto infinity while n/p and SNR are fixed and bounded."}, "http://arxiv.org/abs/2108.04201": {"title": "Guaranteed Functional Tensor Singular Value Decomposition", "link": "http://arxiv.org/abs/2108.04201", "description": "This paper introduces the functional tensor singular value decomposition\n(FTSVD), a novel dimension reduction framework for tensors with one functional\nmode and several tabular modes. The problem is motivated by high-order\nlongitudinal data analysis. Our model assumes the observed data to be a random\nrealization of an approximate CP low-rank functional tensor measured on a\ndiscrete time grid. Incorporating tensor algebra and the theory of Reproducing\nKernel Hilbert Space (RKHS), we propose a novel RKHS-based constrained power\niteration with spectral initialization. Our method can successfully estimate\nboth singular vectors and functions of the low-rank structure in the observed\ndata. With mild assumptions, we establish the non-asymptotic contractive error\nbounds for the proposed algorithm. The superiority of the proposed framework is\ndemonstrated via extensive experiments on both simulated and real data."}, "http://arxiv.org/abs/2202.02146": {"title": "Elastic Gradient Descent, an Iterative Optimization Method Approximating the Solution Paths of the Elastic Net", "link": "http://arxiv.org/abs/2202.02146", "description": "The elastic net combines lasso and ridge regression to fuse the sparsity\nproperty of lasso with the grouping property of ridge regression. The\nconnections between ridge regression and gradient descent and between lasso and\nforward stagewise regression have previously been shown. Similar to how the\nelastic net generalizes lasso and ridge regression, we introduce elastic\ngradient descent, a generalization of gradient descent and forward stagewise\nregression. We theoretically analyze elastic gradient descent and compare it to\nthe elastic net and forward stagewise regression. Parts of the analysis are\nbased on elastic gradient flow, a piecewise analytical construction, obtained\nfor elastic gradient descent with infinitesimal step size. We also compare\nelastic gradient descent to the elastic net on real and simulated data and show\nthat it provides similar solution paths, but is several orders of magnitude\nfaster. Compared to forward stagewise regression, elastic gradient descent\nselects a model that, although still sparse, provides considerably lower\nprediction and estimation errors."}, "http://arxiv.org/abs/2202.03897": {"title": "Inference from Sampling with Response Probabilities Estimated via Calibration", "link": "http://arxiv.org/abs/2202.03897", "description": "A solution to control for nonresponse bias consists of multiplying the design\nweights of respondents by the inverse of estimated response probabilities to\ncompensate for the nonrespondents. Maximum likelihood and calibration are two\napproaches that can be applied to obtain estimated response probabilities. We\nconsider a common framework in which these approaches can be compared. We\ndevelop an asymptotic study of the behavior of the resulting estimator when\ncalibration is applied. A logistic regression model for the response\nprobabilities is postulated. Missing at random and unclustered data are\nsupposed. Three main contributions of this work are: 1) we show that the\nestimators with the response probabilities estimated via calibration are\nasymptotically equivalent to unbiased estimators and that a gain in efficiency\nis obtained when estimating the response probabilities via calibration as\ncompared to the estimator with the true response probabilities, 2) we show that\nthe estimators with the response probabilities estimated via calibration are\ndoubly robust to model misspecification and explain why double robustness is\nnot guaranteed when maximum likelihood is applied, and 3) we discuss and\nillustrate problems related to response probabilities estimation, namely\nexistence of a solution to the estimating equations, problems of convergence,\nand extreme weights. We explain and illustrate why the first aforementioned\nproblem is more likely with calibration than with maximum likelihood\nestimation. We present the results of a simulation study in order to illustrate\nthese elements."}, "http://arxiv.org/abs/2208.14951": {"title": "Statistical inference for multivariate extremes via a geometric approach", "link": "http://arxiv.org/abs/2208.14951", "description": "A geometric representation for multivariate extremes, based on the shapes of\nscaled sample clouds in light-tailed margins and their so-called limit sets,\nhas recently been shown to connect several existing extremal dependence\nconcepts. However, these results are purely probabilistic, and the geometric\napproach itself has not been fully exploited for statistical inference. We\noutline a method for parametric estimation of the limit set shape, which\nincludes a useful non/semi-parametric estimate as a pre-processing step. More\nfundamentally, our approach provides a new class of asymptotically-motivated\nstatistical models for the tails of multivariate distributions, and such models\ncan accommodate any combination of simultaneous or non-simultaneous extremes\nthrough appropriate parametric forms for the limit set shape. Extrapolation\nfurther into the tail of the distribution is possible via simulation from the\nfitted model. A simulation study confirms that our methodology is very\ncompetitive with existing approaches, and can successfully allow estimation of\nsmall probabilities in regions where other methods struggle. We apply the\nmethodology to two environmental datasets, with diagnostics demonstrating a\ngood fit."}, "http://arxiv.org/abs/2209.08889": {"title": "Inference of nonlinear causal effects with GWAS summary data", "link": "http://arxiv.org/abs/2209.08889", "description": "Large-scale genome-wide association studies (GWAS) have offered an exciting\nopportunity to discover putative causal genes or risk factors associated with\ndiseases by using SNPs as instrumental variables (IVs). However, conventional\napproaches assume linear causal relations partly for simplicity and partly for\nthe availability of GWAS summary data. In this work, we propose a novel model\n{for transcriptome-wide association studies (TWAS)} to incorporate nonlinear\nrelationships across IVs, an exposure/gene, and an outcome, which is robust\nagainst violations of the valid IV assumptions, permits the use of GWAS summary\ndata, and covers two-stage least squares as a special case. We decouple the\nestimation of a marginal causal effect and a nonlinear transformation, where\nthe former is estimated via sliced inverse regression and a sparse instrumental\nvariable regression, and the latter is estimated by a ratio-adjusted inverse\nregression. On this ground, we propose an inferential procedure. An application\nof the proposed method to the ADNI gene expression data and the IGAP GWAS\nsummary data identifies 18 causal genes associated with Alzheimer's disease,\nincluding APOE and TOMM40, in addition to 7 other genes missed by two-stage\nleast squares considering only linear relationships. Our findings suggest that\nnonlinear modeling is required to unleash the power of IV regression for\nidentifying potentially nonlinear gene-trait associations. Accompanying this\npaper is our Python library \\texttt{nl-causal}\n(\\url{https://nonlinear-causal.readthedocs.io/}) that implements the proposed\nmethod."}, "http://arxiv.org/abs/2301.03038": {"title": "Skewed Bernstein-von Mises theorem and skew-modal approximations", "link": "http://arxiv.org/abs/2301.03038", "description": "Gaussian approximations are routinely employed in Bayesian statistics to ease\ninference when the target posterior is intractable. Although these\napproximations are asymptotically justified by Bernstein-von Mises type\nresults, in practice the expected Gaussian behavior may poorly represent the\nshape of the posterior, thus affecting approximation accuracy. Motivated by\nthese considerations, we derive an improved class of closed-form approximations\nof posterior distributions which arise from a new treatment of a third-order\nversion of the Laplace method yielding approximations in a tractable family of\nskew-symmetric distributions. Under general assumptions which account for\nmisspecified models and non-i.i.d. settings, this family of approximations is\nshown to have a total variation distance from the target posterior whose rate\nof convergence improves by at least one order of magnitude the one established\nby the classical Bernstein-von Mises theorem. Specializing this result to the\ncase of regular parametric models shows that the same improvement in\napproximation accuracy can be also derived for polynomially bounded posterior\nfunctionals. Unlike other higher-order approximations, our results prove that\nit is possible to derive closed-form and valid densities which are expected to\nprovide, in practice, a more accurate, yet similarly-tractable, alternative to\nGaussian approximations of the target posterior, while inheriting its limiting\nfrequentist properties. We strengthen such arguments by developing a practical\nskew-modal approximation for both joint and marginal posteriors that achieves\nthe same theoretical guarantees of its theoretical counterpart by replacing the\nunknown model parameters with the corresponding MAP estimate. Empirical studies\nconfirm that our theoretical results closely match the remarkable performance\nobserved in practice, even in finite, possibly small, sample regimes."}, "http://arxiv.org/abs/2303.05878": {"title": "Identification and Estimation of Causal Effects with Confounders Missing Not at Random", "link": "http://arxiv.org/abs/2303.05878", "description": "Making causal inferences from observational studies can be challenging when\nconfounders are missing not at random. In such cases, identifying causal\neffects is often not guaranteed. Motivated by a real example, we consider a\ntreatment-independent missingness assumption under which we establish the\nidentification of causal effects when confounders are missing not at random. We\npropose a weighted estimating equation (WEE) approach for estimating model\nparameters and introduce three estimators for the average causal effect, based\non regression, propensity score weighting, and doubly robust estimation. We\nevaluate the performance of these estimators through simulations, and provide a\nreal data analysis to illustrate our proposed method."}, "http://arxiv.org/abs/2305.12283": {"title": "Distribution-Free Model-Agnostic Regression Calibration via Nonparametric Methods", "link": "http://arxiv.org/abs/2305.12283", "description": "In this paper, we consider the uncertainty quantification problem for\nregression models. Specifically, we consider an individual calibration\nobjective for characterizing the quantiles of the prediction model. While such\nan objective is well-motivated from downstream tasks such as newsvendor cost,\nthe existing methods have been largely heuristic and lack of statistical\nguarantee in terms of individual calibration. We show via simple examples that\nthe existing methods focusing on population-level calibration guarantees such\nas average calibration or sharpness can lead to harmful and unexpected results.\nWe propose simple nonparametric calibration methods that are agnostic of the\nunderlying prediction model and enjoy both computational efficiency and\nstatistical consistency. Our approach enables a better understanding of the\npossibility of individual calibration, and we establish matching upper and\nlower bounds for the calibration error of our proposed methods. Technically,\nour analysis combines the nonparametric analysis with a covering number\nargument for parametric analysis, which advances the existing theoretical\nanalyses in the literature of nonparametric density estimation and quantile\nbandit problems. Importantly, the nonparametric perspective sheds new\ntheoretical insights into regression calibration in terms of the curse of\ndimensionality and reconciles the existing results on the impossibility of\nindividual calibration. To our knowledge, we make the first effort to reach\nboth individual calibration and finite-sample guarantee with minimal\nassumptions in terms of conformal prediction. Numerical experiments show the\nadvantage of such a simple approach under various metrics, and also under\ncovariates shift. We hope our work provides a simple benchmark and a starting\npoint of theoretical ground for future research on regression calibration."}, "http://arxiv.org/abs/2305.14943": {"title": "Learning Rate Free Bayesian Inference in Constrained Domains", "link": "http://arxiv.org/abs/2305.14943", "description": "We introduce a suite of new particle-based algorithms for sampling on\nconstrained domains which are entirely learning rate free. Our approach\nleverages coin betting ideas from convex optimisation, and the viewpoint of\nconstrained sampling as a mirrored optimisation problem on the space of\nprobability measures. Based on this viewpoint, we also introduce a unifying\nframework for several existing constrained sampling algorithms, including\nmirrored Langevin dynamics and mirrored Stein variational gradient descent. We\ndemonstrate the performance of our algorithms on a range of numerical examples,\nincluding sampling from targets on the simplex, sampling with fairness\nconstraints, and constrained sampling problems in post-selection inference. Our\nresults indicate that our algorithms achieve competitive performance with\nexisting constrained sampling methods, without the need to tune any\nhyperparameters."}, "http://arxiv.org/abs/2308.07983": {"title": "Monte Carlo guided Diffusion for Bayesian linear inverse problems", "link": "http://arxiv.org/abs/2308.07983", "description": "Ill-posed linear inverse problems arise frequently in various applications,\nfrom computational photography to medical imaging. A recent line of research\nexploits Bayesian inference with informative priors to handle the ill-posedness\nof such problems. Amongst such priors, score-based generative models (SGM) have\nrecently been successfully applied to several different inverse problems. In\nthis study, we exploit the particular structure of the prior defined by the SGM\nto define a sequence of intermediate linear inverse problems. As the noise\nlevel decreases, the posteriors of these inverse problems get closer to the\ntarget posterior of the original inverse problem. To sample from this sequence\nof posteriors, we propose the use of Sequential Monte Carlo (SMC) methods. The\nproposed algorithm, MCGDiff, is shown to be theoretically grounded and we\nprovide numerical simulations showing that it outperforms competing baselines\nwhen dealing with ill-posed inverse problems in a Bayesian setting."}, "http://arxiv.org/abs/2308.12485": {"title": "Optimal Shrinkage Estimation of Fixed Effects in Linear Panel Data Models", "link": "http://arxiv.org/abs/2308.12485", "description": "Shrinkage methods are frequently used to estimate fixed effects to reduce the\nnoisiness of the least squares estimators. However, widely used shrinkage\nestimators guarantee such noise reduction only under strong distributional\nassumptions. I develop an estimator for the fixed effects that obtains the best\npossible mean squared error within a class of shrinkage estimators. This class\nincludes conventional shrinkage estimators and the optimality does not require\ndistributional assumptions. The estimator has an intuitive form and is easy to\nimplement. Moreover, the fixed effects are allowed to vary with time and to be\nserially correlated, and the shrinkage optimally incorporates the underlying\ncorrelation structure in this case. In such a context, I also provide a method\nto forecast fixed effects one period ahead."}, "http://arxiv.org/abs/2309.16843": {"title": "A Mean Field Approach to Empirical Bayes Estimation in High-dimensional Linear Regression", "link": "http://arxiv.org/abs/2309.16843", "description": "We study empirical Bayes estimation in high-dimensional linear regression. To\nfacilitate computationally efficient estimation of the underlying prior, we\nadopt a variational empirical Bayes approach, introduced originally in\nCarbonetto and Stephens (2012) and Kim et al. (2022). We establish asymptotic\nconsistency of the nonparametric maximum likelihood estimator (NPMLE) and its\n(computable) naive mean field variational surrogate under mild assumptions on\nthe design and the prior. Assuming, in addition, that the naive mean field\napproximation has a dominant optimizer, we develop a computationally efficient\napproximation to the oracle posterior distribution, and establish its accuracy\nunder the 1-Wasserstein metric. This enables computationally feasible Bayesian\ninference; e.g., construction of posterior credible intervals with an average\ncoverage guarantee, Bayes optimal estimation for the regression coefficients,\nestimation of the proportion of non-nulls, etc. Our analysis covers both\ndeterministic and random designs, and accommodates correlations among the\nfeatures. To the best of our knowledge, this provides the first rigorous\nnonparametric empirical Bayes method in a high-dimensional regression setting\nwithout sparsity."}, "http://arxiv.org/abs/2310.17679": {"title": "Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score Search and Grow-Shrink Trees", "link": "http://arxiv.org/abs/2310.17679", "description": "Learning graphical conditional independence structures is an important\nmachine learning problem and a cornerstone of causal discovery. However, the\naccuracy and execution time of learning algorithms generally struggle to scale\nto problems with hundreds of highly connected variables -- for instance,\nrecovering brain networks from fMRI data. We introduce the best order score\nsearch (BOSS) and grow-shrink trees (GSTs) for learning directed acyclic graphs\n(DAGs) in this paradigm. BOSS greedily searches over permutations of variables,\nusing GSTs to construct and score DAGs from permutations. GSTs efficiently\ncache scores to eliminate redundant calculations. BOSS achieves\nstate-of-the-art performance in accuracy and execution time, comparing\nfavorably to a variety of combinatorial and gradient-based learning algorithms\nunder a broad range of conditions. To demonstrate its practicality, we apply\nBOSS to two sets of resting-state fMRI data: simulated data with\npseudo-empirical noise distributions derived from randomized empirical fMRI\ncortical signals and clinical data from 3T fMRI scans processed into cortical\nparcels. BOSS is available for use within the TETRAD project which includes\nPython and R wrappers."}, "http://arxiv.org/abs/2310.17712": {"title": "Community Detection and Classification Guarantees Using Embeddings Learned by Node2Vec", "link": "http://arxiv.org/abs/2310.17712", "description": "Embedding the nodes of a large network into an Euclidean space is a common\nobjective in modern machine learning, with a variety of tools available. These\nembeddings can then be used as features for tasks such as community\ndetection/node clustering or link prediction, where they achieve state of the\nart performance. With the exception of spectral clustering methods, there is\nlittle theoretical understanding for other commonly used approaches to learning\nembeddings. In this work we examine the theoretical properties of the\nembeddings learned by node2vec. Our main result shows that the use of k-means\nclustering on the embedding vectors produced by node2vec gives weakly\nconsistent community recovery for the nodes in (degree corrected) stochastic\nblock models. We also discuss the use of these embeddings for node and link\nprediction tasks. We demonstrate this result empirically, and examine how this\nrelates to other embedding tools for network data."}, "http://arxiv.org/abs/2310.17760": {"title": "Novel Models for Multiple Dependent Heteroskedastic Time Series", "link": "http://arxiv.org/abs/2310.17760", "description": "Functional magnetic resonance imaging or functional MRI (fMRI) is a very\npopular tool used for differing brain regions by measuring brain activity. It\nis affected by physiological noise, such as head and brain movement in the\nscanner from breathing, heart beats, or the subject fidgeting. The purpose of\nthis paper is to propose a novel approach to handling fMRI data for infants\nwith high volatility caused by sudden head movements. Another purpose is to\nevaluate the volatility modelling performance of multiple dependent fMRI time\nseries data. The models examined in this paper are AR and GARCH and the\nmodelling performance is evaluated by several statistical performance measures.\nThe conclusions of this paper are that multiple dependent fMRI series data can\nbe fitted with AR + GARCH model if the multiple fMRI data have many sudden head\nmovements. The GARCH model can capture the shared volatility clustering caused\nby head movements across brain regions. However, the multiple fMRI data without\nmany head movements have fitted AR + GARCH model with different performance.\nThe conclusions are supported by statistical tests and measures. This paper\nhighlights the difference between the proposed approach from traditional\napproaches when estimating model parameters and modelling conditional variances\non multiple dependent time series. In the future, the proposed approach can be\napplied to other research fields, such as financial economics, and signal\nprocessing. Code is available at \\url{https://github.<a href=\"https://export.arxiv.org/abs/com/1320494\">com/1320494</a>2/STAT40710}."}, "http://arxiv.org/abs/2310.17766": {"title": "Minibatch Markov chain Monte Carlo Algorithms for Fitting Gaussian Processes", "link": "http://arxiv.org/abs/2310.17766", "description": "Gaussian processes (GPs) are a highly flexible, nonparametric statistical\nmodel that are commonly used to fit nonlinear relationships or account for\ncorrelation between observations. However, the computational load of fitting a\nGaussian process is $\\mathcal{O}(n^3)$ making them infeasible for use on large\ndatasets. To make GPs more feasible for large datasets, this research focuses\non the use of minibatching to estimate GP parameters. Specifically, we outline\nboth approximate and exact minibatch Markov chain Monte Carlo algorithms that\nsubstantially reduce the computation of fitting a GP by only considering small\nsubsets of the data at a time. We demonstrate and compare this methodology\nusing various simulations and real datasets."}, "http://arxiv.org/abs/2310.17806": {"title": "Transporting treatment effects from difference-in-differences studies", "link": "http://arxiv.org/abs/2310.17806", "description": "Difference-in-differences (DID) is a popular approach to identify the causal\neffects of treatments and policies in the presence of unmeasured confounding.\nDID identifies the sample average treatment effect in the treated (SATT).\nHowever, a goal of such research is often to inform decision-making in target\npopulations outside the treated sample. Transportability methods have been\ndeveloped to extend inferences from study samples to external target\npopulations; these methods have primarily been developed and applied in\nsettings where identification is based on conditional independence between the\ntreatment and potential outcomes, such as in a randomized trial. This paper\ndevelops identification and estimators for effects in a target population,\nbased on DID conducted in a study sample that differs from the target\npopulation. We present a range of assumptions under which one may identify\ncausal effects in the target population and employ causal diagrams to\nillustrate these assumptions. In most realistic settings, results depend\ncritically on the assumption that any unmeasured confounders are not effect\nmeasure modifiers on the scale of the effect of interest. We develop several\nestimators of transported effects, including a doubly robust estimator based on\nthe efficient influence function. Simulation results support theoretical\nproperties of the proposed estimators. We discuss the potential application of\nour approach to a study of the effects of a US federal smoke-free housing\npolicy, where the original study was conducted in New York City alone and the\ngoal is extend inferences to other US cities."}, "http://arxiv.org/abs/2310.17816": {"title": "Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs", "link": "http://arxiv.org/abs/2310.17816", "description": "This work addresses the problem of automated covariate selection under\nlimited prior knowledge. Given an exposure-outcome pair {X,Y} and a variable\nset Z of unknown causal structure, the Local Discovery by Partitioning (LDP)\nalgorithm partitions Z into subsets defined by their relation to {X,Y}. We\nenumerate eight exhaustive and mutually exclusive partitions of any arbitrary Z\nand leverage this taxonomy to differentiate confounders from other variable\ntypes. LDP is motivated by valid adjustment set identification, but avoids the\npretreatment assumption commonly made by automated covariate selection methods.\nWe provide theoretical guarantees that LDP returns a valid adjustment set for\nany Z that meets sufficient graphical conditions. Under stronger conditions, we\nprove that partition labels are asymptotically correct. Total independence\ntests is worst-case quadratic in |Z|, with sub-quadratic runtimes observed\nempirically. We numerically validate our theoretical guarantees on synthetic\nand semi-synthetic graphs. Adjustment sets from LDP yield less biased and more\nprecise average treatment effect estimates than baselines, with LDP\noutperforming on confounder recall, test count, and runtime for valid\nadjustment set discovery."}, "http://arxiv.org/abs/2310.17820": {"title": "Sparse Bayesian Multidimensional Item Response Theory", "link": "http://arxiv.org/abs/2310.17820", "description": "Multivariate Item Response Theory (MIRT) is sought-after widely by applied\nresearchers looking for interpretable (sparse) explanations underlying response\npatterns in questionnaire data. There is, however, an unmet demand for such\nsparsity discovery tools in practice. Our paper develops a Bayesian platform\nfor binary and ordinal item MIRT which requires minimal tuning and scales well\non relatively large datasets due to its parallelizable features. Bayesian\nmethodology for MIRT models has traditionally relied on MCMC simulation, which\ncannot only be slow in practice, but also often renders exact sparsity recovery\nimpossible without additional thresholding. In this work, we develop a scalable\nBayesian EM algorithm to estimate sparse factor loadings from binary and\nordinal item responses. We address the seemingly insurmountable problem of\nunknown latent factor dimensionality with tools from Bayesian nonparametrics\nwhich enable estimating the number of factors. Rotations to sparsity through\nparameter expansion further enhance convergence and interpretability without\nidentifiability constraints. In our simulation study, we show that our method\nreliably recovers both the factor dimensionality as well as the latent\nstructure on high-dimensional synthetic data even for small samples. We\ndemonstrate the practical usefulness of our approach on two datasets: an\neducational item response dataset and a quality-of-life measurement dataset.\nBoth demonstrations show that our tool yields interpretable estimates,\nfacilitating interesting discoveries that might otherwise go unnoticed under a\npure confirmatory factor analysis setting. We provide an easy-to-use software\nwhich is a useful new addition to the MIRT toolkit and which will hopefully\nserve as the go-to method for practitioners."}, "http://arxiv.org/abs/2310.17845": {"title": "A Unified and Optimal Multiple Testing Framework based on rho-values", "link": "http://arxiv.org/abs/2310.17845", "description": "Multiple testing is an important research direction that has gained major\nattention in recent years. Currently, most multiple testing procedures are\ndesigned with p-values or Local false discovery rate (Lfdr) statistics.\nHowever, p-values obtained by applying probability integral transform to some\nwell-known test statistics often do not incorporate information from the\nalternatives, resulting in suboptimal procedures. On the other hand, Lfdr based\nprocedures can be asymptotically optimal but their guarantee on false discovery\nrate (FDR) control relies on consistent estimation of Lfdr, which is often\ndifficult in practice especially when the incorporation of side information is\ndesirable. In this article, we propose a novel and flexibly constructed class\nof statistics, called rho-values, which combines the merits of both p-values\nand Lfdr while enjoys superiorities over methods based on these two types of\nstatistics. Specifically, it unifies these two frameworks and operates in two\nsteps, ranking and thresholding. The ranking produced by rho-values mimics that\nproduced by Lfdr statistics, and the strategy for choosing the threshold is\nsimilar to that of p-value based procedures. Therefore, the proposed framework\nguarantees FDR control under weak assumptions; it maintains the integrity of\nthe structural information encoded by the summary statistics and the auxiliary\ncovariates and hence can be asymptotically optimal. We demonstrate the efficacy\nof the new framework through extensive simulations and two data applications."}, "http://arxiv.org/abs/2310.17999": {"title": "Automated threshold selection and associated inference uncertainty for univariate extremes", "link": "http://arxiv.org/abs/2310.17999", "description": "Threshold selection is a fundamental problem in any threshold-based extreme\nvalue analysis. While models are asymptotically motivated, selecting an\nappropriate threshold for finite samples can be difficult through standard\nmethods. Inference can also be highly sensitive to the choice of threshold. Too\nlow a threshold choice leads to bias in the fit of the extreme value model,\nwhile too high a choice leads to unnecessary additional uncertainty in the\nestimation of model parameters. In this paper, we develop a novel methodology\nfor automated threshold selection that directly tackles this bias-variance\ntrade-off. We also develop a method to account for the uncertainty in this\nthreshold choice and propagate this uncertainty through to high quantile\ninference. Through a simulation study, we demonstrate the effectiveness of our\nmethod for threshold selection and subsequent extreme quantile estimation. We\napply our method to the well-known, troublesome example of the River Nidd\ndataset."}, "http://arxiv.org/abs/2310.18027": {"title": "Bayesian Prognostic Covariate Adjustment With Additive Mixture Priors", "link": "http://arxiv.org/abs/2310.18027", "description": "Effective and rapid decision-making from randomized controlled trials (RCTs)\nrequires unbiased and precise treatment effect inferences. Two strategies to\naddress this requirement are to adjust for covariates that are highly\ncorrelated with the outcome, and to leverage historical control information via\nBayes' theorem. We propose a new Bayesian prognostic covariate adjustment\nmethodology, referred to as Bayesian PROCOVA, that combines these two\nstrategies. Covariate adjustment is based on generative artificial intelligence\n(AI) algorithms that construct a digital twin generator (DTG) for RCT\nparticipants. The DTG is trained on historical control data and yields a\ndigital twin (DT) probability distribution for each participant's control\noutcome. The expectation of the DT distribution defines the single covariate\nfor adjustment. Historical control information are leveraged via an additive\nmixture prior with two components: an informative prior probability\ndistribution specified based on historical control data, and a non-informative\nprior distribution. The weight parameter in the mixture has a prior\ndistribution as well, so that the entire additive mixture prior distribution is\ncompletely pre-specifiable and does not involve any information from the RCT.\nWe establish an efficient Gibbs algorithm for sampling from the posterior\ndistribution, and derive closed-form expressions for the posterior mean and\nvariance of the treatment effect conditional on the weight parameter, of\nBayesian PROCOVA. We evaluate the bias control and variance reduction of\nBayesian PROCOVA compared to frequentist prognostic covariate adjustment\n(PROCOVA) via simulation studies that encompass different types of\ndiscrepancies between the historical control and RCT data. Ultimately, Bayesian\nPROCOVA can yield informative treatment effect inferences with fewer control\nparticipants, accelerating effective decision-making."}, "http://arxiv.org/abs/2310.18047": {"title": "Robust Bayesian Inference on Riemannian Submanifold", "link": "http://arxiv.org/abs/2310.18047", "description": "Non-Euclidean spaces routinely arise in modern statistical applications such\nas in medical imaging, robotics, and computer vision, to name a few. While\ntraditional Bayesian approaches are applicable to such settings by considering\nan ambient Euclidean space as the parameter space, we demonstrate the benefits\nof integrating manifold structure into the Bayesian framework, both\ntheoretically and computationally. Moreover, existing Bayesian approaches which\nare designed specifically for manifold-valued parameters are primarily\nmodel-based, which are typically subject to inaccurate uncertainty\nquantification under model misspecification. In this article, we propose a\nrobust model-free Bayesian inference for parameters defined on a Riemannian\nsubmanifold, which is shown to provide valid uncertainty quantification from a\nfrequentist perspective. Computationally, we propose a Markov chain Monte Carlo\nto sample from the posterior on the Riemannian submanifold, where the mixing\ntime, in the large sample regime, is shown to depend only on the intrinsic\ndimension of the parameter space instead of the potentially much larger ambient\ndimension. Our numerical results demonstrate the effectiveness of our approach\non a variety of problems, such as reduced-rank multiple quantile regression,\nprincipal component analysis, and Fr\\'{e}chet mean estimation."}, "http://arxiv.org/abs/2310.18108": {"title": "Transductive conformal inference with adaptive scores", "link": "http://arxiv.org/abs/2310.18108", "description": "Conformal inference is a fundamental and versatile tool that provides\ndistribution-free guarantees for many machine learning tasks. We consider the\ntransductive setting, where decisions are made on a test sample of $m$ new\npoints, giving rise to $m$ conformal $p$-values. {While classical results only\nconcern their marginal distribution, we show that their joint distribution\nfollows a P\\'olya urn model, and establish a concentration inequality for their\nempirical distribution function.} The results hold for arbitrary exchangeable\nscores, including {\\it adaptive} ones that can use the covariates of the\ntest+calibration samples at training stage for increased accuracy. We\ndemonstrate the usefulness of these theoretical results through uniform,\nin-probability guarantees for two machine learning tasks of current interest:\ninterval prediction for transductive transfer learning and novelty detection\nbased on two-class classification."}, "http://arxiv.org/abs/2310.18212": {"title": "Robustness of Algorithms for Causal Structure Learning to Hyperparameter Choice", "link": "http://arxiv.org/abs/2310.18212", "description": "Hyperparameters play a critical role in machine learning. Hyperparameter\ntuning can make the difference between state-of-the-art and poor prediction\nperformance for any algorithm, but it is particularly challenging for structure\nlearning due to its unsupervised nature. As a result, hyperparameter tuning is\noften neglected in favour of using the default values provided by a particular\nimplementation of an algorithm. While there have been numerous studies on\nperformance evaluation of causal discovery algorithms, how hyperparameters\naffect individual algorithms, as well as the choice of the best algorithm for a\nspecific problem, has not been studied in depth before. This work addresses\nthis gap by investigating the influence of hyperparameters on causal structure\nlearning tasks. Specifically, we perform an empirical evaluation of\nhyperparameter selection for some seminal learning algorithms on datasets of\nvarying levels of complexity. We find that, while the choice of algorithm\nremains crucial to obtaining state-of-the-art performance, hyperparameter\nselection in ensemble settings strongly influences the choice of algorithm, in\nthat a poor choice of hyperparameters can lead to analysts using algorithms\nwhich do not give state-of-the-art performance for their data."}, "http://arxiv.org/abs/2310.18261": {"title": "Label Shift Estimators for Non-Ignorable Missing Data", "link": "http://arxiv.org/abs/2310.18261", "description": "We consider the problem of estimating the mean of a random variable Y subject\nto non-ignorable missingness, i.e., where the missingness mechanism depends on\nY . We connect the auxiliary proxy variable framework for non-ignorable\nmissingness (West and Little, 2013) to the label shift setting (Saerens et al.,\n2002). Exploiting this connection, we construct an estimator for non-ignorable\nmissing data that uses high-dimensional covariates (or proxies) without the\nneed for a generative model. In synthetic and semi-synthetic experiments, we\nstudy the behavior of the proposed estimator, comparing it to commonly used\nignorable estimators in both well-specified and misspecified settings.\nAdditionally, we develop a score to assess how consistent the data are with the\nlabel shift assumption. We use our approach to estimate disease prevalence\nusing a large health survey, comparing ignorable and non-ignorable approaches.\nWe show that failing to account for non-ignorable missingness can have profound\nconsequences on conclusions drawn from non-representative samples."}, "http://arxiv.org/abs/2102.12698": {"title": "Improving the Hosmer-Lemeshow Goodness-of-Fit Test in Large Models with Replicated Trials", "link": "http://arxiv.org/abs/2102.12698", "description": "The Hosmer-Lemeshow (HL) test is a commonly used global goodness-of-fit (GOF)\ntest that assesses the quality of the overall fit of a logistic regression\nmodel. In this paper, we give results from simulations showing that the type 1\nerror rate (and hence power) of the HL test decreases as model complexity\ngrows, provided that the sample size remains fixed and binary replicates are\npresent in the data. We demonstrate that the generalized version of the HL test\nby Surjanovic et al. (2020) can offer some protection against this power loss.\nWe conclude with a brief discussion explaining the behaviour of the HL test,\nalong with some guidance on how to choose between the two tests."}, "http://arxiv.org/abs/2110.04852": {"title": "Mixture representations and Bayesian nonparametric inference for likelihood ratio ordered distributions", "link": "http://arxiv.org/abs/2110.04852", "description": "In this article, we introduce mixture representations for likelihood ratio\nordered distributions. Essentially, the ratio of two probability densities, or\nmass functions, is monotone if and only if one can be expressed as a mixture of\none-sided truncations of the other. To illustrate the practical value of the\nmixture representations, we address the problem of density estimation for\nlikelihood ratio ordered distributions. In particular, we propose a\nnonparametric Bayesian solution which takes advantage of the mixture\nrepresentations. The prior distribution is constructed from Dirichlet process\nmixtures and has large support on the space of pairs of densities satisfying\nthe monotone ratio constraint. Posterior consistency holds under reasonable\nconditions on the prior specification and the true unknown densities. To our\nknowledge, this is the first posterior consistency result in the literature on\norder constrained inference. With a simple modification to the prior\ndistribution, we can test the equality of two distributions against the\nalternative of likelihood ratio ordering. We develop a Markov chain Monte Carlo\nalgorithm for posterior inference and demonstrate the method in a biomedical\napplication."}, "http://arxiv.org/abs/2207.08911": {"title": "Deeply-Learned Generalized Linear Models with Missing Data", "link": "http://arxiv.org/abs/2207.08911", "description": "Deep Learning (DL) methods have dramatically increased in popularity in\nrecent years, with significant growth in their application to supervised\nlearning problems in the biomedical sciences. However, the greater prevalence\nand complexity of missing data in modern biomedical datasets present\nsignificant challenges for DL methods. Here, we provide a formal treatment of\nmissing data in the context of deeply learned generalized linear models, a\nsupervised DL architecture for regression and classification problems. We\npropose a new architecture, \\textit{dlglm}, that is one of the first to be able\nto flexibly account for both ignorable and non-ignorable patterns of\nmissingness in input features and response at training time. We demonstrate\nthrough statistical simulation that our method outperforms existing approaches\nfor supervised learning tasks in the presence of missing not at random (MNAR)\nmissingness. We conclude with a case study of a Bank Marketing dataset from the\nUCI Machine Learning Repository, in which we predict whether clients subscribed\nto a product based on phone survey data. Supplementary materials for this\narticle are available online."}, "http://arxiv.org/abs/2208.04627": {"title": "Causal Effect Identification in Uncertain Causal Networks", "link": "http://arxiv.org/abs/2208.04627", "description": "Causal identification is at the core of the causal inference literature,\nwhere complete algorithms have been proposed to identify causal queries of\ninterest. The validity of these algorithms hinges on the restrictive assumption\nof having access to a correctly specified causal structure. In this work, we\nstudy the setting where a probabilistic model of the causal structure is\navailable. Specifically, the edges in a causal graph exist with uncertainties\nwhich may, for example, represent degree of belief from domain experts.\nAlternatively, the uncertainty about an edge may reflect the confidence of a\nparticular statistical test. The question that naturally arises in this setting\nis: Given such a probabilistic graph and a specific causal effect of interest,\nwhat is the subgraph which has the highest plausibility and for which the\ncausal effect is identifiable? We show that answering this question reduces to\nsolving an NP-complete combinatorial optimization problem which we call the\nedge ID problem. We propose efficient algorithms to approximate this problem\nand evaluate them against both real-world networks and randomly generated\ngraphs."}, "http://arxiv.org/abs/2211.00268": {"title": "Stacking designs: designing multi-fidelity computer experiments with target predictive accuracy", "link": "http://arxiv.org/abs/2211.00268", "description": "In an era where scientific experiments can be very costly, multi-fidelity\nemulators provide a useful tool for cost-efficient predictive scientific\ncomputing. For scientific applications, the experimenter is often limited by a\ntight computational budget, and thus wishes to (i) maximize predictive power of\nthe multi-fidelity emulator via a careful design of experiments, and (ii)\nensure this model achieves a desired error tolerance with some notion of\nconfidence. Existing design methods, however, do not jointly tackle objectives\n(i) and (ii). We propose a novel stacking design approach that addresses both\ngoals. A multi-level reproducing kernel Hilbert space (RKHS) interpolator is\nfirst introduced to build the emulator, under which our stacking design\nprovides a sequential approach for designing multi-fidelity runs such that a\ndesired prediction error of $\\epsilon &gt; 0$ is met under regularity assumptions.\nWe then prove a novel cost complexity theorem that, under this multi-level\ninterpolator, establishes a bound on the computation cost (for training data\nsimulation) needed to achieve a prediction bound of $\\epsilon$. This result\nprovides novel insights on conditions under which the proposed multi-fidelity\napproach improves upon a conventional RKHS interpolator which relies on a\nsingle fidelity level. Finally, we demonstrate the effectiveness of stacking\ndesigns in a suite of simulation experiments and an application to finite\nelement analysis."}, "http://arxiv.org/abs/2211.05357": {"title": "Bayesian score calibration for approximate models", "link": "http://arxiv.org/abs/2211.05357", "description": "Scientists continue to develop increasingly complex mechanistic models to\nreflect their knowledge more realistically. Statistical inference using these\nmodels can be challenging since the corresponding likelihood function is often\nintractable and model simulation may be computationally burdensome.\nFortunately, in many of these situations, it is possible to adopt a surrogate\nmodel or approximate likelihood function. It may be convenient to conduct\nBayesian inference directly with the surrogate, but this can result in bias and\npoor uncertainty quantification. In this paper we propose a new method for\nadjusting approximate posterior samples to reduce bias and produce more\naccurate uncertainty quantification. We do this by optimizing a transform of\nthe approximate posterior that maximizes a scoring rule. Our approach requires\nonly a (fixed) small number of complex model simulations and is numerically\nstable. We demonstrate good performance of the new method on several examples\nof increasing complexity."}, "http://arxiv.org/abs/2302.00993": {"title": "Unpaired Multi-Domain Causal Representation Learning", "link": "http://arxiv.org/abs/2302.00993", "description": "The goal of causal representation learning is to find a representation of\ndata that consists of causally related latent variables. We consider a setup\nwhere one has access to data from multiple domains that potentially share a\ncausal representation. Crucially, observations in different domains are assumed\nto be unpaired, that is, we only observe the marginal distribution in each\ndomain but not their joint distribution. In this paper, we give sufficient\nconditions for identifiability of the joint distribution and the shared causal\ngraph in a linear setup. Identifiability holds if we can uniquely recover the\njoint distribution and the shared causal representation from the marginal\ndistributions in each domain. We transform our identifiability results into a\npractical method to recover the shared latent causal graph."}, "http://arxiv.org/abs/2303.17277": {"title": "Cross-temporal probabilistic forecast reconciliation: Methodological and practical issues", "link": "http://arxiv.org/abs/2303.17277", "description": "Forecast reconciliation is a post-forecasting process that involves\ntransforming a set of incoherent forecasts into coherent forecasts which\nsatisfy a given set of linear constraints for a multivariate time series. In\nthis paper we extend the current state-of-the-art cross-sectional probabilistic\nforecast reconciliation approach to encompass a cross-temporal framework, where\ntemporal constraints are also applied. Our proposed methodology employs both\nparametric Gaussian and non-parametric bootstrap approaches to draw samples\nfrom an incoherent cross-temporal distribution. To improve the estimation of\nthe forecast error covariance matrix, we propose using multi-step residuals,\nespecially in the time dimension where the usual one-step residuals fail. To\naddress high-dimensionality issues, we present four alternatives for the\ncovariance matrix, where we exploit the two-fold nature (cross-sectional and\ntemporal) of the cross-temporal structure, and introduce the idea of\noverlapping residuals. We assess the effectiveness of the proposed\ncross-temporal reconciliation approaches through a simulation study that\ninvestigates their theoretical and empirical properties and two forecasting\nexperiments, using the Australian GDP and the Australian Tourism Demand\ndatasets. For both applications, the optimal cross-temporal reconciliation\napproaches significantly outperform the incoherent base forecasts in terms of\nthe Continuous Ranked Probability Score and the Energy Score. Overall, the\nresults highlight the potential of the proposed methods to improve the accuracy\nof probabilistic forecasts and to address the challenge of integrating\ndisparate scenarios while coherently taking into account short-term\noperational, medium-term tactical, and long-term strategic planning."}, "http://arxiv.org/abs/2309.07867": {"title": "Beta Diffusion", "link": "http://arxiv.org/abs/2309.07867", "description": "We introduce beta diffusion, a novel generative modeling method that\nintegrates demasking and denoising to generate data within bounded ranges.\nUsing scaled and shifted beta distributions, beta diffusion utilizes\nmultiplicative transitions over time to create both forward and reverse\ndiffusion processes, maintaining beta distributions in both the forward\nmarginals and the reverse conditionals, given the data at any point in time.\nUnlike traditional diffusion-based generative models relying on additive\nGaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is\nmultiplicative and optimized with KL-divergence upper bounds (KLUBs) derived\nfrom the convexity of the KL divergence. We demonstrate that the proposed KLUBs\nare more effective for optimizing beta diffusion compared to negative ELBOs,\nwhich can also be derived as the KLUBs of the same KL divergence with its two\narguments swapped. The loss function of beta diffusion, expressed in terms of\nBregman divergence, further supports the efficacy of KLUBs for optimization.\nExperimental results on both synthetic data and natural images demonstrate the\nunique capabilities of beta diffusion in generative modeling of range-bounded\ndata and validate the effectiveness of KLUBs in optimizing diffusion models,\nthereby making them valuable additions to the family of diffusion-based\ngenerative models and the optimization techniques used to train them."}, "http://arxiv.org/abs/2310.18422": {"title": "Inference via Wild Bootstrap and Multiple Imputation under Fine-Gray Models with Incomplete Data", "link": "http://arxiv.org/abs/2310.18422", "description": "Fine-Gray models specify the subdistribution hazards for one out of multiple\ncompeting risks to be proportional. The estimators of parameters and cumulative\nincidence functions under Fine-Gray models have a simpler structure when data\nare censoring-complete than when they are more generally incomplete. This paper\nconsiders the case of incomplete data but it exploits the above-mentioned\nsimpler estimator structure for which there exists a wild bootstrap approach\nfor inferential purposes. The present idea is to link the methodology under\ncensoring-completeness with the more general right-censoring regime with the\nhelp of multiple imputation. In a simulation study, this approach is compared\nto the estimation procedure proposed in the original paper by Fine and Gray\nwhen it is combined with a bootstrap approach. An application to a data set\nabout hospital-acquired infections illustrates the method."}, "http://arxiv.org/abs/2310.18474": {"title": "Robust Bayesian Graphical Regression Models for Assessing Tumor Heterogeneity in Proteomic Networks", "link": "http://arxiv.org/abs/2310.18474", "description": "Graphical models are powerful tools to investigate complex dependency\nstructures in high-throughput datasets. However, most existing graphical models\nmake one of the two canonical assumptions: (i) a homogeneous graph with a\ncommon network for all subjects; or (ii) an assumption of normality especially\nin the context of Gaussian graphical models. Both assumptions are restrictive\nand can fail to hold in certain applications such as proteomic networks in\ncancer. To this end, we propose an approach termed robust Bayesian graphical\nregression (rBGR) to estimate heterogeneous graphs for non-normally distributed\ndata. rBGR is a flexible framework that accommodates non-normality through\nrandom marginal transformations and constructs covariate-dependent graphs to\naccommodate heterogeneity through graphical regression techniques. We formulate\na new characterization of edge dependencies in such models called conditional\nsign independence with covariates along with an efficient posterior sampling\nalgorithm. In simulation studies, we demonstrate that rBGR outperforms existing\ngraphical regression models for data generated under various levels of\nnon-normality in both edge and covariate selection. We use rBGR to assess\nproteomic networks across two cancers: lung and ovarian, to systematically\ninvestigate the effects of immunogenic heterogeneity within tumors. Our\nanalyses reveal several important protein-protein interactions that are\ndifferentially impacted by the immune cell abundance; some corroborate existing\nbiological knowledge whereas others are novel findings."}, "http://arxiv.org/abs/2310.18500": {"title": "Designing Randomized Experiments to Predict Unit-Specific Treatment Effects", "link": "http://arxiv.org/abs/2310.18500", "description": "Typically, a randomized experiment is designed to test a hypothesis about the\naverage treatment effect and sometimes hypotheses about treatment effect\nvariation. The results of such a study may then be used to inform policy and\npractice for units not in the study. In this paper, we argue that given this\nuse, randomized experiments should instead be designed to predict unit-specific\ntreatment effects in a well-defined population. We then consider how different\nsampling processes and models affect the bias, variance, and mean squared\nprediction error of these predictions. The results indicate, for example, that\nproblems of generalizability (differences between samples and populations) can\ngreatly affect bias both in predictive models and in measures of error in these\nmodels. We also examine when the average treatment effect estimate outperforms\nunit-specific treatment effect predictive models and implications of this for\nplanning studies."}, "http://arxiv.org/abs/2310.18527": {"title": "Multiple Imputation Method for High-Dimensional Neuroimaging Data", "link": "http://arxiv.org/abs/2310.18527", "description": "Missingness is a common issue for neuroimaging data, and neglecting it in\ndownstream statistical analysis can introduce bias and lead to misguided\ninferential conclusions. It is therefore crucial to conduct appropriate\nstatistical methods to address this issue. While multiple imputation is a\npopular technique for handling missing data, its application to neuroimaging\ndata is hindered by high dimensionality and complex dependence structures of\nmultivariate neuroimaging variables. To tackle this challenge, we propose a\nnovel approach, named High Dimensional Multiple Imputation (HIMA), based on\nBayesian models. HIMA develops a new computational strategy for sampling large\ncovariance matrices based on a robustly estimated posterior mode, which\ndrastically enhances computational efficiency and numerical stability. To\nassess the effectiveness of HIMA, we conducted extensive simulation studies and\nreal-data analysis using neuroimaging data from a Schizophrenia study. HIMA\nshowcases a computational efficiency improvement of over 2000 times when\ncompared to traditional approaches, while also producing imputed datasets with\nimproved precision and stability."}, "http://arxiv.org/abs/2310.18533": {"title": "Evaluating the effects of high-throughput structural neuroimaging predictors on whole-brain functional connectome outcomes via network-based vector-on-matrix regression", "link": "http://arxiv.org/abs/2310.18533", "description": "The joint analysis of multimodal neuroimaging data is critical in the field\nof brain research because it reveals complex interactive relationships between\nneurobiological structures and functions. In this study, we focus on\ninvestigating the effects of structural imaging (SI) features, including white\nmatter micro-structure integrity (WMMI) and cortical thickness, on the whole\nbrain functional connectome (FC) network. To achieve this goal, we propose a\nnetwork-based vector-on-matrix regression model to characterize the FC-SI\nassociation patterns. We have developed a novel multi-level dense bipartite and\nclique subgraph extraction method to identify which subsets of spatially\nspecific SI features intensively influence organized FC sub-networks. The\nproposed method can simultaneously identify highly correlated\nstructural-connectomic association patterns and suppress false positive\nfindings while handling millions of potential interactions. We apply our method\nto a multimodal neuroimaging dataset of 4,242 participants from the UK Biobank\nto evaluate the effects of whole-brain WMMI and cortical thickness on the\nresting-state FC. The results reveal that the WMMI on corticospinal tracts and\ninferior cerebellar peduncle significantly affect functional connections of\nsensorimotor, salience, and executive sub-networks with an average correlation\nof 0.81 (p&lt;0.001)."}, "http://arxiv.org/abs/2310.18536": {"title": "Efficient Fully Bayesian Approach to Brain Activity Mapping with Complex-Valued fMRI Data", "link": "http://arxiv.org/abs/2310.18536", "description": "Functional magnetic resonance imaging (fMRI) enables indirect detection of\nbrain activity changes via the blood-oxygen-level-dependent (BOLD) signal.\nConventional analysis methods mainly rely on the real-valued magnitude of these\nsignals. In contrast, research suggests that analyzing both real and imaginary\ncomponents of the complex-valued fMRI (cv-fMRI) signal provides a more holistic\napproach that can increase power to detect neuronal activation. We propose a\nfully Bayesian model for brain activity mapping with cv-fMRI data. Our model\naccommodates temporal and spatial dynamics. Additionally, we propose a\ncomputationally efficient sampling algorithm, which enhances processing speed\nthrough image partitioning. Our approach is shown to be computationally\nefficient via image partitioning and parallel computation while being\ncompetitive with state-of-the-art methods. We support these claims with both\nsimulated numerical studies and an application to real cv-fMRI data obtained\nfrom a finger-tapping experiment."}, "http://arxiv.org/abs/2310.18556": {"title": "Design-Based Causal Inference with Missing Outcomes: Missingness Mechanisms, Imputation-Assisted Randomization Tests, and Covariate Adjustment", "link": "http://arxiv.org/abs/2310.18556", "description": "Design-based causal inference is one of the most widely used frameworks for\ntesting causal null hypotheses or inferring about causal parameters from\nexperimental or observational data. The most significant merit of design-based\ncausal inference is that its statistical validity only comes from the study\ndesign (e.g., randomization design) and does not require assuming any\noutcome-generating distributions or models. Although immune to model\nmisspecification, design-based causal inference can still suffer from other\ndata challenges, among which missingness in outcomes is a significant one.\nHowever, compared with model-based causal inference, outcome missingness in\ndesign-based causal inference is much less studied, largely due to the\nchallenge that design-based causal inference does not assume any outcome\ndistributions/models and, therefore, cannot directly adopt any existing\nmodel-based approaches for missing data. To fill this gap, we systematically\nstudy the missing outcomes problem in design-based causal inference. First, we\nuse the potential outcomes framework to clarify the minimal assumption\n(concerning the outcome missingness mechanism) needed for conducting\nfinite-population-exact randomization tests for the null effect (i.e., Fisher's\nsharp null) and that needed for constructing finite-population-exact confidence\nsets with missing outcomes. Second, we propose a general framework called\n``imputation and re-imputation\" for conducting finite-population-exact\nrandomization tests in design-based causal studies with missing outcomes. Our\nframework can incorporate any existing outcome imputation algorithms and\nmeanwhile guarantee finite-population-exact type-I error rate control. Third,\nwe extend our framework to conduct covariate adjustment in an exact\nrandomization test with missing outcomes and to construct\nfinite-population-exact confidence sets with missing outcomes."}, "http://arxiv.org/abs/2310.18563": {"title": "Covariate Balancing and the Equivalence of Weighting and Doubly Robust Estimators of Average Treatment Effects", "link": "http://arxiv.org/abs/2310.18563", "description": "We show that when the propensity score is estimated using a suitable\ncovariate balancing procedure, the commonly used inverse probability weighting\n(IPW) estimator, augmented inverse probability weighting (AIPW) with linear\nconditional mean, and inverse probability weighted regression adjustment\n(IPWRA) with linear conditional mean are all numerically the same for\nestimating the average treatment effect (ATE) or the average treatment effect\non the treated (ATT). Further, suitably chosen covariate balancing weights are\nautomatically normalized, which means that normalized and unnormalized versions\nof IPW and AIPW are identical. For estimating the ATE, the weights that achieve\nthe algebraic equivalence of IPW, AIPW, and IPWRA are based on propensity\nscores estimated using the inverse probability tilting (IPT) method of Graham,\nPinto and Egel (2012). For the ATT, the weights are obtained using the\ncovariate balancing propensity score (CBPS) method developed in Imai and\nRatkovic (2014). These equivalences also make covariate balancing methods\nattractive when the treatment is confounded and one is interested in the local\naverage treatment effect."}, "http://arxiv.org/abs/2310.18611": {"title": "Sequential Kalman filter for fast online changepoint detection in longitudinal health records", "link": "http://arxiv.org/abs/2310.18611", "description": "This article introduces the sequential Kalman filter, a computationally\nscalable approach for online changepoint detection with temporally correlated\ndata. The temporal correlation was not considered in the Bayesian online\nchangepoint detection approach due to the large computational cost. Motivated\nby detecting COVID-19 infections for dialysis patients from massive\nlongitudinal health records with a large number of covariates, we develop a\nscalable approach to detect multiple changepoints from correlated data by\nsequentially stitching Kalman filters of subsequences to compute the joint\ndistribution of the observations, which has linear computational complexity\nwith respect to the number of observations between the last detected\nchangepoint and the current observation at each time point, without\napproximating the likelihood function. Compared to other online changepoint\ndetection methods, simulated experiments show that our approach is more precise\nin detecting single or multiple changes in mean, variance, or correlation for\ntemporally correlated data. Furthermore, we propose a new way to integrate\nclassification and changepoint detection approaches that improve the detection\ndelay and accuracy for detecting COVID-19 infection compared to other\nalternatives."}, "http://arxiv.org/abs/2310.18733": {"title": "Threshold detection under a semiparametric regression model", "link": "http://arxiv.org/abs/2310.18733", "description": "Linear regression models have been extensively considered in the literature.\nHowever, in some practical applications they may not be appropriate all over\nthe range of the covariate. In this paper, a more flexible model is introduced\nby considering a regression model $Y=r(X)+\\varepsilon$ where the regression\nfunction $r(\\cdot)$ is assumed to be linear for large values in the domain of\nthe predictor variable $X$. More precisely, we assume that\n$r(x)=\\alpha_0+\\beta_0 x$ for $x&gt; u_0$, where the value $u_0$ is identified as\nthe smallest value satisfying such a property. A penalized procedure is\nintroduced to estimate the threshold $u_0$. The considered proposal focusses on\na semiparametric approach since no parametric model is assumed for the\nregression function for values smaller than $u_0$. Consistency properties of\nboth the threshold estimator and the estimators of $(\\alpha_0,\\beta_0)$ are\nderived, under mild assumptions. Through a numerical study, the small sample\nproperties of the proposed procedure and the importance of introducing a\npenalization are investigated. The analysis of a real data set allows us to\ndemonstrate the usefulness of the penalized estimators."}, "http://arxiv.org/abs/2310.18766": {"title": "Discussion of ''A Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks''", "link": "http://arxiv.org/abs/2310.18766", "description": "This review discusses the paper ''A Tale of Two Datasets: Representativeness\nand Generalisability of Inference for Samples of Networks'' by Krivitsky,\nColetti, and Hens, published in the Journal of the American Statistical\nAssociation in 2023."}, "http://arxiv.org/abs/2310.18836": {"title": "Design of Cluster-Randomized Trials with Cross-Cluster Interference", "link": "http://arxiv.org/abs/2310.18836", "description": "Cluster-randomized trials often involve units that are irregularly\ndistributed in space without well-separated communities. In these settings,\ncluster construction is a critical aspect of the design due to the potential\nfor cross-cluster interference. The existing literature relies on partial\ninterference models, which take clusters as given and assume no cross-cluster\ninterference. We relax this assumption by allowing interference to decay with\ngeographic distance between units. This induces a bias-variance trade-off:\nconstructing fewer, larger clusters reduces bias due to interference but\nincreases variance. We propose new estimators that exclude units most\npotentially impacted by cross-cluster interference and show that this\nsubstantially reduces asymptotic bias relative to conventional\ndifference-in-means estimators. We then study the design of clusters to\noptimize the estimators' rates of convergence. We provide formal justification\nfor a new design that chooses the number of clusters to balance the asymptotic\nbias and variance of our estimators and uses unsupervised learning to automate\ncluster construction."}, "http://arxiv.org/abs/2310.18858": {"title": "Estimating a function of the scale parameter in a gamma distribution with bounded variance", "link": "http://arxiv.org/abs/2310.18858", "description": "Given a gamma population with known shape parameter $\\alpha$, we develop a\ngeneral theory for estimating a function $g(\\cdot)$ of the scale parameter\n$\\beta$ with bounded variance. We begin by defining a sequential sampling\nprocedure with $g(\\cdot)$ satisfying some desired condition in proposing the\nstopping rule, and show the procedure enjoys appealing asymptotic properties.\nAfter these general conditions, we substitute $g(\\cdot)$ with specific\nfunctions including the gamma mean, the gamma variance, the gamma rate\nparameter, and a gamma survival probability as four possible illustrations. For\neach illustration, Monte Carlo simulations are carried out to justify the\nremarkable performance of our proposed sequential procedure. This is further\nsubstantiated with a real data study on weights of newly born babies."}, "http://arxiv.org/abs/2310.18875": {"title": "Feature calibration for computer models", "link": "http://arxiv.org/abs/2310.18875", "description": "Computer model calibration involves using partial and imperfect observations\nof the real world to learn which values of a model's input parameters lead to\noutputs that are consistent with real-world observations. When calibrating\nmodels with high-dimensional output (e.g. a spatial field), it is common to\nrepresent the output as a linear combination of a small set of basis vectors.\nOften, when trying to calibrate to such output, what is important to the\ncredibility of the model is that key emergent physical phenomena are\nrepresented, even if not faithfully or in the right place. In these cases,\ncomparison of model output and data in a linear subspace is inappropriate and\nwill usually lead to poor model calibration. To overcome this, we present\nkernel-based history matching (KHM), generalising the meaning of the technique\nsufficiently to be able to project model outputs and observations into a\nhigher-dimensional feature space, where patterns can be compared without their\nlocation necessarily being fixed. We develop the technical methodology, present\nan expert-driven kernel selection algorithm, and then apply the techniques to\nthe calibration of boundary layer clouds for the French climate model IPSL-CM."}, "http://arxiv.org/abs/2310.18905": {"title": "Incorporating nonparametric methods for estimating causal excursion effects in mobile health with zero-inflated count outcomes", "link": "http://arxiv.org/abs/2310.18905", "description": "In the domain of mobile health, tailoring interventions for real-time\ndelivery is of paramount importance. Micro-randomized trials have emerged as\nthe \"gold-standard\" methodology for developing such interventions. Analyzing\ndata from these trials provides insights into the efficacy of interventions and\nthe potential moderation by specific covariates. The \"causal excursion effect\",\na novel class of causal estimand, addresses these inquiries, backed by current\nsemiparametric inference techniques. Yet, existing methods mainly focus on\ncontinuous or binary data, leaving count data largely unexplored. The current\nwork is motivated by the Drink Less micro-randomized trial from the UK, which\nfocuses on a zero-inflated proximal outcome, the number of screen views in the\nsubsequent hour following the intervention decision point. In the current\npaper, we revisit the concept of causal excursion effects, specifically for\nzero-inflated count outcomes, and introduce novel estimation approaches that\nincorporate nonparametric techniques. Bidirectional asymptotics are derived for\nthe proposed estimators. Through extensive simulation studies, we evaluate the\nperformance of the proposed estimators. As an illustration, we also employ the\nproposed methods to the Drink Less trial data."}, "http://arxiv.org/abs/2310.18963": {"title": "Expectile-based conditional tail moments with covariates", "link": "http://arxiv.org/abs/2310.18963", "description": "Expectile, as the minimizer of an asymmetric quadratic loss function, is a\ncoherent risk measure and is helpful to use more information about the\ndistribution of the considered risk. In this paper, we propose a new risk\nmeasure by replacing quantiles by expectiles, called expectile-based\nconditional tail moment, and focus on the estimation of this new risk measure\nas the conditional survival function of the risk, given the risk exceeding the\nexpectile and given a value of the covariates, is heavy tail. Under some\nregular conditions, asymptotic properties of this new estimator are considered.\nThe extrapolated estimation of the conditional tail moments is also\ninvestigated. These results are illustrated both on simulated data and on a\nreal insurance data."}, "http://arxiv.org/abs/2310.19043": {"title": "Differentially Private Permutation Tests: Applications to Kernel Methods", "link": "http://arxiv.org/abs/2310.19043", "description": "Recent years have witnessed growing concerns about the privacy of sensitive\ndata. In response to these concerns, differential privacy has emerged as a\nrigorous framework for privacy protection, gaining widespread recognition in\nboth academic and industrial circles. While substantial progress has been made\nin private data analysis, existing methods often suffer from impracticality or\na significant loss of statistical efficiency. This paper aims to alleviate\nthese concerns in the context of hypothesis testing by introducing\ndifferentially private permutation tests. The proposed framework extends\nclassical non-private permutation tests to private settings, maintaining both\nfinite-sample validity and differential privacy in a rigorous manner. The power\nof the proposed test depends on the choice of a test statistic, and we\nestablish general conditions for consistency and non-asymptotic uniform power.\nTo demonstrate the utility and practicality of our framework, we focus on\nreproducing kernel-based test statistics and introduce differentially private\nkernel tests for two-sample and independence testing: dpMMD and dpHSIC. The\nproposed kernel tests are straightforward to implement, applicable to various\ntypes of data, and attain minimax optimal power across different privacy\nregimes. Our empirical evaluations further highlight their competitive power\nunder various synthetic and real-world scenarios, emphasizing their practical\nvalue. The code is publicly available to facilitate the implementation of our\nframework."}, "http://arxiv.org/abs/2310.19051": {"title": "A Survey of Methods for Estimating Hurst Exponent of Time Sequence", "link": "http://arxiv.org/abs/2310.19051", "description": "The Hurst exponent is a significant indicator for characterizing the\nself-similarity and long-term memory properties of time sequences. It has wide\napplications in physics, technologies, engineering, mathematics, statistics,\neconomics, psychology and so on. Currently, available methods for estimating\nthe Hurst exponent of time sequences can be divided into different categories:\ntime-domain methods and spectrum-domain methods based on the representation of\ntime sequence, linear regression methods and Bayesian methods based on\nparameter estimation methods. Although various methods are discussed in\nliterature, there are still some deficiencies: the descriptions of the\nestimation algorithms are just mathematics-oriented and the pseudo-codes are\nmissing; the effectiveness and accuracy of the estimation algorithms are not\nclear; the classification of estimation methods is not considered and there is\na lack of guidance for selecting the estimation methods. In this work, the\nemphasis is put on thirteen dominant methods for estimating the Hurst exponent.\nFor the purpose of decreasing the difficulty of implementing the estimation\nmethods with computer programs, the mathematical principles are discussed\nbriefly and the pseudo-codes of algorithms are presented with necessary\ndetails. It is expected that the survey could help the researchers to select,\nimplement and apply the estimation algorithms of interest in practical\nsituations in an easy way."}, "http://arxiv.org/abs/2310.19091": {"title": "Bridging the Gap: Towards an Expanded Toolkit for ML-Supported Decision-Making in the Public Sector", "link": "http://arxiv.org/abs/2310.19091", "description": "Machine Learning (ML) systems are becoming instrumental in the public sector,\nwith applications spanning areas like criminal justice, social welfare,\nfinancial fraud detection, and public health. While these systems offer great\npotential benefits to institutional decision-making processes, such as improved\nefficiency and reliability, they still face the challenge of aligning intricate\nand nuanced policy objectives with the precise formalization requirements\nnecessitated by ML models. In this paper, we aim to bridge the gap between ML\nand public sector decision-making by presenting a comprehensive overview of key\ntechnical challenges where disjunctions between policy goals and ML models\ncommonly arise. We concentrate on pivotal points of the ML pipeline that\nconnect the model to its operational environment, delving into the significance\nof representative training data and highlighting the importance of a model\nsetup that facilitates effective decision-making. Additionally, we link these\nchallenges with emerging methodological advancements, encompassing causal ML,\ndomain adaptation, uncertainty quantification, and multi-objective\noptimization, illustrating the path forward for harmonizing ML and public\nsector objectives."}, "http://arxiv.org/abs/2310.19114": {"title": "Sparse Fr\\'echet Sufficient Dimension Reduction with Graphical Structure Among Predictors", "link": "http://arxiv.org/abs/2310.19114", "description": "Fr\\'echet regression has received considerable attention to model\nmetric-space valued responses that are complex and non-Euclidean data, such as\nprobability distributions and vectors on the unit sphere. However, existing\nFr\\'echet regression literature focuses on the classical setting where the\npredictor dimension is fixed, and the sample size goes to infinity. This paper\nproposes sparse Fr\\'echet sufficient dimension reduction with graphical\nstructure among high-dimensional Euclidean predictors. In particular, we\npropose a convex optimization problem that leverages the graphical information\namong predictors and avoids inverting the high-dimensional covariance matrix.\nWe also provide the Alternating Direction Method of Multipliers (ADMM)\nalgorithm to solve the optimization problem. Theoretically, the proposed method\nachieves subspace estimation and variable selection consistency under suitable\nconditions. Extensive simulations and a real data analysis are carried out to\nillustrate the finite-sample performance of the proposed method."}, "http://arxiv.org/abs/2310.19246": {"title": "A spectral regularisation framework for latent variable models designed for single channel applications", "link": "http://arxiv.org/abs/2310.19246", "description": "Latent variable models (LVMs) are commonly used to capture the underlying\ndependencies, patterns, and hidden structure in observed data. Source\nduplication is a by-product of the data hankelisation pre-processing step\ncommon to single channel LVM applications, which hinders practical LVM\nutilisation. In this article, a Python package titled\nspectrally-regularised-LVMs is presented. The proposed package addresses the\nsource duplication issue via the addition of a novel spectral regularisation\nterm. This package provides a framework for spectral regularisation in single\nchannel LVM applications, thereby making it easier to investigate and utilise\nLVMs with spectral regularisation. This is achieved via the use of symbolic or\nexplicit representations of potential LVM objective functions which are\nincorporated into a framework that uses spectral regularisation during the LVM\nparameter estimation process. The objective of this package is to provide a\nconsistent linear LVM optimisation framework which incorporates spectral\nregularisation and caters to single channel time-series applications."}, "http://arxiv.org/abs/2310.19253": {"title": "Flow-based Distributionally Robust Optimization", "link": "http://arxiv.org/abs/2310.19253", "description": "We present a computationally efficient framework, called \\texttt{FlowDRO},\nfor solving flow-based distributionally robust optimization (DRO) problems with\nWasserstein uncertainty sets, when requiring the worst-case distribution (also\ncalled the Least Favorable Distribution, LFD) to be continuous so that the\nalgorithm can be scalable to problems with larger sample sizes and achieve\nbetter generalization capability for the induced robust algorithms. To tackle\nthe computationally challenging infinitely dimensional optimization problem, we\nleverage flow-based models, continuous-time invertible transport maps between\nthe data distribution and the target distribution, and develop a Wasserstein\nproximal gradient flow type of algorithm. In practice, we parameterize the\ntransport maps by a sequence of neural networks progressively trained in blocks\nby gradient descent. Our computational framework is general, can handle\nhigh-dimensional data with large sample sizes, and can be useful for various\napplications. We demonstrate its usage in adversarial learning,\ndistributionally robust hypothesis testing, and a new mechanism for data-driven\ndistribution perturbation differential privacy, where the proposed method gives\nstrong empirical performance on real high-dimensional data."}, "http://arxiv.org/abs/2310.19343": {"title": "Quantile Super Learning for independent and online settings with application to solar power forecasting", "link": "http://arxiv.org/abs/2310.19343", "description": "Estimating quantiles of an outcome conditional on covariates is of\nfundamental interest in statistics with broad application in probabilistic\nprediction and forecasting. We propose an ensemble method for conditional\nquantile estimation, Quantile Super Learning, that combines predictions from\nmultiple candidate algorithms based on their empirical performance measured\nwith respect to a cross-validated empirical risk of the quantile loss function.\nWe present theoretical guarantees for both iid and online data scenarios. The\nperformance of our approach for quantile estimation and in forming prediction\nintervals is tested in simulation studies. Two case studies related to solar\nenergy are used to illustrate Quantile Super Learning: in an iid setting, we\npredict the physical properties of perovskite materials for photovoltaic cells,\nand in an online setting we forecast ground solar irradiance based on output\nfrom dynamic weather ensemble models."}, "http://arxiv.org/abs/2310.19433": {"title": "Ordinal classification for interval-valued data and interval-valued functional data", "link": "http://arxiv.org/abs/2310.19433", "description": "The aim of ordinal classification is to predict the ordered labels of the\noutput from a set of observed inputs. Interval-valued data refers to data in\nthe form of intervals. For the first time, interval-valued data and\ninterval-valued functional data are considered as inputs in an ordinal\nclassification problem. Six ordinal classifiers for interval data and\ninterval-valued functional data are proposed. Three of them are parametric, one\nof them is based on ordinal binary decompositions and the other two are based\non ordered logistic regression. The other three methods are based on the use of\ndistances between interval data and kernels on interval data. One of the\nmethods uses the weighted $k$-nearest-neighbor technique for ordinal\nclassification. Another method considers kernel principal component analysis\nplus an ordinal classifier. And the sixth method, which is the method that\nperforms best, uses a kernel-induced ordinal random forest. They are compared\nwith na\\\"ive approaches in an extensive experimental study with synthetic and\noriginal real data sets, about human global development, and weather data. The\nresults show that considering ordering and interval-valued information improves\nthe accuracy. The source code and data sets are available at\nhttps://github.com/aleixalcacer/OCFIVD."}, "http://arxiv.org/abs/2310.19435": {"title": "A novel characterization of structures in smooth regression curves: from a viewpoint of persistent homology", "link": "http://arxiv.org/abs/2310.19435", "description": "We characterize structures such as monotonicity, convexity, and modality in\nsmooth regression curves using persistent homology. Persistent homology is a\nkey tool in topological data analysis that detects higher dimensional\ntopological features such as connected components and holes (cycles or loops)\nin the data. In other words, persistent homology is a multiscale version of\nhomology that characterizes sets based on the connected components and holes.\nWe use super-level sets of functions to extract geometric features via\npersistent homology. In particular, we explore structures in regression curves\nvia the persistent homology of super-level sets of a function, where the\nfunction of interest is - the first derivative of the regression function.\n\nIn the course of this study, we extend an existing procedure of estimating\nthe persistent homology for the first derivative of a regression function and\nestablish its consistency. Moreover, as an application of the proposed\nmethodology, we demonstrate that the persistent homology of the derivative of a\nfunction can reveal hidden structures in the function that are not visible from\nthe persistent homology of the function itself. In addition, we also illustrate\nthat the proposed procedure can be used to compare the shapes of two or more\nregression curves which is not possible merely from the persistent homology of\nthe function itself."}, "http://arxiv.org/abs/2310.19519": {"title": "A General Neural Causal Model for Interactive Recommendation", "link": "http://arxiv.org/abs/2310.19519", "description": "Survivor bias in observational data leads the optimization of recommender\nsystems towards local optima. Currently most solutions re-mines existing\nhuman-system collaboration patterns to maximize longer-term satisfaction by\nreinforcement learning. However, from the causal perspective, mitigating\nsurvivor effects requires answering a counterfactual problem, which is\ngenerally unidentifiable and inestimable. In this work, we propose a neural\ncausal model to achieve counterfactual inference. Specifically, we first build\na learnable structural causal model based on its available graphical\nrepresentations which qualitatively characterizes the preference transitions.\nMitigation of the survivor bias is achieved though counterfactual consistency.\nTo identify the consistency, we use the Gumbel-max function as structural\nconstrains. To estimate the consistency, we apply reinforcement optimizations,\nand use Gumbel-Softmax as a trade-off to get a differentiable function. Both\ntheoretical and empirical studies demonstrate the effectiveness of our\nsolution."}, "http://arxiv.org/abs/2310.19621": {"title": "A Bayesian Methodology for Estimation for Sparse Canonical Correlation", "link": "http://arxiv.org/abs/2310.19621", "description": "It can be challenging to perform an integrative statistical analysis of\nmulti-view high-dimensional data acquired from different experiments on each\nsubject who participated in a joint study. Canonical Correlation Analysis (CCA)\nis a statistical procedure for identifying relationships between such data\nsets. In that context, Structured Sparse CCA (ScSCCA) is a rapidly emerging\nmethodological area that aims for robust modeling of the interrelations between\nthe different data modalities by assuming the corresponding CCA directional\nvectors to be sparse. Although it is a rapidly growing area of statistical\nmethodology development, there is a need for developing related methodologies\nin the Bayesian paradigm. In this manuscript, we propose a novel ScSCCA\napproach where we employ a Bayesian infinite factor model and aim to achieve\nrobust estimation by encouraging sparsity in two different levels of the\nmodeling framework. Firstly, we utilize a multiplicative Half-Cauchy process\nprior to encourage sparsity at the level of the latent variable loading\nmatrices. Additionally, we promote further sparsity in the covariance matrix by\nusing graphical horseshoe prior or diagonal structure. We conduct multiple\nsimulations to compare the performance of the proposed method with that of\nother frequently used CCA procedures, and we apply the developed procedures to\nanalyze multi-omics data arising from a breast cancer study."}, "http://arxiv.org/abs/2310.19683": {"title": "An Online Bootstrap for Time Series", "link": "http://arxiv.org/abs/2310.19683", "description": "Resampling methods such as the bootstrap have proven invaluable in the field\nof machine learning. However, the applicability of traditional bootstrap\nmethods is limited when dealing with large streams of dependent data, such as\ntime series or spatially correlated observations. In this paper, we propose a\nnovel bootstrap method that is designed to account for data dependencies and\ncan be executed online, making it particularly suitable for real-time\napplications. This method is based on an autoregressive sequence of\nincreasingly dependent resampling weights. We prove the theoretical validity of\nthe proposed bootstrap scheme under general conditions. We demonstrate the\neffectiveness of our approach through extensive simulations and show that it\nprovides reliable uncertainty quantification even in the presence of complex\ndata dependencies. Our work bridges the gap between classical resampling\ntechniques and the demands of modern data analysis, providing a valuable tool\nfor researchers and practitioners in dynamic, data-rich environments."}, "http://arxiv.org/abs/2310.19787": {"title": "$e^{\\text{RPCA}}$: Robust Principal Component Analysis for Exponential Family Distributions", "link": "http://arxiv.org/abs/2310.19787", "description": "Robust Principal Component Analysis (RPCA) is a widely used method for\nrecovering low-rank structure from data matrices corrupted by significant and\nsparse outliers. These corruptions may arise from occlusions, malicious\ntampering, or other causes for anomalies, and the joint identification of such\ncorruptions with low-rank background is critical for process monitoring and\ndiagnosis. However, existing RPCA methods and their extensions largely do not\naccount for the underlying probabilistic distribution for the data matrices,\nwhich in many applications are known and can be highly non-Gaussian. We thus\npropose a new method called Robust Principal Component Analysis for Exponential\nFamily distributions ($e^{\\text{RPCA}}$), which can perform the desired\ndecomposition into low-rank and sparse matrices when such a distribution falls\nwithin the exponential family. We present a novel alternating direction method\nof multiplier optimization algorithm for efficient $e^{\\text{RPCA}}$\ndecomposition. The effectiveness of $e^{\\text{RPCA}}$ is then demonstrated in\ntwo applications: the first for steel sheet defect detection, and the second\nfor crime activity monitoring in the Atlanta metropolitan area."}, "http://arxiv.org/abs/2310.19788": {"title": "Locally Optimal Best Arm Identification with a Fixed Budget", "link": "http://arxiv.org/abs/2310.19788", "description": "This study investigates the problem of identifying the best treatment arm, a\ntreatment arm with the highest expected outcome. We aim to identify the best\ntreatment arm with a lower probability of misidentification, which has been\nexplored under various names across numerous research fields, including\n\\emph{best arm identification} (BAI) and ordinal optimization. In our\nexperiments, the number of treatment-allocation rounds is fixed. In each round,\na decision-maker allocates a treatment arm to an experimental unit and observes\na corresponding outcome, which follows a Gaussian distribution with a variance\ndifferent among treatment arms. At the end of the experiment, we recommend one\nof the treatment arms as an estimate of the best treatment arm based on the\nobservations. The objective of the decision-maker is to design an experiment\nthat minimizes the probability of misidentifying the best treatment arm. With\nthis objective in mind, we develop lower bounds for the probability of\nmisidentification under the small-gap regime, where the gaps of the expected\noutcomes between the best and suboptimal treatment arms approach zero. Then,\nassuming that the variances are known, we design the\nGeneralized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, which is\nan extension of the Neyman allocation proposed by Neyman (1934) and the\nUniform-EBA strategy proposed by Bubeck et al. (2011). For the GNA-EBA\nstrategy, we show that the strategy is asymptotically optimal because its\nprobability of misidentification aligns with the lower bounds as the sample\nsize approaches infinity under the small-gap regime. We refer to such optimal\nstrategies as locally asymptotic optimal because their performance aligns with\nthe lower bounds within restricted situations characterized by the small-gap\nregime."}, "http://arxiv.org/abs/2110.00152": {"title": "ebnm: An R Package for Solving the Empirical Bayes Normal Means Problem Using a Variety of Prior Families", "link": "http://arxiv.org/abs/2110.00152", "description": "The empirical Bayes normal means (EBNM) model is important to many areas of\nstatistics, including (but not limited to) multiple testing, wavelet denoising,\nmultiple linear regression, and matrix factorization. There are several\nexisting software packages that can fit EBNM models under different prior\nassumptions and using different algorithms; however, the differences across\ninterfaces complicate direct comparisons. Further, a number of important prior\nassumptions do not yet have implementations. Motivated by these issues, we\ndeveloped the R package ebnm, which provides a unified interface for\nefficiently fitting EBNM models using a variety of prior assumptions, including\nnonparametric approaches. In some cases, we incorporated existing\nimplementations into ebnm; in others, we implemented new fitting procedures\nwith a focus on speed and numerical stability. To demonstrate the capabilities\nof the unified interface, we compare results using different prior assumptions\nin two extended examples: the shrinkage estimation of baseball statistics; and\nthe matrix factorization of genetics data (via the new R package flashier). In\nsummary, ebnm is a convenient and comprehensive package for performing EBNM\nanalyses under a wide range of prior assumptions."}, "http://arxiv.org/abs/2110.02440": {"title": "Inverse Probability Weighting-based Mediation Analysis for Microbiome Data", "link": "http://arxiv.org/abs/2110.02440", "description": "Mediation analysis is an important tool to study causal associations in\nbiomedical and other scientific areas and has recently gained attention in\nmicrobiome studies. Using a microbiome study of acute myeloid leukemia (AML)\npatients, we investigate whether the effect of induction chemotherapy intensity\nlevels on the infection status is mediated by the microbial taxa abundance. The\nunique characteristics of the microbial mediators -- high-dimensionality,\nzero-inflation, and dependence -- call for new methodological developments in\nmediation analysis. The presence of an exposure-induced mediator-outcome\nconfounder, antibiotic use, further requires a delicate treatment in the\nanalysis. To address these unique challenges in our motivating AML microbiome\nstudy, we propose a novel nonparametric identification formula for the\ninterventional indirect effect (IIE), a measure recently developed for studying\nmediation effects. We develop the corresponding estimation algorithm using the\ninverse probability weighting method. We also test the presence of mediation\neffects via constructing the standard normal bootstrap confidence intervals.\nSimulation studies show that the proposed method has good finite-sample\nperformance in terms of the IIE estimation, and type-I error rate and power of\nthe corresponding test. In the AML microbiome study, our findings suggest that\nthe effect of induction chemotherapy intensity levels on infection is mainly\nmediated by patients' gut microbiome."}, "http://arxiv.org/abs/2203.03532": {"title": "E-detectors: a nonparametric framework for sequential change detection", "link": "http://arxiv.org/abs/2203.03532", "description": "Sequential change detection is a classical problem with a variety of\napplications. However, the majority of prior work has been parametric, for\nexample, focusing on exponential families. We develop a fundamentally new and\ngeneral framework for sequential change detection when the pre- and post-change\ndistributions are nonparametrically specified (and thus composite). Our\nprocedures come with clean, nonasymptotic bounds on the average run length\n(frequency of false alarms). In certain nonparametric cases (like sub-Gaussian\nor sub-exponential), we also provide near-optimal bounds on the detection delay\nfollowing a changepoint. The primary technical tool that we introduce is called\nan \\emph{e-detector}, which is composed of sums of e-processes -- a fundamental\ngeneralization of nonnegative supermartingales -- that are started at\nconsecutive times. We first introduce simple Shiryaev-Roberts and CUSUM-style\ne-detectors, and then show how to design their mixtures in order to achieve\nboth statistical and computational efficiency. Our e-detector framework can be\ninstantiated to recover classical likelihood-based procedures for parametric\nproblems, as well as yielding the first change detection method for many\nnonparametric problems. As a running example, we tackle the problem of\ndetecting changes in the mean of a bounded random variable without i.i.d.\nassumptions, with an application to tracking the performance of a basketball\nteam over multiple seasons."}, "http://arxiv.org/abs/2208.02942": {"title": "sparsegl: An R Package for Estimating Sparse Group Lasso", "link": "http://arxiv.org/abs/2208.02942", "description": "The sparse group lasso is a high-dimensional regression technique that is\nuseful for problems whose predictors have a naturally grouped structure and\nwhere sparsity is encouraged at both the group and individual predictor level.\nIn this paper we discuss a new R package for computing such regularized models.\nThe intention is to provide highly optimized solution routines enabling\nanalysis of very large datasets, especially in the context of sparse design\nmatrices."}, "http://arxiv.org/abs/2208.06236": {"title": "Differentially Private Kolmogorov-Smirnov-Type Tests", "link": "http://arxiv.org/abs/2208.06236", "description": "Hypothesis testing is a central problem in statistical analysis, and there is\ncurrently a lack of differentially private tests which are both statistically\nvalid and powerful. In this paper, we develop several new differentially\nprivate (DP) nonparametric hypothesis tests. Our tests are based on\nKolmogorov-Smirnov, Kuiper, Cram\\'er-von Mises, and Wasserstein test\nstatistics, which can all be expressed as a pseudo-metric on empirical\ncumulative distribution functions (ecdfs), and can be used to test hypotheses\non goodness-of-fit, two samples, and paired data. We show that these test\nstatistics have low sensitivity, requiring minimal noise to satisfy DP. In\nparticular, we show that the sensitivity of these test statistics can be\nexpressed in terms of the base sensitivity, which is the pseudo-metric distance\nbetween the ecdfs of adjacent databases and is easily calculated. The sampling\ndistribution of our test statistics are distribution-free under the null\nhypothesis, enabling easy computation of $p$-values by Monte Carlo methods. We\nshow that in several settings, especially with small privacy budgets or\nheavy-tailed data, our new DP tests outperform alternative nonparametric DP\ntests."}, "http://arxiv.org/abs/2208.10027": {"title": "Learning Invariant Representations under General Interventions on the Response", "link": "http://arxiv.org/abs/2208.10027", "description": "It has become increasingly common nowadays to collect observations of feature\nand response pairs from different environments. As a consequence, one has to\napply learned predictors to data with a different distribution due to\ndistribution shifts. One principled approach is to adopt the structural causal\nmodels to describe training and test models, following the invariance principle\nwhich says that the conditional distribution of the response given its\npredictors remains the same across environments. However, this principle might\nbe violated in practical settings when the response is intervened. A natural\nquestion is whether it is still possible to identify other forms of invariance\nto facilitate prediction in unseen environments. To shed light on this\nchallenging scenario, we focus on linear structural causal models (SCMs) and\nintroduce invariant matching property (IMP), an explicit relation to capture\ninterventions through an additional feature, leading to an alternative form of\ninvariance that enables a unified treatment of general interventions on the\nresponse as well as the predictors. We analyze the asymptotic generalization\nerrors of our method under both the discrete and continuous environment\nsettings, where the continuous case is handled by relating it to the\nsemiparametric varying coefficient models. We present algorithms that show\ncompetitive performance compared to existing methods over various experimental\nsettings including a COVID dataset."}, "http://arxiv.org/abs/2208.11756": {"title": "Testing Many Constraints in Possibly Irregular Models Using Incomplete U-Statistics", "link": "http://arxiv.org/abs/2208.11756", "description": "We consider the problem of testing a null hypothesis defined by equality and\ninequality constraints on a statistical parameter. Testing such hypotheses can\nbe challenging because the number of relevant constraints may be on the same\norder or even larger than the number of observed samples. Moreover, standard\ndistributional approximations may be invalid due to irregularities in the null\nhypothesis. We propose a general testing methodology that aims to circumvent\nthese difficulties. The constraints are estimated by incomplete U-statistics,\nand we derive critical values by Gaussian multiplier bootstrap. We show that\nthe bootstrap approximation of incomplete U-statistics is valid for kernels\nthat we call mixed degenerate when the number of combinations used to compute\nthe incomplete U-statistic is of the same order as the sample size. It follows\nthat our test controls type I error even in irregular settings. Furthermore,\nthe bootstrap approximation covers high-dimensional settings making our testing\nstrategy applicable for problems with many constraints. The methodology is\napplicable, in particular, when the constraints to be tested are polynomials in\nU-estimable parameters. As an application, we consider goodness-of-fit tests of\nlatent tree models for multivariate data."}, "http://arxiv.org/abs/2210.05538": {"title": "Estimating optimal treatment regimes in survival contexts using an instrumental variable", "link": "http://arxiv.org/abs/2210.05538", "description": "In survival contexts, substantial literature exists on estimating optimal\ntreatment regimes, where treatments are assigned based on personal\ncharacteristics for the purpose of maximizing the survival probability. These\nmethods assume that a set of covariates is sufficient to deconfound the\ntreatment-outcome relationship. Nevertheless, the assumption can be limited in\nobservational studies or randomized trials in which non-adherence occurs. Thus,\nwe propose a novel approach for estimating the optimal treatment regime when\ncertain confounders are not observable and a binary instrumental variable is\navailable. Specifically, via a binary instrumental variable, we propose two\nsemiparametric estimators for the optimal treatment regime by maximizing\nKaplan-Meier-like estimators within a pre-defined class of regimes, one of\nwhich possesses the desirable property of double robustness. Because the\nKaplan-Meier-like estimators are jagged, we incorporate kernel smoothing\nmethods to enhance their performance. Under appropriate regularity conditions,\nthe asymptotic properties are rigorously established. Furthermore, the finite\nsample performance is assessed through simulation studies. Finally, we\nexemplify our method using data from the National Cancer Institute's (NCI)\nprostate, lung, colorectal, and ovarian cancer screening trial."}, "http://arxiv.org/abs/2302.00878": {"title": "The Contextual Lasso: Sparse Linear Models via Deep Neural Networks", "link": "http://arxiv.org/abs/2302.00878", "description": "Sparse linear models are one of several core tools for interpretable machine\nlearning, a field of emerging importance as predictive models permeate\ndecision-making in many domains. Unfortunately, sparse linear models are far\nless flexible as functions of their input features than black-box models like\ndeep neural networks. With this capability gap in mind, we study a not-uncommon\nsituation where the input features dichotomize into two groups: explanatory\nfeatures, which are candidates for inclusion as variables in an interpretable\nmodel, and contextual features, which select from the candidate variables and\ndetermine their effects. This dichotomy leads us to the contextual lasso, a new\nstatistical estimator that fits a sparse linear model to the explanatory\nfeatures such that the sparsity pattern and coefficients vary as a function of\nthe contextual features. The fitting process learns this function\nnonparametrically via a deep neural network. To attain sparse coefficients, we\ntrain the network with a novel lasso regularizer in the form of a projection\nlayer that maps the network's output onto the space of $\\ell_1$-constrained\nlinear models. An extensive suite of experiments on real and synthetic data\nsuggests that the learned models, which remain highly transparent, can be\nsparser than the regular lasso without sacrificing the predictive power of a\nstandard deep neural network."}, "http://arxiv.org/abs/2302.02560": {"title": "Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US", "link": "http://arxiv.org/abs/2302.02560", "description": "In policy research, one of the most critical analytic tasks is to estimate\nthe causal effect of a policy-relevant shift to the distribution of a\ncontinuous exposure/treatment on an outcome of interest. We call this problem\nshift-response function (SRF) estimation. Existing neural network methods\ninvolving robust causal-effect estimators lack theoretical guarantees and\npractical implementations for SRF estimation. Motivated by a key\npolicy-relevant question in public health, we develop a neural network method\nand its theoretical underpinnings to estimate SRFs with robustness and\nefficiency guarantees. We then apply our method to data consisting of 68\nmillion individuals and 27 million deaths across the U.S. to estimate the\ncausal effect from revising the US National Ambient Air Quality Standards\n(NAAQS) for PM 2.5 from 12 $\\mu g/m^3$ to 9 $\\mu g/m^3$. This change has been\nrecently proposed by the US Environmental Protection Agency (EPA). Our goal is\nto estimate, for the first time, the reduction in deaths that would result from\nthis anticipated revision using causal methods for SRFs. Our proposed method,\ncalled {T}argeted {R}egularization for {E}xposure {S}hifts with Neural\n{Net}works (TRESNET), contributes to the neural network literature for causal\ninference in two ways: first, it proposes a targeted regularization loss with\ntheoretical properties that ensure double robustness and achieves asymptotic\nefficiency specific for SRF estimation; second, it enables loss functions from\nthe exponential family of distributions to accommodate non-continuous outcome\ndistributions (such as hospitalization or mortality counts). We complement our\napplication with benchmark experiments that demonstrate TRESNET's broad\napplicability and competitiveness."}, "http://arxiv.org/abs/2303.03502": {"title": "A Semi-Parametric Model Simultaneously Handling Unmeasured Confounding, Informative Cluster Size, and Truncation by Death with a Data Application in Medicare Claims", "link": "http://arxiv.org/abs/2303.03502", "description": "Nearly 300,000 older adults experience a hip fracture every year, the\nmajority of which occur following a fall. Unfortunately, recovery after\nfall-related trauma such as hip fracture is poor, where older adults diagnosed\nwith Alzheimer's Disease and Related Dementia (ADRD) spend a particularly long\ntime in hospitals or rehabilitation facilities during the post-operative\nrecuperation period. Because older adults value functional recovery and\nspending time at home versus facilities as key outcomes after hospitalization,\nidentifying factors that influence days spent at home after hospitalization is\nimperative. While several individual-level factors have been identified, the\ncharacteristics of the treating hospital have recently been identified as\ncontributors. However, few methodological rigorous approaches are available to\nhelp overcome potential sources of bias such as hospital-level unmeasured\nconfounders, informative hospital size, and loss to follow-up due to death.\nThis article develops a useful tool equipped with unsupervised learning to\nsimultaneously handle statistical complexities that are often encountered in\nhealth services research, especially when using large administrative claims\ndatabases. The proposed estimator has a closed form, thus only requiring light\ncomputation load in a large-scale study. We further develop its asymptotic\nproperties that can be used to make statistical inference in practice.\nExtensive simulation studies demonstrate superiority of the proposed estimator\ncompared to existing estimators."}, "http://arxiv.org/abs/2305.04113": {"title": "Inferring Covariance Structure from Multiple Data Sources via Subspace Factor Analysis", "link": "http://arxiv.org/abs/2305.04113", "description": "Factor analysis provides a canonical framework for imposing lower-dimensional\nstructure such as sparse covariance in high-dimensional data. High-dimensional\ndata on the same set of variables are often collected under different\nconditions, for instance in reproducing studies across research groups. In such\ncases, it is natural to seek to learn the shared versus condition-specific\nstructure. Existing hierarchical extensions of factor analysis have been\nproposed, but face practical issues including identifiability problems. To\naddress these shortcomings, we propose a class of SUbspace Factor Analysis\n(SUFA) models, which characterize variation across groups at the level of a\nlower-dimensional subspace. We prove that the proposed class of SUFA models\nlead to identifiability of the shared versus group-specific components of the\ncovariance, and study their posterior contraction properties. Taking a Bayesian\napproach, these contributions are developed alongside efficient posterior\ncomputation algorithms. Our sampler fully integrates out latent variables, is\neasily parallelizable and has complexity that does not depend on sample size.\nWe illustrate the methods through application to integration of multiple gene\nexpression datasets relevant to immunology."}, "http://arxiv.org/abs/2305.17570": {"title": "Auditing Fairness by Betting", "link": "http://arxiv.org/abs/2305.17570", "description": "We provide practical, efficient, and nonparametric methods for auditing the\nfairness of deployed classification and regression models. Whereas previous\nwork relies on a fixed-sample size, our methods are sequential and allow for\nthe continuous monitoring of incoming data, making them highly amenable to\ntracking the fairness of real-world systems. We also allow the data to be\ncollected by a probabilistic policy as opposed to sampled uniformly from the\npopulation. This enables auditing to be conducted on data gathered for another\npurpose. Moreover, this policy may change over time and different policies may\nbe used on different subpopulations. Finally, our methods can handle\ndistribution shift resulting from either changes to the model or changes in the\nunderlying population. Our approach is based on recent progress in\nanytime-valid inference and game-theoretic statistics-the \"testing by betting\"\nframework in particular. These connections ensure that our methods are\ninterpretable, fast, and easy to implement. We demonstrate the efficacy of our\napproach on three benchmark fairness datasets."}, "http://arxiv.org/abs/2306.02948": {"title": "Learning under random distributional shifts", "link": "http://arxiv.org/abs/2306.02948", "description": "Many existing approaches for generating predictions in settings with\ndistribution shift model distribution shifts as adversarial or low-rank in\nsuitable representations. In various real-world settings, however, we might\nexpect shifts to arise through the superposition of many small and random\nchanges in the population and environment. Thus, we consider a class of random\ndistribution shift models that capture arbitrary changes in the underlying\ncovariate space, and dense, random shocks to the relationship between the\ncovariates and the outcomes. In this setting, we characterize the benefits and\ndrawbacks of several alternative prediction strategies: the standard approach\nthat directly predicts the long-term outcome of interest, the proxy approach\nthat directly predicts a shorter-term proxy outcome, and a hybrid approach that\nutilizes both the long-term policy outcome and (shorter-term) proxy outcome(s).\nWe show that the hybrid approach is robust to the strength of the distribution\nshift and the proxy relationship. We apply this method to datasets in two\nhigh-impact domains: asylum-seeker assignment and early childhood education. In\nboth settings, we find that the proposed approach results in substantially\nlower mean-squared error than current approaches."}, "http://arxiv.org/abs/2306.05751": {"title": "Advancing Counterfactual Inference through Quantile Regression", "link": "http://arxiv.org/abs/2306.05751", "description": "The capacity to address counterfactual \"what if\" inquiries is crucial for\nunderstanding and making use of causal influences. Traditional counterfactual\ninference usually assumes the availability of a structural causal model. Yet,\nin practice, such a causal model is often unknown and may not be identifiable.\nThis paper aims to perform reliable counterfactual inference based on the\n(learned) qualitative causal structure and observational data, without\nnecessitating a given causal model or even the direct estimation of conditional\ndistributions. We re-cast counterfactual reasoning as an extended quantile\nregression problem, implemented with deep neural networks to capture general\ncausal relationships and data distributions. The proposed approach offers\nsuperior statistical efficiency compared to existing ones, and further, it\nenhances the potential for generalizing the estimated counterfactual outcomes\nto previously unseen data, providing an upper bound on the generalization\nerror. Empirical results conducted on multiple datasets offer compelling\nsupport for our theoretical assertions."}, "http://arxiv.org/abs/2306.06155": {"title": "Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks", "link": "http://arxiv.org/abs/2306.06155", "description": "We present a new representation learning framework, Intensity Profile\nProjection, for continuous-time dynamic network data. Given triples $(i,j,t)$,\neach representing a time-stamped ($t$) interaction between two entities\n($i,j$), our procedure returns a continuous-time trajectory for each node,\nrepresenting its behaviour over time. The framework consists of three stages:\nestimating pairwise intensity functions, e.g. via kernel smoothing; learning a\nprojection which minimises a notion of intensity reconstruction error; and\nconstructing evolving node representations via the learned projection. The\ntrajectories satisfy two properties, known as structural and temporal\ncoherence, which we see as fundamental for reliable inference. Moreoever, we\ndevelop estimation theory providing tight control on the error of any estimated\ntrajectory, indicating that the representations could even be used in quite\nnoise-sensitive follow-on analyses. The theory also elucidates the role of\nsmoothing as a bias-variance trade-off, and shows how we can reduce the level\nof smoothing as the signal-to-noise ratio increases on account of the algorithm\n`borrowing strength' across the network."}, "http://arxiv.org/abs/2306.08777": {"title": "MMD-FUSE: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting", "link": "http://arxiv.org/abs/2306.08777", "description": "We propose novel statistics which maximise the power of a two-sample test\nbased on the Maximum Mean Discrepancy (MMD), by adapting over the set of\nkernels used in defining it. For finite sets, this reduces to combining\n(normalised) MMD values under each of these kernels via a weighted soft\nmaximum. Exponential concentration bounds are proved for our proposed\nstatistics under the null and alternative. We further show how these kernels\ncan be chosen in a data-dependent but permutation-independent way, in a\nwell-calibrated test, avoiding data splitting. This technique applies more\nbroadly to general permutation-based MMD testing, and includes the use of deep\nkernels with features learnt using unsupervised models such as auto-encoders.\nWe highlight the applicability of our MMD-FUSE test on both synthetic\nlow-dimensional and real-world high-dimensional data, and compare its\nperformance in terms of power against current state-of-the-art kernel tests."}, "http://arxiv.org/abs/2306.09335": {"title": "Class-Conditional Conformal Prediction with Many Classes", "link": "http://arxiv.org/abs/2306.09335", "description": "Standard conformal prediction methods provide a marginal coverage guarantee,\nwhich means that for a random test point, the conformal prediction set contains\nthe true label with a user-specified probability. In many classification\nproblems, we would like to obtain a stronger guarantee--that for test points of\na specific class, the prediction set contains the true label with the same\nuser-chosen probability. For the latter goal, existing conformal prediction\nmethods do not work well when there is a limited amount of labeled data per\nclass, as is often the case in real applications where the number of classes is\nlarge. We propose a method called clustered conformal prediction that clusters\ntogether classes having \"similar\" conformal scores and performs conformal\nprediction at the cluster level. Based on empirical evaluation across four\nimage data sets with many (up to 1000) classes, we find that clustered\nconformal typically outperforms existing methods in terms of class-conditional\ncoverage and set size metrics."}, "http://arxiv.org/abs/2306.11839": {"title": "Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations", "link": "http://arxiv.org/abs/2306.11839", "description": "Randomized experiments often need to be stopped prematurely due to the\ntreatment having an unintended harmful effect. Existing methods that determine\nwhen to stop an experiment early are typically applied to the data in aggregate\nand do not account for treatment effect heterogeneity. In this paper, we study\nthe early stopping of experiments for harm on heterogeneous populations. We\nfirst establish that current methods often fail to stop experiments when the\ntreatment harms a minority group of participants. We then use causal machine\nlearning to develop CLASH, the first broadly-applicable method for\nheterogeneous early stopping. We demonstrate CLASH's performance on simulated\nand real data and show that it yields effective early stopping for both\nclinical trials and A/B tests."}, "http://arxiv.org/abs/2307.01357": {"title": "Adaptive Principal Component Regression with Applications to Panel Data", "link": "http://arxiv.org/abs/2307.01357", "description": "Principal component regression (PCR) is a popular technique for fixed-design\nerror-in-variables regression, a generalization of the linear regression\nsetting in which the observed covariates are corrupted with random noise. We\nprovide the first time-uniform finite sample guarantees for online\n(regularized) PCR whenever data is collected adaptively. Since the proof\ntechniques for analyzing PCR in the fixed design setting do not readily extend\nto the online setting, our results rely on adapting tools from modern\nmartingale concentration to the error-in-variables setting. As an application\nof our bounds, we provide a framework for experiment design in panel data\nsettings when interventions are assigned adaptively. Our framework may be\nthought of as a generalization of the synthetic control and synthetic\ninterventions frameworks, where data is collected via an adaptive intervention\nassignment policy."}, "http://arxiv.org/abs/2307.02520": {"title": "Conditional independence testing under misspecified inductive biases", "link": "http://arxiv.org/abs/2307.02520", "description": "Conditional independence (CI) testing is a fundamental and challenging task\nin modern statistics and machine learning. Many modern methods for CI testing\nrely on powerful supervised learning methods to learn regression functions or\nBayes predictors as an intermediate step; we refer to this class of tests as\nregression-based tests. Although these methods are guaranteed to control Type-I\nerror when the supervised learning methods accurately estimate the regression\nfunctions or Bayes predictors of interest, their behavior is less understood\nwhen they fail due to misspecified inductive biases; in other words, when the\nemployed models are not flexible enough or when the training algorithm does not\ninduce the desired predictors. Then, we study the performance of\nregression-based CI tests under misspecified inductive biases. Namely, we\npropose new approximations or upper bounds for the testing errors of three\nregression-based tests that depend on misspecification errors. Moreover, we\nintroduce the Rao-Blackwellized Predictor Test (RBPT), a regression-based CI\ntest robust against misspecified inductive biases. Finally, we conduct\nexperiments with artificial and real data, showcasing the usefulness of our\ntheory and methods."}, "http://arxiv.org/abs/2308.03801": {"title": "On problematic practice of using normalization in Self-modeling/Multivariate Curve Resolution (S/MCR)", "link": "http://arxiv.org/abs/2308.03801", "description": "The paper is briefly dealing with greater or lesser misused normalization in\nself-modeling/multivariate curve resolution (S/MCR) practice. The importance of\nthe correct use of the ode solvers and apt kinetic illustrations are\nelucidated. The new terms, external and internal normalizations are defined and\ninterpreted. The problem of reducibility of a matrix is touched. Improper\ngeneralization/development of normalization-based methods are cited as\nexamples. The position of the extreme values of the signal contribution\nfunction is clarified. An Executable Notebook with Matlab Live Editor was\ncreated for algorithmic explanations and depictions."}, "http://arxiv.org/abs/2308.05373": {"title": "Conditional Independence Testing for Discrete Distributions: Beyond $\\chi^2$- and $G$-tests", "link": "http://arxiv.org/abs/2308.05373", "description": "This paper is concerned with the problem of conditional independence testing\nfor discrete data. In recent years, researchers have shed new light on this\nfundamental problem, emphasizing finite-sample optimality. The non-asymptotic\nviewpoint adapted in these works has led to novel conditional independence\ntests that enjoy certain optimality under various regimes. Despite their\nattractive theoretical properties, the considered tests are not necessarily\npractical, relying on a Poissonization trick and unspecified constants in their\ncritical values. In this work, we attempt to bridge the gap between theory and\npractice by reproving optimality without Poissonization and calibrating tests\nusing Monte Carlo permutations. Along the way, we also prove that classical\nasymptotic $\\chi^2$- and $G$-tests are notably sub-optimal in a\nhigh-dimensional regime, which justifies the demand for new tools. Our\ntheoretical results are complemented by experiments on both simulated and\nreal-world datasets. Accompanying this paper is an R package UCI that\nimplements the proposed tests."}, "http://arxiv.org/abs/2309.03875": {"title": "Network Sampling Methods for Estimating Social Networks, Population Percentages, and Totals of People Experiencing Unsheltered Homelessness", "link": "http://arxiv.org/abs/2309.03875", "description": "In this article, we propose using network-based sampling strategies to\nestimate the number of unsheltered people experiencing homelessness within a\ngiven administrative service unit, known as a Continuum of Care. We demonstrate\nthe effectiveness of network sampling methods to solve this problem. Here, we\nfocus on Respondent Driven Sampling (RDS), which has been shown to provide\nunbiased or low-biased estimates of totals and proportions for hard-to-reach\npopulations in contexts where a sampling frame (e.g., housing addresses) is not\navailable. To make the RDS estimator work for estimating the total number of\npeople living unsheltered, we introduce a new method that leverages\nadministrative data from the HUD-mandated Homeless Management Information\nSystem (HMIS). The HMIS provides high-quality counts and demographics for\npeople experiencing homelessness who sleep in emergency shelters. We then\ndemonstrate this method using network data collected in Nashville, TN, combined\nwith simulation methods to illustrate the efficacy of this approach and\nintroduce a method for performing a power analysis to find the optimal sample\nsize in this setting. We conclude with the RDS unsheltered PIT count conducted\nby King County Regional Homelessness Authority in 2022 (data publicly available\non the HUD website) and perform a comparative analysis between the 2022 RDS\nestimate of unsheltered people experiencing homelessness and an ARIMA forecast\nof the visual unsheltered PIT count. Finally, we discuss how this method works\nfor estimating the unsheltered population of people experiencing homelessness\nand future areas of research."}, "http://arxiv.org/abs/2310.19973": {"title": "Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy", "link": "http://arxiv.org/abs/2310.19973", "description": "Differentially private (DP) machine learning algorithms incur many sources of\nrandomness, such as random initialization, random batch subsampling, and\nshuffling. However, such randomness is difficult to take into account when\nproving differential privacy bounds because it induces mixture distributions\nfor the algorithm's output that are difficult to analyze. This paper focuses on\nimproving privacy bounds for shuffling models and one-iteration differentially\nprivate gradient descent (DP-GD) with random initializations using $f$-DP. We\nderive a closed-form expression of the trade-off function for shuffling models\nthat outperforms the most up-to-date results based on $(\\epsilon,\\delta)$-DP.\nMoreover, we investigate the effects of random initialization on the privacy of\none-iteration DP-GD. Our numerical computations of the trade-off function\nindicate that random initialization can enhance the privacy of DP-GD. Our\nanalysis of $f$-DP guarantees for these mixture mechanisms relies on an\ninequality for trade-off functions introduced in this paper. This inequality\nimplies the joint convexity of $F$-divergences. Finally, we study an $f$-DP\nanalog of the advanced joint convexity of the hockey-stick divergence related\nto $(\\epsilon,\\delta)$-DP and apply it to analyze the privacy of mixture\nmechanisms."}, "http://arxiv.org/abs/2310.19985": {"title": "Modeling random directions of changes in simplex-valued data", "link": "http://arxiv.org/abs/2310.19985", "description": "We propose models and algorithms for learning about random directions in\nsimplex-valued data. The models are applied to the study of income level\nproportions and their changes over time in a geostatistical area. There are\nseveral notable challenges in the analysis of simplex-valued data: the\nmeasurements must respect the simplex constraint and the changes exhibit\nspatiotemporal smoothness and may be heterogeneous. To that end, we propose\nBayesian models that draw from and expand upon building blocks in circular and\nspatial statistics by exploiting a suitable transformation for the\nsimplex-valued data. Our models also account for spatial correlation across\nlocations in the simplex and the heterogeneous patterns via mixture modeling.\nWe describe some properties of the models and model fitting via MCMC\ntechniques. Our models and methods are applied to an analysis of movements and\ntrends of income categories using the Home Mortgage Disclosure Act data."}, "http://arxiv.org/abs/2310.19988": {"title": "Counterfactual fairness for small subgroups", "link": "http://arxiv.org/abs/2310.19988", "description": "While methods for measuring and correcting differential performance in risk\nprediction models have proliferated in recent years, most existing techniques\ncan only be used to assess fairness across relatively large subgroups. The\npurpose of algorithmic fairness efforts is often to redress discrimination\nagainst groups that are both marginalized and small, so this sample size\nlimitation often prevents existing techniques from accomplishing their main\naim. We take a three-pronged approach to address the problem of quantifying\nfairness with small subgroups. First, we propose new estimands built on the\n\"counterfactual fairness\" framework that leverage information across groups.\nSecond, we estimate these quantities using a larger volume of data than\nexisting techniques. Finally, we propose a novel data borrowing approach to\nincorporate \"external data\" that lacks outcomes and predictions but contains\ncovariate and group membership information. This less stringent requirement on\nthe external data allows for more possibilities for external data sources. We\ndemonstrate practical application of our estimators to a risk prediction model\nused by a major Midwestern health system during the COVID-19 pandemic."}, "http://arxiv.org/abs/2310.20058": {"title": "New Asymptotic Limit Theory and Inference for Monotone Regression", "link": "http://arxiv.org/abs/2310.20058", "description": "Nonparametric regression problems with qualitative constraints such as\nmonotonicity or convexity are ubiquitous in applications. For example, in\npredicting the yield of a factory in terms of the number of labor hours, the\nmonotonicity of the conditional mean function is a natural constraint. One can\nestimate a monotone conditional mean function using nonparametric least squares\nestimation, which involves no tuning parameters. Several interesting properties\nof the isotonic LSE are known including its rate of convergence, adaptivity\nproperties, and pointwise asymptotic distribution. However, we believe that the\nfull richness of the asymptotic limit theory has not been explored in the\nliterature which we do in this paper. Moreover, the inference problem is not\nfully settled. In this paper, we present some new results for monotone\nregression including an extension of existing results to triangular arrays, and\nprovide asymptotically valid confidence intervals that are uniformly valid over\na large class of distributions."}, "http://arxiv.org/abs/2310.20075": {"title": "Meek Separators and Their Applications in Targeted Causal Discovery", "link": "http://arxiv.org/abs/2310.20075", "description": "Learning causal structures from interventional data is a fundamental problem\nwith broad applications across various fields. While many previous works have\nfocused on recovering the entire causal graph, in practice, there are scenarios\nwhere learning only part of the causal graph suffices. This is called\n$targeted$ causal discovery. In our work, we focus on two such well-motivated\nproblems: subset search and causal matching. We aim to minimize the number of\ninterventions in both cases.\n\nTowards this, we introduce the $Meek~separator$, which is a subset of\nvertices that, when intervened, decomposes the remaining unoriented edges into\nsmaller connected components. We then present an efficient algorithm to find\nMeek separators that are of small sizes. Such a procedure is helpful in\ndesigning various divide-and-conquer-based approaches. In particular, we\npropose two randomized algorithms that achieve logarithmic approximation for\nsubset search and causal matching, respectively. Our results provide the first\nknown average-case provable guarantees for both problems. We believe that this\nopens up possibilities to design near-optimal methods for many other targeted\ncausal structure learning problems arising from various applications."}, "http://arxiv.org/abs/2310.20087": {"title": "PAM-HC: A Bayesian Nonparametric Construction of Hybrid Control for Randomized Clinical Trials Using External Data", "link": "http://arxiv.org/abs/2310.20087", "description": "It is highly desirable to borrow information from external data to augment a\ncontrol arm in a randomized clinical trial, especially in settings where the\nsample size for the control arm is limited. However, a main challenge in\nborrowing information from external data is to accommodate potential\nheterogeneous subpopulations across the external and trial data. We apply a\nBayesian nonparametric model called Plaid Atoms Model (PAM) to identify\noverlapping and unique subpopulations across datasets, with which we restrict\nthe information borrowing to the common subpopulations. This forms a hybrid\ncontrol (HC) that leads to more precise estimation of treatment effects\nSimulation studies demonstrate the robustness of the new method, and an\napplication to an Atopic Dermatitis dataset shows improved treatment effect\nestimation."}, "http://arxiv.org/abs/2310.20088": {"title": "Optimal transport representations and functional principal components for distribution-valued processes", "link": "http://arxiv.org/abs/2310.20088", "description": "We develop statistical models for samples of distribution-valued stochastic\nprocesses through time-varying optimal transport process representations under\nthe Wasserstein metric when the values of the process are univariate\ndistributions. While functional data analysis provides a toolbox for the\nanalysis of samples of real- or vector-valued processes, there is at present no\ncoherent statistical methodology available for samples of distribution-valued\nprocesses, which are increasingly encountered in data analysis. To address the\nneed for such methodology, we introduce a transport model for samples of\ndistribution-valued stochastic processes that implements an intrinsic approach\nwhereby distributions are represented by optimal transports. Substituting\ntransports for distributions addresses the challenge of centering\ndistribution-valued processes and leads to a useful and interpretable\nrepresentation of each realized process by an overall transport and a\nreal-valued trajectory, utilizing a scalar multiplication operation for\ntransports. This representation facilitates a connection to Gaussian processes\nthat proves useful, especially for the case where the distribution-valued\nprocesses are only observed on a sparse grid of time points. We study the\nconvergence of the key components of the proposed representation to their\npopulation targets and demonstrate the practical utility of the proposed\napproach through simulations and application examples."}, "http://arxiv.org/abs/2310.20182": {"title": "Explicit Form of the Asymptotic Variance Estimator for IPW-type Estimators of Certain Estimands", "link": "http://arxiv.org/abs/2310.20182", "description": "Confidence intervals (CI) for the IPW estimators of the ATT and ATO might not\nalways yield conservative CIs when using the 'robust sandwich variance'\nestimator. In this manuscript, we identify scenarios where this variance\nestimator can be employed to derive conservative CIs. Specifically, for the\nATT, a conservative CI can be derived when there's a homogeneous treatment\neffect or the interaction effect surpasses the effect from the covariates\nalone. For the ATO, conservative CIs can be derived under certain conditions,\nsuch as when there are homogeneous treatment effects, when there exists\nsignificant treatment-confounder interactions, or when there's a large number\nof members in the control groups."}, "http://arxiv.org/abs/2310.20294": {"title": "Robust nonparametric regression based on deep ReLU neural networks", "link": "http://arxiv.org/abs/2310.20294", "description": "In this paper, we consider robust nonparametric regression using deep neural\nnetworks with ReLU activation function. While several existing theoretically\njustified methods are geared towards robustness against identical heavy-tailed\nnoise distributions, the rise of adversarial attacks has emphasized the\nimportance of safeguarding estimation procedures against systematic\ncontamination. We approach this statistical issue by shifting our focus towards\nestimating conditional distributions. To address it robustly, we introduce a\nnovel estimation procedure based on $\\ell$-estimation. Under a mild model\nassumption, we establish general non-asymptotic risk bounds for the resulting\nestimators, showcasing their robustness against contamination, outliers, and\nmodel misspecification. We then delve into the application of our approach\nusing deep ReLU neural networks. When the model is well-specified and the\nregression function belongs to an $\\alpha$-H\\\"older class, employing\n$\\ell$-type estimation on suitable networks enables the resulting estimators to\nachieve the minimax optimal rate of convergence. Additionally, we demonstrate\nthat deep $\\ell$-type estimators can circumvent the curse of dimensionality by\nassuming the regression function closely resembles the composition of several\nH\\\"older functions. To attain this, new deep fully-connected ReLU neural\nnetworks have been designed to approximate this composition class. This\napproximation result can be of independent interest."}, "http://arxiv.org/abs/2310.20376": {"title": "Mixture modeling via vectors of normalized independent finite point processes", "link": "http://arxiv.org/abs/2310.20376", "description": "Statistical modeling in presence of hierarchical data is a crucial task in\nBayesian statistics. The Hierarchical Dirichlet Process (HDP) represents the\nutmost tool to handle data organized in groups through mixture modeling.\nAlthough the HDP is mathematically tractable, its computational cost is\ntypically demanding, and its analytical complexity represents a barrier for\npractitioners. The present paper conceives a mixture model based on a novel\nfamily of Bayesian priors designed for multilevel data and obtained by\nnormalizing a finite point process. A full distribution theory for this new\nfamily and the induced clustering is developed, including tractable expressions\nfor marginal, posterior and predictive distributions. Efficient marginal and\nconditional Gibbs samplers are designed for providing posterior inference. The\nproposed mixture model overcomes the HDP in terms of analytical feasibility,\nclustering discovery, and computational time. The motivating application comes\nfrom the analysis of shot put data, which contains performance measurements of\nathletes across different seasons. In this setting, the proposed model is\nexploited to induce clustering of the observations across seasons and athletes.\nBy linking clusters across seasons, similarities and differences in athlete's\nperformances are identified."}, "http://arxiv.org/abs/2310.20409": {"title": "Detection of nonlinearity, discontinuity and interactions in generalized regression models", "link": "http://arxiv.org/abs/2310.20409", "description": "In generalized regression models the effect of continuous covariates is\ncommonly assumed to be linear. This assumption, however, may be too restrictive\nin applications and may lead to biased effect estimates and decreased\npredictive ability. While a multitude of alternatives for the flexible modeling\nof continuous covariates have been proposed, methods that provide guidance for\nchoosing a suitable functional form are still limited. To address this issue,\nwe propose a detection algorithm that evaluates several approaches for modeling\ncontinuous covariates and guides practitioners to choose the most appropriate\nalternative. The algorithm utilizes a unified framework for tree-structured\nmodeling which makes the results easily interpretable. We assessed the\nperformance of the algorithm by conducting a simulation study. To illustrate\nthe proposed algorithm, we analyzed data of patients suffering from chronic\nkidney disease."}, "http://arxiv.org/abs/2310.20450": {"title": "Safe Testing for Large-Scale Experimentation Platforms", "link": "http://arxiv.org/abs/2310.20450", "description": "In the past two decades, AB testing has proliferated to optimise products in\ndigital domains. Traditional AB tests use fixed-horizon testing, determining\nthe sample size of the experiment and continuing until the experiment has\nconcluded. However, due to the feedback provided by modern data infrastructure,\nexperimenters may take incorrect decisions based on preliminary results of the\ntest. For this reason, anytime-valid inference (AVI) is seeing increased\nadoption as the modern experimenters method for rapid decision making in the\nworld of data streaming.\n\nThis work focuses on Safe Testing, a novel framework for experimentation that\nenables continuous analysis without elevating the risk of incorrect\nconclusions. There exist safe testing equivalents of many common statistical\ntests, including the z-test, the t-test, and the proportion test. We compare\nthe efficacy of safe tests against classical tests and another method for AVI,\nthe mixture sequential probability ratio test (mSPRT). Comparisons are\nconducted first on simulation and then by real-world data from a large\ntechnology company, Vinted, a large European online marketplace for second-hand\nclothing. Our findings indicate that safe tests require fewer samples to detect\nsignificant effects, encouraging its potential for broader adoption."}, "http://arxiv.org/abs/2310.20460": {"title": "Aggregating Dependent Signals with Heavy-Tailed Combination Tests", "link": "http://arxiv.org/abs/2310.20460", "description": "Combining dependent p-values to evaluate the global null hypothesis presents\na longstanding challenge in statistical inference, particularly when\naggregating results from diverse methods to boost signal detection. P-value\ncombination tests using heavy-tailed distribution based transformations, such\nas the Cauchy combination test and the harmonic mean p-value, have recently\ngarnered significant interest for their potential to efficiently handle\narbitrary p-value dependencies. Despite their growing popularity in practical\napplications, there is a gap in comprehensive theoretical and empirical\nevaluations of these methods. This paper conducts an extensive investigation,\nrevealing that, theoretically, while these combination tests are asymptotically\nvalid for pairwise quasi-asymptotically independent test statistics, such as\nbivariate normal variables, they are also asymptotically equivalent to the\nBonferroni test under the same conditions. However, extensive simulations\nunveil their practical utility, especially in scenarios where stringent type-I\nerror control is not necessary and signals are dense. Both the heaviness of the\ndistribution and its support substantially impact the tests' non-asymptotic\nvalidity and power, and we recommend using a truncated Cauchy distribution in\npractice. Moreover, we show that under the violation of quasi-asymptotic\nindependence among test statistics, these tests remain valid and, in fact, can\nbe considerably less conservative than the Bonferroni test. We also present two\ncase studies in genetics and genomics, showcasing the potential of the\ncombination tests to significantly enhance statistical power while effectively\ncontrolling type-I errors."}, "http://arxiv.org/abs/2310.20483": {"title": "Measuring multidimensional heterogeneity in emergent social phenomena", "link": "http://arxiv.org/abs/2310.20483", "description": "Measuring inequalities in a multidimensional framework is a challenging\nproblem which is common to most field of science and engineering. Nevertheless,\ndespite the enormous amount of researches illustrating the fields of\napplication of inequality indices, and of the Gini index in particular, very\nfew consider the case of a multidimensional variable. In this paper, we\nconsider in some details a new inequality index, based on the Fourier\ntransform, that can be fruitfully applied to measure the degree of\ninhomogeneity of multivariate probability distributions. This index exhibits a\nnumber of interesting properties that make it very promising in quantifying the\ndegree of inequality in data sets of complex and multifaceted social phenomena."}, "http://arxiv.org/abs/2310.20537": {"title": "Directed Cyclic Graph for Causal Discovery from Multivariate Functional Data", "link": "http://arxiv.org/abs/2310.20537", "description": "Discovering causal relationship using multivariate functional data has\nreceived a significant amount of attention very recently. In this article, we\nintroduce a functional linear structural equation model for causal structure\nlearning when the underlying graph involving the multivariate functions may\nhave cycles. To enhance interpretability, our model involves a low-dimensional\ncausal embedded space such that all the relevant causal information in the\nmultivariate functional data is preserved in this lower-dimensional subspace.\nWe prove that the proposed model is causally identifiable under standard\nassumptions that are often made in the causal discovery literature. To carry\nout inference of our model, we develop a fully Bayesian framework with suitable\nprior specifications and uncertainty quantification through posterior\nsummaries. We illustrate the superior performance of our method over existing\nmethods in terms of causal graph estimation through extensive simulation\nstudies. We also demonstrate the proposed method using a brain EEG dataset."}, "http://arxiv.org/abs/2310.20697": {"title": "Text-Transport: Toward Learning Causal Effects of Natural Language", "link": "http://arxiv.org/abs/2310.20697", "description": "As language technologies gain prominence in real-world settings, it is\nimportant to understand how changes to language affect reader perceptions. This\ncan be formalized as the causal effect of varying a linguistic attribute (e.g.,\nsentiment) on a reader's response to the text. In this paper, we introduce\nText-Transport, a method for estimation of causal effects from natural language\nunder any text distribution. Current approaches for valid causal effect\nestimation require strong assumptions about the data, meaning the data from\nwhich one can estimate valid causal effects often is not representative of the\nactual target domain of interest. To address this issue, we leverage the notion\nof distribution shift to describe an estimator that transports causal effects\nbetween domains, bypassing the need for strong assumptions in the target\ndomain. We derive statistical guarantees on the uncertainty of this estimator,\nand we report empirical results and analyses that support the validity of\nText-Transport across data settings. Finally, we use Text-Transport to study a\nrealistic setting--hate speech on social media--in which causal effects do\nshift significantly between text domains, demonstrating the necessity of\ntransport when conducting causal inference on natural language."}, "http://arxiv.org/abs/2203.15009": {"title": "DAMNETS: A Deep Autoregressive Model for Generating Markovian Network Time Series", "link": "http://arxiv.org/abs/2203.15009", "description": "Generative models for network time series (also known as dynamic graphs) have\ntremendous potential in fields such as epidemiology, biology and economics,\nwhere complex graph-based dynamics are core objects of study. Designing\nflexible and scalable generative models is a very challenging task due to the\nhigh dimensionality of the data, as well as the need to represent temporal\ndependencies and marginal network structure. Here we introduce DAMNETS, a\nscalable deep generative model for network time series. DAMNETS outperforms\ncompeting methods on all of our measures of sample quality, over both real and\nsynthetic data sets."}, "http://arxiv.org/abs/2207.04481": {"title": "Detecting Grouped Local Average Treatment Effects and Selecting True Instruments", "link": "http://arxiv.org/abs/2207.04481", "description": "Under an endogenous binary treatment with heterogeneous effects and multiple\ninstruments, we propose a two-step procedure for identifying complier groups\nwith identical local average treatment effects (LATE) despite relying on\ndistinct instruments, even if several instruments violate the identifying\nassumptions. We use the fact that the LATE is homogeneous for instruments which\n(i) satisfy the LATE assumptions (instrument validity and treatment\nmonotonicity in the instrument) and (ii) generate identical complier groups in\nterms of treatment propensities given the respective instruments. We propose a\ntwo-step procedure, where we first cluster the propensity scores in the first\nstep and find groups of IVs with the same reduced form parameters in the second\nstep. Under the plurality assumption that within each set of instruments with\nidentical treatment propensities, instruments truly satisfying the LATE\nassumptions are the largest group, our procedure permits identifying these true\ninstruments in a data driven way. We show that our procedure is consistent and\nprovides consistent and asymptotically normal estimators of underlying LATEs.\nWe also provide a simulation study investigating the finite sample properties\nof our approach and an empirical application investigating the effect of\nincarceration on recidivism in the US with judge assignments serving as\ninstruments."}, "http://arxiv.org/abs/2212.01792": {"title": "Classification by sparse additive models", "link": "http://arxiv.org/abs/2212.01792", "description": "We consider (nonparametric) sparse additive models (SpAM) for classification.\nThe design of a SpAM classifier is based on minimizing the logistic loss with a\nsparse group Lasso and more general sparse group Slope-type penalties on the\ncoefficients of univariate components' expansions in orthonormal series (e.g.,\nFourier or wavelets). The resulting classifiers are inherently adaptive to the\nunknown sparsity and smoothness. We show that under certain sparse group\nrestricted eigenvalue condition the sparse group Lasso classifier is\nnearly-minimax (up to log-factors) within the entire range of analytic, Sobolev\nand Besov classes while the sparse group Slope classifier achieves the exact\nminimax order (without the extra log-factors) for sparse and moderately dense\nsetups. The performance of the proposed classifier is illustrated on the\nreal-data example."}, "http://arxiv.org/abs/2302.11656": {"title": "Confounder-Dependent Bayesian Mixture Model: Characterizing Heterogeneity of Causal Effects in Air Pollution Epidemiology", "link": "http://arxiv.org/abs/2302.11656", "description": "Several epidemiological studies have provided evidence that long-term\nexposure to fine particulate matter (PM2.5) increases mortality risk.\nFurthermore, some population characteristics (e.g., age, race, and\nsocioeconomic status) might play a crucial role in understanding vulnerability\nto air pollution. To inform policy, it is necessary to identify groups of the\npopulation that are more or less vulnerable to air pollution. In causal\ninference literature, the Group Average Treatment Effect (GATE) is a\ndistinctive facet of the conditional average treatment effect. This widely\nemployed metric serves to characterize the heterogeneity of a treatment effect\nbased on some population characteristics. In this work, we introduce a novel\nConfounder-Dependent Bayesian Mixture Model (CDBMM) to characterize causal\neffect heterogeneity. More specifically, our method leverages the flexibility\nof the dependent Dirichlet process to model the distribution of the potential\noutcomes conditionally to the covariates and the treatment levels, thus\nenabling us to: (i) identify heterogeneous and mutually exclusive population\ngroups defined by similar GATEs in a data-driven way, and (ii) estimate and\ncharacterize the causal effects within each of the identified groups. Through\nsimulations, we demonstrate the effectiveness of our method in uncovering key\ninsights about treatment effects heterogeneity. We apply our method to claims\ndata from Medicare enrollees in Texas. We found six mutually exclusive groups\nwhere the causal effects of PM2.5 on mortality are heterogeneous."}, "http://arxiv.org/abs/2305.03149": {"title": "A Spectral Method for Identifiable Grade of Membership Analysis with Binary Responses", "link": "http://arxiv.org/abs/2305.03149", "description": "Grade of Membership (GoM) models are popular individual-level mixture models\nfor multivariate categorical data. GoM allows each subject to have mixed\nmemberships in multiple extreme latent profiles. Therefore GoM models have a\nricher modeling capacity than latent class models that restrict each subject to\nbelong to a single profile. The flexibility of GoM comes at the cost of more\nchallenging identifiability and estimation problems. In this work, we propose a\nsingular value decomposition (SVD) based spectral approach to GoM analysis with\nmultivariate binary responses. Our approach hinges on the observation that the\nexpectation of the data matrix has a low-rank decomposition under a GoM model.\nFor identifiability, we develop sufficient and almost necessary conditions for\na notion of expectation identifiability. For estimation, we extract only a few\nleading singular vectors of the observed data matrix, and exploit the simplex\ngeometry of these vectors to estimate the mixed membership scores and other\nparameters. We also establish the consistency of our estimator in the\ndouble-asymptotic regime where both the number of subjects and the number of\nitems grow to infinity. Our spectral method has a huge computational advantage\nover Bayesian or likelihood-based methods and is scalable to large-scale and\nhigh-dimensional data. Extensive simulation studies demonstrate the superior\nefficiency and accuracy of our method. We also illustrate our method by\napplying it to a personality test dataset."}, "http://arxiv.org/abs/2305.05276": {"title": "Causal Discovery from Subsampled Time Series with Proxy Variables", "link": "http://arxiv.org/abs/2305.05276", "description": "Inferring causal structures from time series data is the central interest of\nmany scientific inquiries. A major barrier to such inference is the problem of\nsubsampling, i.e., the frequency of measurement is much lower than that of\ncausal influence. To overcome this problem, numerous methods have been\nproposed, yet either was limited to the linear case or failed to achieve\nidentifiability. In this paper, we propose a constraint-based algorithm that\ncan identify the entire causal structure from subsampled time series, without\nany parametric constraint. Our observation is that the challenge of subsampling\narises mainly from hidden variables at the unobserved time steps. Meanwhile,\nevery hidden variable has an observed proxy, which is essentially itself at\nsome observable time in the future, benefiting from the temporal structure.\nBased on these, we can leverage the proxies to remove the bias induced by the\nhidden variables and hence achieve identifiability. Following this intuition,\nwe propose a proxy-based causal discovery algorithm. Our algorithm is\nnonparametric and can achieve full causal identification. Theoretical\nadvantages are reflected in synthetic and real-world experiments."}, "http://arxiv.org/abs/2305.08942": {"title": "Probabilistic forecast of nonlinear dynamical systems with uncertainty quantification", "link": "http://arxiv.org/abs/2305.08942", "description": "Data-driven modeling is useful for reconstructing nonlinear dynamical systems\nwhen the underlying process is unknown or too expensive to compute. Having\nreliable uncertainty assessment of the forecast enables tools to be deployed to\npredict new scenarios unobserved before. In this work, we first extend parallel\npartial Gaussian processes for predicting the vector-valued transition function\nthat links the observations between the current and next time points, and\nquantify the uncertainty of predictions by posterior sampling. Second, we show\nthe equivalence between the dynamic mode decomposition and the maximum\nlikelihood estimator of the linear mapping matrix in the linear state space\nmodel. The connection provides a {probabilistic generative} model of dynamic\nmode decomposition and thus, uncertainty of predictions can be obtained.\nFurthermore, we draw close connections between different data-driven models for\napproximating nonlinear dynamics, through a unified view of generative models.\nWe study two numerical examples, where the inputs of the dynamics are assumed\nto be known in the first example and the inputs are unknown in the second\nexample. The examples indicate that uncertainty of forecast can be properly\nquantified, whereas model or input misspecification can degrade the accuracy of\nuncertainty quantification."}, "http://arxiv.org/abs/2305.16795": {"title": "On Consistent Bayesian Inference from Synthetic Data", "link": "http://arxiv.org/abs/2305.16795", "description": "Generating synthetic data, with or without differential privacy, has\nattracted significant attention as a potential solution to the dilemma between\nmaking data easily available, and the privacy of data subjects. Several works\nhave shown that consistency of downstream analyses from synthetic data,\nincluding accurate uncertainty estimation, requires accounting for the\nsynthetic data generation. There are very few methods of doing so, most of them\nfor frequentist analysis. In this paper, we study how to perform consistent\nBayesian inference from synthetic data. We prove that mixing posterior samples\nobtained separately from multiple large synthetic data sets converges to the\nposterior of the downstream analysis under standard regularity conditions when\nthe analyst's model is compatible with the data provider's model. We also\npresent several examples showing how the theory works in practice, and showing\nhow Bayesian inference can fail when the compatibility assumption is not met,\nor the synthetic data set is not significantly larger than the original."}, "http://arxiv.org/abs/2306.04746": {"title": "Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models", "link": "http://arxiv.org/abs/2306.04746", "description": "In computational social science (CSS), researchers analyze documents to\nexplain social and political phenomena. In most scenarios, CSS researchers\nfirst obtain labels for documents and then explain labels using interpretable\nregression analyses in the second step. One increasingly common way to annotate\ndocuments cheaply at scale is through large language models (LLMs). However,\nlike other scalable ways of producing annotations, such surrogate labels are\noften imperfect and biased. We present a new algorithm for using imperfect\nannotation surrogates for downstream statistical analyses while guaranteeing\nstatistical properties -- like asymptotic unbiasedness and proper uncertainty\nquantification -- which are fundamental to CSS research. We show that direct\nuse of surrogate labels in downstream statistical analyses leads to substantial\nbias and invalid confidence intervals, even with high surrogate accuracy of\n80--90\\%. To address this, we build on debiased machine learning to propose the\ndesign-based supervised learning (DSL) estimator. DSL employs a doubly-robust\nprocedure to combine surrogate labels with a smaller number of high-quality,\ngold-standard labels. Our approach guarantees valid inference for downstream\nstatistical analyses, even when surrogates are arbitrarily biased and without\nrequiring stringent assumptions, by controlling the probability of sampling\ndocuments for gold-standard labeling. Both our theoretical analysis and\nexperimental results show that DSL provides valid statistical inference while\nachieving root mean squared errors comparable to existing alternatives that\nfocus only on prediction without inferential guarantees."}, "http://arxiv.org/abs/2308.13713": {"title": "Causally Sound Priors for Binary Experiments", "link": "http://arxiv.org/abs/2308.13713", "description": "We introduce the BREASE framework for the Bayesian analysis of randomized\ncontrolled trials with a binary treatment and a binary outcome. Approaching the\nproblem from a causal inference perspective, we propose parameterizing the\nlikelihood in terms of the baseline risk, efficacy, and adverse side effects of\nthe treatment, along with a flexible, yet intuitive and tractable jointly\nindependent beta prior distribution on these parameters, which we show to be a\ngeneralization of the Dirichlet prior for the joint distribution of potential\noutcomes. Our approach has a number of desirable characteristics when compared\nto current mainstream alternatives: (i) it naturally induces prior dependence\nbetween expected outcomes in the treatment and control groups; (ii) as the\nbaseline risk, efficacy and risk of adverse side effects are quantities\ncommonly present in the clinicians' vocabulary, the hyperparameters of the\nprior are directly interpretable, thus facilitating the elicitation of prior\nknowledge and sensitivity analysis; and (iii) we provide analytical formulae\nfor the marginal likelihood, Bayes factor, and other posterior quantities, as\nwell as exact posterior sampling via simulation, in cases where traditional\nMCMC fails. Empirical examples demonstrate the utility of our methods for\nestimation, hypothesis testing, and sensitivity analysis of treatment effects."}, "http://arxiv.org/abs/2309.01608": {"title": "Supervised dimensionality reduction for multiple imputation by chained equations", "link": "http://arxiv.org/abs/2309.01608", "description": "Multivariate imputation by chained equations (MICE) is one of the most\npopular approaches to address missing values in a data set. This approach\nrequires specifying a univariate imputation model for every variable under\nimputation. The specification of which predictors should be included in these\nunivariate imputation models can be a daunting task. Principal component\nanalysis (PCA) can simplify this process by replacing all of the potential\nimputation model predictors with a few components summarizing their variance.\nIn this article, we extend the use of PCA with MICE to include a supervised\naspect whereby information from the variables under imputation is incorporated\ninto the principal component estimation. We conducted an extensive simulation\nstudy to assess the statistical properties of MICE with different versions of\nsupervised dimensionality reduction and we compared them with the use of\nclassical unsupervised PCA as a simpler dimensionality reduction technique."}, "http://arxiv.org/abs/2311.00118": {"title": "Extracting the Multiscale Causal Backbone of Brain Dynamics", "link": "http://arxiv.org/abs/2311.00118", "description": "The bulk of the research effort on brain connectivity revolves around\nstatistical associations among brain regions, which do not directly relate to\nthe causal mechanisms governing brain dynamics. Here we propose the multiscale\ncausal backbone (MCB) of brain dynamics shared by a set of individuals across\nmultiple temporal scales, and devise a principled methodology to extract it.\n\nOur approach leverages recent advances in multiscale causal structure\nlearning and optimizes the trade-off between the model fitting and its\ncomplexity. Empirical assessment on synthetic data shows the superiority of our\nmethodology over a baseline based on canonical functional connectivity\nnetworks. When applied to resting-state fMRI data, we find sparse MCBs for both\nthe left and right brain hemispheres. Thanks to its multiscale nature, our\napproach shows that at low-frequency bands, causal dynamics are driven by brain\nregions associated with high-level cognitive functions; at higher frequencies\ninstead, nodes related to sensory processing play a crucial role. Finally, our\nanalysis of individual multiscale causal structures confirms the existence of a\ncausal fingerprint of brain connectivity, thus supporting from a causal\nperspective the existing extensive research in brain connectivity\nfingerprinting."}, "http://arxiv.org/abs/2311.00122": {"title": "Statistical Network Analysis: Past, Present, and Future", "link": "http://arxiv.org/abs/2311.00122", "description": "This article provides a brief overview of statistical network analysis, a\nrapidly evolving field of statistics, which encompasses statistical models,\nalgorithms, and inferential methods for analyzing data in the form of networks.\nParticular emphasis is given to connecting the historical developments in\nnetwork science to today's statistical network analysis, and outlining\nimportant new areas for future research.\n\nThis invited article is intended as a book chapter for the volume \"Frontiers\nof Statistics and Data Science\" edited by Subhashis Ghoshal and Anindya Roy for\nthe International Indian Statistical Association Series on Statistics and Data\nScience, published by Springer. This review article covers the material from\nthe short course titled \"Statistical Network Analysis: Past, Present, and\nFuture\" taught by the author at the Annual Conference of the International\nIndian Statistical Association, June 6-10, 2023, at Golden, Colorado."}, "http://arxiv.org/abs/2311.00210": {"title": "Broken Adaptive Ridge Method for Variable Selection in Generalized Partly Linear Models with Application to the Coronary Artery Disease Data", "link": "http://arxiv.org/abs/2311.00210", "description": "Motivated by the CATHGEN data, we develop a new statistical learning method\nfor simultaneous variable selection and parameter estimation under the context\nof generalized partly linear models for data with high-dimensional covariates.\nThe method is referred to as the broken adaptive ridge (BAR) estimator, which\nis an approximation of the $L_0$-penalized regression by iteratively performing\nreweighted squared $L_2$-penalized regression. The generalized partly linear\nmodel extends the generalized linear model by including a non-parametric\ncomponent to construct a flexible model for modeling various types of covariate\neffects. We employ the Bernstein polynomials as the sieve space to approximate\nthe non-parametric functions so that our method can be implemented easily using\nthe existing R packages. Extensive simulation studies suggest that the proposed\nmethod performs better than other commonly used penalty-based variable\nselection methods. We apply the method to the CATHGEN data with a binary\nresponse from a coronary artery disease study, which motivated our research,\nand obtained new findings in both high-dimensional genetic and low-dimensional\nnon-genetic covariates."}, "http://arxiv.org/abs/2311.00294": {"title": "Multi-step ahead prediction intervals for non-parametric autoregressions via bootstrap: consistency, debiasing and pertinence", "link": "http://arxiv.org/abs/2311.00294", "description": "To address the difficult problem of multi-step ahead prediction of\nnon-parametric autoregressions, we consider a forward bootstrap approach.\nEmploying a local constant estimator, we can analyze a general type of\nnon-parametric time series model, and show that the proposed point predictions\nare consistent with the true optimal predictor. We construct a quantile\nprediction interval that is asymptotically valid. Moreover, using a debiasing\ntechnique, we can asymptotically approximate the distribution of multi-step\nahead non-parametric estimation by bootstrap. As a result, we can build\nbootstrap prediction intervals that are pertinent, i.e., can capture the model\nestimation variability, thus improving upon the standard quantile prediction\nintervals. Simulation studies are given to illustrate the performance of our\npoint predictions and pertinent prediction intervals for finite samples."}, "http://arxiv.org/abs/2311.00528": {"title": "On the Comparative Analysis of Average Treatment Effects Estimation via Data Combination", "link": "http://arxiv.org/abs/2311.00528", "description": "There is growing interest in exploring causal effects in target populations\nby combining multiple datasets. Nevertheless, most approaches are tailored to\nspecific settings and lack comprehensive comparative analyses across different\nsettings. In this article, within the typical scenario of a source dataset and\na target dataset, we establish a unified framework for comparing various\nsettings in causal inference via data combination. We first design six distinct\nsettings, each with different available datasets and identifiability\nassumptions. The six settings cover a wide range of scenarios in the existing\nliterature. We then conduct a comprehensive efficiency comparative analysis\nacross these settings by calculating and comparing the semiparametric\nefficiency bounds for the average treatment effect (ATE) in the target\npopulation. Our findings reveal the key factors contributing to efficiency\ngains or losses across these settings. In addition, we extend our analysis to\nother estimands, including ATE in the source population and the average\ntreatment effect on treated (ATT) in both the source and target populations.\nFurthermore, we empirically validate our findings by constructing locally\nefficient estimators and conducting extensive simulation studies. We\ndemonstrate the proposed approaches using a real application to a MIMIC-III\ndataset as the target population and an eICU dataset as the source population."}, "http://arxiv.org/abs/2311.00541": {"title": "An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek", "link": "http://arxiv.org/abs/2311.00541", "description": "Word meanings change over time, and word senses evolve, emerge or die out in\nthe process. For ancient languages, where the corpora are often small, sparse\nand noisy, modelling such changes accurately proves challenging, and\nquantifying uncertainty in sense-change estimates consequently becomes\nimportant. GASC and DiSC are existing generative models that have been used to\nanalyse sense change for target words from an ancient Greek text corpus, using\nunsupervised learning without the help of any pre-training. These models\nrepresent the senses of a given target word such as \"kosmos\" (meaning\ndecoration, order or world) as distributions over context words, and sense\nprevalence as a distribution over senses. The models are fitted using MCMC\nmethods to measure temporal changes in these representations. In this paper, we\nintroduce EDiSC, an embedded version of DiSC, which combines word embeddings\nwith DiSC to provide superior model performance. We show empirically that EDiSC\noffers improved predictive accuracy, ground-truth recovery and uncertainty\nquantification, as well as better sampling efficiency and scalability\nproperties with MCMC methods. We also discuss the challenges of fitting these\nmodels."}, "http://arxiv.org/abs/2311.00553": {"title": "Polynomial Chaos Surrogate Construction for Random Fields with Parametric Uncertainty", "link": "http://arxiv.org/abs/2311.00553", "description": "Engineering and applied science rely on computational experiments to\nrigorously study physical systems. The mathematical models used to probe these\nsystems are highly complex, and sampling-intensive studies often require\nprohibitively many simulations for acceptable accuracy. Surrogate models\nprovide a means of circumventing the high computational expense of sampling\nsuch complex models. In particular, polynomial chaos expansions (PCEs) have\nbeen successfully used for uncertainty quantification studies of deterministic\nmodels where the dominant source of uncertainty is parametric. We discuss an\nextension to conventional PCE surrogate modeling to enable surrogate\nconstruction for stochastic computational models that have intrinsic noise in\naddition to parametric uncertainty. We develop a PCE surrogate on a joint space\nof intrinsic and parametric uncertainty, enabled by Rosenblatt transformations,\nand then extend the construction to random field data via the Karhunen-Loeve\nexpansion. We then take advantage of closed-form solutions for computing PCE\nSobol indices to perform a global sensitivity analysis of the model which\nquantifies the intrinsic noise contribution to the overall model output\nvariance. Additionally, the resulting joint PCE is generative in the sense that\nit allows generating random realizations at any input parameter setting that\nare statistically approximately equivalent to realizations from the underlying\nstochastic model. The method is demonstrated on a chemical catalysis example\nmodel."}, "http://arxiv.org/abs/2311.00568": {"title": "Scalable kernel balancing weights in a nationwide observational study of hospital profit status and heart attack outcomes", "link": "http://arxiv.org/abs/2311.00568", "description": "Weighting is a general and often-used method for statistical adjustment.\nWeighting has two objectives: first, to balance covariate distributions, and\nsecond, to ensure that the weights have minimal dispersion and thus produce a\nmore stable estimator. A recent, increasingly common approach directly\noptimizes the weights toward these two objectives. However, this approach has\nnot yet been feasible in large-scale datasets when investigators wish to\nflexibly balance general basis functions in an extended feature space. For\nexample, many balancing approaches cannot scale to national-level health\nservices research studies. To address this practical problem, we describe a\nscalable and flexible approach to weighting that integrates a basis expansion\nin a reproducing kernel Hilbert space with state-of-the-art convex optimization\ntechniques. Specifically, we use the rank-restricted Nystr\\\"{o}m method to\nefficiently compute a kernel basis for balancing in {nearly} linear time and\nspace, and then use the specialized first-order alternating direction method of\nmultipliers to rapidly find the optimal weights. In an extensive simulation\nstudy, we provide new insights into the performance of weighting estimators in\nlarge datasets, showing that the proposed approach substantially outperforms\nothers in terms of accuracy and speed. Finally, we use this weighting approach\nto conduct a national study of the relationship between hospital profit status\nand heart attack outcomes in a comprehensive dataset of 1.27 million patients.\nWe find that for-profit hospitals use interventional cardiology to treat heart\nattacks at similar rates as other hospitals, but have higher mortality and\nreadmission rates."}, "http://arxiv.org/abs/2311.00577": {"title": "Personalized Assignment to One of Many Treatment Arms via Regularized and Clustered Joint Assignment Forests", "link": "http://arxiv.org/abs/2311.00577", "description": "We consider learning personalized assignments to one of many treatment arms\nfrom a randomized controlled trial. Standard methods that estimate\nheterogeneous treatment effects separately for each arm may perform poorly in\nthis case due to excess variance. We instead propose methods that pool\ninformation across treatment arms: First, we consider a regularized\nforest-based assignment algorithm based on greedy recursive partitioning that\nshrinks effect estimates across arms. Second, we augment our algorithm by a\nclustering scheme that combines treatment arms with consistently similar\noutcomes. In a simulation study, we compare the performance of these approaches\nto predicting arm-wise outcomes separately, and document gains of directly\noptimizing the treatment assignment with regularization and clustering. In a\ntheoretical model, we illustrate how a high number of treatment arms makes\nfinding the best arm hard, while we can achieve sizable utility gains from\npersonalization by regularized optimization."}, "http://arxiv.org/abs/2311.00596": {"title": "Evaluating Binary Outcome Classifiers Estimated from Survey Data", "link": "http://arxiv.org/abs/2311.00596", "description": "Surveys are commonly used to facilitate research in epidemiology, health, and\nthe social and behavioral sciences. Often, these surveys are not simple random\nsamples, and respondents are given weights reflecting their probability of\nselection into the survey. It is well known that analysts can use these survey\nweights to produce unbiased estimates of population quantities like totals. In\nthis article, we show that survey weights also can be beneficial for evaluating\nthe quality of predictive models when splitting data into training and test\nsets. In particular, we characterize model assessment statistics, such as\nsensitivity and specificity, as finite population quantities, and compute\nsurvey-weighted estimates of these quantities with sample test data comprising\na random subset of the original data.Using simulations with data from the\nNational Survey on Drug Use and Health and the National Comorbidity Survey, we\nshow that unweighted metrics estimated with sample test data can misrepresent\npopulation performance, but weighted metrics appropriately adjust for the\ncomplex sampling design. We also show that this conclusion holds for models\ntrained using upsampling for mitigating class imbalance. The results suggest\nthat weighted metrics should be used when evaluating performance on sample test\ndata."}, "http://arxiv.org/abs/2206.10866": {"title": "Nearest Neighbor Classification based on Imbalanced Data: A Statistical Approach", "link": "http://arxiv.org/abs/2206.10866", "description": "When the competing classes in a classification problem are not of comparable\nsize, many popular classifiers exhibit a bias towards larger classes, and the\nnearest neighbor classifier is no exception. To take care of this problem, we\ndevelop a statistical method for nearest neighbor classification based on such\nimbalanced data sets. First, we construct a classifier for the binary\nclassification problem and then extend it for classification problems involving\nmore than two classes. Unlike the existing oversampling or undersampling\nmethods, our proposed classifiers do not need to generate any pseudo\nobservations or remove any existing observations, hence the results are exactly\nreproducible. We establish the Bayes risk consistency of these classifiers\nunder appropriate regularity conditions. Their superior performance over the\nexisting methods is amply demonstrated by analyzing several benchmark data\nsets."}, "http://arxiv.org/abs/2209.08892": {"title": "High-dimensional data segmentation in regression settings permitting temporal dependence and non-Gaussianity", "link": "http://arxiv.org/abs/2209.08892", "description": "We propose a data segmentation methodology for the high-dimensional linear\nregression problem where regression parameters are allowed to undergo multiple\nchanges. The proposed methodology, MOSEG, proceeds in two stages: first, the\ndata are scanned for multiple change points using a moving window-based\nprocedure, which is followed by a location refinement stage. MOSEG enjoys\ncomputational efficiency thanks to the adoption of a coarse grid in the first\nstage, and achieves theoretical consistency in estimating both the total number\nand the locations of the change points, under general conditions permitting\nserial dependence and non-Gaussianity. We also propose MOSEG.MS, a multiscale\nextension of MOSEG which, while comparable to MOSEG in terms of computational\ncomplexity, achieves theoretical consistency for a broader parameter space\nwhere large parameter shifts over short intervals and small changes over long\nstretches of stationarity are simultaneously allowed. We demonstrate good\nperformance of the proposed methods in comparative simulation studies and in an\napplication to predicting the equity premium."}, "http://arxiv.org/abs/2210.02341": {"title": "A Distributed Block-Split Gibbs Sampler with Hypergraph Structure for High-Dimensional Inverse Problems", "link": "http://arxiv.org/abs/2210.02341", "description": "Sampling-based algorithms are classical approaches to perform Bayesian\ninference in inverse problems. They provide estimators with the associated\ncredibility intervals to quantify the uncertainty on the estimators. Although\nthese methods hardly scale to high dimensional problems, they have recently\nbeen paired with optimization techniques, such as proximal and splitting\napproaches, to address this issue. Such approaches pave the way to distributed\nsamplers, splitting computations to make inference more scalable and faster. We\nintroduce a distributed Split Gibbs sampler (SGS) to efficiently solve such\nproblems involving distributions with multiple smooth and non-smooth functions\ncomposed with linear operators. The proposed approach leverages a recent\napproximate augmentation technique reminiscent of primal-dual optimization\nmethods. It is further combined with a block-coordinate approach to split the\nprimal and dual variables into blocks, leading to a distributed\nblock-coordinate SGS. The resulting algorithm exploits the hypergraph structure\nof the involved linear operators to efficiently distribute the variables over\nmultiple workers under controlled communication costs. It accommodates several\ndistributed architectures, such as the Single Program Multiple Data and\nclient-server architectures. Experiments on a large image deblurring problem\nshow the performance of the proposed approach to produce high quality estimates\nwith credibility intervals in a small amount of time. Supplementary material to\nreproduce the experiments is available online."}, "http://arxiv.org/abs/2210.14086": {"title": "A Global Wavelet Based Bootstrapped Test of Covariance Stationarity", "link": "http://arxiv.org/abs/2210.14086", "description": "We propose a covariance stationarity test for an otherwise dependent and\npossibly globally non-stationary time series. We work in a generalized version\nof the new setting in Jin, Wang and Wang (2015), who exploit Walsh (1923)\nfunctions in order to compare sub-sample covariances with the full sample\ncounterpart. They impose strict stationarity under the null, only consider\nlinear processes under either hypothesis in order to achieve a parametric\nestimator for an inverted high dimensional asymptotic covariance matrix, and do\nnot consider any other orthonormal basis. Conversely, we work with a general\northonormal basis under mild conditions that include Haar wavelet and Walsh\nfunctions; and we allow for linear or nonlinear processes with possibly non-iid\ninnovations. This is important in macroeconomics and finance where nonlinear\nfeedback and random volatility occur in many settings. We completely sidestep\nasymptotic covariance matrix estimation and inversion by bootstrapping a\nmax-correlation difference statistic, where the maximum is taken over the\ncorrelation lag $h$ and basis generated sub-sample counter $k$ (the number of\nsystematic samples). We achieve a higher feasible rate of increase for the\nmaximum lag and counter $\\mathcal{H}_{T}$ and $\\mathcal{K}_{T}$. Of particular\nnote, our test is capable of detecting breaks in variance, and distant, or very\nmild, deviations from stationarity."}, "http://arxiv.org/abs/2211.03031": {"title": "A framework for leveraging machine learning tools to estimate personalized survival curves", "link": "http://arxiv.org/abs/2211.03031", "description": "The conditional survival function of a time-to-event outcome subject to\ncensoring and truncation is a common target of estimation in survival analysis.\nThis parameter may be of scientific interest and also often appears as a\nnuisance in nonparametric and semiparametric problems. In addition to classical\nparametric and semiparametric methods (e.g., based on the Cox proportional\nhazards model), flexible machine learning approaches have been developed to\nestimate the conditional survival function. However, many of these methods are\neither implicitly or explicitly targeted toward risk stratification rather than\noverall survival function estimation. Others apply only to discrete-time\nsettings or require inverse probability of censoring weights, which can be as\ndifficult to estimate as the outcome survival function itself. Here, we employ\na decomposition of the conditional survival function in terms of observable\nregression models in which censoring and truncation play no role. This allows\napplication of an array of flexible regression and classification methods\nrather than only approaches that explicitly handle the complexities inherent to\nsurvival data. We outline estimation procedures based on this decomposition,\nempirically assess their performance, and demonstrate their use on data from an\nHIV vaccine trial."}, "http://arxiv.org/abs/2301.12389": {"title": "On Learning Necessary and Sufficient Causal Graphs", "link": "http://arxiv.org/abs/2301.12389", "description": "The causal revolution has stimulated interest in understanding complex\nrelationships in various fields. Most of the existing methods aim to discover\ncausal relationships among all variables within a complex large-scale graph.\nHowever, in practice, only a small subset of variables in the graph are\nrelevant to the outcomes of interest. Consequently, causal estimation with the\nfull causal graph -- particularly given limited data -- could lead to numerous\nfalsely discovered, spurious variables that exhibit high correlation with, but\nexert no causal impact on, the target outcome. In this paper, we propose\nlearning a class of necessary and sufficient causal graphs (NSCG) that\nexclusively comprises causally relevant variables for an outcome of interest,\nwhich we term causal features. The key idea is to employ probabilities of\ncausation to systematically evaluate the importance of features in the causal\ngraph, allowing us to identify a subgraph relevant to the outcome of interest.\nTo learn NSCG from data, we develop a necessary and sufficient causal\nstructural learning (NSCSL) algorithm, by establishing theoretical properties\nand relationships between probabilities of causation and natural causal effects\nof features. Across empirical studies of simulated and real data, we\ndemonstrate that NSCSL outperforms existing algorithms and can reveal crucial\nyeast genes for target heritable traits of interest."}, "http://arxiv.org/abs/2303.18211": {"title": "A Scale-Invariant Sorting Criterion to Find a Causal Order in Additive Noise Models", "link": "http://arxiv.org/abs/2303.18211", "description": "Additive Noise Models (ANMs) are a common model class for causal discovery\nfrom observational data and are often used to generate synthetic data for\ncausal discovery benchmarking. Specifying an ANM requires choosing all\nparameters, including those not fixed by explicit assumptions. Reisach et al.\n(2021) show that sorting variables by increasing variance often yields an\nordering close to a causal order and introduce var-sortability to quantify this\nalignment. Since increasing variances may be unrealistic and are\nscale-dependent, ANM data are often standardized in benchmarks.\n\nWe show that synthetic ANM data are characterized by another pattern that is\nscale-invariant: the explainable fraction of a variable's variance, as captured\nby the coefficient of determination $R^2$, tends to increase along the causal\norder. The result is high $R^2$-sortability, meaning that sorting the variables\nby increasing $R^2$ yields an ordering close to a causal order. We propose an\nefficient baseline algorithm termed $R^2$-SortnRegress that exploits high\n$R^2$-sortability and that can match and exceed the performance of established\ncausal discovery algorithms. We show analytically that sufficiently high edge\nweights lead to a relative decrease of the noise contributions along causal\nchains, resulting in increasingly deterministic relationships and high $R^2$.\nWe characterize $R^2$-sortability for different simulation parameters and find\nhigh values in common settings. Our findings reveal high $R^2$-sortability as\nan assumption about the data generating process relevant to causal discovery\nand implicit in many ANM sampling schemes. It should be made explicit, as its\nprevalence in real-world data is unknown. For causal discovery benchmarking, we\nimplement $R^2$-sortability, the $R^2$-SortnRegress algorithm, and ANM\nsimulation procedures in our library CausalDisco at\nhttps://causaldisco.github.io/CausalDisco/."}, "http://arxiv.org/abs/2304.11491": {"title": "Bayesian Boundary Trend Filtering", "link": "http://arxiv.org/abs/2304.11491", "description": "Estimating boundary curves has many applications such as economics, climate\nscience, and medicine. Bayesian trend filtering has been developed as one of\nlocally adaptive smoothing methods to estimate the non-stationary trend of\ndata. This paper develops a Bayesian trend filtering for estimating boundary\ntrend. To this end, the truncated multivariate normal working likelihood and\nglobal-local shrinkage priors based on scale mixtures of normal distribution\nare introduced. In particular, well-known horseshoe prior for difference leads\nto locally adaptive shrinkage estimation for boundary trend. However, the full\nconditional distributions of the Gibbs sampler involve high-dimensional\ntruncated multivariate normal distribution. To overcome the difficulty of\nsampling, an approximation of truncated multivariate normal distribution is\nemployed. Using the approximation, the proposed models lead to an efficient\nGibbs sampling algorithm via P\\'olya-Gamma data augmentation. The proposed\nmethod is also extended by considering nearly isotonic constraint. The\nperformance of the proposed method is illustrated through some numerical\nexperiments and real data examples."}, "http://arxiv.org/abs/2304.13237": {"title": "An Efficient Doubly-Robust Test for the Kernel Treatment Effect", "link": "http://arxiv.org/abs/2304.13237", "description": "The average treatment effect, which is the difference in expectation of the\ncounterfactuals, is probably the most popular target effect in causal inference\nwith binary treatments. However, treatments may have effects beyond the mean,\nfor instance decreasing or increasing the variance. We propose a new\nkernel-based test for distributional effects of the treatment. It is, to the\nbest of our knowledge, the first kernel-based, doubly-robust test with provably\nvalid type-I error. Furthermore, our proposed algorithm is computationally\nefficient, avoiding the use of permutations."}, "http://arxiv.org/abs/2305.07981": {"title": "Inferring Stochastic Group Interactions within Structured Populations via Coupled Autoregression", "link": "http://arxiv.org/abs/2305.07981", "description": "The internal behaviour of a population is an important feature to take\naccount of when modelling their dynamics. In line with kin selection theory,\nmany social species tend to cluster into distinct groups in order to enhance\ntheir overall population fitness. Temporal interactions between populations are\noften modelled using classical mathematical models, but these sometimes fail to\ndelve deeper into the, often uncertain, relationships within populations. Here,\nwe introduce a stochastic framework that aims to capture the interactions of\nanimal groups and an auxiliary population over time. We demonstrate the model's\ncapabilities, from a Bayesian perspective, through simulation studies and by\nfitting it to predator-prey count time series data. We then derive an\napproximation to the group correlation structure within such a population,\nwhile also taking account of the effect of the auxiliary population. We finally\ndiscuss how this approximation can lead to ecologically realistic\ninterpretations in a predator-prey context. This approximation also serves as\nverification to whether the population in question satisfies our various\nassumptions. Our modelling approach will be useful for empiricists for\nmonitoring groups within a conservation framework and also theoreticians\nwanting to quantify interactions, to study cooperation and other phenomena\nwithin social populations."}, "http://arxiv.org/abs/2306.09520": {"title": "Ensembled Prediction Intervals for Causal Outcomes Under Hidden Confounding", "link": "http://arxiv.org/abs/2306.09520", "description": "Causal inference of exact individual treatment outcomes in the presence of\nhidden confounders is rarely possible. Recent work has extended prediction\nintervals with finite-sample guarantees to partially identifiable causal\noutcomes, by means of a sensitivity model for hidden confounding. In deep\nlearning, predictors can exploit their inductive biases for better\ngeneralization out of sample. We argue that the structure inherent to a deep\nensemble should inform a tighter partial identification of the causal outcomes\nthat they predict. We therefore introduce an approach termed Caus-Modens, for\ncharacterizing causal outcome intervals by modulated ensembles. We present a\nsimple approach to partial identification using existing causal sensitivity\nmodels and show empirically that Caus-Modens gives tighter outcome intervals,\nas measured by the necessary interval size to achieve sufficient coverage. The\nlast of our three diverse benchmarks is a novel usage of GPT-4 for\nobservational experiments with unknown but probeable ground truth."}, "http://arxiv.org/abs/2306.16838": {"title": "Solving Kernel Ridge Regression with Gradient-Based Optimization Methods", "link": "http://arxiv.org/abs/2306.16838", "description": "Kernel ridge regression, KRR, is a generalization of linear ridge regression\nthat is non-linear in the data, but linear in the parameters. Here, we\nintroduce an equivalent formulation of the objective function of KRR, opening\nup both for using penalties other than the ridge penalty and for studying\nkernel ridge regression from the perspective of gradient descent. Using a\ncontinuous-time perspective, we derive a closed-form solution for solving\nkernel regression with gradient descent, something we refer to as kernel\ngradient flow, KGF, and theoretically bound the differences between KRR and\nKGF, where, for the latter, regularization is obtained through early stopping.\nWe also generalize KRR by replacing the ridge penalty with the $\\ell_1$ and\n$\\ell_\\infty$ penalties, respectively, and use the fact that analogous to the\nsimilarities between KGF and KRR, $\\ell_1$ regularization and forward stagewise\nregression (also known as coordinate descent), and $\\ell_\\infty$ regularization\nand sign gradient descent, follow similar solution paths. We can thus alleviate\nthe need for computationally heavy algorithms based on proximal gradient\ndescent. We show theoretically and empirically how the $\\ell_1$ and\n$\\ell_\\infty$ penalties, and the corresponding gradient-based optimization\nalgorithms, produce sparse and robust kernel regression solutions,\nrespectively."}, "http://arxiv.org/abs/2311.00820": {"title": "Bayesian inference for generalized linear models via quasi-posteriors", "link": "http://arxiv.org/abs/2311.00820", "description": "Generalized linear models (GLMs) are routinely used for modeling\nrelationships between a response variable and a set of covariates. The simple\nform of a GLM comes with easy interpretability, but also leads to concerns\nabout model misspecification impacting inferential conclusions. A popular\nsemi-parametric solution adopted in the frequentist literature is\nquasi-likelihood, which improves robustness by only requiring correct\nspecification of the first two moments. We develop a robust approach to\nBayesian inference in GLMs through quasi-posterior distributions. We show that\nquasi-posteriors provide a coherent generalized Bayes inference method, while\nalso approximating so-called coarsened posteriors. In so doing, we obtain new\ninsights into the choice of coarsening parameter. Asymptotically, the\nquasi-posterior converges in total variation to a normal distribution and has\nimportant connections with the loss-likelihood bootstrap posterior. We\ndemonstrate that it is also well-calibrated in terms of frequentist coverage.\nMoreover, the loss-scale parameter has a clear interpretation as a dispersion,\nand this leads to a consolidated method of moments estimator."}, "http://arxiv.org/abs/2311.00878": {"title": "Backward Joint Model for Dynamic Prediction using Multivariate Longitudinal and Competing Risk Data", "link": "http://arxiv.org/abs/2311.00878", "description": "Joint modeling is a useful approach to dynamic prediction of clinical\noutcomes using longitudinally measured predictors. When the outcomes are\ncompeting risk events, fitting the conventional shared random effects joint\nmodel often involves intensive computation, especially when multiple\nlongitudinal biomarkers are be used as predictors, as is often desired in\nprediction problems. Motivated by a longitudinal cohort study of chronic kidney\ndisease, this paper proposes a new joint model for the dynamic prediction of\nend-stage renal disease with the competing risk of death. The model factorizes\nthe likelihood into the distribution of the competing risks data and the\ndistribution of longitudinal data given the competing risks data. The\nestimation with the EM algorithm is efficient, stable and fast, with a\none-dimensional integral in the E-step and convex optimization for most\nparameters in the M-step, regardless of the number of longitudinal predictors.\nThe model also comes with a consistent albeit less efficient estimation method\nthat can be quickly implemented with standard software, ideal for model\nbuilding and diagnotics. This model enables the prediction of future\nlongitudinal data trajectories conditional on being at risk at a future time, a\npractically significant problem that has not been studied in the statistical\nliterature. We study the properties of the proposed method using simulations\nand a real dataset and compare its performance with the shared random effects\njoint model."}, "http://arxiv.org/abs/2311.00885": {"title": "Controlling the number of significant effects in multiple testing", "link": "http://arxiv.org/abs/2311.00885", "description": "In multiple testing several criteria to control for type I errors exist. The\nfalse discovery rate, which evaluates the expected proportion of false\ndiscoveries among the rejected null hypotheses, has become the standard\napproach in this setting. However, false discovery rate control may be too\nconservative when the effects are weak. In this paper we alternatively propose\nto control the number of significant effects, where 'significant' refers to a\npre-specified threshold $\\gamma$. This means that a $(1-\\alpha)$-lower\nconfidence bound $L$ for the number of non-true null hypothesis with p-values\nbelow $\\gamma$ is provided. When one rejects the nulls corresponding to the $L$\nsmallest p-values, the probability that the number of false positives exceeds\nthe number of false negatives among the significant effects is bounded by\n$\\alpha$. Relative merits of the proposed criterion are discussed. Procedures\nto control for the number of significant effects in practice are introduced and\ninvestigated both theoretically and through simulations. Illustrative real data\napplications are given."}, "http://arxiv.org/abs/2311.00923": {"title": "A Review and Roadmap of Deep Causal Model from Different Causal Structures and Representations", "link": "http://arxiv.org/abs/2311.00923", "description": "The fusion of causal models with deep learning introducing increasingly\nintricate data sets, such as the causal associations within images or between\ntextual components, has surfaced as a focal research area. Nonetheless, the\nbroadening of original causal concepts and theories to such complex,\nnon-statistical data has been met with serious challenges. In response, our\nstudy proposes redefinitions of causal data into three distinct categories from\nthe standpoint of causal structure and representation: definite data,\nsemi-definite data, and indefinite data. Definite data chiefly pertains to\nstatistical data used in conventional causal scenarios, while semi-definite\ndata refers to a spectrum of data formats germane to deep learning, including\ntime-series, images, text, and others. Indefinite data is an emergent research\nsphere inferred from the progression of data forms by us. To comprehensively\npresent these three data paradigms, we elaborate on their formal definitions,\ndifferences manifested in datasets, resolution pathways, and development of\nresearch. We summarize key tasks and achievements pertaining to definite and\nsemi-definite data from myriad research undertakings, present a roadmap for\nindefinite data, beginning with its current research conundrums. Lastly, we\nclassify and scrutinize the key datasets presently utilized within these three\nparadigms."}, "http://arxiv.org/abs/2311.00927": {"title": "Scalable Counterfactual Distribution Estimation in Multivariate Causal Models", "link": "http://arxiv.org/abs/2311.00927", "description": "We consider the problem of estimating the counterfactual joint distribution\nof multiple quantities of interests (e.g., outcomes) in a multivariate causal\nmodel extended from the classical difference-in-difference design. Existing\nmethods for this task either ignore the correlation structures among dimensions\nof the multivariate outcome by considering univariate causal models on each\ndimension separately and hence produce incorrect counterfactual distributions,\nor poorly scale even for moderate-size datasets when directly dealing with such\nmultivariate causal model. We propose a method that alleviates both issues\nsimultaneously by leveraging a robust latent one-dimensional subspace of the\noriginal high-dimension space and exploiting the efficient estimation from the\nunivariate causal model on such space. Since the construction of the\none-dimensional subspace uses information from all the dimensions, our method\ncan capture the correlation structures and produce good estimates of the\ncounterfactual distribution. We demonstrate the advantages of our approach over\nexisting methods on both synthetic and real-world data."}, "http://arxiv.org/abs/2311.01021": {"title": "ABC-based Forecasting in State Space Models", "link": "http://arxiv.org/abs/2311.01021", "description": "Approximate Bayesian Computation (ABC) has gained popularity as a method for\nconducting inference and forecasting in complex models, most notably those\nwhich are intractable in some sense. In this paper we use ABC to produce\nprobabilistic forecasts in state space models (SSMs). Whilst ABC-based\nforecasting in correctly-specified SSMs has been studied, the misspecified case\nhas not been investigated, and it is that case which we emphasize. We invoke\nrecent principles of 'focused' Bayesian prediction, whereby Bayesian updates\nare driven by a scoring rule that rewards predictive accuracy; the aim being to\nproduce predictives that perform well in that rule, despite misspecification.\nTwo methods are investigated for producing the focused predictions. In a\nsimulation setting, 'coherent' predictions are in evidence for both methods:\nthe predictive constructed via the use of a particular scoring rule predicts\nbest according to that rule. Importantly, both focused methods typically\nproduce more accurate forecasts than an exact, but misspecified, predictive. An\nempirical application to a truly intractable SSM completes the paper."}, "http://arxiv.org/abs/2311.01147": {"title": "Variational Inference for Sparse Poisson Regression", "link": "http://arxiv.org/abs/2311.01147", "description": "We have utilized the non-conjugate VB method for the problem of the sparse\nPoisson regression model. To provide an approximated conjugacy in the model,\nthe likelihood is approximated by a quadratic function, which provides the\nconjugacy of the approximation component with the Gaussian prior to the\nregression coefficient. Three sparsity-enforcing priors are used for this\nproblem. The proposed models are compared with each other and two frequentist\nsparse Poisson methods (LASSO and SCAD) to evaluate the prediction performance,\nas well as, the sparsing performance of the proposed methods. Throughout a\nsimulated data example, the accuracy of the VB methods is computed compared to\nthe corresponding benchmark MCMC methods. It can be observed that the proposed\nVB methods have provided a good approximation to the posterior distribution of\nthe parameters, while the VB methods are much faster than the MCMC ones. Using\nseveral benchmark count response data sets, the prediction performance of the\nproposed methods is evaluated in real-world applications."}, "http://arxiv.org/abs/2311.01287": {"title": "Semiparametric Latent ANOVA Model for Event-Related Potentials", "link": "http://arxiv.org/abs/2311.01287", "description": "Event-related potentials (ERPs) extracted from electroencephalography (EEG)\ndata in response to stimuli are widely used in psychological and neuroscience\nexperiments. A major goal is to link ERP characteristic components to\nsubject-level covariates. Existing methods typically follow two-step\napproaches, first identifying ERP components using peak detection methods and\nthen relating them to the covariates. This approach, however, can lead to loss\nof efficiency due to inaccurate estimates in the initial step, especially\nconsidering the low signal-to-noise ratio of EEG data. To address this\nchallenge, we propose a semiparametric latent ANOVA model (SLAM) that unifies\ninference on ERP components and their association to covariates. SLAM models\nERP waveforms via a structured Gaussian process prior that encodes ERP latency\nin its derivative and links the subject-level latencies to covariates using a\nlatent ANOVA. This unified Bayesian framework provides estimation at both\npopulation- and subject- levels, improving the efficiency of the inference by\nleveraging information across subjects. We automate posterior inference and\nhyperparameter tuning using a Monte Carlo expectation-maximization algorithm.\nWe demonstrate the advantages of SLAM over competing methods via simulations.\nOur method allows us to examine how factors or covariates affect the magnitude\nand/or latency of ERP components, which in turn reflect cognitive,\npsychological or neural processes. We exemplify this via an application to data\nfrom an ERP experiment on speech recognition, where we assess the effect of age\non two components of interest. Our results verify the scientific findings that\nolder people take a longer reaction time to respond to external stimuli because\nof the delay in perception and brain processes."}, "http://arxiv.org/abs/2311.01297": {"title": "Bias correction in multiple-systems estimation", "link": "http://arxiv.org/abs/2311.01297", "description": "If part of a population is hidden but two or more sources are available that\neach cover parts of this population, dual- or multiple-system(s) estimation can\nbe applied to estimate this population. For this it is common to use the\nlog-linear model, estimated with maximum likelihood. These maximum likelihood\nestimates are based on a non-linear model and therefore suffer from\nfinite-sample bias, which can be substantial in case of small samples or a\nsmall population size. This problem was recognised by Chapman, who derived an\nestimator with good small sample properties in case of two available sources.\nHowever, he did not derive an estimator for more than two sources. We propose\nan estimator that is an extension of Chapman's estimator to three or more\nsources and compare this estimator with other bias-reduced estimators in a\nsimulation study. The proposed estimator performs well, and much better than\nthe other estimators. A real data example on homelessness in the Netherlands\nshows that our proposed model can make a substantial difference."}, "http://arxiv.org/abs/2311.01301": {"title": "TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models", "link": "http://arxiv.org/abs/2311.01301", "description": "The rapid digitization of real-world data offers an unprecedented opportunity\nfor optimizing healthcare delivery and accelerating biomedical discovery. In\npractice, however, such data is most abundantly available in unstructured\nforms, such as clinical notes in electronic medical records (EMRs), and it is\ngenerally plagued by confounders. In this paper, we present TRIALSCOPE, a\nunifying framework for distilling real-world evidence from population-level\nobservational data. TRIALSCOPE leverages biomedical language models to\nstructure clinical text at scale, employs advanced probabilistic modeling for\ndenoising and imputation, and incorporates state-of-the-art causal inference\ntechniques to combat common confounders. Using clinical trial specification as\ngeneric representation, TRIALSCOPE provides a turn-key solution to generate and\nreason with clinical hypotheses using observational data. In extensive\nexperiments and analyses on a large-scale real-world dataset with over one\nmillion cancer patients from a large US healthcare network, we show that\nTRIALSCOPE can produce high-quality structuring of real-world data and\ngenerates comparable results to marquee cancer trials. In addition to\nfacilitating in-silicon clinical trial design and optimization, TRIALSCOPE may\nbe used to empower synthetic controls, pragmatic trials, post-market\nsurveillance, as well as support fine-grained patient-like-me reasoning in\nprecision diagnosis and treatment."}, "http://arxiv.org/abs/2311.01303": {"title": "Local differential privacy in survival analysis using private failure indicators", "link": "http://arxiv.org/abs/2311.01303", "description": "We consider the estimation of the cumulative hazard function, and\nequivalently the distribution function, with censored data under a setup that\npreserves the privacy of the survival database. This is done through a\n$\\alpha$-locally differentially private mechanism for the failure indicators\nand by proposing a non-parametric kernel estimator for the cumulative hazard\nfunction that remains consistent under the privatization. Under mild\nconditions, we also prove lowers bounds for the minimax rates of convergence\nand show that estimator is minimax optimal under a well-chosen bandwidth."}, "http://arxiv.org/abs/2311.01341": {"title": "Composite Dyadic Models for Spatio-Temporal Data", "link": "http://arxiv.org/abs/2311.01341", "description": "Mechanistic statistical models are commonly used to study the flow of\nbiological processes. For example, in landscape genetics, the aim is to infer\nmechanisms that govern gene flow in populations. Existing statistical\napproaches in landscape genetics do not account for temporal dependence in the\ndata and may be computationally prohibitive. We infer mechanisms with a\nBayesian hierarchical dyadic model that scales well with large data sets and\nthat accounts for spatial and temporal dependence. We construct a\nfully-connected network comprising spatio-temporal data for the dyadic model\nand use normalized composite likelihoods to account for the dependence\nstructure in space and time. Our motivation for developing a dyadic model was\nto account for physical mechanisms commonly found in physical-statistical\nmodels. However, a numerical solver is not required in our approach because we\nmodel first-order changes directly. We apply our methods to ancient human DNA\ndata to infer the mechanisms that affected human movement in Bronze Age Europe."}, "http://arxiv.org/abs/2311.01412": {"title": "Castor: Causal Temporal Regime Structure Learning", "link": "http://arxiv.org/abs/2311.01412", "description": "The task of uncovering causal relationships among multivariate time series\ndata stands as an essential and challenging objective that cuts across a broad\narray of disciplines ranging from climate science to healthcare. Such data\nentails linear or non-linear relationships, and usually follow multiple a\npriori unknown regimes. Existing causal discovery methods can infer summary\ncausal graphs from heterogeneous data with known regimes, but they fall short\nin comprehensively learning both regimes and the corresponding causal graph. In\nthis paper, we introduce CASTOR, a novel framework designed to learn causal\nrelationships in heterogeneous time series data composed of various regimes,\neach governed by a distinct causal graph. Through the maximization of a score\nfunction via the EM algorithm, CASTOR infers the number of regimes and learns\nlinear or non-linear causal relationships in each regime. We demonstrate the\nrobust convergence properties of CASTOR, specifically highlighting its\nproficiency in accurately identifying unique regimes. Empirical evidence,\ngarnered from exhaustive synthetic experiments and two real-world benchmarks,\nconfirm CASTOR's superior performance in causal discovery compared to baseline\nmethods. By learning a full temporal causal graph for each regime, CASTOR\nestablishes itself as a distinctly interpretable method for causal discovery in\nheterogeneous time series."}, "http://arxiv.org/abs/2311.01453": {"title": "PPI++: Efficient Prediction-Powered Inference", "link": "http://arxiv.org/abs/2311.01453", "description": "We present PPI++: a computationally lightweight methodology for estimation\nand inference based on a small labeled dataset and a typically much larger\ndataset of machine-learning predictions. The methods automatically adapt to the\nquality of available predictions, yielding easy-to-compute confidence sets --\nfor parameters of any dimensionality -- that always improve on classical\nintervals using only the labeled data. PPI++ builds on prediction-powered\ninference (PPI), which targets the same problem setting, improving its\ncomputational and statistical efficiency. Real and synthetic experiments\ndemonstrate the benefits of the proposed adaptations."}, "http://arxiv.org/abs/2008.00707": {"title": "Heterogeneous Treatment and Spillover Effects under Clustered Network Interference", "link": "http://arxiv.org/abs/2008.00707", "description": "The bulk of causal inference studies rule out the presence of interference\nbetween units. However, in many real-world scenarios, units are interconnected\nby social, physical, or virtual ties, and the effect of the treatment can spill\nfrom one unit to other connected individuals in the network. In this paper, we\ndevelop a machine learning method that uses tree-based algorithms and a\nHorvitz-Thompson estimator to assess the heterogeneity of treatment and\nspillover effects with respect to individual, neighborhood, and network\ncharacteristics in the context of clustered networks and neighborhood\ninterference within clusters. The proposed Network Causal Tree (NCT) algorithm\nhas several advantages. First, it allows the investigation of the treatment\neffect heterogeneity, avoiding potential bias due to the presence of\ninterference. Second, understanding the heterogeneity of both treatment and\nspillover effects can guide policy-makers in scaling up interventions,\ndesigning targeting strategies, and increasing cost-effectiveness. We\ninvestigate the performance of our NCT method using a Monte Carlo simulation\nstudy, and we illustrate its application to assess the heterogeneous effects of\ninformation sessions on the uptake of a new weather insurance policy in rural\nChina."}, "http://arxiv.org/abs/2107.01773": {"title": "Extending Latent Basis Growth Model to Explore Joint Development in the Framework of Individual Measurement Occasions", "link": "http://arxiv.org/abs/2107.01773", "description": "Longitudinal processes often pose nonlinear change patterns. Latent basis\ngrowth models (LBGMs) provide a versatile solution without requiring specific\nfunctional forms. Building on the LBGM specification for unequally-spaced waves\nand individual occasions proposed by Liu and Perera (2023), we extend LBGMs to\nmultivariate longitudinal outcomes. This provides a unified approach to\nnonlinear, interconnected trajectories. Simulation studies demonstrate that the\nproposed model can provide unbiased and accurate estimates with target coverage\nprobabilities for the parameters of interest. Real-world analyses of reading\nand mathematics scores demonstrates its effectiveness in analyzing joint\ndevelopmental processes that vary in temporal patterns. Computational code is\nincluded."}, "http://arxiv.org/abs/2112.03152": {"title": "Bounding Wasserstein distance with couplings", "link": "http://arxiv.org/abs/2112.03152", "description": "Markov chain Monte Carlo (MCMC) provides asymptotically consistent estimates\nof intractable posterior expectations as the number of iterations tends to\ninfinity. However, in large data applications, MCMC can be computationally\nexpensive per iteration. This has catalyzed interest in approximating MCMC in a\nmanner that improves computational speed per iteration but does not produce\nasymptotically consistent estimates. In this article, we propose estimators\nbased on couplings of Markov chains to assess the quality of such\nasymptotically biased sampling methods. The estimators give empirical upper\nbounds of the Wasserstein distance between the limiting distribution of the\nasymptotically biased sampling method and the original target distribution of\ninterest. We establish theoretical guarantees for our upper bounds and show\nthat our estimators can remain effective in high dimensions. We apply our\nquality measures to stochastic gradient MCMC, variational Bayes, and Laplace\napproximations for tall data and to approximate MCMC for Bayesian logistic\nregression in 4500 dimensions and Bayesian linear regression in 50000\ndimensions."}, "http://arxiv.org/abs/2112.13398": {"title": "Long Story Short: Omitted Variable Bias in Causal Machine Learning", "link": "http://arxiv.org/abs/2112.13398", "description": "We derive general, yet simple, sharp bounds on the size of the omitted\nvariable bias for a broad class of causal parameters that can be identified as\nlinear functionals of the conditional expectation function of the outcome. Such\nfunctionals encompass many of the traditional targets of investigation in\ncausal inference studies, such as, for example, (weighted) average of potential\noutcomes, average treatment effects (including subgroup effects, such as the\neffect on the treated), (weighted) average derivatives, and policy effects from\nshifts in covariate distribution -- all for general, nonparametric causal\nmodels. Our construction relies on the Riesz-Frechet representation of the\ntarget functional. Specifically, we show how the bound on the bias depends only\non the additional variation that the latent variables create both in the\noutcome and in the Riesz representer for the parameter of interest. Moreover,\nin many important cases (e.g, average treatment effects and avearage\nderivatives) the bound is shown to depend on easily interpretable quantities\nthat measure the explanatory power of the omitted variables. Therefore, simple\nplausibility judgments on the maximum explanatory power of omitted variables\n(in explaining treatment and outcome variation) are sufficient to place overall\nbounds on the size of the bias. Furthermore, we use debiased machine learning\nto provide flexible and efficient statistical inference on learnable components\nof the bounds. Finally, empirical examples demonstrate the usefulness of the\napproach."}, "http://arxiv.org/abs/2204.02954": {"title": "Strongly convergent homogeneous approximations to inhomogeneous Markov jump processes and applications", "link": "http://arxiv.org/abs/2204.02954", "description": "The study of time-inhomogeneous Markov jump processes is a traditional topic\nwithin probability theory that has recently attracted substantial attention in\nvarious applications. However, their flexibility also incurs a substantial\nmathematical burden which is usually circumvented by using well-known generic\ndistributional approximations or simulations. This article provides a novel\napproximation method that tailors the dynamics of a time-homogeneous Markov\njump process to meet those of its time-inhomogeneous counterpart on an\nincreasingly fine Poisson grid. Strong convergence of the processes in terms of\nthe Skorokhod $J_1$ metric is established, and convergence rates are provided.\nUnder traditional regularity assumptions, distributional convergence is\nestablished for unconditional proxies, to the same limit. Special attention is\ndevoted to the case where the target process has one absorbing state and the\nremaining ones transient, for which the absorption times also converge. Some\napplications are outlined, such as univariate hazard-rate density estimation,\nruin probabilities, and multivariate phase-type density evaluation."}, "http://arxiv.org/abs/2301.07210": {"title": "Causal Falsification of Digital Twins", "link": "http://arxiv.org/abs/2301.07210", "description": "Digital twins are virtual systems designed to predict how a real-world\nprocess will evolve in response to interventions. This modelling paradigm holds\nsubstantial promise in many applications, but rigorous procedures for assessing\ntheir accuracy are essential for safety-critical settings. We consider how to\nassess the accuracy of a digital twin using real-world data. We formulate this\nas causal inference problem, which leads to a precise definition of what it\nmeans for a twin to be \"correct\" appropriate for many applications.\nUnfortunately, fundamental results from causal inference mean observational\ndata cannot be used to certify that a twin is correct in this sense unless\npotentially tenuous assumptions are made, such as that the data are\nunconfounded. To avoid these assumptions, we propose instead to find situations\nin which the twin is not correct, and present a general-purpose statistical\nprocedure for doing so. Our approach yields reliable and actionable information\nabout the twin under only the assumption of an i.i.d. dataset of observational\ntrajectories, and remains sound even if the data are confounded. We apply our\nmethodology to a large-scale, real-world case study involving sepsis modelling\nwithin the Pulse Physiology Engine, which we assess using the MIMIC-III dataset\nof ICU patients."}, "http://arxiv.org/abs/2301.11472": {"title": "Fast Bayesian Inference for Spatial Mean-Parameterized Conway--Maxwell--Poisson Models", "link": "http://arxiv.org/abs/2301.11472", "description": "Count data with complex features arise in many disciplines, including\necology, agriculture, criminology, medicine, and public health. Zero inflation,\nspatial dependence, and non-equidispersion are common features in count data.\nThere are two classes of models that allow for these features -- the\nmode-parameterized Conway--Maxwell--Poisson (COMP) distribution and the\ngeneralized Poisson model. However both require the use of either constraints\non the parameter space or a parameterization that leads to challenges in\ninterpretability. We propose a spatial mean-parameterized COMP model that\nretains the flexibility of these models while resolving the above issues. We\nuse a Bayesian spatial filtering approach in order to efficiently handle\nhigh-dimensional spatial data and we use reversible-jump MCMC to automatically\nchoose the basis vectors for spatial filtering. The COMP distribution poses two\nadditional computational challenges -- an intractable normalizing function in\nthe likelihood and no closed-form expression for the mean. We propose a fast\ncomputational approach that addresses these challenges by, respectively,\nintroducing an efficient auxiliary variable algorithm and pre-computing key\napproximations for fast likelihood evaluation. We illustrate the application of\nour methodology to simulated and real datasets, including Texas HPV-cancer data\nand US vaccine refusal data."}, "http://arxiv.org/abs/2305.08529": {"title": "Kernel-based Joint Independence Tests for Multivariate Stationary and Non-stationary Time Series", "link": "http://arxiv.org/abs/2305.08529", "description": "Multivariate time series data that capture the temporal evolution of\ninterconnected systems are ubiquitous in diverse areas. Understanding the\ncomplex relationships and potential dependencies among co-observed variables is\ncrucial for the accurate statistical modelling and analysis of such systems.\nHere, we introduce kernel-based statistical tests of joint independence in\nmultivariate time series by extending the $d$-variable Hilbert-Schmidt\nindependence criterion (dHSIC) to encompass both stationary and non-stationary\nprocesses, thus allowing broader real-world applications. By leveraging\nresampling techniques tailored for both single- and multiple-realisation time\nseries, we show how the method robustly uncovers significant higher-order\ndependencies in synthetic examples, including frequency mixing data and logic\ngates, as well as real-world climate, neuroscience, and socioeconomic data. Our\nmethod adds to the mathematical toolbox for the analysis of multivariate time\nseries and can aid in uncovering high-order interactions in data."}, "http://arxiv.org/abs/2306.07769": {"title": "Amortized Simulation-Based Frequentist Inference for Tractable and Intractable Likelihoods", "link": "http://arxiv.org/abs/2306.07769", "description": "High-fidelity simulators that connect theoretical models with observations\nare indispensable tools in many sciences. When coupled with machine learning, a\nsimulator makes it possible to infer the parameters of a theoretical model\ndirectly from real and simulated observations without explicit use of the\nlikelihood function. This is of particular interest when the latter is\nintractable. In this work, we introduce a simple extension of the recently\nproposed likelihood-free frequentist inference (LF2I) approach that has some\ncomputational advantages. Like LF2I, this extension yields provably valid\nconfidence sets in parameter inference problems in which a high-fidelity\nsimulator is available. The utility of our algorithm is illustrated by applying\nit to three pedagogically interesting examples: the first is from cosmology,\nthe second from high-energy physics and astronomy, both with tractable\nlikelihoods, while the third, with an intractable likelihood, is from\nepidemiology."}, "http://arxiv.org/abs/2307.05732": {"title": "Semiparametric Shape-restricted Estimators for Nonparametric Regression", "link": "http://arxiv.org/abs/2307.05732", "description": "Estimating the conditional mean function that relates predictive covariates\nto a response variable of interest is a fundamental task in economics and\nstatistics. In this manuscript, we propose some general nonparametric\nregression approaches that are widely applicable based on a simple yet\nsignificant decomposition of nonparametric functions into a semiparametric\nmodel with shape-restricted components. For instance, we observe that every\nLipschitz function can be expressed as a sum of a monotone function and a\nlinear function. We implement well-established shape-restricted estimation\nprocedures, such as isotonic regression, to handle the ``nonparametric\"\ncomponents of the true regression function and combine them with a simple\nsample-splitting procedure to estimate the parametric components. The resulting\nestimators inherit several favorable properties from the shape-restricted\nregression estimators. Notably, it is practically tuning parameter free,\nconverges at the minimax optimal rate, and exhibits an adaptive rate when the\ntrue regression function is ``simple\". We also confirm these theoretical\nproperties and compare the practice performance with existing methods via a\nseries of numerical studies."}, "http://arxiv.org/abs/2311.01470": {"title": "Preliminary Estimators of Population Mean using Ranked Set Sampling in the Presence of Measurement Error and Non-Response Error", "link": "http://arxiv.org/abs/2311.01470", "description": "In order to estimate the population mean in the presence of both non-response\nand measurement errors that are uncorrelated, the paper presents some novel\nestimators employing ranked set sampling by utilizing auxiliary information.Up\nto the first order of approximation, the equations for the bias and mean\nsquared error of the suggested estimators are produced, and it is found that\nthe proposed estimators outperform the other existing estimators analysed in\nthis study. Investigations using simulation studies and numerical examples show\nhow well the suggested estimators perform in the presence of measurement and\nnon-response errors. The relative efficiency of the suggested estimators\ncompared to the existing estimators has been expressed as a percentage, and the\nimpact of measurement errors has been expressed as a percentage computation of\nmeasurement errors."}, "http://arxiv.org/abs/2311.01484": {"title": "Comparison of methods for analyzing environmental mixtures effects on survival outcomes and application to a population-based cohort study", "link": "http://arxiv.org/abs/2311.01484", "description": "The estimation of the effect of environmental exposures and overall mixtures\non a survival time outcome is common in environmental epidemiological studies.\nWhile advanced statistical methods are increasingly being used for mixture\nanalyses, their applicability and performance for survival outcomes has yet to\nbe explored. We identified readily available methods for analyzing an\nenvironmental mixture's effect on a survival outcome and assessed their\nperformance via simulations replicating various real-life scenarios. Using\nprespecified criteria, we selected Bayesian Additive Regression Trees (BART),\nCox Elastic Net, Cox Proportional Hazards (PH) with and without penalized\nsplines, Gaussian Process Regression (GPR) and Multivariate Adaptive Regression\nSplines (MARS) to compare the bias and efficiency produced when estimating\nindividual exposure, overall mixture, and interaction effects on a survival\noutcome. We illustrate the selected methods in a real-world data application.\nWe estimated the effects of arsenic, cadmium, molybdenum, selenium, tungsten,\nand zinc on incidence of cardiovascular disease in American Indians using data\nfrom the Strong Heart Study (SHS). In the simulation study, there was a\nconsistent bias-variance trade off. The more flexible models (BART, GPR and\nMARS) were found to be most advantageous in the presence of nonproportional\nhazards, where the Cox models often did not capture the true effects due to\ntheir higher bias and lower variance. In the SHS, estimates of the effect of\nselenium and the overall mixture indicated negative effects, but the magnitudes\nof the estimated effects varied across methods. In practice, we recommend\nevaluating if findings are consistent across methods."}, "http://arxiv.org/abs/2311.01485": {"title": "Subgroup identification using individual participant data from multiple trials on low back pain", "link": "http://arxiv.org/abs/2311.01485", "description": "Model-based recursive partitioning (MOB) and its extension, metaMOB, are\npotent tools for identifying subgroups with differential treatment effects. In\nthe metaMOB approach random effects are used to model heterogeneity of the\ntreatment effects when pooling data from various trials. In situations where\ninterventions offer only small overall benefits and require extensive, costly\ntrials with a large participant enrollment, leveraging individual-participant\ndata (IPD) from multiple trials can help identify individuals who are most\nlikely to benefit from the intervention. We explore the application of MOB and\nmetaMOB in the context of non specific low back pain treatment, using\nsynthesized data based on a subset of the individual participant data\nmeta-analysis by Patel et al. Our study underscores the need to explore\nheterogeneity in intercepts and treatment effects to identify subgroups with\ndifferential treatment effects in IPD meta-analyses."}, "http://arxiv.org/abs/2311.01538": {"title": "A reluctant additive model framework for interpretable nonlinear individualized treatment rules", "link": "http://arxiv.org/abs/2311.01538", "description": "Individualized treatment rules (ITRs) for treatment recommendation is an\nimportant topic for precision medicine as not all beneficial treatments work\nwell for all individuals. Interpretability is a desirable property of ITRs, as\nit helps practitioners make sense of treatment decisions, yet there is a need\nfor ITRs to be flexible to effectively model complex biomedical data for\ntreatment decision making. Many ITR approaches either focus on linear ITRs,\nwhich may perform poorly when true optimal ITRs are nonlinear, or black-box\nnonlinear ITRs, which may be hard to interpret and can be overly complex. This\ndilemma indicates a tension between interpretability and accuracy of treatment\ndecisions. Here we propose an additive model-based nonlinear ITR learning\nmethod that balances interpretability and flexibility of the ITR. Our approach\naims to strike this balance by allowing both linear and nonlinear terms of the\ncovariates in the final ITR. Our approach is parsimonious in that the nonlinear\nterm is included in the final ITR only when it substantially improves the ITR\nperformance. To prevent overfitting, we combine cross-fitting and a specialized\ninformation criterion for model selection. Through extensive simulations, we\nshow that our methods are data-adaptive to the degree of nonlinearity and can\nfavorably balance ITR interpretability and flexibility. We further demonstrate\nthe robust performance of our methods with an application to a cancer drug\nsensitive study."}, "http://arxiv.org/abs/2311.01596": {"title": "Local Bayesian Dirichlet mixing of imperfect models", "link": "http://arxiv.org/abs/2311.01596", "description": "To improve the predictability of complex computational models in the\nexperimentally-unknown domains, we propose a Bayesian statistical machine\nlearning framework utilizing the Dirichlet distribution that combines results\nof several imperfect models. This framework can be viewed as an extension of\nBayesian stacking. To illustrate the method, we study the ability of Bayesian\nmodel averaging and mixing techniques to mine nuclear masses. We show that the\nglobal and local mixtures of models reach excellent performance on both\nprediction accuracy and uncertainty quantification and are preferable to\nclassical Bayesian model averaging. Additionally, our statistical analysis\nindicates that improving model predictions through mixing rather than mixing of\ncorrected models leads to more robust extrapolations."}, "http://arxiv.org/abs/2311.01625": {"title": "Topological inference on brain networks across subtypes of post-stroke aphasia", "link": "http://arxiv.org/abs/2311.01625", "description": "Persistent homology (PH) characterizes the shape of brain networks through\nthe persistence features. Group comparison of persistence features from brain\nnetworks can be challenging as they are inherently heterogeneous. A recent\nscale-space representation of persistence diagram (PD) through heat diffusion\nreparameterizes using the finite number of Fourier coefficients with respect to\nthe Laplace-Beltrami (LB) eigenfunction expansion of the domain, which provides\na powerful vectorized algebraic representation for group comparisons of PDs. In\nthis study, we advance a transposition-based permutation test for comparing\nmultiple groups of PDs through the heat-diffusion estimates of the PDs. We\nevaluate the empirical performance of the spectral transposition test in\ncapturing within- and between-group similarity and dissimilarity with respect\nto statistical variation of topological noise and hole location. We also\nillustrate how the method extends naturally into a clustering scheme by\nsubtyping individuals with post-stroke aphasia through the PDs of their\nresting-state functional brain networks."}, "http://arxiv.org/abs/2311.01638": {"title": "Inference on summaries of a model-agnostic longitudinal variable importance trajectory", "link": "http://arxiv.org/abs/2311.01638", "description": "In prediction settings where data are collected over time, it is often of\ninterest to understand both the importance of variables for predicting the\nresponse at each time point and the importance summarized over the time series.\nBuilding on recent advances in estimation and inference for variable importance\nmeasures, we define summaries of variable importance trajectories. These\nmeasures can be estimated and the same approaches for inference can be applied\nregardless of the choice of the algorithm(s) used to estimate the prediction\nfunction. We propose a nonparametric efficient estimation and inference\nprocedure as well as a null hypothesis testing procedure that are valid even\nwhen complex machine learning tools are used for prediction. Through\nsimulations, we demonstrate that our proposed procedures have good operating\ncharacteristics, and we illustrate their use by investigating the longitudinal\nimportance of risk factors for suicide attempt."}, "http://arxiv.org/abs/2311.01681": {"title": "The R", "link": "http://arxiv.org/abs/2311.01681", "description": "We propose a prognostic stratum matching framework that addresses the\ndeficiencies of Randomized trial data subgroup analysis and transforms\nObservAtional Data to be used as if they were randomized, thus paving the road\nfor precision medicine. Our approach counters the effects of unobserved\nconfounding in observational data by correcting the estimated probabilities of\nthe outcome under a treatment through a novel two-step process. These\nprobabilities are then used to train Optimal Policy Trees (OPTs), which are\ndecision trees that optimally assign treatments to subgroups of patients based\non their characteristics. This facilitates the creation of clinically intuitive\ntreatment recommendations. We applied our framework to observational data of\npatients with gastrointestinal stromal tumors (GIST) and validated the OPTs in\nan external cohort using the sensitivity and specificity metrics. We show that\nthese recommendations outperformed those of experts in GIST. We further applied\nthe same framework to randomized clinical trial (RCT) data of patients with\nextremity sarcomas. Remarkably, despite the initial trial results suggesting\nthat all patients should receive treatment, our framework, after addressing\nimbalances in patient distribution due to the trial's small sample size,\nidentified through the OPTs a subset of patients with unique characteristics\nwho may not require treatment. Again, we successfully validated our\nrecommendations in an external cohort."}, "http://arxiv.org/abs/2311.01709": {"title": "Causal inference with Machine Learning-Based Covariate Representation", "link": "http://arxiv.org/abs/2311.01709", "description": "Utilizing covariate information has been a powerful approach to improve the\nefficiency and accuracy for causal inference, which support massive amount of\nrandomized experiments run on data-driven enterprises. However, state-of-art\napproaches can become practically unreliable when the dimension of covariate\nincreases to just 50, whereas experiments on large platforms can observe even\nhigher dimension of covariate. We propose a machine-learning-assisted covariate\nrepresentation approach that can effectively make use of historical experiment\nor observational data that are run on the same platform to understand which\nlower dimensions can effectively represent the higher-dimensional covariate. We\nthen propose design and estimation methods with the covariate representation.\nWe prove statistically reliability and performance guarantees for the proposed\nmethods. The empirical performance is demonstrated using numerical experiments."}, "http://arxiv.org/abs/2311.01762": {"title": "Solving Kernel Ridge Regression with Gradient Descent for a Non-Constant Kernel", "link": "http://arxiv.org/abs/2311.01762", "description": "Kernel ridge regression, KRR, is a generalization of linear ridge regression\nthat is non-linear in the data, but linear in the parameters. The solution can\nbe obtained either as a closed-form solution, which includes a matrix\ninversion, or iteratively through gradient descent. Using the iterative\napproach opens up for changing the kernel during training, something that is\ninvestigated in this paper. We theoretically address the effects this has on\nmodel complexity and generalization. Based on our findings, we propose an\nupdate scheme for the bandwidth of translational-invariant kernels, where we\nlet the bandwidth decrease to zero during training, thus circumventing the need\nfor hyper-parameter selection. We demonstrate on real and synthetic data how\ndecreasing the bandwidth during training outperforms using a constant\nbandwidth, selected by cross-validation and marginal likelihood maximization.\nWe also show theoretically and empirically that using a decreasing bandwidth,\nwe are able to achieve both zero training error in combination with good\ngeneralization, and a double descent behavior, phenomena that do not occur for\nKRR with constant bandwidth but are known to appear for neural networks."}, "http://arxiv.org/abs/2311.01833": {"title": "Similarity network aggregation for the analysis of glacier ecosystems", "link": "http://arxiv.org/abs/2311.01833", "description": "The synthesis of information deriving from complex networks is a topic\nreceiving increasing relevance in ecology and environmental sciences. In\nparticular, the aggregation of multilayer networks, i.e. network structures\nformed by multiple interacting networks (the layers), constitutes a\nfast-growing field. In several environmental applications, the layers of a\nmultilayer network are modelled as a collection of similarity matrices\ndescribing how similar pairs of biological entities are, based on different\ntypes of features (e.g. biological traits). The present paper first discusses\ntwo main techniques for combining the multi-layered information into a single\nnetwork (the so-called monoplex), i.e. Similarity Network Fusion (SNF) and\nSimilarity Matrix Average (SMA). Then, the effectiveness of the two methods is\ntested on a real-world dataset of the relative abundance of microbial species\nin the ecosystems of nine glaciers (four glaciers in the Alps and five in the\nAndes). A preliminary clustering analysis on the monoplexes obtained with\ndifferent methods shows the emergence of a tightly connected community formed\nby species that are typical of cryoconite holes worldwide. Moreover, the\nweights assigned to different layers by the SMA algorithm suggest that two\nlarge South American glaciers (Exploradores and Perito Moreno) are structurally\ndifferent from the smaller glaciers in both Europe and South America. Overall,\nthese results highlight the importance of integration methods in the discovery\nof the underlying organizational structure of biological entities in multilayer\necological networks."}, "http://arxiv.org/abs/2311.01872": {"title": "The use of restricted mean survival time to estimate treatment effect under model misspecification, a simulation study", "link": "http://arxiv.org/abs/2311.01872", "description": "The use of the non-parametric Restricted Mean Survival Time endpoint (RMST)\nhas grown in popularity as trialists look to analyse time-to-event outcomes\nwithout the restrictions of the proportional hazards assumption. In this paper,\nwe evaluate the power and type I error rate of the parametric and\nnon-parametric RMST estimators when treatment effect is explained by multiple\ncovariates, including an interaction term. Utilising the RMST estimator in this\nway allows the combined treatment effect to be summarised as a one-dimensional\nestimator, which is evaluated using a one-sided hypothesis Z-test. The\nestimators are either fully specified or misspecified, both in terms of\nunaccounted covariates or misspecified knot points (where trials exhibit\ncrossing survival curves). A placebo-controlled trial of Gamma interferon is\nused as a motivating example to simulate associated survival times. When\ncorrectly specified, the parametric RMST estimator has the greatest power,\nregardless of the time of analysis. The misspecified RMST estimator generally\nperforms similarly when covariates mirror those of the fitted case study\ndataset. However, as the magnitude of the unaccounted covariate increases, the\nassociated power of the estimator decreases. In all cases, the non-parametric\nRMST estimator has the lowest power, and power remains very reliant on the time\nof analysis (with a later analysis time correlated with greater power)."}, "http://arxiv.org/abs/2311.01902": {"title": "High Precision Causal Model Evaluation with Conditional Randomization", "link": "http://arxiv.org/abs/2311.01902", "description": "The gold standard for causal model evaluation involves comparing model\npredictions with true effects estimated from randomized controlled trials\n(RCT). However, RCTs are not always feasible or ethical to perform. In\ncontrast, conditionally randomized experiments based on inverse probability\nweighting (IPW) offer a more realistic approach but may suffer from high\nestimation variance. To tackle this challenge and enhance causal model\nevaluation in real-world conditional randomization settings, we introduce a\nnovel low-variance estimator for causal error, dubbed as the pairs estimator.\nBy applying the same IPW estimator to both the model and true experimental\neffects, our estimator effectively cancels out the variance due to IPW and\nachieves a smaller asymptotic variance. Empirical studies demonstrate the\nimproved of our estimator, highlighting its potential on achieving near-RCT\nperformance. Our method offers a simple yet powerful solution to evaluate\ncausal inference models in conditional randomization settings without\ncomplicated modification of the IPW estimator itself, paving the way for more\nrobust and reliable model assessments."}, "http://arxiv.org/abs/2311.01913": {"title": "Extended Relative Power Contribution that Allows to Evaluate the Effect of Correlated Noise", "link": "http://arxiv.org/abs/2311.01913", "description": "We proposed an extension of Akaike's relative power contribution that could\nbe applied to data with correlations between noises. This method decomposes the\npower spectrum into a contribution of the terms caused by correlation between\ntwo noises, in addition to the contributions of the independent noises.\nNumerical examples confirm that some of the correlated noise has the effect of\nreducing the power spectrum."}, "http://arxiv.org/abs/2311.02019": {"title": "Reproducible Parameter Inference Using Bagged Posteriors", "link": "http://arxiv.org/abs/2311.02019", "description": "Under model misspecification, it is known that Bayesian posteriors often do\nnot properly quantify uncertainty about true or pseudo-true parameters. Even\nmore fundamentally, misspecification leads to a lack of reproducibility in the\nsense that the same model will yield contradictory posteriors on independent\ndata sets from the true distribution. To define a criterion for reproducible\nuncertainty quantification under misspecification, we consider the probability\nthat two confidence sets constructed from independent data sets have nonempty\noverlap, and we establish a lower bound on this overlap probability that holds\nfor any valid confidence sets. We prove that credible sets from the standard\nposterior can strongly violate this bound, particularly in high-dimensional\nsettings (i.e., with dimension increasing with sample size), indicating that it\nis not internally coherent under misspecification. To improve reproducibility\nin an easy-to-use and widely applicable way, we propose to apply bagging to the\nBayesian posterior (\"BayesBag\"'); that is, to use the average of posterior\ndistributions conditioned on bootstrapped datasets. We motivate BayesBag from\nfirst principles based on Jeffrey conditionalization and show that the bagged\nposterior typically satisfies the overlap lower bound. Further, we prove a\nBernstein--Von Mises theorem for the bagged posterior, establishing its\nasymptotic normal distribution. We demonstrate the benefits of BayesBag via\nsimulation experiments and an application to crime rate prediction."}, "http://arxiv.org/abs/2311.02043": {"title": "Bayesian Quantile Regression with Subset Selection: A Posterior Summarization Perspective", "link": "http://arxiv.org/abs/2311.02043", "description": "Quantile regression is a powerful tool for inferring how covariates affect\nspecific percentiles of the response distribution. Existing methods either\nestimate conditional quantiles separately for each quantile of interest or\nestimate the entire conditional distribution using semi- or non-parametric\nmodels. The former often produce inadequate models for real data and do not\nshare information across quantiles, while the latter are characterized by\ncomplex and constrained models that can be difficult to interpret and\ncomputationally inefficient. Further, neither approach is well-suited for\nquantile-specific subset selection. Instead, we pose the fundamental problems\nof linear quantile estimation, uncertainty quantification, and subset selection\nfrom a Bayesian decision analysis perspective. For any Bayesian regression\nmodel, we derive optimal and interpretable linear estimates and uncertainty\nquantification for each model-based conditional quantile. Our approach\nintroduces a quantile-focused squared error loss, which enables efficient,\nclosed-form computing and maintains a close relationship with Wasserstein-based\ndensity estimation. In an extensive simulation study, our methods demonstrate\nsubstantial gains in quantile estimation accuracy, variable selection, and\ninference over frequentist and Bayesian competitors. We apply these tools to\nidentify the quantile-specific impacts of social and environmental stressors on\neducational outcomes for a large cohort of children in North Carolina."}, "http://arxiv.org/abs/2010.08627": {"title": "Minimax Quasi-Bayesian estimation in sparse canonical correlation analysis via a Rayleigh quotient function", "link": "http://arxiv.org/abs/2010.08627", "description": "Canonical correlation analysis (CCA) is a popular statistical technique for\nexploring relationships between datasets. In recent years, the estimation of\nsparse canonical vectors has emerged as an important but challenging variant of\nthe CCA problem, with widespread applications. Unfortunately, existing\nrate-optimal estimators for sparse canonical vectors have high computational\ncost. We propose a quasi-Bayesian estimation procedure that not only achieves\nthe minimax estimation rate, but also is easy to compute by Markov Chain Monte\nCarlo (MCMC). The method builds on Tan et al. (2018) and uses a re-scaled\nRayleigh quotient function as the quasi-log-likelihood. However, unlike Tan et\nal. (2018), we adopt a Bayesian framework that combines this\nquasi-log-likelihood with a spike-and-slab prior to regularize the inference\nand promote sparsity. We investigate the empirical behavior of the proposed\nmethod on both continuous and truncated data, and we demonstrate that it\noutperforms several state-of-the-art methods. As an application, we use the\nproposed methodology to maximally correlate clinical variables and proteomic\ndata for better understanding the Covid-19 disease."}, "http://arxiv.org/abs/2104.08300": {"title": "Semiparametric Sensitivity Analysis: Unmeasured Confounding In Observational Studies", "link": "http://arxiv.org/abs/2104.08300", "description": "Establishing cause-effect relationships from observational data often relies\non untestable assumptions. It is crucial to know whether, and to what extent,\nthe conclusions drawn from non-experimental studies are robust to potential\nunmeasured confounding. In this paper, we focus on the average causal effect\n(ACE) as our target of inference. We generalize the sensitivity analysis\napproach developed by Robins et al. (2000), Franks et al. (2020) and Zhou and\nYao (2023. We use semiparametric theory to derive the non-parametric efficient\ninfluence function of the ACE, for fixed sensitivity parameters. We use this\ninfluence function to construct a one-step bias-corrected estimator of the ACE.\nOur estimator depends on semiparametric models for the distribution of the\nobserved data; importantly, these models do not impose any restrictions on the\nvalues of sensitivity analysis parameters. We establish sufficient conditions\nensuring that our estimator has root-n asymptotics. We use our methodology to\nevaluate the causal effect of smoking during pregnancy on birth weight. We also\nevaluate the performance of estimation procedure in a simulation study."}, "http://arxiv.org/abs/2206.04157": {"title": "Inference for Matched Tuples and Fully Blocked Factorial Designs", "link": "http://arxiv.org/abs/2206.04157", "description": "This paper studies inference in randomized controlled trials with multiple\ntreatments, where treatment status is determined according to a \"matched\ntuples\" design. Here, by a matched tuples design, we mean an experimental\ndesign where units are sampled i.i.d. from the population of interest, grouped\ninto \"homogeneous\" blocks with cardinality equal to the number of treatments,\nand finally, within each block, each treatment is assigned exactly once\nuniformly at random. We first study estimation and inference for matched tuples\ndesigns in the general setting where the parameter of interest is a vector of\nlinear contrasts over the collection of average potential outcomes for each\ntreatment. Parameters of this form include standard average treatment effects\nused to compare one treatment relative to another, but also include parameters\nwhich may be of interest in the analysis of factorial designs. We first\nestablish conditions under which a sample analogue estimator is asymptotically\nnormal and construct a consistent estimator of its corresponding asymptotic\nvariance. Combining these results establishes the asymptotic exactness of tests\nbased on these estimators. In contrast, we show that, for two common testing\nprocedures based on t-tests constructed from linear regressions, one test is\ngenerally conservative while the other generally invalid. We go on to apply our\nresults to study the asymptotic properties of what we call \"fully-blocked\" 2^K\nfactorial designs, which are simply matched tuples designs applied to a full\nfactorial experiment. Leveraging our previous results, we establish that our\nestimator achieves a lower asymptotic variance under the fully-blocked design\nthan that under any stratified factorial design which stratifies the\nexperimental sample into a finite number of \"large\" strata. A simulation study\nand empirical application illustrate the practical relevance of our results."}, "http://arxiv.org/abs/2207.00100": {"title": "A Bayesian 'sandwich' for variance estimation", "link": "http://arxiv.org/abs/2207.00100", "description": "Large-sample Bayesian analogs exist for many frequentist methods, but are\nless well-known for the widely-used 'sandwich' or 'robust' variance estimates.\nWe review existing approaches to Bayesian analogs of sandwich variance\nestimates and propose a new analog, as the Bayes rule under a form of balanced\nloss function, that combines elements of standard parametric inference with\nfidelity of the data to the model. Our development is general, for essentially\nany regression setting with independent outcomes. Being the large-sample\nequivalent of its frequentist counterpart, we show by simulation that Bayesian\nrobust standard error estimates can faithfully quantify the variability of\nparameter estimates even under model misspecification -- thus retaining the\nmajor attraction of the original frequentist version. We demonstrate our\nBayesian analog of standard error estimates when studying the association\nbetween age and systolic blood pressure in NHANES."}, "http://arxiv.org/abs/2210.17514": {"title": "Cost-aware Generalized $\\alpha$-investing for Multiple Hypothesis Testing", "link": "http://arxiv.org/abs/2210.17514", "description": "We consider the problem of sequential multiple hypothesis testing with\nnontrivial data collection costs. This problem appears, for example, when\nconducting biological experiments to identify differentially expressed genes of\na disease process. This work builds on the generalized $\\alpha$-investing\nframework which enables control of the false discovery rate in a sequential\ntesting setting. We make a theoretical analysis of the long term asymptotic\nbehavior of $\\alpha$-wealth which motivates a consideration of sample size in\nthe $\\alpha$-investing decision rule. Posing the testing process as a game with\nnature, we construct a decision rule that optimizes the expected\n$\\alpha$-wealth reward (ERO) and provides an optimal sample size for each test.\nEmpirical results show that a cost-aware ERO decision rule correctly rejects\nmore false null hypotheses than other methods for $n=1$ where $n$ is the sample\nsize. When the sample size is not fixed cost-aware ERO uses a prior on the null\nhypothesis to adaptively allocate of the sample budget to each test. We extend\ncost-aware ERO investing to finite-horizon testing which enables the decision\nrule to allocate samples in a non-myopic manner. Finally, empirical tests on\nreal data sets from biological experiments show that cost-aware ERO balances\nthe allocation of samples to an individual test against the allocation of\nsamples across multiple tests."}, "http://arxiv.org/abs/2301.01480": {"title": "A new over-dispersed count model", "link": "http://arxiv.org/abs/2301.01480", "description": "A new two-parameter discrete distribution, namely the PoiG distribution is\nderived by the convolution of a Poisson variate and an independently\ndistributed geometric random variable. This distribution generalizes both the\nPoisson and geometric distributions and can be used for modelling\nover-dispersed as well as equi-dispersed count data. A number of important\nstatistical properties of the proposed count model, such as the probability\ngenerating function, the moment generating function, the moments, the survival\nfunction and the hazard rate function. Monotonic properties are studied, such\nas the log concavity and the stochastic ordering are also investigated in\ndetail. Method of moment and the maximum likelihood estimators of the\nparameters of the proposed model are presented. It is envisaged that the\nproposed distribution may prove to be useful for the practitioners for\nmodelling over-dispersed count data compared to its closest competitors."}, "http://arxiv.org/abs/2305.10050": {"title": "The Impact of Missing Data on Causal Discovery: A Multicentric Clinical Study", "link": "http://arxiv.org/abs/2305.10050", "description": "Causal inference for testing clinical hypotheses from observational data\npresents many difficulties because the underlying data-generating model and the\nassociated causal graph are not usually available. Furthermore, observational\ndata may contain missing values, which impact the recovery of the causal graph\nby causal discovery algorithms: a crucial issue often ignored in clinical\nstudies. In this work, we use data from a multi-centric study on endometrial\ncancer to analyze the impact of different missingness mechanisms on the\nrecovered causal graph. This is achieved by extending state-of-the-art causal\ndiscovery algorithms to exploit expert knowledge without sacrificing\ntheoretical soundness. We validate the recovered graph with expert physicians,\nshowing that our approach finds clinically-relevant solutions. Finally, we\ndiscuss the goodness of fit of our graph and its consistency from a clinical\ndecision-making perspective using graphical separation to validate causal\npathways."}, "http://arxiv.org/abs/2309.03952": {"title": "The Causal Roadmap and simulation studies to inform the Statistical Analysis Plan for real-data applications", "link": "http://arxiv.org/abs/2309.03952", "description": "The Causal Roadmap outlines a systematic approach to our research endeavors:\ndefine quantity of interest, evaluate needed assumptions, conduct statistical\nestimation, and carefully interpret of results. At the estimation step, it is\nessential that the estimation algorithm be chosen thoughtfully for its\ntheoretical properties and expected performance. Simulations can help\nresearchers gain a better understanding of an estimator's statistical\nperformance under conditions unique to the real-data application. This in turn\ncan inform the rigorous pre-specification of a Statistical Analysis Plan (SAP),\nnot only stating the estimand (e.g., G-computation formula), the estimator\n(e.g., targeted minimum loss-based estimation [TMLE]), and adjustment\nvariables, but also the implementation of the estimator -- including nuisance\nparameter estimation and approach for variance estimation. Doing so helps\nensure valid inference (e.g., 95% confidence intervals with appropriate\ncoverage). Failing to pre-specify estimation can lead to data dredging and\ninflated Type-I error rates."}, "http://arxiv.org/abs/2311.02273": {"title": "A Sequential Learning Procedure with Applications to Online Sales Examination", "link": "http://arxiv.org/abs/2311.02273", "description": "In this paper, we consider the problem of estimating parameters in a linear\nregression model. We propose a sequential learning procedure to determine the\nsample size for achieving a given small estimation risk, under the widely used\nGauss-Markov setup with independent normal errors. The procedure is proven to\nenjoy the second-order efficiency and risk-efficiency properties, which are\nvalidated through Monte Carlo simulation studies. Using e-commerce data, we\nimplement the procedure to examine the influential factors of online sales."}, "http://arxiv.org/abs/2311.02299": {"title": "The Fragility of Sparsity", "link": "http://arxiv.org/abs/2311.02299", "description": "We show, using three empirical applications, that linear regression estimates\nwhich rely on the assumption of sparsity are fragile in two ways. First, we\ndocument that different choices of the regressor matrix that don't impact\nordinary least squares (OLS) estimates, such as the choice of baseline category\nwith categorical controls, can move sparsity-based estimates two standard\nerrors or more. Second, we develop two tests of the sparsity assumption based\non comparing sparsity-based estimators with OLS. The tests tend to reject the\nsparsity assumption in all three applications. Unless the number of regressors\nis comparable to or exceeds the sample size, OLS yields more robust results at\nlittle efficiency cost."}, "http://arxiv.org/abs/2311.02306": {"title": "Heteroskedastic Tensor Clustering", "link": "http://arxiv.org/abs/2311.02306", "description": "Tensor clustering, which seeks to extract underlying cluster structures from\nnoisy tensor observations, has gained increasing attention. One extensively\nstudied model for tensor clustering is the tensor block model, which postulates\nthe existence of clustering structures along each mode and has found broad\napplications in areas like multi-tissue gene expression analysis and multilayer\nnetwork analysis. However, currently available computationally feasible methods\nfor tensor clustering either are limited to handling i.i.d. sub-Gaussian noise\nor suffer from suboptimal statistical performance, which restrains their\nutility in applications that have to deal with heteroskedastic data and/or low\nsignal-to-noise-ratio (SNR).\n\nTo overcome these challenges, we propose a two-stage method, named\n$\\mathsf{High\\text{-}order~HeteroClustering}$ ($\\mathsf{HHC}$), which starts by\nperforming tensor subspace estimation via a novel spectral algorithm called\n$\\mathsf{Thresholded~Deflated\\text{-}HeteroPCA}$, followed by approximate\n$k$-means to obtain cluster nodes. Encouragingly, our algorithm provably\nachieves exact clustering as long as the SNR exceeds the computational limit\n(ignoring logarithmic factors); here, the SNR refers to the ratio of the\npairwise disparity between nodes to the noise level, and the computational\nlimit indicates the lowest SNR that enables exact clustering with polynomial\nruntime. Comprehensive simulation and real-data experiments suggest that our\nalgorithm outperforms existing algorithms across various settings, delivering\nmore reliable clustering performance."}, "http://arxiv.org/abs/2311.02308": {"title": "Kernel-based sensitivity indices for any model behavior and screening", "link": "http://arxiv.org/abs/2311.02308", "description": "Complex models are often used to understand interactions and drivers of\nhuman-induced and/or natural phenomena. It is worth identifying the input\nvariables that drive the model output(s) in a given domain and/or govern\nspecific model behaviors such as contextual indicators based on\nsocio-environmental models. Using the theory of multivariate weighted\ndistributions to characterize specific model behaviors, we propose new measures\nof association between inputs and such behaviors. Our measures rely on\nsensitivity functionals (SFs) and kernel methods, including variance-based\nsensitivity analysis. The proposed $\\ell_1$-based kernel indices account for\ninteractions among inputs, higher-order moments of SFs, and their upper bounds\nare somehow equivalent to the Morris-type screening measures, including\ndependent elementary effects. Empirical kernel-based indices are derived,\nincluding their statistical properties for the computational issues, and\nnumerical results are provided."}, "http://arxiv.org/abs/2311.02312": {"title": "Efficient Change Point Detection and Estimation in High-Dimensional Correlation Matrices", "link": "http://arxiv.org/abs/2311.02312", "description": "This paper considers the problems of detecting a change point and estimating\nthe location in the correlation matrices of a sequence of high-dimensional\nvectors, where the dimension is large enough to be comparable to the sample\nsize or even much larger. A new break test is proposed based on signflip\nparallel analysis to detect the existence of change points. Furthermore, a\ntwo-step approach combining a signflip permutation dimension reduction step and\na CUSUM statistic is proposed to estimate the change point's location and\nrecover the support of changes. The consistency of the estimator is\nconstructed. Simulation examples and real data applications illustrate the\nsuperior empirical performance of the proposed methods. Especially, the\nproposed methods outperform existing ones for non-Gaussian data and the change\npoint in the extreme tail of a sequence and become more accurate as the\ndimension p increases. Supplementary materials for this article are available\nonline."}, "http://arxiv.org/abs/2311.02450": {"title": "Factor-guided estimation of large covariance matrix function with conditional functional sparsity", "link": "http://arxiv.org/abs/2311.02450", "description": "This paper addresses the fundamental task of estimating covariance matrix\nfunctions for high-dimensional functional data/functional time series. We\nconsider two functional factor structures encompassing either functional\nfactors with scalar loadings or scalar factors with functional loadings, and\npostulate functional sparsity on the covariance of idiosyncratic errors after\ntaking out the common unobserved factors. To facilitate estimation, we rely on\nthe spiked matrix model and its functional generalization, and derive some\nnovel asymptotic identifiability results, based on which we develop DIGIT and\nFPOET estimators under two functional factor models, respectively. Both\nestimators involve performing associated eigenanalysis to estimate the\ncovariance of common components, followed by adaptive functional thresholding\napplied to the residual covariance. We also develop functional information\ncriteria for the purpose of model selection. The convergence rates of estimated\nfactors, loadings, and conditional sparse covariance matrix functions under\nvarious functional matrix norms, are respectively established for DIGIT and\nFPOET estimators. Numerical studies including extensive simulations and two\nreal data applications on mortality rates and functional portfolio allocation\nare conducted to examine the finite-sample performance of the proposed\nmethodology."}, "http://arxiv.org/abs/2311.02467": {"title": "Individualized Policy Evaluation and Learning under Clustered Network Interference", "link": "http://arxiv.org/abs/2311.02467", "description": "While there now exists a large literature on policy evaluation and learning,\nmuch of prior work assumes that the treatment assignment of one unit does not\naffect the outcome of another unit. Unfortunately, ignoring interference may\nlead to biased policy evaluation and yield ineffective learned policies. For\nexample, treating influential individuals who have many friends can generate\npositive spillover effects, thereby improving the overall performance of an\nindividualized treatment rule (ITR). We consider the problem of evaluating and\nlearning an optimal ITR under clustered network (or partial) interference where\nclusters of units are sampled from a population and units may influence one\nanother within each cluster. Under this model, we propose an estimator that can\nbe used to evaluate the empirical performance of an ITR. We show that this\nestimator is substantially more efficient than the standard inverse probability\nweighting estimator, which does not impose any assumption about spillover\neffects. We derive the finite-sample regret bound for a learned ITR, showing\nthat the use of our efficient evaluation estimator leads to the improved\nperformance of learned policies. Finally, we conduct simulation and empirical\nstudies to illustrate the advantages of the proposed methodology."}, "http://arxiv.org/abs/2311.02532": {"title": "Optimal Treatment Allocation for Efficient Policy Evaluation in Sequential Decision Making", "link": "http://arxiv.org/abs/2311.02532", "description": "A/B testing is critical for modern technological companies to evaluate the\neffectiveness of newly developed products against standard baselines. This\npaper studies optimal designs that aim to maximize the amount of information\nobtained from online experiments to estimate treatment effects accurately. We\npropose three optimal allocation strategies in a dynamic setting where\ntreatments are sequentially assigned over time. These strategies are designed\nto minimize the variance of the treatment effect estimator when data follow a\nnon-Markov decision process or a (time-varying) Markov decision process. We\nfurther develop estimation procedures based on existing off-policy evaluation\n(OPE) methods and conduct extensive experiments in various environments to\ndemonstrate the effectiveness of the proposed methodologies. In theory, we\nprove the optimality of the proposed treatment allocation design and establish\nupper bounds for the mean squared errors of the resulting treatment effect\nestimators."}, "http://arxiv.org/abs/2311.02543": {"title": "Pairwise likelihood estimation and limited information goodness-of-fit test statistics for binary factor analysis models under complex survey sampling", "link": "http://arxiv.org/abs/2311.02543", "description": "This paper discusses estimation and limited information goodness-of-fit test\nstatistics in factor models for binary data using pairwise likelihood\nestimation and sampling weights. The paper extends the applicability of\npairwise likelihood estimation for factor models with binary data to\naccommodate complex sampling designs. Additionally, it introduces two key\nlimited information test statistics: the Pearson chi-squared test and the Wald\ntest. To enhance computational efficiency, the paper introduces modifications\nto both test statistics. The performance of the estimation and the proposed\ntest statistics under simple random sampling and unequal probability sampling\nis evaluated using simulated data."}, "http://arxiv.org/abs/2311.02574": {"title": "Semi-supervised Estimation of Event Rate with Doubly-censored Survival Data", "link": "http://arxiv.org/abs/2311.02574", "description": "Electronic Health Record (EHR) has emerged as a valuable source of data for\ntranslational research. To leverage EHR data for risk prediction and\nsubsequently clinical decision support, clinical endpoints are often time to\nonset of a clinical condition of interest. Precise information on clinical\nevent times is often not directly available and requires labor-intensive manual\nchart review to ascertain. In addition, events may occur outside of the\nhospital system, resulting in both left and right censoring often termed double\ncensoring. On the other hand, proxies such as time to the first diagnostic code\nare readily available yet with varying degrees of accuracy. Using error-prone\nevent times derived from these proxies can lead to biased risk estimates while\nonly relying on manually annotated event times, which are typically only\navailable for a small subset of patients, can lead to high variability. This\nsignifies the need for semi-supervised estimation methods that can efficiently\ncombine information from both the small subset of labeled observations and a\nlarge size of surrogate proxies. While semi-supervised estimation methods have\nbeen recently developed for binary and right-censored data, no methods\ncurrently exist in the presence of double censoring. This paper fills the gap\nby developing a robust and efficient Semi-supervised Estimation of Event rate\nwith Doubly-censored Survival data (SEEDS) by leveraging a small set of gold\nstandard labels and a large set of surrogate features. Under regularity\nconditions, we demonstrate that the proposed SEEDS estimator is consistent and\nasymptotically normal. Simulation results illustrate that SEEDS performs well\nin finite samples and can be substantially more efficient compared to the\nsupervised counterpart. We apply the SEEDS to estimate the age-specific\nsurvival rate of type 2 diabetes using EHR data from Mass General Brigham."}, "http://arxiv.org/abs/2311.02610": {"title": "An adaptive standardisation model for Day-Ahead electricity price forecasting", "link": "http://arxiv.org/abs/2311.02610", "description": "The study of Day-Ahead prices in the electricity market is one of the most\npopular problems in time series forecasting. Previous research has focused on\nemploying increasingly complex learning algorithms to capture the sophisticated\ndynamics of the market. However, there is a threshold where increased\ncomplexity fails to yield substantial improvements. In this work, we propose an\nalternative approach by introducing an adaptive standardisation to mitigate the\neffects of dataset shifts that commonly occur in the market. By doing so,\nlearning algorithms can prioritize uncovering the true relationship between the\ntarget variable and the explanatory variables. We investigate four distinct\nmarkets, including two novel datasets, previously unexplored in the literature.\nThese datasets provide a more realistic representation of the current market\ncontext, that conventional datasets do not show. The results demonstrate a\nsignificant improvement across all four markets, using learning algorithms that\nare less complex yet widely accepted in the literature. This significant\nadvancement unveils opens up new lines of research in this field, highlighting\nthe potential of adaptive transformations in enhancing the performance of\nforecasting models."}, "http://arxiv.org/abs/2311.02634": {"title": "Pointwise Data Depth for Univariate and Multivariate Functional Outlier Detection", "link": "http://arxiv.org/abs/2311.02634", "description": "Data depth is an efficient tool for robustly summarizing the distribution of\nfunctional data and detecting potential magnitude and shape outliers. Commonly\nused functional data depth notions, such as the modified band depth and\nextremal depth, are estimated from pointwise depth for each observed functional\nobservation. However, these techniques require calculating one single depth\nvalue for each functional observation, which may not be sufficient to\ncharacterize the distribution of the functional data and detect potential\noutliers. This paper presents an innovative approach to make the best use of\npointwise depth. We propose using the pointwise depth distribution for\nmagnitude outlier visualization and the correlation between pairwise depth for\nshape outlier detection. Furthermore, a bootstrap-based testing procedure has\nbeen introduced for the correlation to test whether there is any shape outlier.\nThe proposed univariate methods are then extended to bivariate functional data.\nThe performance of the proposed methods is examined and compared to\nconventional outlier detection techniques by intensive simulation studies. In\naddition, the developed methods are applied to simulated solar energy datasets\nfrom a photovoltaic system. Results revealed that the proposed method offers\nsuperior detection performance over conventional techniques. These findings\nwill benefit engineers and practitioners in monitoring photovoltaic systems by\ndetecting unnoticed anomalies and outliers."}, "http://arxiv.org/abs/2311.02658": {"title": "Nonparametric Estimation and Comparison of Distance Distributions from Censored Data", "link": "http://arxiv.org/abs/2311.02658", "description": "Transportation distance information is a powerful resource, but location\nrecords are often censored due to privacy concerns or regulatory mandates. We\nconsider the problem of transportation event distance distribution\nreconstruction, which aims to handle this obstacle and has applications to\npublic health informatics, logistics, and more. We propose numerical methods to\napproximate, sample from, and compare distributions of distances between\ncensored location pairs. We validate empirically and demonstrate applicability\nto practical geospatial data analysis tasks. Our code is available on GitHub."}, "http://arxiv.org/abs/2311.02766": {"title": "Riemannian Laplace Approximation with the Fisher Metric", "link": "http://arxiv.org/abs/2311.02766", "description": "The Laplace's method approximates a target density with a Gaussian\ndistribution at its mode. It is computationally efficient and asymptotically\nexact for Bayesian inference due to the Bernstein-von Mises theorem, but for\ncomplex targets and finite-data posteriors it is often too crude an\napproximation. A recent generalization of the Laplace Approximation transforms\nthe Gaussian approximation according to a chosen Riemannian geometry providing\na richer approximation family, while still retaining computational efficiency.\nHowever, as shown here, its properties heavily depend on the chosen metric,\nindeed the metric adopted in previous work results in approximations that are\noverly narrow as well as being biased even at the limit of infinite data. We\ncorrect this shortcoming by developing the approximation family further,\nderiving two alternative variants that are exact at the limit of infinite data,\nextending the theoretical analysis of the method, and demonstrating practical\nimprovements in a range of experiments."}, "http://arxiv.org/abs/2311.02808": {"title": "Nonparametric Estimation of Conditional Copula using Smoothed Checkerboard Bernstein Sieves", "link": "http://arxiv.org/abs/2311.02808", "description": "Conditional copulas are useful tools for modeling the dependence between\nmultiple response variables that may vary with a given set of predictor\nvariables. Conditional dependence measures such as conditional Kendall's tau\nand Spearman's rho that can be expressed as functionals of the conditional\ncopula are often used to evaluate the strength of dependence conditioning on\nthe covariates. In general, semiparametric estimation methods of conditional\ncopulas rely on an assumed parametric copula family where the copula parameter\nis assumed to be a function of the covariates. The functional relationship can\nbe estimated nonparametrically using different techniques but it is required to\nchoose an appropriate copula model from various candidate families. In this\npaper, by employing the empirical checkerboard Bernstein copula (ECBC)\nestimator we propose a fully nonparametric approach for estimating conditional\ncopulas, which doesn't require any selection of parametric copula models.\nClosed-form estimates of the conditional dependence measures are derived\ndirectly from the proposed ECBC-based conditional copula estimator. We provide\nthe large-sample consistency of the proposed estimator as well as the estimates\nof conditional dependence measures. The finite-sample performance of the\nproposed estimator and comparison with semiparametric methods are investigated\nthrough simulation studies. An application to real case studies is also\nprovided."}, "http://arxiv.org/abs/2311.02822": {"title": "Robust estimation of heteroscedastic regression models: a brief overview and new proposals", "link": "http://arxiv.org/abs/2311.02822", "description": "We collect robust proposals given in the field of regression models with\nheteroscedastic errors. Our motivation stems from the fact that the\npractitioner frequently faces the confluence of two phenomena in the context of\ndata analysis: non--linearity and heteroscedasticity. The impact of\nheteroscedasticity on the precision of the estimators is well--known, however\nthe conjunction of these two phenomena makes handling outliers more difficult.\n\nAn iterative procedure to estimate the parameters of a heteroscedastic\nnon--linear model is considered. The studied estimators combine weighted\n$MM-$regression estimators, to control the impact of high leverage points, and\na robust method to estimate the parameters of the variance function."}, "http://arxiv.org/abs/2311.03247": {"title": "Multivariate selfsimilarity: Multiscale eigen-structures for selfsimilarity parameter estimation", "link": "http://arxiv.org/abs/2311.03247", "description": "Scale-free dynamics, formalized by selfsimilarity, provides a versatile\nparadigm massively and ubiquitously used to model temporal dynamics in\nreal-world data. However, its practical use has mostly remained univariate so\nfar. By contrast, modern applications often demand multivariate data analysis.\nAccordingly, models for multivariate selfsimilarity were recently proposed.\nNevertheless, they have remained rarely used in practice because of a lack of\navailable robust estimation procedures for the vector of selfsimilarity\nparameters. Building upon recent mathematical developments, the present work\nputs forth an efficient estimation procedure based on the theoretical study of\nthe multiscale eigenstructure of the wavelet spectrum of multivariate\nselfsimilar processes. The estimation performance is studied theoretically in\nthe asymptotic limits of large scale and sample sizes, and computationally for\nfinite-size samples. As a practical outcome, a fully operational and documented\nmultivariate signal processing estimation toolbox is made freely available and\nis ready for practical use on real-world data. Its potential benefits are\nillustrated in epileptic seizure prediction from multi-channel EEG data."}, "http://arxiv.org/abs/2311.03289": {"title": "Batch effect correction with sample remeasurement in highly confounded case-control studies", "link": "http://arxiv.org/abs/2311.03289", "description": "Batch effects are pervasive in biomedical studies. One approach to address\nthe batch effects is repeatedly measuring a subset of samples in each batch.\nThese remeasured samples are used to estimate and correct the batch effects.\nHowever, rigorous statistical methods for batch effect correction with\nremeasured samples are severely under-developed. In this study, we developed a\nframework for batch effect correction using remeasured samples in highly\nconfounded case-control studies. We provided theoretical analyses of the\nproposed procedure, evaluated its power characteristics, and provided a power\ncalculation tool to aid in the study design. We found that the number of\nsamples that need to be remeasured depends strongly on the between-batch\ncorrelation. When the correlation is high, remeasuring a small subset of\nsamples is possible to rescue most of the power."}, "http://arxiv.org/abs/2311.03343": {"title": "Distribution-uniform anytime-valid inference", "link": "http://arxiv.org/abs/2311.03343", "description": "Are asymptotic confidence sequences and anytime $p$-values uniformly valid\nfor a nontrivial class of distributions $\\mathcal{P}$? We give a positive\nanswer to this question by deriving distribution-uniform anytime-valid\ninference procedures. Historically, anytime-valid methods -- including\nconfidence sequences, anytime $p$-values, and sequential hypothesis tests that\nenable inference at stopping times -- have been justified nonasymptotically.\nNevertheless, asymptotic procedures such as those based on the central limit\ntheorem occupy an important part of statistical toolbox due to their\nsimplicity, universality, and weak assumptions. While recent work has derived\nasymptotic analogues of anytime-valid methods with the aforementioned benefits,\nthese were not shown to be $\\mathcal{P}$-uniform, meaning that their\nasymptotics are not uniformly valid in a class of distributions $\\mathcal{P}$.\nIndeed, the anytime-valid inference literature currently has no central limit\ntheory to draw from that is both uniform in $\\mathcal{P}$ and in the sample\nsize $n$. This paper fills that gap by deriving a novel $\\mathcal{P}$-uniform\nstrong Gaussian approximation theorem, enabling $\\mathcal{P}$-uniform\nanytime-valid inference for the first time. Along the way, our Gaussian\napproximation also yields a $\\mathcal{P}$-uniform law of the iterated\nlogarithm."}, "http://arxiv.org/abs/2009.10780": {"title": "Independent finite approximations for Bayesian nonparametric inference", "link": "http://arxiv.org/abs/2009.10780", "description": "Completely random measures (CRMs) and their normalizations (NCRMs) offer\nflexible models in Bayesian nonparametrics. But their infinite dimensionality\npresents challenges for inference. Two popular finite approximations are\ntruncated finite approximations (TFAs) and independent finite approximations\n(IFAs). While the former have been well-studied, IFAs lack similarly general\nbounds on approximation error, and there has been no systematic comparison\nbetween the two options. In the present work, we propose a general recipe to\nconstruct practical finite-dimensional approximations for homogeneous CRMs and\nNCRMs, in the presence or absence of power laws. We call our construction the\nautomated independent finite approximation (AIFA). Relative to TFAs, we show\nthat AIFAs facilitate more straightforward derivations and use of parallel\ncomputing in approximate inference. We upper bound the approximation error of\nAIFAs for a wide class of common CRMs and NCRMs -- and thereby develop\nguidelines for choosing the approximation level. Our lower bounds in key cases\nsuggest that our upper bounds are tight. We prove that, for worst-case choices\nof observation likelihoods, TFAs are more efficient than AIFAs. Conversely, we\nfind that in real-data experiments with standard likelihoods, AIFAs and TFAs\nperform similarly. Moreover, we demonstrate that AIFAs can be used for\nhyperparameter estimation even when other potential IFA options struggle or do\nnot apply."}, "http://arxiv.org/abs/2111.07517": {"title": "Correlation Improves Group Testing: Capturing the Dilution Effect", "link": "http://arxiv.org/abs/2111.07517", "description": "Population-wide screening to identify and isolate infectious individuals is a\npowerful tool for controlling COVID-19 and other infectious diseases. Group\ntesting can enable such screening despite limited testing resources. Samples'\nviral loads are often positively correlated, either because prevalence and\nsample collection are both correlated with geography, or through intentional\nenhancement, e.g., by pooling samples from people in similar risk groups. Such\ncorrelation is known to improve test efficiency in mathematical models with\nfixed sensitivity. In reality, however, dilution degrades a pooled test's\nsensitivity by an amount that varies with the number of positives in the pool.\nIn the presence of this dilution effect, we study the impact of correlation on\nthe most widely-used group testing procedure, the Dorfman procedure. We show\nthat correlation's effects are significantly altered by the dilution effect. We\nprove that under a general correlation structure, pooling correlated samples\ntogether (called correlated pooling) achieves higher sensitivity but can\ndegrade test efficiency compared to independently pooling the samples (called\nnaive pooling) using the same pool size. We identify an alternative measure of\ntest resource usage, the number of positives found per test consumed, which we\nargue is better aligned with infection control, and show that correlated\npooling outperforms naive pooling on this measure. We build a realistic\nagent-based simulation to contextualize our theoretical results within an\nepidemic control framework. We argue that the dilution effect makes it even\nmore important for policy-makers evaluating group testing protocols for\nlarge-scale screening to incorporate naturally arising correlation and to\nintentionally maximize correlation."}, "http://arxiv.org/abs/2202.05349": {"title": "Robust Parameter Estimation for the Lee-Carter Family: A Probabilistic Principal Component Approach", "link": "http://arxiv.org/abs/2202.05349", "description": "The well-known Lee-Carter model uses a bilinear form\n$\\log(m_{x,t})=a_x+b_xk_t$ to represent the log mortality rate and has been\nwidely researched and developed over the past thirty years. However, there has\nbeen little attention being paid to the robustness of the parameters against\noutliers, especially when estimating $b_x$. In response, we propose a robust\nestimation method for a wide family of Lee-Carter-type models, treating the\nproblem as a Probabilistic Principal Component Analysis (PPCA) with\nmultivariate $t$-distributions. An efficient Expectation-Maximization (EM)\nalgorithm is also derived for implementation.\n\nThe benefits of the method are threefold: 1) it produces more robust\nestimates of both $b_x$ and $k_t$, 2) it can be naturally extended to a large\nfamily of Lee-Carter type models, including those for modelling multiple\npopulations, and 3) it can be integrated with other existing time series models\nfor $k_t$. Using numerical studies based on United States mortality data from\nthe Human Mortality Database, we show the proposed model performs more robust\ncompared to conventional methods in the presence of outliers."}, "http://arxiv.org/abs/2204.11979": {"title": "Semi-Parametric Sensitivity Analysis for Trials with Irregular and Informative Assessment Times", "link": "http://arxiv.org/abs/2204.11979", "description": "Many trials are designed to collect outcomes at or around pre-specified times\nafter randomization. In practice, there can be substantial variability in the\ntimes when participants are actually assessed. Such irregular assessment times\npose a challenge to learning the effect of treatment since not all participants\nhave outcome assessments at the times of interest. Furthermore, observed\noutcome values may not be representative of all participants' outcomes at a\ngiven time. This problem, known as informative assessment times, can arise if\nparticipants tend to have assessments when their outcomes are better (or worse)\nthan at other times, or if participants with better outcomes tend to have more\n(or fewer) assessments. Methods have been developed that account for some types\nof informative assessment; however, since these methods rely on untestable\nassumptions, sensitivity analyses are needed. We develop a sensitivity analysis\nmethodology by extending existing weighting methods. Our method accounts for\nthe possibility that participants with worse outcomes at a given time are more\n(or less) likely than other participants to have an assessment at that time,\neven after controlling for variables observed earlier in the study. We apply\nour method to a randomized trial of low-income individuals with uncontrolled\nasthma. We illustrate implementation of our influence-function based estimation\nprocedure in detail, and we derive the large-sample distribution of our\nestimator and evaluate its finite-sample performance."}, "http://arxiv.org/abs/2205.13935": {"title": "Detecting hidden confounding in observational data using multiple environments", "link": "http://arxiv.org/abs/2205.13935", "description": "A common assumption in causal inference from observational data is that there\nis no hidden confounding. Yet it is, in general, impossible to verify this\nassumption from a single dataset. Under the assumption of independent causal\nmechanisms underlying the data-generating process, we demonstrate a way to\ndetect unobserved confounders when having multiple observational datasets\ncoming from different environments. We present a theory for testable\nconditional independencies that are only absent when there is hidden\nconfounding and examine cases where we violate its assumptions: degenerate &amp;\ndependent mechanisms, and faithfulness violations. Additionally, we propose a\nprocedure to test these independencies and study its empirical finite-sample\nbehavior using simulation studies and semi-synthetic data based on a real-world\ndataset. In most cases, the proposed procedure correctly predicts the presence\nof hidden confounding, particularly when the confounding bias is large."}, "http://arxiv.org/abs/2206.09444": {"title": "Bayesian non-conjugate regression via variational message passing", "link": "http://arxiv.org/abs/2206.09444", "description": "Variational inference is a popular method for approximating the posterior\ndistribution of hierarchical Bayesian models. It is well-recognized in the\nliterature that the choice of the approximation family and the regularity\nproperties of the posterior strongly influence the efficiency and accuracy of\nvariational methods. While model-specific conjugate approximations offer\nsimplicity, they often converge slowly and may yield poor approximations.\nNon-conjugate approximations instead are more flexible but typically require\nthe calculation of expensive multidimensional integrals. This study focuses on\nBayesian regression models that use possibly non-differentiable loss functions\nto measure prediction misfit. The data behavior is modeled using a linear\npredictor, potentially transformed using a bijective link function. Examples\ninclude generalized linear models, mixed additive models, support vector\nmachines, and quantile regression. To address the limitations of non-conjugate\nsettings, the study proposes an efficient non-conjugate variational message\npassing method for approximate posterior inference, which only requires the\ncalculation of univariate numerical integrals when analytical solutions are not\navailable. The approach does not require differentiability, conjugacy, or\nmodel-specific data-augmentation strategies, thereby naturally extending to\nmodels with non-conjugate likelihood functions. Additionally, a stochastic\nimplementation is provided to handle large-scale data problems. The proposed\nmethod's performances are evaluated through extensive simulations and real data\nexamples. Overall, the results highlight the effectiveness of the proposed\nvariational message passing method, demonstrating its computational efficiency\nand approximation accuracy as an alternative to existing methods in Bayesian\ninference for regression models."}, "http://arxiv.org/abs/2206.10143": {"title": "A Contrastive Approach to Online Change Point Detection", "link": "http://arxiv.org/abs/2206.10143", "description": "We suggest a novel procedure for online change point detection. Our approach\nexpands an idea of maximizing a discrepancy measure between points from\npre-change and post-change distributions. This leads to a flexible procedure\nsuitable for both parametric and nonparametric scenarios. We prove\nnon-asymptotic bounds on the average running length of the procedure and its\nexpected detection delay. The efficiency of the algorithm is illustrated with\nnumerical experiments on synthetic and real-world data sets."}, "http://arxiv.org/abs/2206.15367": {"title": "Targeted learning in observational studies with multi-valued treatments: An evaluation of antipsychotic drug treatment safety", "link": "http://arxiv.org/abs/2206.15367", "description": "We investigate estimation of causal effects of multiple competing\n(multi-valued) treatments in the absence of randomization. Our work is\nmotivated by an intention-to-treat study of the relative cardiometabolic risk\nof assignment to one of six commonly prescribed antipsychotic drugs in a cohort\nof nearly 39,000 adults adults with serious mental illness. Doubly-robust\nestimators, such as targeted minimum loss-based estimation (TMLE), require\ncorrect specification of either the treatment model or outcome model to ensure\nconsistent estimation; however, common TMLE implementations estimate treatment\nprobabilities using multiple binomial regressions rather than multinomial\nregression. We implement a TMLE estimator that uses multinomial treatment\nassignment and ensemble machine learning to estimate average treatment effects.\nOur multinomial implementation improves coverage, but does not necessarily\nreduce bias, relative to the binomial implementation in simulation experiments\nwith varying treatment propensity overlap and event rates. Evaluating the\ncausal effects of the antipsychotics on 3-year diabetes risk or death, we find\na safety benefit of moving from a second-generation drug considered among the\nsafest of the second-generation drugs to an infrequently prescribed\nfirst-generation drug thought to pose a generally low cardiometabolic risk."}, "http://arxiv.org/abs/2209.04364": {"title": "Evaluating tests for cluster-randomized trials with few clusters under generalized linear mixed models with covariate adjustment: a simulation study", "link": "http://arxiv.org/abs/2209.04364", "description": "Generalized linear mixed models (GLMM) are commonly used to analyze clustered\ndata, but when the number of clusters is small to moderate, standard\nstatistical tests may produce elevated type I error rates. Small-sample\ncorrections have been proposed for continuous or binary outcomes without\ncovariate adjustment. However, appropriate tests to use for count outcomes or\nunder covariate-adjusted models remains unknown. An important setting in which\nthis issue arises is in cluster-randomized trials (CRTs). Because many CRTs\nhave just a few clusters (e.g., clinics or health systems), covariate\nadjustment is particularly critical to address potential chance imbalance\nand/or low power (e.g., adjustment following stratified randomization or for\nthe baseline value of the outcome). We conducted simulations to evaluate\nGLMM-based tests of the treatment effect that account for the small (10) or\nmoderate (20) number of clusters under a parallel-group CRT setting across\nscenarios of covariate adjustment (including adjustment for one or more\nperson-level or cluster-level covariates) for both binary and count outcomes.\nWe find that when the intraclass correlation is non-negligible ($\\geq 0.01$)\nand the number of covariates is small ($\\leq 2$), likelihood ratio tests with a\nbetween-within denominator degree of freedom have type I error rates close to\nthe nominal level. When the number of covariates is moderate ($\\geq 5$), across\nour simulation scenarios, the relative performance of the tests varied\nconsiderably and no method performed uniformly well. Therefore, we recommend\nadjusting for no more than a few covariates and using likelihood ratio tests\nwith a between-within denominator degree of freedom."}, "http://arxiv.org/abs/2211.14578": {"title": "Estimation and inference for transfer learning with high-dimensional quantile regression", "link": "http://arxiv.org/abs/2211.14578", "description": "Transfer learning has become an essential technique to exploit information\nfrom the source domain to boost performance of the target task. Despite the\nprevalence in high-dimensional data, heterogeneity and heavy tails are\ninsufficiently accounted for by current transfer learning approaches and thus\nmay undermine the resulting performance. We propose a transfer learning\nprocedure in the framework of high-dimensional quantile regression models to\naccommodate heterogeneity and heavy tails in the source and target domains. We\nestablish error bounds of transfer learning estimator based on delicately\nselected transferable source domains, showing that lower error bounds can be\nachieved for critical selection criterion and larger sample size of source\ntasks. We further propose valid confidence interval and hypothesis test\nprocedures for individual component of high-dimensional quantile regression\ncoefficients by advocating a double transfer learning estimator, which is\none-step debiased estimator for the transfer learning estimator wherein the\ntechnique of transfer learning is designed again. By adopting data-splitting\ntechnique, we advocate a transferability detection approach that guarantees to\ncircumvent negative transfer and identify transferable sources with high\nprobability. Simulation results demonstrate that the proposed method exhibits\nsome favorable and compelling performances and the practical utility is further\nillustrated by analyzing a real example."}, "http://arxiv.org/abs/2306.03302": {"title": "Statistical Inference Under Constrained Selection Bias", "link": "http://arxiv.org/abs/2306.03302", "description": "Large-scale datasets are increasingly being used to inform decision making.\nWhile this effort aims to ground policy in real-world evidence, challenges have\narisen as selection bias and other forms of distribution shifts often plague\nobservational data. Previous attempts to provide robust inference have given\nguarantees depending on a user-specified amount of possible distribution shift\n(e.g., the maximum KL divergence between the observed and target\ndistributions). However, decision makers will often have additional knowledge\nabout the target distribution which constrains the kind of possible shifts. To\nleverage such information, we propose a framework that enables statistical\ninference in the presence of selection bias which obeys user-specified\nconstraints in the form of functions whose expectation is known under the\ntarget distribution. The output is high-probability bounds on the value of an\nestimand for the target distribution. Hence, our method leverages domain\nknowledge in order to partially identify a wide class of estimands. We analyze\nthe computational and statistical properties of methods to estimate these\nbounds and show that our method can produce informative bounds on a variety of\nsimulated and semisynthetic tasks, as well as in a real-world use case."}, "http://arxiv.org/abs/2307.15348": {"title": "Stratified principal component analysis", "link": "http://arxiv.org/abs/2307.15348", "description": "This paper investigates a general family of covariance models with repeated\neigenvalues extending probabilistic principal component analysis (PPCA). A\ngeometric interpretation shows that these models are parameterised by flag\nmanifolds and stratify the space of covariance matrices according to the\nsequence of eigenvalue multiplicities. The subsequent analysis sheds light on\nPPCA and answers an important question on the practical identifiability of\nindividual eigenvectors. It notably shows that one rarely has enough samples to\nfit a covariance model with distinct eigenvalues and that block-averaging the\nadjacent sample eigenvalues with small gaps achieves a better\ncomplexity/goodness-of-fit tradeoff."}, "http://arxiv.org/abs/2308.02005": {"title": "Bias Correction for Randomization-Based Estimation in Inexactly Matched Observational Studies", "link": "http://arxiv.org/abs/2308.02005", "description": "Matching has been widely used to mimic a randomized experiment with\nobservational data. Ideally, treated subjects are exactly matched with controls\nfor the covariates, and randomization-based estimation can then be conducted as\nin a randomized experiment (assuming no unobserved covariates). However, when\nthere exists continuous covariates or many covariates, matching typically\nshould be inexact. Previous studies have routinely ignored inexact matching in\nthe downstream randomization-based estimation as long as some covariate balance\ncriteria are satisfied, which can cause severe estimation bias. Built on the\ncovariate-adaptive randomization inference framework, in this research note, we\npropose two new classes of bias-corrected randomization-based estimators to\nreduce estimation bias due to inexact matching: the bias-corrected maximum\n$p$-value estimator for the constant treatment effect and the bias-corrected\ndifference-in-means estimator for the average treatment effect. Our simulation\nresults show that the proposed bias-corrected estimators can effectively reduce\nestimation bias due to inexact matching."}, "http://arxiv.org/abs/2310.01153": {"title": "Online Permutation Tests: $e$-values and Likelihood Ratios for Testing Group Invariance", "link": "http://arxiv.org/abs/2310.01153", "description": "We develop a flexible online version of the permutation test. This allows us\nto test exchangeability as the data is arriving, where we can choose to stop or\ncontinue without invalidating the size of the test. Our methods generalize\nbeyond exchangeability to other forms of invariance under a compact group. Our\napproach relies on constructing an $e$-process that is the running product of\nmultiple conditional $e$-values. To construct $e$-values, we first develop an\nessentially complete class of admissible $e$-values in which one can flexibly\n`plug in' almost any desired test statistic. To make the $e$-values\nconditional, we explore the intersection between the concepts of conditional\ninvariance and sequential invariance, and find that the appropriate conditional\ndistribution can be captured by a compact subgroup. To find powerful $e$-values\nfor given alternatives, we develop the theory of likelihood ratios for testing\ngroup invariance yielding new optimality results for group invariance tests.\nThese statistics turn out to exist in three different flavors, depending on the\nspace on which we specify our alternative. We apply these statistics to test\nagainst a Gaussian location shift, which yields connections to the $t$-test\nwhen testing sphericity, connections to the softmax function and its\ntemperature when testing exchangeability, and yields an improved version of a\nknown $e$-value for testing sign-symmetry. Moreover, we introduce an impatience\nparameter that allows users to obtain more power now in exchange for less power\nin the long run."}, "http://arxiv.org/abs/2311.03381": {"title": "Separating and Learning Latent Confounders to Enhancing User Preferences Modeling", "link": "http://arxiv.org/abs/2311.03381", "description": "Recommender models aim to capture user preferences from historical feedback\nand then predict user-specific feedback on candidate items. However, the\npresence of various unmeasured confounders causes deviations between the user\npreferences in the historical feedback and the true preferences, resulting in\nmodels not meeting their expected performance. Existing debias models either\n(1) specific to solving one particular bias or (2) directly obtain auxiliary\ninformation from user historical feedback, which cannot identify whether the\nlearned preferences are true user preferences or mixed with unmeasured\nconfounders. Moreover, we find that the former recommender system is not only a\nsuccessor to unmeasured confounders but also acts as an unmeasured confounder\naffecting user preference modeling, which has always been neglected in previous\nstudies. To this end, we incorporate the effect of the former recommender\nsystem and treat it as a proxy for all unmeasured confounders. We propose a\nnovel framework, \\textbf{S}eparating and \\textbf{L}earning Latent Confounders\n\\textbf{F}or \\textbf{R}ecommendation (\\textbf{SLFR}), which obtains the\nrepresentation of unmeasured confounders to identify the counterfactual\nfeedback by disentangling user preferences and unmeasured confounders, then\nguides the target model to capture the true preferences of users. Extensive\nexperiments in five real-world datasets validate the advantages of our method."}, "http://arxiv.org/abs/2311.03382": {"title": "Causal Structure Representation Learning of Confounders in Latent Space for Recommendation", "link": "http://arxiv.org/abs/2311.03382", "description": "Inferring user preferences from the historical feedback of users is a\nvaluable problem in recommender systems. Conventional approaches often rely on\nthe assumption that user preferences in the feedback data are equivalent to the\nreal user preferences without additional noise, which simplifies the problem\nmodeling. However, there are various confounders during user-item interactions,\nsuch as weather and even the recommendation system itself. Therefore,\nneglecting the influence of confounders will result in inaccurate user\npreferences and suboptimal performance of the model. Furthermore, the\nunobservability of confounders poses a challenge in further addressing the\nproblem. To address these issues, we refine the problem and propose a more\nrational solution. Specifically, we consider the influence of confounders,\ndisentangle them from user preferences in the latent space, and employ causal\ngraphs to model their interdependencies without specific labels. By cleverly\ncombining local and global causal graphs, we capture the user-specificity of\nconfounders on user preferences. We theoretically demonstrate the\nidentifiability of the obtained causal graph. Finally, we propose our model\nbased on Variational Autoencoders, named Causal Structure representation\nlearning of Confounders in latent space (CSC). We conducted extensive\nexperiments on one synthetic dataset and five real-world datasets,\ndemonstrating the superiority of our model. Furthermore, we demonstrate that\nthe learned causal representations of confounders are controllable, potentially\noffering users fine-grained control over the objectives of their recommendation\nlists with the learned causal graphs."}, "http://arxiv.org/abs/2311.03554": {"title": "Conditional Randomization Tests for Behavioral and Neural Time Series", "link": "http://arxiv.org/abs/2311.03554", "description": "Randomization tests allow simple and unambiguous tests of null hypotheses, by\ncomparing observed data to a null ensemble in which experimentally-controlled\nvariables are randomly resampled. In behavioral and neuroscience experiments,\nhowever, the stimuli presented often depend on the subject's previous actions,\nso simple randomization tests are not possible. We describe how conditional\nrandomization can be used to perform exact hypothesis tests in this situation,\nand illustrate it with two examples. We contrast conditional randomization with\na related approach of tangent randomization, in which stimuli are resampled\nbased only on events occurring in the past, which is not valid for all choices\nof test statistic. We discuss how to design experiments that allow conditional\nrandomization tests to be used."}, "http://arxiv.org/abs/2311.03630": {"title": "Counterfactual Data Augmentation with Contrastive Learning", "link": "http://arxiv.org/abs/2311.03630", "description": "Statistical disparity between distinct treatment groups is one of the most\nsignificant challenges for estimating Conditional Average Treatment Effects\n(CATE). To address this, we introduce a model-agnostic data augmentation method\nthat imputes the counterfactual outcomes for a selected subset of individuals.\nSpecifically, we utilize contrastive learning to learn a representation space\nand a similarity measure such that in the learned representation space close\nindividuals identified by the learned similarity measure have similar potential\noutcomes. This property ensures reliable imputation of counterfactual outcomes\nfor the individuals with close neighbors from the alternative treatment group.\nBy augmenting the original dataset with these reliable imputations, we can\neffectively reduce the discrepancy between different treatment groups, while\ninducing minimal imputation error. The augmented dataset is subsequently\nemployed to train CATE estimation models. Theoretical analysis and experimental\nstudies on synthetic and semi-synthetic benchmarks demonstrate that our method\nachieves significant improvements in both performance and robustness to\noverfitting across state-of-the-art models."}, "http://arxiv.org/abs/2311.03644": {"title": "BOB: Bayesian Optimized Bootstrap with Applications to Gaussian Mixture Models", "link": "http://arxiv.org/abs/2311.03644", "description": "Sampling from the joint posterior distribution of Gaussian mixture models\n(GMMs) via standard Markov chain Monte Carlo (MCMC) imposes several\ncomputational challenges, which have prevented a broader full Bayesian\nimplementation of these models. A growing body of literature has introduced the\nWeighted Likelihood Bootstrap and the Weighted Bayesian Bootstrap as\nalternatives to MCMC sampling. The core idea of these methods is to repeatedly\ncompute maximum a posteriori (MAP) estimates on many randomly weighted\nposterior densities. These MAP estimates then can be treated as approximate\nposterior draws. Nonetheless, a central question remains unanswered: How to\nselect the distribution of the random weights under arbitrary sample sizes.\nThus, we introduce the Bayesian Optimized Bootstrap (BOB), a computational\nmethod to automatically select the weights distribution by minimizing, through\nBayesian Optimization, a black-box and noisy version of the reverse KL\ndivergence between the Bayesian posterior and an approximate posterior obtained\nvia random weighting. Our proposed method allows for uncertainty\nquantification, approximate posterior sampling, and embraces recent\ndevelopments in parallel computing. We show that BOB outperforms competing\napproaches in recovering the Bayesian posterior, while retaining key\ntheoretical properties from existing methods. BOB's performance is demonstrated\nthrough extensive simulations, along with real-world data analyses."}, "http://arxiv.org/abs/2311.03660": {"title": "Sampling via F\\\"ollmer Flow", "link": "http://arxiv.org/abs/2311.03660", "description": "We introduce a novel unit-time ordinary differential equation (ODE) flow\ncalled the preconditioned F\\\"{o}llmer flow, which efficiently transforms a\nGaussian measure into a desired target measure at time 1. To discretize the\nflow, we apply Euler's method, where the velocity field is calculated either\nanalytically or through Monte Carlo approximation using Gaussian samples. Under\nreasonable conditions, we derive a non-asymptotic error bound in the\nWasserstein distance between the sampling distribution and the target\ndistribution. Through numerical experiments on mixture distributions in 1D, 2D,\nand high-dimensional spaces, we demonstrate that the samples generated by our\nproposed flow exhibit higher quality compared to those obtained by several\nexisting methods. Furthermore, we propose leveraging the F\\\"{o}llmer flow as a\nwarmstart strategy for existing Markov Chain Monte Carlo (MCMC) methods, aiming\nto mitigate mode collapses and enhance their performance. Finally, thanks to\nthe deterministic nature of the F\\\"{o}llmer flow, we can leverage deep neural\nnetworks to fit the trajectory of sample evaluations. This allows us to obtain\na generator for one-step sampling as a result."}, "http://arxiv.org/abs/2311.03763": {"title": "Thresholding the higher criticism test statistics for optimality in a heterogeneous setting", "link": "http://arxiv.org/abs/2311.03763", "description": "Donoho and Kipnis (2022) showed that the the higher criticism (HC) test\nstatistic has a non-Gaussian phase transition but remarked that it is probably\nnot optimal, in the detection of sparse differences between two large frequency\ntables when the counts are low. The setting can be considered to be\nheterogeneous, with cells containing larger total counts more able to detect\nsmaller differences. We provide a general study here of sparse detection\narising from such heterogeneous settings, and showed that optimality of the HC\ntest statistic requires thresholding, for example in the case of frequency\ntable comparison, to restrict to p-values of cells with total counts exceeding\na threshold. The use of thresholding also leads to optimality of the HC test\nstatistic when it is applied on the sparse Poisson means model of Arias-Castro\nand Wang (2015). The phase transitions we consider here are non-Gaussian, and\ninvolve an interplay between the rate functions of the response and sample size\ndistributions. We also showed, both theoretically and in a numerical study,\nthat applying thresholding to the Bonferroni test statistic results in better\nsparse mixture detection in heterogeneous settings."}, "http://arxiv.org/abs/2311.03769": {"title": "Nonparametric Screening for Additive Quantile Regression in Ultra-high Dimension", "link": "http://arxiv.org/abs/2311.03769", "description": "In practical applications, one often does not know the \"true\" structure of\nthe underlying conditional quantile function, especially in the ultra-high\ndimensional setting. To deal with ultra-high dimensionality, quantile-adaptive\nmarginal nonparametric screening methods have been recently developed. However,\nthese approaches may miss important covariates that are marginally independent\nof the response, or may select unimportant covariates due to their high\ncorrelations with important covariates. To mitigate such shortcomings, we\ndevelop a conditional nonparametric quantile screening procedure (complemented\nby subsequent selection) for nonparametric additive quantile regression models.\nUnder some mild conditions, we show that the proposed screening method can\nidentify all relevant covariates in a small number of steps with probability\napproaching one. The subsequent narrowed best subset (via a modified Bayesian\ninformation criterion) also contains all the relevant covariates with\noverwhelming probability. The advantages of our proposed procedure are\ndemonstrated through simulation studies and a real data example."}, "http://arxiv.org/abs/2311.03829": {"title": "Multilevel mixtures of latent trait analyzers for clustering multi-layer bipartite networks", "link": "http://arxiv.org/abs/2311.03829", "description": "Within network data analysis, bipartite networks represent a particular type\nof network where relationships occur between two disjoint sets of nodes,\nformally called sending and receiving nodes. In this context, sending nodes may\nbe organized into layers on the basis of some defined characteristics,\nresulting in a special case of multilayer bipartite network, where each layer\nincludes a specific set of sending nodes. To perform a clustering of sending\nnodes in multi-layer bipartite network, we extend the Mixture of Latent Trait\nAnalyzers (MLTA), also taking into account the influence of concomitant\nvariables on clustering formation and the multi-layer structure of the data. To\nthis aim, a multilevel approach offers a useful methodological tool to properly\naccount for the hierarchical structure of the data and for the unobserved\nsources of heterogeneity at multiple levels. A simulation study is conducted to\ntest the performance of the proposal in terms of parameters' and clustering\nrecovery. Furthermore, the model is applied to the European Social Survey data\n(ESS) to i) perform a clustering of individuals (sending nodes) based on their\ndigital skills (receiving nodes); ii) understand how socio-economic and\ndemographic characteristics influence the individual digitalization level; iii)\naccount for the multilevel structure of the data; iv) obtain a clustering of\ncountries in terms of the base-line attitude to digital technologies of their\nresidents."}, "http://arxiv.org/abs/2311.03989": {"title": "Learned Causal Method Prediction", "link": "http://arxiv.org/abs/2311.03989", "description": "For a given causal question, it is important to efficiently decide which\ncausal inference method to use for a given dataset. This is challenging because\ncausal methods typically rely on complex and difficult-to-verify assumptions,\nand cross-validation is not applicable since ground truth causal quantities are\nunobserved.In this work, we propose CAusal Method Predictor (CAMP), a framework\nfor predicting the best method for a given dataset. To this end, we generate\ndatasets from a diverse set of synthetic causal models, score the candidate\nmethods, and train a model to directly predict the highest-scoring method for\nthat dataset. Next, by formulating a self-supervised pre-training objective\ncentered on dataset assumptions relevant for causal inference, we significantly\nreduce the need for costly labeled data and enhance training efficiency. Our\nstrategy learns to map implicit dataset properties to the best method in a\ndata-driven manner. In our experiments, we focus on method prediction for\ncausal discovery. CAMP outperforms selecting any individual candidate method\nand demonstrates promising generalization to unseen semi-synthetic and\nreal-world benchmarks."}, "http://arxiv.org/abs/2311.04017": {"title": "Multivariate quantile-based permutation tests with application to functional data", "link": "http://arxiv.org/abs/2311.04017", "description": "Permutation tests enable testing statistical hypotheses in situations when\nthe distribution of the test statistic is complicated or not available. In some\nsituations, the test statistic under investigation is multivariate, with the\nmultiple testing problem being an important example. The corresponding\nmultivariate permutation tests are then typically based on a\nsuitableone-dimensional transformation of the vector of partial permutation\np-values via so called combining functions. This paper proposes a new approach\nthat utilizes the optimal measure transportation concept. The final single\np-value is computed from the empirical center-outward distribution function of\nthe permuted multivariate test statistics. This method avoids computation of\nthe partial p-values and it is easy to be implemented. In addition, it allows\nto compute and interpret contributions of the components of the multivariate\ntest statistic to the non-conformity score and to the rejection of the null\nhypothesis. Apart from this method, the measure transportation is applied also\nto the vector of partial p-values as an alternative to the classical combining\nfunctions. Both techniques are compared with the standard approaches using\nvarious practical examples in a Monte Carlo study. An application on a\nfunctional data set is provided as well."}, "http://arxiv.org/abs/2311.04037": {"title": "Causal Discovery Under Local Privacy", "link": "http://arxiv.org/abs/2311.04037", "description": "Differential privacy is a widely adopted framework designed to safeguard the\nsensitive information of data providers within a data set. It is based on the\napplication of controlled noise at the interface between the server that stores\nand processes the data, and the data consumers. Local differential privacy is a\nvariant that allows data providers to apply the privatization mechanism\nthemselves on their data individually. Therefore it provides protection also in\ncontexts in which the server, or even the data collector, cannot be trusted.\nThe introduction of noise, however, inevitably affects the utility of the data,\nparticularly by distorting the correlations between individual data components.\nThis distortion can prove detrimental to tasks such as causal discovery. In\nthis paper, we consider various well-known locally differentially private\nmechanisms and compare the trade-off between the privacy they provide, and the\naccuracy of the causal structure produced by algorithms for causal learning\nwhen applied to data obfuscated by these mechanisms. Our analysis yields\nvaluable insights for selecting appropriate local differentially private\nprotocols for causal discovery tasks. We foresee that our findings will aid\nresearchers and practitioners in conducting locally private causal discovery."}, "http://arxiv.org/abs/2311.04103": {"title": "Joint modelling of recurrent and terminal events with discretely-distributed non-parametric frailty: application on re-hospitalizations and death in heart failure patients", "link": "http://arxiv.org/abs/2311.04103", "description": "In the context of clinical and biomedical studies, joint frailty models have\nbeen developed to study the joint temporal evolution of recurrent and terminal\nevents, capturing both the heterogeneous susceptibility to experiencing a new\nepisode and the dependence between the two processes. While\ndiscretely-distributed frailty is usually more exploitable by clinicians and\nhealthcare providers, existing literature on joint frailty models predominantly\nassumes continuous distributions for the random effects. In this article, we\npresent a novel joint frailty model that assumes bivariate\ndiscretely-distributed non-parametric frailties, with an unknown finite number\nof mass points. This approach facilitates the identification of latent\nstructures among subjects, grouping them into sub-populations defined by a\nshared frailty value. We propose an estimation routine via\nExpectation-Maximization algorithm, which not only estimates the number of\nsubgroups but also serves as an unsupervised classification tool. This work is\nmotivated by a study of patients with Heart Failure (HF) receiving ACE\ninhibitors treatment in the Lombardia region of Italy. Recurrent events of\ninterest are hospitalizations due to HF and terminal event is death for any\ncause."}, "http://arxiv.org/abs/2311.04159": {"title": "Statistical Inference on Simulation Output: Batching as an Inferential Device", "link": "http://arxiv.org/abs/2311.04159", "description": "We present {batching} as an omnibus device for statistical inference on\nsimulation output. We consider the classical context of a simulationist\nperforming statistical inference on an estimator $\\theta_n$ (of an unknown\nfixed quantity $\\theta$) using only the output data $(Y_1,Y_2,\\ldots,Y_n)$\ngathered from a simulation. By \\emph{statistical inference}, we mean\napproximating the sampling distribution of the error $\\theta_n-\\theta$ toward:\n(A) estimating an ``assessment'' functional $\\psi$, e.g., bias, variance, or\nquantile; or (B) constructing a $(1-\\alpha)$-confidence region on $\\theta$. We\nargue that batching is a remarkably simple and effective inference device that\nis especially suited for handling dependent output data such as what one\nfrequently encounters in simulation contexts. We demonstrate that if the number\nof batches and the extent of their overlap are chosen correctly, batching\nretains bootstrap's attractive theoretical properties of {strong consistency}\nand {higher-order accuracy}. For constructing confidence regions, we\ncharacterize two limiting distributions associated with a Studentized\nstatistic. Our extensive numerical experience confirms theoretical insight,\nespecially about the effects of batch size and batch overlap."}, "http://arxiv.org/abs/2002.03355": {"title": "Scalable Function-on-Scalar Quantile Regression for Densely Sampled Functional Data", "link": "http://arxiv.org/abs/2002.03355", "description": "Functional quantile regression (FQR) is a useful alternative to mean\nregression for functional data as it provides a comprehensive understanding of\nhow scalar predictors influence the conditional distribution of functional\nresponses. In this article, we study the FQR model for densely sampled,\nhigh-dimensional functional data without relying on parametric error or\nindependent stochastic process assumptions, with the focus on statistical\ninference under this challenging regime along with scalable implementation.\nThis is achieved by a simple but powerful distributed strategy, in which we\nfirst perform separate quantile regression to compute $M$-estimators at each\nsampling location, and then carry out estimation and inference for the entire\ncoefficient functions by properly exploiting the uncertainty quantification and\ndependence structure of $M$-estimators. We derive a uniform Bahadur\nrepresentation and a strong Gaussian approximation result for the\n$M$-estimators on the discrete sampling grid, leading to dimension reduction\nand serving as the basis for inference. An interpolation-based estimator with\nminimax optimality is proposed, and large sample properties for point and\nsimultaneous interval estimators are established. The obtained minimax optimal\nrate under the FQR model shows an interesting phase transition phenomenon that\nhas been previously observed in functional mean regression. The proposed\nmethods are illustrated via simulations and an application to a mass\nspectrometry proteomics dataset."}, "http://arxiv.org/abs/2201.06110": {"title": "FNETS: Factor-adjusted network estimation and forecasting for high-dimensional time series", "link": "http://arxiv.org/abs/2201.06110", "description": "We propose FNETS, a methodology for network estimation and forecasting of\nhigh-dimensional time series exhibiting strong serial- and cross-sectional\ncorrelations. We operate under a factor-adjusted vector autoregressive (VAR)\nmodel which, after accounting for pervasive co-movements of the variables by\n{\\it common} factors, models the remaining {\\it idiosyncratic} dynamic\ndependence between the variables as a sparse VAR process. Network estimation of\nFNETS consists of three steps: (i) factor-adjustment via dynamic principal\ncomponent analysis, (ii) estimation of the latent VAR process via\n$\\ell_1$-regularised Yule-Walker estimator, and (iii) estimation of partial\ncorrelation and long-run partial correlation matrices. In doing so, we learn\nthree networks underpinning the VAR process, namely a directed network\nrepresenting the Granger causal linkages between the variables, an undirected\none embedding their contemporaneous relationships and finally, an undirected\nnetwork that summarises both lead-lag and contemporaneous linkages. In\naddition, FNETS provides a suite of methods for forecasting the factor-driven\nand the idiosyncratic VAR processes. Under general conditions permitting tails\nheavier than the Gaussian one, we derive uniform consistency rates for the\nestimators in both network estimation and forecasting, which hold as the\ndimension of the panel and the sample size diverge. Simulation studies and real\ndata application confirm the good performance of FNETS."}, "http://arxiv.org/abs/2202.01650": {"title": "Exposure Effects on Count Outcomes with Observational Data, with Application to Incarcerated Women", "link": "http://arxiv.org/abs/2202.01650", "description": "Causal inference methods can be applied to estimate the effect of a point\nexposure or treatment on an outcome of interest using data from observational\nstudies. For example, in the Women's Interagency HIV Study, it is of interest\nto understand the effects of incarceration on the number of sexual partners and\nthe number of cigarettes smoked after incarceration. In settings like this\nwhere the outcome is a count, the estimand is often the causal mean ratio,\ni.e., the ratio of the counterfactual mean count under exposure to the\ncounterfactual mean count under no exposure. This paper considers estimators of\nthe causal mean ratio based on inverse probability of treatment weights, the\nparametric g-formula, and doubly robust estimation, each of which can account\nfor overdispersion, zero-inflation, and heaping in the measured outcome.\nMethods are compared in simulations and are applied to data from the Women's\nInteragency HIV Study."}, "http://arxiv.org/abs/2208.14960": {"title": "Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces I: the compact case", "link": "http://arxiv.org/abs/2208.14960", "description": "Gaussian processes are arguably the most important class of spatiotemporal\nmodels within machine learning. They encode prior information about the modeled\nfunction and can be used for exact or approximate Bayesian learning. In many\napplications, particularly in physical sciences and engineering, but also in\nareas such as geostatistics and neuroscience, invariance to symmetries is one\nof the most fundamental forms of prior information one can consider. The\ninvariance of a Gaussian process' covariance to such symmetries gives rise to\nthe most natural generalization of the concept of stationarity to such spaces.\nIn this work, we develop constructive and practical techniques for building\nstationary Gaussian processes on a very large class of non-Euclidean spaces\narising in the context of symmetries. Our techniques make it possible to (i)\ncalculate covariance kernels and (ii) sample from prior and posterior Gaussian\nprocesses defined on such spaces, both in a practical manner. This work is\nsplit into two parts, each involving different technical considerations: part I\nstudies compact spaces, while part II studies non-compact spaces possessing\ncertain structure. Our contributions make the non-Euclidean Gaussian process\nmodels we study compatible with well-understood computational techniques\navailable in standard Gaussian process software packages, thereby making them\naccessible to practitioners."}, "http://arxiv.org/abs/2210.14455": {"title": "Asymmetric predictability in causal discovery: an information theoretic approach", "link": "http://arxiv.org/abs/2210.14455", "description": "Causal investigations in observational studies pose a great challenge in\nresearch where randomized trials or intervention-based studies are not\nfeasible. We develop an information geometric causal discovery and inference\nframework of \"predictive asymmetry\". For $(X, Y)$, predictive asymmetry enables\nassessment of whether $X$ is more likely to cause $Y$ or vice-versa. The\nasymmetry between cause and effect becomes particularly simple if $X$ and $Y$\nare deterministically related. We propose a new metric called the Directed\nMutual Information ($DMI$) and establish its key statistical properties. $DMI$\nis not only able to detect complex non-linear association patterns in bivariate\ndata, but also is able to detect and infer causal relations. Our proposed\nmethodology relies on scalable non-parametric density estimation using Fourier\ntransform. The resulting estimation method is manyfold faster than the\nclassical bandwidth-based density estimation. We investigate key asymptotic\nproperties of the $DMI$ methodology and a data-splitting technique is utilized\nto facilitate causal inference using the $DMI$. Through simulation studies and\nan application, we illustrate the performance of $DMI$."}, "http://arxiv.org/abs/2211.01227": {"title": "Conformalized survival analysis with adaptive cutoffs", "link": "http://arxiv.org/abs/2211.01227", "description": "This paper introduces an assumption-lean method that constructs valid and\nefficient lower predictive bounds (LPBs) for survival times with censored data.\nWe build on recent work by Cand\\`es et al. (2021), whose approach first subsets\nthe data to discard any data points with early censoring times, and then uses a\nreweighting technique (namely, weighted conformal inference (Tibshirani et al.,\n2019)) to correct for the distribution shift introduced by this subsetting\nprocedure.\n\nFor our new method, instead of constraining to a fixed threshold for the\ncensoring time when subsetting the data, we allow for a covariate-dependent and\ndata-adaptive subsetting step, which is better able to capture the\nheterogeneity of the censoring mechanism. As a result, our method can lead to\nLPBs that are less conservative and give more accurate information. We show\nthat in the Type I right-censoring setting, if either of the censoring\nmechanism or the conditional quantile of survival time is well estimated, our\nproposed procedure achieves nearly exact marginal coverage, where in the latter\ncase we additionally have approximate conditional coverage. We evaluate the\nvalidity and efficiency of our proposed algorithm in numerical experiments,\nillustrating its advantage when compared with other competing methods. Finally,\nour method is applied to a real dataset to generate LPBs for users' active\ntimes on a mobile app."}, "http://arxiv.org/abs/2211.07451": {"title": "Additive Covariance Matrix Models: Modelling Regional Electricity Net-Demand in Great Britain", "link": "http://arxiv.org/abs/2211.07451", "description": "Forecasts of regional electricity net-demand, consumption minus embedded\ngeneration, are an essential input for reliable and economic power system\noperation, and energy trading. While such forecasts are typically performed\nregion by region, operations such as managing power flows require spatially\ncoherent joint forecasts, which account for cross-regional dependencies. Here,\nwe forecast the joint distribution of net-demand across the 14 regions\nconstituting Great Britain's electricity network. Joint modelling is\ncomplicated by the fact that the net-demand variability within each region, and\nthe dependencies between regions, vary with temporal, socio-economical and\nweather-related factors. We accommodate for these characteristics by proposing\na multivariate Gaussian model based on a modified Cholesky parametrisation,\nwhich allows us to model each unconstrained parameter via an additive model.\nGiven that the number of model parameters and covariates is large, we adopt a\nsemi-automated approach to model selection, based on gradient boosting. In\naddition to comparing the forecasting performance of several versions of the\nproposed model with that of two non-Gaussian copula-based models, we visually\nexplore the model output to interpret how the covariates affect net-demand\nvariability and dependencies.\n\nThe code for reproducing the results in this paper is available at\nhttps://doi.org/10.5281/zenodo.7315105, while methods for building and fitting\nmultivariate Gaussian additive models are provided by the SCM R package,\navailable at https://github.com/VinGioia90/SCM."}, "http://arxiv.org/abs/2211.16182": {"title": "Residual Permutation Test for High-Dimensional Regression Coefficient Testing", "link": "http://arxiv.org/abs/2211.16182", "description": "We consider the problem of testing whether a single coefficient is equal to\nzero in fixed-design linear models under a moderately high-dimensional regime,\nwhere the dimension of covariates $p$ is allowed to be in the same order of\nmagnitude as sample size $n$. In this regime, to achieve finite-population\nvalidity, existing methods usually require strong distributional assumptions on\nthe noise vector (such as Gaussian or rotationally invariant), which limits\ntheir applications in practice. In this paper, we propose a new method, called\nresidual permutation test (RPT), which is constructed by projecting the\nregression residuals onto the space orthogonal to the union of the column\nspaces of the original and permuted design matrices. RPT can be proved to\nachieve finite-population size validity under fixed design with just\nexchangeable noises, whenever $p &lt; n / 2$. Moreover, RPT is shown to be\nasymptotically powerful for heavy tailed noises with bounded $(1+t)$-th order\nmoment when the true coefficient is at least of order $n^{-t/(1+t)}$ for $t \\in\n[0,1]$. We further proved that this signal size requirement is essentially\nrate-optimal in the minimax sense. Numerical studies confirm that RPT performs\nwell in a wide range of simulation settings with normal and heavy-tailed noise\ndistributions."}, "http://arxiv.org/abs/2303.11399": {"title": "How Much Should We Trust Instrumental Variable Estimates in Political Science? Practical Advice Based on Over 60 Replicated Studies", "link": "http://arxiv.org/abs/2303.11399", "description": "Instrumental variable (IV) strategies are widely used in political science to\nestablish causal relationships. However, the identifying assumptions required\nby an IV design are demanding, and it remains challenging for researchers to\nassess their validity. In this paper, we replicate 67 papers published in three\ntop journals in political science during 2010-2022 and identify several\ntroubling patterns. First, researchers often overestimate the strength of their\nIVs due to non-i.i.d. errors, such as a clustering structure. Second, the most\ncommonly used t-test for the two-stage-least-squares (2SLS) estimates often\nseverely underestimates uncertainty. Using more robust inferential methods, we\nfind that around 19-30% of the 2SLS estimates in our sample are underpowered.\nThird, in the majority of the replicated studies, the 2SLS estimates are much\nlarger than the ordinary-least-squares estimates, and their ratio is negatively\ncorrelated with the strength of the IVs in studies where the IVs are not\nexperimentally generated, suggesting potential violations of unconfoundedness\nor the exclusion restriction. To help researchers avoid these pitfalls, we\nprovide a checklist for better practice."}, "http://arxiv.org/abs/2306.14351": {"title": "Comparing Causal Frameworks: Potential Outcomes, Structural Models, Graphs, and Abstractions", "link": "http://arxiv.org/abs/2306.14351", "description": "The aim of this paper is to make clear and precise the relationship between\nthe Rubin causal model (RCM) and structural causal model (SCM) frameworks for\ncausal inference. Adopting a neutral logical perspective, and drawing on\nprevious work, we show what is required for an RCM to be representable by an\nSCM. A key result then shows that every RCM -- including those that violate\nalgebraic principles implied by the SCM framework -- emerges as an abstraction\nof some representable RCM. Finally, we illustrate the power of this\nconciliatory perspective by pinpointing an important role for SCM principles in\nclassic applications of RCMs; conversely, we offer a characterization of the\nalgebraic constraints implied by a graph, helping to substantiate further\ncomparisons between the two frameworks."}, "http://arxiv.org/abs/2307.07342": {"title": "Bounded-memory adjusted scores estimation in generalized linear models with large data sets", "link": "http://arxiv.org/abs/2307.07342", "description": "The widespread use of maximum Jeffreys'-prior penalized likelihood in\nbinomial-response generalized linear models, and in logistic regression, in\nparticular, are supported by the results of Kosmidis and Firth (2021,\nBiometrika), who show that the resulting estimates are also always\nfinite-valued, even in cases where the maximum likelihood estimates are not,\nwhich is a practical issue regardless of the size of the data set. In logistic\nregression, the implied adjusted score equations are formally bias-reducing in\nasymptotic frameworks with a fixed number of parameters and appear to deliver a\nsubstantial reduction in the persistent bias of the maximum likelihood\nestimator in high-dimensional settings where the number of parameters grows\nasymptotically linearly and slower than the number of observations. In this\nwork, we develop and present two new variants of iteratively reweighted least\nsquares for estimating generalized linear models with adjusted score equations\nfor mean bias reduction and maximization of the likelihood penalized by a\npositive power of the Jeffreys-prior penalty, which eliminate the requirement\nof storing $O(n)$ quantities in memory, and can operate with data sets that\nexceed computer memory or even hard drive capacity. We achieve that through\nincremental QR decompositions, which enable IWLS iterations to have access only\nto data chunks of predetermined size. We assess the procedures through a\nreal-data application with millions of observations."}, "http://arxiv.org/abs/2309.02422": {"title": "Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test", "link": "http://arxiv.org/abs/2309.02422", "description": "Maximum mean discrepancy (MMD) refers to a general class of nonparametric\ntwo-sample tests that are based on maximizing the mean difference over samples\nfrom one distribution $P$ versus another $Q$, over all choices of data\ntransformations $f$ living in some function space $\\mathcal{F}$. Inspired by\nrecent work that connects what are known as functions of $\\textit{Radon bounded\nvariation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study\nthe MMD defined by taking $\\mathcal{F}$ to be the unit ball in the RBV space of\na given smoothness order $k \\geq 0$. This test, which we refer to as the\n$\\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as a\ngeneralization of the well-known and classical Kolmogorov-Smirnov (KS) test to\nmultiple dimensions and higher orders of smoothness. It is also intimately\nconnected to neural networks: we prove that the witness in the RKS test -- the\nfunction $f$ achieving the maximum mean difference -- is always a ridge spline\nof degree $k$, i.e., a single neuron in a neural network. This allows us to\nleverage the power of modern deep learning toolkits to (approximately) optimize\nthe criterion that underlies the RKS test. We prove that the RKS test has\nasymptotically full power at distinguishing any distinct pair $P \\not= Q$ of\ndistributions, derive its asymptotic null distribution, and carry out extensive\nexperiments to elucidate the strengths and weakenesses of the RKS test versus\nthe more traditional kernel MMD test."}, "http://arxiv.org/abs/2311.04318": {"title": "Estimation for multistate models subject to reporting delays and incomplete event adjudication", "link": "http://arxiv.org/abs/2311.04318", "description": "Complete observation of event histories is often impossible due to sampling\neffects such as right-censoring and left-truncation, but also due to reporting\ndelays and incomplete event adjudication. This is for example the case during\ninterim stages of clinical trials and for health insurance claims. In this\npaper, we develop a parametric method that takes the aforementioned effects\ninto account, treating the latter two as partially exogenous. The method, which\ntakes the form of a two-step M-estimation procedure, is applicable to\nmultistate models in general, including competing risks and recurrent event\nmodels. The effect of reporting delays is derived via thinning, extending\nexisting results for Poisson models. To address incomplete event adjudication,\nwe propose an imputed likelihood approach which, compared to existing methods,\nhas the advantage of allowing for dependencies between the event history and\nadjudication processes as well as allowing for unreported events and multiple\nevent types. We establish consistency and asymptotic normality under standard\nidentifiability, integrability, and smoothness conditions, and we demonstrate\nthe validity of the percentile bootstrap. Finally, a simulation study shows\nfavorable finite sample performance of our method compared to other\nalternatives, while an application to disability insurance data illustrates its\npractical potential."}, "http://arxiv.org/abs/2311.04359": {"title": "Flexibly Estimating and Interpreting Heterogeneous Treatment Effects of Laparoscopic Surgery for Cholecystitis Patients", "link": "http://arxiv.org/abs/2311.04359", "description": "Laparoscopic surgery has been shown through a number of randomized trials to\nbe an effective form of treatment for cholecystitis. Given this evidence, one\nnatural question for clinical practice is: does the effectiveness of\nlaparoscopic surgery vary among patients? It might be the case that, while the\noverall effect is positive, some patients treated with laparoscopic surgery may\nrespond positively to the intervention while others do not or may be harmed. In\nour study, we focus on conditional average treatment effects to understand\nwhether treatment effects vary systematically with patient characteristics.\nRecent methodological work has developed a meta-learner framework for flexible\nestimation of conditional causal effects. In this framework, nonparametric\nestimation methods can be used to avoid bias from model misspecification while\npreserving statistical efficiency. In addition, researchers can flexibly and\neffectively explore whether treatment effects vary with a large number of\npossible effect modifiers. However, these methods have certain limitations. For\nexample, conducting inference can be challenging if black-box models are used.\nFurther, interpreting and visualizing the effect estimates can be difficult\nwhen there are multi-valued effect modifiers. In this paper, we develop new\nmethods that allow for interpretable results and inference from the\nmeta-learner framework for heterogeneous treatment effects estimation. We also\ndemonstrate methods that allow for an exploratory analysis to identify possible\neffect modifiers. We apply our methods to a large database for the use of\nlaparoscopic surgery in treating cholecystitis. We also conduct a series of\nsimulation studies to understand the relative performance of the methods we\ndevelop. Our study provides key guidelines for the interpretation of\nconditional causal effects from the meta-learner framework."}, "http://arxiv.org/abs/2311.04540": {"title": "On the estimation of the number of components in multivariate functional principal component analysis", "link": "http://arxiv.org/abs/2311.04540", "description": "Happ and Greven (2018) developed a methodology for principal components\nanalysis of multivariate functional data for data observed on different\ndimensional domains. Their approach relies on an estimation of univariate\nfunctional principal components for each univariate functional feature. In this\npaper, we present extensive simulations to investigate choosing the number of\nprincipal components to retain. We show empirically that the conventional\napproach of using a percentage of variance explained threshold for each\nunivariate functional feature may be unreliable when aiming to explain an\noverall percentage of variance in the multivariate functional data, and thus we\nadvise practitioners to be careful when using it."}, "http://arxiv.org/abs/2311.04585": {"title": "Goodness-of-Fit Tests for Linear Non-Gaussian Structural Equation Models", "link": "http://arxiv.org/abs/2311.04585", "description": "The field of causal discovery develops model selection methods to infer\ncause-effect relations among a set of random variables. For this purpose,\ndifferent modelling assumptions have been proposed to render cause-effect\nrelations identifiable. One prominent assumption is that the joint distribution\nof the observed variables follows a linear non-Gaussian structural equation\nmodel. In this paper, we develop novel goodness-of-fit tests that assess the\nvalidity of this assumption in the basic setting without latent confounders as\nwell as in extension to linear models that incorporate latent confounders. Our\napproach involves testing algebraic relations among second and higher moments\nthat hold as a consequence of the linearity of the structural equations.\nSpecifically, we show that the linearity implies rank constraints on matrices\nand tensors derived from moments. For a practical implementation of our tests,\nwe consider a multiplier bootstrap method that uses incomplete U-statistics to\nestimate subdeterminants, as well as asymptotic approximations to the null\ndistribution of singular values. The methods are illustrated, in particular,\nfor the T\\\"ubingen collection of benchmark data sets on cause-effect pairs."}, "http://arxiv.org/abs/2311.04657": {"title": "Long-Term Causal Inference with Imperfect Surrogates using Many Weak Experiments, Proxies, and Cross-Fold Moments", "link": "http://arxiv.org/abs/2311.04657", "description": "Inferring causal effects on long-term outcomes using short-term surrogates is\ncrucial to rapid innovation. However, even when treatments are randomized and\nsurrogates fully mediate their effect on outcomes, it's possible that we get\nthe direction of causal effects wrong due to confounding between surrogates and\noutcomes -- a situation famously known as the surrogate paradox. The\navailability of many historical experiments offer the opportunity to instrument\nfor the surrogate and bypass this confounding. However, even as the number of\nexperiments grows, two-stage least squares has non-vanishing bias if each\nexperiment has a bounded size, and this bias is exacerbated when most\nexperiments barely move metrics, as occurs in practice. We show how to\neliminate this bias using cross-fold procedures, JIVE being one example, and\nconstruct valid confidence intervals for the long-term effect in new\nexperiments where long-term outcome has not yet been observed. Our methodology\nfurther allows to proxy for effects not perfectly mediated by the surrogates,\nallowing us to handle both confounding and effect leakage as violations of\nstandard statistical surrogacy conditions."}, "http://arxiv.org/abs/2311.04696": {"title": "Generative causality: using Shannon's information theory to infer underlying asymmetry in causal relations", "link": "http://arxiv.org/abs/2311.04696", "description": "Causal investigations in observational studies pose a great challenge in\nscientific research where randomized trials or intervention-based studies are\nnot feasible. Leveraging Shannon's seminal work on information theory, we\nconsider a framework of asymmetry where any causal link between putative cause\nand effect must be explained through a mechanism governing the cause as well as\na generative process yielding an effect of the cause. Under weak assumptions,\nthis framework enables the assessment of whether X is a stronger predictor of Y\nor vice-versa. Under stronger identifiability assumptions our framework is able\nto distinguish between cause and effect using observational data. We establish\nkey statistical properties of this framework. Our proposed methodology relies\non scalable non-parametric density estimation using fast Fourier\ntransformation. The resulting estimation method is manyfold faster than the\nclassical bandwidth-based density estimation while maintaining comparable mean\nintegrated squared error rates. We investigate key asymptotic properties of our\nmethodology and introduce a data-splitting technique to facilitate inference.\nThe key attraction of our framework is its inference toolkit, which allows\nresearchers to quantify uncertainty in causal discovery findings. We illustrate\nthe performance of our methodology through simulation studies as well as\nmultiple real data examples."}, "http://arxiv.org/abs/2311.04812": {"title": "Is it possible to obtain reliable estimates for the prevalence of anemia and childhood stunting among children under 5 in the poorest districts in Peru?", "link": "http://arxiv.org/abs/2311.04812", "description": "In this article we describe and apply the Fay-Herriot model with spatially\ncorrelated random area effects (Pratesi, M., &amp; Salvati, N. (2008)), in order to\npredict the prevalence of anemia and childhood stunting in Peruvian districts,\nbased on the data from the Demographic and Family Health Survey of the year\n2019, which collects data about anemia and childhood stunting for children\nunder the age of 12 years, and the National Census carried out in 2017. Our\nmain objective is to produce reliable predictions for the districts, where\nsample sizes are too small to provide good direct estimates, and for the\ndistricts, which were not included in the sample. The basic Fay-Herriot model\n(Fay &amp; Herriot, 1979) tackles this problem by incorporating auxiliary\ninformation, which is generally available from administrative or census\nrecords. The Fay-Herriot model with spatially correlated random area effects,\nin addition to auxiliary information, incorporates geographic information about\nthe areas, such as latitude and longitude. This permits modeling spatial\nautocorrelations, which are not unusual in socioeconomic and health surveys. To\nevaluate the mean square error of the above-mentioned predictors, we use the\nparametric bootstrap procedure, developed in Molina et al. (2009)."}, "http://arxiv.org/abs/2311.04855": {"title": "Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values", "link": "http://arxiv.org/abs/2311.04855", "description": "Non-negative matrix factorization (NMF) is a dimensionality reduction\ntechnique that has shown promise for analyzing noisy data, especially\nastronomical data. For these datasets, the observed data may contain negative\nvalues due to noise even when the true underlying physical signal is strictly\npositive. Prior NMF work has not treated negative data in a statistically\nconsistent manner, which becomes problematic for low signal-to-noise data with\nmany negative values. In this paper we present two algorithms, Shift-NMF and\nNearly-NMF, that can handle both the noisiness of the input data and also any\nintroduced negativity. Both of these algorithms use the negative data space\nwithout clipping, and correctly recover non-negative signals without any\nintroduced positive offset that occurs when clipping negative data. We\ndemonstrate this numerically on both simple and more realistic examples, and\nprove that both algorithms have monotonically decreasing update rules."}, "http://arxiv.org/abs/2311.04871": {"title": "Integration of Summary Information from External Studies for Semiparametric Models", "link": "http://arxiv.org/abs/2311.04871", "description": "With the development of biomedical science, researchers have increasing\naccess to an abundance of studies focusing on similar research questions. There\nis a growing interest in the integration of summary information from those\nstudies to enhance the efficiency of estimation in their own internal studies.\nIn this work, we present a comprehensive framework on integration of summary\ninformation from external studies when the data are modeled by semiparametric\nmodels. Our novel framework offers straightforward estimators that update\nconventional estimations with auxiliary information. It addresses computational\nchallenges by capitalizing on the intricate mathematical structure inherent to\nthe problem. We demonstrate the conditions when the proposed estimators are\ntheoretically more efficient than initial estimate based solely on internal\ndata. Several special cases such as proportional hazards model in survival\nanalysis are provided with numerical examples."}, "http://arxiv.org/abs/2107.10885": {"title": "Laplace and Saddlepoint Approximations in High Dimensions", "link": "http://arxiv.org/abs/2107.10885", "description": "We examine the behaviour of the Laplace and saddlepoint approximations in the\nhigh-dimensional setting, where the dimension of the model is allowed to\nincrease with the number of observations. Approximations to the joint density,\nthe marginal posterior density and the conditional density are considered. Our\nresults show that under the mildest assumptions on the model, the error of the\njoint density approximation is $O(p^4/n)$ if $p = o(n^{1/4})$ for the Laplace\napproximation and saddlepoint approximation, and $O(p^3/n)$ if $p = o(n^{1/3})$\nunder additional assumptions on the second derivative of the log-likelihood.\nStronger results are obtained for the approximation to the marginal posterior\ndensity."}, "http://arxiv.org/abs/2111.00280": {"title": "Testing semiparametric model-equivalence hypotheses based on the characteristic function", "link": "http://arxiv.org/abs/2111.00280", "description": "We propose three test criteria each of which is appropriate for testing,\nrespectively, the equivalence hypotheses of symmetry, of homogeneity, and of\nindependence, with multivariate data. All quantities have the common feature of\ninvolving weighted--type distances between characteristic functions and are\nconvenient from the computational point of view if the weight function is\nproperly chosen. The asymptotic behavior of the tests under the null hypothesis\nis investigated, and numerical studies are conducted in order to examine the\nperformance of the criteria in finite samples."}, "http://arxiv.org/abs/2112.11079": {"title": "Data fission: splitting a single data point", "link": "http://arxiv.org/abs/2112.11079", "description": "Suppose we observe a random vector $X$ from some distribution $P$ in a known\nfamily with unknown parameters. We ask the following question: when is it\npossible to split $X$ into two parts $f(X)$ and $g(X)$ such that neither part\nis sufficient to reconstruct $X$ by itself, but both together can recover $X$\nfully, and the joint distribution of $(f(X),g(X))$ is tractable? As one\nexample, if $X=(X_1,\\dots,X_n)$ and $P$ is a product distribution, then for any\n$m&lt;n$, we can split the sample to define $f(X)=(X_1,\\dots,X_m)$ and\n$g(X)=(X_{m+1},\\dots,X_n)$. Rasines and Young (2022) offers an alternative\nroute of accomplishing this task through randomization of $X$ with additive\nGaussian noise which enables post-selection inference in finite samples for\nGaussian distributed data and asymptotically for non-Gaussian additive models.\nIn this paper, we offer a more general methodology for achieving such a split\nin finite samples by borrowing ideas from Bayesian inference to yield a\n(frequentist) solution that can be viewed as a continuous analog of data\nsplitting. We call our method data fission, as an alternative to data\nsplitting, data carving and p-value masking. We exemplify the method on a few\nprototypical applications, such as post-selection inference for trend filtering\nand other regression problems."}, "http://arxiv.org/abs/2207.04598": {"title": "Differential item functioning via robust scaling", "link": "http://arxiv.org/abs/2207.04598", "description": "This paper proposes a method for assessing differential item functioning\n(DIF) in item response theory (IRT) models. The method does not require\npre-specification of anchor items, which is its main virtue. It is developed in\ntwo main steps, first by showing how DIF can be re-formulated as a problem of\noutlier detection in IRT-based scaling, then tackling the latter using methods\nfrom robust statistics. The proposal is a redescending M-estimator of IRT\nscaling parameters that is tuned to flag items with DIF at the desired\nasymptotic Type I Error rate. Theoretical results describe the efficiency of\nthe estimator in the absence of DIF and its robustness in the presence of DIF.\nSimulation studies show that the proposed method compares favorably to\ncurrently available approaches for DIF detection, and a real data example\nillustrates its application in a research context where pre-specification of\nanchor items is infeasible. The focus of the paper is the two-parameter\nlogistic model in two independent groups, with extensions to other settings\nconsidered in the conclusion."}, "http://arxiv.org/abs/2208.02028": {"title": "Bootstrap inference in the presence of bias", "link": "http://arxiv.org/abs/2208.02028", "description": "We consider bootstrap inference for estimators which are (asymptotically)\nbiased. We show that, even when the bias term cannot be consistently estimated,\nvalid inference can be obtained by proper implementations of the bootstrap.\nSpecifically, we show that the prepivoting approach of Beran (1987, 1988),\noriginally proposed to deliver higher-order refinements, restores bootstrap\nvalidity by transforming the original bootstrap p-value into an asymptotically\nuniform random variable. We propose two different implementations of\nprepivoting (plug-in and double bootstrap), and provide general high-level\nconditions that imply validity of bootstrap inference. To illustrate the\npractical relevance and implementation of our results, we discuss five\nexamples: (i) inference on a target parameter based on model averaging; (ii)\nridge-type regularized estimators; (iii) nonparametric regression; (iv) a\nlocation model for infinite variance data; and (v) dynamic panel data models."}, "http://arxiv.org/abs/2209.12345": {"title": "Berry-Esseen bounds for design-based causal inference with possibly diverging treatment levels and varying group sizes", "link": "http://arxiv.org/abs/2209.12345", "description": "Neyman (1923/1990) introduced the randomization model, which contains the\nnotation of potential outcomes to define causal effects and a framework for\nlarge-sample inference based on the design of the experiment. However, the\nexisting theory for this framework is far from complete especially when the\nnumber of treatment levels diverges and the treatment group sizes vary. We\nprovide a unified discussion of statistical inference under the randomization\nmodel with general treatment group sizes. We formulate the estimator in terms\nof a linear permutational statistic and use results based on Stein's method to\nderive various Berry--Esseen bounds on the linear and quadratic functions of\nthe estimator. These new Berry--Esseen bounds serve as basis for design-based\ncausal inference with possibly diverging treatment levels and a diverging\nnumber of causal parameters of interest. We also fill an important gap by\nproposing novel variance estimators for experiments with possibly many\ntreatment levels without replications. Equipped with the newly developed\nresults, design-based causal inference in general settings becomes more\nconvenient with stronger theoretical guarantees."}, "http://arxiv.org/abs/2210.02014": {"title": "Doubly Robust Proximal Synthetic Controls", "link": "http://arxiv.org/abs/2210.02014", "description": "To infer the treatment effect for a single treated unit using panel data,\nsynthetic control methods construct a linear combination of control units'\noutcomes that mimics the treated unit's pre-treatment outcome trajectory. This\nlinear combination is subsequently used to impute the counterfactual outcomes\nof the treated unit had it not been treated in the post-treatment period, and\nused to estimate the treatment effect. Existing synthetic control methods rely\non correctly modeling certain aspects of the counterfactual outcome generating\nmechanism and may require near-perfect matching of the pre-treatment\ntrajectory. Inspired by proximal causal inference, we obtain two novel\nnonparametric identifying formulas for the average treatment effect for the\ntreated unit: one is based on weighting, and the other combines models for the\ncounterfactual outcome and the weighting function. We introduce the concept of\ncovariate shift to synthetic controls to obtain these identification results\nconditional on the treatment assignment. We also develop two treatment effect\nestimators based on these two formulas and the generalized method of moments.\nOne new estimator is doubly robust: it is consistent and asymptotically normal\nif at least one of the outcome and weighting models is correctly specified. We\ndemonstrate the performance of the methods via simulations and apply them to\nevaluate the effectiveness of a Pneumococcal conjugate vaccine on the risk of\nall-cause pneumonia in Brazil."}, "http://arxiv.org/abs/2303.01385": {"title": "Hyperlink communities in higher-order networks", "link": "http://arxiv.org/abs/2303.01385", "description": "Many networks can be characterised by the presence of communities, which are\ngroups of units that are closely linked and can be relevant in understanding\nthe system's overall function. Recently, hypergraphs have emerged as a\nfundamental tool for modelling systems where interactions are not limited to\npairs but may involve an arbitrary number of nodes. Using a dual approach to\ncommunity detection, in this study we extend the concept of link communities to\nhypergraphs, allowing us to extract informative clusters of highly related\nhyperedges. We analyze the dendrograms obtained by applying hierarchical\nclustering to distance matrices among hyperedges on a variety of real-world\ndata, showing that hyperlink communities naturally highlight the hierarchical\nand multiscale structure of higher-order networks. Moreover, by using hyperlink\ncommunities, we are able to extract overlapping memberships from nodes,\novercoming limitations of traditional hard clustering methods. Finally, we\nintroduce higher-order network cartography as a practical tool for categorizing\nnodes into different structural roles based on their interaction patterns and\ncommunity participation. This approach helps identify different types of\nindividuals in a variety of real-world social systems. Our work contributes to\na better understanding of the structural organization of real-world\nhigher-order systems."}, "http://arxiv.org/abs/2305.05931": {"title": "Generalised shot noise representations of stochastic systems driven by non-Gaussian L\\'evy processes", "link": "http://arxiv.org/abs/2305.05931", "description": "We consider the problem of obtaining effective representations for the\nsolutions of linear, vector-valued stochastic differential equations (SDEs)\ndriven by non-Gaussian pure-jump L\\'evy processes, and we show how such\nrepresentations lead to efficient simulation methods. The processes considered\nconstitute a broad class of models that find application across the physical\nand biological sciences, mathematics, finance and engineering. Motivated by\nimportant relevant problems in statistical inference, we derive new,\ngeneralised shot-noise simulation methods whenever a normal variance-mean (NVM)\nmixture representation exists for the driving L\\'evy process, including the\ngeneralised hyperbolic, normal-Gamma, and normal tempered stable cases. Simple,\nexplicit conditions are identified for the convergence of the residual of a\ntruncated shot-noise representation to a Brownian motion in the case of the\npure L\\'evy process, and to a Brownian-driven SDE in the case of the\nL\\'evy-driven SDE. These results provide Gaussian approximations to the small\njumps of the process under the NVM representation. The resulting\nrepresentations are of particular importance in state inference and parameter\nestimation for L\\'evy-driven SDE models, since the resulting conditionally\nGaussian structures can be readily incorporated into latent variable inference\nmethods such as Markov chain Monte Carlo (MCMC), Expectation-Maximisation (EM),\nand sequential Monte Carlo."}, "http://arxiv.org/abs/2305.17517": {"title": "Stochastic Nonparametric Estimation of the Density-Flow Curve", "link": "http://arxiv.org/abs/2305.17517", "description": "The fundamental diagram serves as the foundation of traffic flow modeling for\nalmost a century. With the increasing availability of road sensor data,\ndeterministic parametric models have proved inadequate in describing the\nvariability of real-world data, especially in congested area of the\ndensity-flow diagram. In this paper we estimate the stochastic density-flow\nrelation introducing a nonparametric method called convex quantile regression.\nThe proposed method does not depend on any prior functional form assumptions,\nbut thanks to the concavity constraints, the estimated function satisfies the\ntheoretical properties of the density-flow curve. The second contribution is to\ndevelop the new convex quantile regression with bags (CQRb) approach to\nfacilitate practical implementation of CQR to the real-world data. We\nillustrate the CQRb estimation process using the road sensor data from Finland\nin years 2016-2018. Our third contribution is to demonstrate the excellent\nout-of-sample predictive power of the proposed CQRb method in comparison to the\nstandard parametric deterministic approach."}, "http://arxiv.org/abs/2305.19180": {"title": "Transfer Learning With Efficient Estimators to Optimally Leverage Historical Data in Analysis of Randomized Trials", "link": "http://arxiv.org/abs/2305.19180", "description": "Although randomized controlled trials (RCTs) are a cornerstone of comparative\neffectiveness, they typically have much smaller sample size than observational\nstudies because of financial and ethical considerations. Therefore there is\ninterest in using plentiful historical data (either observational data or prior\ntrials) to reduce trial sizes. Previous estimators developed for this purpose\nrely on unrealistic assumptions, without which the added data can bias the\ntreatment effect estimate. Recent work proposed an alternative method\n(prognostic covariate adjustment) that imposes no additional assumptions and\nincreases efficiency in trial analyses. The idea is to use historical data to\nlearn a prognostic model: a regression of the outcome onto the covariates. The\npredictions from this model, generated from the RCT subjects' baseline\nvariables, are then used as a covariate in a linear regression analysis of the\ntrial data. In this work, we extend prognostic adjustment to trial analyses\nwith nonparametric efficient estimators, which are more powerful than linear\nregression. We provide theory that explains why prognostic adjustment improves\nsmall-sample point estimation and inference without any possibility of bias.\nSimulations corroborate the theory: efficient estimators using prognostic\nadjustment compared to without provides greater power (i.e., smaller standard\nerrors) when the trial is small. Population shifts between historical and trial\ndata attenuate benefits but do not introduce bias. We showcase our estimator\nusing clinical trial data provided by Novo Nordisk A/S that evaluates insulin\ntherapy for individuals with type II diabetes."}, "http://arxiv.org/abs/2308.13928": {"title": "A flexible Bayesian tool for CoDa mixed models: logistic-normal distribution with Dirichlet covariance", "link": "http://arxiv.org/abs/2308.13928", "description": "Compositional Data Analysis (CoDa) has gained popularity in recent years.\nThis type of data consists of values from disjoint categories that sum up to a\nconstant. Both Dirichlet regression and logistic-normal regression have become\npopular as CoDa analysis methods. However, fitting this kind of multivariate\nmodels presents challenges, especially when structured random effects are\nincluded in the model, such as temporal or spatial effects.\n\nTo overcome these challenges, we propose the logistic-normal Dirichlet Model\n(LNDM). We seamlessly incorporate this approach into the R-INLA package,\nfacilitating model fitting and model prediction within the framework of Latent\nGaussian Models (LGMs). Moreover, we explore metrics like Deviance Information\nCriteria (DIC), Watanabe Akaike information criterion (WAIC), and\ncross-validation measure conditional predictive ordinate (CPO) for model\nselection in R-INLA for CoDa.\n\nIllustrating LNDM through a simple simulated example and with an ecological\ncase study on Arabidopsis thaliana in the Iberian Peninsula, we underscore its\npotential as an effective tool for managing CoDa and large CoDa databases."}, "http://arxiv.org/abs/2310.02968": {"title": "Sampling depth trade-off in function estimation under a two-level design", "link": "http://arxiv.org/abs/2310.02968", "description": "Many modern statistical applications involve a two-level sampling scheme that\nfirst samples subjects from a population and then samples observations on each\nsubject. These schemes often are designed to learn both the population-level\nfunctional structures shared by the subjects and the functional characteristics\nspecific to individual subjects. Common wisdom suggests that learning\npopulation-level structures benefits from sampling more subjects whereas\nlearning subject-specific structures benefits from deeper sampling within each\nsubject. Oftentimes these two objectives compete for limited sampling\nresources, which raises the question of how to optimally sample at the two\nlevels. We quantify such sampling-depth trade-offs by establishing the $L_2$\nminimax risk rates for learning the population-level and subject-specific\nstructures under a hierarchical Gaussian process model framework where we\nconsider a Bayesian and a frequentist perspective on the unknown\npopulation-level structure. These rates provide general lessons for designing\ntwo-level sampling schemes. Interestingly, subject-specific learning\noccasionally benefits more by sampling more subjects than by deeper\nwithin-subject sampling. We also construct estimators that adapt to unknown\nsmoothness and achieve the corresponding minimax rates. We conduct two\nsimulation experiments validating our theory and illustrating the sampling\ntrade-off in practice, and apply these estimators to two real datasets."}, "http://arxiv.org/abs/2311.05025": {"title": "Unbiased Kinetic Langevin Monte Carlo with Inexact Gradients", "link": "http://arxiv.org/abs/2311.05025", "description": "We present an unbiased method for Bayesian posterior means based on kinetic\nLangevin dynamics that combines advanced splitting methods with enhanced\ngradient approximations. Our approach avoids Metropolis correction by coupling\nMarkov chains at different discretization levels in a multilevel Monte Carlo\napproach. Theoretical analysis demonstrates that our proposed estimator is\nunbiased, attains finite variance, and satisfies a central limit theorem. It\ncan achieve accuracy $\\epsilon&gt;0$ for estimating expectations of Lipschitz\nfunctions in $d$ dimensions with $\\mathcal{O}(d^{1/4}\\epsilon^{-2})$ expected\ngradient evaluations, without assuming warm start. We exhibit similar bounds\nusing both approximate and stochastic gradients, and our method's computational\ncost is shown to scale logarithmically with the size of the dataset. The\nproposed method is tested using a multinomial regression problem on the MNIST\ndataset and a Poisson regression model for soccer scores. Experiments indicate\nthat the number of gradient evaluations per effective sample is independent of\ndimension, even when using inexact gradients. For product distributions, we\ngive dimension-independent variance bounds. Our results demonstrate that the\nunbiased algorithm we present can be much more efficient than the\n``gold-standard\" randomized Hamiltonian Monte Carlo."}, "http://arxiv.org/abs/2311.05056": {"title": "High-dimensional Newey-Powell Test Via Approximate Message Passing", "link": "http://arxiv.org/abs/2311.05056", "description": "Homoscedastic regression error is a common assumption in many\nhigh-dimensional regression models and theories. Although heteroscedastic error\ncommonly exists in real-world datasets, testing heteroscedasticity remains\nlargely underexplored under high-dimensional settings. We consider the\nheteroscedasticity test proposed in Newey and Powell (1987), whose asymptotic\ntheory has been well-established for the low-dimensional setting. We show that\nthe Newey-Powell test can be developed for high-dimensional data. For\nasymptotic theory, we consider the setting where the number of dimensions grows\nwith the sample size at a linear rate. The asymptotic analysis for the test\nstatistic utilizes the Approximate Message Passing (AMP) algorithm, from which\nwe obtain the limiting distribution of the test. The numerical performance of\nthe test is investigated through an extensive simulation study. As real-data\napplications, we present the analysis based on \"international economic growth\"\ndata (Belloni et al. 2011), which is found to be homoscedastic, and\n\"supermarket\" data (Lan et al., 2016), which is found to be heteroscedastic."}, "http://arxiv.org/abs/2311.05200": {"title": "An efficient Bayesian approach to joint functional principal component analysis for complex sampling designs", "link": "http://arxiv.org/abs/2311.05200", "description": "The analysis of multivariate functional curves has the potential to yield\nimportant scientific discoveries in domains such as healthcare, medicine,\neconomics and social sciences. However it is common for real-world settings to\npresent data that are both sparse and irregularly sampled, and this introduces\nimportant challenges for the current functional data methodology. Here we\npropose a Bayesian hierarchical framework for multivariate functional principal\ncomponent analysis which accommodates the intricacies of such sampling designs\nby flexibly pooling information across subjects and correlated curves. Our\nmodel represents common latent dynamics via shared functional principal\ncomponent scores, thereby effectively borrowing strength across curves while\ncircumventing the computationally challenging task of estimating covariance\nmatrices. These scores also provide a parsimonious representation of the major\nmodes of joint variation of the curves, and constitute interpretable scalar\nsummaries that can be employed in follow-up analyses. We perform inference\nusing a variational message passing algorithm which combines efficiency,\nmodularity and approximate posterior density estimation, enabling the joint\nanalysis of large datasets with parameter uncertainty quantification. We\nconduct detailed simulations to assess the effectiveness of our approach in\nsharing information under complex sampling designs. We also exploit it to\nestimate the molecular disease courses of individual patients with SARS-CoV-2\ninfection and characterise patient heterogeneity in recovery outcomes; this\nstudy reveals key coordinated dynamics across the immune, inflammatory and\nmetabolic systems, which are associated with survival and long-COVID symptoms\nup to one year post disease onset. Our approach is implemented in the R package\nbayesFPCA."}, "http://arxiv.org/abs/2311.05248": {"title": "A General Space of Belief Updates for Model Misspecification in Bayesian Networks", "link": "http://arxiv.org/abs/2311.05248", "description": "In an ideal setting for Bayesian agents, a perfect description of the rules\nof the environment (i.e., the objective observation model) is available,\nallowing them to reason through the Bayesian posterior to update their beliefs\nin an optimal way. But such an ideal setting hardly ever exists in the natural\nworld, so agents have to make do with reasoning about how they should update\ntheir beliefs simultaneously. This introduces a number of related challenges\nfor a number of research areas: (1) For Bayesian statistics, this deviation of\nthe subjective model from the true data-generating mechanism is termed model\nmisspecification in the literature. (2) For neuroscience, it introduces the\nnecessity to model how the agents' belief updates (how they use evidence to\nupdate their belief) and how their belief changes over time. The current paper\naddresses these two challenges by (a) providing a general class of\nposteriors/belief updates called cut-posteriors of Bayesian networks that have\na much greater expressivity, and (b) parameterizing the space of possible\nposteriors to make meta-learning (i.e., choosing the belief update from this\nspace in a principled manner) possible. For (a), it is noteworthy that any\ncut-posterior has local computation only, making computation tractable for\nhuman or artificial agents. For (b), a Markov Chain Monte Carlo algorithm to\nperform such meta-learning will be sketched here, though it is only an\nillustration and but no means the only possible meta-learning procedure\npossible for the space of cut-posteriors. Operationally, this work gives a\ngeneral algorithm to take in an arbitrary Bayesian network and output all\npossible cut-posteriors in the space."}, "http://arxiv.org/abs/2311.05272": {"title": "deform: An R Package for Nonstationary Spatial Gaussian Process Models by Deformations and Dimension Expansion", "link": "http://arxiv.org/abs/2311.05272", "description": "Gaussian processes (GP) are a popular and powerful tool for spatial modelling\nof data, especially data that quantify environmental processes. However, in\nstationary form, whether covariance is isotropic or anisotropic, GPs may lack\nthe flexibility to capture dependence across a continuous spatial process,\nespecially across a large domains. The deform package aims to provide users\nwith user-friendly R functions for the fitting and visualization of\nnonstationary spatial Gaussian processes. Users can choose to capture\nnonstationarity with either the spatial deformation approach of Sampson and\nGuttorp (1992) or the dimension expansion approach of Bornn, Shaddick, and\nZidek (2012). Thin plate regression splines are used for both approaches to\nbring transformations of locations to give a new set of locations that bring\nisotropic covariance. Fitted models in deform can be used to predict these new\nlocations and to simulate nonstationary Gaussian processes for an arbitrary set\nof locations."}, "http://arxiv.org/abs/2311.05330": {"title": "Applying a new category association estimator to sentiment analysis on the Web", "link": "http://arxiv.org/abs/2311.05330", "description": "This paper introduces a novel Bayesian method for measuring the degree of\nassociation between categorical variables. The method is grounded in the formal\ndefinition of variable independence and was implemented using MCMC techniques.\nUnlike existing methods, this approach does not assume prior knowledge of the\ntotal number of occurrences for any category, making it particularly\nwell-suited for applications like sentiment analysis. We applied the method to\na dataset comprising 4,613 tweets written in Portuguese, each annotated for 30\npossibly overlapping emotional categories. Through this analysis, we identified\npairs of emotions that exhibit associations and mutually exclusive pairs.\nFurthermore, the method identifies hierarchical relations between categories, a\nfeature observed in our data, and was used to cluster emotions into basic level\ngroups."}, "http://arxiv.org/abs/2311.05339": {"title": "An iterative algorithm for high-dimensional linear models with both sparse and non-sparse structures", "link": "http://arxiv.org/abs/2311.05339", "description": "Numerous practical medical problems often involve data that possess a\ncombination of both sparse and non-sparse structures. Traditional penalized\nregularizations techniques, primarily designed for promoting sparsity, are\ninadequate to capture the optimal solutions in such scenarios. To address these\nchallenges, this paper introduces a novel algorithm named Non-sparse Iteration\n(NSI). The NSI algorithm allows for the existence of both sparse and non-sparse\nstructures and estimates them simultaneously and accurately. We provide\ntheoretical guarantees that the proposed algorithm converges to the oracle\nsolution and achieves the optimal rate for the upper bound of the $l_2$-norm\nerror. Through simulations and practical applications, NSI consistently\nexhibits superior statistical performance in terms of estimation accuracy,\nprediction efficacy, and variable selection compared to several existing\nmethods. The proposed method is also applied to breast cancer data, revealing\nrepeated selection of specific genes for in-depth analysis."}, "http://arxiv.org/abs/2311.05421": {"title": "Diffusion Based Causal Representation Learning", "link": "http://arxiv.org/abs/2311.05421", "description": "Causal reasoning can be considered a cornerstone of intelligent systems.\nHaving access to an underlying causal graph comes with the promise of\ncause-effect estimation and the identification of efficient and safe\ninterventions. However, learning causal representations remains a major\nchallenge, due to the complexity of many real-world systems. Previous works on\ncausal representation learning have mostly focused on Variational Auto-Encoders\n(VAE). These methods only provide representations from a point estimate, and\nthey are unsuitable to handle high dimensions. To overcome these problems, we\nproposed a new Diffusion-based Causal Representation Learning (DCRL) algorithm.\nThis algorithm uses diffusion-based representations for causal discovery. DCRL\noffers access to infinite dimensional latent codes, which encode different\nlevels of information in the latent code. In a first proof of principle, we\ninvestigate the use of DCRL for causal representation learning. We further\ndemonstrate experimentally that this approach performs comparably well in\nidentifying the causal structure and causal variables."}, "http://arxiv.org/abs/2311.05532": {"title": "Uncertainty-Aware Bayes' Rule and Its Applications", "link": "http://arxiv.org/abs/2311.05532", "description": "Bayes' rule has enabled innumerable powerful algorithms of statistical signal\nprocessing and statistical machine learning. However, when there exist model\nmisspecifications in prior distributions and/or data distributions, the direct\napplication of Bayes' rule is questionable. Philosophically, the key is to\nbalance the relative importance of prior and data distributions when\ncalculating posterior distributions: if prior (resp. data) distributions are\noverly conservative, we should upweight the prior belief (resp. data evidence);\nif prior (resp. data) distributions are overly opportunistic, we should\ndownweight the prior belief (resp. data evidence). This paper derives a\ngeneralized Bayes' rule, called uncertainty-aware Bayes' rule, to technically\nrealize the above philosophy, i.e., to combat the model uncertainties in prior\ndistributions and/or data distributions. Simulated and real-world experiments\nshowcase the superiority of the presented uncertainty-aware Bayes' rule over\nthe conventional Bayes' rule: In particular, the uncertainty-aware Kalman\nfilter, the uncertainty-aware particle filter, and the uncertainty-aware\ninteractive multiple model filter are suggested and validated."}, "http://arxiv.org/abs/2107.13737": {"title": "Design-Robust Two-Way-Fixed-Effects Regression For Panel Data", "link": "http://arxiv.org/abs/2107.13737", "description": "We propose a new estimator for average causal effects of a binary treatment\nwith panel data in settings with general treatment patterns. Our approach\naugments the popular two-way-fixed-effects specification with unit-specific\nweights that arise from a model for the assignment mechanism. We show how to\nconstruct these weights in various settings, including the staggered adoption\nsetting, where units opt into the treatment sequentially but permanently. The\nresulting estimator converges to an average (over units and time) treatment\neffect under the correct specification of the assignment model, even if the\nfixed effect model is misspecified. We show that our estimator is more robust\nthan the conventional two-way estimator: it remains consistent if either the\nassignment mechanism or the two-way regression model is correctly specified. In\naddition, the proposed estimator performs better than the two-way-fixed-effect\nestimator if the outcome model and assignment mechanism are locally\nmisspecified. This strong double robustness property underlines and quantifies\nthe benefits of modeling the assignment process and motivates using our\nestimator in practice. We also discuss an extension of our estimator to handle\ndynamic treatment effects."}, "http://arxiv.org/abs/2110.00115": {"title": "Comparing Sequential Forecasters", "link": "http://arxiv.org/abs/2110.00115", "description": "Consider two forecasters, each making a single prediction for a sequence of\nevents over time. We ask a relatively basic question: how might we compare\nthese forecasters, either online or post-hoc, while avoiding unverifiable\nassumptions on how the forecasts and outcomes were generated? In this paper, we\npresent a rigorous answer to this question by designing novel sequential\ninference procedures for estimating the time-varying difference in forecast\nscores. To do this, we employ confidence sequences (CS), which are sequences of\nconfidence intervals that can be continuously monitored and are valid at\narbitrary data-dependent stopping times (\"anytime-valid\"). The widths of our\nCSs are adaptive to the underlying variance of the score differences.\nUnderlying their construction is a game-theoretic statistical framework, in\nwhich we further identify e-processes and p-processes for sequentially testing\na weak null hypothesis -- whether one forecaster outperforms another on average\n(rather than always). Our methods do not make distributional assumptions on the\nforecasts or outcomes; our main theorems apply to any bounded scores, and we\nlater provide alternative methods for unbounded scores. We empirically validate\nour approaches by comparing real-world baseball and weather forecasters."}, "http://arxiv.org/abs/2207.14753": {"title": "Estimating Causal Effects with Hidden Confounding using Instrumental Variables and Environments", "link": "http://arxiv.org/abs/2207.14753", "description": "Recent works have proposed regression models which are invariant across data\ncollection environments. These estimators often have a causal interpretation\nunder conditions on the environments and type of invariance imposed. One recent\nexample, the Causal Dantzig (CD), is consistent under hidden confounding and\nrepresents an alternative to classical instrumental variable estimators such as\nTwo Stage Least Squares (TSLS). In this work we derive the CD as a generalized\nmethod of moments (GMM) estimator. The GMM representation leads to several\npractical results, including 1) creation of the Generalized Causal Dantzig\n(GCD) estimator which can be applied to problems with continuous environments\nwhere the CD cannot be fit 2) a Hybrid (GCD-TSLS combination) estimator which\nhas properties superior to GCD or TSLS alone 3) straightforward asymptotic\nresults for all methods using GMM theory. We compare the CD, GCD, TSLS, and\nHybrid estimators in simulations and an application to a Flow Cytometry data\nset. The newly proposed GCD and Hybrid estimators have superior performance to\nexisting methods in many settings."}, "http://arxiv.org/abs/2208.02948": {"title": "A Feature Selection Method that Controls the False Discovery Rate", "link": "http://arxiv.org/abs/2208.02948", "description": "The problem of selecting a handful of truly relevant variables in supervised\nmachine learning algorithms is a challenging problem in terms of untestable\nassumptions that must hold and unavailability of theoretical assurances that\nselection errors are under control. We propose a distribution-free feature\nselection method, referred to as Data Splitting Selection (DSS) which controls\nFalse Discovery Rate (FDR) of feature selection while obtaining a high power.\nAnother version of DSS is proposed with a higher power which \"almost\" controls\nFDR. No assumptions are made on the distribution of the response or on the\njoint distribution of the features. Extensive simulation is performed to\ncompare the performance of the proposed methods with the existing ones."}, "http://arxiv.org/abs/2209.03474": {"title": "An extension of the Unified Skew-Normal family of distributions and application to Bayesian binary regression", "link": "http://arxiv.org/abs/2209.03474", "description": "We consider the general problem of Bayesian binary regression and we\nintroduce a new class of distributions, the Perturbed Unified Skew Normal\n(pSUN, henceforth), which generalizes the Unified Skew-Normal (SUN) class. We\nshow that the new class is conjugate to any binary regression model, provided\nthat the link function may be expressed as a scale mixture of Gaussian\ndensities. We discuss in detail the popular logit case, and we show that, when\na logistic regression model is combined with a Gaussian prior, posterior\nsummaries such as cumulants and normalizing constants can be easily obtained\nthrough the use of an importance sampling approach, opening the way to\nstraightforward variable selection procedures. For more general priors, the\nproposed methodology is based on a simple Gibbs sampler algorithm. We also\nclaim that, in the p &gt; n case, the proposed methodology shows better\nperformances - both in terms of mixing and accuracy - compared to the existing\nmethods. We illustrate the performance through several simulation studies and\ntwo data analyses."}, "http://arxiv.org/abs/2211.15070": {"title": "Online Kernel CUSUM for Change-Point Detection", "link": "http://arxiv.org/abs/2211.15070", "description": "We present a computationally efficient online kernel Cumulative Sum (CUSUM)\nmethod for change-point detection that utilizes the maximum over a set of\nkernel statistics to account for the unknown change-point location. Our\napproach exhibits increased sensitivity to small changes compared to existing\nkernel-based change-point detection methods, including Scan-B statistic,\ncorresponding to a non-parametric Shewhart chart-type procedure. We provide\naccurate analytic approximations for two key performance metrics: the Average\nRun Length (ARL) and Expected Detection Delay (EDD), which enable us to\nestablish an optimal window length to be on the order of the logarithm of ARL\nto ensure minimal power loss relative to an oracle procedure with infinite\nmemory. Moreover, we introduce a recursive calculation procedure for detection\nstatistics to ensure constant computational and memory complexity, which is\nessential for online implementation. Through extensive experiments on both\nsimulated and real data, we demonstrate the competitive performance of our\nmethod and validate our theoretical results."}, "http://arxiv.org/abs/2301.09633": {"title": "Prediction-Powered Inference", "link": "http://arxiv.org/abs/2301.09633", "description": "Prediction-powered inference is a framework for performing valid statistical\ninference when an experimental dataset is supplemented with predictions from a\nmachine-learning system. The framework yields simple algorithms for computing\nprovably valid confidence intervals for quantities such as means, quantiles,\nand linear and logistic regression coefficients, without making any assumptions\non the machine-learning algorithm that supplies the predictions. Furthermore,\nmore accurate predictions translate to smaller confidence intervals.\nPrediction-powered inference could enable researchers to draw valid and more\ndata-efficient conclusions using machine learning. The benefits of\nprediction-powered inference are demonstrated with datasets from proteomics,\nastronomy, genomics, remote sensing, census analysis, and ecology."}, "http://arxiv.org/abs/2303.03521": {"title": "Bayesian Adaptive Selection of Variables for Function-on-Scalar Regression Models", "link": "http://arxiv.org/abs/2303.03521", "description": "Considering the field of functional data analysis, we developed a new\nBayesian method for variable selection in function-on-scalar regression (FOSR).\nOur approach uses latent variables, allowing an adaptive selection since it can\ndetermine the number of variables and which ones should be selected for a\nfunction-on-scalar regression model. Simulation studies show the proposed\nmethod's main properties, such as its accuracy in estimating the coefficients\nand high capacity to select variables correctly. Furthermore, we conducted\ncomparative studies with the main competing methods, such as the BGLSS method\nas well as the group LASSO, the group MCP and the group SCAD. We also used a\nCOVID-19 dataset and some socioeconomic data from Brazil for real data\napplication. In short, the proposed Bayesian variable selection model is\nextremely competitive, showing significant predictive and selective quality."}, "http://arxiv.org/abs/2305.10564": {"title": "Counterfactually Comparing Abstaining Classifiers", "link": "http://arxiv.org/abs/2305.10564", "description": "Abstaining classifiers have the option to abstain from making predictions on\ninputs that they are unsure about. These classifiers are becoming increasingly\npopular in high-stakes decision-making problems, as they can withhold uncertain\npredictions to improve their reliability and safety. When evaluating black-box\nabstaining classifier(s), however, we lack a principled approach that accounts\nfor what the classifier would have predicted on its abstentions. These missing\npredictions matter when they can eventually be utilized, either directly or as\na backup option in a failure mode. In this paper, we introduce a novel approach\nand perspective to the problem of evaluating and comparing abstaining\nclassifiers by treating abstentions as missing data. Our evaluation approach is\ncentered around defining the counterfactual score of an abstaining classifier,\ndefined as the expected performance of the classifier had it not been allowed\nto abstain. We specify the conditions under which the counterfactual score is\nidentifiable: if the abstentions are stochastic, and if the evaluation data is\nindependent of the training data (ensuring that the predictions are missing at\nrandom), then the score is identifiable. Note that, if abstentions are\ndeterministic, then the score is unidentifiable because the classifier can\nperform arbitrarily poorly on its abstentions. Leveraging tools from\nobservational causal inference, we then develop nonparametric and doubly robust\nmethods to efficiently estimate this quantity under identification. Our\napproach is examined in both simulated and real data experiments."}, "http://arxiv.org/abs/2307.01539": {"title": "Implementing measurement error models with mechanistic mathematical models in a likelihood-based framework for estimation, identifiability analysis, and prediction in the life sciences", "link": "http://arxiv.org/abs/2307.01539", "description": "Throughout the life sciences we routinely seek to interpret measurements and\nobservations using parameterised mechanistic mathematical models. A fundamental\nand often overlooked choice in this approach involves relating the solution of\na mathematical model with noisy and incomplete measurement data. This is often\nachieved by assuming that the data are noisy measurements of the solution of a\ndeterministic mathematical model, and that measurement errors are additive and\nnormally distributed. While this assumption of additive Gaussian noise is\nextremely common and simple to implement and interpret, it is often unjustified\nand can lead to poor parameter estimates and non-physical predictions. One way\nto overcome this challenge is to implement a different measurement error model.\nIn this review, we demonstrate how to implement a range of measurement error\nmodels in a likelihood-based framework for estimation, identifiability\nanalysis, and prediction, called Profile-Wise Analysis. This frequentist\napproach to uncertainty quantification for mechanistic models leverages the\nprofile likelihood for targeting parameters and understanding their influence\non predictions. Case studies, motivated by simple caricature models routinely\nused in systems biology and mathematical biology literature, illustrate how the\nsame ideas apply to different types of mathematical models. Open-source Julia\ncode to reproduce results is available on GitHub."}, "http://arxiv.org/abs/2307.04548": {"title": "Beyond the Two-Trials Rule", "link": "http://arxiv.org/abs/2307.04548", "description": "The two-trials rule for drug approval requires \"at least two adequate and\nwell-controlled studies, each convincing on its own, to establish\neffectiveness\". This is usually implemented by requiring two significant\npivotal trials and is the standard regulatory requirement to provide evidence\nfor a new drug's efficacy. However, there is need to develop suitable\nalternatives to this rule for a number of reasons, including the possible\navailability of data from more than two trials. I consider the case of up to 3\nstudies and stress the importance to control the partial Type-I error rate,\nwhere only some studies have a true null effect, while maintaining the overall\nType-I error rate of the two-trials rule, where all studies have a null effect.\nSome less-known $p$-value combination methods are useful to achieve this:\nPearson's method, Edgington's method and the recently proposed harmonic mean\n$\\chi^2$-test. I study their properties and discuss how they can be extended to\na sequential assessment of success while still ensuring overall Type-I error\ncontrol. I compare the different methods in terms of partial Type-I error rate,\nproject power and the expected number of studies required. Edgington's method\nis eventually recommended as it is easy to implement and communicate, has only\nmoderate partial Type-I error rate inflation but substantially increased\nproject power."}, "http://arxiv.org/abs/2307.06250": {"title": "Identifiability Guarantees for Causal Disentanglement from Soft Interventions", "link": "http://arxiv.org/abs/2307.06250", "description": "Causal disentanglement aims to uncover a representation of data using latent\nvariables that are interrelated through a causal model. Such a representation\nis identifiable if the latent model that explains the data is unique. In this\npaper, we focus on the scenario where unpaired observational and interventional\ndata are available, with each intervention changing the mechanism of a latent\nvariable. When the causal variables are fully observed, statistically\nconsistent algorithms have been developed to identify the causal model under\nfaithfulness assumptions. We here show that identifiability can still be\nachieved with unobserved causal variables, given a generalized notion of\nfaithfulness. Our results guarantee that we can recover the latent causal model\nup to an equivalence class and predict the effect of unseen combinations of\ninterventions, in the limit of infinite data. We implement our causal\ndisentanglement framework by developing an autoencoding variational Bayes\nalgorithm and apply it to the problem of predicting combinatorial perturbation\neffects in genomics."}, "http://arxiv.org/abs/2311.05794": {"title": "An Experimental Design for Anytime-Valid Causal Inference on Multi-Armed Bandits", "link": "http://arxiv.org/abs/2311.05794", "description": "Typically, multi-armed bandit (MAB) experiments are analyzed at the end of\nthe study and thus require the analyst to specify a fixed sample size in\nadvance. However, in many online learning applications, it is advantageous to\ncontinuously produce inference on the average treatment effect (ATE) between\narms as new data arrive and determine a data-driven stopping time for the\nexperiment. Existing work on continuous inference for adaptive experiments\nassumes that the treatment assignment probabilities are bounded away from zero\nand one, thus excluding nearly all standard bandit algorithms. In this work, we\ndevelop the Mixture Adaptive Design (MAD), a new experimental design for\nmulti-armed bandits that enables continuous inference on the ATE with\nguarantees on statistical validity and power for nearly any bandit algorithm.\nOn a high level, the MAD \"mixes\" a bandit algorithm of the user's choice with a\nBernoulli design through a tuning parameter $\\delta_t$, where $\\delta_t$ is a\ndeterministic sequence that controls the priority placed on the Bernoulli\ndesign as the sample size grows. We show that for $\\delta_t =\no\\left(1/t^{1/4}\\right)$, the MAD produces a confidence sequence that is\nasymptotically valid and guaranteed to shrink around the true ATE. We\nempirically show that the MAD improves the coverage and power of ATE inference\nin MAB experiments without significant losses in finite-sample reward."}, "http://arxiv.org/abs/2311.05806": {"title": "Likelihood ratio tests in random graph models with increasing dimensions", "link": "http://arxiv.org/abs/2311.05806", "description": "We explore the Wilks phenomena in two random graph models: the $\\beta$-model\nand the Bradley-Terry model. For two increasing dimensional null hypotheses,\nincluding a specified null $H_0: \\beta_i=\\beta_i^0$ for $i=1,\\ldots, r$ and a\nhomogenous null $H_0: \\beta_1=\\cdots=\\beta_r$, we reveal high dimensional\nWilks' phenomena that the normalized log-likelihood ratio statistic,\n$[2\\{\\ell(\\widehat{\\mathbf{\\beta}}) - \\ell(\\widehat{\\mathbf{\\beta}}^0)\\}\n-r]/(2r)^{1/2}$, converges in distribution to the standard normal distribution\nas $r$ goes to infinity. Here, $\\ell( \\mathbf{\\beta})$ is the log-likelihood\nfunction on the model parameter $\\mathbf{\\beta}=(\\beta_1, \\ldots,\n\\beta_n)^\\top$, $\\widehat{\\mathbf{\\beta}}$ is its maximum likelihood estimator\n(MLE) under the full parameter space, and $\\widehat{\\mathbf{\\beta}}^0$ is the\nrestricted MLE under the null parameter space. For the homogenous null with a\nfixed $r$, we establish Wilks-type theorems that\n$2\\{\\ell(\\widehat{\\mathbf{\\beta}}) - \\ell(\\widehat{\\mathbf{\\beta}}^0)\\}$\nconverges in distribution to a chi-square distribution with $r-1$ degrees of\nfreedom, as the total number of parameters, $n$, goes to infinity. When testing\nthe fixed dimensional specified null, we find that its asymptotic null\ndistribution is a chi-square distribution in the $\\beta$-model. However,\nunexpectedly, this is not true in the Bradley-Terry model. By developing\nseveral novel technical methods for asymptotic expansion, we explore Wilks type\nresults in a principled manner; these principled methods should be applicable\nto a class of random graph models beyond the $\\beta$-model and the\nBradley-Terry model. Simulation studies and real network data applications\nfurther demonstrate the theoretical results."}, "http://arxiv.org/abs/2311.05819": {"title": "A flexible framework for synthesizing human activity patterns with application to sequential categorical data", "link": "http://arxiv.org/abs/2311.05819", "description": "The ability to synthesize realistic data in a parametrizable way is valuable\nfor a number of reasons, including privacy, missing data imputation, and\nevaluating the performance of statistical and computational methods. When the\nunderlying data generating process is complex, data synthesis requires\napproaches that balance realism and simplicity. In this paper, we address the\nproblem of synthesizing sequential categorical data of the type that is\nincreasingly available from mobile applications and sensors that record\nparticipant status continuously over the course of multiple days and weeks. We\npropose the paired Markov Chain (paired-MC) method, a flexible framework that\nproduces sequences that closely mimic real data while providing a\nstraightforward mechanism for modifying characteristics of the synthesized\nsequences. We demonstrate the paired-MC method on two datasets, one reflecting\ndaily human activity patterns collected via a smartphone application, and one\nencoding the intensities of physical activity measured by wearable\naccelerometers. In both settings, sequences synthesized by paired-MC better\ncapture key characteristics of the real data than alternative approaches."}, "http://arxiv.org/abs/2311.05847": {"title": "Threshold distribution of equal states for quantitative amplitude fluctuations", "link": "http://arxiv.org/abs/2311.05847", "description": "Objective. The distribution of equal states (DES) quantifies amplitude\nfluctuations in biomedical signals. However, under certain conditions, such as\na high resolution of data collection or special signal processing techniques,\nequal states may be very rare, whereupon the DES fails to measure the amplitude\nfluctuations. Approach. To address this problem, we develop a novel threshold\nDES (tDES) that measures the distribution of differential states within a\nthreshold. To evaluate the proposed tDES, we first analyze five sets of\nsynthetic signals generated in different frequency bands. We then analyze sleep\nelectroencephalography (EEG) datasets taken from the public PhysioNet. Main\nresults. Synthetic signals and detrend-filtered sleep EEGs have no neighboring\nequal values; however, tDES can effectively measure the amplitude fluctuations\nwithin these data. The tDES of EEG data increases significantly as the sleep\nstage increases, even with datasets covering very short periods, indicating\ndecreased amplitude fluctuations in sleep EEGs. Generally speaking, the\npresence of more low-frequency components in a physiological series reflects\nsmaller amplitude fluctuations and larger DES. Significance.The tDES provides a\nreliable computing method for quantifying amplitude fluctuations, exhibiting\nthe characteristics of conceptual simplicity and computational robustness. Our\nfindings broaden the application of quantitative amplitude fluctuations and\ncontribute to the classification of sleep stages based on EEG data."}, "http://arxiv.org/abs/2311.05914": {"title": "Efficient Case-Cohort Design using Balanced Sampling", "link": "http://arxiv.org/abs/2311.05914", "description": "A case-cohort design is a two-phase sampling design frequently used to\nanalyze censored survival data in a cost-effective way, where a subcohort is\nusually selected using simple random sampling or stratified simple random\nsampling. In this paper, we propose an efficient sampling procedure based on\nbalanced sampling when selecting a subcohort in a case-cohort design. A sample\nselected via a balanced sampling procedure automatically calibrates auxiliary\nvariables. When fitting a Cox model, calibrating sampling weights has been\nshown to lead to more efficient estimators of the regression coefficients\n(Breslow et al., 2009a, b). The reduced variabilities over its counterpart with\na simple random sampling are shown via extensive simulation experiments. The\nproposed design and estimation procedure are also illustrated with the\nwell-known National Wilms Tumor Study dataset."}, "http://arxiv.org/abs/2311.06076": {"title": "Bayesian Tensor Factorisations for Time Series of Counts", "link": "http://arxiv.org/abs/2311.06076", "description": "We propose a flexible nonparametric Bayesian modelling framework for\nmultivariate time series of count data based on tensor factorisations. Our\nmodels can be viewed as infinite state space Markov chains of known maximal\norder with non-linear serial dependence through the introduction of appropriate\nlatent variables. Alternatively, our models can be viewed as Bayesian\nhierarchical models with conditionally independent Poisson distributed\nobservations. Inference about the important lags and their complex interactions\nis achieved via MCMC. When the observed counts are large, we deal with the\nresulting computational complexity of Bayesian inference via a two-step\ninferential strategy based on an initial analysis of a training set of the\ndata. Our methodology is illustrated using simulation experiments and analysis\nof real-world data."}, "http://arxiv.org/abs/2311.06086": {"title": "A three-step approach to production frontier estimation and the Matsuoka's distribution", "link": "http://arxiv.org/abs/2311.06086", "description": "In this work, we introduce a three-step semiparametric methodology for the\nestimation of production frontiers. We consider a model inspired by the\nwell-known Cobb-Douglas production function, wherein input factors operate\nmultiplicatively within the model. Efficiency in the proposed model is assumed\nto follow a continuous univariate uniparametric distribution in $(0,1)$,\nreferred to as Matsuoka's distribution, which is introduced and explored.\nFollowing model linearization, the first step of the procedure is to\nsemiparametrically estimate the regression function through a local linear\nsmoother. The second step focuses on the estimation of the efficiency parameter\nin which the properties of the Matsuoka's distribution are employed. Finally,\nwe estimate the production frontier through a plug-in methodology. We present a\nrigorous asymptotic theory related to the proposed three-step estimation,\nincluding consistency, and asymptotic normality, and derive rates for the\nconvergences presented. Incidentally, we also introduce and study the\nMatsuoka's distribution, deriving its main properties, including quantiles,\nmoments, $\\alpha$-expectiles, entropies, and stress-strength reliability, among\nothers. The Matsuoka's distribution exhibits a versatile array of shapes\ncapable of effectively encapsulating the typical behavior of efficiency within\nproduction frontier models. To complement the large sample results obtained, a\nMonte Carlo simulation study is conducted to assess the finite sample\nperformance of the proposed three-step methodology. An empirical application\nusing a dataset of Danish milk producers is also presented."}, "http://arxiv.org/abs/2311.06220": {"title": "Bayesian nonparametric generative modeling of large multivariate non-Gaussian spatial fields", "link": "http://arxiv.org/abs/2311.06220", "description": "Multivariate spatial fields are of interest in many applications, including\nclimate model emulation. Not only can the marginal spatial fields be subject to\nnonstationarity, but the dependence structure among the marginal fields and\nbetween the fields might also differ substantially. Extending a recently\nproposed Bayesian approach to describe the distribution of a nonstationary\nunivariate spatial field using a triangular transport map, we cast the\ninference problem for a multivariate spatial field for a small number of\nreplicates into a series of independent Gaussian process (GP) regression tasks\nwith Gaussian errors. Due to the potential nonlinearity in the conditional\nmeans, the joint distribution modeled can be non-Gaussian. The resulting\nnonparametric Bayesian methodology scales well to high-dimensional spatial\nfields. It is especially useful when only a few training samples are available,\nbecause it employs regularization priors and quantifies uncertainty. Inference\nis conducted in an empirical Bayes setting by a highly scalable stochastic\ngradient approach. The implementation benefits from mini-batching and could be\naccelerated with parallel computing. We illustrate the extended transport-map\nmodel by studying hydrological variables from non-Gaussian climate-model\noutput."}, "http://arxiv.org/abs/2207.12705": {"title": "Efficient shape-constrained inference for the autocovariance sequence from a reversible Markov chain", "link": "http://arxiv.org/abs/2207.12705", "description": "In this paper, we study the problem of estimating the autocovariance sequence\nresulting from a reversible Markov chain. A motivating application for studying\nthis problem is the estimation of the asymptotic variance in central limit\ntheorems for Markov chains. We propose a novel shape-constrained estimator of\nthe autocovariance sequence, which is based on the key observation that the\nrepresentability of the autocovariance sequence as a moment sequence imposes\ncertain shape constraints. We examine the theoretical properties of the\nproposed estimator and provide strong consistency guarantees for our estimator.\nIn particular, for geometrically ergodic reversible Markov chains, we show that\nour estimator is strongly consistent for the true autocovariance sequence with\nrespect to an $\\ell_2$ distance, and that our estimator leads to strongly\nconsistent estimates of the asymptotic variance. Finally, we perform empirical\nstudies to illustrate the theoretical properties of the proposed estimator as\nwell as to demonstrate the effectiveness of our estimator in comparison with\nother current state-of-the-art methods for Markov chain Monte Carlo variance\nestimation, including batch means, spectral variance estimators, and the\ninitial convex sequence estimator."}, "http://arxiv.org/abs/2209.04321": {"title": "Estimating Racial Disparities in Emergency General Surgery", "link": "http://arxiv.org/abs/2209.04321", "description": "Research documents that Black patients experience worse general surgery\noutcomes than white patients in the United States. In this paper, we focus on\nan important but less-examined category: the surgical treatment of emergency\ngeneral surgery (EGS) conditions, which refers to medical emergencies where the\ninjury is \"endogenous,\" such as a burst appendix. Our goal is to assess racial\ndisparities for common outcomes after EGS treatment using an administrative\ndatabase of hospital claims in New York, Florida, and Pennsylvania, and to\nunderstand the extent to which differences are attributable to patient-level\nrisk factors versus hospital-level factors. To do so, we use a class of linear\nweighting estimators that re-weight white patients to have a similar\ndistribution of baseline characteristics as Black patients. This framework\nnests many common approaches, including matching and linear regression, but\noffers important advantages over these methods in terms of controlling\nimbalance between groups, minimizing extrapolation, and reducing computation\ntime. Applying this approach to the claims data, we find that disparities\nestimates that adjust for the admitting hospital are substantially smaller than\nestimates that adjust for patient baseline characteristics only, suggesting\nthat hospital-specific factors are important drivers of racial disparities in\nEGS outcomes."}, "http://arxiv.org/abs/2210.12759": {"title": "Robust angle-based transfer learning in high dimensions", "link": "http://arxiv.org/abs/2210.12759", "description": "Transfer learning aims to improve the performance of a target model by\nleveraging data from related source populations, which is known to be\nespecially helpful in cases with insufficient target data. In this paper, we\nstudy the problem of how to train a high-dimensional ridge regression model\nusing limited target data and existing regression models trained in\nheterogeneous source populations. We consider a practical setting where only\nthe parameter estimates of the fitted source models are accessible, instead of\nthe individual-level source data. Under the setting with only one source model,\nwe propose a novel flexible angle-based transfer learning (angleTL) method,\nwhich leverages the concordance between the source and the target model\nparameters. We show that angleTL unifies several benchmark methods by\nconstruction, including the target-only model trained using target data alone,\nthe source model fitted on source data, and distance-based transfer learning\nmethod that incorporates the source parameter estimates and the target data\nunder a distance-based similarity constraint. We also provide algorithms to\neffectively incorporate multiple source models accounting for the fact that\nsome source models may be more helpful than others. Our high-dimensional\nasymptotic analysis provides interpretations and insights regarding when a\nsource model can be helpful to the target model, and demonstrates the\nsuperiority of angleTL over other benchmark methods. We perform extensive\nsimulation studies to validate our theoretical conclusions and show the\nfeasibility of applying angleTL to transfer existing genetic risk prediction\nmodels across multiple biobanks."}, "http://arxiv.org/abs/2211.04370": {"title": "NESTER: An Adaptive Neurosymbolic Method for Causal Effect Estimation", "link": "http://arxiv.org/abs/2211.04370", "description": "Causal effect estimation from observational data is a central problem in\ncausal inference. Methods based on potential outcomes framework solve this\nproblem by exploiting inductive biases and heuristics from causal inference.\nEach of these methods addresses a specific aspect of causal effect estimation,\nsuch as controlling propensity score, enforcing randomization, etc., by\ndesigning neural network (NN) architectures and regularizers. In this paper, we\npropose an adaptive method called Neurosymbolic Causal Effect Estimator\n(NESTER), a generalized method for causal effect estimation. NESTER integrates\nthe ideas used in existing methods based on multi-head NNs for causal effect\nestimation into one framework. We design a Domain Specific Language (DSL)\ntailored for causal effect estimation based on causal inductive biases used in\nliterature. We conduct a theoretical analysis to investigate NESTER's efficacy\nin estimating causal effects. Our comprehensive empirical results show that\nNESTER performs better than state-of-the-art methods on benchmark datasets."}, "http://arxiv.org/abs/2211.13374": {"title": "A Multivariate Non-Gaussian Bayesian Filter Using Power Moments", "link": "http://arxiv.org/abs/2211.13374", "description": "In this paper, we extend our results on the univariate non-Gaussian Bayesian\nfilter using power moments to the multivariate systems, which can be either\nlinear or nonlinear. Doing this introduces several challenging problems, for\nexample a positive parametrization of the density surrogate, which is not only\na problem of filter design, but also one of the multiple dimensional Hamburger\nmoment problem. We propose a parametrization of the density surrogate with the\nproofs to its existence, Positivstellensatz and uniqueness. Based on it, we\nanalyze the errors of moments of the density estimates by the proposed density\nsurrogate. A discussion on continuous and discrete treatments to the\nnon-Gaussian Bayesian filtering problem is proposed to motivate the research on\ncontinuous parametrization of the system state. Simulation results on\nestimating different types of multivariate density functions are given to\nvalidate our proposed filter. To the best of our knowledge, the proposed filter\nis the first one implementing the multivariate Bayesian filter with the system\nstate parameterized as a continuous function, which only requires the true\nstates being Lebesgue integrable."}, "http://arxiv.org/abs/2303.02637": {"title": "A Semi-Bayesian Nonparametric Estimator of the Maximum Mean Discrepancy Measure: Applications in Goodness-of-Fit Testing and Generative Adversarial Networks", "link": "http://arxiv.org/abs/2303.02637", "description": "A classic inferential statistical problem is the goodness-of-fit (GOF) test.\nSuch a test can be challenging when the hypothesized parametric model has an\nintractable likelihood and its distributional form is not available. Bayesian\nmethods for GOF can be appealing due to their ability to incorporate expert\nknowledge through prior distributions.\n\nHowever, standard Bayesian methods for this test often require strong\ndistributional assumptions on the data and their relevant parameters. To\naddress this issue, we propose a semi-Bayesian nonparametric (semi-BNP)\nprocedure in the context of the maximum mean discrepancy (MMD) measure that can\nbe applied to the GOF test. Our method introduces a novel Bayesian estimator\nfor the MMD, enabling the development of a measure-based hypothesis test for\nintractable models. Through extensive experiments, we demonstrate that our\nproposed test outperforms frequentist MMD-based methods by achieving a lower\nfalse rejection and acceptance rate of the null hypothesis. Furthermore, we\nshowcase the versatility of our approach by embedding the proposed estimator\nwithin a generative adversarial network (GAN) framework. It facilitates a\nrobust BNP learning approach as another significant application of our method.\nWith our BNP procedure, this new GAN approach can enhance sample diversity and\nimprove inferential accuracy compared to traditional techniques."}, "http://arxiv.org/abs/2304.07113": {"title": "Causal inference with a functional outcome", "link": "http://arxiv.org/abs/2304.07113", "description": "This paper presents methods to study the causal effect of a binary treatment\non a functional outcome with observational data. We define a Functional Average\nTreatment Effect and develop an outcome regression estimator. We show how to\nobtain valid inference on the FATE using simultaneous confidence bands, which\ncover the FATE with a given probability over the entire domain. Simulation\nexperiments illustrate how the simultaneous confidence bands take the multiple\ncomparison problem into account. Finally, we use the methods to infer the\neffect of early adult location on subsequent income development for one Swedish\nbirth cohort."}, "http://arxiv.org/abs/2310.00233": {"title": "CausalImages: An R Package for Causal Inference with Earth Observation, Bio-medical, and Social Science Images", "link": "http://arxiv.org/abs/2310.00233", "description": "The causalimages R package enables causal inference with image and image\nsequence data, providing new tools for integrating novel data sources like\nsatellite and bio-medical imagery into the study of cause and effect. One set\nof functions enables image-based causal inference analyses. For example, one\nkey function decomposes treatment effect heterogeneity by images using an\ninterpretable Bayesian framework. This allows for determining which types of\nimages or image sequences are most responsive to interventions. A second\nmodeling function allows researchers to control for confounding using images.\nThe package also allows investigators to produce embeddings that serve as\nvector summaries of the image or video content. Finally, infrastructural\nfunctions are also provided, such as tools for writing large-scale image and\nimage sequence data as sequentialized byte strings for more rapid image\nanalysis. causalimages therefore opens new capabilities for causal inference in\nR, letting researchers use informative imagery in substantive analyses in a\nfast and accessible manner."}, "http://arxiv.org/abs/2311.06409": {"title": "Flexible joint models for multivariate longitudinal and time-to-event data using multivariate functional principal components", "link": "http://arxiv.org/abs/2311.06409", "description": "The joint modeling of multiple longitudinal biomarkers together with a\ntime-to-event outcome is a challenging modeling task of continued scientific\ninterest. In particular, the computational complexity of high dimensional\n(generalized) mixed effects models often restricts the flexibility of shared\nparameter joint models, even when the subject-specific marker trajectories\nfollow highly nonlinear courses. We propose a parsimonious multivariate\nfunctional principal components representation of the shared random effects.\nThis allows better scalability, as the dimension of the random effects does not\ndirectly increase with the number of markers, only with the chosen number of\nprincipal component basis functions used in the approximation of the random\neffects. The functional principal component representation additionally allows\nto estimate highly flexible subject-specific random trajectories without\nparametric assumptions. The modeled trajectories can thus be distinctly\ndifferent for each biomarker. We build on the framework of flexible Bayesian\nadditive joint models implemented in the R-package 'bamlss', which also\nsupports estimation of nonlinear covariate effects via Bayesian P-splines. The\nflexible yet parsimonious functional principal components basis used in the\nestimation of the joint model is first estimated in a preliminary step. We\nvalidate our approach in a simulation study and illustrate its advantages by\nanalyzing a study on primary biliary cholangitis."}, "http://arxiv.org/abs/2311.06412": {"title": "Online multiple testing with e-values", "link": "http://arxiv.org/abs/2311.06412", "description": "A scientist tests a continuous stream of hypotheses over time in the course\nof her investigation -- she does not test a predetermined, fixed number of\nhypotheses. The scientist wishes to make as many discoveries as possible while\nensuring the number of false discoveries is controlled -- a well recognized way\nfor accomplishing this is to control the false discovery rate (FDR). Prior\nmethods for FDR control in the online setting have focused on formulating\nalgorithms when specific dependency structures are assumed to exist between the\ntest statistics of each hypothesis. However, in practice, these dependencies\noften cannot be known beforehand or tested after the fact. Our algorithm,\ne-LOND, provides FDR control under arbitrary, possibly unknown, dependence. We\nshow that our method is more powerful than existing approaches to this problem\nthrough simulations. We also formulate extensions of this algorithm to utilize\nrandomization for increased power, and for constructing confidence intervals in\nonline selective inference."}, "http://arxiv.org/abs/2311.06415": {"title": "Long-Term Dagum-PVF Frailty Regression Model: Application in Health Studies", "link": "http://arxiv.org/abs/2311.06415", "description": "Survival models incorporating cure fractions, commonly known as cure fraction\nmodels or long-term survival models, are widely employed in epidemiological\nstudies to account for both immune and susceptible patients in relation to the\nfailure event of interest under investigation. In such studies, there is also a\nneed to estimate the unobservable heterogeneity caused by prognostic factors\nthat cannot be observed. Moreover, the hazard function may exhibit a\nnon-monotonic form, specifically, an unimodal hazard function. In this article,\nwe propose a long-term survival model based on the defective version of the\nDagum distribution, with a power variance function (PVF) frailty term\nintroduced in the hazard function to control for unobservable heterogeneity in\npatient populations, which is useful for accommodating survival data in the\npresence of a cure fraction and with a non-monotone hazard function. The\ndistribution is conveniently reparameterized in terms of the cure fraction, and\nthen associated with the covariates via a logit link function, enabling direct\ninterpretation of the covariate effects on the cure fraction, which is not\nusual in the defective approach. It is also proven a result that generates\ndefective models induced by PVF frailty distribution. We discuss maximum\nlikelihood estimation for model parameters and evaluate its performance through\nMonte Carlo simulation studies. Finally, the practicality and benefits of our\nmodel are demonstrated through two health-related datasets, focusing on severe\ncases of COVID-19 in pregnant and postpartum women and on patients with\nmalignant skin neoplasms."}, "http://arxiv.org/abs/2311.06458": {"title": "Conditional Adjustment in a Markov Equivalence Class", "link": "http://arxiv.org/abs/2311.06458", "description": "We consider the problem of identifying a conditional causal effect through\ncovariate adjustment. We focus on the setting where the causal graph is known\nup to one of two types of graphs: a maximally oriented partially directed\nacyclic graph (MPDAG) or a partial ancestral graph (PAG). Both MPDAGs and PAGs\nrepresent equivalence classes of possible underlying causal models. After\ndefining adjustment sets in this setting, we provide a necessary and sufficient\ngraphical criterion -- the conditional adjustment criterion -- for finding\nthese sets under conditioning on variables unaffected by treatment. We further\nprovide explicit sets from the graph that satisfy the conditional adjustment\ncriterion, and therefore, can be used as adjustment sets for conditional causal\neffect identification."}, "http://arxiv.org/abs/2311.06537": {"title": "Is Machine Learning Unsafe and Irresponsible in Social Sciences? Paradoxes and Reconsidering from Recidivism Prediction Tasks", "link": "http://arxiv.org/abs/2311.06537", "description": "The paper addresses some fundamental and hotly debated issues for high-stakes\nevent predictions underpinning the computational approach to social sciences.\nWe question several prevalent views against machine learning and outline a new\nparadigm that highlights the promises and promotes the infusion of\ncomputational methods and conventional social science approaches."}, "http://arxiv.org/abs/2311.06590": {"title": "Optimal resource allocation: Convex quantile regression approach", "link": "http://arxiv.org/abs/2311.06590", "description": "Optimal allocation of resources across sub-units in the context of\ncentralized decision-making systems such as bank branches or supermarket chains\nis a classical application of operations research and management science. In\nthis paper, we develop quantile allocation models to examine how much the\noutput and productivity could potentially increase if the resources were\nefficiently allocated between units. We increase robustness to random noise and\nheteroscedasticity by utilizing the local estimation of multiple production\nfunctions using convex quantile regression. The quantile allocation models then\nrely on the estimated shadow prices instead of detailed data of units and allow\nthe entry and exit of units. Our empirical results on Finland's business sector\nreveal a large potential for productivity gains through better allocation,\nkeeping the current technology and resources fixed."}, "http://arxiv.org/abs/2311.06681": {"title": "SpICE: An interpretable method for spatial data", "link": "http://arxiv.org/abs/2311.06681", "description": "Statistical learning methods are widely utilized in tackling complex problems\ndue to their flexibility, good predictive performance and its ability to\ncapture complex relationships among variables. Additionally, recently developed\nautomatic workflows have provided a standardized approach to implementing\nstatistical learning methods across various applications. However these tools\nhighlight a main drawbacks of statistical learning: its lack of interpretation\nin their results. In the past few years an important amount of research has\nbeen focused on methods for interpreting black box models. Having interpretable\nstatistical learning methods is relevant to have a deeper understanding of the\nmodel. In problems were spatial information is relevant, combined interpretable\nmethods with spatial data can help to get better understanding of the problem\nand interpretation of the results.\n\nThis paper is focused in the individual conditional expectation (ICE-plot), a\nmodel agnostic methods for interpreting statistical learning models and\ncombined them with spatial information. ICE-plot extension is proposed where\nspatial information is used as restriction to define Spatial ICE curves\n(SpICE). Spatial ICE curves are estimated using real data in the context of an\neconomic problem concerning property valuation in Montevideo, Uruguay.\nUnderstanding the key factors that influence property valuation is essential\nfor decision-making, and spatial data plays a relevant role in this regard."}, "http://arxiv.org/abs/2311.06719": {"title": "Efficient Multiple-Robust Estimation for Nonresponse Data Under Informative Sampling", "link": "http://arxiv.org/abs/2311.06719", "description": "Nonresponse after probability sampling is a universal challenge in survey\nsampling, often necessitating adjustments to mitigate sampling and selection\nbias simultaneously. This study explored the removal of bias and effective\nutilization of available information, not just in nonresponse but also in the\nscenario of data integration, where summary statistics from other data sources\nare accessible. We reformulate these settings within a two-step monotone\nmissing data framework, where the first step of missingness arises from\nsampling and the second originates from nonresponse. Subsequently, we derive\nthe semiparametric efficiency bound for the target parameter. We also propose\nadaptive estimators utilizing methods of moments and empirical likelihood\napproaches to attain the lower bound. The proposed estimator exhibits both\nefficiency and double robustness. However, attaining efficiency with an\nadaptive estimator requires the correct specification of certain working\nmodels. To reinforce robustness against the misspecification of working models,\nwe extend the property of double robustness to multiple robustness by proposing\na two-step empirical likelihood method that effectively leverages empirical\nweights. A numerical study is undertaken to investigate the finite-sample\nperformance of the proposed methods. We further applied our methods to a\ndataset from the National Health and Nutrition Examination Survey data by\nefficiently incorporating summary statistics from the National Health Interview\nSurvey data."}, "http://arxiv.org/abs/2311.06831": {"title": "Quasi-Bayes in Latent Variable Models", "link": "http://arxiv.org/abs/2311.06831", "description": "Latent variable models are widely used to account for unobserved determinants\nof economic behavior. Traditional nonparametric methods to estimate latent\nheterogeneity do not scale well into multidimensional settings. Distributional\nrestrictions alleviate tractability concerns but may impart non-trivial\nmisspecification bias. Motivated by these concerns, this paper introduces a\nquasi-Bayes approach to estimate a large class of multidimensional latent\nvariable models. Our approach to quasi-Bayes is novel in that we center it\naround relating the characteristic function of observables to the distribution\nof unobservables. We propose a computationally attractive class of priors that\nare supported on Gaussian mixtures and derive contraction rates for a variety\nof latent variable models."}, "http://arxiv.org/abs/2311.06840": {"title": "Distribution Re-weighting and Voting Paradoxes", "link": "http://arxiv.org/abs/2311.06840", "description": "We explore a specific type of distribution shift called domain expertise, in\nwhich training is limited to a subset of all possible labels. This setting is\ncommon among specialized human experts, or specific focused studies. We show\nhow the standard approach to distribution shift, which involves re-weighting\ndata, can result in paradoxical disagreements among differing domain expertise.\nWe also demonstrate how standard adjustments for causal inference lead to the\nsame paradox. We prove that the characteristics of these paradoxes exactly\nmimic another set of paradoxes which arise among sets of voter preferences."}, "http://arxiv.org/abs/2311.06928": {"title": "Attention for Causal Relationship Discovery from Biological Neural Dynamics", "link": "http://arxiv.org/abs/2311.06928", "description": "This paper explores the potential of the transformer models for learning\nGranger causality in networks with complex nonlinear dynamics at every node, as\nin neurobiological and biophysical networks. Our study primarily focuses on a\nproof-of-concept investigation based on simulated neural dynamics, for which\nthe ground-truth causality is known through the underlying connectivity matrix.\nFor transformer models trained to forecast neuronal population dynamics, we\nshow that the cross attention module effectively captures the causal\nrelationship among neurons, with an accuracy equal or superior to that for the\nmost popular Granger causality analysis method. While we acknowledge that\nreal-world neurobiology data will bring further challenges, including dynamic\nconnectivity and unobserved variability, this research offers an encouraging\npreliminary glimpse into the utility of the transformer model for causal\nrepresentation learning in neuroscience."}, "http://arxiv.org/abs/2311.06945": {"title": "An Efficient Approach for Identifying Important Biomarkers for Biomedical Diagnosis", "link": "http://arxiv.org/abs/2311.06945", "description": "In this paper, we explore the challenges associated with biomarker\nidentification for diagnosis purpose in biomedical experiments, and propose a\nnovel approach to handle the above challenging scenario via the generalization\nof the Dantzig selector. To improve the efficiency of the regularization\nmethod, we introduce a transformation from an inherent nonlinear programming\ndue to its nonlinear link function into a linear programming framework. We\nillustrate the use of of our method on an experiment with binary response,\nshowing superior performance on biomarker identification studies when compared\nto their conventional analysis. Our proposed method does not merely serve as a\nvariable/biomarker selection tool, its ranking of variable importance provides\nvaluable reference information for practitioners to reach informed decisions\nregarding the prioritization of factors for further investigations."}, "http://arxiv.org/abs/2311.07034": {"title": "Regularized Halfspace Depth for Functional Data", "link": "http://arxiv.org/abs/2311.07034", "description": "Data depth is a powerful nonparametric tool originally proposed to rank\nmultivariate data from center outward. In this context, one of the most\narchetypical depth notions is Tukey's halfspace depth. In the last few decades\nnotions of depth have also been proposed for functional data. However, Tukey's\ndepth cannot be extended to handle functional data because of its degeneracy.\nHere, we propose a new halfspace depth for functional data which avoids\ndegeneracy by regularization. The halfspace projection directions are\nconstrained to have a small reproducing kernel Hilbert space norm. Desirable\ntheoretical properties of the proposed depth, such as isometry invariance,\nmaximality at center, monotonicity relative to a deepest point, upper\nsemi-continuity, and consistency are established. Moreover, the regularized\nhalfspace depth can rank functional data with varying emphasis in shape or\nmagnitude, depending on the regularization. A new outlier detection approach is\nalso proposed, which is capable of detecting both shape and magnitude outliers.\nIt is applicable to trajectories in L2, a very general space of functions that\ninclude non-smooth trajectories. Based on extensive numerical studies, our\nmethods are shown to perform well in terms of detecting outliers of different\ntypes. Three real data examples showcase the proposed depth notion."}, "http://arxiv.org/abs/2311.07156": {"title": "Deep mixture of linear mixed models for complex longitudinal data", "link": "http://arxiv.org/abs/2311.07156", "description": "Mixtures of linear mixed models are widely used for modelling longitudinal\ndata for which observation times differ between subjects. In typical\napplications, temporal trends are described using a basis expansion, with basis\ncoefficients treated as random effects varying by subject. Additional random\neffects can describe variation between mixture components, or other known\nsources of variation in complex experimental designs. A key advantage of these\nmodels is that they provide a natural mechanism for clustering, which can be\nhelpful for interpretation in many applications. Current versions of mixtures\nof linear mixed models are not specifically designed for the case where there\nare many observations per subject and a complex temporal trend, which requires\na large number of basis functions to capture. In this case, the\nsubject-specific basis coefficients are a high-dimensional random effects\nvector, for which the covariance matrix is hard to specify and estimate,\nespecially if it varies between mixture components. To address this issue, we\nconsider the use of recently-developed deep mixture of factor analyzers models\nas the prior for the random effects. The resulting deep mixture of linear mixed\nmodels is well-suited to high-dimensional settings, and we describe an\nefficient variational inference approach to posterior computation. The efficacy\nof the method is demonstrated on both real and simulated data."}, "http://arxiv.org/abs/2311.07243": {"title": "Optimal Estimation of Large-Dimensional Nonlinear Factor Models", "link": "http://arxiv.org/abs/2311.07243", "description": "This paper studies optimal estimation of large-dimensional nonlinear factor\nmodels. The key challenge is that the observed variables are possibly nonlinear\nfunctions of some latent variables where the functional forms are left\nunspecified. A local principal component analysis method is proposed to\nestimate the factor structure and recover information on latent variables and\nlatent functions, which combines $K$-nearest neighbors matching and principal\ncomponent analysis. Large-sample properties are established, including a sharp\nbound on the matching discrepancy of nearest neighbors, sup-norm error bounds\nfor estimated local factors and factor loadings, and the uniform convergence\nrate of the factor structure estimator. Under mild conditions our estimator of\nthe latent factor structure can achieve the optimal rate of uniform convergence\nfor nonparametric regression. The method is illustrated with a Monte Carlo\nexperiment and an empirical application studying the effect of tax cuts on\neconomic growth."}, "http://arxiv.org/abs/2311.07371": {"title": "Scalable Estimation for Structured Additive Distributional Regression Through Variational Inference", "link": "http://arxiv.org/abs/2311.07371", "description": "Structured additive distributional regression models offer a versatile\nframework for estimating complete conditional distributions by relating all\nparameters of a parametric distribution to covariates. Although these models\nefficiently leverage information in vast and intricate data sets, they often\nresult in highly-parameterized models with many unknowns. Standard estimation\nmethods, like Bayesian approaches based on Markov chain Monte Carlo methods,\nface challenges in estimating these models due to their complexity and\ncostliness. To overcome these issues, we suggest a fast and scalable\nalternative based on variational inference. Our approach combines a\nparsimonious parametric approximation for the posteriors of regression\ncoefficients, with the exact conditional posterior for hyperparameters. For\noptimization, we use a stochastic gradient ascent method combined with an\nefficient strategy to reduce the variance of estimators. We provide theoretical\nproperties and investigate global and local annealing to enhance robustness,\nparticularly against data outliers. Our implementation is very general,\nallowing us to include various functional effects like penalized splines or\ncomplex tensor product interactions. In a simulation study, we demonstrate the\nefficacy of our approach in terms of accuracy and computation time. Lastly, we\npresent two real examples illustrating the modeling of infectious COVID-19\noutbreaks and outlier detection in brain activity."}, "http://arxiv.org/abs/2311.07474": {"title": "A Federated Data Fusion-Based Prognostic Model for Applications with Multi-Stream Incomplete Signals", "link": "http://arxiv.org/abs/2311.07474", "description": "Most prognostic methods require a decent amount of data for model training.\nIn reality, however, the amount of historical data owned by a single\norganization might be small or not large enough to train a reliable prognostic\nmodel. To address this challenge, this article proposes a federated prognostic\nmodel that allows multiple users to jointly construct a failure time prediction\nmodel using their multi-stream, high-dimensional, and incomplete data while\nkeeping each user's data local and confidential. The prognostic model first\nemploys multivariate functional principal component analysis to fuse the\nmulti-stream degradation signals. Then, the fused features coupled with the\ntimes-to-failure are utilized to build a (log)-location-scale regression model\nfor failure prediction. To estimate parameters using distributed datasets and\nkeep the data privacy of all participants, we propose a new federated algorithm\nfor feature extraction. Numerical studies indicate that the performance of the\nproposed model is the same as that of classic non-federated prognostic models\nand is better than that of the models constructed by each user itself."}, "http://arxiv.org/abs/2311.07511": {"title": "Machine learning for uncertainty estimation in fusing precipitation observations from satellites and ground-based gauges", "link": "http://arxiv.org/abs/2311.07511", "description": "To form precipitation datasets that are accurate and, at the same time, have\nhigh spatial densities, data from satellites and gauges are often merged in the\nliterature. However, uncertainty estimates for the data acquired in this manner\nare scarcely provided, although the importance of uncertainty quantification in\npredictive modelling is widely recognized. Furthermore, the benefits that\nmachine learning can bring to the task of providing such estimates have not\nbeen broadly realized and properly explored through benchmark experiments. The\npresent study aims at filling in this specific gap by conducting the first\nbenchmark tests on the topic. On a large dataset that comprises 15-year-long\nmonthly data spanning across the contiguous United States, we extensively\ncompared six learners that are, by their construction, appropriate for\npredictive uncertainty quantification. These are the quantile regression (QR),\nquantile regression forests (QRF), generalized random forests (GRF), gradient\nboosting machines (GBM), light gradient boosting machines (LightGBM) and\nquantile regression neural networks (QRNN). The comparison referred to the\ncompetence of the learners in issuing predictive quantiles at nine levels that\nfacilitate a good approximation of the entire predictive probability\ndistribution, and was primarily based on the quantile and continuous ranked\nprobability skill scores. Three types of predictor variables (i.e., satellite\nprecipitation variables, distances between a point of interest and satellite\ngrid points, and elevation at a point of interest) were used in the comparison\nand were additionally compared with each other. This additional comparison was\nbased on the explainable machine learning concept of feature importance. The\nresults suggest that the order from the best to the worst of the learners for\nthe task investigated is the following: LightGBM, QRF, GRF, GBM, QRNN and QR..."}, "http://arxiv.org/abs/2311.07524": {"title": "The Link Between Health Insurance Coverage and Citizenship Among Immigrants: Bayesian Unit-Level Regression Modeling of Categorical Survey Data Observed with Measurement Error", "link": "http://arxiv.org/abs/2311.07524", "description": "Social scientists are interested in studying the impact that citizenship\nstatus has on health insurance coverage among immigrants in the United States.\nThis can be done using data from the Survey of Income and Program Participation\n(SIPP); however, two primary challenges emerge. First, statistical models must\naccount for the survey design in some fashion to reduce the risk of bias due to\ninformative sampling. Second, it has been observed that survey respondents\nmisreport citizenship status at nontrivial rates. This too can induce bias\nwithin a statistical model. Thus, we propose the use of a weighted\npseudo-likelihood mixture of categorical distributions, where the mixture\ncomponent is determined by the latent true response variable, in order to model\nthe misreported data. We illustrate through an empirical simulation study that\nthis approach can mitigate the two sources of bias attributable to the sample\ndesign and misreporting. Importantly, our misreporting model can be further\nused as a component in a deeper hierarchical model. With this in mind, we\nconduct an analysis of the relationship between health insurance coverage and\ncitizenship status using data from the SIPP."}, "http://arxiv.org/abs/1902.09608": {"title": "On Binscatter", "link": "http://arxiv.org/abs/1902.09608", "description": "Binscatter is a popular method for visualizing bivariate relationships and\nconducting informal specification testing. We study the properties of this\nmethod formally and develop enhanced visualization and econometric binscatter\ntools. These include estimating conditional means with optimal binning and\nquantifying uncertainty. We also highlight a methodological problem related to\ncovariate adjustment that can yield incorrect conclusions. We revisit two\napplications using our methodology and find substantially different results\nrelative to those obtained using prior informal binscatter methods. General\npurpose software in Python, R, and Stata is provided. Our technical work is of\nindependent interest for the nonparametric partition-based estimation\nliterature."}, "http://arxiv.org/abs/2110.10195": {"title": "Operator-induced structural variable selection for identifying materials genes", "link": "http://arxiv.org/abs/2110.10195", "description": "In the emerging field of materials informatics, a fundamental task is to\nidentify physicochemically meaningful descriptors, or materials genes, which\nare engineered from primary features and a set of elementary algebraic\noperators through compositions. Standard practice directly analyzes the\nhigh-dimensional candidate predictor space in a linear model; statistical\nanalyses are then substantially hampered by the daunting challenge posed by the\nastronomically large number of correlated predictors with limited sample size.\nWe formulate this problem as variable selection with operator-induced structure\n(OIS) and propose a new method to achieve unconventional dimension reduction by\nutilizing the geometry embedded in OIS. Although the model remains linear, we\niterate nonparametric variable selection for effective dimension reduction.\nThis enables variable selection based on ab initio primary features, leading to\na method that is orders of magnitude faster than existing methods, with\nimproved accuracy. To select the nonparametric module, we discuss a desired\nperformance criterion that is uniquely induced by variable selection with OIS;\nin particular, we propose to employ a Bayesian Additive Regression Trees\n(BART)-based variable selection method. Numerical studies show superiority of\nthe proposed method, which continues to exhibit robust performance when the\ninput dimension is out of reach of existing methods. Our analysis of\nsingle-atom catalysis identifies physical descriptors that explain the binding\nenergy of metal-support pairs with high explanatory power, leading to\ninterpretable insights to guide the prevention of a notorious problem called\nsintering and aid catalysis design."}, "http://arxiv.org/abs/2204.12699": {"title": "Randomness of Shapes and Statistical Inference on Shapes via the Smooth Euler Characteristic Transform", "link": "http://arxiv.org/abs/2204.12699", "description": "In this article, we establish the mathematical foundations for modeling the\nrandomness of shapes and conducting statistical inference on shapes using the\nsmooth Euler characteristic transform. Based on these foundations, we propose\ntwo parametric algorithms for testing hypotheses on random shapes. Simulation\nstudies are presented to validate our mathematical derivations and to compare\nour algorithms with state-of-the-art methods to demonstrate the utility of our\nproposed framework. As real applications, we analyze a data set of mandibular\nmolars from four genera of primates and show that our algorithms have the power\nto detect significant shape differences that recapitulate known morphological\nvariation across suborders. Altogether, our discussions bridge the following\nfields: algebraic and computational topology, probability theory and stochastic\nprocesses, Sobolev spaces and functional analysis, statistical inference, and\ngeometric morphometrics."}, "http://arxiv.org/abs/2207.05019": {"title": "Covariate-adaptive randomization inference in matched designs", "link": "http://arxiv.org/abs/2207.05019", "description": "It is common to conduct causal inference in matched observational studies by\nproceeding as though treatment assignments within matched sets are assigned\nuniformly at random and using this distribution as the basis for inference.\nThis approach ignores observed discrepancies in matched sets that may be\nconsequential for the distribution of treatment, which are succinctly captured\nby within-set differences in the propensity score. We address this problem via\ncovariate-adaptive randomization inference, which modifies the permutation\nprobabilities to vary with estimated propensity score discrepancies and avoids\nrequirements to exclude matched pairs or model an outcome variable. We show\nthat the test achieves type I error control arbitrarily close to the nominal\nlevel when large samples are available for propensity score estimation. We\ncharacterize the large-sample behavior of the new randomization test for a\ndifference-in-means estimator of a constant additive effect. We also show that\nexisting methods of sensitivity analysis generalize effectively to\ncovariate-adaptive randomization inference. Finally, we evaluate the empirical\nvalue of covariate-adaptive randomization procedures via comparisons to\ntraditional uniform inference in matched designs with and without propensity\nscore calipers and regression adjustment using simulations and analyses of\ngenetic damage among welders and right-heart catheterization in surgical\npatients."}, "http://arxiv.org/abs/2209.07091": {"title": "A new Kernel Regression approach for Robustified $L_2$ Boosting", "link": "http://arxiv.org/abs/2209.07091", "description": "We investigate $L_2$ boosting in the context of kernel regression. Kernel\nsmoothers, in general, lack appealing traits like symmetry and positive\ndefiniteness, which are critical not only for understanding theoretical aspects\nbut also for achieving good practical performance. We consider a\nprojection-based smoother (Huang and Chen, 2008) that is symmetric, positive\ndefinite, and shrinking. Theoretical results based on the orthonormal\ndecomposition of the smoother reveal additional insights into the boosting\nalgorithm. In our asymptotic framework, we may replace the full-rank smoother\nwith a low-rank approximation. We demonstrate that the smoother's low-rank\n($d(n)$) is bounded above by $O(h^{-1})$, where $h$ is the bandwidth. Our\nnumerical findings show that, in terms of prediction accuracy, low-rank\nsmoothers may outperform full-rank smoothers. Furthermore, we show that the\nboosting estimator with low-rank smoother achieves the optimal convergence\nrate. Finally, to improve the performance of the boosting algorithm in the\npresence of outliers, we propose a novel robustified boosting algorithm which\ncan be used with any smoother discussed in the study. We investigate the\nnumerical performance of the proposed approaches using simulations and a\nreal-world case."}, "http://arxiv.org/abs/2210.01757": {"title": "Transportability of model-based estimands in evidence synthesis", "link": "http://arxiv.org/abs/2210.01757", "description": "In evidence synthesis, effect modifiers are typically described as variables\nthat induce treatment effect heterogeneity at the individual level, through\ntreatment-covariate interactions in an outcome model parametrized at such\nlevel. As such, effect modification is defined with respect to a conditional\nmeasure, but marginal effect estimates are required for population-level\ndecisions in health technology assessment. For non-collapsible measures, purely\nprognostic variables that are not determinants of treatment response at the\nindividual level may modify marginal effects, even where there is\nindividual-level treatment effect homogeneity. With heterogeneity, marginal\neffects for measures that are not directly collapsible cannot be expressed in\nterms of marginal covariate moments, and generally depend on the joint\ndistribution of conditional effect measure modifiers and purely prognostic\nvariables. There are implications for recommended practices in evidence\nsynthesis. Unadjusted anchored indirect comparisons can be biased in the\nabsence of individual-level treatment effect heterogeneity, or when marginal\ncovariate moments are balanced across studies. Covariate adjustment may be\nnecessary to account for cross-study imbalances in joint covariate\ndistributions involving purely prognostic variables. In the absence of\nindividual patient data for the target, covariate adjustment approaches are\ninherently limited in their ability to remove bias for measures that are not\ndirectly collapsible. Directly collapsible measures would facilitate the\ntransportability of marginal effects between studies by: (1) reducing\ndependence on model-based covariate adjustment where there is individual-level\ntreatment effect homogeneity and marginal covariate moments are balanced; and\n(2) facilitating the selection of baseline covariates for adjustment where\nthere is individual-level treatment effect heterogeneity."}, "http://arxiv.org/abs/2212.01900": {"title": "Bayesian survival analysis with INLA", "link": "http://arxiv.org/abs/2212.01900", "description": "This tutorial shows how various Bayesian survival models can be fitted using\nthe integrated nested Laplace approximation in a clear, legible, and\ncomprehensible manner using the INLA and INLAjoint R-packages. Such models\ninclude accelerated failure time, proportional hazards, mixture cure, competing\nrisks, multi-state, frailty, and joint models of longitudinal and survival\ndata, originally presented in the article \"Bayesian survival analysis with\nBUGS\" (Alvares et al., 2021). In addition, we illustrate the implementation of\na new joint model for a longitudinal semicontinuous marker, recurrent events,\nand a terminal event. Our proposal aims to provide the reader with syntax\nexamples for implementing survival models using a fast and accurate approximate\nBayesian inferential approach."}, "http://arxiv.org/abs/2301.03661": {"title": "Generative Quantile Regression with Variability Penalty", "link": "http://arxiv.org/abs/2301.03661", "description": "Quantile regression and conditional density estimation can reveal structure\nthat is missed by mean regression, such as multimodality and skewness. In this\npaper, we introduce a deep learning generative model for joint quantile\nestimation called Penalized Generative Quantile Regression (PGQR). Our approach\nsimultaneously generates samples from many random quantile levels, allowing us\nto infer the conditional distribution of a response variable given a set of\ncovariates. Our method employs a novel variability penalty to avoid the problem\nof vanishing variability, or memorization, in deep generative models. Further,\nwe introduce a new family of partial monotonic neural networks (PMNN) to\ncircumvent the problem of crossing quantile curves. A major benefit of PGQR is\nthat it can be fit using a single optimization, thus bypassing the need to\nrepeatedly train the model at multiple quantile levels or use computationally\nexpensive cross-validation to tune the penalty parameter. We illustrate the\nefficacy of PGQR through extensive simulation studies and analysis of real\ndatasets. Code to implement our method is available at\nhttps://github.com/shijiew97/PGQR."}, "http://arxiv.org/abs/2302.09526": {"title": "Mixed Semi-Supervised Generalized-Linear-Regression with applications to Deep-Learning and Interpolators", "link": "http://arxiv.org/abs/2302.09526", "description": "We present a methodology for using unlabeled data to design semi supervised\nlearning (SSL) methods that improve the prediction performance of supervised\nlearning for regression tasks. The main idea is to design different mechanisms\nfor integrating the unlabeled data, and include in each of them a mixing\nparameter $\\alpha$, controlling the weight given to the unlabeled data.\nFocusing on Generalized Linear Models (GLM) and linear interpolators classes of\nmodels, we analyze the characteristics of different mixing mechanisms, and\nprove that in all cases, it is invariably beneficial to integrate the unlabeled\ndata with some nonzero mixing ratio $\\alpha&gt;0$, in terms of predictive\nperformance. Moreover, we provide a rigorous framework to estimate the best\nmixing ratio $\\alpha^*$ where mixed SSL delivers the best predictive\nperformance, while using the labeled and unlabeled data on hand.\n\nThe effectiveness of our methodology in delivering substantial improvement\ncompared to the standard supervised models, in a variety of settings, is\ndemonstrated empirically through extensive simulation, in a manner that\nsupports the theoretical analysis. We also demonstrate the applicability of our\nmethodology (with some intuitive modifications) to improve more complex models,\nsuch as deep neural networks, in real-world regression tasks."}, "http://arxiv.org/abs/2305.13421": {"title": "Sequential Estimation using Hierarchically Stratified Domains with Latin Hypercube Sampling", "link": "http://arxiv.org/abs/2305.13421", "description": "Quantifying the effect of uncertainties in systems where only point\nevaluations in the stochastic domain but no regularity conditions are available\nis limited to sampling-based techniques. This work presents an adaptive\nsequential stratification estimation method that uses Latin Hypercube Sampling\nwithin each stratum. The adaptation is achieved through a sequential\nhierarchical refinement of the stratification, guided by previous estimators\nusing local (i.e., stratum-dependent) variability indicators based on\ngeneralized polynomial chaos expansions and Sobol decompositions. For a given\ntotal number of samples $N$, the corresponding hierarchically constructed\nsequence of Stratified Sampling estimators combined with Latin Hypercube\nsampling is adequately averaged to provide a final estimator with reduced\nvariance. Numerical experiments illustrate the procedure's efficiency,\nindicating that it can offer a variance decay proportional to $N^{-2}$ in some\ncases."}, "http://arxiv.org/abs/2306.08794": {"title": "Quantile autoregressive conditional heteroscedasticity", "link": "http://arxiv.org/abs/2306.08794", "description": "This paper proposes a novel conditional heteroscedastic time series model by\napplying the framework of quantile regression processes to the ARCH(\\infty)\nform of the GARCH model. This model can provide varying structures for\nconditional quantiles of the time series across different quantile levels,\nwhile including the commonly used GARCH model as a special case. The strict\nstationarity of the model is discussed. For robustness against heavy-tailed\ndistributions, a self-weighted quantile regression (QR) estimator is proposed.\nWhile QR performs satisfactorily at intermediate quantile levels, its accuracy\ndeteriorates at high quantile levels due to data scarcity. As a remedy, a\nself-weighted composite quantile regression (CQR) estimator is further\nintroduced and, based on an approximate GARCH model with a flexible\nTukey-lambda distribution for the innovations, we can extrapolate the high\nquantile levels by borrowing information from intermediate ones. Asymptotic\nproperties for the proposed estimators are established. Simulation experiments\nare carried out to access the finite sample performance of the proposed\nmethods, and an empirical example is presented to illustrate the usefulness of\nthe new model."}, "http://arxiv.org/abs/2309.00948": {"title": "Marginalised Normal Regression: Unbiased curve fitting in the presence of x-errors", "link": "http://arxiv.org/abs/2309.00948", "description": "The history of the seemingly simple problem of straight line fitting in the\npresence of both $x$ and $y$ errors has been fraught with misadventure, with\nstatistically ad hoc and poorly tested methods abounding in the literature. The\nproblem stems from the emergence of latent variables describing the \"true\"\nvalues of the independent variables, the priors on which have a significant\nimpact on the regression result. By analytic calculation of maximum a\nposteriori values and biases, and comprehensive numerical mock tests, we assess\nthe quality of possible priors. In the presence of intrinsic scatter, the only\nprior that we find to give reliably unbiased results in general is a mixture of\none or more Gaussians with means and variances determined as part of the\ninference. We find that a single Gaussian is typically sufficient and dub this\nmodel Marginalised Normal Regression (MNR). We illustrate the necessity for MNR\nby comparing it to alternative methods on an important linear relation in\ncosmology, and extend it to nonlinear regression and an arbitrary covariance\nmatrix linking $x$ and $y$. We publicly release a Python/Jax implementation of\nMNR and its Gaussian mixture model extension that is coupled to Hamiltonian\nMonte Carlo for efficient sampling, which we call ROXY (Regression and\nOptimisation with X and Y errors)."}, "http://arxiv.org/abs/2311.07733": {"title": "Credible Intervals for Probability of Failure with Gaussian Processes", "link": "http://arxiv.org/abs/2311.07733", "description": "Efficiently approximating the probability of system failure has gained\nincreasing importance as expensive simulations begin to play a larger role in\nreliability quantification tasks in areas such as structural design, power grid\ndesign, and safety certification among others. This work derives credible\nintervals on the probability of failure for a simulation which we assume is a\nrealizations of a Gaussian process. We connect these intervals to binary\nclassification error and comment on their applicability to a broad class of\niterative schemes proposed throughout the literature. A novel iterative\nsampling scheme is proposed which can suggest multiple samples per batch for\nsimulations with parallel implementations. We empirically test our scalable,\nopen-source implementation on a variety simulations including a Tsunami model\nwhere failure is quantified in terms of maximum wave hight."}, "http://arxiv.org/abs/2311.07736": {"title": "Use of Equivalent Relative Utility (ERU) to Evaluate Artificial Intelligence-Enabled Rule-Out Devices", "link": "http://arxiv.org/abs/2311.07736", "description": "We investigated the use of equivalent relative utility (ERU) to evaluate the\neffectiveness of artificial intelligence (AI)-enabled rule-out devices that use\nAI to identify and autonomously remove non-cancer patient images from\nradiologist review in screening mammography.We reviewed two performance metrics\nthat can be used to compare the diagnostic performance between the\nradiologist-with-rule-out-device and radiologist-without-device workflows:\npositive/negative predictive values (PPV/NPV) and equivalent relative utility\n(ERU). To demonstrate the use of the two evaluation metrics, we applied both\nmethods to a recent US-based study that reported an improved performance of the\nradiologist-with-device workflow compared to the one without the device by\nretrospectively applying their AI algorithm to a large mammography dataset. We\nfurther applied the ERU method to a European study utilizing their reported\nrecall rates and cancer detection rates at different thresholds of their AI\nalgorithm to compare the potential utility among different thresholds. For the\nstudy using US data, neither the PPV/NPV nor the ERU method can conclude a\nsignificant improvement in diagnostic performance for any of the algorithm\nthresholds reported. For the study using European data, ERU values at lower AI\nthresholds are found to be higher than that at a higher threshold because more\nfalse-negative cases would be ruled-out at higher threshold, reducing the\noverall diagnostic performance. Both PPV/NPV and ERU methods can be used to\ncompare the diagnostic performance between the radiologist-with-device workflow\nand that without. One limitation of the ERU method is the need to measure the\nbaseline, standard-of-care relative utility (RU) value for mammography\nscreening in the US. Once the baseline value is known, the ERU method can be\napplied to large US datasets without knowing the true prevalence of the\ndataset."}, "http://arxiv.org/abs/2311.07752": {"title": "Doubly Robust Estimation under Possibly Misspecified Marginal Structural Cox Model", "link": "http://arxiv.org/abs/2311.07752", "description": "In this paper we address the challenges posed by non-proportional hazards and\ninformative censoring, offering a path toward more meaningful causal inference\nconclusions. We start from the marginal structural Cox model, which has been\nwidely used for analyzing observational studies with survival outcomes, and\ntypically relies on the inverse probability weighting method. The latter hinges\nupon a propensity score model for the treatment assignment, and a censoring\nmodel which incorporates both the treatment and the covariates. In such\nsettings, model misspecification can occur quite effortlessly, and the Cox\nregression model's non-collapsibility has historically posed challenges when\nstriving to guard against model misspecification through augmentation. We\nintroduce an augmented inverse probability weighted estimator which, enriched\nwith doubly robust properties, paves the way for integrating machine learning\nand a plethora of nonparametric methods, effectively overcoming the challenges\nof non-collapsibility. The estimator extends naturally to estimating a\ntime-average treatment effect when the proportional hazards assumption fails.\nWe closely examine its theoretical and practical performance, showing that it\nsatisfies both the assumption-lean and the well-specification criteria\ndiscussed in the recent literature. Finally, its application to a dataset\nreveals insights into the impact of mid-life alcohol consumption on mortality\nin later life."}, "http://arxiv.org/abs/2311.07762": {"title": "Finite Mixtures of Multivariate Poisson-Log Normal Factor Analyzers for Clustering Count Data", "link": "http://arxiv.org/abs/2311.07762", "description": "A mixture of multivariate Poisson-log normal factor analyzers is introduced\nby imposing constraints on the covariance matrix, which resulted in flexible\nmodels for clustering purposes. In particular, a class of eight parsimonious\nmixture models based on the mixtures of factor analyzers model are introduced.\nVariational Gaussian approximation is used for parameter estimation, and\ninformation criteria are used for model selection. The proposed models are\nexplored in the context of clustering discrete data arising from RNA sequencing\nstudies. Using real and simulated data, the models are shown to give favourable\nclustering performance. The GitHub R package for this work is available at\nhttps://github.com/anjalisilva/mixMPLNFA and is released under the open-source\nMIT license."}, "http://arxiv.org/abs/2311.07793": {"title": "The brain uses renewal points to model random sequences of stimuli", "link": "http://arxiv.org/abs/2311.07793", "description": "It has been classically conjectured that the brain assigns probabilistic\nmodels to sequences of stimuli. An important issue associated with this\nconjecture is the identification of the classes of models used by the brain to\nperform this task. We address this issue by using a new clustering procedure\nfor sets of electroencephalographic (EEG) data recorded from participants\nexposed to a sequence of auditory stimuli generated by a stochastic chain. This\nclustering procedure indicates that the brain uses renewal points in the\nstochastic sequence of auditory stimuli in order to build a model."}, "http://arxiv.org/abs/2311.07906": {"title": "Mixture Conditional Regression with Ultrahigh Dimensional Text Data for Estimating Extralegal Factor Effects", "link": "http://arxiv.org/abs/2311.07906", "description": "Testing judicial impartiality is a problem of fundamental importance in\nempirical legal studies, for which standard regression methods have been\npopularly used to estimate the extralegal factor effects. However, those\nmethods cannot handle control variables with ultrahigh dimensionality, such as\nfound in judgment documents recorded in text format. To solve this problem, we\ndevelop a novel mixture conditional regression (MCR) approach, assuming that\nthe whole sample can be classified into a number of latent classes. Within each\nlatent class, a standard linear regression model can be used to model the\nrelationship between the response and a key feature vector, which is assumed to\nbe of a fixed dimension. Meanwhile, ultrahigh dimensional control variables are\nthen used to determine the latent class membership, where a Na\\\"ive Bayes type\nmodel is used to describe the relationship. Hence, the dimension of control\nvariables is allowed to be arbitrarily high. A novel expectation-maximization\nalgorithm is developed for model estimation. Therefore, we are able to estimate\nthe interested key parameters as efficiently as if the true class membership\nwere known in advance. Simulation studies are presented to demonstrate the\nproposed MCR method. A real dataset of Chinese burglary offenses is analyzed\nfor illustration purpose."}, "http://arxiv.org/abs/2311.07951": {"title": "A Fast and Simple Algorithm for computing the MLE of Amplitude Density Function Parameters", "link": "http://arxiv.org/abs/2311.07951", "description": "Over the last decades, the family of $\\alpha$-stale distributions has proven\nto be useful for modelling in telecommunication systems. Particularly, in the\ncase of radar applications, finding a fast and accurate estimation for the\namplitude density function parameters appears to be very important. In this\nwork, the maximum likelihood estimator (MLE) is proposed for parameters of the\namplitude distribution. To do this, the amplitude data are \\emph{projected} on\nthe horizontal and vertical axes using two simple transformations. It is proved\nthat the \\emph{projected} data follow a zero-location symmetric $\\alpha$-stale\ndistribution for which the MLE can be computed quite fast. The average of\ncomputed MLEs based on two \\emph{projections} is considered as estimator for\nparameters of the amplitude distribution. Performance of the proposed\n\\emph{projection} method is demonstrated through simulation study and analysis\nof two sets of real radar data."}, "http://arxiv.org/abs/2311.07972": {"title": "Residual Importance Weighted Transfer Learning For High-dimensional Linear Regression", "link": "http://arxiv.org/abs/2311.07972", "description": "Transfer learning is an emerging paradigm for leveraging multiple sources to\nimprove the statistical inference on a single target. In this paper, we propose\na novel approach named residual importance weighted transfer learning (RIW-TL)\nfor high-dimensional linear models built on penalized likelihood. Compared to\nexisting methods such as Trans-Lasso that selects sources in an all-in-all-out\nmanner, RIW-TL includes samples via importance weighting and thus may permit\nmore effective sample use. To determine the weights, remarkably RIW-TL only\nrequires the knowledge of one-dimensional densities dependent on residuals,\nthus overcoming the curse of dimensionality of having to estimate\nhigh-dimensional densities in naive importance weighting. We show that the\noracle RIW-TL provides a faster rate than its competitors and develop a\ncross-fitting procedure to estimate this oracle. We discuss variants of RIW-TL\nby adopting different choices for residual weighting. The theoretical\nproperties of RIW-TL and its variants are established and compared with those\nof LASSO and Trans-Lasso. Extensive simulation and a real data analysis confirm\nits advantages."}, "http://arxiv.org/abs/2311.08004": {"title": "Nonlinear blind source separation exploiting spatial nonstationarity", "link": "http://arxiv.org/abs/2311.08004", "description": "In spatial blind source separation the observed multivariate random fields\nare assumed to be mixtures of latent spatially dependent random fields. The\nobjective is to recover latent random fields by estimating the unmixing\ntransformation. Currently, the algorithms for spatial blind source separation\ncan only estimate linear unmixing transformations. Nonlinear blind source\nseparation methods for spatial data are scarce. In this paper we extend an\nidentifiable variational autoencoder that can estimate nonlinear unmixing\ntransformations to spatially dependent data and demonstrate its performance for\nboth stationary and nonstationary spatial data using simulations. In addition,\nwe introduce scaled mean absolute Shapley additive explanations for\ninterpreting the latent components through nonlinear mixing transformation. The\nspatial identifiable variational autoencoder is applied to a geochemical\ndataset to find the latent random fields, which are then interpreted by using\nthe scaled mean absolute Shapley additive explanations."}, "http://arxiv.org/abs/2311.08050": {"title": "INLA+ -- Approximate Bayesian inference for non-sparse models using HPC", "link": "http://arxiv.org/abs/2311.08050", "description": "The integrated nested Laplace approximations (INLA) method has become a\nwidely utilized tool for researchers and practitioners seeking to perform\napproximate Bayesian inference across various fields of application. To address\nthe growing demand for incorporating more complex models and enhancing the\nmethod's capabilities, this paper introduces a novel framework that leverages\ndense matrices for performing approximate Bayesian inference based on INLA\nacross multiple computing nodes using HPC. When dealing with non-sparse\nprecision or covariance matrices, this new approach scales better compared to\nthe current INLA method, capitalizing on the computational power offered by\nmultiprocessors in shared and distributed memory architectures available in\ncontemporary computing resources and specialized dense matrix algebra. To\nvalidate the efficacy of this approach, we conduct a simulation study then\napply it to analyze cancer mortality data in Spain, employing a three-way\nspatio-temporal interaction model."}, "http://arxiv.org/abs/2311.08139": {"title": "Feedforward neural networks as statistical models: Improving interpretability through uncertainty quantification", "link": "http://arxiv.org/abs/2311.08139", "description": "Feedforward neural networks (FNNs) are typically viewed as pure prediction\nalgorithms, and their strong predictive performance has led to their use in\nmany machine-learning applications. However, their flexibility comes with an\ninterpretability trade-off; thus, FNNs have been historically less popular\namong statisticians. Nevertheless, classical statistical theory, such as\nsignificance testing and uncertainty quantification, is still relevant.\nSupplementing FNNs with methods of statistical inference, and covariate-effect\nvisualisations, can shift the focus away from black-box prediction and make\nFNNs more akin to traditional statistical models. This can allow for more\ninferential analysis, and, hence, make FNNs more accessible within the\nstatistical-modelling context."}, "http://arxiv.org/abs/2311.08168": {"title": "Time-Uniform Confidence Spheres for Means of Random Vectors", "link": "http://arxiv.org/abs/2311.08168", "description": "We derive and study time-uniform confidence spheres - termed confidence\nsphere sequences (CSSs) - which contain the mean of random vectors with high\nprobability simultaneously across all sample sizes. Inspired by the original\nwork of Catoni and Giulini, we unify and extend their analysis to cover both\nthe sequential setting and to handle a variety of distributional assumptions.\nMore concretely, our results include an empirical-Bernstein CSS for bounded\nrandom vectors (resulting in a novel empirical-Bernstein confidence interval),\na CSS for sub-$\\psi$ random vectors, and a CSS for heavy-tailed random vectors\nbased on a sequentially valid Catoni-Giulini estimator. Finally, we provide a\nversion of our empirical-Bernstein CSS that is robust to contamination by Huber\nnoise."}, "http://arxiv.org/abs/2311.08181": {"title": "Frame to frame interpolation for high-dimensional data visualisation using the woylier package", "link": "http://arxiv.org/abs/2311.08181", "description": "The woylier package implements tour interpolation paths between frames using\nGivens rotations. This provides an alternative to the geodesic interpolation\nbetween planes currently available in the tourr package. Tours are used to\nvisualise high-dimensional data and models, to detect clustering, anomalies and\nnon-linear relationships. Frame-to-frame interpolation can be useful for\nprojection pursuit guided tours when the index is not rotationally invariant.\nIt also provides a way to specifically reach a given target frame. We\ndemonstrate the method for exploring non-linear relationships between currency\ncross-rates."}, "http://arxiv.org/abs/2311.08254": {"title": "Identifiable and interpretable nonparametric factor analysis", "link": "http://arxiv.org/abs/2311.08254", "description": "Factor models have been widely used to summarize the variability of\nhigh-dimensional data through a set of factors with much lower dimensionality.\nGaussian linear factor models have been particularly popular due to their\ninterpretability and ease of computation. However, in practice, data often\nviolate the multivariate Gaussian assumption. To characterize higher-order\ndependence and nonlinearity, models that include factors as predictors in\nflexible multivariate regression are popular, with GP-LVMs using Gaussian\nprocess (GP) priors for the regression function and VAEs using deep neural\nnetworks. Unfortunately, such approaches lack identifiability and\ninterpretability and tend to produce brittle and non-reproducible results. To\naddress these problems by simplifying the nonparametric factor model while\nmaintaining flexibility, we propose the NIFTY framework, which parsimoniously\ntransforms uniform latent variables using one-dimensional nonlinear mappings\nand then applies a linear generative model. The induced multivariate\ndistribution falls into a flexible class while maintaining simple computation\nand interpretation. We prove that this model is identifiable and empirically\nstudy NIFTY using simulated data, observing good performance in density\nestimation and data visualization. We then apply NIFTY to bird song data in an\nenvironmental monitoring application."}, "http://arxiv.org/abs/2311.08315": {"title": "Total Empiricism: Learning from Data", "link": "http://arxiv.org/abs/2311.08315", "description": "Statistical analysis is an important tool to distinguish systematic from\nchance findings. Current statistical analyses rely on distributional\nassumptions reflecting the structure of some underlying model, which if not met\nlead to problems in the analysis and interpretation of the results. Instead of\ntrying to fix the model or \"correct\" the data, we here describe a totally\nempirical statistical approach that does not rely on ad hoc distributional\nassumptions in order to overcome many problems in contemporary statistics.\nStarting from elementary combinatorics, we motivate an information-guided\nformalism to quantify knowledge extracted from the given data. Subsequently, we\nderive model-agnostic methods to identify patterns that are solely evidenced by\nthe data based on our prior knowledge. The data-centric character of empiricism\nallows for its universal applicability, particularly as sample size grows\nlarger. In this comprehensive framework, we re-interpret and extend model\ndistributions, scores and statistical tests used in different schools of\nstatistics."}, "http://arxiv.org/abs/2311.08335": {"title": "Distinguishing immunological and behavioral effects of vaccination", "link": "http://arxiv.org/abs/2311.08335", "description": "The interpretation of vaccine efficacy estimands is subtle, even in\nrandomized trials designed to quantify immunological effects of vaccination. In\nthis article, we introduce terminology to distinguish between different vaccine\nefficacy estimands and clarify their interpretations. This allows us to\nexplicitly consider immunological and behavioural effects of vaccination, and\nestablish that policy-relevant estimands can differ substantially from those\ncommonly reported in vaccine trials. We further show that a conventional\nvaccine trial allows identification and estimation of different vaccine\nestimands under plausible conditions, if one additional post-treatment variable\nis measured. Specifically, we utilize a ``belief variable'' that indicates the\ntreatment an individual believed they had received. The belief variable is\nsimilar to ``blinding assessment'' variables that are occasionally collected in\nplacebo-controlled trials in other fields. We illustrate the relations between\nthe different estimands, and their practical relevance, in numerical examples\nbased on an influenza vaccine trial."}, "http://arxiv.org/abs/2311.08340": {"title": "Causal Message Passing: A Method for Experiments with Unknown and General Network Interference", "link": "http://arxiv.org/abs/2311.08340", "description": "Randomized experiments are a powerful methodology for data-driven evaluation\nof decisions or interventions. Yet, their validity may be undermined by network\ninterference. This occurs when the treatment of one unit impacts not only its\noutcome but also that of connected units, biasing traditional treatment effect\nestimations. Our study introduces a new framework to accommodate complex and\nunknown network interference, moving beyond specialized models in the existing\nliterature. Our framework, which we term causal message-passing, is grounded in\na high-dimensional approximate message passing methodology and is specifically\ntailored to experimental design settings with prevalent network interference.\nUtilizing causal message-passing, we present a practical algorithm for\nestimating the total treatment effect and demonstrate its efficacy in four\nnumerical scenarios, each with its unique interference structure."}, "http://arxiv.org/abs/2104.14987": {"title": "Emulating complex dynamical simulators with random Fourier features", "link": "http://arxiv.org/abs/2104.14987", "description": "A Gaussian process (GP)-based methodology is proposed to emulate complex\ndynamical computer models (or simulators). The method relies on emulating the\nnumerical flow map of the system over an initial (short) time step, where the\nflow map is a function that describes the evolution of the system from an\ninitial condition to a subsequent value at the next time step. This yields a\nprobabilistic distribution over the entire flow map function, with each draw\noffering an approximation to the flow map. The model output times series is\nthen predicted (under the Markov assumption) by drawing a sample from the\nemulated flow map (i.e., its posterior distribution) and using it to iterate\nfrom the initial condition ahead in time. Repeating this procedure with\nmultiple such draws creates a distribution over the time series. The mean and\nvariance of this distribution at a specific time point serve as the model\noutput prediction and the associated uncertainty, respectively. However,\ndrawing a GP posterior sample that represents the underlying function across\nits entire domain is computationally infeasible, given the infinite-dimensional\nnature of this object. To overcome this limitation, one can generate such a\nsample in an approximate manner using random Fourier features (RFF). RFF is an\nefficient technique for approximating the kernel and generating GP samples,\noffering both computational efficiency and theoretical guarantees. The proposed\nmethod is applied to emulate several dynamic nonlinear simulators including the\nwell-known Lorenz and van der Pol models. The results suggest that our approach\nhas a high predictive performance and the associated uncertainty can capture\nthe dynamics of the system accurately."}, "http://arxiv.org/abs/2111.12945": {"title": "Low-rank variational Bayes correction to the Laplace method", "link": "http://arxiv.org/abs/2111.12945", "description": "Approximate inference methods like the Laplace method, Laplace approximations\nand variational methods, amongst others, are popular methods when exact\ninference is not feasible due to the complexity of the model or the abundance\nof data. In this paper we propose a hybrid approximate method called Low-Rank\nVariational Bayes correction (VBC), that uses the Laplace method and\nsubsequently a Variational Bayes correction in a lower dimension, to the joint\nposterior mean. The cost is essentially that of the Laplace method which\nensures scalability of the method, in both model complexity and data size.\nModels with fixed and unknown hyperparameters are considered, for simulated and\nreal examples, for small and large datasets."}, "http://arxiv.org/abs/2202.13961": {"title": "Spatio-Causal Patterns of Sample Growth", "link": "http://arxiv.org/abs/2202.13961", "description": "Different statistical samples (e.g., from different locations) offer\npopulations and learning systems observations with distinct statistical\nproperties. Samples under (1) 'Unconfounded' growth preserve systems' ability\nto determine the independent effects of their individual variables on any\noutcome-of-interest (and lead, therefore, to fair and interpretable black-box\npredictions). Samples under (2) 'Externally-Valid' growth preserve their\nability to make predictions that generalize across out-of-sample variation. The\nfirst promotes predictions that generalize over populations, the second over\ntheir shared uncontrolled factors. We illustrate these theoretic patterns in\nthe full American census from 1840 to 1940, and samples ranging from the\nstreet-level all the way to the national. This reveals sample requirements for\ngeneralizability over space and time, and new connections among the Shapley\nvalue, counterfactual statistics, and hyperbolic geometry."}, "http://arxiv.org/abs/2211.04958": {"title": "Black-Box Model Confidence Sets Using Cross-Validation with High-Dimensional Gaussian Comparison", "link": "http://arxiv.org/abs/2211.04958", "description": "We derive high-dimensional Gaussian comparison results for the standard\n$V$-fold cross-validated risk estimates. Our results combine a recent\nstability-based argument for the low-dimensional central limit theorem of\ncross-validation with the high-dimensional Gaussian comparison framework for\nsums of independent random variables. These results give new insights into the\njoint sampling distribution of cross-validated risks in the context of model\ncomparison and tuning parameter selection, where the number of candidate models\nand tuning parameters can be larger than the fitting sample size. As a\nconsequence, our results provide theoretical support for a recent\nmethodological development that constructs model confidence sets using\ncross-validation."}, "http://arxiv.org/abs/2311.08427": {"title": "Towards a Transportable Causal Network Model Based on Observational Healthcare Data", "link": "http://arxiv.org/abs/2311.08427", "description": "Over the last decades, many prognostic models based on artificial\nintelligence techniques have been used to provide detailed predictions in\nhealthcare. Unfortunately, the real-world observational data used to train and\nvalidate these models are almost always affected by biases that can strongly\nimpact the outcomes validity: two examples are values missing not-at-random and\nselection bias. Addressing them is a key element in achieving transportability\nand in studying the causal relationships that are critical in clinical decision\nmaking, going beyond simpler statistical approaches based on probabilistic\nassociation.\n\nIn this context, we propose a novel approach that combines selection\ndiagrams, missingness graphs, causal discovery and prior knowledge into a\nsingle graphical model to estimate the cardiovascular risk of adolescent and\nyoung females who survived breast cancer. We learn this model from data\ncomprising two different cohorts of patients. The resulting causal network\nmodel is validated by expert clinicians in terms of risk assessment, accuracy\nand explainability, and provides a prognostic model that outperforms competing\nmachine learning methods."}, "http://arxiv.org/abs/2311.08484": {"title": "Covariance Assisted Multivariate Penalized Additive Regression (CoMPAdRe)", "link": "http://arxiv.org/abs/2311.08484", "description": "We propose a new method for the simultaneous selection and estimation of\nmultivariate sparse additive models with correlated errors. Our method called\nCovariance Assisted Multivariate Penalized Additive Regression (CoMPAdRe)\nsimultaneously selects among null, linear, and smooth non-linear effects for\neach predictor while incorporating joint estimation of the sparse residual\nstructure among responses, with the motivation that accounting for\ninter-response correlation structure can lead to improved accuracy in variable\nselection and estimation efficiency. CoMPAdRe is constructed in a\ncomputationally efficient way that allows the selection and estimation of\nlinear and non-linear covariates to be conducted in parallel across responses.\nCompared to single-response approaches that marginally select linear and\nnon-linear covariate effects, we demonstrate in simulation studies that the\njoint multivariate modeling leads to gains in both estimation efficiency and\nselection accuracy, of greater magnitude in settings where signal is moderate\nrelative to the level of noise. We apply our approach to protein-mRNA\nexpression levels from multiple breast cancer pathways obtained from The Cancer\nProteome Atlas and characterize both mRNA-protein associations and\nprotein-protein subnetworks for each pathway. We find non-linear mRNA-protein\nassociations for the Core Reactive, EMT, PIK-AKT, and RTK pathways."}, "http://arxiv.org/abs/2311.08527": {"title": "Inferring the Long-Term Causal Effects of Long-Term Treatments from Short-Term Experiments", "link": "http://arxiv.org/abs/2311.08527", "description": "We study inference on the long-term causal effect of a continual exposure to\na novel intervention, which we term a long-term treatment, based on an\nexperiment involving only short-term observations. Key examples include the\nlong-term health effects of regularly-taken medicine or of environmental\nhazards and the long-term effects on users of changes to an online platform.\nThis stands in contrast to short-term treatments or \"shocks,\" whose long-term\neffect can reasonably be mediated by short-term observations, enabling the use\nof surrogate methods. Long-term treatments by definition have direct effects on\nlong-term outcomes via continual exposure so surrogacy cannot reasonably hold.\n\nOur approach instead learns long-term temporal dynamics directly from\nshort-term experimental data, assuming that the initial dynamics observed\npersist but avoiding the need for both surrogacy assumptions and auxiliary data\nwith long-term observations. We connect the problem with offline reinforcement\nlearning, leveraging doubly-robust estimators to estimate long-term causal\neffects for long-term treatments and construct confidence intervals. Finally,\nwe demonstrate the method in simulated experiments."}, "http://arxiv.org/abs/2311.08561": {"title": "Measuring association with recursive rank binning", "link": "http://arxiv.org/abs/2311.08561", "description": "Pairwise measures of dependence are a common tool to map data in the early\nstages of analysis with several modern examples based on maximized partitions\nof the pairwise sample space. Following a short survey of modern measures of\ndependence, we introduce a new measure which recursively splits the ranks of a\npair of variables to partition the sample space and computes the $\\chi^2$\nstatistic on the resulting bins. Splitting logic is detailed for splits\nmaximizing a score function and randomly selected splits. Simulations indicate\nthat random splitting produces a statistic conservatively approximated by the\n$\\chi^2$ distribution without a loss of power to detect numerous different data\npatterns compared to maximized binning. Though it seems to add no power to\ndetect dependence, maximized recursive binning is shown to produce a natural\nvisualization of the data and the measure. Applying maximized recursive rank\nbinning to S&amp;P 500 constituent data suggests the automatic detection of tail\ndependence."}, "http://arxiv.org/abs/2311.08604": {"title": "Incremental Cost-Effectiveness Statistical Inference: Calculations and Communications", "link": "http://arxiv.org/abs/2311.08604", "description": "We illustrate use of nonparametric statistical methods to compare alternative\ntreatments for a particular disease or condition on both their relative\neffectiveness and their relative cost. These Incremental Cost Effectiveness\n(ICE) methods are based upon Bootstrapping, i.e. Resampling with Replacement\nfrom observational or clinical-trial data on individual patients. We first show\nhow a reasonable numerical value for the \"Shadow Price of Health\" can be chosen\nusing functions within the ICEinfer R-package when effectiveness is not\nmeasured in \"QALY\"s. We also argue that simple histograms are ideal for\ncommunicating key findings to regulators, while our more detailed graphics may\nwell be more informative and compelling for other health-care stakeholders."}, "http://arxiv.org/abs/2311.08658": {"title": "Structured Estimation of Heterogeneous Time Series", "link": "http://arxiv.org/abs/2311.08658", "description": "How best to model structurally heterogeneous processes is a foundational\nquestion in the social, health and behavioral sciences. Recently, Fisher et\nal., (2022) introduced the multi-VAR approach for simultaneously estimating\nmultiple-subject multivariate time series characterized by common and\nindividualizing features using penalized estimation. This approach differs from\nmany popular modeling approaches for multiple-subject time series in that\nqualitative and quantitative differences in a large number of individual\ndynamics are well-accommodated. The current work extends the multi-VAR\nframework to include new adaptive weighting schemes that greatly improve\nestimation performance. In a small set of simulation studies we compare\nadaptive multi-VAR with these new penalty weights to common alternative\nestimators in terms of path recovery and bias. Furthermore, we provide toy\nexamples and code demonstrating the utility of multi-VAR under different\nheterogeneity regimes using the multivar package for R (Fisher, 2022)."}, "http://arxiv.org/abs/2311.08690": {"title": "Enabling CMF Estimation in Data-Constrained Scenarios: A Semantic-Encoding Knowledge Mining Model", "link": "http://arxiv.org/abs/2311.08690", "description": "Precise estimation of Crash Modification Factors (CMFs) is central to\nevaluating the effectiveness of various road safety treatments and prioritizing\ninfrastructure investment accordingly. While customized study for each\ncountermeasure scenario is desired, the conventional CMF estimation approaches\nrely heavily on the availability of crash data at given sites. This not only\nmakes the estimation costly, but the results are also less transferable, since\nthe intrinsic similarities between different safety countermeasure scenarios\nare not fully explored. Aiming to fill this gap, this study introduces a novel\nknowledge-mining framework for CMF prediction. This framework delves into the\nconnections of existing countermeasures and reduces the reliance of CMF\nestimation on crash data availability and manual data collection. Specifically,\nit draws inspiration from human comprehension processes and introduces advanced\nNatural Language Processing (NLP) techniques to extract intricate variations\nand patterns from existing CMF knowledge. It effectively encodes unstructured\ncountermeasure scenarios into machine-readable representations and models the\ncomplex relationships between scenarios and CMF values. This new data-driven\nframework provides a cost-effective and adaptable solution that complements the\ncase-specific approaches for CMF estimation, which is particularly beneficial\nwhen availability of crash data or time imposes constraints. Experimental\nvalidation using real-world CMF Clearinghouse data demonstrates the\neffectiveness of this new approach, which shows significant accuracy\nimprovements compared to baseline methods. This approach provides insights into\nnew possibilities of harnessing accumulated transportation knowledge in various\napplications."}, "http://arxiv.org/abs/2311.08691": {"title": "On Doubly Robust Estimation with Nonignorable Missing Data Using Instrumental Variables", "link": "http://arxiv.org/abs/2311.08691", "description": "Suppose we are interested in the mean of an outcome that is subject to\nnonignorable nonresponse. This paper develops new semiparametric estimation\nmethods with instrumental variables which affect nonresponse, but not the\noutcome. The proposed estimators remain consistent and asymptotically normal\neven under partial model misspecifications for two variation independent\nnuisance functions. We evaluate the performance of the proposed estimators via\na simulation study, and apply them in adjusting for missing data induced by HIV\ntesting refusal in the evaluation of HIV seroprevalence in Mochudi, Botswana,\nusing interviewer experience as an instrumental variable."}, "http://arxiv.org/abs/2311.08743": {"title": "Kernel-based independence tests for causal structure learning on functional data", "link": "http://arxiv.org/abs/2311.08743", "description": "Measurements of systems taken along a continuous functional dimension, such\nas time or space, are ubiquitous in many fields, from the physical and\nbiological sciences to economics and engineering.Such measurements can be\nviewed as realisations of an underlying smooth process sampled over the\ncontinuum. However, traditional methods for independence testing and causal\nlearning are not directly applicable to such data, as they do not take into\naccount the dependence along the functional dimension. By using specifically\ndesigned kernels, we introduce statistical tests for bivariate, joint, and\nconditional independence for functional variables. Our method not only extends\nthe applicability to functional data of the HSIC and its d-variate version\n(d-HSIC), but also allows us to introduce a test for conditional independence\nby defining a novel statistic for the CPT based on the HSCIC, with optimised\nregularisation strength estimated through an evaluation rejection rate. Our\nempirical results of the size and power of these tests on synthetic functional\ndata show good performance, and we then exemplify their application to several\nconstraint- and regression-based causal structure learning problems, including\nboth synthetic examples and real socio-economic data."}, "http://arxiv.org/abs/2311.08752": {"title": "ProSpar-GP: scalable Gaussian process modeling with massive non-stationary datasets", "link": "http://arxiv.org/abs/2311.08752", "description": "Gaussian processes (GPs) are a popular class of Bayesian nonparametric\nmodels, but its training can be computationally burdensome for massive training\ndatasets. While there has been notable work on scaling up these models for big\ndata, existing methods typically rely on a stationary GP assumption for\napproximation, and can thus perform poorly when the underlying response surface\nis non-stationary, i.e., it has some regions of rapid change and other regions\nwith little change. Such non-stationarity is, however, ubiquitous in real-world\nproblems, including our motivating application for surrogate modeling of\ncomputer experiments. We thus propose a new Product of Sparse GP (ProSpar-GP)\nmethod for scalable GP modeling with massive non-stationary data. The\nProSpar-GP makes use of a carefully-constructed product-of-experts formulation\nof sparse GP experts, where different experts are placed within local regions\nof non-stationarity. These GP experts are fit via a novel variational inference\napproach, which capitalizes on mini-batching and GPU acceleration for efficient\noptimization of inducing points and length-scale parameters for each expert. We\nfurther show that the ProSpar-GP is Kolmogorov-consistent, in that its\ngenerative distribution defines a valid stochastic process over the prediction\nspace; such a property provides essential stability for variational inference,\nparticularly in the presence of non-stationarity. We then demonstrate the\nimproved performance of the ProSpar-GP over the state-of-the-art, in a suite of\nnumerical experiments and an application for surrogate modeling of a satellite\ndrag simulator."}, "http://arxiv.org/abs/2311.08812": {"title": "Optimal subsampling algorithm for the marginal model with large longitudinal data", "link": "http://arxiv.org/abs/2311.08812", "description": "Big data is ubiquitous in practices, and it has also led to heavy computation\nburden. To reduce the calculation cost and ensure the effectiveness of\nparameter estimators, an optimal subset sampling method is proposed to estimate\nthe parameters in marginal models with massive longitudinal data. The optimal\nsubsampling probabilities are derived, and the corresponding asymptotic\nproperties are established to ensure the consistency and asymptotic normality\nof the estimator. Extensive simulation studies are carried out to evaluate the\nperformance of the proposed method for continuous, binary and count data and\nwith four different working correlation matrices. A depression data is used to\nillustrate the proposed method."}, "http://arxiv.org/abs/2311.08845": {"title": "Statistical learning by sparse deep neural networks", "link": "http://arxiv.org/abs/2311.08845", "description": "We consider a deep neural network estimator based on empirical risk\nminimization with l_1-regularization. We derive a general bound for its excess\nrisk in regression and classification (including multiclass), and prove that it\nis adaptively nearly-minimax (up to log-factors) simultaneously across the\nentire range of various function classes."}, "http://arxiv.org/abs/2311.08908": {"title": "Robust Brain MRI Image Classification with SIBOW-SVM", "link": "http://arxiv.org/abs/2311.08908", "description": "The majority of primary Central Nervous System (CNS) tumors in the brain are\namong the most aggressive diseases affecting humans. Early detection of brain\ntumor types, whether benign or malignant, glial or non-glial, is critical for\ncancer prevention and treatment, ultimately improving human life expectancy.\nMagnetic Resonance Imaging (MRI) stands as the most effective technique to\ndetect brain tumors by generating comprehensive brain images through scans.\nHowever, human examination can be error-prone and inefficient due to the\ncomplexity, size, and location variability of brain tumors. Recently, automated\nclassification techniques using machine learning (ML) methods, such as\nConvolutional Neural Network (CNN), have demonstrated significantly higher\naccuracy than manual screening, while maintaining low computational costs.\nNonetheless, deep learning-based image classification methods, including CNN,\nface challenges in estimating class probabilities without proper model\ncalibration. In this paper, we propose a novel brain tumor image classification\nmethod, called SIBOW-SVM, which integrates the Bag-of-Features (BoF) model with\nSIFT feature extraction and weighted Support Vector Machines (wSVMs). This new\napproach effectively captures hidden image features, enabling the\ndifferentiation of various tumor types and accurate label predictions.\nAdditionally, the SIBOW-SVM is able to estimate the probabilities of images\nbelonging to each class, thereby providing high-confidence classification\ndecisions. We have also developed scalable and parallelable algorithms to\nfacilitate the practical implementation of SIBOW-SVM for massive images. As a\nbenchmark, we apply the SIBOW-SVM to a public data set of brain tumor MRI\nimages containing four classes: glioma, meningioma, pituitary, and normal. Our\nresults show that the new method outperforms state-of-the-art methods,\nincluding CNN."}, "http://arxiv.org/abs/2311.09015": {"title": "Identification and Estimation for Nonignorable Missing Data: A Data Fusion Approach", "link": "http://arxiv.org/abs/2311.09015", "description": "We consider the task of identifying and estimating a parameter of interest in\nsettings where data is missing not at random (MNAR). In general, such\nparameters are not identified without strong assumptions on the missing data\nmodel. In this paper, we take an alternative approach and introduce a method\ninspired by data fusion, where information in an MNAR dataset is augmented by\ninformation in an auxiliary dataset subject to missingness at random (MAR). We\nshow that even if the parameter of interest cannot be identified given either\ndataset alone, it can be identified given pooled data, under two complementary\nsets of assumptions. We derive an inverse probability weighted (IPW) estimator\nfor identified parameters, and evaluate the performance of our estimation\nstrategies via simulation studies."}, "http://arxiv.org/abs/2311.09081": {"title": "Posterior accuracy and calibration under misspecification in Bayesian generalized linear models", "link": "http://arxiv.org/abs/2311.09081", "description": "Generalized linear models (GLMs) are popular for data-analysis in almost all\nquantitative sciences, but the choice of likelihood family and link function is\noften difficult. This motivates the search for likelihoods and links that\nminimize the impact of potential misspecification. We perform a large-scale\nsimulation study on double-bounded and lower-bounded response data where we\nsystematically vary both true and assumed likelihoods and links. In contrast to\nprevious studies, we also study posterior calibration and uncertainty metrics\nin addition to point-estimate accuracy. Our results indicate that certain\nlikelihoods and links can be remarkably robust to misspecification, performing\nalmost on par with their respective true counterparts. Additionally, normal\nlikelihood models with identity link (i.e., linear regression) often achieve\ncalibration comparable to the more structurally faithful alternatives, at least\nin the studied scenarios. On the basis of our findings, we provide practical\nsuggestions for robust likelihood and link choices in GLMs."}, "http://arxiv.org/abs/2311.09107": {"title": "Illness-death model with renewal", "link": "http://arxiv.org/abs/2311.09107", "description": "The illness-death model for chronic conditions is combined with a renewal\nequation for the number of newborns taking into account possibly different\nfertility rates in the healthy and diseased parts of the population. The\nresulting boundary value problem consists of a system of partial differential\nequations with an integral boundary condition. As an application, the boundary\nvalue problem is applied to an example about type 2 diabetes."}, "http://arxiv.org/abs/2311.09137": {"title": "Causal prediction models for medication safety monitoring: The diagnosis of vancomycin-induced acute kidney injury", "link": "http://arxiv.org/abs/2311.09137", "description": "The current best practice approach for the retrospective diagnosis of adverse\ndrug events (ADEs) in hospitalized patients relies on a full patient chart\nreview and a formal causality assessment by multiple medical experts. This\nevaluation serves to qualitatively estimate the probability of causation (PC);\nthe probability that a drug was a necessary cause of an adverse event. This\npractice is manual, resource intensive and prone to human biases, and may thus\nbenefit from data-driven decision support. Here, we pioneer a causal modeling\napproach using observational data to estimate a lower bound of the PC\n(PC$_{low}$). This method includes two key causal inference components: (1) the\ntarget trial emulation framework and (2) estimation of individualized treatment\neffects using machine learning. We apply our method to the clinically relevant\nuse-case of vancomycin-induced acute kidney injury in intensive care patients,\nand compare our causal model-based PC$_{low}$ estimates to qualitative\nestimates of the PC provided by a medical expert. Important limitations and\npotential improvements are discussed, and we conclude that future improved\ncausal models could provide essential data-driven support for medication safety\nmonitoring in hospitalized patients."}, "http://arxiv.org/abs/1911.03071": {"title": "Balancing Covariates in Randomized Experiments with the Gram-Schmidt Walk Design", "link": "http://arxiv.org/abs/1911.03071", "description": "The design of experiments involves a compromise between covariate balance and\nrobustness. This paper provides a formalization of this trade-off and describes\nan experimental design that allows experimenters to navigate it. The design is\nspecified by a robustness parameter that bounds the worst-case mean squared\nerror of an estimator of the average treatment effect. Subject to the\nexperimenter's desired level of robustness, the design aims to simultaneously\nbalance all linear functions of potentially many covariates. Less robustness\nallows for more balance. We show that the mean squared error of the estimator\nis bounded in finite samples by the minimum of the loss function of an implicit\nridge regression of the potential outcomes on the covariates. Asymptotically,\nthe design perfectly balances all linear functions of a growing number of\ncovariates with a diminishing reduction in robustness, effectively allowing\nexperimenters to escape the compromise between balance and robustness in large\nsamples. Finally, we describe conditions that ensure asymptotic normality and\nprovide a conservative variance estimator, which facilitate the construction of\nasymptotically valid confidence intervals."}, "http://arxiv.org/abs/2007.10432": {"title": "Treatment Effects with Targeting Instruments", "link": "http://arxiv.org/abs/2007.10432", "description": "Multivalued treatments are commonplace in applications. We explore the use of\ndiscrete-valued instruments to control for selection bias in this setting. Our\ndiscussion revolves around the concept of targeting instruments: which\ninstruments target which treatments. It allows us to establish conditions under\nwhich counterfactual averages and treatment effects are point- or\npartially-identified for composite complier groups. We illustrate the\nusefulness of our framework by applying it to data from the Head Start Impact\nStudy. Under a plausible positive selection assumption, we derive informative\nbounds that suggest less beneficial effects of Head Start expansions than the\nparametric estimates of Kline and Walters (2016)."}, "http://arxiv.org/abs/2102.07356": {"title": "Asymptotic properties of generalized closed-form maximum likelihood estimators", "link": "http://arxiv.org/abs/2102.07356", "description": "The maximum likelihood estimator (MLE) is pivotal in statistical inference,\nyet its application is often hindered by the absence of closed-form solutions\nfor many models. This poses challenges in real-time computation scenarios,\nparticularly within embedded systems technology, where numerical methods are\nimpractical. This study introduces a generalized form of the MLE that yields\nclosed-form estimators under certain conditions. We derive the asymptotic\nproperties of the proposed estimator and demonstrate that our approach retains\nkey properties such as invariance under one-to-one transformations, strong\nconsistency, and an asymptotic normal distribution. The effectiveness of the\ngeneralized MLE is exemplified through its application to the Gamma, Nakagami,\nand Beta distributions, showcasing improvements over the traditional MLE.\nAdditionally, we extend this methodology to a bivariate gamma distribution,\nsuccessfully deriving closed-form estimators. This advancement presents\nsignificant implications for real-time statistical analysis across various\napplications."}, "http://arxiv.org/abs/2207.13493": {"title": "The Cellwise Minimum Covariance Determinant Estimator", "link": "http://arxiv.org/abs/2207.13493", "description": "The usual Minimum Covariance Determinant (MCD) estimator of a covariance\nmatrix is robust against casewise outliers. These are cases (that is, rows of\nthe data matrix) that behave differently from the majority of cases, raising\nsuspicion that they might belong to a different population. On the other hand,\ncellwise outliers are individual cells in the data matrix. When a row contains\none or more outlying cells, the other cells in the same row still contain\nuseful information that we wish to preserve. We propose a cellwise robust\nversion of the MCD method, called cellMCD. Its main building blocks are\nobserved likelihood and a penalty term on the number of flagged cellwise\noutliers. It possesses good breakdown properties. We construct a fast algorithm\nfor cellMCD based on concentration steps (C-steps) that always lower the\nobjective. The method performs well in simulations with cellwise outliers, and\nhas high finite-sample efficiency on clean data. It is illustrated on real data\nwith visualizations of the results."}, "http://arxiv.org/abs/2208.07086": {"title": "Flexible Bayesian Multiple Comparison Adjustment Using Dirichlet Process and Beta-Binomial Model Priors", "link": "http://arxiv.org/abs/2208.07086", "description": "Researchers frequently wish to assess the equality or inequality of groups,\nbut this comes with the challenge of adequately adjusting for multiple\ncomparisons. Statistically, all possible configurations of equality and\ninequality constraints can be uniquely represented as partitions of the groups,\nwhere any number of groups are equal if they are in the same partition. In a\nBayesian framework, one can adjust for multiple comparisons by constructing a\nsuitable prior distribution over all possible partitions. Inspired by work on\nvariable selection in regression, we propose a class of flexible beta-binomial\npriors for Bayesian multiple comparison adjustment. We compare this prior setup\nto the Dirichlet process prior suggested by Gopalan and Berry (1998) and\nmultiple comparison adjustment methods that do not specify a prior over\npartitions directly. Our approach to multiple comparison adjustment not only\nallows researchers to assess all pairwise (in)equalities, but in fact all\npossible (in)equalities among all groups. As a consequence, the space of\npossible partitions grows quickly - for ten groups, there are already 115,975\npossible partitions - and we set up a stochastic search algorithm to\nefficiently explore the space. Our method is implemented in the Julia package\nEqualitySampler, and we illustrate it on examples related to the comparison of\nmeans, variances, and proportions."}, "http://arxiv.org/abs/2208.07959": {"title": "Variable Selection in Latent Regression IRT Models via Knockoffs: An Application to International Large-scale Assessment in Education", "link": "http://arxiv.org/abs/2208.07959", "description": "International large-scale assessments (ILSAs) play an important role in\neducational research and policy making. They collect valuable data on education\nquality and performance development across many education systems, giving\ncountries the opportunity to share techniques, organizational structures, and\npolicies that have proven efficient and successful. To gain insights from ILSA\ndata, we identify non-cognitive variables associated with students' academic\nperformance. This problem has three analytical challenges: 1) academic\nperformance is measured by cognitive items under a matrix sampling design; 2)\nthere are many missing values in the non-cognitive variables; and 3) multiple\ncomparisons due to a large number of non-cognitive variables. We consider an\napplication to the Programme for International Student Assessment (PISA),\naiming to identify non-cognitive variables associated with students'\nperformance in science. We formulate it as a variable selection problem under a\ngeneral latent variable model framework and further propose a knockoff method\nthat conducts variable selection with a controlled error rate for false\nselections."}, "http://arxiv.org/abs/2210.06927": {"title": "Prediction can be safely used as a proxy for explanation in causally consistent Bayesian generalized linear models", "link": "http://arxiv.org/abs/2210.06927", "description": "Bayesian modeling provides a principled approach to quantifying uncertainty\nin model parameters and model structure and has seen a surge of applications in\nrecent years. Within the context of a Bayesian workflow, we are concerned with\nmodel selection for the purpose of finding models that best explain the data,\nthat is, help us understand the underlying data generating process. Since we\nrarely have access to the true process, all we are left with during real-world\nanalyses is incomplete causal knowledge from sources outside of the current\ndata and model predictions of said data. This leads to the important question\nof when the use of prediction as a proxy for explanation for the purpose of\nmodel selection is valid. We approach this question by means of large-scale\nsimulations of Bayesian generalized linear models where we investigate various\ncausal and statistical misspecifications. Our results indicate that the use of\nprediction as proxy for explanation is valid and safe only when the models\nunder consideration are sufficiently consistent with the underlying causal\nstructure of the true data generating process."}, "http://arxiv.org/abs/2212.04550": {"title": "Modern Statistical Models and Methods for Estimating Fatigue-Life and Fatigue-Strength Distributions from Experimental Data", "link": "http://arxiv.org/abs/2212.04550", "description": "Engineers and scientists have been collecting and analyzing fatigue data\nsince the 1800s to ensure the reliability of life-critical structures.\nApplications include (but are not limited to) bridges, building structures,\naircraft and spacecraft components, ships, ground-based vehicles, and medical\ndevices. Engineers need to estimate S-N relationships (Stress or Strain versus\nNumber of cycles to failure), typically with a focus on estimating small\nquantiles of the fatigue-life distribution. Estimates from this kind of model\nare used as input to models (e.g., cumulative damage models) that predict\nfailure-time distributions under varying stress patterns. Also, design\nengineers need to estimate lower-tail quantiles of the closely related\nfatigue-strength distribution. The history of applying incorrect statistical\nmethods is nearly as long and such practices continue to the present. Examples\ninclude treating the applied stress (or strain) as the response and the number\nof cycles to failure as the explanatory variable in regression analyses\n(because of the need to estimate strength distributions) and ignoring or\notherwise mishandling censored observations (known as runouts in the fatigue\nliterature). The first part of the paper reviews the traditional modeling\napproach where a fatigue-life model is specified. We then show how this\nspecification induces a corresponding fatigue-strength model. The second part\nof the paper presents a novel alternative modeling approach where a\nfatigue-strength model is specified and a corresponding fatigue-life model is\ninduced. We explain and illustrate the important advantages of this new\nmodeling approach."}, "http://arxiv.org/abs/2303.01186": {"title": "Discrete-time Competing-Risks Regression with or without Penalization", "link": "http://arxiv.org/abs/2303.01186", "description": "Many studies employ the analysis of time-to-event data that incorporates\ncompeting risks and right censoring. Most methods and software packages are\ngeared towards analyzing data that comes from a continuous failure time\ndistribution. However, failure-time data may sometimes be discrete either\nbecause time is inherently discrete or due to imprecise measurement. This paper\nintroduces a novel estimation procedure for discrete-time survival analysis\nwith competing events. The proposed approach offers two key advantages over\nexisting procedures: first, it expedites the estimation process for a large\nnumber of unique failure time points; second, it allows for straightforward\nintegration and application of widely used regularized regression and screening\nmethods. We illustrate the benefits of our proposed approach by conducting a\ncomprehensive simulation study. Additionally, we showcase the utility of our\nprocedure by estimating a survival model for the length of stay of patients\nhospitalized in the intensive care unit, considering three competing events:\ndischarge to home, transfer to another medical facility, and in-hospital death."}, "http://arxiv.org/abs/2306.11281": {"title": "Towards Characterizing Domain Counterfactuals For Invertible Latent Causal Models", "link": "http://arxiv.org/abs/2306.11281", "description": "Answering counterfactual queries has many important applications such as\nknowledge discovery and explainability, but is challenging when causal\nvariables are unobserved and we only see a projection onto an observation\nspace, for instance, image pixels. One approach is to recover the latent\nStructural Causal Model (SCM), but this typically needs unrealistic\nassumptions, such as linearity of the causal mechanisms. Another approach is to\nuse na\\\"ive ML approximations, such as generative models, to generate\ncounterfactual samples; however, these lack guarantees of accuracy. In this\nwork, we strive to strike a balance between practicality and theoretical\nguarantees by focusing on a specific type of causal query called domain\ncounterfactuals, which hypothesizes what a sample would have looked like if it\nhad been generated in a different domain (or environment). Concretely, by only\nassuming invertibility, sparse domain interventions and access to observational\ndata from different domains, we aim to improve domain counterfactual estimation\nboth theoretically and practically with less restrictive assumptions. We define\ndomain counterfactually equivalent models and prove necessary and sufficient\nproperties for equivalent models that provide a tight characterization of the\ndomain counterfactual equivalence classes. Building upon this result, we prove\nthat every equivalence class contains a model where all intervened variables\nare at the end when topologically sorted by the causal DAG. This surprising\nresult suggests that a model design that only allows intervention in the last\n$k$ latent variables may improve model estimation for counterfactuals. We then\ntest this model design on extensive simulated and image-based experiments which\nshow the sparse canonical model indeed improves counterfactual estimation over\nbaseline non-sparse models."}, "http://arxiv.org/abs/2309.10378": {"title": "Group Spike and Slab Variational Bayes", "link": "http://arxiv.org/abs/2309.10378", "description": "We introduce Group Spike-and-slab Variational Bayes (GSVB), a scalable method\nfor group sparse regression. A fast co-ordinate ascent variational inference\n(CAVI) algorithm is developed for several common model families including\nGaussian, Binomial and Poisson. Theoretical guarantees for our proposed\napproach are provided by deriving contraction rates for the variational\nposterior in grouped linear regression. Through extensive numerical studies, we\ndemonstrate that GSVB provides state-of-the-art performance, offering a\ncomputationally inexpensive substitute to MCMC, whilst performing comparably or\nbetter than existing MAP methods. Additionally, we analyze three real world\ndatasets wherein we highlight the practical utility of our method,\ndemonstrating that GSVB provides parsimonious models with excellent predictive\nperformance, variable selection and uncertainty quantification."}, "http://arxiv.org/abs/2309.12632": {"title": "Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?", "link": "http://arxiv.org/abs/2309.12632", "description": "Following the great success of various deep learning methods in image and\nobject classification, the biomedical image processing society is also\noverwhelmed with their applications to various automatic diagnosis cases.\nUnfortunately, most of the deep learning-based classification attempts in the\nliterature solely focus on the aim of extreme accuracy scores, without\nconsidering interpretability, or patient-wise separation of training and test\ndata. For example, most lung nodule classification papers using deep learning\nrandomly shuffle data and split it into training, validation, and test sets,\ncausing certain images from the CT scan of a person to be in the training set,\nwhile other images of the exact same person to be in the validation or testing\nimage sets. This can result in reporting misleading accuracy rates and the\nlearning of irrelevant features, ultimately reducing the real-life usability of\nthese models. When the deep neural networks trained on the traditional, unfair\ndata shuffling method are challenged with new patient images, it is observed\nthat the trained models perform poorly. In contrast, deep neural networks\ntrained with strict patient-level separation maintain their accuracy rates even\nwhen new patient images are tested. Heat-map visualizations of the activations\nof the deep neural networks trained with strict patient-level separation\nindicate a higher degree of focus on the relevant nodules. We argue that the\nresearch question posed in the title has a positive answer only if the deep\nneural networks are trained with images of patients that are strictly isolated\nfrom the validation and testing patient sets."}, "http://arxiv.org/abs/2311.09388": {"title": "Synthesis estimators for positivity violations with a continuous covariate", "link": "http://arxiv.org/abs/2311.09388", "description": "Research intended to estimate the effect of an action, like in randomized\ntrials, often do not have random samples of the intended target population.\nInstead, estimates can be transported to the desired target population. Methods\nfor transporting between populations are often premised on a positivity\nassumption, such that all relevant covariate patterns in one population are\nalso present in the other. However, eligibility criteria, particularly in the\ncase of trials, can result in violations of positivity. To address\nnonpositivity, a synthesis of statistical and mechanistic models was previously\nproposed in the context of violations by a single binary covariate. Here, we\nextend the synthesis approach for positivity violations with a continuous\ncovariate. For estimation, two novel augmented inverse probability weighting\nestimators are proposed, with one based on estimating the parameters of a\nmarginal structural model and the other based on estimating the conditional\naverage causal effect. Both estimators are compared to other common approaches\nto address nonpositivity via a simulation study. Finally, the competing\napproaches are illustrated with an example in the context of two-drug versus\none-drug antiretroviral therapy on CD4 T cell counts among women with HIV."}, "http://arxiv.org/abs/2311.09419": {"title": "Change-point Inference for High-dimensional Heteroscedastic Data", "link": "http://arxiv.org/abs/2311.09419", "description": "We propose a bootstrap-based test to detect a mean shift in a sequence of\nhigh-dimensional observations with unknown time-varying heteroscedasticity. The\nproposed test builds on the U-statistic based approach in Wang et al. (2022),\ntargets a dense alternative, and adopts a wild bootstrap procedure to generate\ncritical values. The bootstrap-based test is free of tuning parameters and is\ncapable of accommodating unconditional time varying heteroscedasticity in the\nhigh-dimensional observations, as demonstrated in our theory and simulations.\nTheoretically, we justify the bootstrap consistency by using the recently\nproposed unconditional approach in Bucher and Kojadinovic (2019). Extensions to\ntesting for multiple change-points and estimation using wild binary\nsegmentation are also presented. Numerical simulations demonstrate the\nrobustness of the proposed testing and estimation procedures with respect to\ndifferent kinds of time-varying heteroscedasticity."}, "http://arxiv.org/abs/2311.09423": {"title": "Orthogonal prediction of counterfactual outcomes", "link": "http://arxiv.org/abs/2311.09423", "description": "Orthogonal meta-learners, such as DR-learner, R-learner and IF-learner, are\nincreasingly used to estimate conditional average treatment effects. They\nimprove convergence rates relative to na\\\"{\\i}ve meta-learners (e.g., T-, S-\nand X-learner) through de-biasing procedures that involve applying standard\nlearners to specifically transformed outcome data. This leads them to disregard\nthe possibly constrained outcome space, which can be particularly problematic\nfor dichotomous outcomes: these typically get transformed to values that are no\nlonger constrained to the unit interval, making it difficult for standard\nlearners to guarantee predictions within the unit interval. To address this, we\nconstruct orthogonal meta-learners for the prediction of counterfactual\noutcomes which respect the outcome space. As such, the obtained i-learner or\nimputation-learner is more generally expected to outperform existing learners,\neven when the outcome is unconstrained, as we confirm empirically in simulation\nstudies and an analysis of critical care data. Our development also sheds\nbroader light onto the construction of orthogonal learners for other estimands."}, "http://arxiv.org/abs/2311.09446": {"title": "On simulation-based inference for implicitly defined models", "link": "http://arxiv.org/abs/2311.09446", "description": "In many applications, a stochastic system is studied using a model implicitly\ndefined via a simulator. We develop a simulation-based parameter inference\nmethod for implicitly defined models. Our method differs from traditional\nlikelihood-based inference in that it uses a metamodel for the distribution of\na log-likelihood estimator. The metamodel is built on a local asymptotic\nnormality (LAN) property satisfied by the simulation-based log-likelihood\nestimator under certain conditions. A method for hypothesis test is developed\nunder the metamodel. Our method can enable accurate parameter estimation and\nuncertainty quantification where other Monte Carlo methods for parameter\ninference become highly inefficient due to large Monte Carlo variance. We\ndemonstrate our method using numerical examples including a mechanistic model\nfor the population dynamics of infectious disease."}, "http://arxiv.org/abs/2311.09838": {"title": "Bayesian Inference of Reproduction Number from Epidemiological and Genetic Data Using Particle MCMC", "link": "http://arxiv.org/abs/2311.09838", "description": "Inference of the reproduction number through time is of vital importance\nduring an epidemic outbreak. Typically, epidemiologists tackle this using\nobserved prevalence or incidence data. However, prevalence and incidence data\nalone is often noisy or partial. Models can also have identifiability issues\nwith determining whether a large amount of a small epidemic or a small amount\nof a large epidemic has been observed. Sequencing data however is becoming more\nabundant, so approaches which can incorporate genetic data are an active area\nof research. We propose using particle MCMC methods to infer the time-varying\nreproduction number from a combination of prevalence data reported at a set of\ndiscrete times and a dated phylogeny reconstructed from sequences. We validate\nour approach on simulated epidemics with a variety of scenarios. We then apply\nthe method to a real data set of HIV-1 in North Carolina, USA, between 1957 and\n2019."}, "http://arxiv.org/abs/2311.09875": {"title": "Unbiased and Multilevel Methods for a Class of Diffusions Partially Observed via Marked Point Processes", "link": "http://arxiv.org/abs/2311.09875", "description": "In this article we consider the filtering problem associated to partially\nobserved diffusions, with observations following a marked point process. In the\nmodel, the data form a point process with observation times that have its\nintensity driven by a diffusion, with the associated marks also depending upon\nthe diffusion process. We assume that one must resort to time-discretizing the\ndiffusion process and develop particle and multilevel particle filters to\nrecursively approximate the filter. In particular, we prove that our multilevel\nparticle filter can achieve a mean square error (MSE) of\n$\\mathcal{O}(\\epsilon^2)$ ($\\epsilon&gt;0$ and arbitrary) with a cost of\n$\\mathcal{O}(\\epsilon^{-2.5})$ versus using a particle filter which has a cost\nof $\\mathcal{O}(\\epsilon^{-3})$ to achieve the same MSE. We then show how this\nmethodology can be extended to give unbiased (that is with no\ntime-discretization error) estimators of the filter, which are proved to have\nfinite variance and with high-probability have finite cost. Finally, we extend\nour methodology to the problem of online static-parameter estimation."}, "http://arxiv.org/abs/2311.09935": {"title": "Semi-parametric Benchmark Dose Analysis with Monotone Additive Models", "link": "http://arxiv.org/abs/2311.09935", "description": "Benchmark dose analysis aims to estimate the level of exposure to a toxin\nthat results in a clinically-significant adverse outcome and quantifies\nuncertainty using the lower limit of a confidence interval for this level. We\ndevelop a novel framework for benchmark dose analysis based on monotone\nadditive dose-response models. We first introduce a flexible approach for\nfitting monotone additive models via penalized B-splines and\nLaplace-approximate marginal likelihood. A reflective Newton method is then\ndeveloped that employs de Boor's algorithm for computing splines and their\nderivatives for efficient estimation of the benchmark dose. Finally, we develop\nand assess three approaches for calculating benchmark dose lower limits: a\nnaive one based on asymptotic normality of the estimator, one based on an\napproximate pivot, and one using a Bayesian parametric bootstrap. The latter\napproaches improve upon the naive method in terms of accuracy and are\nguaranteed to return a positive lower limit; the approach based on an\napproximate pivot is typically an order of magnitude faster than the bootstrap,\nalthough they are both practically feasible to compute. We apply the new\nmethods to make inferences about the level of prenatal alcohol exposure\nassociated with clinically significant cognitive defects in children using data\nfrom an NIH-funded longitudinal study. Software to reproduce the results in\nthis paper is available at https://github.com/awstringer1/bmd-paper-code."}, "http://arxiv.org/abs/2311.09961": {"title": "Scan statistics for the detection of anomalies in M-dependent random fields with applications to image data", "link": "http://arxiv.org/abs/2311.09961", "description": "Anomaly detection in random fields is an important problem in many\napplications including the detection of cancerous cells in medicine, obstacles\nin autonomous driving and cracks in the construction material of buildings.\nSuch anomalies are often visible as areas with different expected values\ncompared to the background noise. Scan statistics based on local means have the\npotential to detect such local anomalies by enhancing relevant features. We\nderive limit theorems for a general class of such statistics over M-dependent\nrandom fields of arbitrary but fixed dimension. By allowing for a variety of\ncombinations and contrasts of sample means over differently-shaped local\nwindows, this yields a flexible class of scan statistics that can be tailored\nto the particular application of interest. The latter is demonstrated for crack\ndetection in 2D-images of different types of concrete. Together with a\nsimulation study this indicates the potential of the proposed methodology for\nthe detection of anomalies in a variety of situations."}, "http://arxiv.org/abs/2311.09989": {"title": "Xputer: Bridging Data Gaps with NMF, XGBoost, and a Streamlined GUI Experience", "link": "http://arxiv.org/abs/2311.09989", "description": "The rapid proliferation of data across diverse fields has accentuated the\nimportance of accurate imputation for missing values. This task is crucial for\nensuring data integrity and deriving meaningful insights. In response to this\nchallenge, we present Xputer, a novel imputation tool that adeptly integrates\nNon-negative Matrix Factorization (NMF) with the predictive strengths of\nXGBoost. One of Xputer's standout features is its versatility: it supports zero\nimputation, enables hyperparameter optimization through Optuna, and allows\nusers to define the number of iterations. For enhanced user experience and\naccessibility, we have equipped Xputer with an intuitive Graphical User\nInterface (GUI) ensuring ease of handling, even for those less familiar with\ncomputational tools. In performance benchmarks, Xputer not only rivals the\ncomputational speed of established tools such as IterativeImputer but also\noften outperforms them in terms of imputation accuracy. Furthermore, Xputer\nautonomously handles a diverse spectrum of data types, including categorical,\ncontinuous, and Boolean, eliminating the need for prior preprocessing. Given\nits blend of performance, flexibility, and user-friendly design, Xputer emerges\nas a state-of-the-art solution in the realm of data imputation."}, "http://arxiv.org/abs/2311.10076": {"title": "A decorrelation method for general regression adjustment in randomized experiments", "link": "http://arxiv.org/abs/2311.10076", "description": "We study regression adjustment with general function class approximations for\nestimating the average treatment effect in the design-based setting. Standard\nregression adjustment involves bias due to sample re-use, and this bias leads\nto behavior that is sub-optimal in the sample size, and/or imposes restrictive\nassumptions. Our main contribution is to introduce a novel decorrelation-based\napproach that circumvents these issues. We prove guarantees, both asymptotic\nand non-asymptotic, relative to the oracle functions that are targeted by a\ngiven regression adjustment procedure. We illustrate our method by applying it\nto various high-dimensional and non-parametric problems, exhibiting improved\nsample complexity and weakened assumptions relative to known approaches."}, "http://arxiv.org/abs/2108.09431": {"title": "Equivariant Variance Estimation for Multiple Change-point Model", "link": "http://arxiv.org/abs/2108.09431", "description": "The variance of noise plays an important role in many change-point detection\nprocedures and the associated inferences. Most commonly used variance\nestimators require strong assumptions on the true mean structure or normality\nof the error distribution, which may not hold in applications. More\nimportantly, the qualities of these estimators have not been discussed\nsystematically in the literature. In this paper, we introduce a framework of\nequivariant variance estimation for multiple change-point models. In\nparticular, we characterize the set of all equivariant unbiased quadratic\nvariance estimators for a family of change-point model classes, and develop a\nminimax theory for such estimators."}, "http://arxiv.org/abs/2210.07987": {"title": "Bayesian Learning via Q-Exponential Process", "link": "http://arxiv.org/abs/2210.07987", "description": "Regularization is one of the most fundamental topics in optimization,\nstatistics and machine learning. To get sparsity in estimating a parameter\n$u\\in\\mathbb{R}^d$, an $\\ell_q$ penalty term, $\\Vert u\\Vert_q$, is usually\nadded to the objective function. What is the probabilistic distribution\ncorresponding to such $\\ell_q$ penalty? What is the correct stochastic process\ncorresponding to $\\Vert u\\Vert_q$ when we model functions $u\\in L^q$? This is\nimportant for statistically modeling large dimensional objects, e.g. images,\nwith penalty to preserve certainty properties, e.g. edges in the image. In this\nwork, we generalize the $q$-exponential distribution (with density proportional\nto) $\\exp{(- \\frac{1}{2}|u|^q)}$ to a stochastic process named $Q$-exponential\n(Q-EP) process that corresponds to the $L_q$ regularization of functions. The\nkey step is to specify consistent multivariate $q$-exponential distributions by\nchoosing from a large family of elliptic contour distributions. The work is\nclosely related to Besov process which is usually defined by the expanded\nseries. Q-EP can be regarded as a definition of Besov process with explicit\nprobabilistic formulation and direct control on the correlation length. From\nthe Bayesian perspective, Q-EP provides a flexible prior on functions with\nsharper penalty ($q&lt;2$) than the commonly used Gaussian process (GP). We\ncompare GP, Besov and Q-EP in modeling functional data, reconstructing images,\nand solving inverse problems and demonstrate the advantage of our proposed\nmethodology."}, "http://arxiv.org/abs/2212.06080": {"title": "Logs with zeros? Some problems and solutions", "link": "http://arxiv.org/abs/2212.06080", "description": "When studying an outcome $Y$ that is weakly-positive but can equal zero (e.g.\nearnings), researchers frequently estimate an average treatment effect (ATE)\nfor a \"log-like\" transformation that behaves like $\\log(Y)$ for large $Y$ but\nis defined at zero (e.g. $\\log(1+Y)$, $\\mathrm{arcsinh}(Y)$). We argue that\nATEs for log-like transformations should not be interpreted as approximating\npercentage effects, since unlike a percentage, they depend on the units of the\noutcome. In fact, we show that if the treatment affects the extensive margin,\none can obtain a treatment effect of any magnitude simply by re-scaling the\nunits of $Y$ before taking the log-like transformation. This arbitrary\nunit-dependence arises because an individual-level percentage effect is not\nwell-defined for individuals whose outcome changes from zero to non-zero when\nreceiving treatment, and the units of the outcome implicitly determine how much\nweight the ATE for a log-like transformation places on the extensive margin. We\nfurther establish a trilemma: when the outcome can equal zero, there is no\ntreatment effect parameter that is an average of individual-level treatment\neffects, unit-invariant, and point-identified. We discuss several alternative\napproaches that may be sensible in settings with an intensive and extensive\nmargin, including (i) expressing the ATE in levels as a percentage (e.g. using\nPoisson regression), (ii) explicitly calibrating the value placed on the\nintensive and extensive margins, and (iii) estimating separate effects for the\ntwo margins (e.g. using Lee bounds). We illustrate these approaches in three\nempirical applications."}, "http://arxiv.org/abs/2302.00354": {"title": "The Spatial Kernel Predictor based on Huge Observation Sets", "link": "http://arxiv.org/abs/2302.00354", "description": "Spatial prediction in an arbitrary location, based on a spatial set of\nobservations, is usually performed by Kriging, being the best linear unbiased\npredictor (BLUP) in a least-square sense. In order to predict a continuous\nsurface over a spatial domain a grid representation is most often used. Kriging\npredictions and prediction variances are computed in the nodes of a grid\ncovering the spatial domain, and the continuous surface is assessed from this\ngrid representation. A precise representation usually requires the number of\ngrid nodes to be considerably larger than the number of observations. For a\nGaussian random field model the Kriging predictor coinsides with the\nconditional expectation of the spatial variable given the observation set. An\nalternative expression for this conditional expectation provides a spatial\npredictor on functional form which does not rely on a spatial grid\ndiscretization. This functional predictor, called the Kernel predictor, is\nidentical to the asymptotic grid infill limit of the Kriging-based grid\nrepresentation, and the computational demand is primarily dependent on the\nnumber of observations - not the dimension of the spatial reference domain nor\nany grid discretization. We explore the potential of this Kernel predictor with\nassociated prediction variances. The predictor is valid for Gaussian random\nfields with any eligible spatial correlation function, and large computational\nsavings can be obtained by using a finite-range spatial correlation function.\nFor studies with a huge set of observations, localized predictors must be used,\nand the computational advantage relative to Kriging predictors can be very\nlarge. Moreover, model parameter inference based on a huge observation set can\nbe efficiently made. The methodology is demonstrated in a couple of examples."}, "http://arxiv.org/abs/2302.01861": {"title": "Covariance Matrix Estimation for High-Throughput Biomedical Data with Interconnected Communities", "link": "http://arxiv.org/abs/2302.01861", "description": "Estimating a covariance matrix is central to high-dimensional data analysis.\nEmpirical analyses of high-dimensional biomedical data, including genomics,\nproteomics, microbiome, and neuroimaging, among others, consistently reveal\nstrong modularity in the dependence patterns. In these analyses,\nintercorrelated high-dimensional biomedical features often form communities or\nmodules that can be interconnected with others. While the interconnected\ncommunity structure has been extensively studied in biomedical research (e.g.,\ngene co-expression networks), its potential to assist in the estimation of\ncovariance matrices remains largely unexplored. To address this gap, we propose\na procedure that leverages the commonly observed interconnected community\nstructure in high-dimensional biomedical data to estimate large covariance and\nprecision matrices. We derive the uniformly minimum variance unbiased\nestimators for covariance and precision matrices in closed forms and provide\ntheoretical results on their asymptotic properties. Our proposed method\nenhances the accuracy of covariance- and precision-matrix estimation and\ndemonstrates superior performance compared to the competing methods in both\nsimulations and real data analyses."}, "http://arxiv.org/abs/2303.16299": {"title": "Comparison of Methods that Combine Multiple Randomized Trials to Estimate Heterogeneous Treatment Effects", "link": "http://arxiv.org/abs/2303.16299", "description": "Individualized treatment decisions can improve health outcomes, but using\ndata to make these decisions in a reliable, precise, and generalizable way is\nchallenging with a single dataset. Leveraging multiple randomized controlled\ntrials allows for the combination of datasets with unconfounded treatment\nassignment to better estimate heterogeneous treatment effects. This paper\ndiscusses several non-parametric approaches for estimating heterogeneous\ntreatment effects using data from multiple trials. We extend single-study\nmethods to a scenario with multiple trials and explore their performance\nthrough a simulation study, with data generation scenarios that have differing\nlevels of cross-trial heterogeneity. The simulations demonstrate that methods\nthat directly allow for heterogeneity of the treatment effect across trials\nperform better than methods that do not, and that the choice of single-study\nmethod matters based on the functional form of the treatment effect. Finally,\nwe discuss which methods perform well in each setting and then apply them to\nfour randomized controlled trials to examine effect heterogeneity of treatments\nfor major depressive disorder."}, "http://arxiv.org/abs/2306.17043": {"title": "How trace plots help interpret meta-analysis results", "link": "http://arxiv.org/abs/2306.17043", "description": "The trace plot is seldom used in meta-analysis, yet it is a very informative\nplot. In this article we define and illustrate what the trace plot is, and\ndiscuss why it is important. The Bayesian version of the plot combines the\nposterior density of tau, the between-study standard deviation, and the\nshrunken estimates of the study effects as a function of tau. With a small or\nmoderate number of studies, tau is not estimated with much precision, and\nparameter estimates and shrunken study effect estimates can vary widely\ndepending on the correct value of tau. The trace plot allows visualization of\nthe sensitivity to tau along with a plot that shows which values of tau are\nplausible and which are implausible. A comparable frequentist or empirical\nBayes version provides similar results. The concepts are illustrated using\nexamples in meta-analysis and meta-regression; implementaton in R is\nfacilitated in a Bayesian or frequentist framework using the bayesmeta and\nmetafor packages, respectively."}, "http://arxiv.org/abs/2309.10978": {"title": "Scarcity-Mediated Spillover: An Overlooked Source of Bias in Pragmatic Clinical Trials", "link": "http://arxiv.org/abs/2309.10978", "description": "Pragmatic clinical trials evaluate the effectiveness of health interventions\nin real-world settings. Spillover arises in a pragmatic trial if the study\nintervention affects how scarce resources are allocated between patients in the\nintervention and comparison groups. This can harm patients assigned to the\ncontrol group and lead to overestimation of treatment effect. There is\ncurrently little recognition of this source of bias - which I term\n\"scarcity-mediated spillover\" - in the medical literature. In this article, I\nexamine what causes spillover and how it may have led trial investigators to\noverestimate the effect of patient navigation, AI-based physiological alarms,\nand elective induction of labor. I also suggest ways to detect\nscarcity-mediated spillover, design trials that avoid it, and modify clinical\ntrial guidelines to address this overlooked source of bias."}, "http://arxiv.org/abs/1910.12486": {"title": "Two-stage data segmentation permitting multiscale change points, heavy tails and dependence", "link": "http://arxiv.org/abs/1910.12486", "description": "The segmentation of a time series into piecewise stationary segments, a.k.a.\nmultiple change point analysis, is an important problem both in time series\nanalysis and signal processing. In the presence of multiscale change points\nwith both large jumps over short intervals and small changes over long\nstationary intervals, multiscale methods achieve good adaptivity in their\nlocalisation but at the same time, require the removal of false positives and\nduplicate estimators via a model selection step. In this paper, we propose a\nlocalised application of Schwarz information criterion which, as a generic\nmethodology, is applicable with any multiscale candidate generating procedure\nfulfilling mild assumptions. We establish the theoretical consistency of the\nproposed localised pruning method in estimating the number and locations of\nmultiple change points under general assumptions permitting heavy tails and\ndependence. Further, we show that combined with a MOSUM-based candidate\ngenerating procedure, it attains minimax optimality in terms of detection lower\nbound and localisation for i.i.d. sub-Gaussian errors. A careful comparison\nwith the existing methods by means of (a) theoretical properties such as\ngenerality, optimality and algorithmic complexity, (b) performance on simulated\ndatasets and run time, as well as (c) performance on real data applications,\nconfirm the overall competitiveness of the proposed methodology."}, "http://arxiv.org/abs/2101.04651": {"title": "Moving sum data segmentation for stochastics processes based on invariance", "link": "http://arxiv.org/abs/2101.04651", "description": "The segmentation of data into stationary stretches also known as multiple\nchange point problem is important for many applications in time series analysis\nas well as signal processing. Based on strong invariance principles, we analyse\ndata segmentation methodology using moving sum (MOSUM) statistics for a class\nof regime-switching multivariate processes where each switch results in a\nchange in the drift. In particular, this framework includes the data\nsegmentation of multivariate partial sum, integrated diffusion and renewal\nprocesses even if the distance between change points is sublinear. We study the\nasymptotic behaviour of the corresponding change point estimators, show\nconsistency and derive the corresponding localisation rates which are minimax\noptimal in a variety of situations including an unbounded number of changes in\nWiener processes with drift. Furthermore, we derive the limit distribution of\nthe change point estimators for local changes - a result that can in principle\nbe used to derive confidence intervals for the change points."}, "http://arxiv.org/abs/2207.07396": {"title": "Data Segmentation for Time Series Based on a General Moving Sum Approach", "link": "http://arxiv.org/abs/2207.07396", "description": "In this paper we propose new methodology for the data segmentation, also\nknown as multiple change point problem, in a general framework including\nclassic mean change scenarios, changes in linear regression but also changes in\nthe time series structure such as in the parameters of Poisson-autoregressive\ntime series. In particular, we derive a general theory based on estimating\nequations proving consistency for the number of change points as well as rates\nof convergence for the estimators of the locations of the change points. More\nprecisely, two different types of MOSUM (moving sum) statistics are considered:\nA MOSUM-Wald statistic based on differences of local estimators and a\nMOSUM-score statistic based on a global estimator. The latter is usually\ncomputationally less involved in particular in non-linear problems where no\nclosed form of the estimator is known such that numerical methods are required.\nFinally, we evaluate the methodology by means of simulated data as well as\nusing some geophysical well-log data."}, "http://arxiv.org/abs/2311.10263": {"title": "Stable Differentiable Causal Discovery", "link": "http://arxiv.org/abs/2311.10263", "description": "Inferring causal relationships as directed acyclic graphs (DAGs) is an\nimportant but challenging problem. Differentiable Causal Discovery (DCD) is a\npromising approach to this problem, framing the search as a continuous\noptimization. But existing DCD methods are numerically unstable, with poor\nperformance beyond tens of variables. In this paper, we propose Stable\nDifferentiable Causal Discovery (SDCD), a new method that improves previous DCD\nmethods in two ways: (1) It employs an alternative constraint for acyclicity;\nthis constraint is more stable, both theoretically and empirically, and fast to\ncompute. (2) It uses a training procedure tailored for sparse causal graphs,\nwhich are common in real-world scenarios. We first derive SDCD and prove its\nstability and correctness. We then evaluate it with both observational and\ninterventional data and on both small-scale and large-scale settings. We find\nthat SDCD outperforms existing methods in both convergence speed and accuracy\nand can scale to thousands of variables."}, "http://arxiv.org/abs/2311.10279": {"title": "Differentially private analysis of networks with covariates via a generalized $\\beta$-model", "link": "http://arxiv.org/abs/2311.10279", "description": "How to achieve the tradeoff between privacy and utility is one of fundamental\nproblems in private data analysis.In this paper, we give a rigourous\ndifferential privacy analysis of networks in the appearance of covariates via a\ngeneralized $\\beta$-model, which has an $n$-dimensional degree parameter\n$\\beta$ and a $p$-dimensional homophily parameter $\\gamma$.Under $(k_n,\n\\epsilon_n)$-edge differential privacy, we use the popular Laplace mechanism to\nrelease the network statistics.The method of moments is used to estimate the\nunknown model parameters. We establish the conditions guaranteeing consistency\nof the differentially private estimators $\\widehat{\\beta}$ and\n$\\widehat{\\gamma}$ as the number of nodes $n$ goes to infinity, which reveal an\ninteresting tradeoff between a privacy parameter and model parameters. The\nconsistency is shown by applying a two-stage Newton's method to obtain the\nupper bound of the error between $(\\widehat{\\beta},\\widehat{\\gamma})$ and its\ntrue value $(\\beta, \\gamma)$ in terms of the $\\ell_\\infty$ distance, which has\na convergence rate of rough order $1/n^{1/2}$ for $\\widehat{\\beta}$ and $1/n$\nfor $\\widehat{\\gamma}$, respectively. Further, we derive the asymptotic\nnormalities of $\\widehat{\\beta}$ and $\\widehat{\\gamma}$, whose asymptotic\nvariances are the same as those of the non-private estimators under some\nconditions. Our paper sheds light on how to explore asymptotic theory under\ndifferential privacy in a principled manner; these principled methods should be\napplicable to a class of network models with covariates beyond the generalized\n$\\beta$-model. Numerical studies and a real data analysis demonstrate our\ntheoretical findings."}, "http://arxiv.org/abs/2311.10282": {"title": "Joint clustering with alignment for temporal data in a one-point-per-trajectory setting", "link": "http://arxiv.org/abs/2311.10282", "description": "Temporal data, obtained in the setting where it is only possible to observe\none time point per trajectory, is widely used in different research fields, yet\nremains insufficiently addressed from the statistical point of view. Such data\noften contain observations of a large number of entities, in which case it is\nof interest to identify a small number of representative behavior types. In\nthis paper, we propose a new method performing clustering simultaneously with\nalignment of temporal objects inferred from these data, providing insight into\nthe relationships between the entities. A series of simulations confirm the\nability of the proposed approach to leverage multiple properties of the complex\ndata we target such as accessible uncertainties, correlations and a small\nnumber of time points. We illustrate it on real data encoding cellular response\nto a radiation treatment with high energy, supported with the results of an\nenrichment analysis."}, "http://arxiv.org/abs/2311.10489": {"title": "Handling Overlapping Asymmetric Datasets -- A Twice Penalized P-Spline Approach", "link": "http://arxiv.org/abs/2311.10489", "description": "Overlapping asymmetric datasets are common in data science and pose questions\nof how they can be incorporated together into a predictive analysis. In\nhealthcare datasets there is often a small amount of information that is\navailable for a larger number of patients such as an electronic health record,\nhowever a small number of patients may have had extensive further testing.\nCommon solutions such as missing imputation can often be unwise if the smaller\ncohort is significantly different in scale to the larger sample, therefore the\naim of this research is to develop a new method which can model the smaller\ncohort against a particular response, whilst considering the larger cohort\nalso. Motivated by non-parametric models, and specifically flexible smoothing\ntechniques via generalized additive models, we model a twice penalized P-Spline\napproximation method to firstly prevent over/under-fitting of the smaller\ncohort and secondly to consider the larger cohort. This second penalty is\ncreated through discrepancies in the marginal value of covariates that exist in\nboth the smaller and larger cohorts. Through data simulations, parameter\ntunings and model adaptations to consider a continuous and binary response, we\nfind our twice penalized approach offers an enhanced fit over a linear B-Spline\nand once penalized P-Spline approximation. Applying to a real-life dataset\nrelating to a person's risk of developing Non-Alcoholic Steatohepatitis, we see\nan improved model fit performance of over 65%. Areas for future work within\nthis space include adapting our method to not require dimensionality reduction\nand also consider parametric modelling methods. However, to our knowledge this\nis the first work to propose additional marginal penalties in a flexible\nregression of which we can report a vastly improved model fit that is able to\nconsider asymmetric datasets, without the need for missing data imputation."}, "http://arxiv.org/abs/2311.10638": {"title": "Concept-free Causal Disentanglement with Variational Graph Auto-Encoder", "link": "http://arxiv.org/abs/2311.10638", "description": "In disentangled representation learning, the goal is to achieve a compact\nrepresentation that consists of all interpretable generative factors in the\nobservational data. Learning disentangled representations for graphs becomes\nincreasingly important as graph data rapidly grows. Existing approaches often\nrely on Variational Auto-Encoder (VAE) or its causal structure learning-based\nrefinement, which suffer from sub-optimality in VAEs due to the independence\nfactor assumption and unavailability of concept labels, respectively. In this\npaper, we propose an unsupervised solution, dubbed concept-free causal\ndisentanglement, built on a theoretically provable tight upper bound\napproximating the optimal factor. This results in an SCM-like causal structure\nmodeling that directly learns concept structures from data. Based on this idea,\nwe propose Concept-free Causal VGAE (CCVGAE) by incorporating a novel causal\ndisentanglement layer into Variational Graph Auto-Encoder. Furthermore, we\nprove concept consistency under our concept-free causal disentanglement\nframework, hence employing it to enhance the meta-learning framework, called\nconcept-free causal Meta-Graph (CC-Meta-Graph). We conduct extensive\nexperiments to demonstrate the superiority of the proposed models: CCVGAE and\nCC-Meta-Graph, reaching up to $29\\%$ and $11\\%$ absolute improvements over\nbaselines in terms of AUC, respectively."}, "http://arxiv.org/abs/2009.07055": {"title": "Causal Inference of General Treatment Effects using Neural Networks with A Diverging Number of Confounders", "link": "http://arxiv.org/abs/2009.07055", "description": "Semiparametric efficient estimation of various multi-valued causal effects,\nincluding quantile treatment effects, is important in economic, biomedical, and\nother social sciences. Under the unconfoundedness condition, adjustment for\nconfounders requires estimating the nuisance functions relating outcome or\ntreatment to confounders nonparametrically. This paper considers a generalized\noptimization framework for efficient estimation of general treatment effects\nusing artificial neural networks (ANNs) to approximate the unknown nuisance\nfunction of growing-dimensional confounders. We establish a new approximation\nerror bound for the ANNs to the nuisance function belonging to a mixed\nsmoothness class without a known sparsity structure. We show that the ANNs can\nalleviate the \"curse of dimensionality\" under this circumstance. We establish\nthe root-$n$ consistency and asymptotic normality of the proposed general\ntreatment effects estimators, and apply a weighted bootstrap procedure for\nconducting inference. The proposed methods are illustrated via simulation\nstudies and a real data application."}, "http://arxiv.org/abs/2012.08371": {"title": "Limiting laws and consistent estimation criteria for fixed and diverging number of spiked eigenvalues", "link": "http://arxiv.org/abs/2012.08371", "description": "In this paper, we study limiting laws and consistent estimation criteria for\nthe extreme eigenvalues in a spiked covariance model of dimension $p$. Firstly,\nfor fixed $p$, we propose a generalized estimation criterion that can\nconsistently estimate, $k$, the number of spiked eigenvalues. Compared with the\nexisting literature, we show that consistency can be achieved under weaker\nconditions on the penalty term. Next, allowing both $p$ and $k$ to diverge, we\nderive limiting distributions of the spiked sample eigenvalues using random\nmatrix theory techniques. Notably, our results do not require the spiked\neigenvalues to be uniformly bounded from above or tending to infinity, as have\nbeen assumed in the existing literature. Based on the above derived results, we\nformulate a generalized estimation criterion and show that it can consistently\nestimate $k$, while $k$ can be fixed or grow at an order of $k=o(n^{1/3})$. We\nfurther show that the results in our work continue to hold under a general\npopulation distribution without assuming normality. The efficacy of the\nproposed estimation criteria is illustrated through comparative simulation\nstudies."}, "http://arxiv.org/abs/2104.06296": {"title": "Count Network Autoregression", "link": "http://arxiv.org/abs/2104.06296", "description": "We consider network autoregressive models for count data with a non-random\nneighborhood structure. The main methodological contribution is the development\nof conditions that guarantee stability and valid statistical inference for such\nmodels. We consider both cases of fixed and increasing network dimension and we\nshow that quasi-likelihood inference provides consistent and asymptotically\nnormally distributed estimators. The work is complemented by simulation results\nand a data example."}, "http://arxiv.org/abs/2110.09115": {"title": "Optimal designs for experiments for scalar-on-function linear models", "link": "http://arxiv.org/abs/2110.09115", "description": "The aim of this work is to extend the usual optimal experimental design\nparadigm to experiments where the settings of one or more factors are\nfunctions. Such factors are known as profile factors, or as dynamic factors.\nFor these new experiments, a design consists of combinations of functions for\neach run of the experiment. After briefly introducing the class of profile\nfactors, basis functions are described with primary focus given on the B-spline\nbasis system, due to its computational efficiency and useful properties. Basis\nfunction expansions are applied to a functional linear model consisting of\nprofile factors, reducing the problem to an optimisation of basis coefficients.\nThe methodology developed comprises special cases, including combinations of\nprofile and non-functional factors, interactions, and polynomial effects. The\nmethod is finally applied to an experimental design problem in a\nBiopharmaceutical study that is performed using the Ambr250 modular bioreactor."}, "http://arxiv.org/abs/2207.13071": {"title": "Missing Values Handling for Machine Learning Portfolios", "link": "http://arxiv.org/abs/2207.13071", "description": "We characterize the structure and origins of missingness for 159\ncross-sectional return predictors and study missing value handling for\nportfolios constructed using machine learning. Simply imputing with\ncross-sectional means performs well compared to rigorous\nexpectation-maximization methods. This stems from three facts about predictor\ndata: (1) missingness occurs in large blocks organized by time, (2)\ncross-sectional correlations are small, and (3) missingness tends to occur in\nblocks organized by the underlying data source. As a result, observed data\nprovide little information about missing data. Sophisticated imputations\nintroduce estimation noise that can lead to underperformance if machine\nlearning is not carefully applied."}, "http://arxiv.org/abs/2209.00105": {"title": "Personalized Biopsy Schedules Using an Interval-censored Cause-specific Joint Model", "link": "http://arxiv.org/abs/2209.00105", "description": "Active surveillance (AS), where biopsies are conducted to detect cancer\nprogression, has been acknowledged as an efficient way to reduce the\novertreatment of prostate cancer. Most AS cohorts use fixed biopsy schedules\nfor all patients. However, the ideal test frequency remains unknown, and the\nroutine use of such invasive tests burdens the patients. An emerging idea is to\ngenerate personalized biopsy schedules based on each patient's\nprogression-specific risk. To achieve that, we propose the interval-censored\ncause-specific joint model (ICJM), which models the impact of longitudinal\nbiomarkers on cancer progression while considering the competing event of early\ntreatment initiation. The underlying likelihood function incorporates the\ninterval-censoring of cancer progression, the competing risk of treatment, and\nthe uncertainty about whether cancer progression occurred since the last biopsy\nin patients that are right-censored or experience the competing event. The\nmodel can produce patient-specific risk profiles until a horizon time. If the\nrisk exceeds a certain threshold, a biopsy is conducted. The optimal threshold\ncan be chosen by balancing two indicators of the biopsy schedules: the expected\nnumber of biopsies and expected delay in detection of cancer progression. A\nsimulation study showed that our personalized schedules could considerably\nreduce the number of biopsies per patient by 34%-54% compared to the fixed\nschedules, though at the cost of a slightly longer detection delay."}, "http://arxiv.org/abs/2304.14110": {"title": "A Bayesian Spatio-Temporal Extension to Poisson Auto-Regression: Modeling the Disease Infection Rate of COVID-19 in England", "link": "http://arxiv.org/abs/2304.14110", "description": "The COVID-19 pandemic provided many modeling challenges to investigate the\nevolution of an epidemic process over areal units. A suitable encompassing\nmodel must describe the spatio-temporal variations of the disease infection\nrate of multiple areal processes while adjusting for local and global inputs.\nWe develop an extension to Poisson Auto-Regression that incorporates\nspatio-temporal dependence to characterize the local dynamics while borrowing\ninformation among adjacent areas. The specification includes up to two sets of\nspace-time random effects to capture the spatio-temporal dependence and a\nlinear predictor depending on an arbitrary set of covariates. The proposed\nmodel, adopted in a fully Bayesian framework and implemented through a novel\nsparse-matrix representation in Stan, provides a framework for evaluating local\npolicy changes over the whole spatial and temporal domain of the study. It has\nbeen validated through a substantial simulation study and applied to the weekly\nCOVID-19 cases observed in the English local authority districts between May\n2020 and March 2021. The model detects substantial spatial and temporal\nheterogeneity and allows a full evaluation of the impact of two alternative\nsets of covariates: the level of local restrictions in place and the value of\nthe Google Mobility Indices. The paper also formalizes various novel\nmodel-based investigation methods for assessing additional aspects of disease\nepidemiology."}, "http://arxiv.org/abs/2305.14131": {"title": "Temporally Causal Discovery Tests for Discrete Time Series and Neural Spike Trains", "link": "http://arxiv.org/abs/2305.14131", "description": "We consider the problem of detecting causal relationships between discrete\ntime series, in the presence of potential confounders. A hypothesis test is\nintroduced for identifying the temporally causal influence of $(x_n)$ on\n$(y_n)$, causally conditioned on a possibly confounding third time series\n$(z_n)$. Under natural Markovian modeling assumptions, it is shown that the\nnull hypothesis, corresponding to the absence of temporally causal influence,\nis equivalent to the underlying `causal conditional directed information rate'\nbeing equal to zero. The plug-in estimator for this functional is identified\nwith the log-likelihood ratio test statistic for the desired test. This\nstatistic is shown to be asymptotically normal under the alternative hypothesis\nand asymptotically $\\chi^2$ distributed under the null, facilitating the\ncomputation of $p$-values when used on empirical data. The effectiveness of the\nresulting hypothesis test is illustrated on simulated data, validating the\nunderlying theory. The test is also employed in the analysis of spike train\ndata recorded from neurons in the V4 and FEF brain regions of behaving animals\nduring a visual attention task. There, the test results are seen to identify\ninteresting and biologically relevant information."}, "http://arxiv.org/abs/2307.01748": {"title": "Monotone Cubic B-Splines with a Neural-Network Generator", "link": "http://arxiv.org/abs/2307.01748", "description": "We present a method for fitting monotone curves using cubic B-splines, which\nis equivalent to putting a monotonicity constraint on the coefficients. We\nexplore different ways of enforcing this constraint and analyze their\ntheoretical and empirical properties. We propose two algorithms for solving the\nspline fitting problem: one that uses standard optimization techniques and one\nthat trains a Multi-Layer Perceptrons (MLP) generator to approximate the\nsolutions under various settings and perturbations. The generator approach can\nspeed up the fitting process when we need to solve the problem repeatedly, such\nas when constructing confidence bands using bootstrap. We evaluate our method\nagainst several existing methods, some of which do not use the monotonicity\nconstraint, on some monotone curves with varying noise levels. We demonstrate\nthat our method outperforms the other methods, especially in high-noise\nscenarios. We also apply our method to analyze the polarization-hole phenomenon\nduring star formation in astrophysics. The source code is accessible at\n\\texttt{\\url{https://github.com/szcf-weiya/MonotoneSplines.jl}}."}, "http://arxiv.org/abs/2311.10738": {"title": "Approximation of supply curves", "link": "http://arxiv.org/abs/2311.10738", "description": "In this note, we illustrate the computation of the approximation of the\nsupply curves using a one-step basis. We derive the expression for the L2\napproximation and propose a procedure for the selection of nodes of the\napproximation. We illustrate the use of this approach with three large sets of\nbid curves from European electricity markets."}, "http://arxiv.org/abs/2311.10848": {"title": "Addressing Population Heterogeneity for HIV Incidence Estimation Based on Recency Test", "link": "http://arxiv.org/abs/2311.10848", "description": "Cross-sectional HIV incidence estimation leverages recency test results to\ndetermine the HIV incidence of a population of interest, where recency test\nuses biomarker profiles to infer whether an HIV-positive individual was\n\"recently\" infected. This approach possesses an obvious advantage over the\nconventional cohort follow-up method since it avoids longitudinal follow-up and\nrepeated HIV testing. In this manuscript, we consider the extension of\ncross-sectional incidence estimation to estimate the incidence of a different\ntarget population addressing potential population heterogeneity. We propose a\ngeneral framework that incorporates two settings: one with the target\npopulation that is a subset of the population with cross-sectional recency\ntesting data, e.g., leveraging recency testing data from screening in\nactive-arm trial design, and the other with an external target population. We\nalso propose a method to incorporate HIV subtype, a special covariate that\nmodifies the properties of recency test, into our framework. Through extensive\nsimulation studies and a data application, we demonstrate the excellent\nperformance of the proposed methods. We conclude with a discussion of\nsensitivity analysis and future work to improve our framework."}, "http://arxiv.org/abs/2311.10877": {"title": "Covariate adjustment in randomized experiments with missing outcomes and covariates", "link": "http://arxiv.org/abs/2311.10877", "description": "Covariate adjustment can improve precision in estimating treatment effects\nfrom randomized experiments. With fully observed data, regression adjustment\nand propensity score weighting are two asymptotically equivalent methods for\ncovariate adjustment in randomized experiments. We show that this equivalence\nbreaks down in the presence of missing outcomes, with regression adjustment no\nlonger ensuring efficiency gain when the true outcome model is not linear in\ncovariates. Propensity score weighting, in contrast, still guarantees\nefficiency over unadjusted analysis, and including more covariates in\nadjustment never harms asymptotic efficiency. Moreover, we establish the value\nof using partially observed covariates to secure additional efficiency. Based\non these findings, we recommend a simple double-weighted estimator for\ncovariate adjustment with incomplete outcomes and covariates: (i) impute all\nmissing covariates by zero, and use the union of the completed covariates and\ncorresponding missingness indicators to estimate the probability of treatment\nand the probability of having observed outcome for all units; (ii) estimate the\naverage treatment effect by the coefficient of the treatment from the\nleast-squares regression of the observed outcome on the treatment, where we\nweight each unit by the inverse of the product of these two estimated\nprobabilities."}, "http://arxiv.org/abs/2311.10900": {"title": "A powerful rank-based correction to multiple testing under positive dependency", "link": "http://arxiv.org/abs/2311.10900", "description": "We develop a novel multiple hypothesis testing correction with family-wise\nerror rate (FWER) control that efficiently exploits positive dependencies\nbetween potentially correlated statistical hypothesis tests. Our proposed\nalgorithm $\\texttt{max-rank}$ is conceptually straight-forward, relying on the\nuse of a $\\max$-operator in the rank domain of computed test statistics. We\ncompare our approach to the frequently employed Bonferroni correction,\ntheoretically and empirically demonstrating its superiority over Bonferroni in\nthe case of existing positive dependency, and its equivalence otherwise. Our\nadvantage over Bonferroni increases as the number of tests rises, and we\nmaintain high statistical power whilst ensuring FWER control. We specifically\nframe our algorithm in the context of parallel permutation testing, a scenario\nthat arises in our primary application of conformal prediction, a recently\npopularized approach for quantifying uncertainty in complex predictive\nsettings."}, "http://arxiv.org/abs/2311.11050": {"title": "Functional Neural Network Control Chart", "link": "http://arxiv.org/abs/2311.11050", "description": "In many Industry 4.0 data analytics applications, quality characteristic data\nacquired from manufacturing processes are better modeled as functions, often\nreferred to as profiles. In practice, there are situations where a scalar\nquality characteristic, referred to also as the response, is influenced by one\nor more variables in the form of functional data, referred to as functional\ncovariates. To adjust the monitoring of the scalar response by the effect of\nthis additional information, a new profile monitoring strategy is proposed on\nthe residuals obtained from the functional neural network, which is able to\nlearn a possibly nonlinear relationship between the scalar response and the\nfunctional covariates. An extensive Monte Carlo simulation study is performed\nto assess the performance of the proposed method with respect to other control\ncharts that appeared in the literature before. Finally, a case study in the\nrailway industry is presented with the aim of monitoring the heating,\nventilation and air conditioning systems installed onboard passenger trains."}, "http://arxiv.org/abs/2311.11054": {"title": "Modern extreme value statistics for Utopian extremes", "link": "http://arxiv.org/abs/2311.11054", "description": "Capturing the extremal behaviour of data often requires bespoke marginal and\ndependence models which are grounded in rigorous asymptotic theory, and hence\nprovide reliable extrapolation into the upper tails of the data-generating\ndistribution. We present a modern toolbox of four methodological frameworks,\nmotivated by modern extreme value theory, that can be used to accurately\nestimate extreme exceedance probabilities or the corresponding level in either\na univariate or multivariate setting. Our frameworks were used to facilitate\nthe winning contribution of Team Yalla to the data competition organised for\nthe 13th International Conference on Extreme Value Analysis (EVA2023). This\ncompetition comprised seven teams competing across four separate\nsub-challenges, with each requiring the modelling of data simulated from known,\nyet highly complex, statistical distributions, and extrapolation far beyond the\nrange of the available samples in order to predict probabilities of extreme\nevents. Data were constructed to be representative of real environmental data,\nsampled from the fantasy country of \"Utopia\"."}, "http://arxiv.org/abs/2311.11153": {"title": "Biarchetype analysis: simultaneous learning of observations and features based on extremes", "link": "http://arxiv.org/abs/2311.11153", "description": "A new exploratory technique called biarchetype analysis is defined. We extend\narchetype analysis to find the archetypes of both observations and features\nsimultaneously. The idea of this new unsupervised machine learning tool is to\nrepresent observations and features by instances of pure types (biarchetypes)\nthat can be easily interpreted as they are mixtures of observations and\nfeatures. Furthermore, the observations and features are expressed as mixtures\nof the biarchetypes, which also helps understand the structure of the data. We\npropose an algorithm to solve biarchetype analysis. We show that biarchetype\nanalysis offers advantages over biclustering, especially in terms of\ninterpretability. This is because byarchetypes are extreme instances as opposed\nto the centroids returned by biclustering, which favors human understanding.\nBiarchetype analysis is applied to several machine learning problems to\nillustrate its usefulness."}, "http://arxiv.org/abs/2311.11216": {"title": "Valid Randomization Tests in Inexactly Matched Observational Studies via Iterative Convex Programming", "link": "http://arxiv.org/abs/2311.11216", "description": "In causal inference, matching is one of the most widely used methods to mimic\na randomized experiment using observational (non-experimental) data. Ideally,\ntreated units are exactly matched with control units for the covariates so that\nthe treatments are as-if randomly assigned within each matched set, and valid\nrandomization tests for treatment effects can then be conducted as in a\nrandomized experiment. However, inexact matching typically exists, especially\nwhen there are continuous or many observed covariates or when unobserved\ncovariates exist. Previous matched observational studies routinely conducted\ndownstream randomization tests as if matching was exact, as long as the matched\ndatasets satisfied some prespecified balance criteria or passed some balance\ntests. Some recent studies showed that this routine practice could render a\nhighly inflated type-I error rate of randomization tests, especially when the\nsample size is large. To handle this problem, we propose an iterative convex\nprogramming framework for randomization tests with inexactly matched datasets.\nUnder some commonly used regularity conditions, we show that our approach can\nproduce valid randomization tests (i.e., robustly controlling the type-I error\nrate) for any inexactly matched datasets, even when unobserved covariates\nexist. Our framework allows the incorporation of flexible machine learning\nmodels to better extract information from covariate imbalance while robustly\ncontrolling the type-I error rate."}, "http://arxiv.org/abs/2311.11236": {"title": "Generalized Linear Models via the Lasso: To Scale or Not to Scale?", "link": "http://arxiv.org/abs/2311.11236", "description": "The Lasso regression is a popular regularization method for feature selection\nin statistics. Prior to computing the Lasso estimator in both linear and\ngeneralized linear models, it is common to conduct a preliminary rescaling of\nthe feature matrix to ensure that all the features are standardized. Without\nthis standardization, it is argued, the Lasso estimate will unfortunately\ndepend on the units used to measure the features. We propose a new type of\niterative rescaling of the features in the context of generalized linear\nmodels. Whilst existing Lasso algorithms perform a single scaling as a\npreprocessing step, the proposed rescaling is applied iteratively throughout\nthe Lasso computation until convergence. We provide numerical examples, with\nboth real and simulated data, illustrating that the proposed iterative\nrescaling can significantly improve the statistical performance of the Lasso\nestimator without incurring any significant additional computational cost."}, "http://arxiv.org/abs/2311.11256": {"title": "Bayesian Modeling of Incompatible Spatial Data: A Case Study Involving Post-Adrian Storm Forest Damage Assessment", "link": "http://arxiv.org/abs/2311.11256", "description": "Incompatible spatial data modeling is a pervasive challenge in remote sensing\ndata analysis that involves field data. Typical approaches to addressing this\nchallenge aggregate information to a coarser common scale, i.e., compatible\nresolutions. Such pre-processing aggregation to a common resolution simplifies\nanalysis, but potentially causes information loss and hence compromised\ninference and predictive performance. To incorporate finer information to\nenhance prediction performance, we develop a new Bayesian method aimed at\nimproving predictive accuracy and uncertainty quantification. The main\ncontribution of this work is an efficient algorithm that enables full Bayesian\ninference using finer resolution data while optimizing computational and\nstorage costs. The algorithm is developed and applied to a forest damage\nassessment for the 2018 Adrian storm in Carinthia, Austria, which uses field\ndata and high-resolution LiDAR measurements. Simulation studies demonstrate\nthat this approach substantially improves prediction accuracy and stability,\nproviding more reliable inference to support forest management decisions."}, "http://arxiv.org/abs/2311.11290": {"title": "Jeffreys-prior penalty for high-dimensional logistic regression: A conjecture about aggregate bias", "link": "http://arxiv.org/abs/2311.11290", "description": "Firth (1993, Biometrika) shows that the maximum Jeffreys' prior penalized\nlikelihood estimator in logistic regression has asymptotic bias decreasing with\nthe square of the number of observations when the number of parameters is\nfixed, which is an order faster than the typical rate from maximum likelihood.\nThe widespread use of that estimator in applied work is supported by the\nresults in Kosmidis and Firth (2021, Biometrika), who show that it takes finite\nvalues, even in cases where the maximum likelihood estimate does not exist.\nKosmidis and Firth (2021, Biometrika) also provide empirical evidence that the\nestimator has good bias properties in high-dimensional settings where the\nnumber of parameters grows asymptotically linearly but slower than the number\nof observations. We design and carry out a large-scale computer experiment\ncovering a wide range of such high-dimensional settings and produce strong\nempirical evidence for a simple rescaling of the maximum Jeffreys' prior\npenalized likelihood estimator that delivers high accuracy in signal recovery\nin the presence of an intercept parameter. The rescaled estimator is effective\neven in cases where estimates from maximum likelihood and other recently\nproposed corrective methods based on approximate message passing do not exist."}, "http://arxiv.org/abs/2311.11445": {"title": "Maximum likelihood inference for a class of discrete-time Markov-switching time series models with multiple delays", "link": "http://arxiv.org/abs/2311.11445", "description": "Autoregressive Markov switching (ARMS) time series models are used to\nrepresent real-world signals whose dynamics may change over time. They have\nfound application in many areas of the natural and social sciences, as well as\nin engineering. In general, inference in this kind of systems involves two\nproblems: (a) detecting the number of distinct dynamical models that the signal\nmay adopt and (b) estimating any unknown parameters in these models. In this\npaper, we introduce a class of ARMS time series models that includes many\nsystems resulting from the discretisation of stochastic delay differential\nequations (DDEs). Remarkably, this class includes cases in which the\ndiscretisation time grid is not necessarily aligned with the delays of the DDE,\nresulting in discrete-time ARMS models with real (non-integer) delays. We\ndescribe methods for the maximum likelihood detection of the number of\ndynamical modes and the estimation of unknown parameters (including the\npossibly non-integer delays) and illustrate their application with an ARMS\nmodel of El Ni\\~no--southern oscillation (ENSO) phenomenon."}, "http://arxiv.org/abs/2311.11487": {"title": "Modeling Insurance Claims using Bayesian Nonparametric Regression", "link": "http://arxiv.org/abs/2311.11487", "description": "The prediction of future insurance claims based on observed risk factors, or\ncovariates, help the actuary set insurance premiums. Typically, actuaries use\nparametric regression models to predict claims based on the covariate\ninformation. Such models assume the same functional form tying the response to\nthe covariates for each data point. These models are not flexible enough and\ncan fail to accurately capture at the individual level, the relationship\nbetween the covariates and the claims frequency and severity, which are often\nmultimodal, highly skewed, and heavy-tailed. In this article, we explore the\nuse of Bayesian nonparametric (BNP) regression models to predict claims\nfrequency and severity based on covariates. In particular, we model claims\nfrequency as a mixture of Poisson regression, and the logarithm of claims\nseverity as a mixture of normal regression. We use the Dirichlet process (DP)\nand Pitman-Yor process (PY) as a prior for the mixing distribution over the\nregression parameters. Unlike parametric regression, such models allow each\ndata point to have its individual parameters, making them highly flexible,\nresulting in improved prediction accuracy. We describe model fitting using MCMC\nand illustrate their applicability using French motor insurance claims data."}, "http://arxiv.org/abs/2311.11522": {"title": "A Bayesian two-step multiple imputation approach based on mixed models for the missing in EMA data", "link": "http://arxiv.org/abs/2311.11522", "description": "Ecological Momentary Assessments (EMA) capture real-time thoughts and\nbehaviors in natural settings, producing rich longitudinal data for statistical\nand physiological analyses. However, the robustness of these analyses can be\ncompromised by the large amount of missing in EMA data sets. To address this,\nmultiple imputation, a method that replaces missing values with several\nplausible alternatives, has become increasingly popular. In this paper, we\nintroduce a two-step Bayesian multiple imputation framework which leverages the\nconfiguration of mixed models. We adopt the Random Intercept Linear Mixed\nmodel, the Mixed-effect Location Scale model which accounts for subject\nvariance influenced by covariates and random effects, and the Shared Parameter\nLocation Scale Mixed Effect model which links the missing data to the response\nvariable through a random intercept logistic model, to complete the posterior\ndistribution within the framework. In the simulation study and an application\non data from a study on caregivers of dementia patients, we further adapt this\ntwo-step Bayesian multiple imputation strategy to handle simultaneous missing\nvariables in EMA data sets and compare the effectiveness of multiple\nimputations across different mixed models. The analyses highlight the\nadvantages of multiple imputations over single imputations. Furthermore, we\npropose two pivotal considerations in selecting the optimal mixed model for the\ntwo-step imputation: the influence of covariates as well as random effects on\nthe within-variance, and the nature of missing data in relation to the response\nvariable."}, "http://arxiv.org/abs/2311.11543": {"title": "A Comparison of Parameter Estimation Methods for Shared Frailty Models", "link": "http://arxiv.org/abs/2311.11543", "description": "This paper compares six different parameter estimation methods for shared\nfrailty models via a series of simulation studies. A shared frailty model is a\nsurvival model that incorporates a random effect term, where the frailties are\ncommon or shared among individuals within specific groups. Several parameter\nestimation methods are available for fitting shared frailty models, such as\npenalized partial likelihood (PPL), expectation-maximization (EM), pseudo full\nlikelihood (PFL), hierarchical likelihood (HL), maximum marginal likelihood\n(MML), and maximization penalized likelihood (MPL) algorithms. These estimation\nmethods are implemented in various R packages, providing researchers with\nvarious options for analyzing clustered survival data using shared frailty\nmodels. However, there is a limited amount of research comparing the\nperformance of these parameter estimation methods for fitting shared frailty\nmodels. Consequently, it can be challenging for users to determine the most\nappropriate method for analyzing clustered survival data. To address this gap,\nthis paper aims to conduct a series of simulation studies to compare the\nperformance of different parameter estimation methods implemented in R\npackages. We will evaluate several key aspects, including parameter estimation,\nbias and variance of the parameter estimates, rate of convergence, and\ncomputational time required by each package. Through this systematic\nevaluation, our goal is to provide a comprehensive understanding of the\nadvantages and limitations associated with each estimation method."}, "http://arxiv.org/abs/2311.11563": {"title": "Time-varying effect in the competing risks based on restricted mean time lost", "link": "http://arxiv.org/abs/2311.11563", "description": "Patients with breast cancer tend to die from other diseases, so for studies\nthat focus on breast cancer, a competing risks model is more appropriate.\nConsidering subdistribution hazard ratio, which is used often, limited to model\nassumptions and clinical interpretation, we aimed to quantify the effects of\nprognostic factors by an absolute indicator, the difference in restricted mean\ntime lost (RMTL), which is more intuitive. Additionally, prognostic factors may\nhave dynamic effects (time-varying effects) in long-term follow-up. However,\nexisting competing risks regression models only provide a static view of\ncovariate effects, leading to a distorted assessment of the prognostic factor.\nTo address this issue, we proposed a dynamic effect RMTL regression that can\nexplore the between-group cumulative difference in mean life lost over a period\nof time and obtain the real-time effect by the speed of accumulation, as well\nas personalized predictions on a time scale. Through Monte Carlo simulation, we\nvalidated the dynamic effects estimated by the proposed regression having low\nbias and a coverage rate of around 95%. Applying this model to an elderly\nearly-stage breast cancer cohort, we found that most factors had different\npatterns of dynamic effects, revealing meaningful physiological mechanisms\nunderlying diseases. Moreover, from the perspective of prediction, the mean\nC-index in external validation reached 0.78. Dynamic effect RMTL regression can\nanalyze both dynamic cumulative effects and real-time effects of covariates,\nproviding a more comprehensive prognosis and better prediction when competing\nrisks exist."}, "http://arxiv.org/abs/2311.11575": {"title": "Testing multivariate normality by testing independence", "link": "http://arxiv.org/abs/2311.11575", "description": "We propose a simple multivariate normality test based on Kac-Bernstein's\ncharacterization, which can be conducted by utilising existing statistical\nindependence tests for sums and differences of data samples. We also perform\nits empirical investigation, which reveals that for high-dimensional data, the\nproposed approach may be more efficient than the alternative ones. The\naccompanying code repository is provided at \\url{https://shorturl.at/rtuy5}."}, "http://arxiv.org/abs/2311.11637": {"title": "Modeling economies of scope in joint production: Convex regression of input distance function", "link": "http://arxiv.org/abs/2311.11637", "description": "Modeling of joint production has proved a vexing problem. This paper develops\na radial convex nonparametric least squares (CNLS) approach to estimate the\ninput distance function with multiple outputs. We document the correct input\ndistance function transformation and prove that the necessary orthogonality\nconditions can be satisfied in radial CNLS. A Monte Carlo study is performed to\ncompare the finite sample performance of radial CNLS and other deterministic\nand stochastic frontier approaches in terms of the input distance function\nestimation. We apply our novel approach to the Finnish electricity distribution\nnetwork regulation and empirically confirm that the input isoquants become more\ncurved. In addition, we introduce the weight restriction to radial CNLS to\nmitigate the potential overfitting and increase the out-of-sample performance\nin energy regulation."}, "http://arxiv.org/abs/2311.11657": {"title": "Minimax Two-Stage Gradient Boosting for Parameter Estimation", "link": "http://arxiv.org/abs/2311.11657", "description": "Parameter estimation is an important sub-field in statistics and system\nidentification. Various methods for parameter estimation have been proposed in\nthe literature, among which the Two-Stage (TS) approach is particularly\npromising, due to its ease of implementation and reliable estimates. Among the\ndifferent statistical frameworks used to derive TS estimators, the min-max\nframework is attractive due to its mild dependence on prior knowledge about the\nparameters to be estimated. However, the existing implementation of the minimax\nTS approach has currently limited applicability, due to its heavy computational\nload. In this paper, we overcome this difficulty by using a gradient boosting\nmachine (GBM) in the second stage of TS approach. We call the resulting\nalgorithm the Two-Stage Gradient Boosting Machine (TSGBM) estimator. Finally,\nwe test our proposed TSGBM estimator on several numerical examples including\nmodels of dynamical systems."}, "http://arxiv.org/abs/2311.11765": {"title": "Making accurate and interpretable treatment decisions for binary outcomes", "link": "http://arxiv.org/abs/2311.11765", "description": "Optimal treatment rules can improve health outcomes on average by assigning a\ntreatment associated with the most desirable outcome to each individual. Due to\nan unknown data generation mechanism, it is appealing to use flexible models to\nestimate these rules. However, such models often lead to complex and\nuninterpretable rules. In this article, we introduce an approach aimed at\nestimating optimal treatment rules that have higher accuracy, higher value, and\nlower loss from the same simple model family. We use a flexible model to\nestimate the optimal treatment rules and a simple model to derive interpretable\ntreatment rules. We provide an extensible definition of interpretability and\npresent a method that - given a class of simple models - can be used to select\na preferred model. We conduct a simulation study to evaluate the performance of\nour approach compared to treatment rules obtained by fitting the same simple\nmodel directly to observed data. The results show that our approach has lower\naverage loss, higher average outcome, and greater power in identifying\nindividuals who can benefit from the treatment. We apply our approach to derive\ntreatment rules of adjuvant chemotherapy in colon cancer patients using cancer\nregistry data. The results show that our approach has the potential to improve\ntreatment decisions."}, "http://arxiv.org/abs/2311.11852": {"title": "Statistical Prediction of Peaks Over a Threshold", "link": "http://arxiv.org/abs/2311.11852", "description": "In many applied fields it is desired to make predictions with the aim of\nassessing the plausibility of more severe events than those already recorded to\nsafeguard against calamities that have not yet occurred. This problem can be\nanalysed using extreme value theory. We consider the popular peaks over a\nthreshold method and show that the generalised Pareto approximation of the true\npredictive densities of both a future unobservable excess or peak random\nvariable can be very accurate. We propose both a frequentist and a Bayesian\napproach for the estimation of such predictive densities. We show the\nasymptotic accuracy of the corresponding estimators and, more importantly,\nprove that the resulting predictive inference is asymptotically reliable. We\nshow the utility of the proposed predictive tools analysing extreme\ntemperatures in Milan in Italy."}, "http://arxiv.org/abs/2311.11922": {"title": "Evaluating the Surrogate Index as a Decision-Making Tool Using 200 A/B Tests at Netflix", "link": "http://arxiv.org/abs/2311.11922", "description": "Surrogate index approaches have recently become a popular method of\nestimating longer-term impact from shorter-term outcomes. In this paper, we\nleverage 1098 test arms from 200 A/B tests at Netflix to empirically\ninvestigate to what degree would decisions made using a surrogate index\nutilizing 14 days of data would align with those made using direct measurement\nof day 63 treatment effects. Focusing specifically on linear \"auto-surrogate\"\nmodels that utilize the shorter-term observations of the long-term outcome of\ninterest, we find that the statistical inferences that we would draw from using\nthe surrogate index are ~95% consistent with those from directly measuring the\nlong-term treatment effect. Moreover, when we restrict ourselves to the set of\ntests that would be \"launched\" (i.e. positive and statistically significant)\nbased on the 63-day directly measured treatment effects, we find that relying\ninstead on the surrogate index achieves 79% and 65% recall."}, "http://arxiv.org/abs/2311.12016": {"title": "Bayesian Semiparametric Estimation of Heterogeneous Effects in Matched Case-Control Studies with an Application to Alzheimer's Disease and Heat", "link": "http://arxiv.org/abs/2311.12016", "description": "Epidemiological approaches for examining human health responses to\nenvironmental exposures in observational studies often control for confounding\nby implementing clever matching schemes and using statistical methods based on\nconditional likelihood. Nonparametric regression models have surged in\npopularity in recent years as a tool for estimating individual-level\nheterogeneous effects, which provide a more detailed picture of the\nexposure-response relationship but can also be aggregated to obtain improved\nmarginal estimates at the population level. In this work we incorporate\nBayesian additive regression trees (BART) into the conditional logistic\nregression model to identify heterogeneous effects of environmental exposures\nin a case-crossover design. Conditional logistic BART (CL-BART) utilizes\nreversible jump Markov chain Monte Carlo to bypass the conditional conjugacy\nrequirement of the original BART algorithm. Our work is motivated by the\ngrowing interest in identifying subpopulations more vulnerable to environmental\nexposures. We apply CL-BART to a study of the impact of heatwaves on people\nwith Alzheimer's Disease in California and effect modification by other chronic\nconditions. Through this application, we also describe strategies to examine\nheterogeneous odds ratios through variable importance, partial dependence, and\nlower-dimensional summaries. CL-BART is available in the clbart R package."}, "http://arxiv.org/abs/2311.12020": {"title": "A Heterogeneous Spatial Model for Soil Carbon Mapping of the Contiguous United States Using VNIR Spectra", "link": "http://arxiv.org/abs/2311.12020", "description": "The Rapid Carbon Assessment, conducted by the U.S. Department of Agriculture,\nwas implemented in order to obtain a representative sample of soil organic\ncarbon across the contiguous United States. In conjunction with a statistical\nmodel, the dataset allows for mapping of soil carbon prediction across the\nU.S., however there are two primary challenges to such an effort. First, there\nexists a large degree of heterogeneity in the data, whereby both the first and\nsecond moments of the data generating process seem to vary both spatially and\nfor different land-use categories. Second, the majority of the sampled\nlocations do not actually have lab measured values for soil organic carbon.\nRather, visible and near-infrared (VNIR) spectra were measured at most\nlocations, which act as a proxy to help predict carbon content. Thus, we\ndevelop a heterogeneous model to analyze this data that allows both the mean\nand the variance to vary as a function of space as well as land-use category,\nwhile incorporating VNIR spectra as covariates. After a cross-validation study\nthat establishes the effectiveness of the model, we construct a complete map of\nsoil organic carbon for the contiguous U.S. along with uncertainty\nquantification."}, "http://arxiv.org/abs/1909.10678": {"title": "A Bayesian Approach to Directed Acyclic Graphs with a Candidate Graph", "link": "http://arxiv.org/abs/1909.10678", "description": "Directed acyclic graphs represent the dependence structure among variables.\nWhen learning these graphs from data, different amounts of information may be\navailable for different edges. Although many methods have been developed to\nlearn the topology of these graphs, most of them do not provide a measure of\nuncertainty in the inference. We propose a Bayesian method, baycn (BAYesian\nCausal Network), to estimate the posterior probability of three states for each\nedge: present with one direction ($X \\rightarrow Y$), present with the opposite\ndirection ($X \\leftarrow Y$), and absent. Unlike existing Bayesian methods, our\nmethod requires that the prior probabilities of these states be specified, and\ntherefore provides a benchmark for interpreting the posterior probabilities. We\ndevelop a fast Metropolis-Hastings Markov chain Monte Carlo algorithm for the\ninference. Our algorithm takes as input the edges of a candidate graph, which\nmay be the output of another graph inference method and may contain false\nedges. In simulation studies our method achieves high accuracy with small\nvariation across different scenarios and is comparable or better than existing\nBayesian methods. We apply baycn to genomic data to distinguish the direct and\nindirect targets of genetic variants."}, "http://arxiv.org/abs/2007.05748": {"title": "Probability Models in Statistical Data Analysis: Uses, Interpretations, Frequentism-As-Model", "link": "http://arxiv.org/abs/2007.05748", "description": "Note: Published now as a chapter in \"Handbook of the History and Philosophy\nof Mathematical Practice\" (Springer Nature, editor B. Sriraman,\nhttps://doi.org/10.1007/978-3-030-19071-2_105-1).\n\nThe application of mathematical probability theory in statistics is quite\ncontroversial. Controversies regard both the interpretation of probability, and\napproaches to statistical inference. After having given an overview of the main\napproaches, I will propose a re-interpretation of frequentist probability. Most\nstatisticians are aware that probability models interpreted in a frequentist\nmanner are not really true in objective reality, but only idealisations. I\nargue that this is often ignored when actually applying frequentist methods and\ninterpreting the results, and that keeping up the awareness for the essential\ndifference between reality and models can lead to a more appropriate use and\ninterpretation of frequentist models and methods, called\n\"frequentism-as-model\". This is elaborated showing connections to existing\nwork, appreciating the special role of independently and identically\ndistributed observations and subject matter knowledge, giving an account of how\nand under what conditions models that are not true can be useful, giving\ndetailed interpretations of tests and confidence intervals, confronting their\nimplicit compatibility logic with the inverse probability logic of Bayesian\ninference, re-interpreting the role of model assumptions, appreciating\nrobustness, and the role of \"interpretative equivalence\" of models. Epistemic\nprobability shares the issue that its models are only idealisations, and an\nanalogous \"epistemic-probability-as-model\" can also be developed."}, "http://arxiv.org/abs/2008.09434": {"title": "Correcting a Nonparametric Two-sample Graph Hypothesis Test for Graphs with Different Numbers of Vertices with Applications to Connectomics", "link": "http://arxiv.org/abs/2008.09434", "description": "Random graphs are statistical models that have many applications, ranging\nfrom neuroscience to social network analysis. Of particular interest in some\napplications is the problem of testing two random graphs for equality of\ngenerating distributions. Tang et al. (2017) propose a test for this setting.\nThis test consists of embedding the graph into a low-dimensional space via the\nadjacency spectral embedding (ASE) and subsequently using a kernel two-sample\ntest based on the maximum mean discrepancy. However, if the two graphs being\ncompared have an unequal number of vertices, the test of Tang et al. (2017) may\nnot be valid. We demonstrate the intuition behind this invalidity and propose a\ncorrection that makes any subsequent kernel- or distance-based test valid. Our\nmethod relies on sampling based on the asymptotic distribution for the ASE. We\ncall these altered embeddings the corrected adjacency spectral embeddings\n(CASE). We also show that CASE remedies the exchangeability problem of the\noriginal test and demonstrate the validity and consistency of the test that\nuses CASE via a simulation study. Lastly, we apply our proposed test to the\nproblem of determining equivalence of generating distributions in human\nconnectomes extracted from diffusion magnetic resonance imaging (dMRI) at\ndifferent scales."}, "http://arxiv.org/abs/2008.10055": {"title": "Multiple Network Embedding for Anomaly Detection in Time Series of Graphs", "link": "http://arxiv.org/abs/2008.10055", "description": "This paper considers the graph signal processing problem of anomaly detection\nin time series of graphs. We examine two related, complementary inference\ntasks: the detection of anomalous graphs within a time series, and the\ndetection of temporally anomalous vertices. We approach these tasks via the\nadaptation of statistically principled methods for joint graph inference,\nspecifically \\emph{multiple adjacency spectral embedding} (MASE). We\ndemonstrate that our method is effective for our inference tasks. Moreover, we\nassess the performance of our method in terms of the underlying nature of\ndetectable anomalies. We further provide the theoretical justification for our\nmethod and insight into its use. Applied to the Enron communication graph and a\nlarge-scale commercial search engine time series of graphs, our approaches\ndemonstrate their applicability and identify the anomalous vertices beyond just\nlarge degree change."}, "http://arxiv.org/abs/2011.06127": {"title": "Generalized Kernel Two-Sample Tests", "link": "http://arxiv.org/abs/2011.06127", "description": "Kernel two-sample tests have been widely used for multivariate data to test\nequality of distributions. However, existing tests based on mapping\ndistributions into a reproducing kernel Hilbert space mainly target specific\nalternatives and do not work well for some scenarios when the dimension of the\ndata is moderate to high due to the curse of dimensionality. We propose a new\ntest statistic that makes use of a common pattern under moderate and high\ndimensions and achieves substantial power improvements over existing kernel\ntwo-sample tests for a wide range of alternatives. We also propose alternative\ntesting procedures that maintain high power with low computational cost,\noffering easy off-the-shelf tools for large datasets. The new approaches are\ncompared to other state-of-the-art tests under various settings and show good\nperformance. We showcase the new approaches through two applications: The\ncomparison of musks and non-musks using the shape of molecules, and the\ncomparison of taxi trips starting from John F. Kennedy airport in consecutive\nmonths. All proposed methods are implemented in an R package kerTests."}, "http://arxiv.org/abs/2108.00306": {"title": "A graphical multi-fidelity Gaussian process model, with application to emulation of heavy-ion collisions", "link": "http://arxiv.org/abs/2108.00306", "description": "With advances in scientific computing and mathematical modeling, complex\nscientific phenomena such as galaxy formations and rocket propulsion can now be\nreliably simulated. Such simulations can however be very time-intensive,\nrequiring millions of CPU hours to perform. One solution is multi-fidelity\nemulation, which uses data of different fidelities to train an efficient\npredictive model which emulates the expensive simulator. For complex scientific\nproblems and with careful elicitation from scientists, such multi-fidelity data\nmay often be linked by a directed acyclic graph (DAG) representing its\nscientific model dependencies. We thus propose a new Graphical Multi-fidelity\nGaussian Process (GMGP) model, which embeds this DAG structure (capturing\nscientific dependencies) within a Gaussian process framework. We show that the\nGMGP has desirable modeling traits via two Markov properties, and admits a\nscalable algorithm for recursive computation of the posterior mean and variance\nalong at each depth level of the DAG. We also present a novel experimental\ndesign methodology over the DAG given an experimental budget, and propose a\nnonlinear extension of the GMGP via deep Gaussian processes. The advantages of\nthe GMGP are then demonstrated via a suite of numerical experiments and an\napplication to emulation of heavy-ion collisions, which can be used to study\nthe conditions of matter in the Universe shortly after the Big Bang. The\nproposed model has broader uses in data fusion applications with graphical\nstructure, which we further discuss."}, "http://arxiv.org/abs/2202.06188": {"title": "Testing the number of common factors by bootstrapped sample covariance matrix in high-dimensional factor models", "link": "http://arxiv.org/abs/2202.06188", "description": "This paper studies the impact of bootstrap procedure on the eigenvalue\ndistributions of the sample covariance matrix under a high-dimensional factor\nstructure. We provide asymptotic distributions for the top eigenvalues of\nbootstrapped sample covariance matrix under mild conditions. After bootstrap,\nthe spiked eigenvalues which are driven by common factors will converge weakly\nto Gaussian limits after proper scaling and centralization. However, the\nlargest non-spiked eigenvalue is mainly determined by the order statistics of\nthe bootstrap resampling weights, and follows extreme value distribution. Based\non the disparate behavior of the spiked and non-spiked eigenvalues, we propose\ninnovative methods to test the number of common factors. Indicated by extensive\nnumerical and empirical studies, the proposed methods perform reliably and\nconvincingly under the existence of both weak factors and cross-sectionally\ncorrelated errors. Our technical details contribute to random matrix theory on\nspiked covariance model with convexly decaying density and unbounded support,\nor with general elliptical distributions."}, "http://arxiv.org/abs/2203.04689": {"title": "Tensor Completion for Causal Inference with Multivariate Longitudinal Data: A Reevaluation of COVID-19 Mandates", "link": "http://arxiv.org/abs/2203.04689", "description": "We propose a new method that uses tensor completion to estimate causal\neffects with multivariate longitudinal data, data in which multiple outcomes\nare observed for each unit and time period. Our motivation is to estimate the\nnumber of COVID-19 fatalities prevented by government mandates such as travel\nrestrictions, mask-wearing directives, and vaccination requirements. In\naddition to COVID-19 fatalities, we observe related outcomes such as the number\nof fatalities from other diseases and injuries. The proposed method arranges\nthe data as a tensor with three dimensions (unit, time, and outcome) and uses\ntensor completion to impute the missing counterfactual outcomes. We first prove\nthat under general conditions, combining multiple outcomes using the proposed\nmethod improves the accuracy of counterfactual imputations. We then compare the\nproposed method to other approaches commonly used to evaluate COVID-19\nmandates. Our main finding is that other approaches overestimate the effect of\nmasking-wearing directives and that mask-wearing directives were not an\neffective alternative to travel restrictions. We conclude that while the\nproposed method can be applied whenever multivariate longitudinal data are\navailable, we believe it is particularly timely as governments increasingly\nrely on longitudinal data to choose among policies such as mandates during\npublic health emergencies."}, "http://arxiv.org/abs/2203.10775": {"title": "Modified Method of Moments for Generalized Laplace Distribution", "link": "http://arxiv.org/abs/2203.10775", "description": "In this note, we consider the performance of the classic method of moments\nfor parameter estimation of symmetric variance-gamma (generalized Laplace)\ndistributions. We do this through both theoretical analysis (multivariate delta\nmethod) and a comprehensive simulation study with comparison to maximum\nlikelihood estimation, finding performance is often unsatisfactory. In\naddition, we modify the method of moments by taking absolute moments to improve\nefficiency; in particular, our simulation studies demonstrate that our modified\nestimators have significantly improved performance for parameter values\ntypically encountered in financial modelling, and is also competitive with\nmaximum likelihood estimation."}, "http://arxiv.org/abs/2208.14989": {"title": "Learning Multiscale Non-stationary Causal Structures", "link": "http://arxiv.org/abs/2208.14989", "description": "This paper addresses a gap in the current state of the art by providing a\nsolution for modeling causal relationships that evolve over time and occur at\ndifferent time scales. Specifically, we introduce the multiscale non-stationary\ndirected acyclic graph (MN-DAG), a framework for modeling multivariate time\nseries data. Our contribution is twofold. Firstly, we expose a probabilistic\ngenerative model by leveraging results from spectral and causality theories.\nOur model allows sampling an MN-DAG according to user-specified priors on the\ntime-dependence and multiscale properties of the causal graph. Secondly, we\ndevise a Bayesian method named Multiscale Non-stationary Causal Structure\nLearner (MN-CASTLE) that uses stochastic variational inference to estimate\nMN-DAGs. The method also exploits information from the local partial\ncorrelation between time series over different time resolutions. The data\ngenerated from an MN-DAG reproduces well-known features of time series in\ndifferent domains, such as volatility clustering and serial correlation.\nAdditionally, we show the superior performance of MN-CASTLE on synthetic data\nwith different multiscale and non-stationary properties compared to baseline\nmodels. Finally, we apply MN-CASTLE to identify the drivers of the natural gas\nprices in the US market. Causal relationships have strengthened during the\nCOVID-19 outbreak and the Russian invasion of Ukraine, a fact that baseline\nmethods fail to capture. MN-CASTLE identifies the causal impact of critical\neconomic drivers on natural gas prices, such as seasonal factors, economic\nuncertainty, oil prices, and gas storage deviations."}, "http://arxiv.org/abs/2301.06333": {"title": "Functional concurrent regression with compositional covariates and its application to the time-varying effect of causes of death on human longevity", "link": "http://arxiv.org/abs/2301.06333", "description": "Multivariate functional data that are cross-sectionally compositional data\nare attracting increasing interest in the statistical modeling literature, a\nmajor example being trajectories over time of compositions derived from\ncause-specific mortality rates. In this work, we develop a novel functional\nconcurrent regression model in which independent variables are functional\ncompositions. This allows us to investigate the relationship over time between\nlife expectancy at birth and compositions derived from cause-specific mortality\nrates of four distinct age classes, namely 0--4, 5--39, 40--64 and 65+ in 25\ncountries. A penalized approach is developed to estimate the regression\ncoefficients and select the relevant variables. Then an efficient computational\nstrategy based on an augmented Lagrangian algorithm is derived to solve the\nresulting optimization problem. The good performances of the model in\npredicting the response function and estimating the unknown functional\ncoefficients are shown in a simulation study. The results on real data confirm\nthe important role of neoplasms and cardiovascular diseases in determining life\nexpectancy emerged in other studies and reveal several other contributions not\nyet observed."}, "http://arxiv.org/abs/2305.00961": {"title": "DIF Analysis with Unknown Groups and Anchor Items", "link": "http://arxiv.org/abs/2305.00961", "description": "Ensuring fairness in instruments like survey questionnaires or educational\ntests is crucial. One way to address this is by a Differential Item Functioning\n(DIF) analysis, which examines if different subgroups respond differently to a\nparticular item, controlling for their overall latent construct level. DIF\nanalysis is typically conducted to assess measurement invariance at the item\nlevel. Traditional DIF analysis methods require knowing the comparison groups\n(reference and focal groups) and anchor items (a subset of DIF-free items).\nSuch prior knowledge may not always be available, and psychometric methods have\nbeen proposed for DIF analysis when one piece of information is unknown. More\nspecifically, when the comparison groups are unknown while anchor items are\nknown, latent DIF analysis methods have been proposed that estimate the unknown\ngroups by latent classes. When anchor items are unknown while comparison groups\nare known, methods have also been proposed, typically under a sparsity\nassumption -- the number of DIF items is not too large. However, DIF analysis\nwhen both pieces of information are unknown has not received much attention.\nThis paper proposes a general statistical framework under this setting. In the\nproposed framework, we model the unknown groups by latent classes and introduce\nitem-specific DIF parameters to capture the DIF effects. Assuming the number of\nDIF items is relatively small, an $L_1$-regularised estimator is proposed to\nsimultaneously identify the latent classes and the DIF items. A computationally\nefficient Expectation-Maximisation (EM) algorithm is developed to solve the\nnon-smooth optimisation problem for the regularised estimator. The performance\nof the proposed method is evaluated by simulation studies and an application to\nitem response data from a real-world educational test."}, "http://arxiv.org/abs/2306.14302": {"title": "Improved LM Test for Robust Model Specification Searches in Covariance Structure Analysis", "link": "http://arxiv.org/abs/2306.14302", "description": "Model specification searches and modifications are commonly employed in\ncovariance structure analysis (CSA) or structural equation modeling (SEM) to\nimprove the goodness-of-fit. However, these practices can be susceptible to\ncapitalizing on chance, as a model that fits one sample may not generalize to\nanother sample from the same population. This paper introduces the improved\nLagrange Multipliers (LM) test, which provides a reliable method for conducting\na thorough model specification search and effectively identifying missing\nparameters. By leveraging the stepwise bootstrap method in the standard LM and\nWald tests, our data-driven approach enhances the accuracy of parameter\nidentification. The results from Monte Carlo simulations and two empirical\napplications in political science demonstrate the effectiveness of the improved\nLM test, particularly when dealing with small sample sizes and models with\nlarge degrees of freedom. This approach contributes to better statistical fit\nand addresses the issue of capitalization on chance in model specification."}, "http://arxiv.org/abs/2308.13069": {"title": "The diachronic Bayesian", "link": "http://arxiv.org/abs/2308.13069", "description": "It is well known that a Bayesian probability forecast for the future\nobservations should form a probability measure in order to satisfy a natural\ncondition of coherence. The topic of this paper is the evolution of the\nBayesian probability measure over time. We model the process of updating the\nBayesian's beliefs in terms of prediction markets. The resulting picture is\nadapted to forecasting several steps ahead and making almost optimal decisions."}, "http://arxiv.org/abs/2311.12214": {"title": "Random Fourier Signature Features", "link": "http://arxiv.org/abs/2311.12214", "description": "Tensor algebras give rise to one of the most powerful measures of similarity\nfor sequences of arbitrary length called the signature kernel accompanied with\nattractive theoretical guarantees from stochastic analysis. Previous algorithms\nto compute the signature kernel scale quadratically in terms of the length and\nthe number of the sequences. To mitigate this severe computational bottleneck,\nwe develop a random Fourier feature-based acceleration of the signature kernel\nacting on the inherently non-Euclidean domain of sequences. We show uniform\napproximation guarantees for the proposed unbiased estimator of the signature\nkernel, while keeping its computation linear in the sequence length and number.\nIn addition, combined with recent advances on tensor projections, we derive two\neven more scalable time series features with favourable concentration\nproperties and computational complexity both in time and memory. Our empirical\nresults show that the reduction in computational cost comes at a negligible\nprice in terms of accuracy on moderate-sized datasets, and it enables one to\nscale to large datasets up to a million time series."}, "http://arxiv.org/abs/2311.12252": {"title": "Sensitivity analysis with multiple treatments and multiple outcomes with applications to air pollution mixtures", "link": "http://arxiv.org/abs/2311.12252", "description": "Understanding the health impacts of air pollution is vital in public health\nresearch. Numerous studies have estimated negative health effects of a variety\nof pollutants, but accurately gauging these impacts remains challenging due to\nthe potential for unmeasured confounding bias that is ubiquitous in\nobservational studies. In this study, we develop a framework for sensitivity\nanalysis in settings with both multiple treatments and multiple outcomes\nsimultaneously. This setting is of particular interest because one can identify\nthe strength of association between the unmeasured confounders and both the\ntreatment and outcome, under a factor confounding assumption. This provides\ninformative bounds on the causal effect leading to partial identification\nregions for the effects of multivariate treatments that account for the maximum\npossible bias from unmeasured confounding. We also show that when negative\ncontrols are available, we are able to refine the partial identification\nregions substantially, and in certain cases, even identify the causal effect in\nthe presence of unmeasured confounding. We derive partial identification\nregions for general estimands in this setting, and develop a novel\ncomputational approach to finding these regions."}, "http://arxiv.org/abs/2311.12293": {"title": "Sample size calculation based on the difference in restricted mean time lost for clinical trials with competing risks", "link": "http://arxiv.org/abs/2311.12293", "description": "Computation of sample size is important when designing clinical trials. The\npresence of competing risks makes the design of clinical trials with\ntime-to-event endpoints cumbersome. A model based on the subdistribution hazard\nratio (SHR) is commonly used for trials under competing risks. However, this\napproach has some limitations related to model assumptions and clinical\ninterpretation. Considering such limitations, the difference in restricted mean\ntime lost (RMTLd) is recommended as an alternative indicator. In this paper, we\npropose a sample size calculation method based on the RMTLd for the Weibull\ndistribution (RMTLdWeibull) for clinical trials, which considers experimental\nconditions such as equal allocation, uniform accrual, uniform loss to\nfollow-up, and administrative censoring. Simulation results show that sample\nsize calculation based on the RMTLdWeibull can generally achieve a predefined\npower level and maintain relative robustness. Moreover, the performance of the\nsample size calculation based on the RMTLdWeibull is similar or superior to\nthat based on the SHR. Even if the event time does not follow the Weibull\ndistribution, the sample size calculation based on the RMTLdWeibull still\nperforms well. The results also verify the performance of the sample size\ncalculation method based on the RMTLdWeibull. From the perspective of the\nresults of this study, clinical interpretation, application conditions and\nstatistical performance, we recommend that when designing clinical trials in\nthe presence of competing risks, the RMTLd indicator be applied for sample size\ncalculation and subsequent effect size measurement."}, "http://arxiv.org/abs/2311.12347": {"title": "Bayesian Cluster Geographically Weighted Regression for Spatial Heterogeneous Data", "link": "http://arxiv.org/abs/2311.12347", "description": "Spatial statistical models are commonly used in geographical scenarios to\nensure spatial variation is captured effectively. However, spatial models and\ncluster algorithms can be complicated and expensive. This paper pursues three\nmain objectives. First, it introduces covariate effect clustering by\nintegrating a Bayesian Geographically Weighted Regression (BGWR) with a\nGaussian mixture model and the Dirichlet process mixture model. Second, this\npaper examines situations in which a particular covariate holds significant\nimportance in one region but not in another in the Bayesian framework. Lastly,\nit addresses computational challenges present in existing BGWR, leading to\nnotable enhancements in Markov chain Monte Carlo estimation suitable for large\nspatial datasets. The efficacy of the proposed method is demonstrated using\nsimulated data and is further validated in a case study examining children's\ndevelopment domains in Queensland, Australia, using data provided by Children's\nHealth Queensland and Australia's Early Development Census."}, "http://arxiv.org/abs/2311.12380": {"title": "A multivariate adaptation of direct kernel estimator of density ratio", "link": "http://arxiv.org/abs/2311.12380", "description": "\\'Cwik and Mielniczuk (1989) introduced a univariate kernel density ratio\nestimator, which directly estimates the ratio without estimating the two\ndensities of interest. This study presents its straightforward multivariate\nadaptation."}, "http://arxiv.org/abs/2311.12392": {"title": "Individualized Dynamic Latent Factor Model with Application to Mobile Health Data", "link": "http://arxiv.org/abs/2311.12392", "description": "Mobile health has emerged as a major success in tracking individual health\nstatus, due to the popularity and power of smartphones and wearable devices.\nThis has also brought great challenges in handling heterogeneous,\nmulti-resolution data which arise ubiquitously in mobile health due to\nirregular multivariate measurements collected from individuals. In this paper,\nwe propose an individualized dynamic latent factor model for irregular\nmulti-resolution time series data to interpolate unsampled measurements of time\nseries with low resolution. One major advantage of the proposed method is the\ncapability to integrate multiple irregular time series and multiple subjects by\nmapping the multi-resolution data to the latent space. In addition, the\nproposed individualized dynamic latent factor model is applicable to capturing\nheterogeneous longitudinal information through individualized dynamic latent\nfactors. In theory, we provide the integrated interpolation error bound of the\nproposed estimator and derive the convergence rate with B-spline approximation\nmethods. Both the simulation studies and the application to smartwatch data\ndemonstrate the superior performance of the proposed method compared to\nexisting methods."}, "http://arxiv.org/abs/2311.12452": {"title": "Multi-indication evidence synthesis in oncology health technology assessment", "link": "http://arxiv.org/abs/2311.12452", "description": "Background: Cancer drugs receive licensing extensions to include additional\nindications as trial evidence on treatment effectiveness accumulates. We\ninvestigate how sharing information across indications can strengthen the\ninferences supporting Health Technology Assessment (HTA). Methods: We applied\nmeta-analytic methods to randomised trial data on bevacizumab to share\ninformation across cancer indications on the treatment effect on overall\nsurvival (OS) or progression-free survival (PFS), and on the surrogate\nrelationship between effects on PFS and OS. Common or random parameters were\nused to facilitate sharing and the further flexibility of mixture models was\nexplored. Results: OS treatment effects lacked precision when pooling data\navailable at present-day within each indication, particularly for indications\nwith few trials. There was no suggestion of heterogeneity across indications.\nSharing information across indications provided more precise inferences on\ntreatment effects, and on surrogacy parameters, with the strength of sharing\ndepending on the model. When a surrogate relationship was used to predict OS\neffects, uncertainty was only reduced with sharing imposed on PFS effects in\naddition to surrogacy parameters. Corresponding analyses using the earlier,\nsparser evidence available for particular HTAs showed that sharing on both\nsurrogacy and PFS effects did not notably reduce uncertainty in OS predictions.\nLimited heterogeneity across indications meant that the added flexibility of\nmixture models was unnecessary. Conclusions: Meta-analysis methods can be\nusefully applied to share information on treatment effectiveness across\nindications to increase the precision of target indication estimates in HTA.\nSharing on surrogate relationships requires caution, as meaningful precision\ngains require larger bodies of evidence and clear support for surrogacy from\nother indications."}, "http://arxiv.org/abs/2311.12597": {"title": "Optimal Functional Bilinear Regression with Two-way Functional Covariates via Reproducing Kernel Hilbert Space", "link": "http://arxiv.org/abs/2311.12597", "description": "Traditional functional linear regression usually takes a one-dimensional\nfunctional predictor as input and estimates the continuous coefficient\nfunction. Modern applications often generate two-dimensional covariates, which\nbecome matrices when observed at grid points. To avoid the inefficiency of the\nclassical method involving estimation of a two-dimensional coefficient\nfunction, we propose a functional bilinear regression model, and introduce an\ninnovative three-term penalty to impose roughness penalty in the estimation.\nThe proposed estimator exhibits minimax optimal property for prediction under\nthe framework of reproducing kernel Hilbert space. An iterative generalized\ncross-validation approach is developed to choose tuning parameters, which\nsignificantly improves the computational efficiency over the traditional\ncross-validation approach. The statistical and computational advantages of the\nproposed method over existing methods are further demonstrated via simulated\nexperiments, the Canadian weather data, and a biochemical long-range infrared\nlight detection and ranging data."}, "http://arxiv.org/abs/2311.12685": {"title": "A Graphical Comparison of Screening Designs using Support Recovery Probabilities", "link": "http://arxiv.org/abs/2311.12685", "description": "A screening experiment attempts to identify a subset of important effects\nusing a relatively small number of experimental runs. Given the limited run\nsize and a large number of possible effects, penalized regression is a popular\ntool used to analyze screening designs. In particular, an automated\nimplementation of the Gauss-Dantzig selector has been widely recommended to\ncompare screening design construction methods. Here, we illustrate potential\nreproducibility issues that arise when comparing screening designs via\nsimulation, and recommend a graphical method, based on screening probabilities,\nwhich compares designs by evaluating them along the penalized regression\nsolution path. This method can be implemented using simulation, or, in the case\nof lasso, by using exact local lasso sign recovery probabilities. Our approach\ncircumvents the need to specify tuning parameters associated with\nregularization methods, leading to more reliable design comparisons. This\narticle contains supplementary materials including code to implement the\nproposed methods."}, "http://arxiv.org/abs/2311.12717": {"title": "Phylogenetic least squares estimation without genetic distances", "link": "http://arxiv.org/abs/2311.12717", "description": "Least squares estimation of phylogenies is an established family of methods\nwith good statistical properties. State-of-the-art least squares phylogenetic\nestimation proceeds by first estimating a distance matrix, which is then used\nto determine the phylogeny by minimizing a squared-error loss function. Here,\nwe develop a method for least squares phylogenetic inference that does not rely\non a pre-estimated distance matrix. Our approach allows us to circumvent the\ntypical need to first estimate a distance matrix by forming a new loss function\ninspired by the phylogenetic likelihood score function; in this manner,\ninference is not based on a summary statistic of the sequence data, but\ndirectly on the sequence data itself. We use a Jukes-Cantor substitution model\nto show that our method leads to improvements over ordinary least squares\nphylogenetic inference, and is even observed to rival maximum likelihood\nestimation in terms of topology estimation efficiency. Using a Kimura\n2-parameter model, we show that our method also allows for estimation of the\nglobal transition/transversion ratio simultaneously with the phylogeny and its\nbranch lengths. This is impossible to accomplish with any other distance-based\nmethod as far as we know. Our developments pave the way for more optimal\nphylogenetic inference under the least squares framework, particularly in\nsettings under which likelihood-based inference is infeasible, including when\none desires to build a phylogeny based on information provided by only a subset\nof all possible nucleotide substitutions such as synonymous or non-synonymous\nsubstitutions."}, "http://arxiv.org/abs/2311.12726": {"title": "Nonparametric variable importance for time-to-event outcomes with application to prediction of HIV infection", "link": "http://arxiv.org/abs/2311.12726", "description": "In survival analysis, complex machine learning algorithms have been\nincreasingly used for predictive modeling. Given a collection of features\navailable for inclusion in a predictive model, it may be of interest to\nquantify the relative importance of a subset of features for the prediction\ntask at hand. In particular, in HIV vaccine trials, participant baseline\ncharacteristics are used to predict the probability of infection over the\nintended follow-up period, and investigators may wish to understand how much\ncertain types of predictors, such as behavioral factors, contribute toward\noverall predictiveness. Time-to-event outcomes such as time to infection are\noften subject to right censoring, and existing methods for assessing variable\nimportance are typically not intended to be used in this setting. We describe a\nbroad class of algorithm-agnostic variable importance measures for prediction\nin the context of survival data. We propose a nonparametric efficient\nestimation procedure that incorporates flexible learning of nuisance\nparameters, yields asymptotically valid inference, and enjoys\ndouble-robustness. We assess the performance of our proposed procedure via\nnumerical simulations and analyze data from the HVTN 702 study to inform\nenrollment strategies for future HIV vaccine trials."}, "http://arxiv.org/abs/2202.12989": {"title": "Flexible variable selection in the presence of missing data", "link": "http://arxiv.org/abs/2202.12989", "description": "In many applications, it is of interest to identify a parsimonious set of\nfeatures, or panel, from multiple candidates that achieves a desired level of\nperformance in predicting a response. This task is often complicated in\npractice by missing data arising from the sampling design or other random\nmechanisms. Most recent work on variable selection in missing data contexts\nrelies in some part on a finite-dimensional statistical model, e.g., a\ngeneralized or penalized linear model. In cases where this model is\nmisspecified, the selected variables may not all be truly scientifically\nrelevant and can result in panels with suboptimal classification performance.\nTo address this limitation, we propose a nonparametric variable selection\nalgorithm combined with multiple imputation to develop flexible panels in the\npresence of missing-at-random data. We outline strategies based on the proposed\nalgorithm that achieve control of commonly used error rates. Through\nsimulations, we show that our proposal has good operating characteristics and\nresults in panels with higher classification and variable selection performance\ncompared to several existing penalized regression approaches in cases where a\ngeneralized linear model is misspecified. Finally, we use the proposed method\nto develop biomarker panels for separating pancreatic cysts with differing\nmalignancy potential in a setting where complicated missingness in the\nbiomarkers arose due to limited specimen volumes."}, "http://arxiv.org/abs/2206.12773": {"title": "Scalable and optimal Bayesian inference for sparse covariance matrices via screened beta-mixture prior", "link": "http://arxiv.org/abs/2206.12773", "description": "In this paper, we propose a scalable Bayesian method for sparse covariance\nmatrix estimation by incorporating a continuous shrinkage prior with a\nscreening procedure. In the first step of the procedure, the off-diagonal\nelements with small correlations are screened based on their sample\ncorrelations. In the second step, the posterior of the covariance with the\nscreened elements fixed at $0$ is computed with the beta-mixture prior. The\nscreened elements of the covariance significantly increase the efficiency of\nthe posterior computation. The simulation studies and real data applications\nshow that the proposed method can be used for the high-dimensional problem with\nthe `large p, small n'. In some examples in this paper, the proposed method can\nbe computed in a reasonable amount of time, while no other existing Bayesian\nmethods can be. The proposed method has also sound theoretical properties. The\nscreening procedure has the sure screening property and the selection\nconsistency, and the posterior has the optimal minimax or nearly minimax\nconvergence rate under the Frobeninus norm."}, "http://arxiv.org/abs/2209.05474": {"title": "Consistent Selection of the Number of Groups in Panel Models via Sample-Splitting", "link": "http://arxiv.org/abs/2209.05474", "description": "Group number selection is a key question for group panel data modelling. In\nthis work, we develop a cross validation method to tackle this problem.\nSpecifically, we split the panel data into a training dataset and a testing\ndataset on the time span. We first use the training dataset to estimate the\nparameters and group memberships. Then we apply the fitted model to the testing\ndataset and then the group number is estimated by minimizing certain loss\nfunction values on the testing dataset. We design the loss functions for panel\ndata models either with or without fixed effects. The proposed method has two\nadvantages. First, the method is totally data-driven thus no further tuning\nparameters are involved. Second, the method can be flexibly applied to a wide\nrange of panel data models. Theoretically, we establish the estimation\nconsistency by taking advantage of the optimization property of the estimation\nalgorithm. Experiments on a variety of synthetic and empirical datasets are\ncarried out to further illustrate the advantages of the proposed method."}, "http://arxiv.org/abs/2301.07276": {"title": "Data thinning for convolution-closed distributions", "link": "http://arxiv.org/abs/2301.07276", "description": "We propose data thinning, an approach for splitting an observation into two\nor more independent parts that sum to the original observation, and that follow\nthe same distribution as the original observation, up to a (known) scaling of a\nparameter. This very general proposal is applicable to any convolution-closed\ndistribution, a class that includes the Gaussian, Poisson, negative binomial,\ngamma, and binomial distributions, among others. Data thinning has a number of\napplications to model selection, evaluation, and inference. For instance,\ncross-validation via data thinning provides an attractive alternative to the\nusual approach of cross-validation via sample splitting, especially in settings\nin which the latter is not applicable. In simulations and in an application to\nsingle-cell RNA-sequencing data, we show that data thinning can be used to\nvalidate the results of unsupervised learning approaches, such as k-means\nclustering and principal components analysis, for which traditional sample\nsplitting is unattractive or unavailable."}, "http://arxiv.org/abs/2303.12401": {"title": "Real-time forecasting within soccer matches through a Bayesian lens", "link": "http://arxiv.org/abs/2303.12401", "description": "This paper employs a Bayesian methodology to predict the results of soccer\nmatches in real-time. Using sequential data of various events throughout the\nmatch, we utilize a multinomial probit regression in a novel framework to\nestimate the time-varying impact of covariates and to forecast the outcome.\nEnglish Premier League data from eight seasons are used to evaluate the\nefficacy of our method. Different evaluation metrics establish that the\nproposed model outperforms potential competitors inspired by existing\nstatistical or machine learning algorithms. Additionally, we apply robustness\nchecks to demonstrate the model's accuracy across various scenarios."}, "http://arxiv.org/abs/2305.10524": {"title": "Dynamic Matrix Recovery", "link": "http://arxiv.org/abs/2305.10524", "description": "Matrix recovery from sparse observations is an extensively studied topic\nemerging in various applications, such as recommendation system and signal\nprocessing, which includes the matrix completion and compressed sensing models\nas special cases. In this work we propose a general framework for dynamic\nmatrix recovery of low-rank matrices that evolve smoothly over time. We start\nfrom the setting that the observations are independent across time, then extend\nto the setting that both the design matrix and noise possess certain temporal\ncorrelation via modified concentration inequalities. By pooling neighboring\nobservations, we obtain sharp estimation error bounds of both settings, showing\nthe influence of the underlying smoothness, the dependence and effective\nsamples. We propose a dynamic fast iterative shrinkage thresholding algorithm\nthat is computationally efficient, and characterize the interplay between\nalgorithmic and statistical convergence. Simulated and real data examples are\nprovided to support such findings."}, "http://arxiv.org/abs/2306.07047": {"title": "Foundations of Causal Discovery on Groups of Variables", "link": "http://arxiv.org/abs/2306.07047", "description": "Discovering causal relationships from observational data is a challenging\ntask that relies on assumptions connecting statistical quantities to graphical\nor algebraic causal models. In this work, we focus on widely employed\nassumptions for causal discovery when objects of interest are (multivariate)\ngroups of random variables rather than individual (univariate) random\nvariables, as is the case in a variety of problems in scientific domains such\nas climate science or neuroscience. If the group-level causal models are\nderived from partitioning a micro-level model into groups, we explore the\nrelationship between micro and group-level causal discovery assumptions. We\ninvestigate the conditions under which assumptions like Causal Faithfulness\nhold or fail to hold. Our analysis encompasses graphical causal models that\ncontain cycles and bidirected edges. We also discuss grouped time series causal\ngraphs and variants thereof as special cases of our general theoretical\nframework. Thereby, we aim to provide researchers with a solid theoretical\nfoundation for the development and application of causal discovery methods for\nvariable groups."}, "http://arxiv.org/abs/2309.01721": {"title": "Direct and Indirect Treatment Effects in the Presence of Semi-Competing Risks", "link": "http://arxiv.org/abs/2309.01721", "description": "Semi-competing risks refer to the phenomenon that the terminal event (such as\ndeath) can truncate the non-terminal event (such as disease progression) but\nnot vice versa. The treatment effect on the terminal event can be delivered\neither directly following the treatment or indirectly through the non-terminal\nevent. We consider two strategies to decompose the total effect into a direct\neffect and an indirect effect under the framework of mediation analysis, by\nadjusting the prevalence and hazard of non-terminal events, respectively. They\nrequire slightly different assumptions on cross-world quantities to achieve\nidentifiability. We establish asymptotic properties for the estimated\ncounterfactual cumulative incidences and decomposed treatment effects. Through\nsimulation studies and real-data applications we illustrate the subtle\ndifference between these two decompositions."}, "http://arxiv.org/abs/2311.12825": {"title": "A PSO Based Method to Generate Actionable Counterfactuals for High Dimensional Data", "link": "http://arxiv.org/abs/2311.12825", "description": "Counterfactual explanations (CFE) are methods that explain a machine learning\nmodel by giving an alternate class prediction of a data point with some minimal\nchanges in its features. It helps the users to identify their data attributes\nthat caused an undesirable prediction like a loan or credit card rejection. We\ndescribe an efficient and an actionable counterfactual (CF) generation method\nbased on particle swarm optimization (PSO). We propose a simple objective\nfunction for the optimization of the instance-centric CF generation problem.\nThe PSO brings in a lot of flexibility in terms of carrying out multi-objective\noptimization in large dimensions, capability for multiple CF generation, and\nsetting box constraints or immutability of data attributes. An algorithm is\nproposed that incorporates these features and it enables greater control over\nthe proximity and sparsity properties over the generated CFs. The proposed\nalgorithm is evaluated with a set of action-ability metrics in real-world\ndatasets, and the results were superior compared to that of the\nstate-of-the-arts."}, "http://arxiv.org/abs/2311.12878": {"title": "Adaptive Bayesian Learning with Action and State-Dependent Signal Variance", "link": "http://arxiv.org/abs/2311.12878", "description": "This manuscript presents an advanced framework for Bayesian learning by\nincorporating action and state-dependent signal variances into decision-making\nmodels. This framework is pivotal in understanding complex data-feedback loops\nand decision-making processes in various economic systems. Through a series of\nexamples, we demonstrate the versatility of this approach in different\ncontexts, ranging from simple Bayesian updating in stable environments to\ncomplex models involving social learning and state-dependent uncertainties. The\npaper uniquely contributes to the understanding of the nuanced interplay\nbetween data, actions, outcomes, and the inherent uncertainty in economic\nmodels."}, "http://arxiv.org/abs/2311.12978": {"title": "Physics-Informed Priors with Application to Boundary Layer Velocity", "link": "http://arxiv.org/abs/2311.12978", "description": "One of the most popular recent areas of machine learning predicates the use\nof neural networks augmented by information about the underlying process in the\nform of Partial Differential Equations (PDEs). These physics-informed neural\nnetworks are obtained by penalizing the inference with a PDE, and have been\ncast as a minimization problem currently lacking a formal approach to quantify\nthe uncertainty. In this work, we propose a novel model-based framework which\nregards the PDE as a prior information of a deep Bayesian neural network. The\nprior is calibrated without data to resemble the PDE solution in the prior\nmean, while our degree in confidence on the PDE with respect to the data is\nexpressed in terms of the prior variance. The information embedded in the PDE\nis then propagated to the posterior yielding physics-informed forecasts with\nuncertainty quantification. We apply our approach to a simulated viscous fluid\nand to experimentally-obtained turbulent boundary layer velocity in a wind\ntunnel using an appropriately simplified Navier-Stokes equation. Our approach\nrequires very few observations to produce physically-consistent forecasts as\nopposed to non-physical forecasts stemming from non-informed priors, thereby\nallowing forecasting complex systems where some amount of data as well as some\ncontextual knowledge is available."}, "http://arxiv.org/abs/2311.13017": {"title": "W-kernel and essential subspace for frequencist's evaluation of Bayesian estimators", "link": "http://arxiv.org/abs/2311.13017", "description": "The posterior covariance matrix W defined by the log-likelihood of each\nobservation plays important roles both in the sensitivity analysis and\nfrequencist's evaluation of the Bayesian estimators. This study focused on the\nmatrix W and its principal space; we term the latter as an essential subspace.\nFirst, it is shown that they appear in various statistical settings, such as\nthe evaluation of the posterior sensitivity, assessment of the frequencist's\nuncertainty from posterior samples, and stochastic expansion of the loss; a key\ntool to treat frequencist's properties is the recently proposed Bayesian\ninfinitesimal jackknife approximation (Giordano and Broderick (2023)). In the\nfollowing part, we show that the matrix W can be interpreted as a reproducing\nkernel; it is named as W-kernel. Using the W-kernel, the essential subspace is\nexpressed as a principal space given by the kernel PCA. A relation to the\nFisher kernel and neural tangent kernel is established, which elucidates the\nconnection to the classical asymptotic theory; it also leads to a sort of\nBayesian-frequencist's duality. Finally, two applications, selection of a\nrepresentative set of observations and dimensional reduction in the approximate\nbootstrap, are discussed. In the former, incomplete Cholesky decomposition is\nintroduced as an efficient method to compute the essential subspace. In the\nlatter, different implementations of the approximate bootstrap for posterior\nmeans are compared."}, "http://arxiv.org/abs/2311.13048": {"title": "Weighted composite likelihood for linear mixed models in complex samples", "link": "http://arxiv.org/abs/2311.13048", "description": "Fitting mixed models to complex survey data is a challenging problem. Most\nmethods in the literature, including the most widely used one, require a close\nrelationship between the model structure and the survey design. In this paper\nwe present methods for fitting arbitrary mixed models to data from arbitrary\nsurvey designs. We support this with an implementation that allows for\nmultilevel linear models and multistage designs without any assumptions about\nnesting of model and design, and that also allows for correlation structures\nsuch as those resulting from genetic relatedness. The estimation and inference\napproach uses weighted pairwise (composite) likelihood."}, "http://arxiv.org/abs/2311.13131": {"title": "Pair circulas modelling for multivariate circular time series", "link": "http://arxiv.org/abs/2311.13131", "description": "Modelling multivariate circular time series is considered. The\ncross-sectional and serial dependence is described by circulas, which are\nanalogs of copulas for circular distributions. In order to obtain a simple\nexpression of the dependence structure, we decompose a multivariate circula\ndensity to a product of several pair circula densities. Moreover, to reduce the\nnumber of pair circula densities, we consider strictly stationary multi-order\nMarkov processes. The real data analysis, in which the proposed model is fitted\nto multivariate time series wind direction data is also given."}, "http://arxiv.org/abs/2311.13196": {"title": "Optimal Time of Arrival Estimation for MIMO Backscatter Channels", "link": "http://arxiv.org/abs/2311.13196", "description": "In this paper, we propose a novel time of arrival (TOA) estimator for\nmultiple-input-multiple-output (MIMO) backscatter channels in closed form. The\nproposed estimator refines the estimation precision from the topological\nstructure of the MIMO backscatter channels, and can considerably enhance the\nestimation accuracy. Particularly, we show that for the general $M \\times N$\nbistatic topology, the mean square error (MSE) is $\\frac{M+N-1}{MN}\\sigma^2_0$,\nand for the general $M \\times M$ monostatic topology, it is\n$\\frac{2M-1}{M^2}\\sigma^2_0$ for the diagonal subchannels, and\n$\\frac{M-1}{M^2}\\sigma^2_0$ for the off-diagonal subchannels, where\n$\\sigma^2_0$ is the MSE of the conventional least square estimator. In\naddition, we derive the Cramer-Rao lower bound (CRLB) for MIMO backscatter TOA\nestimation which indicates that the proposed estimator is optimal. Simulation\nresults verify that the proposed TOA estimator can considerably improve both\nestimation and positioning accuracy, especially when the MIMO scale is large."}, "http://arxiv.org/abs/2311.13202": {"title": "Robust Multi-Model Subset Selection", "link": "http://arxiv.org/abs/2311.13202", "description": "Modern datasets in biology and chemistry are often characterized by the\npresence of a large number of variables and outlying samples due to measurement\nerrors or rare biological and chemical profiles. To handle the characteristics\nof such datasets we introduce a method to learn a robust ensemble comprised of\na small number of sparse, diverse and robust models, the first of its kind in\nthe literature. The degree to which the models are sparse, diverse and\nresistant to data contamination is driven directly by the data based on a\ncross-validation criterion. We establish the finite-sample breakdown of the\nensembles and the models that comprise them, and we develop a tailored\ncomputing algorithm to learn the ensembles by leveraging recent developments in\nl0 optimization. Our extensive numerical experiments on synthetic and\nartificially contaminated real datasets from genomics and cheminformatics\ndemonstrate the competitive advantage of our method over state-of-the-art\nsparse and robust methods. We also demonstrate the applicability of our\nproposal on a cardiac allograft vasculopathy dataset."}, "http://arxiv.org/abs/2311.13247": {"title": "A projected nonlinear state-space model for forecasting time series signals", "link": "http://arxiv.org/abs/2311.13247", "description": "Learning and forecasting stochastic time series is essential in various\nscientific fields. However, despite the proposals of nonlinear filters and\ndeep-learning methods, it remains challenging to capture nonlinear dynamics\nfrom a few noisy samples and predict future trajectories with uncertainty\nestimates while maintaining computational efficiency. Here, we propose a fast\nalgorithm to learn and forecast nonlinear dynamics from noisy time series data.\nA key feature of the proposed model is kernel functions applied to projected\nlines, enabling fast and efficient capture of nonlinearities in the latent\ndynamics. Through empirical case studies and benchmarking, the model\ndemonstrates its effectiveness in learning and forecasting complex nonlinear\ndynamics, offering a valuable tool for researchers and practitioners in time\nseries analysis."}, "http://arxiv.org/abs/2311.13291": {"title": "Robust Functional Regression with Discretely Sampled Predictors", "link": "http://arxiv.org/abs/2311.13291", "description": "The functional linear model is an important extension of the classical\nregression model allowing for scalar responses to be modeled as functions of\nstochastic processes. Yet, despite the usefulness and popularity of the\nfunctional linear model in recent years, most treatments, theoretical and\npractical alike, suffer either from (i) lack of resistance towards the many\ntypes of anomalies one may encounter with functional data or (ii) biases\nresulting from the use of discretely sampled functional data instead of\ncompletely observed data. To address these deficiencies, this paper introduces\nand studies the first class of robust functional regression estimators for\npartially observed functional data. The proposed broad class of estimators is\nbased on thin-plate splines with a novel computationally efficient quadratic\npenalty, is easily implementable and enjoys good theoretical properties under\nweak assumptions. We show that, in the incomplete data setting, both the sample\nsize and discretization error of the processes determine the asymptotic rate of\nconvergence of functional regression estimators and the latter cannot be\nignored. These theoretical properties remain valid even with multi-dimensional\nrandom fields acting as predictors and random smoothing parameters. The\neffectiveness of the proposed class of estimators in practice is demonstrated\nby means of a simulation study and a real-data example."}, "http://arxiv.org/abs/2311.13327": {"title": "Regressions under Adverse Conditions", "link": "http://arxiv.org/abs/2311.13327", "description": "We introduce a new regression method that relates the mean of an outcome\nvariable to covariates, given the \"adverse condition\" that a distress variable\nfalls in its tail. This allows to tailor classical mean regressions to adverse\neconomic scenarios, which receive increasing interest in managing macroeconomic\nand financial risks, among many others. In the terminology of the systemic risk\nliterature, our method can be interpreted as a regression for the Marginal\nExpected Shortfall. We propose a two-step procedure to estimate the new models,\nshow consistency and asymptotic normality of the estimator, and propose\nfeasible inference under weak conditions allowing for cross-sectional and time\nseries applications. The accuracy of the asymptotic approximations of the\ntwo-step estimator is verified in simulations. Two empirical applications show\nthat our regressions under adverse conditions are valuable in such diverse\nfields as the study of the relation between systemic risk and asset price\nbubbles, and dissecting macroeconomic growth vulnerabilities into individual\ncomponents."}, "http://arxiv.org/abs/2311.13347": {"title": "Loss-based Objective and Penalizing Priors for Model Selection Problems", "link": "http://arxiv.org/abs/2311.13347", "description": "Many Bayesian model selection problems, such as variable selection or cluster\nanalysis, start by setting prior model probabilities on a structured model\nspace. Based on a chosen loss function between models, model selection is often\nperformed with a Bayes estimator that minimizes the posterior expected loss.\nThe prior model probabilities and the choice of loss both highly affect the\nmodel selection results, especially for data with small sample sizes, and their\nproper calibration and careful reflection of no prior model preference are\ncrucial in objective Bayesian analysis. We propose risk equilibrium priors as\nan objective choice for prior model probabilities that only depend on the model\nspace and the choice of loss. Under the risk equilibrium priors, the Bayes\naction becomes indifferent before observing data, and the family of the risk\nequilibrium priors includes existing popular objective priors in Bayesian\nvariable selection problems. We generalize the result to the elicitation of\nobjective priors for Bayesian cluster analysis with Binder's loss. We also\npropose risk penalization priors, where the Bayes action chooses the simplest\nmodel before seeing data. The concept of risk equilibrium and penalization\npriors allows us to interpret prior properties in light of the effect of loss\nfunctions, and also provides new insight into the sensitivity of Bayes\nestimators under the same prior but different loss. We illustrate the proposed\nconcepts with variable selection simulation studies and cluster analysis on a\ngalaxy dataset."}, "http://arxiv.org/abs/2311.13410": {"title": "Towards Sensitivity Analysis: A Workflow", "link": "http://arxiv.org/abs/2311.13410", "description": "Establishing causal claims is one of the primary endeavors in sociological\nresearch. Statistical causal inference is a promising way to achieve this\nthrough the potential outcome framework or structural causal models, which are\nbased on a set of identification assumptions. However, identification\nassumptions are often not fully discussed in practice, which harms the validity\nof causal claims. In this article, we focus on the unmeasurededness assumption\nthat assumes no unmeasured confounders in models, which is often violated in\npractice. This article reviews a set of papers in two leading sociological\njournals to check the practice of causal inference and relevant identification\nassumptions, indicating the lack of discussion on sensitivity analysis methods\non unconfoundedness in practice. And then, a blueprint of how to conduct\nsensitivity analysis methods on unconfoundedness is built, including six steps\nof proper choices on practices of sensitivity analysis to evaluate the impacts\nof unmeasured confounders."}, "http://arxiv.org/abs/2311.13481": {"title": "Synergizing Roughness Penalization and Basis Selection in Bayesian Spline Regression", "link": "http://arxiv.org/abs/2311.13481", "description": "Bayesian P-splines and basis determination through Bayesian model selection\nare both commonly employed strategies for nonparametric regression using spline\nbasis expansions within the Bayesian framework. Although both methods are\nwidely employed, they each have particular limitations that may introduce\npotential estimation bias depending on the nature of the target function. To\novercome the limitations associated with each method while capitalizing on\ntheir respective strengths, we propose a new prior distribution that integrates\nthe essentials of both approaches. The proposed prior distribution assesses the\ncomplexity of the spline model based on a penalty term formed by a convex\ncombination of the penalties from both methods. The proposed method exhibits\nadaptability to the unknown level of smoothness while achieving the\nminimax-optimal posterior contraction rate up to a logarithmic factor. We\nprovide an efficient Markov chain Monte Carlo algorithm for implementing the\nproposed approach. Our extensive simulation study reveals that the proposed\nmethod outperforms other competitors in terms of performance metrics or model\ncomplexity. An application to a real dataset substantiates the validity of our\nproposed approach."}, "http://arxiv.org/abs/2311.13556": {"title": "Universally Optimal Multivariate Crossover Designs", "link": "http://arxiv.org/abs/2311.13556", "description": "In this article, universally optimal multivariate crossover designs are\nstudied. The multiple response crossover design is motivated by a $3 \\times 3$\ncrossover setup, where the effect of $3$ doses of an oral drug are studied on\ngene expressions related to mucosal inflammation. Subjects are assigned to\nthree treatment sequences and response measurements on 5 different gene\nexpressions are taken from each subject in each of the $3$ time periods. To\nmodel multiple or $g$ responses, where $g&gt;1$, in a crossover setup, a\nmultivariate fixed effect model with both direct and carryover treatment\neffects is considered. It is assumed that there are non zero within response\ncorrelations, while between response correlations are taken to be zero. The\ninformation matrix corresponding to the direct effects is obtained and some\nresults are studied. The information matrix in the multivariate case is shown\nto differ from the univariate case, particularly in the completely symmetric\nproperty. For the $g&gt;1$ case, with $t$ treatments and $p$ periods, for $p=t\n\\geq 3$, the design represented by a Type $I$ orthogonal array of strength $2$\nis proved to be universally optimal over the class of binary designs, for the\ndirect treatment effects."}, "http://arxiv.org/abs/2311.13595": {"title": "Covariance alignment: from maximum likelihood estimation to Gromov-Wasserstein", "link": "http://arxiv.org/abs/2311.13595", "description": "Feature alignment methods are used in many scientific disciplines for data\npooling, annotation, and comparison. As an instance of a permutation learning\nproblem, feature alignment presents significant statistical and computational\nchallenges. In this work, we propose the covariance alignment model to study\nand compare various alignment methods and establish a minimax lower bound for\ncovariance alignment that has a non-standard dimension scaling because of the\npresence of a nuisance parameter. This lower bound is in fact minimax optimal\nand is achieved by a natural quasi MLE. However, this estimator involves a\nsearch over all permutations which is computationally infeasible even when the\nproblem has moderate size. To overcome this limitation, we show that the\ncelebrated Gromov-Wasserstein algorithm from optimal transport which is more\namenable to fast implementation even on large-scale problems is also minimax\noptimal. These results give the first statistical justification for the\ndeployment of the Gromov-Wasserstein algorithm in practice."}, "http://arxiv.org/abs/2107.06093": {"title": "A generalized hypothesis test for community structure in networks", "link": "http://arxiv.org/abs/2107.06093", "description": "Researchers theorize that many real-world networks exhibit community\nstructure where within-community edges are more likely than between-community\nedges. While numerous methods exist to cluster nodes into different\ncommunities, less work has addressed this question: given some network, does it\nexhibit statistically meaningful community structure? We answer this question\nin a principled manner by framing it as a statistical hypothesis test in terms\nof a general and model-agnostic community structure parameter. Leveraging this\nparameter, we propose a simple and interpretable test statistic used to\nformulate two separate hypothesis testing frameworks. The first is an\nasymptotic test against a baseline value of the parameter while the second\ntests against a baseline model using bootstrap-based thresholds. We prove\ntheoretical properties of these tests and demonstrate how the proposed method\nyields rich insights into real-world data sets."}, "http://arxiv.org/abs/2205.08030": {"title": "Interpretable sensitivity analysis for the Baron-Kenny approach to mediation with unmeasured confounding", "link": "http://arxiv.org/abs/2205.08030", "description": "Mediation analysis assesses the extent to which the exposure affects the\noutcome indirectly through a mediator and the extent to which it operates\ndirectly through other pathways. As the most popular method in empirical\nmediation analysis, the Baron-Kenny approach estimates the indirect and direct\neffects of the exposure on the outcome based on linear structural equation\nmodels. However, when the exposure and the mediator are not randomized, the\nestimates may be biased due to unmeasured confounding among the exposure,\nmediator, and outcome. Building on Cinelli and Hazlett (2020), we derive\ngeneral omitted-variable bias formulas in linear regressions with vector\nresponses and regressors. We then use the formulas to develop a sensitivity\nanalysis method for the Baron-Kenny approach to mediation in the presence of\nunmeasured confounding. To ensure interpretability, we express the sensitivity\nparameters to correspond to the natural factorization of the joint distribution\nof the direct acyclic graph for mediation analysis. They measure the partial\ncorrelation between the unmeasured confounder and the exposure, mediator,\noutcome, respectively. With the sensitivity parameters, we propose a novel\nmeasure called the \"robustness value for mediation\" or simply the \"robustness\nvalue\", to assess the robustness of results based on the Baron-Kenny approach\nwith respect to unmeasured confounding. Intuitively, the robustness value\nmeasures the minimum value of the maximum proportion of variability explained\nby the unmeasured confounding, for the exposure, mediator and outcome, to\noverturn the results of the point estimate or confidence interval for the\ndirect and indirect effects. Importantly, we prove that all our sensitivity\nbounds are attainable and thus sharp."}, "http://arxiv.org/abs/2208.10106": {"title": "Statistics did not prove that the Huanan Seafood Wholesale Market was the early epicenter of the COVID-19 pandemic", "link": "http://arxiv.org/abs/2208.10106", "description": "In a recent prominent study Worobey et al.\\ (2022, Science, 377, pp.\\ 951--9)\npurported to demonstrate statistically that the Huanan Seafood Wholesale Market\nwas the epicenter of the early COVID-19 epidemic. We show that this statistical\nconclusion is invalid on two grounds: (1) The assumption that a centroid of\nearly case locations or another simply constructed point is the origin of an\nepidemic is unproved. (2) A Monte Carlo test used to conclude that no other\nlocation than the seafood market can be the origin is flawed. Hence, the\nquestion of the origin of the pandemic has not been answered by their\nstatistical analysis."}, "http://arxiv.org/abs/2209.10433": {"title": "Arithmetic Average Density Fusion -- Part II: Unified Derivation for Unlabeled and Labeled RFS Fusion", "link": "http://arxiv.org/abs/2209.10433", "description": "As a fundamental information fusion approach, the arithmetic average (AA)\nfusion has recently been investigated for various random finite set (RFS)\nfilter fusion in the context of multi-sensor multi-target tracking. It is not a\nstraightforward extension of the ordinary density-AA fusion to the RFS\ndistribution but has to preserve the form of the fusing multi-target density.\nIn this work, we first propose a statistical concept, probability hypothesis\ndensity (PHD) consistency, and explain how it can be achieved by the PHD-AA\nfusion and lead to more accurate and robust detection and localization of the\npresent targets. This forms a both theoretically sound and technically\nmeaningful reason for performing inter-filter PHD AA-fusion/consensus, while\npreserving the form of the fusing RFS filter. Then, we derive and analyze the\nproper AA fusion formulations for most existing unlabeled/labeled RFS filters\nbasing on the (labeled) PHD-AA/consistency. These derivations are theoretically\nunified, exact, need no approximation and greatly enable heterogenous unlabeled\nand labeled RFS density fusion which is separately demonstrated in two\nconsequent companion papers."}, "http://arxiv.org/abs/2209.14846": {"title": "Factor Modeling of a High-Dimensional Matrix-Variate and Statistical Learning for Matrix-Valued Sequences", "link": "http://arxiv.org/abs/2209.14846", "description": "We propose a new matrix factor model, named RaDFaM, the latent structure of\nwhich is strictly derived based on a hierarchical rank decomposition of a\nmatrix. Hierarchy is in the sense that all basis vectors of the column space of\neach multiplier matrix are assumed the structure of a vector factor model.\nCompared to the most commonly used matrix factor model that takes the latent\nstructure of a bilinear form, RaDFaM involves additional row-wise and\ncolumn-wise matrix latent factors. This yields modest dimension reduction and\nstronger signal intensity from the sight of tensor subspace learning, though\nposes challenges of new estimation procedure and concomitant inferential theory\nfor a collection of matrix-valued observations. We develop a class of\nestimation procedure that makes use of the separable covariance structure under\nRaDFaM and approximate least squares, and derive its superiority in the merit\nof the peak signal-to-noise ratio. We also establish the asymptotic theory when\nthe matrix-valued observations are uncorrelated or weakly correlated.\nNumerically, in terms of image/matrix reconstruction, supervised learning, and\nso forth, we demonstrate the excellent performance of RaDFaM through two\nmatrix-valued sequence datasets of independent 2D images and multinational\nmacroeconomic indices time series, respectively."}, "http://arxiv.org/abs/2210.14745": {"title": "Identifying Counterfactual Queries with the R Package cfid", "link": "http://arxiv.org/abs/2210.14745", "description": "In the framework of structural causal models, counterfactual queries describe\nevents that concern multiple alternative states of the system under study.\nCounterfactual queries often take the form of \"what if\" type questions such as\n\"would an applicant have been hired if they had over 10 years of experience,\nwhen in reality they only had 5 years of experience?\" Such questions and\ncounterfactual inference in general are crucial, for example when addressing\nthe problem of fairness in decision-making. Because counterfactual events\ncontain contradictory states of the world, it is impossible to conduct a\nrandomized experiment to address them without making several restrictive\nassumptions. However, it is sometimes possible to identify such queries from\nobservational and experimental data by representing the system under study as a\ncausal model, and the available data as symbolic probability distributions.\nShpitser and Pearl (2007) constructed two algorithms, called ID* and IDC*, for\nidentifying counterfactual queries and conditional counterfactual queries,\nrespectively. These two algorithms are analogous to the ID and IDC algorithms\nby Shpitser and Pearl (2006) for identification of interventional\ndistributions, which were implemented in R by Tikka and Karvanen (2017) in the\ncausaleffect package. We present the R package cfid that implements the ID* and\nIDC* algorithms. Identification of counterfactual queries and the features of\ncfid are demonstrated via examples."}, "http://arxiv.org/abs/2210.17000": {"title": "Ensemble transport smoothing", "link": "http://arxiv.org/abs/2210.17000", "description": "Smoothers are algorithms for Bayesian time series re-analysis. Most\noperational smoothers rely either on affine Kalman-type transformations or on\nsequential importance sampling. These strategies occupy opposite ends of a\nspectrum that trades computational efficiency and scalability for statistical\ngenerality and consistency: non-Gaussianity renders affine Kalman updates\ninconsistent with the true Bayesian solution, while the ensemble size required\nfor successful importance sampling can be prohibitive. This paper revisits the\nsmoothing problem from the perspective of measure transport, which offers the\nprospect of consistent prior-to-posterior transformations for Bayesian\ninference. We leverage this capacity by proposing a general ensemble framework\nfor transport-based smoothing. Within this framework, we derive a comprehensive\nset of smoothing recursions based on nonlinear transport maps and detail how\nthey exploit the structure of state-space models in fully non-Gaussian\nsettings. We also describe how many standard Kalman-type smoothing algorithms\nemerge as special cases of our framework. A companion paper (Ramgraber et al.,\n2023) explores the implementation of nonlinear ensemble transport smoothers in\ngreater depth."}, "http://arxiv.org/abs/2210.17435": {"title": "Ensemble transport smoothing", "link": "http://arxiv.org/abs/2210.17435", "description": "Smoothing is a specialized form of Bayesian inference for state-space models\nthat characterizes the posterior distribution of a collection of states given\nan associated sequence of observations. Ramgraber et al. (2023) proposes a\ngeneral framework for transport-based ensemble smoothing, which includes linear\nKalman-type smoothers as special cases. Here, we build on this foundation to\nrealize and demonstrate nonlinear backward ensemble transport smoothers. We\ndiscuss parameterization and regularization of the associated transport maps,\nand then examine the performance of these smoothers for nonlinear and chaotic\ndynamical systems that exhibit non-Gaussian behavior. In these settings, our\nnonlinear transport smoothers yield lower estimation error than conventional\nlinear smoothers and state-of-the-art iterative ensemble Kalman smoothers, for\ncomparable numbers of model evaluations."}, "http://arxiv.org/abs/2305.03634": {"title": "On the use of ordered factors as explanatory variables", "link": "http://arxiv.org/abs/2305.03634", "description": "Consider a regression or some regression-type model for a certain response\nvariable where the linear predictor includes an ordered factor among the\nexplanatory variables. The inclusion of a factor of this type can take place is\na few different ways, discussed in the pertaining literature. The present\ncontribution proposes a different way of tackling this problem, by constructing\na numeric variable in an alternative way with respect to the current\nmethodology. The proposed techniques appears to retain the data fitting\ncapability of the existing methodology, but with a simpler interpretation of\nthe model components."}, "http://arxiv.org/abs/2306.13257": {"title": "Semiparametric Estimation of the Shape of the Limiting Multivariate Point Cloud", "link": "http://arxiv.org/abs/2306.13257", "description": "We propose a model to flexibly estimate joint tail properties by exploiting\nthe convergence of an appropriately scaled point cloud onto a compact limit\nset. Characteristics of the shape of the limit set correspond to key tail\ndependence properties. We directly model the shape of the limit set using\nBezier splines, which allow flexible and parsimonious specification of shapes\nin two dimensions. We then fit the Bezier splines to data in pseudo-polar\ncoordinates using Markov chain Monte Carlo, utilizing a limiting approximation\nto the conditional likelihood of the radii given angles. By imposing\nappropriate constraints on the parameters of the Bezier splines, we guarantee\nthat each posterior sample is a valid limit set boundary, allowing direct\nposterior analysis of any quantity derived from the shape of the curve.\nFurthermore, we obtain interpretable inference on the asymptotic dependence\nclass by using mixture priors with point masses on the corner of the unit box.\nFinally, we apply our model to bivariate datasets of extremes of variables\nrelated to fire risk and air pollution."}, "http://arxiv.org/abs/2307.08370": {"title": "Parameter estimation for contact tracing in graph-based models", "link": "http://arxiv.org/abs/2307.08370", "description": "We adopt a maximum-likelihood framework to estimate parameters of a\nstochastic susceptible-infected-recovered (SIR) model with contact tracing on a\nrooted random tree. Given the number of detectees per index case, our estimator\nallows to determine the degree distribution of the random tree as well as the\ntracing probability. Since we do not discover all infectees via contact\ntracing, this estimation is non-trivial. To keep things simple and stable, we\ndevelop an approximation suited for realistic situations (contract tracing\nprobability small, or the probability for the detection of index cases small).\nIn this approximation, the only epidemiological parameter entering the\nestimator is $R_0$.\n\nThe estimator is tested in a simulation study and is furthermore applied to\ncovid-19 contact tracing data from India. The simulation study underlines the\nefficiency of the method. For the empirical covid-19 data, we compare different\ndegree distributions and perform a sensitivity analysis. We find that\nparticularly a power-law and a negative binomial degree distribution fit the\ndata well and that the tracing probability is rather large. The sensitivity\nanalysis shows no strong dependency of the estimates on the reproduction\nnumber. Finally, we discuss the relevance of our findings."}, "http://arxiv.org/abs/2311.13701": {"title": "Reexamining Statistical Significance and P-Values in Nursing Research: Historical Context and Guidance for Interpretation, Alternatives, and Reporting", "link": "http://arxiv.org/abs/2311.13701", "description": "Nurses should rely on the best evidence, but tend to struggle with\nstatistics, impeding research integration into clinical practice. Statistical\nsignificance, a key concept in classical statistics, and its primary metric,\nthe p-value, are frequently misused. This topic has been debated in many\ndisciplines but rarely in nursing. The aim is to present key arguments in the\ndebate surrounding the misuse of p-values, discuss their relevance to nursing,\nand offer recommendations to address them. The literature indicates that the\nconcept of probability in classical statistics is not easily understood,\nleading to misinterpretations of statistical significance. Much of the critique\nconcerning p-values arises from such misunderstandings and imprecise\nterminology. Thus, some scholars have argued for the complete abandonment of\np-values. Instead of discarding p-values, this article provides a comprehensive\naccount of their historical context and the information they convey. This will\nclarify why they are widely used yet often misunderstood. The article also\noffers recommendations for accurate interpretation of statistical significance\nby incorporating other key metrics. To mitigate publication bias resulting from\np-value misuse, pre-registering the analysis plan is recommended. The article\nalso explores alternative approaches, particularly Bayes factors, as they may\nresolve several of these issues. P-values serve a purpose in nursing research\nas an initial safeguard against the influence of randomness. Much criticism\ndirected towards p-values arises from misunderstandings and inaccurate\nterminology. Several considerations and measures are recommended, some which go\nbeyond the conventional, to obtain accurate p-values and to better understand\nstatistical significance. Nurse educators and researchers should considerer\nthese in their educational and research reporting practices."}, "http://arxiv.org/abs/2311.13767": {"title": "Hierarchical False Discovery Rate Control for High-dimensional Survival Analysis with Interactions", "link": "http://arxiv.org/abs/2311.13767", "description": "With the development of data collection techniques, analysis with a survival\nresponse and high-dimensional covariates has become routine. Here we consider\nan interaction model, which includes a set of low-dimensional covariates, a set\nof high-dimensional covariates, and their interactions. This model has been\nmotivated by gene-environment (G-E) interaction analysis, where the E variables\nhave a low dimension, and the G variables have a high dimension. For such a\nmodel, there has been extensive research on estimation and variable selection.\nComparatively, inference studies with a valid false discovery rate (FDR)\ncontrol have been very limited. The existing high-dimensional inference tools\ncannot be directly applied to interaction models, as interactions and main\neffects are not ``equal\". In this article, for high-dimensional survival\nanalysis with interactions, we model survival using the Accelerated Failure\nTime (AFT) model and adopt a ``weighted least squares + debiased Lasso''\napproach for estimation and selection. A hierarchical FDR control approach is\ndeveloped for inference and respect of the ``main effects, interactions''\nhierarchy. { The asymptotic distribution properties of the debiased Lasso\nestimators} are rigorously established. Simulation demonstrates the\nsatisfactory performance of the proposed approach, and the analysis of a breast\ncancer dataset further establishes its practical utility."}, "http://arxiv.org/abs/2311.13768": {"title": "Valid confidence intervals for regression with best subset selection", "link": "http://arxiv.org/abs/2311.13768", "description": "Classical confidence intervals after best subset selection are widely\nimplemented in statistical software and are routinely used to guide\npractitioners in scientific fields to conclude significance. However, there are\nincreasing concerns in the recent literature about the validity of these\nconfidence intervals in that the intended frequentist coverage is not attained.\nIn the context of the Akaike information criterion (AIC), recent studies\nobserve an under-coverage phenomenon in terms of overfitting, where the\nestimate of error variance under the selected submodel is smaller than that for\nthe true model. Under-coverage is particularly troubling in selective inference\nas it points to inflated Type I errors that would invalidate significant\nfindings. In this article, we delineate a complementary, yet provably more\ndeciding factor behind the incorrect coverage of classical confidence intervals\nunder AIC, in terms of altered conditional sampling distributions of pivotal\nquantities. Resting on selective techniques developed in other settings, our\nfinite-sample characterization of the selection event under AIC uncovers its\ngeometry as a union of finitely many intervals on the real line, based on which\nwe derive new confidence intervals with guaranteed coverage for any sample\nsize. This geometry derived for AIC selection enables exact (and typically less\nthan exact) conditioning, circumventing the need for the excessive conditioning\ncommon in other post-selection methods. The proposed methods are easy to\nimplement and can be broadly applied to other commonly used best subset\nselection criteria. In an application to a classical US consumption dataset,\nthe proposed confidence intervals arrive at different conclusions compared to\nthe conventional ones, even when the selected model is the full model, leading\nto interpretable findings that better align with empirical observations."}, "http://arxiv.org/abs/2311.13815": {"title": "Resampling Methods with Imputed Data", "link": "http://arxiv.org/abs/2311.13815", "description": "Resampling techniques have become increasingly popular for estimation of\nuncertainty in data collected via surveys. Survey data are also frequently\nsubject to missing data which are often imputed. This note addresses the issue\nof using resampling methods such as a jackknife or bootstrap in conjunction\nwith imputations that have be sampled stochastically (e.g., in the vein of\nmultiple imputation). It is illustrated that the imputations must be redrawn\nwithin each replicate group of a jackknife or bootstrap. Further, the number of\nmultiply imputed datasets per replicate group must dramatically exceed the\nnumber of replicate groups for a jackknife. However, this is not the case in a\nbootstrap approach. A brief simulation study is provided to support the theory\nintroduced in this note."}, "http://arxiv.org/abs/2311.13825": {"title": "Online Prediction of Extreme Conditional Quantiles via B-Spline Interpolation", "link": "http://arxiv.org/abs/2311.13825", "description": "Extreme quantiles are critical for understanding the behavior of data in the\ntail region of a distribution. It is challenging to estimate extreme quantiles,\nparticularly when dealing with limited data in the tail. In such cases, extreme\nvalue theory offers a solution by approximating the tail distribution using the\nGeneralized Pareto Distribution (GPD). This allows for the extrapolation beyond\nthe range of observed data, making it a valuable tool for various applications.\nHowever, when it comes to conditional cases, where estimation relies on\ncovariates, existing methods may require computationally expensive GPD fitting\nfor different observations. This computational burden becomes even more\nproblematic as the volume of observations increases, sometimes approaching\ninfinity. To address this issue, we propose an interpolation-based algorithm\nnamed EMI. EMI facilitates the online prediction of extreme conditional\nquantiles with finite offline observations. Combining quantile regression and\nGPD-based extrapolation, EMI formulates as a bilevel programming problem,\nefficiently solvable using classic optimization methods. Once estimates for\noffline observations are obtained, EMI employs B-spline interpolation for\ncovariate-dependent variables, enabling estimation for online observations with\nfinite GPD fitting. Simulations and real data analysis demonstrate the\neffectiveness of EMI across various scenarios."}, "http://arxiv.org/abs/2311.13897": {"title": "Super-resolution capacity of variance-based stochastic fluorescence microscopy", "link": "http://arxiv.org/abs/2311.13897", "description": "Improving the resolution of fluorescence microscopy beyond the diffraction\nlimit can be achievedby acquiring and processing multiple images of the sample\nunder different illumination conditions.One of the simplest techniques, Random\nIllumination Microscopy (RIM), forms the super-resolvedimage from the variance\nof images obtained with random speckled illuminations. However, thevalidity of\nthis process has not been fully theorized. In this work, we characterize\nmathematicallythe sample information contained in the variance of\ndiffraction-limited speckled images as a functionof the statistical properties\nof the illuminations. We show that an unambiguous two-fold resolutiongain is\nobtained when the speckle correlation length coincides with the width of the\nobservationpoint spread function. Last, we analyze the difference between the\nvariance-based techniques usingrandom speckled illuminations (as in RIM) and\nthose obtained using random fluorophore activation(as in Super-resolution\nOptical Fluctuation Imaging, SOFI)."}, "http://arxiv.org/abs/2311.13911": {"title": "Identifying Important Pairwise Logratios in Compositional Data with Sparse Principal Component Analysis", "link": "http://arxiv.org/abs/2311.13911", "description": "Compositional data are characterized by the fact that their elemental\ninformation is contained in simple pairwise logratios of the parts that\nconstitute the composition. While pairwise logratios are typically easy to\ninterpret, the number of possible pairs to consider quickly becomes (too) large\neven for medium-sized compositions, which might hinder interpretability in\nfurther multivariate analyses. Sparse methods can therefore be useful to\nidentify few, important pairwise logratios (respectively parts contained in\nthem) from the total candidate set. To this end, we propose a procedure based\non the construction of all possible pairwise logratios and employ sparse\nprincipal component analysis to identify important pairwise logratios. The\nperformance of the procedure is demonstrated both with simulated and real-world\ndata. In our empirical analyses, we propose three visual tools showing (i) the\nbalance between sparsity and explained variability, (ii) stability of the\npairwise logratios, and (iii) importance of the original compositional parts to\naid practitioners with their model interpretation."}, "http://arxiv.org/abs/2311.13923": {"title": "Optimal $F$-score Clustering for Bipartite Record Linkage", "link": "http://arxiv.org/abs/2311.13923", "description": "Probabilistic record linkage is often used to match records from two files,\nin particular when the variables common to both files comprise imperfectly\nmeasured identifiers like names and demographic variables. We consider\nbipartite record linkage settings in which each entity appears at most once\nwithin a file, i.e., there are no duplicates within the files, but some\nentities appear in both files. In this setting, the analyst desires a point\nestimate of the linkage structure that matches each record to at most one\nrecord from the other file. We propose an approach for obtaining this point\nestimate by maximizing the expected $F$-score for the linkage structure. We\ntarget the approach for record linkage methods that produce either (an\napproximate) posterior distribution of the unknown linkage structure or\nprobabilities of matches for record pairs. Using simulations and applications\nwith genuine data, we illustrate that the $F$-score estimators can lead to\nsensible estimates of the linkage structure."}, "http://arxiv.org/abs/2311.13935": {"title": "An analysis of the fragmentation of observing time at the Muztagh-ata site", "link": "http://arxiv.org/abs/2311.13935", "description": "Cloud cover plays a pivotal role in assessing observational conditions for\nastronomical site-testing. Except for the fraction of observing time, its\nfragmentation also wields a significant influence on the quality of nighttime\nsky clarity. In this article, we introduce the function Gamma, designed to\ncomprehensively capture both the fraction of available observing time and its\ncontinuity. Leveraging in situ measurement data gathered at the Muztagh-ata\nsite between 2017 and 2021, we showcase the effectiveness of our approach. The\nstatistical result illustrates that the Muztagh-ata site affords approximately\n122 nights of absolute clear and 205 very good nights annually, corresponding\nto Gamma greater than or equal 0.9 and Gamma greater than or equal 0.36\nrespectively."}, "http://arxiv.org/abs/2311.14042": {"title": "Optimized Covariance Design for AB Test on Social Network under Interference", "link": "http://arxiv.org/abs/2311.14042", "description": "Online A/B tests have become increasingly popular and important for social\nplatforms. However, accurately estimating the global average treatment effect\n(GATE) has proven to be challenging due to network interference, which violates\nthe Stable Unit Treatment Value Assumption (SUTVA) and poses a great challenge\nto experimental design. Existing network experimental design research was\nmostly based on the unbiased Horvitz-Thompson (HT) estimator with substantial\ndata trimming to ensure unbiasedness at the price of high resultant estimation\nvariance. In this paper, we strive to balance the bias and variance in\ndesigning randomized network experiments. Under a potential outcome model with\n1-hop interference, we derive the bias and variance of the standard HT\nestimator and reveal their relation to the network topological structure and\nthe covariance of the treatment assignment vector. We then propose to formulate\nthe experimental design problem to optimize the covariance matrix of the\ntreatment assignment vector to achieve the bias and variance balance by\nminimizing a well-crafted upper bound of the mean squared error (MSE) of the\nestimator, which allows us to decouple the unknown interference effect\ncomponent and the experimental design component. An efficient projected\ngradient descent algorithm is presented to implement the desired randomization\nscheme. Finally, we carry out extensive simulation studies 2 to demonstrate the\nadvantages of our proposed method over other existing methods in many settings,\nwith different levels of model misspecification."}, "http://arxiv.org/abs/2311.14054": {"title": "Analysis of Active/Inactive Patterns in the NHANES Data using Generalized Multilevel Functional Principal Component Analysis", "link": "http://arxiv.org/abs/2311.14054", "description": "Between 2011 and 2014 NHANES collected objectively measured physical activity\ndata using wrist-worn accelerometers for tens of thousands of individuals for\nup to seven days. Here we analyze the minute-level indicators of being active,\nwhich can be viewed as binary (because there is an active indicator at every\nminute), multilevel (because there are multiple days of data for each study\nparticipant), functional (because within-day data can be viewed as a function\nof time) data. To extract within- and between-participant directions of\nvariation in the data, we introduce Generalized Multilevel Functional Principal\nComponent Analysis (GM-FPCA), an approach based on the dimension reduction of\nthe linear predictor. Scores associated with specific patterns of activity are\nshown to be strongly associated with time to death. In particular, we confirm\nthat increased activity is associated with time to death, a result that has\nbeen reported on other data sets. In addition, our method shows the previously\nunreported finding that maintaining a consistent day-to-day routine is strongly\nassociated with a reduced risk of mortality (p-value $&lt; 0.001$) even after\nadjusting for traditional risk factors. Extensive simulation studies indicate\nthat GM-FPCA provides accurate estimation of model parameters, is\ncomputationally stable, and is scalable in the number of study participants,\nvisits, and observations within visits. R code for implementing the method is\nprovided."}, "http://arxiv.org/abs/2311.14122": {"title": "Decompositions of the mean continuous ranked probability score", "link": "http://arxiv.org/abs/2311.14122", "description": "The continuous ranked probability score (crps) is the most commonly used\nscoring rule in the evaluation of probabilistic forecasts for real-valued\noutcomes. To assess and rank forecasting methods, researchers compute the mean\ncrps over given sets of forecast situations, based on the respective predictive\ndistributions and outcomes. We propose a new, isotonicity-based decomposition\nof the mean crps into interpretable components that quantify miscalibration\n(MSC), discrimination ability (DSC), and uncertainty (UNC), respectively. In a\ndetailed theoretical analysis, we compare the new approach to empirical\ndecompositions proposed earlier, generalize to population versions, analyse\ntheir properties and relationships, and relate to a hierarchy of notions of\ncalibration. The isotonicity-based decomposition guarantees the nonnegativity\nof the components and quantifies calibration in a sense that is stronger than\nfor other types of decompositions, subject to the nondegeneracy of empirical\ndecompositions. We illustrate the usage of the isotonicity-based decomposition\nin case studies from weather prediction and machine learning."}, "http://arxiv.org/abs/2311.14204": {"title": "Reproducible Aggregation of Sample-Split Statistics", "link": "http://arxiv.org/abs/2311.14204", "description": "Statistical inference is often simplified by sample-splitting. This\nsimplification comes at the cost of the introduction of randomness that is not\nnative to the data. We propose a simple procedure for sequentially aggregating\nstatistics constructed with multiple splits of the same sample. The user\nspecifies a bound and a nominal error rate. If the procedure is implemented\ntwice on the same data, the nominal error rate approximates the chance that the\nresults differ by more than the bound. We provide a non-asymptotic analysis of\nthe accuracy of the nominal error rate and illustrate the application of the\nprocedure to several widely applied statistical methods."}, "http://arxiv.org/abs/2311.14212": {"title": "Annotation Sensitivity: Training Data Collection Methods Affect Model Performance", "link": "http://arxiv.org/abs/2311.14212", "description": "When training data are collected from human annotators, the design of the\nannotation instrument, the instructions given to annotators, the\ncharacteristics of the annotators, and their interactions can impact training\ndata. This study demonstrates that design choices made when creating an\nannotation instrument also impact the models trained on the resulting\nannotations.\n\nWe introduce the term annotation sensitivity to refer to the impact of\nannotation data collection methods on the annotations themselves and on\ndownstream model performance and predictions.\n\nWe collect annotations of hate speech and offensive language in five\nexperimental conditions of an annotation instrument, randomly assigning\nannotators to conditions. We then fine-tune BERT models on each of the five\nresulting datasets and evaluate model performance on a holdout portion of each\ncondition. We find considerable differences between the conditions for 1) the\nshare of hate speech/offensive language annotations, 2) model performance, 3)\nmodel predictions, and 4) model learning curves.\n\nOur results emphasize the crucial role played by the annotation instrument\nwhich has received little attention in the machine learning literature. We call\nfor additional research into how and why the instrument impacts the annotations\nto inform the development of best practices in instrument design."}, "http://arxiv.org/abs/2311.14220": {"title": "Assumption-lean and Data-adaptive Post-Prediction Inference", "link": "http://arxiv.org/abs/2311.14220", "description": "A primary challenge facing modern scientific research is the limited\navailability of gold-standard data which can be both costly and labor-intensive\nto obtain. With the rapid development of machine learning (ML), scientists have\nrelied on ML algorithms to predict these gold-standard outcomes with easily\nobtained covariates. However, these predicted outcomes are often used directly\nin subsequent statistical analyses, ignoring imprecision and heterogeneity\nintroduced by the prediction procedure. This will likely result in false\npositive findings and invalid scientific conclusions. In this work, we\nintroduce an assumption-lean and data-adaptive Post-Prediction Inference\n(POP-Inf) procedure that allows valid and powerful inference based on\nML-predicted outcomes. Its \"assumption-lean\" property guarantees reliable\nstatistical inference without assumptions on the ML-prediction, for a wide\nrange of statistical quantities. Its \"data-adaptive'\" feature guarantees an\nefficiency gain over existing post-prediction inference methods, regardless of\nthe accuracy of ML-prediction. We demonstrate the superiority and applicability\nof our method through simulations and large-scale genomic data."}, "http://arxiv.org/abs/2311.14356": {"title": "Lagged coherence: explicit and testable definition", "link": "http://arxiv.org/abs/2311.14356", "description": "Measures of association between cortical regions based on activity signals\nprovide useful information for studying brain functional connectivity.\nDifficulties occur with signals of electric neuronal activity, where an\nobserved signal is a mixture, i.e. an instantaneous weighted average of the\ntrue, unobserved signals from all regions, due to volume conduction and low\nspatial resolution. This is why measures of lagged association are of interest,\nsince at least theoretically, \"lagged association\" is of physiological origin.\nIn contrast, the actual physiological instantaneous zero-lag association is\nmasked and confounded by the mixing artifact. A minimum requirement for a\nmeasure of lagged association is that it must not tend to zero with an increase\nof strength of true instantaneous physiological association. Such biased\nmeasures cannot tell apart if a change in its value is due to a change in\nlagged or a change in instantaneous association. An explicit testable\ndefinition for frequency domain lagged connectivity between two multivariate\ntime series is proposed. It is endowed with two important properties: it is\ninvariant to non-singular linear transformations of each vector time series\nseparately, and it is invariant to instantaneous association. As a sanity\ncheck, in the case of two univariate time series, the new definition leads back\nto the bivariate lagged coherence of 2007 (eqs 25 and 26 in\nhttps://doi.org/10.48550/arXiv.<a href=\"https://export.arxiv.org/abs/0706.1776\">0706.1776</a>)."}, "http://arxiv.org/abs/2311.14367": {"title": "Cultural data integration via random graphical modelling", "link": "http://arxiv.org/abs/2311.14367", "description": "Cultural values vary significantly around the world. Despite a large\nheterogeneity, similarities across national cultures are to be expected. This\npaper studies cross-country culture heterogeneity via the joint inference of\ncopula graphical models. To this end, a random graph generative model is\nintroduced, with a latent space that embeds cultural relatedness across\ncountries. Taking world-wide country-specific survey data as the primary source\nof information, the modelling framework allows to integrate external data, both\nat the level of cultural traits and of their interdependence. In this way, we\nare able to identify several dimensions of culture."}, "http://arxiv.org/abs/2311.14412": {"title": "A Comparison of PDF Projection with Normalizing Flows and SurVAE", "link": "http://arxiv.org/abs/2311.14412", "description": "Normalizing flows (NF) recently gained attention as a way to construct\ngenerative networks with exact likelihood calculation out of composable layers.\nHowever, NF is restricted to dimension-preserving transformations. Surjection\nVAE (SurVAE) has been proposed to extend NF to dimension-altering\ntransformations. Such networks are desirable because they are expressive and\ncan be precisely trained. We show that the approaches are a re-invention of PDF\nprojection, which appeared over twenty years earlier and is much further\ndeveloped."}, "http://arxiv.org/abs/2311.14424": {"title": "Exact confidence intervals for the mixing distribution from binomial mixture distribution samples", "link": "http://arxiv.org/abs/2311.14424", "description": "We present methodology for constructing pointwise confidence intervals for\nthe cumulative distribution function and the quantiles of mixing distributions\non the unit interval from binomial mixture distribution samples. No assumptions\nare made on the shape of the mixing distribution. The confidence intervals are\nconstructed by inverting exact tests of composite null hypotheses regarding the\nmixing distribution. Our method may be applied to any deconvolution approach\nthat produces test statistics whose distribution is stochastically monotone for\nstochastic increase of the mixing distribution. We propose a hierarchical Bayes\napproach, which uses finite Polya Trees for modelling the mixing distribution,\nthat provides stable and accurate deconvolution estimates without the need for\nadditional tuning parameters. Our main technical result establishes the\nstochastic monotonicity property of the test statistics produced by the\nhierarchical Bayes approach. Leveraging the need for the stochastic\nmonotonicity property, we explicitly derive the smallest asymptotic confidence\nintervals that may be constructed using our methodology. Raising the question\nwhether it is possible to construct smaller confidence intervals for the mixing\ndistribution without making parametric assumptions on its shape."}, "http://arxiv.org/abs/2311.14487": {"title": "Reconciliation of expert priors for quantities and events and application within the probabilistic Delphi method", "link": "http://arxiv.org/abs/2311.14487", "description": "We consider the problem of aggregating the judgements of a group of experts\nto form a single prior distribution representing the judgements of the group.\nWe develop a Bayesian hierarchical model to reconcile the judgements of the\ngroup of experts based on elicited quantiles for continuous quantities and\nprobabilities for one-off events. Previous Bayesian reconciliation methods have\nnot been used widely, if at all, in contrast to pooling methods and\nconsensus-based approaches. To address this we embed Bayesian reconciliation\nwithin the probabilistic Delphi method. The result is to furnish the outcome of\nthe probabilistic Delphi method with a direct probabilistic interpretation,\nwith the resulting prior representing the judgements of the decision maker. We\ncan use the rationales from the Delphi process to group the experts for the\nhierarchical modelling. We illustrate the approach with applications to studies\nevaluating erosion in embankment dams and pump failures in a water pumping\nstation, and assess the properties of the approach using the TU Delft database\nof expert judgement studies. We see that, even using an off-the-shelf\nimplementation of the approach, it out-performs individual experts, equal\nweighting of experts and the classical method based on the log score."}, "http://arxiv.org/abs/2311.14502": {"title": "Informed Random Partition Models with Temporal Dependence", "link": "http://arxiv.org/abs/2311.14502", "description": "Model-based clustering is a powerful tool that is often used to discover\nhidden structure in data by grouping observational units that exhibit similar\nresponse values. Recently, clustering methods have been developed that permit\nincorporating an ``initial'' partition informed by expert opinion. Then, using\nsome similarity criteria, partitions different from the initial one are down\nweighted, i.e. they are assigned reduced probabilities. These methods represent\nan exciting new direction of method development in clustering techniques. We\nadd to this literature a method that very flexibly permits assigning varying\nlevels of uncertainty to any subset of the partition. This is particularly\nuseful in practice as there is rarely clear prior information with regards to\nthe entire partition. Our approach is not based on partition penalties but\nconsiders individual allocation probabilities for each unit (e.g., locally\nweighted prior information). We illustrate the gains in prior specification\nflexibility via simulation studies and an application to a dataset concerning\nspatio-temporal evolution of ${\\rm PM}_{10}$ measurements in Germany."}, "http://arxiv.org/abs/2311.14655": {"title": "A Sparse Factor Model for Clustering High-Dimensional Longitudinal Data", "link": "http://arxiv.org/abs/2311.14655", "description": "Recent advances in engineering technologies have enabled the collection of a\nlarge number of longitudinal features. This wealth of information presents\nunique opportunities for researchers to investigate the complex nature of\ndiseases and uncover underlying disease mechanisms. However, analyzing such\nkind of data can be difficult due to its high dimensionality, heterogeneity and\ncomputational challenges. In this paper, we propose a Bayesian nonparametric\nmixture model for clustering high-dimensional mixed-type (e.g., continuous,\ndiscrete and categorical) longitudinal features. We employ a sparse factor\nmodel on the joint distribution of random effects and the key idea is to induce\nclustering at the latent factor level instead of the original data to escape\nthe curse of dimensionality. The number of clusters is estimated through a\nDirichlet process prior. An efficient Gibbs sampler is developed to estimate\nthe posterior distribution of the model parameters. Analysis of real and\nsimulated data is presented and discussed. Our study demonstrates that the\nproposed model serves as a useful analytical tool for clustering\nhigh-dimensional longitudinal data."}, "http://arxiv.org/abs/2104.10618": {"title": "Multiple conditional randomization tests for lagged and spillover treatment effects", "link": "http://arxiv.org/abs/2104.10618", "description": "We consider the problem of constructing multiple conditional randomization\ntests. They may test different causal hypotheses but always aim to be nearly\nindependent, allowing the randomization p-values to be interpreted individually\nand combined using standard methods. We start with a simple, sequential\nconstruction of such tests, and then discuss its application to three problems:\nevidence factors for observational studies, lagged treatment effect in\nstepped-wedge trials, and spillover effect in randomized trials with\ninterference. We compare the proposed approach with some existing methods using\nsimulated and real datasets. Finally, we establish a general sufficient\ncondition for constructing multiple nearly independent conditional\nrandomization tests."}, "http://arxiv.org/abs/2109.02487": {"title": "Robust Narrowest Significance Pursuit: Inference for multiple change-points in the median", "link": "http://arxiv.org/abs/2109.02487", "description": "We propose Robust Narrowest Significance Pursuit (RNSP), a methodology for\ndetecting localized regions in data sequences which each must contain a\nchange-point in the median, at a prescribed global significance level. RNSP\nworks by fitting the postulated constant model over many regions of the data\nusing a new sign-multiresolution sup-norm-type loss, and greedily identifying\nthe shortest intervals on which the constancy is significantly violated. By\nworking with the signs of the data around fitted model candidates, RNSP fulfils\nits coverage promises under minimal assumptions, requiring only sign-symmetry\nand serial independence of the signs of the true residuals. In particular, it\npermits their heterogeneity and arbitrarily heavy tails. The intervals of\nsignificance returned by RNSP have a finite-sample character, are unconditional\nin nature and do not rely on any assumptions on the true signal. Code\nimplementing RNSP is available at https://github.com/pfryz/nsp."}, "http://arxiv.org/abs/2111.12720": {"title": "Machine learning assisted Bayesian model comparison: learnt harmonic mean estimator", "link": "http://arxiv.org/abs/2111.12720", "description": "We resurrect the infamous harmonic mean estimator for computing the marginal\nlikelihood (Bayesian evidence) and solve its problematic large variance. The\nmarginal likelihood is a key component of Bayesian model selection to evaluate\nmodel posterior probabilities; however, its computation is challenging. The\noriginal harmonic mean estimator, first proposed by Newton and Raftery in 1994,\ninvolves computing the harmonic mean of the likelihood given samples from the\nposterior. It was immediately realised that the original estimator can fail\ncatastrophically since its variance can become very large (possibly not\nfinite). A number of variants of the harmonic mean estimator have been proposed\nto address this issue although none have proven fully satisfactory. We present\nthe \\emph{learnt harmonic mean estimator}, a variant of the original estimator\nthat solves its large variance problem. This is achieved by interpreting the\nharmonic mean estimator as importance sampling and introducing a new target\ndistribution. The new target distribution is learned to approximate the optimal\nbut inaccessible target, while minimising the variance of the resulting\nestimator. Since the estimator requires samples of the posterior only, it is\nagnostic to the sampling strategy used. We validate the estimator on a variety\nof numerical experiments, including a number of pathological examples where the\noriginal harmonic mean estimator fails catastrophically. We also consider a\ncosmological application, where our approach leads to $\\sim$ 3 to 6 times more\nsamples than current state-of-the-art techniques in 1/3 of the time. In all\ncases our learnt harmonic mean estimator is shown to be highly accurate. The\nestimator is computationally scalable and can be applied to problems of\ndimension $O(10^3)$ and beyond. Code implementing the learnt harmonic mean\nestimator is made publicly available"}, "http://arxiv.org/abs/2205.07378": {"title": "Proximal MCMC for Bayesian Inference of Constrained and Regularized Estimation", "link": "http://arxiv.org/abs/2205.07378", "description": "This paper advocates proximal Markov Chain Monte Carlo (ProxMCMC) as a\nflexible and general Bayesian inference framework for constrained or\nregularized estimation. Originally introduced in the Bayesian imaging\nliterature, ProxMCMC employs the Moreau-Yosida envelope for a smooth\napproximation of the total-variation regularization term, fixes variance and\nregularization strength parameters as constants, and uses the Langevin\nalgorithm for the posterior sampling. We extend ProxMCMC to be fully Bayesian\nby providing data-adaptive estimation of all parameters including the\nregularization strength parameter. More powerful sampling algorithms such as\nHamiltonian Monte Carlo are employed to scale ProxMCMC to high-dimensional\nproblems. Analogous to the proximal algorithms in optimization, ProxMCMC offers\na versatile and modularized procedure for conducting statistical inference on\nconstrained and regularized problems. The power of ProxMCMC is illustrated on\nvarious statistical estimation and machine learning tasks, the inference of\nwhich is traditionally considered difficult from both frequentist and Bayesian\nperspectives."}, "http://arxiv.org/abs/2211.10032": {"title": "Modular Regression: Improving Linear Models by Incorporating Auxiliary Data", "link": "http://arxiv.org/abs/2211.10032", "description": "This paper develops a new framework, called modular regression, to utilize\nauxiliary information -- such as variables other than the original features or\nadditional data sets -- in the training process of linear models. At a high\nlevel, our method follows the routine: (i) decomposing the regression task into\nseveral sub-tasks, (ii) fitting the sub-task models, and (iii) using the\nsub-task models to provide an improved estimate for the original regression\nproblem. This routine applies to widely-used low-dimensional (generalized)\nlinear models and high-dimensional regularized linear regression. It also\nnaturally extends to missing-data settings where only partial observations are\navailable. By incorporating auxiliary information, our approach improves the\nestimation efficiency and prediction accuracy upon linear regression or the\nLasso under a conditional independence assumption for predicting the outcome.\nFor high-dimensional settings, we develop an extension of our procedure that is\nrobust to violations of the conditional independence assumption, in the sense\nthat it improves efficiency if this assumption holds and coincides with the\nLasso otherwise. We demonstrate the efficacy of our methods with simulated and\nreal data sets."}, "http://arxiv.org/abs/2211.13478": {"title": "A New Spatio-Temporal Model Exploiting Hamiltonian Equations", "link": "http://arxiv.org/abs/2211.13478", "description": "The solutions of Hamiltonian equations are known to describe the underlying\nphase space of the mechanical system. Hamiltonian Monte Carlo is the sole use\nof the properties of solutions to the Hamiltonian equations in Bayesian\nstatistics. In this article, we propose a novel spatio-temporal model using a\nstrategic modification of the Hamiltonian equations, incorporating appropriate\nstochasticity via Gaussian processes. The resultant sptaio-temporal process,\ncontinuously varying with time, turns out to be nonparametric, nonstationary,\nnonseparable and non-Gaussian. Additionally, as the spatio-temporal lag goes to\ninfinity, the lagged correlations converge to zero. We investigate the\ntheoretical properties of the new spatio-temporal process, including its\ncontinuity and smoothness properties. In the Bayesian paradigm, we derive\nmethods for complete Bayesian inference using MCMC techniques. The performance\nof our method has been compared with that of non-stationary Gaussian process\n(GP) using two simulation studies, where our method shows a significant\nimprovement over the non-stationary GP. Further, application of our new model\nto two real data sets revealed encouraging performance."}, "http://arxiv.org/abs/2212.08968": {"title": "Covariate Adjustment in Bayesian Adaptive Randomized Controlled Trials", "link": "http://arxiv.org/abs/2212.08968", "description": "In conventional randomized controlled trials, adjustment for baseline values\nof covariates known to be at least moderately associated with the outcome\nincreases the power of the trial. Recent work has shown particular benefit for\nmore flexible frequentist designs, such as information adaptive and adaptive\nmulti-arm designs. However, covariate adjustment has not been characterized\nwithin the more flexible Bayesian adaptive designs, despite their growing\npopularity. We focus on a subclass of these which allow for early stopping at\nan interim analysis given evidence of treatment superiority. We consider both\ncollapsible and non-collapsible estimands, and show how to obtain posterior\nsamples of marginal estimands from adjusted analyses. We describe several\nestimands for three common outcome types. We perform a simulation study to\nassess the impact of covariate adjustment using a variety of adjustment models\nin several different scenarios. This is followed by a real world application of\nthe compared approaches to a COVID-19 trial with a binary endpoint. For all\nscenarios, it is shown that covariate adjustment increases power and the\nprobability of stopping the trials early, and decreases the expected sample\nsizes as compared to unadjusted analyses."}, "http://arxiv.org/abs/2301.11873": {"title": "A Deep Learning Method for Comparing Bayesian Hierarchical Models", "link": "http://arxiv.org/abs/2301.11873", "description": "Bayesian model comparison (BMC) offers a principled approach for assessing\nthe relative merits of competing computational models and propagating\nuncertainty into model selection decisions. However, BMC is often intractable\nfor the popular class of hierarchical models due to their high-dimensional\nnested parameter structure. To address this intractability, we propose a deep\nlearning method for performing BMC on any set of hierarchical models which can\nbe instantiated as probabilistic programs. Since our method enables amortized\ninference, it allows efficient re-estimation of posterior model probabilities\nand fast performance validation prior to any real-data application. In a series\nof extensive validation studies, we benchmark the performance of our method\nagainst the state-of-the-art bridge sampling method and demonstrate excellent\namortized inference across all BMC settings. We then showcase our method by\ncomparing four hierarchical evidence accumulation models that have previously\nbeen deemed intractable for BMC due to partly implicit likelihoods.\nAdditionally, we demonstrate how transfer learning can be leveraged to enhance\ntraining efficiency. We provide reproducible code for all analyses and an\nopen-source implementation of our method."}, "http://arxiv.org/abs/2304.04124": {"title": "Nonparametric Confidence Intervals for Generalized Lorenz Curve using Modified Empirical Likelihood", "link": "http://arxiv.org/abs/2304.04124", "description": "The Lorenz curve portrays the inequality of income distribution. In this\narticle, we develop three modified empirical likelihood (EL) approaches\nincluding adjusted empirical likelihood, transformed empirical likelihood, and\ntransformed adjusted empirical likelihood to construct confidence intervals for\nthe generalized Lorenz ordinate. We have shown that the limiting distribution\nof the modified EL ratio statistics for the generalized Lorenz ordinate follows\nthe scaled Chi-Squared distributions with one degree of freedom. The coverage\nprobabilities and mean lengths of confidence intervals are compared of the\nproposed methods with the traditional EL method through simulations under\nvarious scenarios. Finally, the proposed methods are illustrated using a real\ndata application to construct confidence intervals."}, "http://arxiv.org/abs/2306.04702": {"title": "Efficient sparsity adaptive changepoint estimation", "link": "http://arxiv.org/abs/2306.04702", "description": "We propose a new, computationally efficient, sparsity adaptive changepoint\nestimator for detecting changes in unknown subsets of a high-dimensional data\nsequence. Assuming the data sequence is Gaussian, we prove that the new method\nsuccessfully estimates the number and locations of changepoints with a given\nerror rate and under minimal conditions, for all sparsities of the changing\nsubset. Moreover, our method has computational complexity linear up to\nlogarithmic factors in both the length and number of time series, making it\napplicable to large data sets. Through extensive numerical studies we show that\nthe new methodology is highly competitive in terms of both estimation accuracy\nand computational cost. The practical usefulness of the method is illustrated\nby analysing sensor data from a hydro power plant. An efficient R\nimplementation is available."}, "http://arxiv.org/abs/2306.07119": {"title": "Improving Forecasts for Heterogeneous Time Series by \"Averaging\", with Application to Food Demand Forecast", "link": "http://arxiv.org/abs/2306.07119", "description": "A common forecasting setting in real world applications considers a set of\npossibly heterogeneous time series of the same domain. Due to different\nproperties of each time series such as length, obtaining forecasts for each\nindividual time series in a straight-forward way is challenging. This paper\nproposes a general framework utilizing a similarity measure in Dynamic Time\nWarping to find similar time series to build neighborhoods in a k-Nearest\nNeighbor fashion, and improve forecasts of possibly simple models by averaging.\nSeveral ways of performing the averaging are suggested, and theoretical\narguments underline the usefulness of averaging for forecasting. Additionally,\ndiagnostics tools are proposed allowing a deep understanding of the procedure."}, "http://arxiv.org/abs/2307.07898": {"title": "A Graph-Prediction-Based Approach for Debiasing Underreported Data", "link": "http://arxiv.org/abs/2307.07898", "description": "We present a novel Graph-based debiasing Algorithm for Underreported Data\n(GRAUD) aiming at an efficient joint estimation of event counts and discovery\nprobabilities across spatial or graphical structures. This innovative method\nprovides a solution to problems seen in fields such as policing data and\nCOVID-$19$ data analysis. Our approach avoids the need for strong priors\ntypically associated with Bayesian frameworks. By leveraging the graph\nstructures on unknown variables $n$ and $p$, our method debiases the\nunder-report data and estimates the discovery probability at the same time. We\nvalidate the effectiveness of our method through simulation experiments and\nillustrate its practicality in one real-world application: police 911\ncalls-to-service data."}, "http://arxiv.org/abs/2307.12832": {"title": "More Power by using Fewer Permutations", "link": "http://arxiv.org/abs/2307.12832", "description": "It is conventionally believed that a permutation test should ideally use all\npermutations. If this is computationally unaffordable, it is believed one\nshould use the largest affordable Monte Carlo sample or (algebraic) subgroup of\npermutations. We challenge this belief by showing we can sometimes obtain\ndramatically more power by using a tiny subgroup. As the subgroup is tiny, this\nsimultaneously comes at a much lower computational cost. We exploit this to\nimprove the popular permutation-based Westfall &amp; Young MaxT multiple testing\nmethod. We study the relative efficiency in a Gaussian location model, and find\nthe largest gain in high dimensions."}, "http://arxiv.org/abs/2309.14156": {"title": "Designing and evaluating an online reinforcement learning agent for physical exercise recommendations in N-of-1 trials", "link": "http://arxiv.org/abs/2309.14156", "description": "Personalized adaptive interventions offer the opportunity to increase patient\nbenefits, however, there are challenges in their planning and implementation.\nOnce implemented, it is an important question whether personalized adaptive\ninterventions are indeed clinically more effective compared to a fixed gold\nstandard intervention. In this paper, we present an innovative N-of-1 trial\nstudy design testing whether implementing a personalized intervention by an\nonline reinforcement learning agent is feasible and effective. Throughout, we\nuse a new study on physical exercise recommendations to reduce pain in\nendometriosis for illustration. We describe the design of a contextual bandit\nrecommendation agent and evaluate the agent in simulation studies. The results\nshow that, first, implementing a personalized intervention by an online\nreinforcement learning agent is feasible. Second, such adaptive interventions\nhave the potential to improve patients' benefits even if only few observations\nare available. As one challenge, they add complexity to the design and\nimplementation process. In order to quantify the expected benefit, data from\nprevious interventional studies is required. We expect our approach to be\ntransferable to other interventions and clinical interventions."}, "http://arxiv.org/abs/2311.14698": {"title": "Business Policy Experiments using Fractional Factorial Designs: Consumer Retention on DoorDash", "link": "http://arxiv.org/abs/2311.14698", "description": "This paper investigates an approach to both speed up business decision-making\nand lower the cost of learning through experimentation by factorizing business\npolicies and employing fractional factorial experimental designs for their\nevaluation. We illustrate how this method integrates with advances in the\nestimation of heterogeneous treatment effects, elaborating on its advantages\nand foundational assumptions. We empirically demonstrate the implementation and\nbenefits of our approach and assess its validity in evaluating consumer\npromotion policies at DoorDash, which is one of the largest delivery platforms\nin the US. Our approach discovers a policy with 5% incremental profit at 67%\nlower implementation cost."}, "http://arxiv.org/abs/2311.14766": {"title": "Reinforcement Learning from Statistical Feedback: the Journey from AB Testing to ANT Testing", "link": "http://arxiv.org/abs/2311.14766", "description": "Reinforcement Learning from Human Feedback (RLHF) has played a crucial role\nin the success of large models such as ChatGPT. RLHF is a reinforcement\nlearning framework which combines human feedback to improve learning\neffectiveness and performance. However, obtaining preferences feedback manually\nis quite expensive in commercial applications. Some statistical commercial\nindicators are usually more valuable and always ignored in RLHF. There exists a\ngap between commercial target and model training. In our research, we will\nattempt to fill this gap with statistical business feedback instead of human\nfeedback, using AB testing which is a well-established statistical method.\nReinforcement Learning from Statistical Feedback (RLSF) based on AB testing is\nproposed. Statistical inference methods are used to obtain preferences for\ntraining the reward network, which fine-tunes the pre-trained model in\nreinforcement learning framework, achieving greater business value.\nFurthermore, we extend AB testing with double selections at a single time-point\nto ANT testing with multiple selections at different feedback time points.\nMoreover, we design numerical experiences to validate the effectiveness of our\nalgorithm framework."}, "http://arxiv.org/abs/2311.14846": {"title": "Fast Estimation of the Renshaw-Haberman Model and Its Variants", "link": "http://arxiv.org/abs/2311.14846", "description": "In mortality modelling, cohort effects are often taken into consideration as\nthey add insights about variations in mortality across different generations.\nStatistically speaking, models such as the Renshaw-Haberman model may provide a\nbetter fit to historical data compared to their counterparts that incorporate\nno cohort effects. However, when such models are estimated using an iterative\nmaximum likelihood method in which parameters are updated one at a time,\nconvergence is typically slow and may not even be reached within a reasonably\nestablished maximum number of iterations. Among others, the slow convergence\nproblem hinders the study of parameter uncertainty through bootstrapping\nmethods.\n\nIn this paper, we propose an intuitive estimation method that minimizes the\nsum of squared errors between actual and fitted log central death rates. The\ncomplications arising from the incorporation of cohort effects are overcome by\nformulating part of the optimization as a principal component analysis with\nmissing values. We also show how the proposed method can be generalized to\nvariants of the Renshaw-Haberman model with further computational improvement,\neither with a simplified model structure or an additional constraint. Using\nmortality data from the Human Mortality Database (HMD), we demonstrate that our\nproposed method produces satisfactory estimation results and is significantly\nmore efficient compared to the traditional likelihood-based approach."}, "http://arxiv.org/abs/2311.14867": {"title": "Disaggregating Time-Series with Many Indicators: An Overview of the DisaggregateTS Package", "link": "http://arxiv.org/abs/2311.14867", "description": "Low-frequency time-series (e.g., quarterly data) are often treated as\nbenchmarks for interpolating to higher frequencies, since they generally\nexhibit greater precision and accuracy in contrast to their high-frequency\ncounterparts (e.g., monthly data) reported by governmental bodies. An array of\nregression-based methods have been proposed in the literature which aim to\nestimate a target high-frequency series using higher frequency indicators.\nHowever, in the era of big data and with the prevalence of large volume of\nadministrative data-sources there is a need to extend traditional methods to\nwork in high-dimensional settings, i.e. where the number of indicators is\nsimilar or larger than the number of low-frequency samples. The package\nDisaggregateTS includes both classical regressions-based disaggregation methods\nalongside recent extensions to high-dimensional settings, c.f. Mosley et al.\n(2022). This paper provides guidance on how to implement these methods via the\npackage in R, and demonstrates their use in an application to disaggregating\nCO2 emissions."}, "http://arxiv.org/abs/2311.14889": {"title": "Modern approaches for evaluating treatment effect heterogeneity from clinical trials and observational data", "link": "http://arxiv.org/abs/2311.14889", "description": "In this paper we review recent advances in statistical methods for the\nevaluation of the heterogeneity of treatment effects (HTE), including subgroup\nidentification and estimation of individualized treatment regimens, from\nrandomized clinical trials and observational studies. We identify several types\nof approaches using the features introduced in Lipkovich, Dmitrienko and\nD'Agostino (2017) that distinguish the recommended principled methods from\nbasic methods for HTE evaluation that typically rely on rules of thumb and\ngeneral guidelines (the methods are often referred to as common practices). We\ndiscuss the advantages and disadvantages of various principled methods as well\nas common measures for evaluating their performance. We use simulated data and\na case study based on a historical clinical trial to illustrate several new\napproaches to HTE evaluation."}, "http://arxiv.org/abs/2311.14894": {"title": "Kernel-based measures of association between inputs and outputs based on ANOVA", "link": "http://arxiv.org/abs/2311.14894", "description": "ANOVA decomposition of function with random input variables provides ANOVA\nfunctionals (AFs), which contain information about the contributions of the\ninput variables on the output variable(s). By embedding AFs into an appropriate\nreproducing kernel Hilbert space regarding their distributions, we propose an\nefficient statistical test of independence between the input variables and\noutput variable(s). The resulting test statistic leads to new dependent\nmeasures of association between inputs and outputs that allow for i) dealing\nwith any distribution of AFs, including the Cauchy distribution, ii) accounting\nfor the necessary or desirable moments of AFs and the interactions among the\ninput variables. In uncertainty quantification for mathematical models, a\nnumber of existing measures are special cases of this framework. We then\nprovide unified and general global sensitivity indices and their consistent\nestimators, including asymptotic distributions. For Gaussian-distributed AFs,\nwe obtain Sobol' indices and dependent generalized sensitivity indices using\nquadratic kernels."}, "http://arxiv.org/abs/2311.15012": {"title": "False Discovery Rate Controlling Procedures with BLOSUM62 substitution matrix and their application to HIV Data", "link": "http://arxiv.org/abs/2311.15012", "description": "Identifying significant sites in sequence data and analogous data is of\nfundamental importance in many biological fields. Fisher's exact test is a\npopular technique, however this approach to sparse count data is not\nappropriate due to conservative decisions. Since count data in HIV data are\ntypically very sparse, it is crucial to use additional information to\nstatistical models to improve testing power. In order to develop new approaches\nto incorporate biological information in the false discovery controlling\nprocedure, we propose two models: one based on the empirical Bayes model under\nindependence of amino acids and the other uses pairwise associations of amino\nacids based on Markov random field with on the BLOSUM62 substitution matrix. We\napply the proposed methods to HIV data and identify significant sites\nincorporating BLOSUM62 matrix while the traditional method based on Fisher's\ntest does not discover any site. These newly developed methods have the\npotential to handle many biological problems in the studies of vaccine and drug\ntrials and phenotype studies."}, "http://arxiv.org/abs/2311.15031": {"title": "Robust and Efficient Semi-supervised Learning for Ising Model", "link": "http://arxiv.org/abs/2311.15031", "description": "In biomedical studies, it is often desirable to characterize the interactive\nmode of multiple disease outcomes beyond their marginal risk. Ising model is\none of the most popular choices serving for this purpose. Nevertheless,\nlearning efficiency of Ising models can be impeded by the scarcity of accurate\ndisease labels, which is a prominent problem in contemporary studies driven by\nelectronic health records (EHR). Semi-supervised learning (SSL) leverages the\nlarge unlabeled sample with auxiliary EHR features to assist the learning with\nlabeled data only and is a potential solution to this issue. In this paper, we\ndevelop a novel SSL method for efficient inference of Ising model. Our method\nfirst models the outcomes against the auxiliary features, then uses it to\nproject the score function of the supervised estimator onto the EHR features,\nand incorporates the unlabeled sample to augment the supervised estimator for\nvariance reduction without introducing bias. For the key step of conditional\nmodeling, we propose strategies that can effectively leverage the auxiliary EHR\ninformation while maintaining moderate model complexity. In addition, we\nintroduce approaches including intrinsic efficient updates and ensemble, to\novercome the potential misspecification of the conditional model that may cause\nefficiency loss. Our method is justified by asymptotic theory and shown to\noutperform existing SSL methods through simulation studies. We also illustrate\nits utility in a real example about several key phenotypes related to frequent\nICU admission on MIMIC-III data set."}, "http://arxiv.org/abs/2311.15257": {"title": "Bayesian Imputation of Revolving Doors", "link": "http://arxiv.org/abs/2311.15257", "description": "Political scientists and sociologists study how individuals switch back and\nforth between public and private organizations, for example between regulator\nand lobbyist positions, a phenomenon called \"revolving doors\". However, they\nface an important issue of data missingness, as not all data relevant to this\nquestion is freely available. For example, the nomination of an individual in a\ngiven public-sector position of power might be publically disclosed, but not\ntheir subsequent positions in the private sector. In this article, we adopt a\nBayesian data augmentation strategy for discrete time series and propose\nmeasures of public-private mobility across the French state at large,\nmobilizing administrative and digital data. We relax homogeneity hypotheses of\ntraditional hidden Markov models and implement a version of a Markov switching\nmodel, which allows for varying parameters across individuals and time and\nauto-correlated behaviors. We describe how the revolving doors phenomenon\nvaries across the French state and how it has evolved between 1990 and 2022."}, "http://arxiv.org/abs/2311.15322": {"title": "False Discovery Rate Control For Structured Multiple Testing: Asymmetric Rules And Conformal Q-values", "link": "http://arxiv.org/abs/2311.15322", "description": "The effective utilization of structural information in data while ensuring\nstatistical validity poses a significant challenge in false discovery rate\n(FDR) analyses. Conformal inference provides rigorous theory for grounding\ncomplex machine learning methods without relying on strong assumptions or\nhighly idealized models. However, existing conformal methods have limitations\nin handling structured multiple testing. This is because their validity\nrequires the deployment of symmetric rules, which assume the exchangeability of\ndata points and permutation-invariance of fitting algorithms. To overcome these\nlimitations, we introduce the pseudo local index of significance (PLIS)\nprocedure, which is capable of accommodating asymmetric rules and requires only\npairwise exchangeability between the null conformity scores. We demonstrate\nthat PLIS offers finite-sample guarantees in FDR control and the ability to\nassign higher weights to relevant data points. Numerical results confirm the\neffectiveness and robustness of PLIS and show improvements in power compared to\nexisting model-free methods in various scenarios."}, "http://arxiv.org/abs/2311.15359": {"title": "Goodness-of-fit tests for the one-sided L\\'evy distribution based on quantile conditional moments", "link": "http://arxiv.org/abs/2311.15359", "description": "In this paper we introduce a novel statistical framework based on the first\ntwo quantile conditional moments that facilitates effective goodness-of-fit\ntesting for one-sided L\\'evy distributions. The scale-ratio framework\nintroduced in this paper extends our previous results in which we have shown\nhow to extract unique distribution features using conditional variance ratio\nfor the generic class of {\\alpha}-stable distributions. We show that the\nconditional moment-based goodness-of-fit statistics are a good alternative to\nother methods introduced in the literature tailored to the one-sided L\\'evy\ndistributions. The usefulness of our approach is verified using an empirical\ntest power study. For completeness, we also derive the asymptotic distributions\nof the test statistics and show how to apply our framework to real data."}, "http://arxiv.org/abs/2311.15384": {"title": "Robust and Automatic Data Clustering: Dirichlet Process meets Median-of-Means", "link": "http://arxiv.org/abs/2311.15384", "description": "Clustering stands as one of the most prominent challenges within the realm of\nunsupervised machine learning. Among the array of centroid-based clustering\nalgorithms, the classic $k$-means algorithm, rooted in Lloyd's heuristic, takes\ncenter stage as one of the extensively employed techniques in the literature.\nNonetheless, both $k$-means and its variants grapple with noteworthy\nlimitations. These encompass a heavy reliance on initial cluster centroids,\nsusceptibility to converging into local minima of the objective function, and\nsensitivity to outliers and noise in the data. When confronted with data\ncontaining noisy or outlier-laden observations, the Median-of-Means (MoM)\nestimator emerges as a stabilizing force for any centroid-based clustering\nframework. On a different note, a prevalent constraint among existing\nclustering methodologies resides in the prerequisite knowledge of the number of\nclusters prior to analysis. Utilizing model-based methodologies, such as\nBayesian nonparametric models, offers the advantage of infinite mixture models,\nthereby circumventing the need for such requirements. Motivated by these facts,\nin this article, we present an efficient and automatic clustering technique by\nintegrating the principles of model-based and centroid-based methodologies that\nmitigates the effect of noise on the quality of clustering while ensuring that\nthe number of clusters need not be specified in advance. Statistical guarantees\non the upper bound of clustering error, and rigorous assessment through\nsimulated and real datasets suggest the advantages of our proposed method over\nexisting state-of-the-art clustering algorithms."}, "http://arxiv.org/abs/2311.15410": {"title": "A Comprehensive Analysis of HIV Treatment Efficacy in the ACTG 175 Trial Through Multiple-Endpoint Approaches", "link": "http://arxiv.org/abs/2311.15410", "description": "In the realm of medical research, the intricate interplay of epidemiological\nrisk, genomic activity, adverse events, and clinical response necessitates a\nnuanced consideration of multiple variables. Clinical trials, designed to\nmeticulously assess the efficacy and safety of interventions, routinely\nincorporate a diverse array of endpoints. While a primary endpoint is\ncustomary, supplemented by key secondary endpoints, the statistical\nsignificance is typically evaluated independently for each. To address the\ninherent challenges in studying multiple endpoints, diverse strategies,\nincluding composite endpoints and global testing, have been proposed. This work\nstands apart by focusing on the evaluation of a clinical trial, deviating from\nthe conventional approach to underscore the efficacy of a multiple-endpoint\nprocedure. A double-blind study was conducted to gauge the treatment efficacy\nin adults infected with human immunodeficiency virus type 1 (HIV-1), featuring\nCD4 cell counts ranging from 200 to 500 per cubic millimeter. A total of 2467\nHIV-1-infected patients (43 percent without prior antiretroviral treatment)\nwere randomly assigned to one of four daily regimens: 600 mg of zidovudine; 600\nmg of zidovudine plus 400 mg of didanosine; 600 mg of zidovudine plus 2.25 mg\nof zalcitabine; or 400 mg of didanosine. The primary endpoint comprised a &gt;50\npercent decline in CD4 cell count, development of acquired immunodeficiency\nsyndrome (AIDS), or death. This study sought to determine the efficacy and\nsafety of zidovudine (AZT) versus didanosine (ddI), AZT plus ddI, and AZT plus\nzalcitabine (ddC) in preventing disease progression in HIV-infected patients\nwith CD4 counts of 200-500 cells/mm3. By jointly considering all endpoints, the\nmultiple-endpoints approach yields results of greater significance than a\nsingle-endpoint approach."}, "http://arxiv.org/abs/2311.15434": {"title": "Structural Discovery with Partial Ordering Information for Time-Dependent Data with Convergence Guarantees", "link": "http://arxiv.org/abs/2311.15434", "description": "Structural discovery amongst a set of variables is of interest in both static\nand dynamic settings. In the presence of lead-lag dependencies in the data, the\ndynamics of the system can be represented through a structural equation model\n(SEM) that simultaneously captures the contemporaneous and temporal\nrelationships amongst the variables, with the former encoded through a directed\nacyclic graph (DAG) for model identification. In many real applications, a\npartial ordering amongst the nodes of the DAG is available, which makes it\neither beneficial or imperative to incorporate it as a constraint in the\nproblem formulation. This paper develops an algorithm that can seamlessly\nincorporate a priori partial ordering information for solving a linear SEM\n(also known as Structural Vector Autoregression) under a high-dimensional\nsetting. The proposed algorithm is provably convergent to a stationary point,\nand exhibits competitive performance on both synthetic and real data sets."}, "http://arxiv.org/abs/2311.15485": {"title": "Calibrated Generalized Bayesian Inference", "link": "http://arxiv.org/abs/2311.15485", "description": "We provide a simple and general solution to the fundamental open problem of\ninaccurate uncertainty quantification of Bayesian inference in misspecified or\napproximate models, and of generalized Bayesian posteriors more generally.\nWhile existing solutions are based on explicit Gaussian posterior\napproximations, or computationally onerous post-processing procedures, we\ndemonstrate that correct uncertainty quantification can be achieved by\nsubstituting the usual posterior with an alternative posterior that conveys the\nsame information. This solution applies to both likelihood-based and loss-based\nposteriors, and we formally demonstrate the reliable uncertainty quantification\nof this approach. The new approach is demonstrated through a range of examples,\nincluding generalized linear models, and doubly intractable models."}, "http://arxiv.org/abs/2311.15498": {"title": "Adjusted inference for multiple testing procedure in group sequential designs", "link": "http://arxiv.org/abs/2311.15498", "description": "Adjustment of statistical significance levels for repeated analysis in group\nsequential trials has been understood for some time. Similarly, methods for\nadjustment accounting for testing multiple hypotheses are common. There is\nlimited research on simultaneously adjusting for both multiple hypothesis\ntesting and multiple analyses of one or more hypotheses. We address this gap by\nproposing adjusted-sequential p-values that reject an elementary hypothesis\nwhen its adjusted-sequential p-values are less than or equal to the family-wise\nType I error rate (FWER) in a group sequential design. We also propose\nsequential p-values for intersection hypotheses as a tool to compute adjusted\nsequential p-values for elementary hypotheses. We demonstrate the application\nusing weighted Bonferroni tests and weighted parametric tests, comparing\nadjusted sequential p-values to a desired FWER for inference on each elementary\nhypothesis tested."}, "http://arxiv.org/abs/2311.15598": {"title": "Optimal Clustering of Discrete Mixtures: Binomial, Poisson, Block Models, and Multi-layer Networks", "link": "http://arxiv.org/abs/2311.15598", "description": "In this paper, we first study the fundamental limit of clustering networks\nwhen a multi-layer network is present. Under the mixture multi-layer stochastic\nblock model (MMSBM), we show that the minimax optimal network clustering error\nrate, which takes an exponential form and is characterized by the Renyi\ndivergence between the edge probability distributions of the component\nnetworks. We propose a novel two-stage network clustering method including a\ntensor-based initialization algorithm involving both node and sample splitting\nand a refinement procedure by likelihood-based Lloyd algorithm. Network\nclustering must be accompanied by node community detection. Our proposed\nalgorithm achieves the minimax optimal network clustering error rate and allows\nextreme network sparsity under MMSBM. Numerical simulations and real data\nexperiments both validate that our method outperforms existing methods.\nOftentimes, the edges of networks carry count-type weights. We then extend our\nmethodology and analysis framework to study the minimax optimal clustering\nerror rate for mixture of discrete distributions including Binomial, Poisson,\nand multi-layer Poisson networks. The minimax optimal clustering error rates in\nthese discrete mixtures all take the same exponential form characterized by the\nRenyi divergences. These optimal clustering error rates in discrete mixtures\ncan also be achieved by our proposed two-stage clustering algorithm."}, "http://arxiv.org/abs/2311.15610": {"title": "Bayesian Approach to Linear Bayesian Networks", "link": "http://arxiv.org/abs/2311.15610", "description": "This study proposes the first Bayesian approach for learning high-dimensional\nlinear Bayesian networks. The proposed approach iteratively estimates each\nelement of the topological ordering from backward and its parent using the\ninverse of a partial covariance matrix. The proposed method successfully\nrecovers the underlying structure when Bayesian regularization for the inverse\ncovariance matrix with unequal shrinkage is applied. Specifically, it shows\nthat the number of samples $n = \\Omega( d_M^2 \\log p)$ and $n = \\Omega(d_M^2\np^{2/m})$ are sufficient for the proposed algorithm to learn linear Bayesian\nnetworks with sub-Gaussian and 4m-th bounded-moment error distributions,\nrespectively, where $p$ is the number of nodes and $d_M$ is the maximum degree\nof the moralized graph. The theoretical findings are supported by extensive\nsimulation studies including real data analysis. Furthermore the proposed\nmethod is demonstrated to outperform state-of-the-art frequentist approaches,\nsuch as the BHLSM, LISTEN, and TD algorithms in synthetic data."}, "http://arxiv.org/abs/2311.15715": {"title": "Spatio-temporal insights for wind energy harvesting in South Africa", "link": "http://arxiv.org/abs/2311.15715", "description": "Understanding complex spatial dependency structures is a crucial\nconsideration when attempting to build a modeling framework for wind speeds.\nIdeally, wind speed modeling should be very efficient since the wind speed can\nvary significantly from day to day or even hour to hour. But complex models\nusually require high computational resources. This paper illustrates how to\nconstruct and implement a hierarchical Bayesian model for wind speeds using the\nWeibull density function based on a continuously-indexed spatial field. For\nefficient (near real-time) inference the proposed model is implemented in the r\npackage R-INLA, based on the integrated nested Laplace approximation (INLA).\nSpecific attention is given to the theoretical and practical considerations of\nincluding a spatial component within a Bayesian hierarchical model. The\nproposed model is then applied and evaluated using a large volume of real data\nsourced from the coastal regions of South Africa between 2011 and 2021. By\nprojecting the mean and standard deviation of the Matern field, the results\nshow that the spatial modeling component is effectively capturing variation in\nwind speeds which cannot be explained by the other model components. The mean\nof the spatial field varies between $\\pm 0.3$ across the domain. These insights\nare valuable for planning and implementation of green energy resources such as\nwind farms in South Africa. Furthermore, shortcomings in the spatial sampling\ndomain is evident in the analysis and this is important for future sampling\nstrategies. The proposed model, and the conglomerated dataset, can serve as a\nfoundational framework for future investigations into wind energy in South\nAfrica."}, "http://arxiv.org/abs/2311.15860": {"title": "Frequentist Prediction Sets for Species Abundance using Indirect Information", "link": "http://arxiv.org/abs/2311.15860", "description": "Citizen science databases that consist of volunteer-led sampling efforts of\nspecies communities are relied on as essential sources of data in ecology.\nSummarizing such data across counties with frequentist-valid prediction sets\nfor each county provides an interpretable comparison across counties of varying\nsize or composition. As citizen science data often feature unequal sampling\nefforts across a spatial domain, prediction sets constructed with indirect\nmethods that share information across counties may be used to improve\nprecision. In this article, we present a nonparametric framework to obtain\nprecise prediction sets for a multinomial random sample based on indirect\ninformation that maintain frequentist coverage guarantees for each county. We\ndetail a simple algorithm to obtain prediction sets for each county using\nindirect information where the computation time does not depend on the sample\nsize and scales nicely with the number of species considered. The indirect\ninformation may be estimated by a proposed empirical Bayes procedure based on\ninformation from auxiliary data. Our approach makes inference for under-sampled\ncounties more precise, while maintaining area-specific frequentist validity for\neach county. Our method is used to provide a useful description of avian\nspecies abundance in North Carolina, USA based on citizen science data from the\neBird database."}, "http://arxiv.org/abs/2311.15878": {"title": "Individualized Treatment Allocations with Distributional Welfare", "link": "http://arxiv.org/abs/2311.15878", "description": "In this paper, we explore optimal treatment allocation policies that target\ndistributional welfare. Most literature on treatment choice has considered\nutilitarian welfare based on the conditional average treatment effect (ATE).\nWhile average welfare is intuitive, it may yield undesirable allocations\nespecially when individuals are heterogeneous (e.g., with outliers) - the very\nreason individualized treatments were introduced in the first place. This\nobservation motivates us to propose an optimal policy that allocates the\ntreatment based on the conditional \\emph{quantile of individual treatment\neffects} (QoTE). Depending on the choice of the quantile probability, this\ncriterion can accommodate a policymaker who is either prudent or negligent. The\nchallenge of identifying the QoTE lies in its requirement for knowledge of the\njoint distribution of the counterfactual outcomes, which is generally hard to\nrecover even with experimental data. Therefore, we introduce minimax optimal\npolicies that are robust to model uncertainty. We then propose a range of\nidentifying assumptions under which we can point or partially identify the\nQoTE. We establish the asymptotic bound on the regret of implementing the\nproposed policies. We consider both stochastic and deterministic rules. In\nsimulations and two empirical applications, we compare optimal decisions based\non the QoTE with decisions based on other criteria."}, "http://arxiv.org/abs/2311.15982": {"title": "Stab-GKnock: Controlled variable selection for partially linear models using generalized knockoffs", "link": "http://arxiv.org/abs/2311.15982", "description": "The recently proposed fixed-X knockoff is a powerful variable selection\nprocedure that controls the false discovery rate (FDR) in any finite-sample\nsetting, yet its theoretical insights are difficult to show beyond Gaussian\nlinear models. In this paper, we make the first attempt to extend the fixed-X\nknockoff to partially linear models by using generalized knockoff features, and\npropose a new stability generalized knockoff (Stab-GKnock) procedure by\nincorporating selection probability as feature importance score. We provide FDR\ncontrol and power guarantee under some regularity conditions. In addition, we\npropose a two-stage method under high dimensionality by introducing a new joint\nfeature screening procedure, with guaranteed sure screening property. Extensive\nsimulation studies are conducted to evaluate the finite-sample performance of\nthe proposed method. A real data example is also provided for illustration."}, "http://arxiv.org/abs/2311.15988": {"title": "A novel CFA+EFA model to detect aberrant respondents", "link": "http://arxiv.org/abs/2311.15988", "description": "Aberrant respondents are common but yet extremely detrimental to the quality\nof social surveys or questionnaires. Recently, factor mixture models have been\nemployed to identify individuals providing deceptive or careless responses. We\npropose a comprehensive factor mixture model that combines confirmatory and\nexploratory factor models to represent both the non-aberrant and aberrant\ncomponents of the responses. The flexibility of the proposed solution allows\nfor the identification of two of the most common aberant response styles,\nnamely faking and careless responding. We validated our approach by means of\ntwo simulations and two case studies. The results indicate the effectiveness of\nthe proposed model in handling with aberrant responses in social and behavioral\nsurveys."}, "http://arxiv.org/abs/2311.16025": {"title": "Change Point Detection for Random Objects using Distance Profiles", "link": "http://arxiv.org/abs/2311.16025", "description": "We introduce a new powerful scan statistic and an associated test for\ndetecting the presence and pinpointing the location of a change point within\nthe distribution of a data sequence where the data elements take values in a\ngeneral separable metric space $(\\Omega, d)$. These change points mark abrupt\nshifts in the distribution of the data sequence. Our method hinges on distance\nprofiles, where the distance profile of an element $\\omega \\in \\Omega$ is the\ndistribution of distances from $\\omega$ as dictated by the data. Our approach\nis fully non-parametric and universally applicable to diverse data types,\nincluding distributional and network data, as long as distances between the\ndata objects are available. From a practicable point of view, it is nearly\ntuning parameter-free, except for the specification of cut-off intervals near\nthe endpoints where change points are assumed not to occur. Our theoretical\nresults include a precise characterization of the asymptotic distribution of\nthe test statistic under the null hypothesis of no change points and rigorous\nguarantees on the consistency of the test in the presence of change points\nunder contiguous alternatives, as well as for the consistency of the estimated\nchange point location. Through comprehensive simulation studies encompassing\nmultivariate data, bivariate distributional data and sequences of graph\nLaplacians, we demonstrate the effectiveness of our approach in both change\npoint detection power and estimating the location of the change point. We apply\nour method to real datasets, including U.S. electricity generation compositions\nand Bluetooth proximity networks, underscoring its practical relevance."}, "http://arxiv.org/abs/1801.00332": {"title": "Confidence set for group membership", "link": "http://arxiv.org/abs/1801.00332", "description": "Our confidence set quantifies the statistical uncertainty from data-driven\ngroup assignments in grouped panel models. It covers the true group memberships\njointly for all units with pre-specified probability and is constructed by\ninverting many simultaneous unit-specific one-sided tests for group membership.\nWe justify our approach under $N, T \\to \\infty$ asymptotics using tools from\nhigh-dimensional statistics, some of which we extend in this paper. We provide\nMonte Carlo evidence that the confidence set has adequate coverage in finite\nsamples.An empirical application illustrates the use of our confidence set."}, "http://arxiv.org/abs/2004.05027": {"title": "Direct and spillover effects of a new tramway line on the commercial vitality of peripheral streets", "link": "http://arxiv.org/abs/2004.05027", "description": "In cities, the creation of public transport infrastructure such as light\nrails can cause changes on a very detailed spatial scale, with different\nstories unfolding next to each other within a same urban neighborhood. We study\nthe direct effect of a light rail line built in Florence (Italy) on the retail\ndensity of the street where it was built and and its spillover effect on other\nstreets in the treated street's neighborhood. To this aim, we investigate the\nuse of the Synthetic Control Group (SCG) methods in panel comparative case\nstudies where interference between the treated and the untreated units is\nplausible, an issue still little researched in the SCG methodological\nliterature. We frame our discussion in the potential outcomes approach. Under a\npartial interference assumption, we formally define relevant direct and\nspillover causal effects. We also consider the ``unrealized'' spillover effect\non the treated street in the hypothetical scenario that another street in the\ntreated unit's neighborhood had been assigned to the intervention."}, "http://arxiv.org/abs/2004.09458": {"title": "Noise-Induced Randomization in Regression Discontinuity Designs", "link": "http://arxiv.org/abs/2004.09458", "description": "Regression discontinuity designs assess causal effects in settings where\ntreatment is determined by whether an observed running variable crosses a\npre-specified threshold. Here we propose a new approach to identification,\nestimation, and inference in regression discontinuity designs that uses\nknowledge about exogenous noise (e.g., measurement error) in the running\nvariable. In our strategy, we weight treated and control units to balance a\nlatent variable of which the running variable is a noisy measure. Our approach\nis explicitly randomization-based and complements standard formal analyses that\nappeal to continuity arguments while ignoring the stochastic nature of the\nassignment mechanism."}, "http://arxiv.org/abs/2111.12258": {"title": "On Recoding Ordered Treatments as Binary Indicators", "link": "http://arxiv.org/abs/2111.12258", "description": "Researchers using instrumental variables to investigate ordered treatments\noften recode treatment into an indicator for any exposure. We investigate this\nestimand under the assumption that the instruments shift compliers from no\ntreatment to some but not from some treatment to more. We show that when there\nare extensive margin compliers only (EMCO) this estimand captures a weighted\naverage of treatment effects that can be partially unbundled into each complier\ngroup's potential outcome means. We also establish an equivalence between EMCO\nand a two-factor selection model and apply our results to study treatment\nheterogeneity in the Oregon Health Insurance Experiment."}, "http://arxiv.org/abs/2203.02849": {"title": "Variable Selection with the Knockoffs: Composite Null Hypotheses", "link": "http://arxiv.org/abs/2203.02849", "description": "The fixed-X knockoff filter is a flexible framework for variable selection\nwith false discovery rate (FDR) control in linear models with arbitrary design\nmatrices (of full column rank) and it allows for finite-sample selective\ninference via the Lasso estimates. In this paper, we extend the theory of the\nknockoff procedure to tests with composite null hypotheses, which are usually\nmore relevant to real-world problems. The main technical challenge lies in\nhandling composite nulls in tandem with dependent features from arbitrary\ndesigns. We develop two methods for composite inference with the knockoffs,\nnamely, shifted ordinary least-squares (S-OLS) and feature-response product\nperturbation (FRPP), building on new structural properties of test statistics\nunder composite nulls. We also propose two heuristic variants of S-OLS method\nthat outperform the celebrated Benjamini-Hochberg (BH) procedure for composite\nnulls, which serves as a heuristic baseline under dependent test statistics.\nFinally, we analyze the loss in FDR when the original knockoff procedure is\nnaively applied on composite tests."}, "http://arxiv.org/abs/2205.02617": {"title": "COMBSS: Best Subset Selection via Continuous Optimization", "link": "http://arxiv.org/abs/2205.02617", "description": "The problem of best subset selection in linear regression is considered with\nthe aim to find a fixed size subset of features that best fits the response.\nThis is particularly challenging when the total available number of features is\nvery large compared to the number of data samples. Existing optimal methods for\nsolving this problem tend to be slow while fast methods tend to have low\naccuracy. Ideally, new methods perform best subset selection faster than\nexisting optimal methods but with comparable accuracy, or, being more accurate\nthan methods of comparable computational speed. Here, we propose a novel\ncontinuous optimization method that identifies a subset solution path, a small\nset of models of varying size, that consists of candidates for the single best\nsubset of features, that is optimal in a specific sense in linear regression.\nOur method turns out to be fast, making the best subset selection possible when\nthe number of features is well in excess of thousands. Because of the\noutstanding overall performance, framing the best subset selection challenge as\na continuous optimization problem opens new research directions for feature\nextraction for a large variety of regression models."}, "http://arxiv.org/abs/2210.09339": {"title": "Probability Weighted Clustered Coefficients Regression Models in Complex Survey Sampling", "link": "http://arxiv.org/abs/2210.09339", "description": "Regression analysis is commonly conducted in survey sampling. However,\nexisting methods fail when the relationships vary across different areas or\ndomains. In this paper, we propose a unified framework to study the group-wise\ncovariate effect under complex survey sampling based on pairwise penalties, and\nthe associated objective function is solved by the alternating direction method\nof multipliers. Theoretical properties of the proposed method are investigated\nunder some generality conditions. Numerical experiments demonstrate the\nsuperiority of the proposed method in terms of identifying groups and\nestimation efficiency for both linear regression models and logistic regression\nmodels."}, "http://arxiv.org/abs/2211.00460": {"title": "Augmentation Invariant Manifold Learning", "link": "http://arxiv.org/abs/2211.00460", "description": "Data augmentation is a widely used technique and an essential ingredient in\nthe recent advance in self-supervised representation learning. By preserving\nthe similarity between augmented data, the resulting data representation can\nimprove various downstream analyses and achieve state-of-the-art performance in\nmany applications. Despite the empirical effectiveness, most existing methods\nlack theoretical understanding under a general nonlinear setting. To fill this\ngap, we develop a statistical framework on a low-dimension product manifold to\nmodel the data augmentation transformation. Under this framework, we introduce\na new representation learning method called augmentation invariant manifold\nlearning and design a computationally efficient algorithm by reformulating it\nas a stochastic optimization problem. Compared with existing self-supervised\nmethods, the new method simultaneously exploits the manifold's geometric\nstructure and invariant property of augmented data and has an explicit\ntheoretical guarantee. Our theoretical investigation characterizes the role of\ndata augmentation in the proposed method and reveals why and how the data\nrepresentation learned from augmented data can improve the $k$-nearest neighbor\nclassifier in the downstream analysis, showing that a more complex data\naugmentation leads to more improvement in downstream analysis. Finally,\nnumerical experiments on simulated and real datasets are presented to\ndemonstrate the merit of the proposed method."}, "http://arxiv.org/abs/2212.05831": {"title": "Conditional-mean Multiplicative Operator Models for Count Time Series", "link": "http://arxiv.org/abs/2212.05831", "description": "Multiplicative error models (MEMs) are commonly used for real-valued time\nseries, but they cannot be applied to discrete-valued count time series as the\ninvolved multiplication would not preserve the integer nature of the data.\nThus, the concept of a multiplicative operator for counts is proposed (as well\nas several specific instances thereof), which are then used to develop a kind\nof MEMs for count time series (CMEMs). If equipped with a linear conditional\nmean, the resulting CMEMs are closely related to the class of so-called\ninteger-valued generalized autoregressive conditional heteroscedasticity\n(INGARCH) models and might be used as a semi-parametric extension thereof.\nImportant stochastic properties of different types of INGARCH-CMEM as well as\nrelevant estimation approaches are derived, namely types of quasi-maximum\nlikelihood and weighted least squares estimation. The performance and\napplication are demonstrated with simulations as well as with two real-world\ndata examples."}, "http://arxiv.org/abs/2301.01381": {"title": "Testing High-dimensional Multinomials with Applications to Text Analysis", "link": "http://arxiv.org/abs/2301.01381", "description": "Motivated by applications in text mining and discrete distribution inference,\nwe investigate the testing for equality of probability mass functions of $K$\ngroups of high-dimensional multinomial distributions. A test statistic, which\nis shown to have an asymptotic standard normal distribution under the null, is\nproposed. The optimal detection boundary is established, and the proposed test\nis shown to achieve this optimal detection boundary across the entire parameter\nspace of interest. The proposed method is demonstrated in simulation studies\nand applied to analyze two real-world datasets to examine variation among\nconsumer reviews of Amazon movies and diversity of statistical paper abstracts."}, "http://arxiv.org/abs/2303.03092": {"title": "Environment Invariant Linear Least Squares", "link": "http://arxiv.org/abs/2303.03092", "description": "This paper considers a multi-environment linear regression model in which\ndata from multiple experimental settings are collected. The joint distribution\nof the response variable and covariates may vary across different environments,\nyet the conditional expectations of $y$ given the unknown set of important\nvariables are invariant. Such a statistical model is related to the problem of\nendogeneity, causal inference, and transfer learning. The motivation behind it\nis illustrated by how the goals of prediction and attribution are inherent in\nestimating the true parameter and the important variable set. We construct a\nnovel environment invariant linear least squares (EILLS) objective function, a\nmulti-environment version of linear least-squares regression that leverages the\nabove conditional expectation invariance structure and heterogeneity among\ndifferent environments to determine the true parameter. Our proposed method is\napplicable without any additional structural knowledge and can identify the\ntrue parameter under a near-minimal identification condition. We establish\nnon-asymptotic $\\ell_2$ error bounds on the estimation error for the EILLS\nestimator in the presence of spurious variables. Moreover, we further show that\nthe $\\ell_0$ penalized EILLS estimator can achieve variable selection\nconsistency in high-dimensional regimes. These non-asymptotic results\ndemonstrate the sample efficiency of the EILLS estimator and its capability to\ncircumvent the curse of endogeneity in an algorithmic manner without any prior\nstructural knowledge. To the best of our knowledge, this paper is the first to\nrealize statistically efficient invariance learning in the general linear\nmodel."}, "http://arxiv.org/abs/2305.07721": {"title": "Designing Optimal Behavioral Experiments Using Machine Learning", "link": "http://arxiv.org/abs/2305.07721", "description": "Computational models are powerful tools for understanding human cognition and\nbehavior. They let us express our theories clearly and precisely, and offer\npredictions that can be subtle and often counter-intuitive. However, this same\nrichness and ability to surprise means our scientific intuitions and\ntraditional tools are ill-suited to designing experiments to test and compare\nthese models. To avoid these pitfalls and realize the full potential of\ncomputational modeling, we require tools to design experiments that provide\nclear answers about what models explain human behavior and the auxiliary\nassumptions those models must make. Bayesian optimal experimental design (BOED)\nformalizes the search for optimal experimental designs by identifying\nexperiments that are expected to yield informative data. In this work, we\nprovide a tutorial on leveraging recent advances in BOED and machine learning\nto find optimal experiments for any kind of model that we can simulate data\nfrom, and show how by-products of this procedure allow for quick and\nstraightforward evaluation of models and their parameters against real\nexperimental data. As a case study, we consider theories of how people balance\nexploration and exploitation in multi-armed bandit decision-making tasks. We\nvalidate the presented approach using simulations and a real-world experiment.\nAs compared to experimental designs commonly used in the literature, we show\nthat our optimal designs more efficiently determine which of a set of models\nbest account for individual human behavior, and more efficiently characterize\nbehavior given a preferred model. At the same time, formalizing a scientific\nquestion such that it can be adequately addressed with BOED can be challenging\nand we discuss several potential caveats and pitfalls that practitioners should\nbe aware of. We provide code and tutorial notebooks to replicate all analyses."}, "http://arxiv.org/abs/2307.05251": {"title": "Minimizing robust density power-based divergences for general parametric density models", "link": "http://arxiv.org/abs/2307.05251", "description": "Density power divergence (DPD) is designed to robustly estimate the\nunderlying distribution of observations, in the presence of outliers. However,\nDPD involves an integral of the power of the parametric density models to be\nestimated; the explicit form of the integral term can be derived only for\nspecific densities, such as normal and exponential densities. While we may\nperform a numerical integration for each iteration of the optimization\nalgorithms, the computational complexity has hindered the practical application\nof DPD-based estimation to more general parametric densities. To address the\nissue, this study introduces a stochastic approach to minimize DPD for general\nparametric density models. The proposed approach also can be employed to\nminimize other density power-based $\\gamma$-divergences, by leveraging\nunnormalized models."}, "http://arxiv.org/abs/2307.15176": {"title": "RCT Rejection Sampling for Causal Estimation Evaluation", "link": "http://arxiv.org/abs/2307.15176", "description": "Confounding is a significant obstacle to unbiased estimation of causal\neffects from observational data. For settings with high-dimensional covariates\n-- such as text data, genomics, or the behavioral social sciences --\nresearchers have proposed methods to adjust for confounding by adapting machine\nlearning methods to the goal of causal estimation. However, empirical\nevaluation of these adjustment methods has been challenging and limited. In\nthis work, we build on a promising empirical evaluation strategy that\nsimplifies evaluation design and uses real data: subsampling randomized\ncontrolled trials (RCTs) to create confounded observational datasets while\nusing the average causal effects from the RCTs as ground-truth. We contribute a\nnew sampling algorithm, which we call RCT rejection sampling, and provide\ntheoretical guarantees that causal identification holds in the observational\ndata to allow for valid comparisons to the ground-truth RCT. Using synthetic\ndata, we show our algorithm indeed results in low bias when oracle estimators\nare evaluated on the confounded samples, which is not always the case for a\npreviously proposed algorithm. In addition to this identification result, we\nhighlight several finite data considerations for evaluation designers who plan\nto use RCT rejection sampling on their own datasets. As a proof of concept, we\nimplement an example evaluation pipeline and walk through these finite data\nconsiderations with a novel, real-world RCT -- which we release publicly --\nconsisting of approximately 70k observations and text data as high-dimensional\ncovariates. Together, these contributions build towards a broader agenda of\nimproved empirical evaluation for causal estimation."}, "http://arxiv.org/abs/2309.09111": {"title": "Reducing sequential change detection to sequential estimation", "link": "http://arxiv.org/abs/2309.09111", "description": "We consider the problem of sequential change detection, where the goal is to\ndesign a scheme for detecting any changes in a parameter or functional $\\theta$\nof the data stream distribution that has small detection delay, but guarantees\ncontrol on the frequency of false alarms in the absence of changes. In this\npaper, we describe a simple reduction from sequential change detection to\nsequential estimation using confidence sequences: we begin a new\n$(1-\\alpha)$-confidence sequence at each time step, and proclaim a change when\nthe intersection of all active confidence sequences becomes empty. We prove\nthat the average run length is at least $1/\\alpha$, resulting in a change\ndetection scheme with minimal structural assumptions~(thus allowing for\npossibly dependent observations, and nonparametric distribution classes), but\nstrong guarantees. Our approach bears an interesting parallel with the\nreduction from change detection to sequential testing of Lorden (1971) and the\ne-detector of Shin et al. (2022)."}, "http://arxiv.org/abs/2311.16181": {"title": "mvlearnR and Shiny App for multiview learning", "link": "http://arxiv.org/abs/2311.16181", "description": "The package mvlearnR and accompanying Shiny App is intended for integrating\ndata from multiple sources or views or modalities (e.g. genomics, proteomics,\nclinical and demographic data). Most existing software packages for multiview\nlearning are decentralized and offer limited capabilities, making it difficult\nfor users to perform comprehensive integrative analysis. The new package wraps\nstatistical and machine learning methods and graphical tools, providing a\nconvenient and easy data integration workflow. For users with limited\nprogramming language, we provide a Shiny Application to facilitate data\nintegration anywhere and on any device. The methods have potential to offer\ndeeper insights into complex disease mechanisms.\n\nAvailability and Implementation: mvlearnR is available from the following\nGitHub repository: https://github.com/lasandrall/mvlearnR. The web application\nis hosted on shinyapps.io and available at:\nhttps://multi-viewlearn.shinyapps.io/MultiView_Modeling/"}, "http://arxiv.org/abs/2311.16260": {"title": "Using Multiple Outcomes to Improve the Synthetic Control Method", "link": "http://arxiv.org/abs/2311.16260", "description": "When there are multiple outcome series of interest, Synthetic Control\nanalyses typically proceed by estimating separate weights for each outcome. In\nthis paper, we instead propose estimating a common set of weights across\noutcomes, by balancing either a vector of all outcomes or an index or average\nof them. Under a low-rank factor model, we show that these approaches lead to\nlower bias bounds than separate weights, and that averaging leads to further\ngains when the number of outcomes grows. We illustrate this via simulation and\nin a re-analysis of the impact of the Flint water crisis on educational\noutcomes."}, "http://arxiv.org/abs/2311.16286": {"title": "A statistical approach to latent dynamic modeling with differential equations", "link": "http://arxiv.org/abs/2311.16286", "description": "Ordinary differential equations (ODEs) can provide mechanistic models of\ntemporally local changes of processes, where parameters are often informed by\nexternal knowledge. While ODEs are popular in systems modeling, they are less\nestablished for statistical modeling of longitudinal cohort data, e.g., in a\nclinical setting. Yet, modeling of local changes could also be attractive for\nassessing the trajectory of an individual in a cohort in the immediate future\ngiven its current status, where ODE parameters could be informed by further\ncharacteristics of the individual. However, several hurdles so far limit such\nuse of ODEs, as compared to regression-based function fitting approaches. The\npotentially higher level of noise in cohort data might be detrimental to ODEs,\nas the shape of the ODE solution heavily depends on the initial value. In\naddition, larger numbers of variables multiply such problems and might be\ndifficult to handle for ODEs. To address this, we propose to use each\nobservation in the course of time as the initial value to obtain multiple local\nODE solutions and build a combined estimator of the underlying dynamics. Neural\nnetworks are used for obtaining a low-dimensional latent space for dynamic\nmodeling from a potentially large number of variables, and for obtaining\npatient-specific ODE parameters from baseline variables. Simultaneous\nidentification of dynamic models and of a latent space is enabled by recently\ndeveloped differentiable programming techniques. We illustrate the proposed\napproach in an application with spinal muscular atrophy patients and a\ncorresponding simulation study. In particular, modeling of local changes in\nhealth status at any point in time is contrasted to the interpretation of\nfunctions obtained from a global regression. This more generally highlights how\ndifferent application settings might demand different modeling strategies."}, "http://arxiv.org/abs/2311.16375": {"title": "Testing for a difference in means of a single feature after clustering", "link": "http://arxiv.org/abs/2311.16375", "description": "For many applications, it is critical to interpret and validate groups of\nobservations obtained via clustering. A common validation approach involves\ntesting differences in feature means between observations in two estimated\nclusters. In this setting, classical hypothesis tests lead to an inflated Type\nI error rate. To overcome this problem, we propose a new test for the\ndifference in means in a single feature between a pair of clusters obtained\nusing hierarchical or $k$-means clustering. The test based on the proposed\n$p$-value controls the selective Type I error rate in finite samples and can be\nefficiently computed. We further illustrate the validity and power of our\nproposal in simulation and demonstrate its use on single-cell RNA-sequencing\ndata."}, "http://arxiv.org/abs/2311.16451": {"title": "Variational Inference for the Latent Shrinkage Position Model", "link": "http://arxiv.org/abs/2311.16451", "description": "The latent position model (LPM) is a popular method used in network data\nanalysis where nodes are assumed to be positioned in a $p$-dimensional latent\nspace. The latent shrinkage position model (LSPM) is an extension of the LPM\nwhich automatically determines the number of effective dimensions of the latent\nspace via a Bayesian nonparametric shrinkage prior. However, the LSPM reliance\non Markov chain Monte Carlo for inference, while rigorous, is computationally\nexpensive, making it challenging to scale to networks with large numbers of\nnodes. We introduce a variational inference approach for the LSPM, aiming to\nreduce computational demands while retaining the model's ability to\nintrinsically determine the number of effective latent dimensions. The\nperformance of the variational LSPM is illustrated through simulation studies\nand its application to real-world network data. To promote wider adoption and\nease of implementation, we also provide open-source code."}, "http://arxiv.org/abs/2311.16529": {"title": "Efficient and Globally Robust Causal Excursion Effect Estimation", "link": "http://arxiv.org/abs/2311.16529", "description": "Causal excursion effect (CEE) characterizes the effect of an intervention\nunder policies that deviate from the experimental policy. It is widely used to\nstudy effect of time-varying interventions that have the potential to be\nfrequently adaptive, such as those delivered through smartphones. We study the\nsemiparametric efficient estimation of CEE and we derive a semiparametric\nefficiency bound for CEE with identity or log link functions under working\nassumptions. We propose a class of two-stage estimators that achieve the\nefficiency bound and are robust to misspecified nuisance models. In deriving\nthe asymptotic property of the estimators, we establish a general theory for\nglobally robust Z-estimators with either cross-fitted or non-cross-fitted\nnuisance parameters. We demonstrate substantial efficiency gain of the proposed\nestimator compared to existing ones through simulations and a real data\napplication using the Drink Less micro-randomized trial."}, "http://arxiv.org/abs/2311.16598": {"title": "Rectangular Hull Confidence Regions for Multivariate Parameters", "link": "http://arxiv.org/abs/2311.16598", "description": "We introduce three notions of multivariate median bias, namely, rectilinear,\nTukey, and orthant median bias. Each of these median biases is zero under a\nsuitable notion of multivariate symmetry. We study the coverage probabilities\nof rectangular hull of $B$ independent multivariate estimators, with special\nattention to the number of estimators $B$ needed to ensure a miscoverage of at\nmost $\\alpha$. It is proved that for estimators with zero orthant median bias,\nwe need $B\\geq c\\log_2(d/\\alpha)$ for some constant $c &gt; 0$. Finally, we show\nthat there exists an asymptotically valid (non-trivial) confidence region for a\nmultivariate parameter $\\theta_0$ if and only if there exists a (non-trivial)\nestimator with an asymptotic orthant median bias of zero."}, "http://arxiv.org/abs/2311.16614": {"title": "A Multivariate Unimodality Test Harnenssing the Dip Statistic of Mahalanobis Distances Over Random Projections", "link": "http://arxiv.org/abs/2311.16614", "description": "Unimodality, pivotal in statistical analysis, offers insights into dataset\nstructures and drives sophisticated analytical procedures. While unimodality's\nconfirmation is straightforward for one-dimensional data using methods like\nSilverman's approach and Hartigans' dip statistic, its generalization to higher\ndimensions remains challenging. By extrapolating one-dimensional unimodality\nprinciples to multi-dimensional spaces through linear random projections and\nleveraging point-to-point distancing, our method, rooted in\n$\\alpha$-unimodality assumptions, presents a novel multivariate unimodality\ntest named mud-pod. Both theoretical and empirical studies confirm the efficacy\nof our method in unimodality assessment of multidimensional datasets as well as\nin estimating the number of clusters."}, "http://arxiv.org/abs/2311.16793": {"title": "Mediation pathway selection with unmeasured mediator-outcome confounding", "link": "http://arxiv.org/abs/2311.16793", "description": "Causal mediation analysis aims to investigate how an intermediary factor,\ncalled a mediator, regulates the causal effect of a treatment on an outcome.\nWith the increasing availability of measurements on a large number of potential\nmediators, methods for selecting important mediators have been proposed.\nHowever, these methods often assume the absence of unmeasured mediator-outcome\nconfounding. We allow for such confounding in a linear structural equation\nmodel for the outcome and further propose an approach to tackle the mediator\nselection issue. To achieve this, we firstly identify causal parameters by\nconstructing a pseudo proxy variable for unmeasured confounding. Leveraging\nthis proxy variable, we propose a partially penalized method to identify\nmediators affecting the outcome. The resultant estimates are consistent, and\nthe estimates of nonzero parameters are asymptotically normal. Motivated by\nthese results, we introduce a two-step procedure to consistently select active\nmediation pathways, eliminating the need to test composite null hypotheses for\neach mediator that are commonly required by traditional methods. Simulation\nstudies demonstrate the superior performance of our approach compared to\nexisting methods. Finally, we apply our approach to genomic data, identifying\ngene expressions that potentially mediate the impact of a genetic variant on\nmouse obesity."}, "http://arxiv.org/abs/2311.16941": {"title": "Debiasing Multimodal Models via Causal Information Minimization", "link": "http://arxiv.org/abs/2311.16941", "description": "Most existing debiasing methods for multimodal models, including causal\nintervention and inference methods, utilize approximate heuristics to represent\nthe biases, such as shallow features from early stages of training or unimodal\nfeatures for multimodal tasks like VQA, etc., which may not be accurate. In\nthis paper, we study bias arising from confounders in a causal graph for\nmultimodal data and examine a novel approach that leverages causally-motivated\ninformation minimization to learn the confounder representations. Robust\npredictive features contain diverse information that helps a model generalize\nto out-of-distribution data. Hence, minimizing the information content of\nfeatures obtained from a pretrained biased model helps learn the simplest\npredictive features that capture the underlying data distribution. We treat\nthese features as confounder representations and use them via methods motivated\nby causal theory to remove bias from models. We find that the learned\nconfounder representations indeed capture dataset biases, and the proposed\ndebiasing methods improve out-of-distribution (OOD) performance on multiple\nmultimodal datasets without sacrificing in-distribution performance.\nAdditionally, we introduce a novel metric to quantify the sufficiency of\nspurious features in models' predictions that further demonstrates the\neffectiveness of our proposed methods. Our code is available at:\nhttps://github.com/Vaidehi99/CausalInfoMin"}, "http://arxiv.org/abs/2311.16984": {"title": "FedECA: A Federated External Control Arm Method for Causal Inference with Time-To-Event Data in Distributed Settings", "link": "http://arxiv.org/abs/2311.16984", "description": "External control arms (ECA) can inform the early clinical development of\nexperimental drugs and provide efficacy evidence for regulatory approval in\nnon-randomized settings. However, the main challenge of implementing ECA lies\nin accessing real-world data or historical clinical trials. Indeed, data\nsharing is often not feasible due to privacy considerations related to data\nleaving the original collection centers, along with pharmaceutical companies'\ncompetitive motives. In this paper, we leverage a privacy-enhancing technology\ncalled federated learning (FL) to remove some of the barriers to data sharing.\nWe introduce a federated learning inverse probability of treatment weighted\n(IPTW) method for time-to-event outcomes called FedECA which eases the\nimplementation of ECA by limiting patients' data exposure. We show with\nextensive experiments that FedECA outperforms its closest competitor,\nmatching-adjusted indirect comparison (MAIC), in terms of statistical power and\nability to balance the treatment and control groups. To encourage the use of\nsuch methods, we publicly release our code which relies on Substra, an\nopen-source FL software with proven experience in privacy-sensitive contexts."}, "http://arxiv.org/abs/2311.16988": {"title": "A Wasserstein-type Distance for Gaussian Mixtures on Vector Bundles with Applications to Shape Analysis", "link": "http://arxiv.org/abs/2311.16988", "description": "This paper uses sample data to study the problem of comparing populations on\nfinite-dimensional parallelizable Riemannian manifolds and more general trivial\nvector bundles. Utilizing triviality, our framework represents populations as\nmixtures of Gaussians on vector bundles and estimates the population parameters\nusing a mode-based clustering algorithm. We derive a Wasserstein-type metric\nbetween Gaussian mixtures, adapted to the manifold geometry, in order to\ncompare estimated distributions. Our contributions include an identifiability\nresult for Gaussian mixtures on manifold domains and a convenient\ncharacterization of optimal couplings of Gaussian mixtures under the derived\nmetric. We demonstrate these tools on some example domains, including the\npre-shape space of planar closed curves, with applications to the shape space\nof triangles and populations of nanoparticles. In the nanoparticle application,\nwe consider a sequence of populations of particle shapes arising from a\nmanufacturing process, and utilize the Wasserstein-type distance to perform\nchange-point detection."}, "http://arxiv.org/abs/2008.11477": {"title": "Bellman filtering and smoothing for state-space models", "link": "http://arxiv.org/abs/2008.11477", "description": "This paper presents a new filter for state-space models based on Bellman's\ndynamic-programming principle, allowing for nonlinearity, non-Gaussianity and\ndegeneracy in the observation and/or state-transition equations. The resulting\nBellman filter is a direct generalisation of the (iterated and extended) Kalman\nfilter, enabling scalability to higher dimensions while remaining\ncomputationally inexpensive. It can also be extended to enable smoothing. Under\nsuitable conditions, the Bellman-filtered states are stable over time and\ncontractive towards a region around the true state at every time step. Static\n(hyper)parameters are estimated by maximising a filter-implied pseudo\nlog-likelihood decomposition. In univariate simulation studies, the Bellman\nfilter performs on par with state-of-the-art simulation-based techniques at a\nfraction of the computational cost. In two empirical applications, involving up\nto 150 spatial dimensions or highly degenerate/nonlinear state dynamics, the\nBellman filter outperforms competing methods in both accuracy and speed."}, "http://arxiv.org/abs/2205.05955": {"title": "Bayesian inference for stochastic oscillatory systems using the phase-corrected Linear Noise Approximation", "link": "http://arxiv.org/abs/2205.05955", "description": "Likelihood-based inference in stochastic non-linear dynamical systems, such\nas those found in chemical reaction networks and biological clock systems, is\ninherently complex and has largely been limited to small and unrealistically\nsimple systems. Recent advances in analytically tractable approximations to the\nunderlying conditional probability distributions enable long-term dynamics to\nbe accurately modelled, and make the large number of model evaluations required\nfor exact Bayesian inference much more feasible. We propose a new methodology\nfor inference in stochastic non-linear dynamical systems exhibiting oscillatory\nbehaviour and show the parameters in these models can be realistically\nestimated from simulated data. Preliminary analyses based on the Fisher\nInformation Matrix of the model can guide the implementation of Bayesian\ninference. We show that this parameter sensitivity analysis can predict which\nparameters are practically identifiable. Several Markov chain Monte Carlo\nalgorithms are compared, with our results suggesting a parallel tempering\nalgorithm consistently gives the best approach for these systems, which are\nshown to frequently exhibit multi-modal posterior distributions."}, "http://arxiv.org/abs/2206.10479": {"title": "Policy Learning with Asymmetric Counterfactual Utilities", "link": "http://arxiv.org/abs/2206.10479", "description": "Data-driven decision making plays an important role even in high stakes\nsettings like medicine and public policy. Learning optimal policies from\nobserved data requires a careful formulation of the utility function whose\nexpected value is maximized across a population. Although researchers typically\nuse utilities that depend on observed outcomes alone, in many settings the\ndecision maker's utility function is more properly characterized by the joint\nset of potential outcomes under all actions. For example, the Hippocratic\nprinciple to \"do no harm\" implies that the cost of causing death to a patient\nwho would otherwise survive without treatment is greater than the cost of\nforgoing life-saving treatment. We consider optimal policy learning with\nasymmetric counterfactual utility functions of this form that consider the\njoint set of potential outcomes. We show that asymmetric counterfactual\nutilities lead to an unidentifiable expected utility function, and so we first\npartially identify it. Drawing on statistical decision theory, we then derive\nminimax decision rules by minimizing the maximum expected utility loss relative\nto different alternative policies. We show that one can learn minimax loss\ndecision rules from observed data by solving intermediate classification\nproblems, and establish that the finite sample excess expected utility loss of\nthis procedure is bounded by the regret of these intermediate classifiers. We\napply this conceptual framework and methodology to the decision about whether\nor not to use right heart catheterization for patients with possible pulmonary\nhypertension."}, "http://arxiv.org/abs/2209.04716": {"title": "Extrapolation before imputation reduces bias when imputing censored covariates", "link": "http://arxiv.org/abs/2209.04716", "description": "Modeling symptom progression to identify informative subjects for a new\nHuntington's disease clinical trial is problematic since time to diagnosis, a\nkey covariate, can be heavily censored. Imputation is an appealing strategy\nwhere censored covariates are replaced with their conditional means, but\nexisting methods saw over 200% bias under heavy censoring. Calculating these\nconditional means well requires estimating and then integrating over the\nsurvival function of the censored covariate from the censored value to\ninfinity. To estimate the survival function flexibly, existing methods use the\nsemiparametric Cox model with Breslow's estimator, leaving the integrand for\nthe conditional means (the estimated survival function) undefined beyond the\nobserved data. The integral is then estimated up to the largest observed\ncovariate value, and this approximation can cut off the tail of the survival\nfunction and lead to severe bias, particularly under heavy censoring. We\npropose a hybrid approach that splices together the semiparametric survival\nestimator with a parametric extension, making it possible to approximate the\nintegral up to infinity. In simulation studies, our proposed approach of\nextrapolation then imputation substantially reduces the bias seen with existing\nimputation methods, even when the parametric extension was misspecified. We\nfurther demonstrate how imputing with corrected conditional means helps to\nprioritize patients for future clinical trials."}, "http://arxiv.org/abs/2210.03559": {"title": "Estimation of the Order of Non-Parametric Hidden Markov Models using the Singular Values of an Integral Operator", "link": "http://arxiv.org/abs/2210.03559", "description": "We are interested in assessing the order of a finite-state Hidden Markov\nModel (HMM) with the only two assumptions that the transition matrix of the\nlatent Markov chain has full rank and that the density functions of the\nemission distributions are linearly independent. We introduce a new procedure\nfor estimating this order by investigating the rank of some well-chosen\nintegral operator which relies on the distribution of a pair of consecutive\nobservations. This method circumvents the usual limits of the spectral method\nwhen it is used for estimating the order of an HMM: it avoids the choice of the\nbasis functions; it does not require any knowledge of an upper-bound on the\norder of the HMM (for the spectral method, such an upper-bound is defined by\nthe number of basis functions); it permits to easily handle different types of\ndata (including continuous data, circular data or multivariate continuous data)\nwith a suitable choice of kernel. The method relies on the fact that the order\nof the HMM can be identified from the distribution of a pair of consecutive\nobservations and that this order is equal to the rank of some integral operator\n(\\emph{i.e.} the number of its singular values that are non-zero). Since only\nthe empirical counter-part of the singular values of the operator can be\nobtained, we propose a data-driven thresholding procedure. An upper-bound on\nthe probability of overestimating the order of the HMM is established.\nMoreover, sufficient conditions on the bandwidth used for kernel density\nestimation and on the threshold are stated to obtain the consistency of the\nestimator of the order of the HMM. The procedure is easily implemented since\nthe values of all the tuning parameters are determined by the sample size."}, "http://arxiv.org/abs/2306.15088": {"title": "Locally tail-scale invariant scoring rules for evaluation of extreme value forecasts", "link": "http://arxiv.org/abs/2306.15088", "description": "Statistical analysis of extremes can be used to predict the probability of\nfuture extreme events, such as large rainfalls or devastating windstorms. The\nquality of these forecasts can be measured through scoring rules. Locally scale\ninvariant scoring rules give equal importance to the forecasts at different\nlocations regardless of differences in the prediction uncertainty. This is a\nuseful feature when computing average scores but can be an unnecessarily strict\nrequirement when mostly concerned with extremes. We propose the concept of\nlocal weight-scale invariance, describing scoring rules fulfilling local scale\ninvariance in a certain region of interest, and as a special case local\ntail-scale invariance, for large events. Moreover, a new version of the\nweighted Continuous Ranked Probability score (wCRPS) called the scaled wCRPS\n(swCRPS) that possesses this property is developed and studied. The score is a\nsuitable alternative for scoring extreme value models over areas with varying\nscale of extreme events, and we derive explicit formulas of the score for the\nGeneralised Extreme Value distribution. The scoring rules are compared through\nsimulation, and their usage is illustrated in modelling of extreme water\nlevels, annual maximum rainfalls, and in an application to non-extreme forecast\nfor the prediction of air pollution."}, "http://arxiv.org/abs/2307.09850": {"title": "Communication-Efficient Distribution-Free Inference Over Networks", "link": "http://arxiv.org/abs/2307.09850", "description": "Consider a star network where each local node possesses a set of test\nstatistics that exhibit a symmetric distribution around zero when their\ncorresponding null hypothesis is true. This paper investigates statistical\ninference problems in networks concerning the aggregation of this general type\nof statistics and global error rate control under communication constraints in\nvarious scenarios. The study proposes communication-efficient algorithms that\nare built on established non-parametric methods, such as the Wilcoxon and sign\ntests, as well as modern inference methods such as the Benjamini-Hochberg (BH)\nand Barber-Candes (BC) procedures, coupled with sampling and quantization\noperations. The proposed methods are evaluated through extensive simulation\nstudies."}, "http://arxiv.org/abs/2308.01747": {"title": "Fusion regression methods with repeated functional data", "link": "http://arxiv.org/abs/2308.01747", "description": "Linear regression and classification methods with repeated functional data\nare considered. For each statistical unit in the sample, a real-valued\nparameter is observed over time under different conditions. Two regression\nmethods based on fusion penalties are presented. The first one is a\ngeneralization of the variable fusion methodology based on the 1-nearest\nneighbor. The second one, called group fusion lasso, assumes some grouping\nstructure of conditions and allows for homogeneity among the regression\ncoefficient functions within groups. A finite sample numerical simulation and\nan application on EEG data are presented."}, "http://arxiv.org/abs/2310.00599": {"title": "Approximate filtering via discrete dual processes", "link": "http://arxiv.org/abs/2310.00599", "description": "We consider the task of filtering a dynamic parameter evolving as a diffusion\nprocess, given data collected at discrete times from a likelihood which is\nconjugate to the marginal law of the diffusion, when a generic dual process on\na discrete state space is available. Recently, it was shown that duality with\nrespect to a death-like process implies that the filtering distributions are\nfinite mixtures, making exact filtering and smoothing feasible through\nrecursive algorithms with polynomial complexity in the number of observations.\nHere we provide general results for the case of duality between the diffusion\nand a regular jump continuous-time Markov chain on a discrete state space,\nwhich typically leads to filtering distribution given by countable mixtures\nindexed by the dual process state space. We investigate the performance of\nseveral approximation strategies on two hidden Markov models driven by\nCox-Ingersoll-Ross and Wright-Fisher diffusions, which admit duals of\nbirth-and-death type, and compare them with the available exact strategies\nbased on death-type duals and with bootstrap particle filtering on the\ndiffusion state space as a general benchmark."}, "http://arxiv.org/abs/2311.17100": {"title": "Automatic cross-validation in structured models: Is it time to leave out leave-one-out?", "link": "http://arxiv.org/abs/2311.17100", "description": "Standard techniques such as leave-one-out cross-validation (LOOCV) might not\nbe suitable for evaluating the predictive performance of models incorporating\nstructured random effects. In such cases, the correlation between the training\nand test sets could have a notable impact on the model's prediction error. To\novercome this issue, an automatic group construction procedure for\nleave-group-out cross validation (LGOCV) has recently emerged as a valuable\ntool for enhancing predictive performance measurement in structured models. The\npurpose of this paper is (i) to compare LOOCV and LGOCV within structured\nmodels, emphasizing model selection and predictive performance, and (ii) to\nprovide real data applications in spatial statistics using complex structured\nmodels fitted with INLA, showcasing the utility of the automatic LGOCV method.\nFirst, we briefly review the key aspects of the recently proposed LGOCV method\nfor automatic group construction in latent Gaussian models. We also demonstrate\nthe effectiveness of this method for selecting the model with the highest\npredictive performance by simulating extrapolation tasks in both temporal and\nspatial data analyses. Finally, we provide insights into the effectiveness of\nthe LGOCV method in modelling complex structured data, encompassing\nspatio-temporal multivariate count data, spatial compositional data, and\nspatio-temporal geospatial data."}, "http://arxiv.org/abs/2311.17102": {"title": "Splinets -- Orthogonal Splines and FDA for the Classification Problem", "link": "http://arxiv.org/abs/2311.17102", "description": "This study introduces an efficient workflow for functional data analysis in\nclassification problems, utilizing advanced orthogonal spline bases. The\nmethodology is based on the flexible Splinets package, featuring a novel spline\nrepresentation designed for enhanced data efficiency. Several innovative\nfeatures contribute to this efficiency: 1)Utilization of Orthonormal Spline\nBases 2)Consideration of Spline Support Sets 3)Data-Driven Knot Selection.\nIllustrating this approach, we applied the workflow to the Fashion MINST\ndataset. We demonstrate the classification process and highlight significant\nefficiency gains. Particularly noteworthy are the improvements that can be\nachieved through the 2D generalization of our methodology, especially in\nscenarios where data sparsity and dimension reduction are critical factors. A\nkey advantage of our workflow is the projection operation into the space of\nsplines with arbitrarily chosen knots, allowing for versatile functional data\nanalysis associated with classification problems. Moreover, the study explores\nSplinets package features suited for functional data analysis. The algebra and\ncalculus of splines use Taylor expansions at the knots within the support sets.\nVarious orthonormalization techniques for B-splines are implemented, including\nthe highly recommended dyadic method, which leads to the creation of splinets.\nImportantly, the locality of B-splines concerning support sets is preserved in\nthe corresponding splinet. Using this locality, along with implemented\nalgorithms, provides a powerful computational tool for functional data\nanalysis."}, "http://arxiv.org/abs/2311.17111": {"title": "Design of variable acceptance sampling plan for exponential distribution under uncertainty", "link": "http://arxiv.org/abs/2311.17111", "description": "In an acceptance monitoring system, acceptance sampling techniques are used\nto increase production, enhance control, and deliver higher-quality products at\na lesser cost. It might not always be possible to define the acceptance\nsampling plan parameters as exact values, especially, when data has\nuncertainty. In this work, acceptance sampling plans for a large number of\nidentical units with exponential lifetimes are obtained by treating acceptable\nquality life, rejectable quality life, consumer's risk, and producer's risk as\nfuzzy parameters. To obtain plan parameters of sequential sampling plans and\nrepetitive group sampling plans, fuzzy hypothesis test is considered. To\nvalidate the sampling plans obtained in this work, some examples are presented.\nOur results are compared with existing results in the literature. Finally, to\ndemonstrate the application of the resulting sampling plans, a real-life case\nstudy is presented."}, "http://arxiv.org/abs/2311.17246": {"title": "Detecting influential observations in single-index models with metric-valued response objects", "link": "http://arxiv.org/abs/2311.17246", "description": "Regression with random data objects is becoming increasingly common in modern\ndata analysis. Unfortunately, like the traditional regression setting with\nEuclidean data, random response regression is not immune to the trouble caused\nby unusual observations. A metric Cook's distance extending the classical\nCook's distances of Cook (1977) to general metric-valued response objects is\nproposed. The performance of the metric Cook's distance in both Euclidean and\nnon-Euclidean response regression with Euclidean predictors is demonstrated in\nan extensive experimental study. A real data analysis of county-level COVID-19\ntransmission in the United States also illustrates the usefulness of this\nmethod in practice."}, "http://arxiv.org/abs/2311.17271": {"title": "Spatial-Temporal Extreme Modeling for Point-to-Area Random Effects (PARE)", "link": "http://arxiv.org/abs/2311.17271", "description": "One measurement modality for rainfall is a fixed location rain gauge.\nHowever, extreme rainfall, flooding, and other climate extremes often occur at\nlarger spatial scales and affect more than one location in a community. For\nexample, in 2017 Hurricane Harvey impacted all of Houston and the surrounding\nregion causing widespread flooding. Flood risk modeling requires understanding\nof rainfall for hydrologic regions, which may contain one or more rain gauges.\nFurther, policy changes to address the risks and damages of natural hazards\nsuch as severe flooding are usually made at the community/neighborhood level or\nhigher geo-spatial scale. Therefore, spatial-temporal methods which convert\nresults from one spatial scale to another are especially useful in applications\nfor evolving environmental extremes. We develop a point-to-area random effects\n(PARE) modeling strategy for understanding spatial-temporal extreme values at\nthe areal level, when the core information are time series at point locations\ndistributed over the region."}, "http://arxiv.org/abs/2311.17303": {"title": "Enhancing the Performance of Neural Networks Through Causal Discovery and Integration of Domain Knowledge", "link": "http://arxiv.org/abs/2311.17303", "description": "In this paper, we develop a generic methodology to encode hierarchical\ncausality structure among observed variables into a neural network in order to\nimprove its predictive performance. The proposed methodology, called\ncausality-informed neural network (CINN), leverages three coherent steps to\nsystematically map the structural causal knowledge into the layer-to-layer\ndesign of neural network while strictly preserving the orientation of every\ncausal relationship. In the first step, CINN discovers causal relationships\nfrom observational data via directed acyclic graph (DAG) learning, where causal\ndiscovery is recast as a continuous optimization problem to avoid the\ncombinatorial nature. In the second step, the discovered hierarchical causality\nstructure among observed variables is systematically encoded into neural\nnetwork through a dedicated architecture and customized loss function. By\ncategorizing variables in the causal DAG as root, intermediate, and leaf nodes,\nthe hierarchical causal DAG is translated into CINN with a one-to-one\ncorrespondence between nodes in the causal DAG and units in the CINN while\nmaintaining the relative order among these nodes. Regarding the loss function,\nboth intermediate and leaf nodes in the DAG graph are treated as target outputs\nduring CINN training so as to drive co-learning of causal relationships among\ndifferent types of nodes. As multiple loss components emerge in CINN, we\nleverage the projection of conflicting gradients to mitigate gradient\ninterference among the multiple learning tasks. Computational experiments\nacross a broad spectrum of UCI data sets demonstrate substantial advantages of\nCINN in predictive performance over other state-of-the-art methods. In\naddition, an ablation study underscores the value of integrating structural and\nquantitative causal knowledge in enhancing the neural network's predictive\nperformance incrementally."}, "http://arxiv.org/abs/2311.17445": {"title": "Interaction tests with covariate-adaptive randomization", "link": "http://arxiv.org/abs/2311.17445", "description": "Treatment-covariate interaction tests are commonly applied by researchers to\nexamine whether the treatment effect varies across patient subgroups defined by\nbaseline characteristics. The objective of this study is to explore\ntreatment-covariate interaction tests involving covariate-adaptive\nrandomization. Without assuming a parametric data generation model, we\ninvestigate usual interaction tests and observe that they tend to be\nconservative: specifically, their limiting rejection probabilities under the\nnull hypothesis do not exceed the nominal level and are typically strictly\nlower than it. To address this problem, we propose modifications to the usual\ntests to obtain corresponding exact tests. Moreover, we introduce a novel class\nof stratified-adjusted interaction tests that are simple, broadly applicable,\nand more powerful than the usual and modified tests. Our findings are relevant\nto two types of interaction tests: one involving stratification covariates and\nthe other involving additional covariates that are not used for randomization."}, "http://arxiv.org/abs/2311.17467": {"title": "Design of platform trials with a change in the control treatment arm", "link": "http://arxiv.org/abs/2311.17467", "description": "Platform trials are a more efficient way of testing multiple treatments\ncompared to running separate trials. In this paper we consider platform trials\nwhere, if a treatment is found to be superior to the control, it will become\nthe new standard of care (and the control in the platform). The remaining\ntreatments are then tested against this new control. In such a setting, one can\neither keep the information on both the new standard of care and the other\nactive treatments before the control is changed or one could discard this\ninformation when testing for benefit of the remaining treatments. We will show\nanalytically and numerically that retaining the information collected before\nthe change in control can be detrimental to the power of the study.\nSpecifically, we consider the overall power, the probability that the active\ntreatment with the greatest treatment effect is found during the trial. We also\nconsider the conditional power of the active treatments, the probability a\ngiven treatment can be found superior against the current control. We prove\nwhen, in a multi-arm multi-stage trial where no arms are added, retaining the\ninformation is detrimental to both overall and conditional power of the\nremaining treatments. This loss of power is studied for a motivating example.\nWe then discuss the effect on platform trials in which arms are added later. On\nthe basis of these observations we discuss different aspects to consider when\ndeciding whether to run a continuous platform trial or whether one may be\nbetter running a new trial."}, "http://arxiv.org/abs/2311.17476": {"title": "Inference of Sample Complier Average Causal Effects in Completely Randomized Experiments", "link": "http://arxiv.org/abs/2311.17476", "description": "In randomized experiments with non-compliance scholars have argued that the\ncomplier average causal effect (CACE) ought to be the main causal estimand. The\nliterature on inference of the complier average treatment effect (CACE) has\nfocused on inference about the population CACE. However, in general individuals\nin the experiments are volunteers. This means that there is a risk that\nindividuals partaking in a given experiment differ in important ways from a\npopulation of interest. It is thus of interest to focus on the sample at hand\nand have easy to use and correct procedures for inference about the sample\nCACE. We consider a more general setting than in the previous literature and\nconstruct a confidence interval based on the Wald estimator in the form of a\nfinite closed interval that is familiar to practitioners. Furthermore, with the\naccess of pre-treatment covariates, we propose a new regression adjustment\nestimator and associated methods for constructing confidence intervals. Finite\nsample performance of the methods is examined through a Monte Carlo simulation\nand the methods are used in an application to a job training experiment."}, "http://arxiv.org/abs/2311.17547": {"title": "Risk-based decision making: estimands for sequential prediction under interventions", "link": "http://arxiv.org/abs/2311.17547", "description": "Prediction models are used amongst others to inform medical decisions on\ninterventions. Typically, individuals with high risks of adverse outcomes are\nadvised to undergo an intervention while those at low risk are advised to\nrefrain from it. Standard prediction models do not always provide risks that\nare relevant to inform such decisions: e.g., an individual may be estimated to\nbe at low risk because similar individuals in the past received an intervention\nwhich lowered their risk. Therefore, prediction models supporting decisions\nshould target risks belonging to defined intervention strategies. Previous\nworks on prediction under interventions assumed that the prediction model was\nused only at one time point to make an intervention decision. In clinical\npractice, intervention decisions are rarely made only once: they might be\nrepeated, deferred and re-evaluated. This requires estimated risks under\ninterventions that can be reconsidered at several potential decision moments.\nIn the current work, we highlight key considerations for formulating estimands\nin sequential prediction under interventions that can inform such intervention\ndecisions. We illustrate these considerations by giving examples of estimands\nfor a case study about choosing between vaginal delivery and cesarean section\nfor women giving birth. Our formalization of prediction tasks in a sequential,\ncausal, and estimand context provides guidance for future studies to ensure\nthat the right question is answered and appropriate causal estimation\napproaches are chosen to develop sequential prediction models that can inform\nintervention decisions."}, "http://arxiv.org/abs/2311.17564": {"title": "Combining Stochastic Tendency and Distribution Overlap Towards Improved Nonparametric Effect Measures and Inference", "link": "http://arxiv.org/abs/2311.17564", "description": "A fundamental functional in nonparametric statistics is the Mann-Whitney\nfunctional ${\\theta} = P (X &lt; Y )$ , which constitutes the basis for the most\npopular nonparametric procedures. The functional ${\\theta}$ measures a location\nor stochastic tendency effect between two distributions. A limitation of\n${\\theta}$ is its inability to capture scale differences. If differences of\nthis nature are to be detected, specific tests for scale or omnibus tests need\nto be employed. However, the latter often suffer from low power, and they do\nnot yield interpretable effect measures. In this manuscript, we extend\n${\\theta}$ by additionally incorporating the recently introduced distribution\noverlap index (nonparametric dispersion measure) $I_2$ that can be expressed in\nterms of the quantile process. We derive the joint asymptotic distribution of\nthe respective estimators of ${\\theta}$ and $I_2$ and construct confidence\nregions. Extending the Wilcoxon- Mann-Whitney test, we introduce a new test\nbased on the joint use of these functionals. It results in much larger\nconsistency regions while maintaining competitive power to the rank sum test\nfor situations in which {\\theta} alone would suffice. Compared with classical\nomnibus tests, the simulated power is much improved. Additionally, the newly\nproposed inference method yields effect measures whose interpretation is\nsurprisingly straightforward."}, "http://arxiv.org/abs/2311.17594": {"title": "Scale Invariant Correspondence Analysis", "link": "http://arxiv.org/abs/2311.17594", "description": "Correspondence analysis is a dimension reduction method for visualization of\nnonnegative data sets, in particular contingency tables ; but it depends on the\nmarginals of the data set. Two transformations of the data have been proposed\nto render correspondence analysis row and column scales invariant : These two\nkinds of transformations change the initial form of the data set into a\nbistochastic form. The power transorfmation applied by Greenacre (2010) has one\npositive parameter. While the transormation applied by Mosteller (1968) and\nGoodman (1996) has (I+J) positive parameters, where the raw data is row and\ncolumn scaled by the Sinkhorn (RAS or ipf) algorithm to render it bistochastic.\nGoodman (1996) named correspondence analsis of a bistochastic matrix\nmarginal-free correspondence analysis. We discuss these two transformations,\nand further generalize Mosteller-Goodman approach."}, "http://arxiv.org/abs/2311.17605": {"title": "Improving the Balance of Unobserved Covariates From Information Theory in Multi-Arm Randomization with Unequal Allocation Ratio", "link": "http://arxiv.org/abs/2311.17605", "description": "Multi-arm randomization has increasingly widespread applications recently and\nit is also crucial to ensure that the distributions of important observed\ncovariates as well as the potential unobserved covariates are similar and\ncomparable among all the treatment. However, the theoretical properties of\nunobserved covariates imbalance in multi-arm randomization with unequal\nallocation ratio remains unknown. In this paper, we give a general framework\nanalysing the moments and distributions of unobserved covariates imbalance and\napply them into different procedures including complete randomization (CR),\nstratified permuted block (STR-PB) and covariate-adaptive randomization (CAR).\nThe general procedures of multi-arm STR-PB and CAR with unequal allocation\nratio are also proposed. In addition, we introduce the concept of entropy to\nmeasure the correlation between discrete covariates and verify that we could\nutilize the correlation to select observed covariates to help better balance\nthe unobserved covariates."}, "http://arxiv.org/abs/2311.17685": {"title": "Enhancing efficiency and robustness in high-dimensional linear regression with additional unlabeled data", "link": "http://arxiv.org/abs/2311.17685", "description": "In semi-supervised learning, the prevailing understanding suggests that\nobserving additional unlabeled samples improves estimation accuracy for linear\nparameters only in the case of model misspecification. This paper challenges\nthis notion, demonstrating its inaccuracy in high dimensions. Initially\nfocusing on a dense scenario, we introduce robust semi-supervised estimators\nfor the regression coefficient without relying on sparse structures in the\npopulation slope. Even when the true underlying model is linear, we show that\nleveraging information from large-scale unlabeled data improves both estimation\naccuracy and inference robustness. Moreover, we propose semi-supervised methods\nwith further enhanced efficiency in scenarios with a sparse linear slope.\nDiverging from the standard semi-supervised literature, we also allow for\ncovariate shift. The performance of the proposed methods is illustrated through\nextensive numerical studies, including simulations and a real-data application\nto the AIDS Clinical Trials Group Protocol 175 (ACTG175)."}, "http://arxiv.org/abs/2311.17797": {"title": "Learning to Simulate: Generative Metamodeling via Quantile Regression", "link": "http://arxiv.org/abs/2311.17797", "description": "Stochastic simulation models, while effective in capturing the dynamics of\ncomplex systems, are often too slow to run for real-time decision-making.\nMetamodeling techniques are widely used to learn the relationship between a\nsummary statistic of the outputs (e.g., the mean or quantile) and the inputs of\nthe simulator, so that it can be used in real time. However, this methodology\nrequires the knowledge of an appropriate summary statistic in advance, making\nit inflexible for many practical situations. In this paper, we propose a new\nmetamodeling concept, called generative metamodeling, which aims to construct a\n\"fast simulator of the simulator\". This technique can generate random outputs\nsubstantially faster than the original simulation model, while retaining an\napproximately equal conditional distribution given the same inputs. Once\nconstructed, a generative metamodel can instantaneously generate a large amount\nof random outputs as soon as the inputs are specified, thereby facilitating the\nimmediate computation of any summary statistic for real-time decision-making.\nFurthermore, we propose a new algorithm -- quantile-regression-based generative\nmetamodeling (QRGMM) -- and study its convergence and rate of convergence.\nExtensive numerical experiments are conducted to investigate the empirical\nperformance of QRGMM, compare it with other state-of-the-art generative\nalgorithms, and demonstrate its usefulness in practical real-time\ndecision-making."}, "http://arxiv.org/abs/2311.17808": {"title": "A Heteroscedastic Bayesian Generalized Logistic Regression Model with Application to Scaling Problems", "link": "http://arxiv.org/abs/2311.17808", "description": "Scaling models have been used to explore relationships between urban\nindicators, population, and, more recently, extended to incorporate rural-urban\nindicator densities and population densities. In the scaling framework, power\nlaws and standard linear regression methods are used to estimate model\nparameters with assumed normality and fixed variance. These assumptions,\ninherited in the scaling field, have recently been demonstrated to be\ninadequate and noted that without consideration lead to model bias. Generalized\nlinear models (GLM) can accommodate a wider range of distributions where the\nchosen distribution must meet the assumptions of the data to prevent model\nbias. We present a widely applicable Bayesian generalized logistic regression\n(BGLR) framework to flexibly model a continuous real response addressing skew\nand heteroscedasticity using Markov Chain Monte Carlo (MCMC) methods. The\nGeneralized Logistic Distribution (GLD) robustly models skewed continuous data\ndue to the additional shape parameter. We compare the BGLR model to standard\nand Bayesian normal methods in fitting power laws to COVID-19 data. The BGLR\nprovides additional useful information beyond previous scaling methods,\nuniquely models variance including a scedasticity parameter and reveals\nparameter bias in widely used methods."}, "http://arxiv.org/abs/2311.17858": {"title": "On the Limits of Regression Adjustment", "link": "http://arxiv.org/abs/2311.17858", "description": "Regression adjustment, sometimes known as Controlled-experiment Using\nPre-Experiment Data (CUPED), is an important technique in internet\nexperimentation. It decreases the variance of effect size estimates, often\ncutting confidence interval widths in half or more while never making them\nworse. It does so by carefully regressing the goal metric against\npre-experiment features to reduce the variance. The tremendous gains of\nregression adjustment begs the question: How much better can we do by\nengineering better features from pre-experiment data, for example by using\nmachine learning techniques or synthetic controls? Could we even reduce the\nvariance in our effect sizes arbitrarily close to zero with the right\npredictors? Unfortunately, our answer is negative. A simple form of regression\nadjustment, which uses just the pre-experiment values of the goal metric,\ncaptures most of the benefit. Specifically, under a mild assumption that\nobservations closer in time are easier to predict that ones further away in\ntime, we upper bound the potential gains of more sophisticated feature\nengineering, with respect to the gains of this simple form of regression\nadjustment. The maximum reduction in variance is $50\\%$ in Theorem 1, or\nequivalently, the confidence interval width can be reduced by at most an\nadditional $29\\%$."}, "http://arxiv.org/abs/2311.17867": {"title": "A Class of Directed Acyclic Graphs with Mixed Data Types in Mediation Analysis", "link": "http://arxiv.org/abs/2311.17867", "description": "We propose a unified class of generalized structural equation models (GSEMs)\nwith data of mixed types in mediation analysis, including continuous,\ncategorical, and count variables. Such models extend substantially the\nclassical linear structural equation model to accommodate many data types\narising from the application of mediation analysis. Invoking the hierarchical\nmodeling approach, we specify GSEMs by a copula joint distribution of outcome\nvariable, mediator and exposure variable, in which marginal distributions are\nbuilt upon generalized linear models (GLMs) with confounding factors. We\ndiscuss the identifiability conditions for the causal mediation effects in the\ncounterfactual paradigm as well as the issue of mediation leakage, and develop\nan asymptotically efficient profile maximum likelihood estimation and inference\nfor two key mediation estimands, natural direct effect and natural indirect\neffect, in different scenarios of mixed data types. The proposed new\nmethodology is illustrated by a motivating epidemiological study that aims to\ninvestigate whether the tempo of reaching infancy BMI peak (delay or on time),\nan important early life growth milestone, may mediate the association between\nprenatal exposure to phthalates and pubertal health outcomes."}, "http://arxiv.org/abs/2311.17885": {"title": "Are ensembles getting better all the time?", "link": "http://arxiv.org/abs/2311.17885", "description": "Ensemble methods combine the predictions of several base models. We study\nwhether or not including more models in an ensemble always improve its average\nperformance. Such a question depends on the kind of ensemble considered, as\nwell as the predictive metric chosen. We focus on situations where all members\nof the ensemble are a priori expected to perform as well, which is the case of\nseveral popular methods like random forests or deep ensembles. In this setting,\nwe essentially show that ensembles are getting better all the time if, and only\nif, the considered loss function is convex. More precisely, in that case, the\naverage loss of the ensemble is a decreasing function of the number of models.\nWhen the loss function is nonconvex, we show a series of results that can be\nsummarised by the insight that ensembles of good models keep getting better,\nand ensembles of bad models keep getting worse. To this end, we prove a new\nresult on the monotonicity of tail probabilities that may be of independent\ninterest. We illustrate our results on a simple machine learning problem\n(diagnosing melanomas using neural nets)."}, "http://arxiv.org/abs/2110.04442": {"title": "A Primer on Deep Learning for Causal Inference", "link": "http://arxiv.org/abs/2110.04442", "description": "This review systematizes the emerging literature for causal inference using\ndeep neural networks under the potential outcomes framework. It provides an\nintuitive introduction on how deep learning can be used to estimate/predict\nheterogeneous treatment effects and extend causal inference to settings where\nconfounding is non-linear, time varying, or encoded in text, networks, and\nimages. To maximize accessibility, we also introduce prerequisite concepts from\ncausal inference and deep learning. The survey differs from other treatments of\ndeep learning and causal inference in its sharp focus on observational causal\nestimation, its extended exposition of key algorithms, and its detailed\ntutorials for implementing, training, and selecting among deep estimators in\nTensorflow 2 available at github.com/kochbj/Deep-Learning-for-Causal-Inference."}, "http://arxiv.org/abs/2111.05243": {"title": "Bounding Treatment Effects by Pooling Limited Information across Observations", "link": "http://arxiv.org/abs/2111.05243", "description": "We provide novel bounds on average treatment effects (on the treated) that\nare valid under an unconfoundedness assumption. Our bounds are designed to be\nrobust in challenging situations, for example, when the conditioning variables\ntake on a large number of different values in the observed sample, or when the\noverlap condition is violated. This robustness is achieved by only using\nlimited \"pooling\" of information across observations. Namely, the bounds are\nconstructed as sample averages over functions of the observed outcomes such\nthat the contribution of each outcome only depends on the treatment status of a\nlimited number of observations. No information pooling across observations\nleads to so-called \"Manski bounds\", while unlimited information pooling leads\nto standard inverse propensity score weighting. We explore the intermediate\nrange between these two extremes and provide corresponding inference methods.\nWe show in Monte Carlo experiments and through an empirical application that\nour bounds are indeed robust and informative in practice."}, "http://arxiv.org/abs/2111.07966": {"title": "Evaluating Treatment Prioritization Rules via Rank-Weighted Average Treatment Effects", "link": "http://arxiv.org/abs/2111.07966", "description": "There are a number of available methods for selecting whom to prioritize for\ntreatment, including ones based on treatment effect estimation, risk scoring,\nand hand-crafted rules. We propose rank-weighted average treatment effect\n(RATE) metrics as a simple and general family of metrics for comparing and\ntesting the quality of treatment prioritization rules. RATE metrics are\nagnostic as to how the prioritization rules were derived, and only assess how\nwell they identify individuals that benefit the most from treatment. We define\na family of RATE estimators and prove a central limit theorem that enables\nasymptotically exact inference in a wide variety of randomized and\nobservational study settings. RATE metrics subsume a number of existing\nmetrics, including the Qini coefficient, and our analysis directly yields\ninference methods for these metrics. We showcase RATE in the context of a\nnumber of applications, including optimal targeting of aspirin to stroke\npatients."}, "http://arxiv.org/abs/2305.02185": {"title": "Doubly Robust Uniform Confidence Bands for Group-Time Conditional Average Treatment Effects in Difference-in-Differences", "link": "http://arxiv.org/abs/2305.02185", "description": "We consider a panel data analysis to examine the heterogeneity in treatment\neffects with respect to a pre-treatment covariate of interest in the staggered\ndifference-in-differences setting of Callaway and Sant'Anna (2021). Under\nstandard identification conditions, a doubly robust estimand conditional on the\ncovariate identifies the group-time conditional average treatment effect given\nthe covariate. Focusing on the case of a continuous covariate, we propose a\nthree-step estimation procedure based on nonparametric local polynomial\nregressions and parametric estimation methods. Using uniformly valid\ndistributional approximation results for empirical processes and multiplier\nbootstrapping, we develop doubly robust inference methods to construct uniform\nconfidence bands for the group-time conditional average treatment effect\nfunction. The accompanying R package didhetero allows for easy implementation\nof the proposed methods."}, "http://arxiv.org/abs/2306.02126": {"title": "A Process of Dependent Quantile Pyramids", "link": "http://arxiv.org/abs/2306.02126", "description": "Despite the practicality of quantile regression (QR), simultaneous estimation\nof multiple QR curves continues to be challenging. We address this problem by\nproposing a Bayesian nonparametric framework that generalizes the quantile\npyramid by replacing each scalar variate in the quantile pyramid with a\nstochastic process on a covariate space. We propose a novel approach to show\nthe existence of a quantile pyramid for all quantiles. The process of dependent\nquantile pyramids allows for non-linear QR and automatically ensures\nnon-crossing of QR curves on the covariate space. Simulation studies document\nthe performance and robustness of our approach. An application to cyclone\nintensity data is presented."}, "http://arxiv.org/abs/2306.02244": {"title": "Optimal neighbourhood selection in structural equation models", "link": "http://arxiv.org/abs/2306.02244", "description": "We study the optimal sample complexity of neighbourhood selection in linear\nstructural equation models, and compare this to best subset selection (BSS) for\nlinear models under general design. We show by example that -- even when the\nstructure is \\emph{unknown} -- the existence of underlying structure can reduce\nthe sample complexity of neighbourhood selection. This result is complicated by\nthe possibility of path cancellation, which we study in detail, and show that\nimprovements are still possible in the presence of path cancellation. Finally,\nwe support these theoretical observations with experiments. The proof\nintroduces a modified BSS estimator, called klBSS, and compares its performance\nto BSS. The analysis of klBSS may also be of independent interest since it\napplies to arbitrary structured models, not necessarily those induced by a\nstructural equation model. Our results have implications for structure learning\nin graphical models, which often relies on neighbourhood selection as a\nsubroutine."}, "http://arxiv.org/abs/2307.13868": {"title": "Learning sources of variability from high-dimensional observational studies", "link": "http://arxiv.org/abs/2307.13868", "description": "Causal inference studies whether the presence of a variable influences an\nobserved outcome. As measured by quantities such as the \"average treatment\neffect,\" this paradigm is employed across numerous biological fields, from\nvaccine and drug development to policy interventions. Unfortunately, the\nmajority of these methods are often limited to univariate outcomes. Our work\ngeneralizes causal estimands to outcomes with any number of dimensions or any\nmeasurable space, and formulates traditional causal estimands for nominal\nvariables as causal discrepancy tests. We propose a simple technique for\nadjusting universally consistent conditional independence tests and prove that\nthese tests are universally consistent causal discrepancy tests. Numerical\nexperiments illustrate that our method, Causal CDcorr, leads to improvements in\nboth finite sample validity and power when compared to existing strategies. Our\nmethods are all open source and available at github.com/ebridge2/cdcorr."}, "http://arxiv.org/abs/2307.16353": {"title": "Single Proxy Synthetic Control", "link": "http://arxiv.org/abs/2307.16353", "description": "Synthetic control methods are widely used to estimate the treatment effect on\na single treated unit in time-series settings. A common approach for estimating\nsynthetic controls is to regress the treated unit's pre-treatment outcome on\nthose of untreated units via ordinary least squares. However, this approach can\nperform poorly if the pre-treatment fit is not near perfect, whether the\nweights are normalized or not. In this paper, we introduce a single proxy\nsynthetic control approach, which views the outcomes of untreated units as\nproxies of the treatment-free potential outcome of the treated unit, a\nperspective we leverage to construct a valid synthetic control. Under this\nframework, we establish alternative identification and estimation methodologies\nfor synthetic controls and for the treatment effect on the treated unit.\nNotably, unlike a proximal synthetic control approach which requires two types\nof proxies for identification, ours relies on a single type of proxy, thus\nfacilitating its practical relevance. Additionally, we adapt a conformal\ninference approach to perform inference about the treatment effect, obviating\nthe need for a large number of post-treatment data. Lastly, our framework can\naccommodate time-varying covariates and nonlinear models. We demonstrate the\nproposed approach in a simulation study and a real-world application."}, "http://arxiv.org/abs/2206.12346": {"title": "A fast and stable approximate maximum-likelihood method for template fits", "link": "http://arxiv.org/abs/2206.12346", "description": "Barlow and Beeston presented an exact likelihood for the problem of fitting a\ncomposite model consisting of binned templates obtained from Monte-Carlo\nsimulation which are fitted to equally binned data. Solving the exact\nlikelihood is technically challenging, and therefore Conway proposed an\napproximate likelihood to address these challenges. In this paper, a new\napproximate likelihood is derived from the exact Barlow-Beeston one. The new\napproximate likelihood and Conway's likelihood are generalized to problems of\nfitting weighted data with weighted templates. The performance of estimates\nobtained with all three likelihoods is studied on two toy examples: a simple\none and a challenging one. The performance of the approximate likelihoods is\ncomparable to the exact Barlow-Beeston likelihood, while the performance in\nfits with weighted templates is better. The approximate likelihoods evaluate\nfaster than the Barlow-Beeston one when the number of bins is large."}, "http://arxiv.org/abs/2307.16370": {"title": "Inference for Low-rank Completion without Sample Splitting with Application to Treatment Effect Estimation", "link": "http://arxiv.org/abs/2307.16370", "description": "This paper studies the inferential theory for estimating low-rank matrices.\nIt also provides an inference method for the average treatment effect as an\napplication. We show that the least square estimation of eigenvectors following\nthe nuclear norm penalization attains the asymptotic normality. The key\ncontribution of our method is that it does not require sample splitting. In\naddition, this paper allows dependent observation patterns and heterogeneous\nobservation probabilities. Empirically, we apply the proposed procedure to\nestimating the impact of the presidential vote on allocating the U.S. federal\nbudget to the states."}, "http://arxiv.org/abs/2311.14679": {"title": "\"Medium-n studies\" in computing education conferences", "link": "http://arxiv.org/abs/2311.14679", "description": "Good (Frequentist) statistical practice requires that statistical tests be\nperformed in order to determine if the phenomenon being observed could\nplausibly occur by chance if the null hypothesis is false. Good practice also\nrequires that a test is not performed if the study is underpowered: if the\nnumber of observations is not sufficiently large to be able to reliably detect\nthe effect one hypothesizes, even if the effect exists. Running underpowered\nstudies runs the risk of false negative results. This creates tension in the\nguidelines and expectations for computer science education conferences: while\nthings are clear for studies with a large number of observations, researchers\nshould in fact not compute p-values and perform statistical tests if the number\nof observations is too small. The issue is particularly live in CSed venues,\nsince class sizes where those issues are salient are common. We outline the\nconsiderations for when to compute and when not to compute p-values in\ndifferent settings encountered by computer science education researchers. We\nsurvey the author and reviewer guidelines in different computer science\neducation conferences (ICER, SIGCSE TS, ITiCSE, EAAI, CompEd, Koli Calling). We\npresent summary data and make several preliminary observations about reviewer\nguidelines: guidelines vary from conference to conference; guidelines allow for\nqualitative studies, and, in some cases, experience reports, but guidelines do\nnot generally explicitly indicate that a paper should have at least one of (1)\nan appropriately-powered statistical analysis or (2) rich qualitative\ndescriptions. We present preliminary ideas for addressing the tension in the\nguidelines between small-n and large-n studies"}, "http://arxiv.org/abs/2311.16440": {"title": "Inference for Low-rank Models without Estimating the Rank", "link": "http://arxiv.org/abs/2311.16440", "description": "This paper studies the inference about linear functionals of high-dimensional\nlow-rank matrices. While most existing inference methods would require\nconsistent estimation of the true rank, our procedure is robust to rank\nmisspecification, making it a promising approach in applications where rank\nestimation can be unreliable. We estimate the low-rank spaces using\npre-specified weighting matrices, known as diversified projections. A novel\nstatistical insight is that, unlike the usual statistical wisdom that\noverfitting mainly introduces additional variances, the over-estimated low-rank\nspace also gives rise to a non-negligible bias due to an implicit ridge-type\nregularization. We develop a new inference procedure and show that the central\nlimit theorem holds as long as the pre-specified rank is no smaller than the\ntrue rank. Empirically, we apply our method to the U.S. federal grants\nallocation data and test the existence of pork-barrel politics."}, "http://arxiv.org/abs/2311.17962": {"title": "In search of the perfect fit: interpretation, flexible modelling, and the existing generalisations of the normal distribution", "link": "http://arxiv.org/abs/2311.17962", "description": "Many generalised distributions exist for modelling data with vastly diverse\ncharacteristics. However, very few of these generalisations of the normal\ndistribution have shape parameters with clear roles that determine, for\ninstance, skewness and tail shape. In this chapter, we review existing skewing\nmechanisms and their properties in detail. Using the knowledge acquired, we add\na skewness parameter to the body-tail generalised normal distribution\n\\cite{BTGN}, that yields the \\ac{FIN} with parameters for location, scale,\nbody-shape, skewness, and tail weight. Basic statistical properties of the\n\\ac{FIN} are provided, such as the \\ac{PDF}, cumulative distribution function,\nmoments, and likelihood equations. Additionally, the \\ac{FIN} \\ac{PDF} is\nextended to a multivariate setting using a student t-copula, yielding the\n\\ac{MFIN}. The \\ac{MFIN} is applied to stock returns data, where it outperforms\nthe t-copula multivariate generalised hyperbolic, Azzalini skew-t, hyperbolic,\nand normal inverse Gaussian distributions."}, "http://arxiv.org/abs/2311.18039": {"title": "Restricted Regression in Networks", "link": "http://arxiv.org/abs/2311.18039", "description": "Network regression with additive node-level random effects can be problematic\nwhen the primary interest is estimating unconditional regression coefficients\nand some covariates are exactly or nearly in the vector space of node-level\neffects. We introduce the Restricted Network Regression model, that removes the\ncollinearity between fixed and random effects in network regression by\northogonalizing the random effects against the covariates. We discuss the\nchange in the interpretation of the regression coefficients in Restricted\nNetwork Regression and analytically characterize the effect of Restricted\nNetwork Regression on the regression coefficients for continuous response data.\nWe show through simulation with continuous and binary response data that\nRestricted Network Regression mitigates, but does not alleviate, network\nconfounding, by providing improved estimation of the regression coefficients.\nWe apply the Restricted Network Regression model in an analysis of 2015\nEurovision Song Contest voting data and show how the choice of regression model\naffects inference."}, "http://arxiv.org/abs/2311.18048": {"title": "An Interventional Perspective on Identifiability in Gaussian LTI Systems with Independent Component Analysis", "link": "http://arxiv.org/abs/2311.18048", "description": "We investigate the relationship between system identification and\nintervention design in dynamical systems. While previous research demonstrated\nhow identifiable representation learning methods, such as Independent Component\nAnalysis (ICA), can reveal cause-effect relationships, it relied on a passive\nperspective without considering how to collect data. Our work shows that in\nGaussian Linear Time-Invariant (LTI) systems, the system parameters can be\nidentified by introducing diverse intervention signals in a multi-environment\nsetting. By harnessing appropriate diversity assumptions motivated by the ICA\nliterature, our findings connect experiment design and representational\nidentifiability in dynamical systems. We corroborate our findings on synthetic\nand (simulated) physical data. Additionally, we show that Hidden Markov Models,\nin general, and (Gaussian) LTI systems, in particular, fulfil a generalization\nof the Causal de Finetti theorem with continuous parameters."}, "http://arxiv.org/abs/2311.18053": {"title": "On Non- and Weakly-Informative Priors for the Conway-Maxwell-Poisson (COM-Poisson) Distribution", "link": "http://arxiv.org/abs/2311.18053", "description": "Previous Bayesian evaluations of the Conway-Maxwell-Poisson (COM-Poisson)\ndistribution have little discussion of non- and weakly-informative priors for\nthe model. While only considering priors with such limited information\nrestricts potential analyses, these priors serve an important first step in the\nmodeling process and are useful when performing sensitivity analyses. We\ndevelop and derive several weakly- and non-informative priors using both the\nestablished conjugate prior and Jeffreys' prior. Our evaluation of each prior\ninvolves an empirical study under varying dispersion types and sample sizes. In\ngeneral, we find the weakly informative priors tend to perform better than the\nnon-informative priors. We also consider several data examples for illustration\nand provide code for implementation of each resulting posterior."}, "http://arxiv.org/abs/2311.18093": {"title": "Shared Control Individuals in Health Policy Evaluations with Application to Medical Cannabis Laws", "link": "http://arxiv.org/abs/2311.18093", "description": "Health policy researchers often have questions about the effects of a policy\nimplemented at some cluster-level unit, e.g., states, counties, hospitals, etc.\non individual-level outcomes collected over multiple time periods. Stacked\ndifference-in-differences is an increasingly popular way to estimate these\neffects. This approach involves estimating treatment effects for each\npolicy-implementing unit, then, if scientifically appropriate, aggregating them\nto an average effect estimate. However, when individual-level data are\navailable and non-implementing units are used as comparators for multiple\npolicy-implementing units, data from untreated individuals may be used across\nmultiple analyses, thereby inducing correlation between effect estimates.\nExisting methods do not quantify or account for this sharing of controls. Here,\nwe describe a stacked difference-in-differences study investigating the effects\nof state medical cannabis laws on treatment for chronic pain management that\nmotivated this work, discuss a framework for estimating and managing this\ncorrelation due to shared control individuals, and show how accounting for it\naffects the substantive results."}, "http://arxiv.org/abs/2311.18146": {"title": "Co-Active Subspace Methods for the Joint Analysis of Adjacent Computer Models", "link": "http://arxiv.org/abs/2311.18146", "description": "Active subspace (AS) methods are a valuable tool for understanding the\nrelationship between the inputs and outputs of a Physics simulation. In this\npaper, an elegant generalization of the traditional ASM is developed to assess\nthe co-activity of two computer models. This generalization, which we refer to\nas a Co-Active Subspace (C-AS) Method, allows for the joint analysis of two or\nmore computer models allowing for thorough exploration of the alignment (or\nnon-alignment) of the respective gradient spaces. We define co-active\ndirections, co-sensitivity indices, and a scalar ``concordance\" metric (and\ncomplementary ``discordance\" pseudo-metric) and we demonstrate that these are\npowerful tools for understanding the behavior of a class of computer models,\nespecially when used to supplement traditional AS analysis. Details for\nefficient estimation of the C-AS and an accompanying R package\n(github.com/knrumsey/concordance) are provided. Practical application is\ndemonstrated through analyzing a set of simulated rate stick experiments for\nPBX 9501, a high explosive, offering insights into complex model dynamics."}, "http://arxiv.org/abs/2311.18274": {"title": "Semiparametric Efficient Inference in Adaptive Experiments", "link": "http://arxiv.org/abs/2311.18274", "description": "We consider the problem of efficient inference of the Average Treatment\nEffect in a sequential experiment where the policy governing the assignment of\nsubjects to treatment or control can change over time. We first provide a\ncentral limit theorem for the Adaptive Augmented Inverse-Probability Weighted\nestimator, which is semiparametric efficient, under weaker assumptions than\nthose previously made in the literature. This central limit theorem enables\nefficient inference at fixed sample sizes. We then consider a sequential\ninference setting, deriving both asymptotic and nonasymptotic confidence\nsequences that are considerably tighter than previous methods. These\nanytime-valid methods enable inference under data-dependent stopping times\n(sample sizes). Additionally, we use propensity score truncation techniques\nfrom the recent off-policy estimation literature to reduce the finite sample\nvariance of our estimator without affecting the asymptotic variance. Empirical\nresults demonstrate that our methods yield narrower confidence sequences than\nthose previously developed in the literature while maintaining time-uniform\nerror control."}, "http://arxiv.org/abs/2311.18294": {"title": "Multivariate Unified Skew-t Distributions And Their Properties", "link": "http://arxiv.org/abs/2311.18294", "description": "The unified skew-t (SUT) is a flexible parametric multivariate distribution\nthat accounts for skewness and heavy tails in the data. A few of its properties\ncan be found scattered in the literature or in a parameterization that does not\nfollow the original one for unified skew-normal (SUN) distributions, yet a\nsystematic study is lacking. In this work, explicit properties of the\nmultivariate SUT distribution are presented, such as its stochastic\nrepresentations, moments, SUN-scale mixture representation, linear\ntransformation, additivity, marginal distribution, canonical form, quadratic\nform, conditional distribution, change of latent dimensions, Mardia measures of\nmultivariate skewness and kurtosis, and non-identifiability issue. These\nresults are given in a parametrization that reduces to the original SUN\ndistribution as a sub-model, hence facilitating the use of the SUT for\napplications. Several models based on the SUT distribution are provided for\nillustration."}, "http://arxiv.org/abs/2311.18446": {"title": "Length-of-stay times in hospital for COVID-19 patients using the smoothed Beran's estimator with bootstrap bandwidth selection", "link": "http://arxiv.org/abs/2311.18446", "description": "The survival function of length-of-stay in hospital ward and ICU for COVID-19\npatients is studied in this paper. Flexible statistical methods are used to\nestimate this survival function given relevant covariates such as age, sex,\nobesity and chronic obstructive pulmonary disease (COPD). A doubly-smoothed\nBeran's estimator has been considered to this aim. The bootstrap method has\nbeen used to produce new smoothing parameter selectors and to construct\nconfidence regions for the conditional survival function. Some simulation\nstudies show the good performance of the proposed methods."}, "http://arxiv.org/abs/2311.18460": {"title": "Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework", "link": "http://arxiv.org/abs/2311.18460", "description": "Fairness for machine learning predictions is widely required in practice for\nlegal, ethical, and societal reasons. Existing work typically focuses on\nsettings without unobserved confounding, even though unobserved confounding can\nlead to severe violations of causal fairness and, thus, unfair predictions. In\nthis work, we analyze the sensitivity of causal fairness to unobserved\nconfounding. Our contributions are three-fold. First, we derive bounds for\ncausal fairness metrics under different sources of unobserved confounding. This\nenables practitioners to examine the sensitivity of their machine learning\nmodels to unobserved confounding in fairness-critical applications. Second, we\npropose a novel neural framework for learning fair predictions, which allows us\nto offer worst-case guarantees of the extent to which causal fairness can be\nviolated due to unobserved confounding. Third, we demonstrate the effectiveness\nof our framework in a series of experiments, including a real-world case study\nabout predicting prison sentences. To the best of our knowledge, ours is the\nfirst work to study causal fairness under unobserved confounding. To this end,\nour work is of direct practical value as a refutation strategy to ensure the\nfairness of predictions in high-stakes applications."}, "http://arxiv.org/abs/2311.18477": {"title": "Intraday foreign exchange rate volatility forecasting: univariate and multilevel functional GARCH models", "link": "http://arxiv.org/abs/2311.18477", "description": "This paper seeks to predict conditional intraday volatility in foreign\nexchange (FX) markets using functional Generalized AutoRegressive Conditional\nHeteroscedasticity (GARCH) models. We contribute to the existing functional\nGARCH-type models by accounting for the stylised features of long-range and\ncross-dependence through estimating the models with long-range dependent and\nmulti-level functional principal component basis functions. Remarkably, we find\nthat taking account of cross-dependency dynamics between the major currencies\nsignificantly improves intraday conditional volatility forecasting.\nAdditionally, incorporating intraday bid-ask spread using a functional GARCH-X\nmodel adds explainability of long-range dependence and further enhances\npredictability. Intraday risk management applications are presented to\nhighlight the practical economic benefits of our proposed approaches."}, "http://arxiv.org/abs/2311.18501": {"title": "Perturbation-based Analysis of Compositional Data", "link": "http://arxiv.org/abs/2311.18501", "description": "Existing statistical methods for compositional data analysis are inadequate\nfor many modern applications for two reasons. First, modern compositional\ndatasets, for example in microbiome research, display traits such as\nhigh-dimensionality and sparsity that are poorly modelled with traditional\napproaches. Second, assessing -- in an unbiased way -- how summary statistics\nof a composition (e.g., racial diversity) affect a response variable is not\nstraightforward. In this work, we propose a framework based on hypothetical\ndata perturbations that addresses both issues. Unlike existing methods for\ncompositional data, we do not transform the data and instead use perturbations\nto define interpretable statistical functionals on the compositions themselves,\nwhich we call average perturbation effects. These average perturbation effects,\nwhich can be employed in many applications, naturally account for confounding\nthat biases frequently used marginal dependence analyses. We show how average\nperturbation effects can be estimated efficiently by deriving a\nperturbation-dependent reparametrization and applying semiparametric estimation\ntechniques. We analyze the proposed estimators empirically on simulated data\nand demonstrate advantages over existing techniques on US census and microbiome\ndata. For all proposed estimators, we provide confidence intervals with uniform\nasymptotic coverage guarantees."}, "http://arxiv.org/abs/2311.18532": {"title": "Local causal effects with continuous exposures: A matching estimator for the average causal derivative effect", "link": "http://arxiv.org/abs/2311.18532", "description": "The estimation of causal effects is a fundamental goal in the field of causal\ninference. However, it is challenging for various reasons. One reason is that\nthe exposure (or treatment) is naturally continuous in many real-world\nscenarios. When dealing with continuous exposure, dichotomizing the exposure\nvariable based on a pre-defined threshold may result in a biased understanding\nof causal relationships. In this paper, we propose a novel causal inference\nframework that can measure the causal effect of continuous exposure. We define\nthe expectation of a derivative of potential outcomes at a specific exposure\nlevel as the average causal derivative effect. Additionally, we propose a\nmatching method for this estimator and propose a permutation approach to test\nthe hypothesis of no local causal effect. We also investigate the asymptotic\nproperties of the proposed estimator and examine its performance through\nsimulation studies. Finally, we apply this causal framework in a real data\nexample of Chronic Obstructive Pulmonary Disease (COPD) patients."}, "http://arxiv.org/abs/2311.18584": {"title": "First-order multivariate integer-valued autoregressive model with multivariate mixture distributions", "link": "http://arxiv.org/abs/2311.18584", "description": "The univariate integer-valued time series has been extensively studied, but\nliterature on multivariate integer-valued time series models is quite limited\nand the complex correlation structure among the multivariate integer-valued\ntime series is barely discussed. In this study, we proposed a first-order\nmultivariate integer-valued autoregressive model to characterize the\ncorrelation among multivariate integer-valued time series with higher\nflexibility. Under the general conditions, we established the stationarity and\nergodicity of the proposed model. With the proposed method, we discussed the\nmodels with multivariate Poisson-lognormal distribution and multivariate\ngeometric-logitnormal distribution and the corresponding properties. The\nestimation method based on EM algorithm was developed for the model parameters\nand extensive simulation studies were performed to evaluate the effectiveness\nof proposed estimation method. Finally, a real crime data was analyzed to\ndemonstrate the advantage of the proposed model with comparison to the other\nmodels."}, "http://arxiv.org/abs/2311.18699": {"title": "Gaussian processes Correlated Bayesian Additive Regression Trees", "link": "http://arxiv.org/abs/2311.18699", "description": "In recent years, Bayesian Additive Regression Trees (BART) has garnered\nincreased attention, leading to the development of various extensions for\ndiverse applications. However, there has been limited exploration of its\nutility in analyzing correlated data. This paper introduces a novel extension\nof BART, named Correlated BART (CBART). Unlike the original BART with\nindependent errors, CBART is specifically designed to handle correlated\n(dependent) errors. Additionally, we propose the integration of CBART with\nGaussian processes (GP) to create a new model termed GP-CBART. This innovative\nmodel combines the strengths of the Gaussian processes and CBART, making it\nparticularly well-suited for analyzing time series or spatial data. In the\nGP-CBART framework, CBART captures the nonlinearity in the mean regression\n(covariates) function, while the Gaussian processes adeptly models the\ncorrelation structure within the response. Additionally, given the high\nflexibility of both CBART and GP models, their combination may lead to\nidentification issues. We provide methods to address these challenges. To\ndemonstrate the effectiveness of CBART and GP-CBART, we present corresponding\nsimulated and real-world examples."}, "http://arxiv.org/abs/2311.18725": {"title": "AI in Pharma for Personalized Sequential Decision-Making: Methods, Applications and Opportunities", "link": "http://arxiv.org/abs/2311.18725", "description": "In the pharmaceutical industry, the use of artificial intelligence (AI) has\nseen consistent growth over the past decade. This rise is attributed to major\nadvancements in statistical machine learning methodologies, computational\ncapabilities and the increased availability of large datasets. AI techniques\nare applied throughout different stages of drug development, ranging from drug\ndiscovery to post-marketing benefit-risk assessment. Kolluri et al. provided a\nreview of several case studies that span these stages, featuring key\napplications such as protein structure prediction, success probability\nestimation, subgroup identification, and AI-assisted clinical trial monitoring.\nFrom a regulatory standpoint, there was a notable uptick in submissions\nincorporating AI components in 2021. The most prevalent therapeutic areas\nleveraging AI were oncology (27%), psychiatry (15%), gastroenterology (12%),\nand neurology (11%). The paradigm of personalized or precision medicine has\ngained significant traction in recent research, partly due to advancements in\nAI techniques \\cite{hamburg2010path}. This shift has had a transformative\nimpact on the pharmaceutical industry. Departing from the traditional\n\"one-size-fits-all\" model, personalized medicine incorporates various\nindividual factors, such as environmental conditions, lifestyle choices, and\nhealth histories, to formulate customized treatment plans. By utilizing\nsophisticated machine learning algorithms, clinicians and researchers are\nbetter equipped to make informed decisions in areas such as disease prevention,\ndiagnosis, and treatment selection, thereby optimizing health outcomes for each\nindividual."}, "http://arxiv.org/abs/2311.18807": {"title": "Pre-registration for Predictive Modeling", "link": "http://arxiv.org/abs/2311.18807", "description": "Amid rising concerns of reproducibility and generalizability in predictive\nmodeling, we explore the possibility and potential benefits of introducing\npre-registration to the field. Despite notable advancements in predictive\nmodeling, spanning core machine learning tasks to various scientific\napplications, challenges such as overlooked contextual factors, data-dependent\ndecision-making, and unintentional re-use of test data have raised questions\nabout the integrity of results. To address these issues, we propose adapting\npre-registration practices from explanatory modeling to predictive modeling. We\ndiscuss current best practices in predictive modeling and their limitations,\nintroduce a lightweight pre-registration template, and present a qualitative\nstudy with machine learning researchers to gain insight into the effectiveness\nof pre-registration in preventing biased estimates and promoting more reliable\nresearch outcomes. We conclude by exploring the scope of problems that\npre-registration can address in predictive modeling and acknowledging its\nlimitations within this context."}, "http://arxiv.org/abs/1910.12431": {"title": "Multilevel Dimension-Independent Likelihood-Informed MCMC for Large-Scale Inverse Problems", "link": "http://arxiv.org/abs/1910.12431", "description": "We present a non-trivial integration of dimension-independent\nlikelihood-informed (DILI) MCMC (Cui, Law, Marzouk, 2016) and the multilevel\nMCMC (Dodwell et al., 2015) to explore the hierarchy of posterior\ndistributions. This integration offers several advantages: First, DILI-MCMC\nemploys an intrinsic likelihood-informed subspace (LIS) (Cui et al., 2014) --\nwhich involves a number of forward and adjoint model simulations -- to design\naccelerated operator-weighted proposals. By exploiting the multilevel structure\nof the discretised parameters and discretised forward models, we design a\nRayleigh-Ritz procedure to significantly reduce the computational effort in\nbuilding the LIS and operating with DILI proposals. Second, the resulting\nDILI-MCMC can drastically improve the sampling efficiency of MCMC at each\nlevel, and hence reduce the integration error of the multilevel algorithm for\nfixed CPU time. Numerical results confirm the improved computational efficiency\nof the multilevel DILI approach."}, "http://arxiv.org/abs/2107.01306": {"title": "The Effect of the Prior and the Experimental Design on the Inference of the Precision Matrix in Gaussian Chain Graph Models", "link": "http://arxiv.org/abs/2107.01306", "description": "Here, we investigate whether (and how) experimental design could aid in the\nestimation of the precision matrix in a Gaussian chain graph model, especially\nthe interplay between the design, the effect of the experiment and prior\nknowledge about the effect. Estimation of the precision matrix is a fundamental\ntask to infer biological graphical structures like microbial networks. We\ncompare the marginal posterior precision of the precision matrix under four\npriors: flat, conjugate Normal-Wishart, Normal-MGIG and a general independent.\nUnder the flat and conjugate priors, the Laplace-approximated posterior\nprecision is not a function of the design matrix rendering useless any efforts\nto find an optimal experimental design to infer the precision matrix. In\ncontrast, the Normal-MGIG and general independent priors do allow for the\nsearch of optimal experimental designs, yet there is a sharp upper bound on the\ninformation that can be extracted from a given experiment. We confirm our\ntheoretical findings via a simulation study comparing i) the KL divergence\nbetween prior and posterior and ii) the Stein's loss difference of MAPs between\nrandom and no experiment. Our findings provide practical advice for domain\nscientists conducting experiments to better infer the precision matrix as a\nrepresentation of a biological network."}, "http://arxiv.org/abs/2206.02508": {"title": "Tucker tensor factor models: matricization and mode-wise PCA estimation", "link": "http://arxiv.org/abs/2206.02508", "description": "High-dimensional, higher-order tensor data are gaining prominence in a\nvariety of fields, including but not limited to computer vision and network\nanalysis. Tensor factor models, induced from noisy versions of tensor\ndecomposition or factorization, are natural potent instruments to study a\ncollection of tensor-variate objects that may be dependent or independent.\nHowever, it is still in the early stage of developing statistical inferential\ntheories for estimation of various low-rank structures, which are customary to\nplay the role of signals of tensor factor models. In this paper, starting from\ntensor matricization, we aim to ``decode\" estimation of a higher-order tensor\nfactor model in the sense that, we recast it into mode-wise traditional\nhigh-dimensional vector/fiber factor models so as to deploy the conventional\nestimation of principle components analysis (PCA). Demonstrated by the Tucker\ntensor factor model (TuTFaM), which is induced from most popular Tucker\ndecomposition, we summarize that estimations on signal components are\nessentially mode-wise PCA techniques, and the involvement of projection and\niteration will enhance the signal-to-noise ratio to various extend. We\nestablish the inferential theory of the proposed estimations and conduct rich\nsimulation experiments under TuTFaM, and illustrate how the proposed\nestimations can work in tensor reconstruction, clustering for video and\neconomic datasets, respectively."}, "http://arxiv.org/abs/2211.13959": {"title": "Testing Homological Equivalence Using Betti Numbers", "link": "http://arxiv.org/abs/2211.13959", "description": "In this article, we propose a one-sample test to check whether the support of\nthe unknown distribution generating the data is homologically equivalent to the\nsupport of some specified distribution or not OR using the corresponding\ntwo-sample test, one can test whether the supports of two unknown distributions\nare homologically equivalent or not. In the course of this study, test\nstatistics based on the Betti numbers are formulated, and the consistency of\nthe tests is established under the critical and the supercritical regimes.\nMoreover, some simulation studies are conducted and results are compared with\nthe existing methodologies such as Robinson's permutation test and test based\non mean persistent landscape functions. Furthermore, the practicability of the\ntests is shown on two well-known real data sets also."}, "http://arxiv.org/abs/2302.08893": {"title": "Active learning for data streams: a survey", "link": "http://arxiv.org/abs/2302.08893", "description": "Online active learning is a paradigm in machine learning that aims to select\nthe most informative data points to label from a data stream. The problem of\nminimizing the cost associated with collecting labeled observations has gained\na lot of attention in recent years, particularly in real-world applications\nwhere data is only available in an unlabeled form. Annotating each observation\ncan be time-consuming and costly, making it difficult to obtain large amounts\nof labeled data. To overcome this issue, many active learning strategies have\nbeen proposed in the last decades, aiming to select the most informative\nobservations for labeling in order to improve the performance of machine\nlearning models. These approaches can be broadly divided into two categories:\nstatic pool-based and stream-based active learning. Pool-based active learning\ninvolves selecting a subset of observations from a closed pool of unlabeled\ndata, and it has been the focus of many surveys and literature reviews.\nHowever, the growing availability of data streams has led to an increase in the\nnumber of approaches that focus on online active learning, which involves\ncontinuously selecting and labeling observations as they arrive in a stream.\nThis work aims to provide an overview of the most recently proposed approaches\nfor selecting the most informative observations from data streams in real time.\nWe review the various techniques that have been proposed and discuss their\nstrengths and limitations, as well as the challenges and opportunities that\nexist in this area of research."}, "http://arxiv.org/abs/2304.14750": {"title": "Bayesian Testing of Scientific Expectations Under Exponential Random Graph Models", "link": "http://arxiv.org/abs/2304.14750", "description": "The exponential random graph (ERGM) model is a commonly used statistical\nframework for studying the determinants of tie formations from social network\ndata. To test scientific theories under the ERGM framework, statistical\ninferential techniques are generally used based on traditional significance\ntesting using p-values. This methodology has certain limitations, however, such\nas its inconsistent behavior when the null hypothesis is true, its inability to\nquantify evidence in favor of a null hypothesis, and its inability to test\nmultiple hypotheses with competing equality and/or order constraints on the\nparameters of interest in a direct manner. To tackle these shortcomings, this\npaper presents Bayes factors and posterior probabilities for testing scientific\nexpectations under a Bayesian framework. The methodology is implemented in the\nR package 'BFpack'. The applicability of the methodology is illustrated using\nempirical collaboration networks and policy networks."}, "http://arxiv.org/abs/2312.00130": {"title": "Sparse Projected Averaged Regression for High-Dimensional Data", "link": "http://arxiv.org/abs/2312.00130", "description": "We examine the linear regression problem in a challenging high-dimensional\nsetting with correlated predictors to explain and predict relevant quantities,\nwith explicitly allowing the regression coefficient to vary from sparse to\ndense. Most classical high-dimensional regression estimators require some\ndegree of sparsity. We discuss the more recent concepts of variable screening\nand random projection as computationally fast dimension reduction tools, and\npropose a new random projection matrix tailored to the linear regression\nproblem with a theoretical bound on the gain in expected prediction error over\nconventional random projections.\n\nAround this new random projection, we built the Sparse Projected Averaged\nRegression (SPAR) method combining probabilistic variable screening steps with\nthe random projection steps to obtain an ensemble of small linear models. In\ndifference to existing methods, we introduce a thresholding parameter to obtain\nsome degree of sparsity.\n\nIn extensive simulations and two real data applications we guide through the\nelements of this method and compare prediction and variable selection\nperformance to various competitors. For prediction, our method performs at\nleast as good as the best competitors in most settings with a high number of\ntruly active variables, while variable selection remains a hard task for all\nmethods in high dimensions."}, "http://arxiv.org/abs/2312.00185": {"title": "On the variance of the Least Mean Square squared-error sample curve", "link": "http://arxiv.org/abs/2312.00185", "description": "Most studies of adaptive algorithm behavior consider performance measures\nbased on mean values such as the mean-square error. The derived models are\nuseful for understanding the algorithm behavior under different environments\nand can be used for design. Nevertheless, from a practical point of view, the\nadaptive filter user has only one realization of the algorithm to obtain the\ndesired result. This letter derives a model for the variance of the\nsquared-error sample curve of the least-mean-square (LMS) adaptive algorithm,\nso that the achievable cancellation level can be predicted based on the\nproperties of the steady-state squared error. The derived results provide the\nuser with useful design guidelines."}, "http://arxiv.org/abs/2312.00219": {"title": "The Functional Average Treatment Effect", "link": "http://arxiv.org/abs/2312.00219", "description": "This paper establishes the functional average as an important estimand for\ncausal inference. The significance of the estimand lies in its robustness\nagainst traditional issues of confounding. We prove that this robustness holds\neven when the probability distribution of the outcome, conditional on treatment\nor some other vector of adjusting variables, differs almost arbitrarily from\nits counterfactual analogue. This paper also examines possible estimators of\nthe functional average, including the sample mid-range, and proposes a new type\nof bootstrap for robust statistical inference: the Hoeffding bootstrap. After\nthis, the paper explores a new class of variables, the $\\mathcal{U}$ class of\nvariables, that simplifies the estimation of functional averages. This class of\nvariables is also used to establish mean exchangeability in some cases and to\nprovide the results of elementary statistical procedures, such as linear\nregression and the analysis of variance, with causal interpretations.\nSimulation evidence is provided. The methods of this paper are also applied to\na National Health and Nutrition Survey data set to investigate the causal\neffect of exercise on the blood pressure of adult smokers."}, "http://arxiv.org/abs/2312.00225": {"title": "Eliminating confounder-induced bias in the statistics of intervention", "link": "http://arxiv.org/abs/2312.00225", "description": "Experimental and observational studies often lead to spurious association\nbetween the outcome and independent variables describing the intervention,\nbecause of confounding to third-party factors. Even in randomized clinical\ntrials, confounding might be unavoidable due to small sample sizes.\nPractically, this poses a problem, because it is either expensive to re-design\nand conduct a new study or even impossible to alleviate the contribution of\nsome confounders due to e.g. ethical concerns. Here, we propose a method to\nconsistently derive hypothetical studies that retain as many of the\ndependencies in the original study as mathematically possible, while removing\nany association of observed confounders to the independent variables. Using\nhistoric studies, we illustrate how the confounding-free scenario re-estimates\nthe effect size of the intervention. The new effect size estimate represents a\nconcise prediction in the hypothetical scenario which paves a way from the\noriginal data towards the design of future studies."}, "http://arxiv.org/abs/2312.00294": {"title": "aeons: approximating the end of nested sampling", "link": "http://arxiv.org/abs/2312.00294", "description": "This paper presents analytic results on the anatomy of nested sampling, from\nwhich a technique is developed to estimate the run-time of the algorithm that\nworks for any nested sampling implementation. We test these methods on both toy\nmodels and true cosmological nested sampling runs. The method gives an\norder-of-magnitude prediction of the end point at all times, forecasting the\ntrue endpoint within standard error around the halfway point."}, "http://arxiv.org/abs/2312.00305": {"title": "Multiple Testing of Linear Forms for Noisy Matrix Completion", "link": "http://arxiv.org/abs/2312.00305", "description": "Many important tasks of large-scale recommender systems can be naturally cast\nas testing multiple linear forms for noisy matrix completion. These problems,\nhowever, present unique challenges because of the subtle bias-and-variance\ntradeoff of and an intricate dependence among the estimated entries induced by\nthe low-rank structure. In this paper, we develop a general approach to\novercome these difficulties by introducing new statistics for individual tests\nwith sharp asymptotics both marginally and jointly, and utilizing them to\ncontrol the false discovery rate (FDR) via a data splitting and symmetric\naggregation scheme. We show that valid FDR control can be achieved with\nguaranteed power under nearly optimal sample size requirements using the\nproposed methodology. Extensive numerical simulations and real data examples\nare also presented to further illustrate its practical merits."}, "http://arxiv.org/abs/2312.00346": {"title": "Supervised Factor Modeling for High-Dimensional Linear Time Series", "link": "http://arxiv.org/abs/2312.00346", "description": "Motivated by Tucker tensor decomposition, this paper imposes low-rank\nstructures to the column and row spaces of coefficient matrices in a\nmultivariate infinite-order vector autoregression (VAR), which leads to a\nsupervised factor model with two factor modelings being conducted to responses\nand predictors simultaneously. Interestingly, the stationarity condition\nimplies an intrinsic weak group sparsity mechanism of infinite-order VAR, and\nhence a rank-constrained group Lasso estimation is considered for\nhigh-dimensional linear time series. Its non-asymptotic properties are\ndiscussed thoughtfully by balancing the estimation, approximation and\ntruncation errors. Moreover, an alternating gradient descent algorithm with\nthresholding is designed to search for high-dimensional estimates, and its\ntheoretical justifications, including statistical and convergence analysis, are\nalso provided. Theoretical and computational properties of the proposed\nmethodology are verified by simulation experiments, and the advantages over\nexisting methods are demonstrated by two real examples."}, "http://arxiv.org/abs/2312.00373": {"title": "Streaming Bayesian Modeling for predicting Fat-Tailed Customer Lifetime Value", "link": "http://arxiv.org/abs/2312.00373", "description": "We develop an online learning MCMC approach applicable for hierarchical\nbayesian models and GLMS. We also develop a fat-tailed LTV model that\ngeneralizes over several kinds of fat and thin tails. We demonstrate both\ndevelopments on commercial LTV data from a large mobile app."}, "http://arxiv.org/abs/2312.00439": {"title": "Modeling the Ratio of Correlated Biomarkers Using Copula Regression", "link": "http://arxiv.org/abs/2312.00439", "description": "Modeling the ratio of two dependent components as a function of covariates is\na frequently pursued objective in observational research. Despite the high\nrelevance of this topic in medical studies, where biomarker ratios are often\nused as surrogate endpoints for specific diseases, existing models are based on\noversimplified assumptions, assuming e.g.\\@ independence or strictly positive\nassociations between the components. In this paper, we close this gap in the\nliterature and propose a regression model where the marginal distributions of\nthe two components are linked by Frank copula. A key feature of our model is\nthat it allows for both positive and negative correlations between the\ncomponents, with one of the model parameters being directly interpretable in\nterms of Kendall's rank correlation coefficient. We study our method\ntheoretically, evaluate finite sample properties in a simulation study and\ndemonstrate its efficacy in an application to diagnosis of Alzheimer's disease\nvia ratios of amyloid-beta and total tau protein biomarkers."}, "http://arxiv.org/abs/2312.00494": {"title": "Applying the estimands framework to non-inferiority trials: guidance on choice of hypothetical estimands for non-adherence and comparison of estimation methods", "link": "http://arxiv.org/abs/2312.00494", "description": "A common concern in non-inferiority (NI) trials is that non adherence due,\nfor example, to poor study conduct can make treatment arms artificially\nsimilar. Because intention to treat analyses can be anti-conservative in this\nsituation, per protocol analyses are sometimes recommended. However, such\nadvice does not consider the estimands framework, nor the risk of bias from per\nprotocol analyses. We therefore sought to update the above guidance using the\nestimands framework, and compare estimators to improve on the performance of\nper protocol analyses. We argue the main threat to validity of NI trials is the\noccurrence of trial specific intercurrent events (IEs), that is, IEs which\noccur in a trial setting, but would not occur in practice. To guard against\nerroneous conclusions of non inferiority, we suggest an estimand using a\nhypothetical strategy for trial specific IEs should be employed, with handling\nof other non trial specific IEs chosen based on clinical considerations. We\nprovide an overview of estimators that could be used to estimate a hypothetical\nestimand, including inverse probability weighting (IPW), and two instrumental\nvariable approaches (one using an informative Bayesian prior on the effect of\nstandard treatment, and one using a treatment by covariate interaction as an\ninstrument). We compare them, using simulation in the setting of all or nothing\ncompliance in two active treatment arms, and conclude both IPW and the\ninstrumental variable method using a Bayesian prior are potentially useful\napproaches, with the choice between them depending on which assumptions are\nmost plausible for a given trial."}, "http://arxiv.org/abs/2312.00501": {"title": "Cautionary Tales on Synthetic Controls in Survival Analyses", "link": "http://arxiv.org/abs/2312.00501", "description": "Synthetic control (SC) methods have gained rapid popularity in economics\nrecently, where they have been applied in the context of inferring the effects\nof treatments on standard continuous outcomes assuming linear input-output\nrelations. In medical applications, conversely, survival outcomes are often of\nprimary interest, a setup in which both commonly assumed data-generating\nprocesses (DGPs) and target parameters are different. In this paper, we\ntherefore investigate whether and when SCs could serve as an alternative to\nmatching methods in survival analyses. We find that, because SCs rely on a\nlinearity assumption, they will generally be biased for the true expected\nsurvival time in commonly assumed survival DGPs -- even when taking into\naccount the possibility of linearity on another scale as in accelerated failure\ntime models. Additionally, we find that, because SC units follow distributions\nwith lower variance than real control units, summaries of their distributions,\nsuch as survival curves, will be biased for the parameters of interest in many\nsurvival analyses. Nonetheless, we also highlight that using SCs can still\nimprove upon matching whenever the biases described above are outweighed by\nextrapolation biases exhibited by imperfect matches, and investigate the use of\nregularization to trade off the shortcomings of both approaches."}, "http://arxiv.org/abs/2312.00509": {"title": "Bayesian causal discovery from unknown general interventions", "link": "http://arxiv.org/abs/2312.00509", "description": "We consider the problem of learning causal Directed Acyclic Graphs (DAGs)\nusing combinations of observational and interventional experimental data.\nCurrent methods tailored to this setting assume that interventions either\ndestroy parent-child relations of the intervened (target) nodes or only alter\nsuch relations without modifying the parent sets, even when the intervention\ntargets are unknown. We relax this assumption by proposing a Bayesian method\nfor causal discovery from general interventions, which allow for modifications\nof the parent sets of the unknown targets. Even in this framework, DAGs and\ngeneral interventions may be identifiable only up to some equivalence classes.\nWe provide graphical characterizations of such interventional Markov\nequivalence and devise compatible priors for Bayesian inference that guarantee\nscore equivalence of indistinguishable structures. We then develop a Markov\nChain Monte Carlo (MCMC) scheme to approximate the posterior distribution over\nDAGs, intervention targets and induced parent sets. Finally, we evaluate the\nproposed methodology on both simulated and real protein expression data."}, "http://arxiv.org/abs/2312.00530": {"title": "New tools for network time series with an application to COVID-19 hospitalisations", "link": "http://arxiv.org/abs/2312.00530", "description": "Network time series are becoming increasingly important across many areas in\nscience and medicine and are often characterised by a known or inferred\nunderlying network structure, which can be exploited to make sense of dynamic\nphenomena that are often high-dimensional. For example, the Generalised Network\nAutoregressive (GNAR) models exploit such structure parsimoniously. We use the\nGNAR framework to introduce two association measures: the network and partial\nnetwork autocorrelation functions, and introduce Corbit (correlation-orbit)\nplots for visualisation. As with regular autocorrelation plots, Corbit plots\npermit interpretation of underlying correlation structures and, crucially, aid\nmodel selection more rapidly than using other tools such as AIC or BIC. We\nadditionally interpret GNAR processes as generalised graphical models, which\nconstrain the processes' autoregressive structure and exhibit interesting\ntheoretical connections to graphical models via utilization of higher-order\ninteractions. We demonstrate how incorporation of prior information is related\nto performing variable selection and shrinkage in the GNAR context. We\nillustrate the usefulness of the GNAR formulation, network autocorrelations and\nCorbit plots by modelling a COVID-19 network time series of the number of\nadmissions to mechanical ventilation beds at 140 NHS Trusts in England &amp; Wales.\nWe introduce the Wagner plot that can analyse correlations over different time\nperiods or with respect to external covariates. In addition, we introduce plots\nthat quantify the relevance and influence of individual nodes. Our modelling\nprovides insight on the underlying dynamics of the COVID-19 series, highlights\ntwo groups of geographically co-located `influential' NHS Trusts and\ndemonstrates superior prediction abilities when compared to existing\ntechniques."}, "http://arxiv.org/abs/2312.00616": {"title": "Investigating a domain adaptation approach for integrating different measurement instruments in a longitudinal clinical registry", "link": "http://arxiv.org/abs/2312.00616", "description": "In a longitudinal clinical registry, different measurement instruments might\nhave been used for assessing individuals at different time points. To combine\nthem, we investigate deep learning techniques for obtaining a joint latent\nrepresentation, to which the items of different measurement instruments are\nmapped. This corresponds to domain adaptation, an established concept in\ncomputer science for image data. Using the proposed approach as an example, we\nevaluate the potential of domain adaptation in a longitudinal cohort setting\nwith a rather small number of time points, motivated by an application with\ndifferent motor function measurement instruments in a registry of spinal\nmuscular atrophy (SMA) patients. There, we model trajectories in the latent\nrepresentation by ordinary differential equations (ODEs), where person-specific\nODE parameters are inferred from baseline characteristics. The goodness of fit\nand complexity of the ODE solutions then allows to judge the measurement\ninstrument mappings. We subsequently explore how alignment can be improved by\nincorporating corresponding penalty terms into model fitting. To systematically\ninvestigate the effect of differences between measurement instruments, we\nconsider several scenarios based on modified SMA data, including scenarios\nwhere a mapping should be feasible in principle and scenarios where no perfect\nmapping is available. While misalignment increases in more complex scenarios,\nsome structure is still recovered, even if the availability of measurement\ninstruments depends on patient state. A reasonable mapping is feasible also in\nthe more complex real SMA dataset. These results indicate that domain\nadaptation might be more generally useful in statistical modeling for\nlongitudinal registry data."}, "http://arxiv.org/abs/2312.00622": {"title": "Practical Path-based Bayesian Optimization", "link": "http://arxiv.org/abs/2312.00622", "description": "There has been a surge in interest in data-driven experimental design with\napplications to chemical engineering and drug manufacturing. Bayesian\noptimization (BO) has proven to be adaptable to such cases, since we can model\nthe reactions of interest as expensive black-box functions. Sometimes, the cost\nof this black-box functions can be separated into two parts: (a) the cost of\nthe experiment itself, and (b) the cost of changing the input parameters. In\nthis short paper, we extend the SnAKe algorithm to deal with both types of\ncosts simultaneously. We further propose extensions to the case of a maximum\nallowable input change, as well as to the multi-objective setting."}, "http://arxiv.org/abs/2312.00710": {"title": "SpaCE: The Spatial Confounding Environment", "link": "http://arxiv.org/abs/2312.00710", "description": "Spatial confounding poses a significant challenge in scientific studies\ninvolving spatial data, where unobserved spatial variables can influence both\ntreatment and outcome, possibly leading to spurious associations. To address\nthis problem, we introduce SpaCE: The Spatial Confounding Environment, the\nfirst toolkit to provide realistic benchmark datasets and tools for\nsystematically evaluating causal inference methods designed to alleviate\nspatial confounding. Each dataset includes training data, true counterfactuals,\na spatial graph with coordinates, and smoothness and confounding scores\ncharacterizing the effect of a missing spatial confounder. It also includes\nrealistic semi-synthetic outcomes and counterfactuals, generated using\nstate-of-the-art machine learning ensembles, following best practices for\ncausal inference benchmarks. The datasets cover real treatment and covariates\nfrom diverse domains, including climate, health and social sciences. SpaCE\nfacilitates an automated end-to-end pipeline, simplifying data loading,\nexperimental setup, and evaluating machine learning and causal inference\nmodels. The SpaCE project provides several dozens of datasets of diverse sizes\nand spatial complexity. It is publicly available as a Python package,\nencouraging community feedback and contributions."}, "http://arxiv.org/abs/2312.00728": {"title": "Soft computing for the posterior of a new matrix t graphical network", "link": "http://arxiv.org/abs/2312.00728", "description": "Modelling noisy data in a network context remains an unavoidable obstacle;\nfortunately, random matrix theory may comprehensively describe network\nenvironments effectively. Thus it necessitates the probabilistic\ncharacterisation of these networks (and accompanying noisy data) using matrix\nvariate models. Denoising network data using a Bayes approach is not common in\nsurveyed literature. This paper adopts the Bayesian viewpoint and introduces a\nnew matrix variate t-model in a prior sense by relying on the matrix variate\ngamma distribution for the noise process, following the Gaussian graphical\nnetwork for the cases when the normality assumption is violated. From a\nstatistical learning viewpoint, such a theoretical consideration indubitably\nbenefits the real-world comprehension of structures causing noisy data with\nnetwork-based attributes as part of machine learning in data science. A full\nstructural learning procedure is provided for calculating and approximating the\nresulting posterior of interest to assess the considered model's network\ncentrality measures. Experiments with synthetic and real-world stock price data\nare performed not only to validate the proposed algorithm's capabilities but\nalso to show that this model has wider flexibility than originally implied in\nBillio et al. (2021)."}, "http://arxiv.org/abs/2312.00770": {"title": "Random Forest for Dynamic Risk Prediction or Recurrent Events: A Pseudo-Observation Approach", "link": "http://arxiv.org/abs/2312.00770", "description": "Recurrent events are common in clinical, healthcare, social and behavioral\nstudies. A recent analysis framework for potentially censored recurrent event\ndata is to construct a censored longitudinal data set consisting of times to\nthe first recurrent event in multiple prespecified follow-up windows of length\n$\\tau$. With the staggering number of potential predictors being generated from\ngenetic, -omic, and electronic health records sources, machine learning\napproaches such as the random forest are growing in popularity, as they can\nincorporate information from highly correlated predictors with non-standard\nrelationships. In this paper, we bridge this gap by developing a random forest\napproach for dynamically predicting probabilities of remaining event-free\nduring a subsequent $\\tau$-duration follow-up period from a reconstructed\ncensored longitudinal data set. We demonstrate the increased ability of our\nrandom forest algorithm for predicting the probability of remaining event-free\nover a $\\tau$-duration follow-up period when compared to the recurrent event\nmodeling framework of Xia et al. (2020) in settings where association between\npredictors and recurrent event outcomes is complex in nature. The proposed\nrandom forest algorithm is demonstrated using recurrent exacerbation data from\nthe Azithromycin for the Prevention of Exacerbations of Chronic Obstructive\nPulmonary Disease (Albert et al., 2011)."}, "http://arxiv.org/abs/2109.00160": {"title": "Novel Bayesian method for simultaneous detection of activation signatures and background connectivity for task fMRI data", "link": "http://arxiv.org/abs/2109.00160", "description": "In this paper, we introduce a new Bayesian approach for analyzing task fMRI\ndata that simultaneously detects activation signatures and background\nconnectivity. Our modeling involves a new hybrid tensor spatial-temporal basis\nstrategy that enables scalable computing yet captures nearby and distant\nintervoxel correlation and long-memory temporal correlation. The spatial basis\ninvolves a composite hybrid transform with two levels: the first accounts for\nwithin-ROI correlation, and second between-ROI distant correlation. We\ndemonstrate in simulations how our basis space regression modeling strategy\nincreases sensitivity for identifying activation signatures, partly driven by\nthe induced background connectivity that itself can be summarized to reveal\nbiological insights. This strategy leads to computationally scalable fully\nBayesian inference at the voxel or ROI level that adjusts for multiple testing.\nWe apply this model to Human Connectome Project data to reveal insights into\nbrain activation patterns and background connectivity related to working memory\ntasks."}, "http://arxiv.org/abs/2202.06117": {"title": "Metric Statistics: Exploration and Inference for Random Objects With Distance Profiles", "link": "http://arxiv.org/abs/2202.06117", "description": "This article provides an overview on the statistical modeling of complex data\nas increasingly encountered in modern data analysis. It is argued that such\ndata can often be described as elements of a metric space that satisfies\ncertain structural conditions and features a probability measure. We refer to\nthe random elements of such spaces as random objects and to the emerging field\nthat deals with their statistical analysis as metric statistics. Metric\nstatistics provides methodology, theory and visualization tools for the\nstatistical description, quantification of variation, centrality and quantiles,\nregression and inference for populations of random objects for which samples\nare available. In addition to a brief review of current concepts, we focus on\ndistance profiles as a major tool for object data in conjunction with the\npairwise Wasserstein transports of the underlying one-dimensional distance\ndistributions. These pairwise transports lead to the definition of intuitive\nand interpretable notions of transport ranks and transport quantiles as well as\ntwo-sample inference. An associated profile metric complements the original\nmetric of the object space and may reveal important features of the object data\nin data analysis We demonstrate these tools for the analysis of complex data\nthrough various examples and visualizations."}, "http://arxiv.org/abs/2205.11956": {"title": "Bandwidth Selection for Gaussian Kernel Ridge Regression via Jacobian Control", "link": "http://arxiv.org/abs/2205.11956", "description": "Most machine learning methods require tuning of hyper-parameters. For kernel\nridge regression with the Gaussian kernel, the hyper-parameter is the\nbandwidth. The bandwidth specifies the length scale of the kernel and has to be\ncarefully selected to obtain a model with good generalization. The default\nmethods for bandwidth selection, cross-validation and marginal likelihood\nmaximization, often yield good results, albeit at high computational costs.\nInspired by Jacobian regularization, we formulate an approximate expression for\nhow the derivatives of the functions inferred by kernel ridge regression with\nthe Gaussian kernel depend on the kernel bandwidth. We use this expression to\npropose a closed-form, computationally feather-light, bandwidth selection\nheuristic, based on controlling the Jacobian. In addition, the Jacobian\nexpression illuminates how the bandwidth selection is a trade-off between the\nsmoothness of the inferred function and the conditioning of the training data\nkernel matrix. We show on real and synthetic data that compared to\ncross-validation and marginal likelihood maximization, our method is on pair in\nterms of model performance, but up to six orders of magnitude faster."}, "http://arxiv.org/abs/2209.00102": {"title": "Bayesian Mixed Multidimensional Scaling for Auditory Processing", "link": "http://arxiv.org/abs/2209.00102", "description": "The human brain distinguishes speech sound categories by representing\nacoustic signals in a latent multidimensional auditory-perceptual space. This\nspace can be statistically constructed using multidimensional scaling, a\ntechnique that can compute lower-dimensional latent features representing the\nspeech signals in such a way that their pairwise distances in the latent space\nclosely resemble the corresponding distances in the observation space. The\ninter-individual and inter-population (e.g., native versus non-native\nlisteners) heterogeneity in such representations is however not well\nunderstood. These questions have often been examined using joint analyses that\nignore individual heterogeneity or using separate analyses that cannot\ncharacterize human similarities. Neither extreme, therefore, allows for\nprincipled comparisons between populations and individuals. The focus of the\ncurrent literature has also often been on inference on latent distances between\nthe categories and not on the latent features themselves, which are crucial for\nour applications, that make up these distances. Motivated by these problems, we\ndevelop a novel Bayesian mixed multidimensional scaling method, taking into\naccount the heterogeneity across populations and subjects. We design a Markov\nchain Monte Carlo algorithm for posterior computation. We then recover the\nlatent features using a post-processing scheme applied to the posterior\nsamples. We evaluate the method's empirical performances through synthetic\nexperiments. Applied to a motivating auditory neuroscience study, the method\nprovides novel insights into how biologically interpretable lower-dimensional\nlatent features reconstruct the observed distances between the stimuli and vary\nbetween individuals and their native language experiences."}, "http://arxiv.org/abs/2209.01297": {"title": "Assessing treatment effect heterogeneity in the presence of missing effect modifier data in cluster-randomized trials", "link": "http://arxiv.org/abs/2209.01297", "description": "Understanding whether and how treatment effects vary across subgroups is\ncrucial to inform clinical practice and recommendations. Accordingly, the\nassessment of heterogeneous treatment effects (HTE) based on pre-specified\npotential effect modifiers has become a common goal in modern randomized\ntrials. However, when one or more potential effect modifiers are missing,\ncomplete-case analysis may lead to bias and under-coverage. While statistical\nmethods for handling missing data have been proposed and compared for\nindividually randomized trials with missing effect modifier data, few\nguidelines exist for the cluster-randomized setting, where intracluster\ncorrelations in the effect modifiers, outcomes, or even missingness mechanisms\nmay introduce further threats to accurate assessment of HTE. In this article,\nthe performance of several missing data methods are compared through a\nsimulation study of cluster-randomized trials with continuous outcome and\nmissing binary effect modifier data, and further illustrated using real data\nfrom the Work, Family, and Health Study. Our results suggest that multilevel\nmultiple imputation (MMI) and Bayesian MMI have better performance than other\navailable methods, and that Bayesian MMI has lower bias and closer to nominal\ncoverage than standard MMI when there are model specification or compatibility\nissues."}, "http://arxiv.org/abs/2305.17083": {"title": "A Policy Gradient Method for Confounded POMDPs", "link": "http://arxiv.org/abs/2305.17083", "description": "In this paper, we propose a policy gradient method for confounded partially\nobservable Markov decision processes (POMDPs) with continuous state and\nobservation spaces in the offline setting. We first establish a novel\nidentification result to non-parametrically estimate any history-dependent\npolicy gradient under POMDPs using the offline data. The identification enables\nus to solve a sequence of conditional moment restrictions and adopt the min-max\nlearning procedure with general function approximation for estimating the\npolicy gradient. We then provide a finite-sample non-asymptotic bound for\nestimating the gradient uniformly over a pre-specified policy class in terms of\nthe sample size, length of horizon, concentratability coefficient and the\nmeasure of ill-posedness in solving the conditional moment restrictions.\nLastly, by deploying the proposed gradient estimation in the gradient ascent\nalgorithm, we show the global convergence of the proposed algorithm in finding\nthe history-dependent optimal policy under some technical conditions. To the\nbest of our knowledge, this is the first work studying the policy gradient\nmethod for POMDPs under the offline setting."}, "http://arxiv.org/abs/2307.06996": {"title": "Spey: smooth inference for reinterpretation studies", "link": "http://arxiv.org/abs/2307.06996", "description": "Statistical models serve as the cornerstone for hypothesis testing in\nempirical studies. This paper introduces a new cross-platform Python-based\npackage designed to utilise different likelihood prescriptions via a flexible\nplug-in system. This framework empowers users to propose, examine, and publish\nnew likelihood prescriptions without developing software infrastructure,\nultimately unifying and generalising different ways of constructing likelihoods\nand employing them for hypothesis testing within a unified platform. We propose\na new simplified likelihood prescription, surpassing previous approximation\naccuracies by incorporating asymmetric uncertainties. Moreover, our package\nfacilitates the integration of various likelihood combination routines, thereby\nbroadening the scope of independent studies through a meta-analysis. By\nremaining agnostic to the source of the likelihood prescription and the signal\nhypothesis generator, our platform allows for the seamless implementation of\npackages with different likelihood prescriptions, fostering compatibility and\ninteroperability."}, "http://arxiv.org/abs/2309.11472": {"title": "Optimizing Dynamic Predictions from Joint Models using Super Learning", "link": "http://arxiv.org/abs/2309.11472", "description": "Joint models for longitudinal and time-to-event data are often employed to\ncalculate dynamic individualized predictions used in numerous applications of\nprecision medicine. Two components of joint models that influence the accuracy\nof these predictions are the shape of the longitudinal trajectories and the\nfunctional form linking the longitudinal outcome history to the hazard of the\nevent. Finding a single well-specified model that produces accurate predictions\nfor all subjects and follow-up times can be challenging, especially when\nconsidering multiple longitudinal outcomes. In this work, we use the concept of\nsuper learning and avoid selecting a single model. In particular, we specify a\nweighted combination of the dynamic predictions calculated from a library of\njoint models with different specifications. The weights are selected to\noptimize a predictive accuracy metric using V-fold cross-validation. We use as\npredictive accuracy measures the expected quadratic prediction error and the\nexpected predictive cross-entropy. In a simulation study, we found that the\nsuper learning approach produces results very similar to the Oracle model,\nwhich was the model with the best performance in the test datasets. All\nproposed methodology is implemented in the freely available R package JMbayes2."}, "http://arxiv.org/abs/2312.00955": {"title": "Identification and Inference for Synthetic Controls with Confounding", "link": "http://arxiv.org/abs/2312.00955", "description": "This paper studies inference on treatment effects in panel data settings with\nunobserved confounding. We model outcome variables through a factor model with\nrandom factors and loadings. Such factors and loadings may act as unobserved\nconfounders: when the treatment is implemented depends on time-varying factors,\nand who receives the treatment depends on unit-level confounders. We study the\nidentification of treatment effects and illustrate the presence of a trade-off\nbetween time and unit-level confounding. We provide asymptotic results for\ninference for several Synthetic Control estimators and show that different\nsources of randomness should be considered for inference, depending on the\nnature of confounding. We conclude with a comparison of Synthetic Control\nestimators with alternatives for factor models."}, "http://arxiv.org/abs/2312.00963": {"title": "Spatiotemporal Transformer for Imputing Sparse Data: A Deep Learning Approach", "link": "http://arxiv.org/abs/2312.00963", "description": "Effective management of environmental resources and agricultural\nsustainability heavily depends on accurate soil moisture data. However,\ndatasets like the SMAP/Sentinel-1 soil moisture product often contain missing\nvalues across their spatiotemporal grid, which poses a significant challenge.\nThis paper introduces a novel Spatiotemporal Transformer model (ST-Transformer)\nspecifically designed to address the issue of missing values in sparse\nspatiotemporal datasets, particularly focusing on soil moisture data. The\nST-Transformer employs multiple spatiotemporal attention layers to capture the\ncomplex spatiotemporal correlations in the data and can integrate additional\nspatiotemporal covariates during the imputation process, thereby enhancing its\naccuracy. The model is trained using a self-supervised approach, enabling it to\nautonomously predict missing values from observed data points. Our model's\nefficacy is demonstrated through its application to the SMAP 1km soil moisture\ndata over a 36 x 36 km grid in Texas. It showcases superior accuracy compared\nto well-known imputation methods. Additionally, our simulation studies on other\ndatasets highlight the model's broader applicability in various spatiotemporal\nimputation tasks."}, "http://arxiv.org/abs/2312.01146": {"title": "Bayesian models are better than frequentist models in identifying differences in small datasets comprising phonetic data", "link": "http://arxiv.org/abs/2312.01146", "description": "While many studies have previously conducted direct comparisons between\nresults obtained from frequentist and Bayesian models, our research introduces\na novel perspective by examining these models in the context of a small dataset\ncomprising phonetic data. Specifically, we employed mixed-effects models and\nBayesian regression models to explore differences between monolingual and\nbilingual populations in the acoustic values of produced vowels. Our findings\nrevealed that Bayesian hypothesis testing exhibited superior accuracy in\nidentifying evidence for differences compared to the posthoc test, which tended\nto underestimate the existence of such differences. These results align with a\nsubstantial body of previous research highlighting the advantages of Bayesian\nover frequentist models, thereby emphasizing the need for methodological\nreform. In conclusion, our study supports the assertion that Bayesian models\nare more suitable for investigating differences in small datasets of phonetic\nand/or linguistic data, suggesting that researchers in these fields may find\ngreater reliability in utilizing such models for their analyses."}, "http://arxiv.org/abs/2312.01162": {"title": "Tests for Many Treatment Effects in Regression Discontinuity Panel Data Models", "link": "http://arxiv.org/abs/2312.01162", "description": "Numerous studies use regression discontinuity design (RDD) for panel data by\nassuming that the treatment effects are homogeneous across all\nindividuals/groups and pooling the data together. It is unclear how to test for\nthe significance of treatment effects when the treatments vary across\nindividuals/groups and the error terms may exhibit complicated dependence\nstructures. This paper examines the estimation and inference of multiple\ntreatment effects when the errors are not independent and identically\ndistributed, and the treatment effects vary across individuals/groups. We\nderive a simple analytical expression for approximating the variance-covariance\nstructure of the treatment effect estimators under general dependence\nconditions and propose two test statistics, one is to test for the overall\nsignificance of the treatment effect and the other for the homogeneity of the\ntreatment effects. We find that in the Gaussian approximations to the test\nstatistics, the dependence structures in the data can be safely ignored due to\nthe localized nature of the statistics. This has the important implication that\nthe simulated critical values can be easily obtained. Simulations demonstrate\nour tests have superb size control and reasonable power performance in finite\nsamples regardless of the presence of strong cross-section dependence or/and\nweak serial dependence in the data. We apply our tests to two datasets and find\nsignificant overall treatment effects in each case."}, "http://arxiv.org/abs/2312.01168": {"title": "MacroPARAFAC for handling rowwise and cellwise outliers in incomplete multi-way data", "link": "http://arxiv.org/abs/2312.01168", "description": "Multi-way data extend two-way matrices into higher-dimensional tensors, often\nexplored through dimensional reduction techniques. In this paper, we study the\nParallel Factor Analysis (PARAFAC) model for handling multi-way data,\nrepresenting it more compactly through a concise set of loading matrices and\nscores. We assume that the data may be incomplete and could contain both\nrowwise and cellwise outliers, signifying cases that deviate from the majority\nand outlying cells dispersed throughout the data array. To address these\nchallenges, we present a novel algorithm designed to robustly estimate both\nloadings and scores. Additionally, we introduce an enhanced outlier map to\ndistinguish various patterns of outlying behavior. Through simulations and the\nanalysis of fluorescence Excitation-Emission Matrix (EEM) data, we demonstrate\nthe robustness of our approach. Our results underscore the effectiveness of\ndiagnostic tools in identifying and interpreting unusual patterns within the\ndata."}, "http://arxiv.org/abs/2312.01210": {"title": "When accurate prediction models yield harmful self-fulfilling prophecies", "link": "http://arxiv.org/abs/2312.01210", "description": "Prediction models are popular in medical research and practice. By predicting\nan outcome of interest for specific patients, these models may help inform\ndifficult treatment decisions, and are often hailed as the poster children for\npersonalized, data-driven healthcare.\n\nWe show however, that using prediction models for decision making can lead to\nharmful decisions, even when the predictions exhibit good discrimination after\ndeployment. These models are harmful self-fulfilling prophecies: their\ndeployment harms a group of patients but the worse outcome of these patients\ndoes not invalidate the predictive power of the model. Our main result is a\nformal characterization of a set of such prediction models. Next we show that\nmodels that are well calibrated before} and after deployment are useless for\ndecision making as they made no change in the data distribution. These results\npoint to the need to revise standard practices for validation, deployment and\nevaluation of prediction models that are used in medical decisions."}, "http://arxiv.org/abs/2312.01238": {"title": "A deep learning pipeline for cross-sectional and longitudinal multiview data integration", "link": "http://arxiv.org/abs/2312.01238", "description": "Biomedical research now commonly integrates diverse data types or views from\nthe same individuals to better understand the pathobiology of complex diseases,\nbut the challenge lies in meaningfully integrating these diverse views.\nExisting methods often require the same type of data from all views\n(cross-sectional data only or longitudinal data only) or do not consider any\nclass outcome in the integration method, presenting limitations. To overcome\nthese limitations, we have developed a pipeline that harnesses the power of\nstatistical and deep learning methods to integrate cross-sectional and\nlongitudinal data from multiple sources. Additionally, it identifies key\nvariables contributing to the association between views and the separation\namong classes, providing deeper biological insights. This pipeline includes\nvariable selection/ranking using linear and nonlinear methods, feature\nextraction using functional principal component analysis and Euler\ncharacteristics, and joint integration and classification using dense\nfeed-forward networks and recurrent neural networks. We applied this pipeline\nto cross-sectional and longitudinal multi-omics data (metagenomics,\ntranscriptomics, and metabolomics) from an inflammatory bowel disease (IBD)\nstudy and we identified microbial pathways, metabolites, and genes that\ndiscriminate by IBD status, providing information on the etiology of IBD. We\nconducted simulations to compare the two feature extraction methods. The\nproposed pipeline is available from the following GitHub repository:\nhttps://github.com/lasandrall/DeepIDA-GRU."}, "http://arxiv.org/abs/2312.01265": {"title": "Concentration of Randomized Functions of Uniformly Bounded Variation", "link": "http://arxiv.org/abs/2312.01265", "description": "A sharp, distribution free, non-asymptotic result is proved for the\nconcentration of a random function around the mean function, when the\nrandomization is generated by a finite sequence of independent data and the\nrandom functions satisfy uniform bounded variation assumptions. The specific\nmotivation for the work comes from the need for inference on the distributional\nimpacts of social policy intervention. However, the family of randomized\nfunctions that we study is broad enough to cover wide-ranging applications. For\nexample, we provide a Kolmogorov-Smirnov like test for randomized functions\nthat are almost surely Lipschitz continuous, and novel tools for inference with\nheterogeneous treatment effects. A Dvoretzky-Kiefer-Wolfowitz like inequality\nis also provided for the sum of almost surely monotone random functions,\nextending the famous non-asymptotic work of Massart for empirical cumulative\ndistribution functions generated by i.i.d. data, to settings without\nmicro-clusters proposed by Canay, Santos, and Shaikh. We illustrate the\nrelevance of our theoretical results for applied work via empirical\napplications. Notably, the proof of our main concentration result relies on a\nnovel stochastic rendition of the fundamental result of Debreu, generally\ndubbed the \"gap lemma,\" that transforms discontinuous utility representations\nof preorders into continuous utility representations, and on an envelope\ntheorem of an infinite dimensional optimisation problem that we carefully\nconstruct."}, "http://arxiv.org/abs/2312.01266": {"title": "A unified framework for covariate adjustment under stratified randomization", "link": "http://arxiv.org/abs/2312.01266", "description": "Randomization, as a key technique in clinical trials, can eliminate sources\nof bias and produce comparable treatment groups. In randomized experiments, the\ntreatment effect is a parameter of general interest. Researchers have explored\nthe validity of using linear models to estimate the treatment effect and\nperform covariate adjustment and thus improve the estimation efficiency.\nHowever, the relationship between covariates and outcomes is not necessarily\nlinear, and is often intricate. Advances in statistical theory and related\ncomputer technology allow us to use nonparametric and machine learning methods\nto better estimate the relationship between covariates and outcomes and thus\nobtain further efficiency gains. However, theoretical studies on how to draw\nvalid inferences when using nonparametric and machine learning methods under\nstratified randomization are yet to be conducted. In this paper, we discuss a\nunified framework for covariate adjustment and corresponding statistical\ninference under stratified randomization and present a detailed proof of the\nvalidity of using local linear kernel-weighted least squares regression for\ncovariate adjustment in treatment effect estimators as a special case. In the\ncase of high-dimensional data, we additionally propose an algorithm for\nstatistical inference using machine learning methods under stratified\nrandomization, which makes use of sample splitting to alleviate the\nrequirements on the asymptotic properties of machine learning methods. Finally,\nwe compare the performances of treatment effect estimators using different\nmachine learning methods by considering various data generation scenarios, to\nguide practical research."}, "http://arxiv.org/abs/2312.01379": {"title": "Relation between PLS and OLS regression in terms of the eigenvalue distribution of the regressor covariance matrix", "link": "http://arxiv.org/abs/2312.01379", "description": "Partial least squares (PLS) is a dimensionality reduction technique\nintroduced in the field of chemometrics and successfully employed in many other\nareas. The PLS components are obtained by maximizing the covariance between\nlinear combinations of the regressors and of the target variables. In this\nwork, we focus on its application to scalar regression problems. PLS regression\nconsists in finding the least squares predictor that is a linear combination of\na subset of the PLS components. Alternatively, PLS regression can be formulated\nas a least squares problem restricted to a Krylov subspace. This equivalent\nformulation is employed to analyze the distance between\n${\\hat{\\boldsymbol\\beta}\\;}_{\\mathrm{PLS}}^{\\scriptscriptstyle {(L)}}$, the PLS\nestimator of the vector of coefficients of the linear regression model based on\n$L$ PLS components, and $\\hat{\\boldsymbol \\beta}_{\\mathrm{OLS}}$, the one\nobtained by ordinary least squares (OLS), as a function of $L$. Specifically,\n${\\hat{\\boldsymbol\\beta}\\;}_{\\mathrm{PLS}}^{\\scriptscriptstyle {(L)}}$ is the\nvector of coefficients in the aforementioned Krylov subspace that is closest to\n$\\hat{\\boldsymbol \\beta}_{\\mathrm{OLS}}$ in terms of the Mahalanobis distance\nwith respect to the covariance matrix of the OLS estimate. We provide a bound\non this distance that depends only on the distribution of the eigenvalues of\nthe regressor covariance matrix. Numerical examples on synthetic and real-world\ndata are used to illustrate how the distance between\n${\\hat{\\boldsymbol\\beta}\\;}_{\\mathrm{PLS}}^{\\scriptscriptstyle {(L)}}$ and\n$\\hat{\\boldsymbol \\beta}_{\\mathrm{OLS}}$ depends on the number of clusters in\nwhich the eigenvalues of the regressor covariance matrix are grouped."}, "http://arxiv.org/abs/2312.01411": {"title": "Bayesian inference on Cox regression models using catalytic prior distributions", "link": "http://arxiv.org/abs/2312.01411", "description": "The Cox proportional hazards model (Cox model) is a popular model for\nsurvival data analysis. When the sample size is small relative to the dimension\nof the model, the standard maximum partial likelihood inference is often\nproblematic. In this work, we propose the Cox catalytic prior distributions for\nBayesian inference on Cox models, which is an extension of a general class of\nprior distributions originally designed for stabilizing complex parametric\nmodels. The Cox catalytic prior is formulated as a weighted likelihood of the\nregression coefficients based on synthetic data and a surrogate baseline hazard\nconstant. This surrogate hazard can be either provided by the user or estimated\nfrom the data, and the synthetic data are generated from the predictive\ndistribution of a fitted simpler model. For point estimation, we derive an\napproximation of the marginal posterior mode, which can be computed\nconveniently as a regularized log partial likelihood estimator. We prove that\nour prior distribution is proper and the resulting estimator is consistent\nunder mild conditions. In simulation studies, our proposed method outperforms\nstandard maximum partial likelihood inference and is on par with existing\nshrinkage methods. We further illustrate the application of our method to a\nreal dataset."}, "http://arxiv.org/abs/2312.01457": {"title": "Marginal Density Ratio for Off-Policy Evaluation in Contextual Bandits", "link": "http://arxiv.org/abs/2312.01457", "description": "Off-Policy Evaluation (OPE) in contextual bandits is crucial for assessing\nnew policies using existing data without costly experimentation. However,\ncurrent OPE methods, such as Inverse Probability Weighting (IPW) and Doubly\nRobust (DR) estimators, suffer from high variance, particularly in cases of low\noverlap between target and behavior policies or large action and context\nspaces. In this paper, we introduce a new OPE estimator for contextual bandits,\nthe Marginal Ratio (MR) estimator, which focuses on the shift in the marginal\ndistribution of outcomes $Y$ instead of the policies themselves. Through\nrigorous theoretical analysis, we demonstrate the benefits of the MR estimator\ncompared to conventional methods like IPW and DR in terms of variance\nreduction. Additionally, we establish a connection between the MR estimator and\nthe state-of-the-art Marginalized Inverse Propensity Score (MIPS) estimator,\nproving that MR achieves lower variance among a generalized family of MIPS\nestimators. We further illustrate the utility of the MR estimator in causal\ninference settings, where it exhibits enhanced performance in estimating\nAverage Treatment Effects (ATE). Our experiments on synthetic and real-world\ndatasets corroborate our theoretical findings and highlight the practical\nadvantages of the MR estimator in OPE for contextual bandits."}, "http://arxiv.org/abs/2312.01496": {"title": "Large-Scale Correlation Screening under Dependence for Brain Functional Connectivity Network Inference", "link": "http://arxiv.org/abs/2312.01496", "description": "Data produced by resting-state functional Magnetic Resonance Imaging are\nwidely used to infer brain functional connectivity networks. Such networks\ncorrelate neural signals to connect brain regions, which consist in groups of\ndependent voxels. Previous work has focused on aggregating data across voxels\nwithin predefined regions. However, the presence of within-region correlations\nhas noticeable impacts on inter-regional correlation detection, and thus edge\nidentification. To alleviate them, we propose to leverage techniques from the\nlarge-scale correlation screening literature, and derive simple and practical\ncharacterizations of the mean number of correlation discoveries that flexibly\nincorporate intra-regional dependence structures. A connectivity network\ninference framework is then presented. First, inter-regional correlation\ndistributions are estimated. Then, correlation thresholds that can be tailored\nto one's application are constructed for each edge. Finally, the proposed\nframework is implemented on synthetic and real-world datasets. This novel\napproach for handling arbitrary intra-regional correlation is shown to limit\nfalse positives while improving true positive rates."}, "http://arxiv.org/abs/2312.01692": {"title": "Risk-Controlling Model Selection via Guided Bayesian Optimization", "link": "http://arxiv.org/abs/2312.01692", "description": "Adjustable hyperparameters of machine learning models typically impact\nvarious key trade-offs such as accuracy, fairness, robustness, or inference\ncost. Our goal in this paper is to find a configuration that adheres to\nuser-specified limits on certain risks while being useful with respect to other\nconflicting metrics. We solve this by combining Bayesian Optimization (BO) with\nrigorous risk-controlling procedures, where our core idea is to steer BO\ntowards an efficient testing strategy. Our BO method identifies a set of Pareto\noptimal configurations residing in a designated region of interest. The\nresulting candidates are statistically verified and the best-performing\nconfiguration is selected with guaranteed risk levels. We demonstrate the\neffectiveness of our approach on a range of tasks with multiple desiderata,\nincluding low error rates, equitable predictions, handling spurious\ncorrelations, managing rate and distortion in generative models, and reducing\ncomputational costs."}, "http://arxiv.org/abs/2312.01723": {"title": "Group Sequential Design Under Non-proportional Hazards", "link": "http://arxiv.org/abs/2312.01723", "description": "Non-proportional hazards (NPH) are often observed in clinical trials with\ntime-to-event endpoints. A common example is a long-term clinical trial with a\ndelayed treatment effect in immunotherapy for cancer. When designing clinical\ntrials with time-to-event endpoints, it is crucial to consider NPH scenarios to\ngain a complete understanding of design operating characteristics. In this\npaper, we focus on group sequential design for three NPH methods: the average\nhazard ratio, the weighted logrank test, and the MaxCombo combination test. For\neach of these approaches, we provide analytic forms of design characteristics\nthat facilitate sample size calculation and bound derivation for group\nsequential designs. Examples are provided to illustrate the proposed methods.\nTo facilitate statisticians in designing and comparing group sequential designs\nunder NPH, we have implemented the group sequential design methodology in the\ngsDesign2 R package at https://cran.r-project.org/web/packages/gsDesign2/."}, "http://arxiv.org/abs/2312.01735": {"title": "Weighted Q-learning for optimal dynamic treatment regimes with MNAR coavriates", "link": "http://arxiv.org/abs/2312.01735", "description": "Dynamic treatment regimes (DTRs) formalize medical decision-making as a\nsequence of rules for different stages, mapping patient-level information to\nrecommended treatments. In practice, estimating an optimal DTR using\nobservational data from electronic medical record (EMR) databases can be\ncomplicated by covariates that are missing not at random (MNAR) due to\ninformative monitoring of patients. Since complete case analysis can result in\nconsistent estimation of outcome model parameters under the assumption of\noutcome-independent missingness \\citep{Yang_Wang_Ding_2019}, Q-learning is a\nnatural approach to accommodating MNAR covariates. However, the backward\ninduction algorithm used in Q-learning can introduce complications, as MNAR\ncovariates at later stages can result in MNAR pseudo-outcomes at earlier\nstages, leading to suboptimal DTRs, even if outcome variables are fully\nobserved. To address this unique missing data problem in DTR settings, we\npropose two weighted Q-learning approaches where inverse probability weights\nfor missingness of the pseudo-outcomes are obtained through estimating\nequations with valid nonresponse instrumental variables or sensitivity\nanalysis. Asymptotic properties of the weighted Q-learning estimators are\nderived and the finite-sample performance of the proposed methods is evaluated\nand compared with alternative methods through extensive simulation studies.\nUsing EMR data from the Medical Information Mart for Intensive Care database,\nwe apply the proposed methods to investigate the optimal fluid strategy for\nsepsis patients in intensive care units."}, "http://arxiv.org/abs/2312.01815": {"title": "Hypothesis Testing in Gaussian Graphical Models: Novel Goodness-of-Fit Tests and Conditional Randomization Tests", "link": "http://arxiv.org/abs/2312.01815", "description": "We introduce novel hypothesis testing methods for Gaussian graphical models,\nwhose foundation is an innovative algorithm that generates exchangeable copies\nfrom these models. We utilize the exchangeable copies to formulate a\ngoodness-of-fit test, which is valid in both low and high-dimensional settings\nand flexible in choosing the test statistic. This test exhibits superior power\nperformance, especially in scenarios where the true precision matrix violates\nthe null hypothesis with many small entries. Furthermore, we adapt the sampling\nalgorithm for constructing a new conditional randomization test for the\nconditional independence between a response $Y$ and a vector of covariates $X$\ngiven some other variables $Z$. Thanks to the model-X framework, this test does\nnot require any modeling assumption about $Y$ and can utilize test statistics\nfrom advanced models. It also relaxes the assumptions of conditional\nrandomization tests by allowing the number of unknown parameters of the\ndistribution of $X$ to be much larger than the sample size. For both of our\ntesting procedures, we propose several test statistics and conduct\ncomprehensive simulation studies to demonstrate their superior performance in\ncontrolling the Type-I error and achieving high power. The usefulness of our\nmethods is further demonstrated through three real-world applications."}, "http://arxiv.org/abs/2312.01870": {"title": "Extreme-value modelling of migratory bird arrival dates: Insights from citizen science data", "link": "http://arxiv.org/abs/2312.01870", "description": "Citizen science mobilises many observers and gathers huge datasets but often\nwithout strict sampling protocols, which results in observation biases due to\nheterogeneity in sampling effort that can lead to biased statistical\ninferences. We develop a spatiotemporal Bayesian hierarchical model for\nbias-corrected estimation of arrival dates of the first migratory bird\nindividuals at a breeding site. Higher sampling effort could be correlated with\nearlier observed dates. We implement data fusion of two citizen-science\ndatasets with sensibly different protocols (BBS, eBird) and map posterior\ndistributions of the latent process, which contains four spatial components\nwith Gaussian process priors: species niche; sampling effort; position and\nscale parameters of annual first date of arrival. The data layer includes four\nresponse variables: counts of observed eBird locations (Poisson);\npresence-absence at observed eBird locations (Binomial); BBS occurrence counts\n(Poisson); first arrival dates (Generalized Extreme-Value). We devise a Markov\nChain Monte Carlo scheme and check by simulation that the latent process\ncomponents are identifiable. We apply our model to several migratory bird\nspecies in the northeastern US for 2001--2021. The sampling effort is shown to\nsignificantly modulate the observed first arrival date. We exploit this\nrelationship to effectively debias predictions of the true first arrival dates."}, "http://arxiv.org/abs/2312.01925": {"title": "Coefficient Shape Alignment in Multivariate Functional Regression", "link": "http://arxiv.org/abs/2312.01925", "description": "In multivariate functional data analysis, different functional covariates can\nbe homogeneous in some sense. The hidden homogeneity structure is informative\nabout the connectivity or association of different covariates. The covariates\nwith pronounced homogeneity can be analyzed jointly in the same group and this\ngives rise to a way of parsimoniously modeling multivariate functional data. In\nthis paper, we develop a multivariate functional regression technique by a new\nregularization approach termed \"coefficient shape alignment\" to tackle the\npotential homogeneity of different functional covariates. The modeling\nprocedure includes two main steps: first the unknown grouping structure is\ndetected with a new regularization approach to aggregate covariates into\ndisjoint groups; and then a grouped multivariate functional regression model is\nestablished based on the detected grouping structure. In this new grouped\nmodel, the coefficient functions of covariates in the same homogeneous group\nshare the same shape invariant to scaling. The new regularization approach\nbuilds on penalizing the discrepancy of coefficient shape. The consistency\nproperty of the detected grouping structure is thoroughly investigated, and the\nconditions that guarantee uncovering the underlying true grouping structure are\ndeveloped. The asymptotic properties of the model estimates are also developed.\nExtensive simulation studies are conducted to investigate the finite-sample\nproperties of the developed methods. The practical utility of the proposed\nmethods is illustrated in an analysis on sugar quality evaluation. This work\nprovides a novel means for analyzing the underlying homogeneity of functional\ncovariates and developing parsimonious model structures for multivariate\nfunctional data."}, "http://arxiv.org/abs/2312.01944": {"title": "New Methods for Network Count Time Series", "link": "http://arxiv.org/abs/2312.01944", "description": "The original generalized network autoregressive models are poor for modelling\ncount data as they are based on the additive and constant noise assumptions,\nwhich is usually inappropriate for count data. We introduce two new models\n(GNARI and NGNAR) for count network time series by adapting and extending\nexisting count-valued time series models. We present results on the statistical\nand asymptotic properties of our new models and their estimates obtained by\nconditional least squares and maximum likelihood. We conduct two simulation\nstudies that verify successful parameter estimation for both models and conduct\na further study that shows, for negative network parameters, that our NGNAR\nmodel outperforms existing models and our other GNARI model in terms of\npredictive performance. We model a network time series constructed from\nCOVID-positive counts for counties in New York State during 2020-22 and show\nthat our new models perform considerably better than existing methods for this\nproblem."}, "http://arxiv.org/abs/2312.01969": {"title": "FDR Control for Online Anomaly Detection", "link": "http://arxiv.org/abs/2312.01969", "description": "The goal of anomaly detection is to identify observations generated by a\nprocess that is different from a reference one. An accurate anomaly detector\nmust ensure low false positive and false negative rates. However in the online\ncontext such a constraint remains highly challenging due to the usual lack of\ncontrol of the False Discovery Rate (FDR). In particular the online framework\nmakes it impossible to use classical multiple testing approaches such as the\nBenjamini-Hochberg (BH) procedure. Our strategy overcomes this difficulty by\nexploiting a local control of the ``modified FDR'' (mFDR). An important\ningredient in this control is the cardinality of the calibration set used for\ncomputing empirical $p$-values, which turns out to be an influential parameter.\nIt results a new strategy for tuning this parameter, which yields the desired\nFDR control over the whole time series. The statistical performance of this\nstrategy is analyzed by theoretical guarantees and its practical behavior is\nassessed by simulation experiments which support our conclusions."}, "http://arxiv.org/abs/2312.02110": {"title": "Fourier Methods for Sufficient Dimension Reduction in Time Series", "link": "http://arxiv.org/abs/2312.02110", "description": "Dimensionality reduction has always been one of the most significant and\nchallenging problems in the analysis of high-dimensional data. In the context\nof time series analysis, our focus is on the estimation and inference of\nconditional mean and variance functions. By using central mean and variance\ndimension reduction subspaces that preserve sufficient information about the\nresponse, one can effectively estimate the unknown mean and variance functions\nof the time series. While the literature presents several approaches to\nestimate the time series central mean and variance subspaces (TS-CMS and\nTS-CVS), these methods tend to be computationally intensive and infeasible for\npractical applications. By employing the Fourier transform, we derive explicit\nestimators for TS-CMS and TS-CVS. These proposed estimators are demonstrated to\nbe consistent, asymptotically normal, and efficient. Simulation studies have\nbeen conducted to evaluate the performance of the proposed method. The results\nshow that our method is significantly more accurate and computationally\nefficient than existing methods. Furthermore, the method has been applied to\nthe Canadian Lynx dataset."}, "http://arxiv.org/abs/2202.03852": {"title": "Nonlinear Network Autoregression", "link": "http://arxiv.org/abs/2202.03852", "description": "We study general nonlinear models for time series networks of integer and\ncontinuous valued data. The vector of high dimensional responses, measured on\nthe nodes of a known network, is regressed non-linearly on its lagged value and\non lagged values of the neighboring nodes by employing a smooth link function.\nWe study stability conditions for such multivariate process and develop quasi\nmaximum likelihood inference when the network dimension is increasing. In\naddition, we study linearity score tests by treating separately the cases of\nidentifiable and non-identifiable parameters. In the case of identifiability,\nthe test statistic converges to a chi-square distribution. When the parameters\nare not-identifiable, we develop a supremum-type test whose p-values are\napproximated adequately by employing a feasible bound and bootstrap\nmethodology. Simulations and data examples support further our findings."}, "http://arxiv.org/abs/2202.10887": {"title": "Policy Evaluation for Temporal and/or Spatial Dependent Experiments", "link": "http://arxiv.org/abs/2202.10887", "description": "The aim of this paper is to establish a causal link between the policies\nimplemented by technology companies and the outcomes they yield within\nintricate temporal and/or spatial dependent experiments. We propose a novel\ntemporal/spatio-temporal Varying Coefficient Decision Process (VCDP) model,\ncapable of effectively capturing the evolving treatment effects in situations\ncharacterized by temporal and/or spatial dependence. Our methodology\nencompasses the decomposition of the Average Treatment Effect (ATE) into the\nDirect Effect (DE) and the Indirect Effect (IE). We subsequently devise\ncomprehensive procedures for estimating and making inferences about both DE and\nIE. Additionally, we provide a rigorous analysis of the statistical properties\nof these procedures, such as asymptotic power. To substantiate the\neffectiveness of our approach, we carry out extensive simulations and real data\nanalyses."}, "http://arxiv.org/abs/2204.07672": {"title": "Abadie's Kappa and Weighting Estimators of the Local Average Treatment Effect", "link": "http://arxiv.org/abs/2204.07672", "description": "In this paper we study the finite sample and asymptotic properties of various\nweighting estimators of the local average treatment effect (LATE), each of\nwhich can be motivated by Abadie's (2003) kappa theorem. Our framework presumes\na binary treatment and a binary instrument, which may only be valid after\nconditioning on additional covariates. We argue that two of the estimators\nunder consideration, which are weight normalized, are generally preferable.\nSeveral other estimators, which are unnormalized, do not satisfy the properties\nof scale invariance with respect to the natural logarithm and translation\ninvariance, thereby exhibiting sensitivity to the units of measurement when\nestimating the LATE in logs and the centering of the outcome variable more\ngenerally. We also demonstrate that, when noncompliance is one sided, certain\nestimators have the advantage of being based on a denominator that is strictly\ngreater than zero by construction. This is the case for only one of the two\nnormalized estimators, and we recommend this estimator for wider use. We\nillustrate our findings with a simulation study and three empirical\napplications. The importance of normalization is particularly apparent in\napplications to real data. The simulations also suggest that covariate\nbalancing estimation of instrument propensity scores may be more robust to\nmisspecification. Software for implementing these methods is available in\nStata."}, "http://arxiv.org/abs/2206.13091": {"title": "Informed censoring: the parametric combination of data and expert information", "link": "http://arxiv.org/abs/2206.13091", "description": "The statistical censoring setup is extended to the situation when random\nmeasures can be assigned to the realization of datapoints, leading to a new way\nof incorporating expert information into the usual parametric estimation\nprocedures. The asymptotic theory is provided for the resulting estimators, and\nsome special cases of practical relevance are studied in more detail. Although\nthe proposed framework mathematically generalizes censoring and coarsening at\nrandom, and borrows techniques from M-estimation theory, it provides a novel\nand transparent methodology which enjoys significant practical applicability in\nsituations where expert information is present. The potential of the approach\nis illustrated by a concrete actuarial application of tail parameter estimation\nfor a heavy-tailed MTPL dataset with limited available expert information."}, "http://arxiv.org/abs/2208.11665": {"title": "Statistical exploration of the Manifold Hypothesis", "link": "http://arxiv.org/abs/2208.11665", "description": "The Manifold Hypothesis is a widely accepted tenet of Machine Learning which\nasserts that nominally high-dimensional data are in fact concentrated near a\nlow-dimensional manifold, embedded in high-dimensional space. This phenomenon\nis observed empirically in many real world situations, has led to development\nof a wide range of statistical methods in the last few decades, and has been\nsuggested as a key factor in the success of modern AI technologies. We show\nthat rich and sometimes intricate manifold structure in data can emerge from a\ngeneric and remarkably simple statistical model -- the Latent Metric Model --\nvia elementary concepts such as latent variables, correlation and stationarity.\nThis establishes a general statistical explanation for why the Manifold\nHypothesis seems to hold in so many situations. Informed by the Latent Metric\nModel we derive procedures to discover and interpret the geometry of\nhigh-dimensional data, and explore hypotheses about the data generating\nmechanism. These procedures operate under minimal assumptions and make use of\nwell known, scaleable graph-analytic algorithms."}, "http://arxiv.org/abs/2210.10852": {"title": "BELIEF in Dependence: Leveraging Atomic Linearity in Data Bits for Rethinking Generalized Linear Models", "link": "http://arxiv.org/abs/2210.10852", "description": "Two linearly uncorrelated binary variables must be also independent because\nnon-linear dependence cannot manifest with only two possible states. This\ninherent linearity is the atom of dependency constituting any complex form of\nrelationship. Inspired by this observation, we develop a framework called\nbinary expansion linear effect (BELIEF) for understanding arbitrary\nrelationships with a binary outcome. Models from the BELIEF framework are\neasily interpretable because they describe the association of binary variables\nin the language of linear models, yielding convenient theoretical insight and\nstriking Gaussian parallels. With BELIEF, one may study generalized linear\nmodels (GLM) through transparent linear models, providing insight into how the\nchoice of link affects modeling. For example, setting a GLM interaction\ncoefficient to zero does not necessarily lead to the kind of no-interaction\nmodel assumption as understood under their linear model counterparts.\nFurthermore, for a binary response, maximum likelihood estimation for GLMs\nparadoxically fails under complete separation, when the data are most\ndiscriminative, whereas BELIEF estimation automatically reveals the perfect\npredictor in the data that is responsible for complete separation. We explore\nthese phenomena and provide related theoretical results. We also provide\npreliminary empirical demonstration of some theoretical results."}, "http://arxiv.org/abs/2302.03750": {"title": "Linking convolutional kernel size to generalization bias in face analysis CNNs", "link": "http://arxiv.org/abs/2302.03750", "description": "Training dataset biases are by far the most scrutinized factors when\nexplaining algorithmic biases of neural networks. In contrast, hyperparameters\nrelated to the neural network architecture have largely been ignored even\nthough different network parameterizations are known to induce different\nimplicit biases over learned features. For example, convolutional kernel size\nis known to affect the frequency content of features learned in CNNs. In this\nwork, we present a causal framework for linking an architectural hyperparameter\nto out-of-distribution algorithmic bias. Our framework is experimental, in that\nwe train several versions of a network with an intervention to a specific\nhyperparameter, and measure the resulting causal effect of this choice on\nperformance bias when a particular out-of-distribution image perturbation is\napplied. In our experiments, we focused on measuring the causal relationship\nbetween convolutional kernel size and face analysis classification bias across\ndifferent subpopulations (race/gender), with respect to high-frequency image\ndetails. We show that modifying kernel size, even in one layer of a CNN,\nchanges the frequency content of learned features significantly across data\nsubgroups leading to biased generalization performance even in the presence of\na balanced dataset."}, "http://arxiv.org/abs/2303.10808": {"title": "Dimension-agnostic Change Point Detection", "link": "http://arxiv.org/abs/2303.10808", "description": "Change point testing for high-dimensional data has attracted a lot of\nattention in statistics and machine learning owing to the emergence of\nhigh-dimensional data with structural breaks from many fields. In practice,\nwhen the dimension is less than the sample size but is not small, it is often\nunclear whether a method that is tailored to high-dimensional data or simply a\nclassical method that is developed and justified for low-dimensional data is\npreferred. In addition, the methods designed for low-dimensional data may not\nwork well in the high-dimensional environment and vice versa. In this paper, we\npropose a dimension-agnostic testing procedure targeting a single change point\nin the mean of a multivariate time series. Specifically, we can show that the\nlimiting null distribution for our test statistic is the same regardless of the\ndimensionality and the magnitude of cross-sectional dependence. The power\nanalysis is also conducted to understand the large sample behavior of the\nproposed test. Through Monte Carlo simulations and a real data illustration, we\ndemonstrate that the finite sample results strongly corroborate the theory and\nsuggest that the proposed test can be used as a benchmark for change-point\ndetection of time series of low, medium, and high dimensions."}, "http://arxiv.org/abs/2306.13870": {"title": "Post-Selection Inference for the Cox Model with Interval-Censored Data", "link": "http://arxiv.org/abs/2306.13870", "description": "We develop a post-selection inference method for the Cox proportional hazards\nmodel with interval-censored data, which provides asymptotically valid p-values\nand confidence intervals conditional on the model selected by lasso. The method\nis based on a pivotal quantity that is shown to converge to a uniform\ndistribution under local alternatives. The proof can be adapted to many other\nregression models, which is illustrated by the extension to generalized linear\nmodels and the Cox model with right-censored data. Our method involves\nestimation of the efficient information matrix, for which several approaches\nare proposed with proof of their consistency. Thorough simulation studies show\nthat our method has satisfactory performance in samples of modest sizes. The\nutility of the method is illustrated via an application to an Alzheimer's\ndisease study."}, "http://arxiv.org/abs/2307.15330": {"title": "Group integrative dynamic factor models with application to multiple subject brain connectivity", "link": "http://arxiv.org/abs/2307.15330", "description": "This work introduces a novel framework for dynamic factor model-based data\nintegration of multiple subjects time series data, called GRoup Integrative\nDYnamic factor (GRIDY) models. The framework identifies and characterizes\ninter-subject differences between two pre-labeled groups by considering a\ncombination of group spatial information and individual temporal dependence.\nFurthermore, it enables the identification of intra-subject differences over\ntime by employing different model configurations for each subject.\nMethodologically, the framework combines a novel principal angle-based rank\nselection algorithm and a non-iterative integrative analysis framework.\nInspired by simultaneous component analysis, this approach also reconstructs\nidentifiable latent factor series with flexible covariance structures. The\nperformance of the GRIDY models is evaluated through simulations conducted\nunder various scenarios. An application is also presented to compare\nresting-state functional MRI data collected from multiple subjects in the\nAutism Spectrum Disorder and control groups."}, "http://arxiv.org/abs/2308.11138": {"title": "NLP-based detection of systematic anomalies among the narratives of consumer complaints", "link": "http://arxiv.org/abs/2308.11138", "description": "We develop an NLP-based procedure for detecting systematic nonmeritorious\nconsumer complaints, simply called systematic anomalies, among complaint\nnarratives. While classification algorithms are used to detect pronounced\nanomalies, in the case of smaller and frequent systematic anomalies, the\nalgorithms may falter due to a variety of reasons, including technical ones as\nwell as natural limitations of human analysts. Therefore, as the next step\nafter classification, we convert the complaint narratives into quantitative\ndata, which are then analyzed using an algorithm for detecting systematic\nanomalies. We illustrate the entire procedure using complaint narratives from\nthe Consumer Complaint Database of the Consumer Financial Protection Bureau."}, "http://arxiv.org/abs/2310.00864": {"title": "Multi-Label Residual Weighted Learning for Individualized Combination Treatment Rule", "link": "http://arxiv.org/abs/2310.00864", "description": "Individualized treatment rules (ITRs) have been widely applied in many fields\nsuch as precision medicine and personalized marketing. Beyond the extensive\nstudies on ITR for binary or multiple treatments, there is considerable\ninterest in applying combination treatments. This paper introduces a novel ITR\nestimation method for combination treatments incorporating interaction effects\namong treatments. Specifically, we propose the generalized $\\psi$-loss as a\nnon-convex surrogate in the residual weighted learning framework, offering\ndesirable statistical and computational properties. Statistically, the\nminimizer of the proposed surrogate loss is Fisher-consistent with the optimal\ndecision rules, incorporating interaction effects at any intensity level - a\nsignificant improvement over existing methods. Computationally, the proposed\nmethod applies the difference-of-convex algorithm for efficient computation.\nThrough simulation studies and real-world data applications, we demonstrate the\nsuperior performance of the proposed method in recommending combination\ntreatments."}, "http://arxiv.org/abs/2312.02167": {"title": "Uncertainty Quantification in Machine Learning Based Segmentation: A Post-Hoc Approach for Left Ventricle Volume Estimation in MRI", "link": "http://arxiv.org/abs/2312.02167", "description": "Recent studies have confirmed cardiovascular diseases remain responsible for\nhighest death toll amongst non-communicable diseases. Accurate left ventricular\n(LV) volume estimation is critical for valid diagnosis and management of\nvarious cardiovascular conditions, but poses significant challenge due to\ninherent uncertainties associated with segmentation algorithms in magnetic\nresonance imaging (MRI). Recent machine learning advancements, particularly\nU-Net-like convolutional networks, have facilitated automated segmentation for\nmedical images, but struggles under certain pathologies and/or different\nscanner vendors and imaging protocols. This study proposes a novel methodology\nfor post-hoc uncertainty estimation in LV volume prediction using It\\^{o}\nstochastic differential equations (SDEs) to model path-wise behavior for the\nprediction error. The model describes the area of the left ventricle along the\nheart's long axis. The method is agnostic to the underlying segmentation\nalgorithm, facilitating its use with various existing and future segmentation\ntechnologies. The proposed approach provides a mechanism for quantifying\nuncertainty, enabling medical professionals to intervene for unreliable\npredictions. This is of utmost importance in critical applications such as\nmedical diagnosis, where prediction accuracy and reliability can directly\nimpact patient outcomes. The method is also robust to dataset changes, enabling\napplication for medical centers with limited access to labeled data. Our\nfindings highlight the proposed uncertainty estimation methodology's potential\nto enhance automated segmentation robustness and generalizability, paving the\nway for more reliable and accurate LV volume estimation in clinical settings as\nwell as opening new avenues for uncertainty quantification in biomedical image\nsegmentation, providing promising directions for future research."}, "http://arxiv.org/abs/2312.02177": {"title": "Entropy generating function for past lifetime and its properties", "link": "http://arxiv.org/abs/2312.02177", "description": "The past entropy is considered as an uncertainty measure for the past\nlifetime distribution. Generating function approach to entropy become popular\nin recent time as it generate several well-known entropy measures. In this\npaper, we introduce the past entropy-generating function. We study certain\nproperties of this measure. It is shown that the past entropy-generating\nfunction uniquely determines the distribution. Further, we present\ncharacterizations for some lifetime models using the relationship between\nreliability concepts and the past entropy-generating function."}, "http://arxiv.org/abs/2312.02404": {"title": "Addressing Unmeasured Confounders in Cox Proportional Hazards Models Using Nonparametric Bayesian Approaches", "link": "http://arxiv.org/abs/2312.02404", "description": "In observational studies, unmeasured confounders present a crucial challenge\nin accurately estimating desired causal effects. To calculate the hazard ratio\n(HR) in Cox proportional hazard models, which are relevant for time-to-event\noutcomes, methods such as Two-Stage Residual Inclusion and Limited Information\nMaximum Likelihood are typically employed. However, these methods raise\nconcerns, including the potential for biased HR estimates and issues with\nparameter identification. This manuscript introduces a novel nonparametric\nBayesian method designed to estimate an unbiased HR, addressing concerns\nrelated to parameter identification. Our proposed method consists of two\nphases: 1) detecting clusters based on the likelihood of the exposure variable,\nand 2) estimating the hazard ratio within each cluster. Although it is\nimplicitly assumed that unmeasured confounders affect outcomes through cluster\neffects, our algorithm is well-suited for such data structures."}, "http://arxiv.org/abs/2312.02482": {"title": "Treatment heterogeneity with right-censored outcomes using grf", "link": "http://arxiv.org/abs/2312.02482", "description": "This article walks through how to estimate conditional average treatment\neffects (CATEs) with right-censored time-to-event outcomes using the function\ncausal_survival_forest (Cui et al., 2023) in the R package grf (Athey et al.,\n2019, Tibshirani et al., 2023)."}, "http://arxiv.org/abs/2312.02513": {"title": "Asymptotic Theory of the Best-Choice Rerandomization using the Mahalanobis Distance", "link": "http://arxiv.org/abs/2312.02513", "description": "Rerandomization, a design that utilizes pretreatment covariates and improves\ntheir balance between different treatment groups, has received attention\nrecently in both theory and practice. There are at least two types of\nrerandomization that are used in practice: the first rerandomizes the treatment\nassignment until covariate imbalance is below a prespecified threshold; the\nsecond randomizes the treatment assignment multiple times and chooses the one\nwith the best covariate balance. In this paper we will consider the second type\nof rerandomization, namely the best-choice rerandomization, whose theory and\ninference are still lacking in the literature. In particular, we will focus on\nthe best-choice rerandomization that uses the Mahalanobis distance to measure\ncovariate imbalance, which is one of the most commonly used imbalance measure\nfor multivariate covariates and is invariant to affine transformations of\ncovariates. We will study the large-sample repeatedly sampling properties of\nthe best-choice rerandomization, allowing both the number of covariates and the\nnumber of tried complete randomizations to increase with the sample size. We\nshow that the asymptotic distribution of the difference-in-means estimator is\nmore concentrated around the true average treatment effect under\nrerandomization than under the complete randomization, and propose large-sample\naccurate confidence intervals for rerandomization that are shorter than that\nfor the completely randomized experiment. We further demonstrate that, with\nmoderate number of covariates and with the number of tried randomizations\nincreasing polynomially with the sample size, the best-choice rerandomization\ncan achieve the ideally optimal precision that one can expect even with\nperfectly balanced covariates. The developed theory and methods for\nrerandomization are also illustrated using real field experiments."}, "http://arxiv.org/abs/2312.02518": {"title": "The general linear hypothesis testing problem for multivariate functional data with applications", "link": "http://arxiv.org/abs/2312.02518", "description": "As technology continues to advance at a rapid pace, the prevalence of\nmultivariate functional data (MFD) has expanded across diverse disciplines,\nspanning biology, climatology, finance, and numerous other fields of study.\nAlthough MFD are encountered in various fields, the development of methods for\nhypotheses on mean functions, especially the general linear hypothesis testing\n(GLHT) problem for such data has been limited. In this study, we propose and\nstudy a new global test for the GLHT problem for MFD, which includes the\none-way FMANOVA, post hoc, and contrast analysis as special cases. The\nasymptotic null distribution of the test statistic is shown to be a\nchi-squared-type mixture dependent of eigenvalues of the heteroscedastic\ncovariance functions. The distribution of the chi-squared-type mixture can be\nwell approximated by a three-cumulant matched chi-squared-approximation with\nits approximation parameters estimated from the data. By incorporating an\nadjustment coefficient, the proposed test performs effectively irrespective of\nthe correlation structure in the functional data, even when dealing with a\nrelatively small sample size. Additionally, the proposed test is shown to be\nroot-n consistent, that is, it has a nontrivial power against a local\nalternative. Simulation studies and a real data example demonstrate\nfinite-sample performance and broad applicability of the proposed test."}, "http://arxiv.org/abs/2312.02591": {"title": "General Spatio-Temporal Factor Models for High-Dimensional Random Fields on a Lattice", "link": "http://arxiv.org/abs/2312.02591", "description": "Motivated by the need for analysing large spatio-temporal panel data, we\nintroduce a novel dimensionality reduction methodology for $n$-dimensional\nrandom fields observed across a number $S$ spatial locations and $T$ time\nperiods. We call it General Spatio-Temporal Factor Model (GSTFM). First, we\nprovide the probabilistic and mathematical underpinning needed for the\nrepresentation of a random field as the sum of two components: the common\ncomponent (driven by a small number $q$ of latent factors) and the\nidiosyncratic component (mildly cross-correlated). We show that the two\ncomponents are identified as $n\\to\\infty$. Second, we propose an estimator of\nthe common component and derive its statistical guarantees (consistency and\nrate of convergence) as $\\min(n, S, T )\\to\\infty$. Third, we propose an\ninformation criterion to determine the number of factors. Estimation makes use\nof Fourier analysis in the frequency domain and thus we fully exploit the\ninformation on the spatio-temporal covariance structure of the whole panel.\nSynthetic data examples illustrate the applicability of GSTFM and its\nadvantages over the extant generalized dynamic factor model that ignores the\nspatial correlations."}, "http://arxiv.org/abs/2312.02717": {"title": "A Graphical Approach to Treatment Effect Estimation with Observational Network Data", "link": "http://arxiv.org/abs/2312.02717", "description": "We propose an easy-to-use adjustment estimator for the effect of a treatment\nbased on observational data from a single (social) network of units. The\napproach allows for interactions among units within the network, called\ninterference, and for observed confounding. We define a simplified causal graph\nthat does not differentiate between units, called generic graph. Using valid\nadjustment sets determined in the generic graph, we can identify the treatment\neffect and build a corresponding estimator. We establish the estimator's\nconsistency and its convergence to a Gaussian limiting distribution at the\nparametric rate under certain regularity conditions that restrict the growth of\ndependencies among units. We empirically verify the theoretical properties of\nour estimator through a simulation study and apply it to estimate the effect of\na strict facial-mask policy on the spread of COVID-19 in Switzerland."}, "http://arxiv.org/abs/2312.02807": {"title": "Online Change Detection in SAR Time-Series with Kronecker Product Structured Scaled Gaussian Models", "link": "http://arxiv.org/abs/2312.02807", "description": "We develop the information geometry of scaled Gaussian distributions for\nwhich the covariance matrix exhibits a Kronecker product structure. This model\nand its geometry are then used to propose an online change detection (CD)\nalgorithm for multivariate image times series (MITS). The proposed approach\nrelies mainly on the online estimation of the structured covariance matrix\nunder the null hypothesis, which is performed through a recursive (natural)\nRiemannian gradient descent. This approach exhibits a practical interest\ncompared to the corresponding offline version, as its computational cost\nremains constant for each new image added in the time series. Simulations show\nthat the proposed recursive estimators reach the Intrinsic Cram\\'er-Rao bound.\nThe interest of the proposed online CD approach is demonstrated on both\nsimulated and real data."}, "http://arxiv.org/abs/2312.02850": {"title": "A Kernel-Based Neural Network Test for High-dimensional Sequencing Data Analysis", "link": "http://arxiv.org/abs/2312.02850", "description": "The recent development of artificial intelligence (AI) technology, especially\nthe advance of deep neural network (DNN) technology, has revolutionized many\nfields. While DNN plays a central role in modern AI technology, it has been\nrarely used in sequencing data analysis due to challenges brought by\nhigh-dimensional sequencing data (e.g., overfitting). Moreover, due to the\ncomplexity of neural networks and their unknown limiting distributions,\nbuilding association tests on neural networks for genetic association analysis\nremains a great challenge. To address these challenges and fill the important\ngap of using AI in high-dimensional sequencing data analysis, we introduce a\nnew kernel-based neural network (KNN) test for complex association analysis of\nsequencing data. The test is built on our previously developed KNN framework,\nwhich uses random effects to model the overall effects of high-dimensional\ngenetic data and adopts kernel-based neural network structures to model complex\ngenotype-phenotype relationships. Based on KNN, a Wald-type test is then\nintroduced to evaluate the joint association of high-dimensional genetic data\nwith a disease phenotype of interest, considering non-linear and non-additive\neffects (e.g., interaction effects). Through simulations, we demonstrated that\nour proposed method attained higher power compared to the sequence kernel\nassociation test (SKAT), especially in the presence of non-linear and\ninteraction effects. Finally, we apply the methods to the whole genome\nsequencing (WGS) dataset from the Alzheimer's Disease Neuroimaging Initiative\n(ADNI) study, investigating new genes associated with the hippocampal volume\nchange over time."}, "http://arxiv.org/abs/2312.02858": {"title": "Towards Causal Representations of Climate Model Data", "link": "http://arxiv.org/abs/2312.02858", "description": "Climate models, such as Earth system models (ESMs), are crucial for\nsimulating future climate change based on projected Shared Socioeconomic\nPathways (SSP) greenhouse gas emissions scenarios. While ESMs are sophisticated\nand invaluable, machine learning-based emulators trained on existing simulation\ndata can project additional climate scenarios much faster and are\ncomputationally efficient. However, they often lack generalizability and\ninterpretability. This work delves into the potential of causal representation\nlearning, specifically the \\emph{Causal Discovery with Single-parent Decoding}\n(CDSD) method, which could render climate model emulation efficient\n\\textit{and} interpretable. We evaluate CDSD on multiple climate datasets,\nfocusing on emissions, temperature, and precipitation. Our findings shed light\non the challenges, limitations, and promise of using CDSD as a stepping stone\ntowards more interpretable and robust climate model emulation."}, "http://arxiv.org/abs/2312.02860": {"title": "Spectral Deconfounding for High-Dimensional Sparse Additive Models", "link": "http://arxiv.org/abs/2312.02860", "description": "Many high-dimensional data sets suffer from hidden confounding. When hidden\nconfounders affect both the predictors and the response in a high-dimensional\nregression problem, standard methods lead to biased estimates. This paper\nsubstantially extends previous work on spectral deconfounding for\nhigh-dimensional linear models to the nonlinear setting and with this,\nestablishes a proof of concept that spectral deconfounding is valid for general\nnonlinear models. Concretely, we propose an algorithm to estimate\nhigh-dimensional additive models in the presence of hidden dense confounding:\narguably, this is a simple yet practically useful nonlinear scope. We prove\nconsistency and convergence rates for our method and evaluate it on synthetic\ndata and a genetic data set."}, "http://arxiv.org/abs/2312.02867": {"title": "Semi-Supervised Health Index Monitoring with Feature Generation and Fusion", "link": "http://arxiv.org/abs/2312.02867", "description": "The Health Index (HI) is crucial for evaluating system health, aiding tasks\nlike anomaly detection and predicting remaining useful life for systems\ndemanding high safety and reliability. Tight monitoring is crucial for\nachieving high precision at a lower cost, with applications such as spray\ncoating. Obtaining HI labels in real-world applications is often\ncost-prohibitive, requiring continuous, precise health measurements. Therefore,\nit is more convenient to leverage run-to failure datasets that may provide\npotential indications of machine wear condition, making it necessary to apply\nsemi-supervised tools for HI construction. In this study, we adapt the Deep\nSemi-supervised Anomaly Detection (DeepSAD) method for HI construction. We use\nthe DeepSAD embedding as a condition indicators to address interpretability\nchallenges and sensitivity to system-specific factors. Then, we introduce a\ndiversity loss to enrich condition indicators. We employ an alternating\nprojection algorithm with isotonic constraints to transform the DeepSAD\nembedding into a normalized HI with an increasing trend. Validation on the PHME\n2010 milling dataset, a recognized benchmark with ground truth HIs demonstrates\nmeaningful HIs estimations. Our methodology is then applied to monitor wear\nstates of thermal spray coatings using high-frequency voltage. Our\ncontributions create opportunities for more accessible and reliable HI\nestimation, particularly in cases where obtaining ground truth HI labels is\nunfeasible."}, "http://arxiv.org/abs/2312.02870": {"title": "Replica analysis of overfitting in regression models for time to event data: the impact of censoring", "link": "http://arxiv.org/abs/2312.02870", "description": "We use statistical mechanics techniques, viz. the replica method, to model\nthe effect of censoring on overfitting in Cox's proportional hazards model, the\ndominant regression method for time-to-event data. In the overfitting regime,\nMaximum Likelihood parameter estimators are known to be biased already for\nsmall values of the ratio of the number of covariates over the number of\nsamples. The inclusion of censoring was avoided in previous overfitting\nanalyses for mathematical convenience, but is vital to make any theory\napplicable to real-world medical data, where censoring is ubiquitous. Upon\nconstructing efficient algorithms for solving the new (and more complex) RS\nequations and comparing the solutions with numerical simulation data, we find\nexcellent agreement, even for large censoring rates. We then address the\npractical problem of using the theory to correct the biased ML estimators\n{without} knowledge of the data-generating distribution. This is achieved via a\nnovel numerical algorithm that self-consistently approximates all relevant\nparameters of the data generating distribution while simultaneously solving the\nRS equations. We investigate numerically the statistics of the corrected\nestimators, and show that the proposed new algorithm indeed succeeds in\nremoving the bias of the ML estimators, for both the association parameters and\nfor the cumulative hazard."}, "http://arxiv.org/abs/2312.02905": {"title": "E-values, Multiple Testing and Beyond", "link": "http://arxiv.org/abs/2312.02905", "description": "We discover a connection between the Benjamini-Hochberg (BH) procedure and\nthe recently proposed e-BH procedure [Wang and Ramdas, 2022] with a suitably\ndefined set of e-values. This insight extends to a generalized version of the\nBH procedure and the model-free multiple testing procedure in Barber and\nCand\\`es [2015] (BC) with a general form of rejection rules. The connection\nprovides an effective way of developing new multiple testing procedures by\naggregating or assembling e-values resulting from the BH and BC procedures and\ntheir use in different subsets of the data. In particular, we propose new\nmultiple testing methodologies in three applications, including a hybrid\napproach that integrates the BH and BC procedures, a multiple testing procedure\naimed at ensuring a new notion of fairness by controlling both the group-wise\nand overall false discovery rates (FDR), and a structure adaptive multiple\ntesting procedure that can incorporate external covariate information to boost\ndetection power. One notable feature of the proposed methods is that we use a\ndata-dependent approach for assigning weights to e-values, significantly\nenhancing the efficiency of the resulting e-BH procedure. The construction of\nthe weights is non-trivial and is motivated by the leave-one-out analysis for\nthe BH and BC procedures. In theory, we prove that the proposed e-BH procedures\nwith data-dependent weights in the three applications ensure finite sample FDR\ncontrol. Furthermore, we demonstrate the efficiency of the proposed methods\nthrough numerical studies in the three applications."}, "http://arxiv.org/abs/2112.06000": {"title": "Multiply robust estimators in longitudinal studies with missing data under control-based imputation", "link": "http://arxiv.org/abs/2112.06000", "description": "Longitudinal studies are often subject to missing data. The ICH E9(R1)\naddendum addresses the importance of defining a treatment effect estimand with\nthe consideration of intercurrent events. Jump-to-reference (J2R) is one\nclassically envisioned control-based scenario for the treatment effect\nevaluation using the hypothetical strategy, where the participants in the\ntreatment group after intercurrent events are assumed to have the same disease\nprogress as those with identical covariates in the control group. We establish\nnew estimators to assess the average treatment effect based on a proposed\npotential outcomes framework under J2R. Various identification formulas are\nconstructed under the assumptions addressed by J2R, motivating estimators that\nrely on different parts of the observed data distribution. Moreover, we obtain\na novel estimator inspired by the efficient influence function, with multiple\nrobustness in the sense that it achieves $n^{1/2}$-consistency if any pairs of\nmultiple nuisance functions are correctly specified, or if the nuisance\nfunctions converge at a rate not slower than $n^{-1/4}$ when using flexible\nmodeling approaches. The finite-sample performance of the proposed estimators\nis validated in simulation studies and an antidepressant clinical trial."}, "http://arxiv.org/abs/2207.03035": {"title": "On the instrumental variable estimation with many weak and invalid instruments", "link": "http://arxiv.org/abs/2207.03035", "description": "We discuss the fundamental issue of identification in linear instrumental\nvariable (IV) models with unknown IV validity. With the assumption of the\n\"sparsest rule\", which is equivalent to the plurality rule but becomes\noperational in computation algorithms, we investigate and prove the advantages\nof non-convex penalized approaches over other IV estimators based on two-step\nselections, in terms of selection consistency and accommodation for\nindividually weak IVs. Furthermore, we propose a surrogate sparsest penalty\nthat aligns with the identification condition and provides oracle sparse\nstructure simultaneously. Desirable theoretical properties are derived for the\nproposed estimator with weaker IV strength conditions compared to the previous\nliterature. Finite sample properties are demonstrated using simulations and the\nselection and estimation method is applied to an empirical study concerning the\neffect of BMI on diastolic blood pressure."}, "http://arxiv.org/abs/2207.13480": {"title": "On Selecting and Conditioning in Multiple Testing and Selective Inference", "link": "http://arxiv.org/abs/2207.13480", "description": "We investigate a class of methods for selective inference that condition on a\nselection event. Such methods follow a two-stage process. First, a data-driven\n(sub)collection of hypotheses is chosen from some large universe of hypotheses.\nSubsequently, inference takes place within this data-driven collection,\nconditioned on the information that was used for the selection. Examples of\nsuch methods include basic data splitting, as well as modern data carving\nmethods and post-selection inference methods for lasso coefficients based on\nthe polyhedral lemma. In this paper, we adopt a holistic view on such methods,\nconsidering the selection, conditioning, and final error control steps together\nas a single method. From this perspective, we demonstrate that multiple testing\nmethods defined directly on the full universe of hypotheses are always at least\nas powerful as selective inference methods based on selection and conditioning.\nThis result holds true even when the universe is potentially infinite and only\nimplicitly defined, such as in the case of data splitting. We provide a\ncomprehensive theoretical framework, along with insights, and delve into\nseveral case studies to illustrate instances where a shift to a non-selective\nor unconditional perspective can yield a power gain."}, "http://arxiv.org/abs/2306.11302": {"title": "A Two-Stage Bayesian Small Area Estimation Approach for Proportions", "link": "http://arxiv.org/abs/2306.11302", "description": "With the rise in popularity of digital Atlases to communicate spatial\nvariation, there is an increasing need for robust small-area estimates.\nHowever, current small-area estimation methods suffer from various modeling\nproblems when data are very sparse or when estimates are required for areas\nwith very small populations. These issues are particularly heightened when\nmodeling proportions. Additionally, recent work has shown significant benefits\nin modeling at both the individual and area levels. We propose a two-stage\nBayesian hierarchical small area estimation approach for proportions that can:\naccount for survey design; reduce direct estimate instability; and generate\nprevalence estimates for small areas with no survey data. Using a simulation\nstudy we show that, compared with existing Bayesian small area estimation\nmethods, our approach can provide optimal predictive performance (Bayesian mean\nrelative root mean squared error, mean absolute relative bias and coverage) of\nproportions under a variety of data conditions, including very sparse and\nunstable data. To assess the model in practice, we compare modeled estimates of\ncurrent smoking prevalence for 1,630 small areas in Australia using the\n2017-2018 National Health Survey data combined with 2016 census data."}, "http://arxiv.org/abs/2308.04368": {"title": "Multiple Testing of Local Extrema for Detection of Structural Breaks in Piecewise Linear Models", "link": "http://arxiv.org/abs/2308.04368", "description": "In this paper, we propose a new generic method for detecting the number and\nlocations of structural breaks or change points in piecewise linear models\nunder stationary Gaussian noise. Our method transforms the change point\ndetection problem into identifying local extrema (local maxima and local\nminima) through kernel smoothing and differentiation of the data sequence. By\ncomputing p-values for all local extrema based on peak height distributions of\nsmooth Gaussian processes, we utilize the Benjamini-Hochberg procedure to\nidentify significant local extrema as the detected change points. Our method\ncan distinguish between two types of change points: continuous breaks (Type I)\nand jumps (Type II). We study three scenarios of piecewise linear signals,\nnamely pure Type I, pure Type II and a mixture of Type I and Type II change\npoints. The results demonstrate that our proposed method ensures asymptotic\ncontrol of the False Discover Rate (FDR) and power consistency, as sequence\nlength, slope changes, and jump size increase. Furthermore, compared to\ntraditional change point detection methods based on recursive segmentation, our\napproach only requires a single test for all candidate local extrema, thereby\nachieving the smallest computational complexity proportionate to the data\nsequence length. Additionally, numerical studies illustrate that our method\nmaintains FDR control and power consistency, even in non-asymptotic cases when\nthe size of slope changes or jumps is not large. We have implemented our method\nin the R package \"dSTEM\" (available from\nhttps://cran.r-project.org/web/packages/dSTEM)."}, "http://arxiv.org/abs/2308.05484": {"title": "Filtering Dynamical Systems Using Observations of Statistics", "link": "http://arxiv.org/abs/2308.05484", "description": "We consider the problem of filtering dynamical systems, possibly stochastic,\nusing observations of statistics. Thus the computational task is to estimate a\ntime-evolving density $\\rho(v, t)$ given noisy observations of the true density\n$\\rho^\\dagger$; this contrasts with the standard filtering problem based on\nobservations of the state $v$. The task is naturally formulated as an\ninfinite-dimensional filtering problem in the space of densities $\\rho$.\nHowever, for the purposes of tractability, we seek algorithms in state space;\nspecifically we introduce a mean field state space model and, using interacting\nparticle system approximations to this model, we propose an ensemble method. We\nrefer to the resulting methodology as the ensemble Fokker-Planck filter\n(EnFPF).\n\nUnder certain restrictive assumptions we show that the EnFPF approximates the\nKalman-Bucy filter for the Fokker-Planck equation, which is the exact solution\nof the infinite-dimensional filtering problem; our numerical experiments show\nthat the methodology is useful beyond this restrictive setting. Specifically\nthe experiments show that the EnFPF is able to correct ensemble statistics, to\naccelerate convergence to the invariant density for autonomous systems, and to\naccelerate convergence to time-dependent invariant densities for non-autonomous\nsystems. We discuss possible applications of the EnFPF to climate ensembles and\nto turbulence modelling."}, "http://arxiv.org/abs/2311.05649": {"title": "Bayesian Image-on-Image Regression via Deep Kernel Learning based Gaussian Processes", "link": "http://arxiv.org/abs/2311.05649", "description": "In neuroimaging studies, it becomes increasingly important to study\nassociations between different imaging modalities using image-on-image\nregression (IIR), which faces challenges in interpretation, statistical\ninference, and prediction. Our motivating problem is how to predict task-evoked\nfMRI activity using resting-state fMRI data in the Human Connectome Project\n(HCP). The main difficulty lies in effectively combining different types of\nimaging predictors with varying resolutions and spatial domains in IIR. To\naddress these issues, we develop Bayesian Image-on-image Regression via Deep\nKernel Learning Gaussian Processes (BIRD-GP) and develop efficient posterior\ncomputation methods through Stein variational gradient descent. We demonstrate\nthe advantages of BIRD-GP over state-of-the-art IIR methods using simulations.\nFor HCP data analysis using BIRD-GP, we combine the voxel-wise fALFF maps and\nregion-wise connectivity matrices to predict fMRI contrast maps for language\nand social recognition tasks. We show that fALFF is less predictive than the\nconnectivity matrix for both tasks, but combining both yields improved results.\nAngular Gyrus Right emerges as the most predictable region for the language\ntask (75.9% predictable voxels), while Superior Parietal Gyrus Right tops for\nthe social recognition task (48.9% predictable voxels). Additionally, we\nidentify features from the resting-state fMRI data that are important for task\nfMRI prediction."}, "http://arxiv.org/abs/2312.03139": {"title": "A Bayesian Skew-heavy-tailed modelling for loss reserving", "link": "http://arxiv.org/abs/2312.03139", "description": "This paper focuses on modelling loss reserving to pay outstanding claims. As\nthe amount liable on any given claim is not known until settlement, we propose\na flexible model via heavy-tailed and skewed distributions to deal with\noutstanding liabilities. The inference relies on Markov chain Monte Carlo via\nGibbs sampler with adaptive Metropolis algorithm steps allowing for fast\ncomputations and providing efficient algorithms. An illustrative example\nemulates a typical dataset based on a runoff triangle and investigates the\nproperties of the proposed models. Also, a case study is considered and shows\nthat the proposed model outperforms the usual loss reserving models well\nestablished in the literature in the presence of skewness and heavy tails."}, "http://arxiv.org/abs/2312.03192": {"title": "Modeling Structure and Country-specific Heterogeneity in Misclassification Matrices of Verbal Autopsy-based Cause of Death Classifiers", "link": "http://arxiv.org/abs/2312.03192", "description": "Verbal autopsy (VA) algorithms are routinely used to determine\nindividual-level causes of death (COD) in many low-and-middle-income countries,\nwhich are then aggregated to derive population-level cause-specific mortality\nfractions (CSMF), essential to informing public health policies. However, VA\nalgorithms frequently misclassify COD and introduce bias in CSMF estimates. A\nrecent method, VA-calibration, can correct for this bias using a VA\nmisclassification matrix estimated from paired data on COD from both VA and\nminimally invasive tissue sampling (MITS) from the Child Health and Mortality\nPrevention Surveillance (CHAMPS) Network. Due to the limited sample size,\nCHAMPS data are pooled across all countries, implicitly assuming that the\nmisclassification rates are homogeneous.\n\nIn this research, we show that the VA misclassification matrices are\nsubstantially heterogeneous across countries, thereby biasing the\nVA-calibration. We develop a coherent framework for modeling country-specific\nmisclassification matrices in data-scarce settings. We first introduce a novel\nbase model based on two latent mechanisms: intrinsic accuracy and systematic\npreference to parsimoniously characterize misclassifications. We prove that\nthey are identifiable from the data and manifest as a form of invariance in\ncertain misclassification odds, a pattern evident in the CHAMPS data. Then we\nexpand from this base model, adding higher complexity and country-specific\nheterogeneity via interpretable effect sizes. Shrinkage priors balance the\nbias-variance tradeoff by adaptively favoring simpler models. We publish\nuncertainty-quantified estimates of VA misclassification rates for 6 countries.\nThis effort broadens VA-calibration's future applicability and strengthens\nongoing efforts of using VA for mortality surveillance."}, "http://arxiv.org/abs/2312.03254": {"title": "Efficiency of Terrestrial Laser Scanning in Survey Works: Assessment, Modelling, and Monitoring", "link": "http://arxiv.org/abs/2312.03254", "description": "Nowadays, static, mobile, terrestrial, and airborne laser scanning\ntechnologies have become familiar data sources for engineering work, especially\nin the area of land surveying. The diversity of Light Detection and Ranging\n(LiDAR) data applications thanks to the accuracy and the high point density in\naddition to the 3D data processing high speed allow laser scanning to occupy an\nadvanced position among other spatial data acquisition technologies. Moreover,\nthe unmanned aerial vehicle drives the airborne scanning progress by solving\nthe flying complexity issues. However, before the employment of the laser\nscanning technique, it is unavoidable to assess the accuracy of the scanner\nbeing used under different circumstances. The key to success is determined by\nthe correct selection of suitable scanning tools for the project. In this\npaper, the terrestrial LiDAR data is tested and used for several laser scanning\nprojects having diverse goals and typology, e.g., road deformation monitoring,\nbuilding facade modelling, road modelling, and stockpile modelling and volume\nmeasuring. The accuracy of direct measurement on the LiDAR point cloud is\nestimated as 4mm which may open the door widely for LiDAR data to play an\nessential role in survey work applications."}, "http://arxiv.org/abs/2312.03257": {"title": "Bayesian Functional Analysis for Untargeted Metabolomics Data with Matching Uncertainty and Small Sample Sizes", "link": "http://arxiv.org/abs/2312.03257", "description": "Untargeted metabolomics based on liquid chromatography-mass spectrometry\ntechnology is quickly gaining widespread application given its ability to\ndepict the global metabolic pattern in biological samples. However, the data is\nnoisy and plagued by the lack of clear identity of data features measured from\nsamples. Multiple potential matchings exist between data features and known\nmetabolites, while the truth can only be one-to-one matches. Some existing\nmethods attempt to reduce the matching uncertainty, but are far from being able\nto remove the uncertainty for most features. The existence of the uncertainty\ncauses major difficulty in downstream functional analysis. To address these\nissues, we develop a novel approach for Bayesian Analysis of Untargeted\nMetabolomics data (BAUM) to integrate previously separate tasks into a single\nframework, including matching uncertainty inference, metabolite selection, and\nfunctional analysis. By incorporating the knowledge graph between variables and\nusing relatively simple assumptions, BAUM can analyze datasets with small\nsample sizes. By allowing different confidence levels of feature-metabolite\nmatching, the method is applicable to datasets in which feature identities are\npartially known. Simulation studies demonstrate that, compared with other\nexisting methods, BAUM achieves better accuracy in selecting important\nmetabolites that tend to be functionally consistent and assigning confidence\nscores to feature-metabolite matches. We analyze a COVID-19 metabolomics\ndataset and a mouse brain metabolomics dataset using BAUM. Even with a very\nsmall sample size of 16 mice per group, BAUM is robust and stable. It finds\npathways that conform to existing knowledge, as well as novel pathways that are\nbiologically plausible."}, "http://arxiv.org/abs/2312.03268": {"title": "Design-based inference for generalized network experiments with stochastic interventions", "link": "http://arxiv.org/abs/2312.03268", "description": "A growing number of scholars and data scientists are conducting randomized\nexperiments to analyze causal relationships in network settings where units\ninfluence one another. A dominant methodology for analyzing these network\nexperiments has been design-based, leveraging randomization of treatment\nassignment as the basis for inference. In this paper, we generalize this\ndesign-based approach so that it can be applied to more complex experiments\nwith a variety of causal estimands with different target populations. An\nimportant special case of such generalized network experiments is a bipartite\nnetwork experiment, in which the treatment assignment is randomized among one\nset of units and the outcome is measured for a separate set of units. We\npropose a broad class of causal estimands based on stochastic intervention for\ngeneralized network experiments. Using a design-based approach, we show how to\nestimate the proposed causal quantities without bias, and develop conservative\nvariance estimators. We apply our methodology to a randomized experiment in\neducation where a group of selected students in middle schools are eligible for\nthe anti-conflict promotion program, and the program participation is\nrandomized within this group. In particular, our analysis estimates the causal\neffects of treating each student or his/her close friends, for different target\npopulations in the network. We find that while the treatment improves the\noverall awareness against conflict among students, it does not significantly\nreduce the total number of conflicts."}, "http://arxiv.org/abs/2312.03274": {"title": "Empirical Bayes Covariance Decomposition, and a solution to the Multiple Tuning Problem in Sparse PCA", "link": "http://arxiv.org/abs/2312.03274", "description": "Sparse Principal Components Analysis (PCA) has been proposed as a way to\nimprove both interpretability and reliability of PCA. However, use of sparse\nPCA in practice is hindered by the difficulty of tuning the multiple\nhyperparameters that control the sparsity of different PCs (the \"multiple\ntuning problem\", MTP). Here we present a solution to the MTP using Empirical\nBayes methods. We first introduce a general formulation for penalized PCA of a\ndata matrix $\\mathbf{X}$, which includes some existing sparse PCA methods as\nspecial cases. We show that this formulation also leads to a penalized\ndecomposition of the covariance (or Gram) matrix, $\\mathbf{X}^T\\mathbf{X}$. We\nintroduce empirical Bayes versions of these penalized problems, in which the\npenalties are determined by prior distributions that are estimated from the\ndata by maximum likelihood rather than cross-validation. The resulting\n\"Empirical Bayes Covariance Decomposition\" provides a principled and efficient\nsolution to the MTP in sparse PCA, and one that can be immediately extended to\nincorporate other structural assumptions (e.g. non-negative PCA). We illustrate\nthe effectiveness of this approach on both simulated and real data examples."}, "http://arxiv.org/abs/2312.03538": {"title": "Bayesian variable selection in sample selection models using spike-and-slab priors", "link": "http://arxiv.org/abs/2312.03538", "description": "Sample selection models represent a common methodology for correcting bias\ninduced by data missing not at random. It is well known that these models are\nnot empirically identifiable without exclusion restrictions. In other words,\nsome variables predictive of missingness do not affect the outcome model of\ninterest. The drive to establish this requirement often leads to the inclusion\nof irrelevant variables in the model. A recent proposal uses adaptive LASSO to\ncircumvent this problem, but its performance depends on the so-called\ncovariance assumption, which can be violated in small to moderate samples.\nAdditionally, there are no tools yet for post-selection inference for this\nmodel. To address these challenges, we propose two families of spike-and-slab\npriors to conduct Bayesian variable selection in sample selection models. These\nprior structures allow for constructing a Gibbs sampler with tractable\nconditionals, which is scalable to the dimensions of practical interest. We\nillustrate the performance of the proposed methodology through a simulation\nstudy and present a comparison against adaptive LASSO and stepwise selection.\nWe also provide two applications using publicly available real data. An\nimplementation and code to reproduce the results in this paper can be found at\nhttps://github.com/adam-iqbal/selection-spike-slab"}, "http://arxiv.org/abs/2312.03561": {"title": "Blueprinting the Future: Automatic Item Categorization using Hierarchical Zero-Shot and Few-Shot Classifiers", "link": "http://arxiv.org/abs/2312.03561", "description": "In testing industry, precise item categorization is pivotal to align exam\nquestions with the designated content domains outlined in the assessment\nblueprint. Traditional methods either entail manual classification, which is\nlaborious and error-prone, or utilize machine learning requiring extensive\ntraining data, often leading to model underfit or overfit issues. This study\nunveils a novel approach employing the zero-shot and few-shot Generative\nPretrained Transformer (GPT) classifier for hierarchical item categorization,\nminimizing the necessity for training data, and instead, leveraging human-like\nlanguage descriptions to define categories. Through a structured python\ndictionary, the hierarchical nature of examination blueprints is navigated\nseamlessly, allowing for a tiered classification of items across multiple\nlevels. An initial simulation with artificial data demonstrates the efficacy of\nthis method, achieving an average accuracy of 92.91% measured by the F1 score.\nThis method was further applied to real exam items from the 2022 In-Training\nExamination (ITE) conducted by the American Board of Family Medicine (ABFM),\nreclassifying 200 items according to a newly formulated blueprint swiftly in 15\nminutes, a task that traditionally could span several days among editors and\nphysicians. This innovative approach not only drastically cuts down\nclassification time but also ensures a consistent, principle-driven\ncategorization, minimizing human biases and discrepancies. The ability to\nrefine classifications by adjusting definitions adds to its robustness and\nsustainability."}, "http://arxiv.org/abs/2312.03643": {"title": "Propagating moments in probabilistic graphical models for decision support systems", "link": "http://arxiv.org/abs/2312.03643", "description": "Probabilistic graphical models are widely used to model complex systems with\nuncertainty. Traditionally, Gaussian directed graphical models are applied for\nanalysis of large networks with continuous variables since they can provide\nconditional and marginal distributions in closed form simplifying the\ninferential task. The Gaussianity and linearity assumptions are often adequate,\nyet can lead to poor performance when dealing with some practical applications.\nIn this paper, we model each variable in graph G as a polynomial regression of\nits parents to capture complex relationships between individual variables and\nwith utility function of polynomial form. Since the marginal posterior\ndistributions of individual variables can become analytically intractable, we\ndevelop a message-passing algorithm to propagate information throughout the\nnetwork solely using moments which enables the expected utility scores to be\ncalculated exactly. We illustrate how the proposed methodology works in a\ndecision problem in energy systems planning."}, "http://arxiv.org/abs/2104.12909": {"title": "Algorithm as Experiment: Machine Learning, Market Design, and Policy Eligibility Rules", "link": "http://arxiv.org/abs/2104.12909", "description": "Algorithms make a growing portion of policy and business decisions. We\ndevelop a treatment-effect estimator using algorithmic decisions as instruments\nfor a class of stochastic and deterministic algorithms. Our estimator is\nconsistent and asymptotically normal for well-defined causal effects. A special\ncase of our setup is multidimensional regression discontinuity designs with\ncomplex boundaries. We apply our estimator to evaluate the Coronavirus Aid,\nRelief, and Economic Security Act, which allocated many billions of dollars\nworth of relief funding to hospitals via an algorithmic rule. The funding is\nshown to have little effect on COVID-19-related hospital activities. Naive\nestimates exhibit selection bias."}, "http://arxiv.org/abs/2112.05623": {"title": "Smooth test for equality of copulas", "link": "http://arxiv.org/abs/2112.05623", "description": "A smooth test to simultaneously compare $K$ copulas, where $K \\geq 2$ is\nproposed. The $K$ observed populations can be paired, and the test statistic is\nconstructed based on the differences between moment sequences, called copula\ncoefficients. These coefficients characterize the copulas, even when the copula\ndensities may not exist. The procedure employs a two-step data-driven\nprocedure. In the initial step, the most significantly different coefficients\nare selected for all pairs of populations. The subsequent step utilizes these\ncoefficients to identify populations that exhibit significant differences. To\ndemonstrate the effectiveness of the method, we provide illustrations through\nnumerical studies and application to two real datasets."}, "http://arxiv.org/abs/2112.12909": {"title": "Optimal Variable Clustering for High-Dimensional Matrix Valued Data", "link": "http://arxiv.org/abs/2112.12909", "description": "Matrix valued data has become increasingly prevalent in many applications.\nMost of the existing clustering methods for this type of data are tailored to\nthe mean model and do not account for the dependence structure of the features,\nwhich can be very informative, especially in high-dimensional settings or when\nmean information is not available. To extract the information from the\ndependence structure for clustering, we propose a new latent variable model for\nthe features arranged in matrix form, with some unknown membership matrices\nrepresenting the clusters for the rows and columns. Under this model, we\nfurther propose a class of hierarchical clustering algorithms using the\ndifference of a weighted covariance matrix as the dissimilarity measure.\nTheoretically, we show that under mild conditions, our algorithm attains\nclustering consistency in the high-dimensional setting. While this consistency\nresult holds for our algorithm with a broad class of weighted covariance\nmatrices, the conditions for this result depend on the choice of the weight. To\ninvestigate how the weight affects the theoretical performance of our\nalgorithm, we establish the minimax lower bound for clustering under our latent\nvariable model in terms of some cluster separation metric. Given these results,\nwe identify the optimal weight in the sense that using this weight guarantees\nour algorithm to be minimax rate-optimal. The practical implementation of our\nalgorithm with the optimal weight is also discussed. Simulation studies show\nthat our algorithm performs better than existing methods in terms of the\nadjusted Rand index (ARI). The method is applied to a genomic dataset and\nyields meaningful interpretations."}, "http://arxiv.org/abs/2302.09392": {"title": "Extended Excess Hazard Models for Spatially Dependent Survival Data", "link": "http://arxiv.org/abs/2302.09392", "description": "Relative survival represents the preferred framework for the analysis of\npopulation cancer survival data. The aim is to model the survival probability\nassociated to cancer in the absence of information about the cause of death.\nRecent data linkage developments have allowed for incorporating the place of\nresidence into the population cancer data bases; however, modeling this spatial\ninformation has received little attention in the relative survival setting. We\npropose a flexible parametric class of spatial excess hazard models (along with\ninference tools), named \"Relative Survival Spatial General Hazard\" (RS-SGH),\nthat allows for the inclusion of fixed and spatial effects in both time-level\nand hazard-level components. We illustrate the performance of the proposed\nmodel using an extensive simulation study, and provide guidelines about the\ninterplay of sample size, censoring, and model misspecification. We present a\ncase study using real data from colon cancer patients in England. This case\nstudy illustrates how a spatial model can be used to identify geographical\nareas with low cancer survival, as well as how to summarize such a model\nthrough marginal survival quantities and spatial effects."}, "http://arxiv.org/abs/2312.03857": {"title": "Population Monte Carlo with Normalizing Flow", "link": "http://arxiv.org/abs/2312.03857", "description": "Adaptive importance sampling (AIS) methods provide a useful alternative to\nMarkov Chain Monte Carlo (MCMC) algorithms for performing inference of\nintractable distributions. Population Monte Carlo (PMC) algorithms constitute a\nfamily of AIS approaches which adapt the proposal distributions iteratively to\nimprove the approximation of the target distribution. Recent work in this area\nprimarily focuses on ameliorating the proposal adaptation procedure for\nhigh-dimensional applications. However, most of the AIS algorithms use simple\nproposal distributions for sampling, which might be inadequate in exploring\ntarget distributions with intricate geometries. In this work, we construct\nexpressive proposal distributions in the AIS framework using normalizing flow,\nan appealing approach for modeling complex distributions. We use an iterative\nparameter update rule to enhance the approximation of the target distribution.\nNumerical experiments show that in high-dimensional settings, the proposed\nalgorithm offers significantly improved performance compared to the existing\ntechniques."}, "http://arxiv.org/abs/2312.03911": {"title": "Improving Gradient-guided Nested Sampling for Posterior Inference", "link": "http://arxiv.org/abs/2312.03911", "description": "We present a performant, general-purpose gradient-guided nested sampling\nalgorithm, ${\\tt GGNS}$, combining the state of the art in differentiable\nprogramming, Hamiltonian slice sampling, clustering, mode separation, dynamic\nnested sampling, and parallelization. This unique combination allows ${\\tt\nGGNS}$ to scale well with dimensionality and perform competitively on a variety\nof synthetic and real-world problems. We also show the potential of combining\nnested sampling with generative flow networks to obtain large amounts of\nhigh-quality samples from the posterior distribution. This combination leads to\nfaster mode discovery and more accurate estimates of the partition function."}, "http://arxiv.org/abs/2312.03967": {"title": "Test-negative designs with various reasons for testing: statistical bias and solution", "link": "http://arxiv.org/abs/2312.03967", "description": "Test-negative designs are widely used for post-market evaluation of vaccine\neffectiveness. Different from classical test-negative designs where only\nhealthcare-seekers with symptoms are included, recent test-negative designs\nhave involved individuals with various reasons for testing, especially in an\noutbreak setting. While including these data can increase sample size and hence\nimprove precision, concerns have been raised about whether they will introduce\nbias into the current framework of test-negative designs, thereby demanding a\nformal statistical examination of this modified design. In this article, using\nstatistical derivations, causal graphs, and numerical simulations, we show that\nthe standard odds ratio estimator may be biased if various reasons for testing\nare not accounted for. To eliminate this bias, we identify three categories of\nreasons for testing, including symptoms, disease-unrelated reasons, and case\ncontact tracing, and characterize associated statistical properties and\nestimands. Based on our characterization, we propose stratified estimators that\ncan incorporate multiple reasons for testing to achieve consistent estimation\nand improve precision by maximizing the use of data. The performance of our\nproposed method is demonstrated through simulation studies."}, "http://arxiv.org/abs/2312.04026": {"title": "Independent-Set Design of Experiments for Estimating Treatment and Spillover Effects under Network Interference", "link": "http://arxiv.org/abs/2312.04026", "description": "Interference is ubiquitous when conducting causal experiments over networks.\nExcept for certain network structures, causal inference on the network in the\npresence of interference is difficult due to the entanglement between the\ntreatment assignments and the interference levels. In this article, we conduct\ncausal inference under interference on an observed, sparse but connected\nnetwork, and we propose a novel design of experiments based on an independent\nset. Compared to conventional designs, the independent-set design focuses on an\nindependent subset of data and controls their interference exposures through\nthe assignments to the rest (auxiliary set). We provide a lower bound on the\nsize of the independent set from a greedy algorithm , and justify the\ntheoretical performance of estimators under the proposed design. Our approach\nis capable of estimating both spillover effects and treatment effects. We\njustify its superiority over conventional methods and illustrate the empirical\nperformance through simulations."}, "http://arxiv.org/abs/2312.04064": {"title": "DiscoBAX: Discovery of Optimal Intervention Sets in Genomic Experiment Design", "link": "http://arxiv.org/abs/2312.04064", "description": "The discovery of therapeutics to treat genetically-driven pathologies relies\non identifying genes involved in the underlying disease mechanisms. Existing\napproaches search over the billions of potential interventions to maximize the\nexpected influence on the target phenotype. However, to reduce the risk of\nfailure in future stages of trials, practical experiment design aims to find a\nset of interventions that maximally change a target phenotype via diverse\nmechanisms. We propose DiscoBAX, a sample-efficient method for maximizing the\nrate of significant discoveries per experiment while simultaneously probing for\na wide range of diverse mechanisms during a genomic experiment campaign. We\nprovide theoretical guarantees of approximate optimality under standard\nassumptions, and conduct a comprehensive experimental evaluation covering both\nsynthetic as well as real-world experimental design tasks. DiscoBAX outperforms\nexisting state-of-the-art methods for experimental design, selecting effective\nand diverse perturbations in biological systems."}, "http://arxiv.org/abs/2312.04077": {"title": "When is Plasmode simulation superior to parametric simulation when estimating the MSE of the least squares estimator in linear regression?", "link": "http://arxiv.org/abs/2312.04077", "description": "Simulation is a crucial tool for the evaluation and comparison of statistical\nmethods. How to design fair and neutral simulation studies is therefore of\ngreat interest for both researchers developing new methods and practitioners\nconfronted with the choice of the most suitable method. The term simulation\nusually refers to parametric simulation, that is, computer experiments using\nartificial data made up of pseudo-random numbers. Plasmode simulation, that is,\ncomputer experiments using the combination of resampling feature data from a\nreal-life dataset and generating the target variable with a user-selected\noutcome-generating model (OGM), is an alternative that is often claimed to\nproduce more realistic data. We compare parametric and Plasmode simulation for\nthe example of estimating the mean squared error of the least squares estimator\nin linear regression. If the true underlying data-generating process (DGP) and\nthe OGM were known, parametric simulation would be the best choice in terms of\nestimating the MSE well. However, in reality, both are usually unknown, so\nresearchers have to make assumptions: in Plasmode simulation studies for the\nOGM, in parametric simulation for both DGP and OGM. Most likely, these\nassumptions do not reflect the truth. Here, we aim to find out how assumptions\ndeviating from the true DGP and the true OGM affect the performance of\nparametric simulation and Plasmode simulations in the context of MSE estimation\nfor the least squares estimator and in which situations which simulation type\nis preferable. Our results suggest that the preferable simulation method\ndepends on many factors, including the number of features, and how the\nassumptions of a parametric simulation differ from the true DGP. Also, the\nresampling strategy used for Plasmode influences the results. In particular,\nsubsampling with a small sampling proportion can be recommended."}, "http://arxiv.org/abs/2312.04078": {"title": "A Review and Taxonomy of Methods for Quantifying Dataset Similarity", "link": "http://arxiv.org/abs/2312.04078", "description": "In statistics and machine learning, measuring the similarity between two or\nmore datasets is important for several purposes. The performance of a\npredictive model on novel datasets, referred to as generalizability, critically\ndepends on how similar the dataset used for fitting the model is to the novel\ndatasets. Exploiting or transferring insights between similar datasets is a key\naspect of meta-learning and transfer-learning. In two-sample testing, it is\nchecked, whether the underlying (multivariate) distributions of two datasets\ncoincide or not.\n\nExtremely many approaches for quantifying dataset similarity have been\nproposed in the literature. A structured overview is a crucial first step for\ncomparisons of approaches. We examine more than 100 methods and provide a\ntaxonomy, classifying them into ten classes, including (i) comparisons of\ncumulative distribution functions, density functions, or characteristic\nfunctions, (ii) methods based on multivariate ranks, (iii) discrepancy measures\nfor distributions, (iv) graph-based methods, (v) methods based on inter-point\ndistances, (vi) kernel-based methods, (vii) methods based on binary\nclassification, (viii) distance and similarity measures for datasets, (ix)\ncomparisons based on summary statistics, and (x) different testing approaches.\nHere, we present an extensive review of these methods. We introduce the main\nunderlying ideas, formal definitions, and important properties."}, "http://arxiv.org/abs/2312.04150": {"title": "A simple sensitivity analysis method for unmeasured confounders via linear programming with estimating equation constraints", "link": "http://arxiv.org/abs/2312.04150", "description": "In estimating the average treatment effect in observational studies, the\ninfluence of confounders should be appropriately addressed. To this end, the\npropensity score is widely used. If the propensity scores are known for all the\nsubjects, bias due to confounders can be adjusted by using the inverse\nprobability weighting (IPW) by the propensity score. Since the propensity score\nis unknown in general, it is usually estimated by the parametric logistic\nregression model with unknown parameters estimated by solving the score\nequation under the strongly ignorable treatment assignment (SITA) assumption.\nViolation of the SITA assumption and/or misspecification of the propensity\nscore model can cause serious bias in estimating the average treatment effect.\nTo relax the SITA assumption, the IPW estimator based on the outcome-dependent\npropensity score has been successfully introduced. However, it still depends on\nthe correctly specified parametric model and its identification. In this paper,\nwe propose a simple sensitivity analysis method for unmeasured confounders. In\nthe standard practice, the estimating equation is used to estimate the unknown\nparameters in the parametric propensity score model. Our idea is to make\ninference on the average causal effect by removing restrictive parametric model\nassumptions while still utilizing the estimating equation. Using estimating\nequations as constraints, which the true propensity scores asymptotically\nsatisfy, we construct the worst-case bounds for the average treatment effect\nwith linear programming. Different from the existing sensitivity analysis\nmethods, we construct the worst-case bounds with minimal assumptions. We\nillustrate our proposal by simulation studies and a real-world example."}, "http://arxiv.org/abs/2312.04444": {"title": "Parameter Inference for Hypo-Elliptic Diffusions under a Weak Design Condition", "link": "http://arxiv.org/abs/2312.04444", "description": "We address the problem of parameter estimation for degenerate diffusion\nprocesses defined via the solution of Stochastic Differential Equations (SDEs)\nwith diffusion matrix that is not full-rank. For this class of hypo-elliptic\ndiffusions recent works have proposed contrast estimators that are\nasymptotically normal, provided that the step-size in-between observations\n$\\Delta=\\Delta_n$ and their total number $n$ satisfy $n \\to \\infty$, $n\n\\Delta_n \\to \\infty$, $\\Delta_n \\to 0$, and additionally $\\Delta_n = o\n(n^{-1/2})$. This latter restriction places a requirement for a so-called\n`rapidly increasing experimental design'. In this paper, we overcome this\nlimitation and develop a general contrast estimator satisfying asymptotic\nnormality under the weaker design condition $\\Delta_n = o(n^{-1/p})$ for\ngeneral $p \\ge 2$. Such a result has been obtained for elliptic SDEs in the\nliterature, but its derivation in a hypo-elliptic setting is highly\nnon-trivial. We provide numerical results to illustrate the advantages of the\ndeveloped theory."}, "http://arxiv.org/abs/2312.04481": {"title": "Wasserstein complexity penalization priors: a new class of penalizing complexity priors", "link": "http://arxiv.org/abs/2312.04481", "description": "Penalizing complexity (PC) priors is a principled framework for designing\npriors that reduce model complexity. PC priors penalize the Kullback-Leibler\nDivergence (KLD) between the distributions induced by a ``simple'' model and\nthat of a more complex model. However, in many common cases, it is impossible\nto construct a prior in this way because the KLD is infinite. Various\napproximations are used to mitigate this problem, but the resulting priors then\nfail to follow the designed principles. We propose a new class of priors, the\nWasserstein complexity penalization (WCP) priors, by replacing KLD with the\nWasserstein distance in the PC prior framework. These priors avoid the infinite\nmodel distance issues and can be derived by following the principles exactly,\nmaking them more interpretable. Furthermore, principles and recipes to\nconstruct joint WCP priors for multiple parameters analytically and numerically\nare proposed and we show that they can be easily obtained, either numerically\nor analytically, for a general class of models. The methods are illustrated\nthrough several examples for which PC priors have previously been applied."}, "http://arxiv.org/abs/2108.02196": {"title": "Synthetic Controls for Experimental Design", "link": "http://arxiv.org/abs/2108.02196", "description": "This article studies experimental design in settings where the experimental\nunits are large aggregate entities (e.g., markets), and only one or a small\nnumber of units can be exposed to the treatment. In such settings,\nrandomization of the treatment may result in treated and control groups with\nvery different characteristics at baseline, inducing biases. We propose a\nvariety of synthetic control designs (Abadie, Diamond and Hainmueller, 2010,\nAbadie and Gardeazabal, 2003) as experimental designs to select treated units\nin non-randomized experiments with large aggregate units, as well as the\nuntreated units to be used as a control group. Average potential outcomes are\nestimated as weighted averages of treated units, for potential outcomes with\ntreatment -- and control units, for potential outcomes without treatment. We\nanalyze the properties of estimators based on synthetic control designs and\npropose new inferential techniques. We show that in experimental settings with\naggregate units, synthetic control designs can substantially reduce estimation\nbiases in comparison to randomization of the treatment."}, "http://arxiv.org/abs/2108.04852": {"title": "Multiway empirical likelihood", "link": "http://arxiv.org/abs/2108.04852", "description": "This paper develops a general methodology to conduct statistical inference\nfor observations indexed by multiple sets of entities. We propose a novel\nmultiway empirical likelihood statistic that converges to a chi-square\ndistribution under the non-degenerate case, where corresponding Hoeffding type\ndecomposition is dominated by linear terms. Our methodology is related to the\nnotion of jackknife empirical likelihood but the leave-out pseudo values are\nconstructed by leaving columns or rows. We further develop a modified version\nof our multiway empirical likelihood statistic, which converges to a chi-square\ndistribution regardless of the degeneracy, and discover its desirable\nhigher-order property compared to the t-ratio by the conventional Eicker-White\ntype variance estimator. The proposed methodology is illustrated by several\nimportant statistical problems, such as bipartite network, generalized\nestimating equations, and three-way observations."}, "http://arxiv.org/abs/2201.08502": {"title": "Curved factor analysis with the Ellipsoid-Gaussian distribution", "link": "http://arxiv.org/abs/2201.08502", "description": "There is a need for new models for characterizing dependence in multivariate\ndata. The multivariate Gaussian distribution is routinely used, but cannot\ncharacterize nonlinear relationships in the data. Most non-linear extensions\ntend to be highly complex; for example, involving estimation of a non-linear\nregression model in latent variables. In this article, we propose a relatively\nsimple class of Ellipsoid-Gaussian multivariate distributions, which are\nderived by using a Gaussian linear factor model involving latent variables\nhaving a von Mises-Fisher distribution on a unit hyper-sphere. We show that the\nEllipsoid-Gaussian distribution can flexibly model curved relationships among\nvariables with lower-dimensional structures. Taking a Bayesian approach, we\npropose a hybrid of gradient-based geodesic Monte Carlo and adaptive Metropolis\nfor posterior sampling. We derive basic properties and illustrate the utility\nof the Ellipsoid-Gaussian distribution on a variety of simulated and real data\napplications. An accompanying R package is also available."}, "http://arxiv.org/abs/2212.12822": {"title": "Simultaneous false discovery proportion bounds via knockoffs and closed testing", "link": "http://arxiv.org/abs/2212.12822", "description": "We propose new methods to obtain simultaneous false discovery proportion\nbounds for knockoff-based approaches. We first investigate an approach based on\nJanson and Su's $k$-familywise error rate control method and interpolation. We\nthen generalize it by considering a collection of $k$ values, and show that the\nbound of Katsevich and Ramdas is a special case of this method and can be\nuniformly improved. Next, we further generalize the method by using closed\ntesting with a multi-weighted-sum local test statistic. This allows us to\nobtain a further uniform improvement and other generalizations over previous\nmethods. We also develop an efficient shortcut for its implementation. We\ncompare the performance of our proposed methods in simulations and apply them\nto a data set from the UK Biobank."}, "http://arxiv.org/abs/2302.06054": {"title": "Single Proxy Control", "link": "http://arxiv.org/abs/2302.06054", "description": "Negative control variables are sometimes used in non-experimental studies to\ndetect the presence of confounding by hidden factors. A negative control\noutcome (NCO) is an outcome that is influenced by unobserved confounders of the\nexposure effects on the outcome in view, but is not causally impacted by the\nexposure. Tchetgen Tchetgen (2013) introduced the Control Outcome Calibration\nApproach (COCA) as a formal NCO counterfactual method to detect and correct for\nresidual confounding bias. For identification, COCA treats the NCO as an\nerror-prone proxy of the treatment-free counterfactual outcome of interest, and\ninvolves regressing the NCO on the treatment-free counterfactual, together with\na rank-preserving structural model which assumes a constant individual-level\ncausal effect. In this work, we establish nonparametric COCA identification for\nthe average causal effect for the treated, without requiring rank-preservation,\ntherefore accommodating unrestricted effect heterogeneity across units. This\nnonparametric identification result has important practical implications, as it\nprovides single proxy confounding control, in contrast to recently proposed\nproximal causal inference, which relies for identification on a pair of\nconfounding proxies. For COCA estimation we propose three separate strategies:\n(i) an extended propensity score approach, (ii) an outcome bridge function\napproach, and (iii) a doubly-robust approach. Finally, we illustrate the\nproposed methods in an application evaluating the causal impact of a Zika virus\noutbreak on birth rate in Brazil."}, "http://arxiv.org/abs/2302.11363": {"title": "lqmix: an R package for longitudinal data analysis via linear quantile mixtures", "link": "http://arxiv.org/abs/2302.11363", "description": "The analysis of longitudinal data gives the chance to observe how unit\nbehaviors change over time, but it also poses series of issues. These have been\nthe focus of a huge literature in the context of linear and generalized linear\nregression moving also, in the last ten years or so, to the context of linear\nquantile regression for continuous responses. In this paper, we present lqmix,\na novel R package that helps estimate a class of linear quantile regression\nmodels for longitudinal data, in the presence of time-constant and/or\ntime-varying, unit-specific, random coefficients, with unspecified\ndistribution. Model parameters are estimated in a maximum likelihood framework,\nvia an extended EM algorithm, and parameters' standard errors are estimated via\na block-bootstrap procedure. The analysis of a benchmark dataset is used to\ngive details on the package functions."}, "http://arxiv.org/abs/2303.04408": {"title": "Principal Component Analysis of Two-dimensional Functional Data with Serial Correlation", "link": "http://arxiv.org/abs/2303.04408", "description": "In this paper, we propose a novel model to analyze serially correlated\ntwo-dimensional functional data observed sparsely and irregularly on a domain\nwhich may not be a rectangle. Our approach employs a mixed effects model that\nspecifies the principal component functions as bivariate splines on\ntriangulations and the principal component scores as random effects which\nfollow an auto-regressive model. We apply the thin-plate penalty for\nregularizing the bivariate function estimation and develop an effective EM\nalgorithm along with Kalman filter and smoother for calculating the penalized\nlikelihood estimates of the parameters. Our approach was applied on simulated\ndatasets and on Texas monthly average temperature data from January year 1915\nto December year 2014."}, "http://arxiv.org/abs/2309.15973": {"title": "Application of data acquisition methods in the field of scientific research of public policy", "link": "http://arxiv.org/abs/2309.15973", "description": "Public policy also represent a special subdiscipline within political\nscience, within political science. They are given increasing importance and\nimportance in the context of scientific research and scientific approaches.\nPublic policy as a discipline of political science have their own special\nsubject and method of research. A particularly important aspect of the\nscientific approach to public policy is the aspect of applying research methods\nas one of the stages and phases of designing scientific research. In this\nsense, the goal of this research is to present the application of scientific\nresearch methods in the field of public policy. Those methods are based on\nscientific achievements developed within the framework of modern methodology of\nsocial sciences. Scientific research methods represent an important functional\npart of the research project as a model of the scientific research system,\npredominantly of an empirical character, which is applicable to all types of\nresearch. This is precisely what imposes the need to develop a project as a\nprerequisite for applying scientific methods and conducting scientific\nresearch, and therefore for a more complete understanding of public policy. The\nconclusions that will be reached point to the fact that scientific research of\npublic policy can not be carried out without the creation of a scientific\nresearch project as a complex scientific and operational document and the\napplication of appropriate methods and techniques developed within the\nframework of scientific achievements of modern social science methodology."}, "http://arxiv.org/abs/2312.04601": {"title": "Estimating Fr\\'echet bounds for validating programmatic weak supervision", "link": "http://arxiv.org/abs/2312.04601", "description": "We develop methods for estimating Fr\\'echet bounds on (possibly\nhigh-dimensional) distribution classes in which some variables are\ncontinuous-valued. We establish the statistical correctness of the computed\nbounds under uncertainty in the marginal constraints and demonstrate the\nusefulness of our algorithms by evaluating the performance of machine learning\n(ML) models trained with programmatic weak supervision (PWS). PWS is a\nframework for principled learning from weak supervision inputs (e.g.,\ncrowdsourced labels, knowledge bases, pre-trained models on related tasks,\netc), and it has achieved remarkable success in many areas of science and\nengineering. Unfortunately, it is generally difficult to validate the\nperformance of ML models trained with PWS due to the absence of labeled data.\nOur algorithms address this issue by estimating sharp lower and upper bounds\nfor performance metrics such as accuracy/recall/precision/F1 score."}, "http://arxiv.org/abs/2312.04661": {"title": "Robust Elastic Net Estimators for High Dimensional Generalized Linear Models", "link": "http://arxiv.org/abs/2312.04661", "description": "Robust estimators for Generalized Linear Models (GLMs) are not easy to\ndevelop because of the nature of the distributions involved. Recently, there\nhas been an increasing interest in this topic, especially in the presence of a\npossibly large number of explanatory variables. Transformed M-estimators (MT)\nare a natural way to extend the methodology of M-estimators to the class of\nGLMs and to obtain robust methods. We introduce a penalized version of\nMT-estimators in order to deal with high-dimensional data. We prove, under\nappropriate assumptions, consistency and asymptotic normality of this new class\nof estimators. The theory is developed for redescending $\\rho$-functions and\nElastic Net penalization. An iterative re-weighted least squares algorithm is\ngiven, together with a procedure to initialize it. The latter is of particular\nimportance, since the estimating equations might have multiple roots. We\nillustrate the performance of this new method for the Poisson family under\nseveral type of contaminations in a Monte Carlo experiment and in an example\nbased on a real dataset."}, "http://arxiv.org/abs/2312.04717": {"title": "A kinetic Monte Carlo Approach for Boolean Logic Functionality in Gold Nanoparticle Networks", "link": "http://arxiv.org/abs/2312.04717", "description": "Nanoparticles interconnected by insulating organic molecules exhibit\nnonlinear switching behavior at low temperatures. By assembling these switches\ninto a network and manipulating charge transport dynamics through surrounding\nelectrodes, the network can be reconfigurably functionalized to act as any\nBoolean logic gate. This work introduces a kinetic Monte Carlo-based simulation\ntool, applying established principles of single electronics to model charge\ntransport dynamics in nanoparticle networks. We functionalize nanoparticle\nnetworks as Boolean logic gates and assess their quality using a fitness\nfunction. Based on the definition of fitness, we derive new metrics to quantify\nessential nonlinear properties of the network, including negative differential\nresistance and nonlinear separability. These nonlinear properties are crucial\nnot only for functionalizing the network as Boolean logic gates but also when\nour networks are functionalized for brain-inspired computing applications in\nthe future. We address fundamental questions about the dependence of fitness\nand nonlinear properties on system size, number of surrounding electrodes, and\nelectrode positioning. We assert the overall benefit of having more electrodes,\nwith proximity to the network's output being pivotal for functionality and\nnonlinearity. Additionally, we demonstrate a optimal system size and argue for\nbreaking symmetry in electrode positioning to favor nonlinear properties."}, "http://arxiv.org/abs/2312.04747": {"title": "MetaDetect: Metamorphic Testing Based Anomaly Detection for Multi-UAV Wireless Networks", "link": "http://arxiv.org/abs/2312.04747", "description": "The reliability of wireless Ad Hoc Networks (WANET) communication is much\nlower than wired networks. WANET will be impacted by node overload, routing\nprotocol, weather, obstacle blockage, and many other factors, all those\nanomalies cannot be avoided. Accurate prediction of the network entirely\nstopping in advance is essential after people could do networking re-routing or\nchanging to different bands. In the present study, there are two primary goals.\nFirstly, design anomaly events detection patterns based on Metamorphic Testing\n(MT) methodology. Secondly, compare the performance of evaluation metrics, such\nas Transfer Rate, Occupancy rate, and the Number of packets received. Compared\nto other studies, the most significant advantage of mathematical\ninterpretability, as well as not requiring dependence on physical environmental\ninformation, only relies on the networking physical layer and Mac layer data.\nThe analysis of the results demonstrates that the proposed MT detection method\nis helpful for automatically identifying incidents/accident events on WANET.\nThe physical layer transfer Rate metric could get the best performance."}, "http://arxiv.org/abs/2312.04924": {"title": "Sparse Anomaly Detection Across Referentials: A Rank-Based Higher Criticism Approach", "link": "http://arxiv.org/abs/2312.04924", "description": "Detecting anomalies in large sets of observations is crucial in various\napplications, such as epidemiological studies, gene expression studies, and\nsystems monitoring. We consider settings where the units of interest result in\nmultiple independent observations from potentially distinct referentials. Scan\nstatistics and related methods are commonly used in such settings, but rely on\nstringent modeling assumptions for proper calibration. We instead propose a\nrank-based variant of the higher criticism statistic that only requires\nindependent observations originating from ordered spaces. We show under what\nconditions the resulting methodology is able to detect the presence of\nanomalies. These conditions are stated in a general, non-parametric manner, and\ndepend solely on the probabilities of anomalous observations exceeding nominal\nobservations. The analysis requires a refined understanding of the distribution\nof the ranks under the presence of anomalies, and in particular of the\nrank-induced dependencies. The methodology is robust against heavy-tailed\ndistributions through the use of ranks. Within the exponential family and a\nfamily of convolutional models, we analytically quantify the asymptotic\nperformance of our methodology and the performance of the oracle, and show the\ndifference is small for many common models. Simulations confirm these results.\nWe show the applicability of the methodology through an analysis of quality\ncontrol data of a pharmaceutical manufacturing process."}, "http://arxiv.org/abs/2312.04950": {"title": "Sequential inductive prediction intervals", "link": "http://arxiv.org/abs/2312.04950", "description": "In this paper we explore the concept of sequential inductive prediction\nintervals using theory from sequential testing. We furthermore introduce a\n3-parameter PAC definition of prediction intervals that allows us via\nsimulation to achieve almost sharp bounds with high probability."}, "http://arxiv.org/abs/2312.04972": {"title": "Comparison of Probabilistic Structural Reliability Methods for Ultimate Limit State Assessment of Wind Turbines", "link": "http://arxiv.org/abs/2312.04972", "description": "The probabilistic design of offshore wind turbines aims to ensure structural\nsafety in a cost-effective way. This involves conducting structural reliability\nassessments for different design options and considering different structural\nresponses. There are several structural reliability methods, and this paper\nwill apply and compare different approaches in some simplified case studies. In\nparticular, the well known environmental contour method will be compared to a\nmore novel approach based on sequential sampling and Gaussian processes\nregression for an ultimate limit state case study. For one of the case studies,\nresults will also be compared to results from a brute force simulation\napproach. Interestingly, the comparison is very different from the two case\nstudies. In one of the cases the environmental contours method agrees well with\nthe sequential sampling method but in the other, results vary considerably.\nProbably, this can be explained by the violation of some of the assumptions\nassociated with the environmental contour approach, i.e. that the short-term\nvariability of the response is large compared to the long-term variability of\nthe environmental conditions. Results from this simple comparison study\nsuggests that the sequential sampling method can be a robust and\ncomputationally effective approach for structural reliability assessment."}, "http://arxiv.org/abs/2312.05077": {"title": "Computation of least squares trimmed regression--an alternative to least trimmed squares regression", "link": "http://arxiv.org/abs/2312.05077", "description": "The least squares of depth trimmed (LST) residuals regression, proposed in\nZuo and Zuo (2023) \\cite{ZZ23}, serves as a robust alternative to the classic\nleast squares (LS) regression as well as a strong competitor to the famous\nleast trimmed squares (LTS) regression of Rousseeuw (1984) \\cite{R84}.\nTheoretical properties of the LST were thoroughly studied in \\cite{ZZ23}.\n\nThe article aims to promote the implementation and computation of the LST\nresiduals regression for a broad group of statisticians in statistical practice\nand demonstrates that (i) the LST is as robust as the benchmark of robust\nregression, the LTS regression, and much more efficient than the latter. (ii)\nIt can be as efficient as (or even more efficient than) the LS in the scenario\nwith errors uncorrelated with mean zero and homoscedastic with finite variance.\n(iii) It can be computed as fast as (or even faster than) the LTS based on a\nnewly proposed algorithm."}, "http://arxiv.org/abs/2312.05127": {"title": "Weighted least squares regression with the best robustness and high computability", "link": "http://arxiv.org/abs/2312.05127", "description": "A novel regression method is introduced and studied. The procedure weights\nsquared residuals based on their magnitude. Unlike the classic least squares\nwhich treats every squared residual equally important, the new procedure\nexponentially down-weights squared-residuals that lie far away from the cloud\nof all residuals and assigns a constant weight (one) to squared-residuals that\nlie close to the center of the squared-residual cloud.\n\nThe new procedure can keep a good balance between robustness and efficiency,\nit possesses the highest breakdown point robustness for any regression\nequivariant procedure, much more robust than the classic least squares, yet\nmuch more efficient than the benchmark of robust method, the least trimmed\nsquares (LTS) of Rousseeuw (1984).\n\nWith a smooth weight function, the new procedure could be computed very fast\nby the first-order (first-derivative) method and the second-order\n(second-derivative) method.\n\nAssertions and other theoretical findings are verified in simulated and real\ndata examples."}, "http://arxiv.org/abs/2005.04141": {"title": "Critical Values Robust to P-hacking", "link": "http://arxiv.org/abs/2005.04141", "description": "P-hacking is prevalent in reality but absent from classical hypothesis\ntesting theory. As a consequence, significant results are much more common than\nthey are supposed to be when the null hypothesis is in fact true. In this\npaper, we build a model of hypothesis testing with p-hacking. From the model,\nwe construct critical values such that, if the values are used to determine\nsignificance, and if scientists' p-hacking behavior adjusts to the new\nsignificance standards, significant results occur with the desired frequency.\nSuch robust critical values allow for p-hacking so they are larger than\nclassical critical values. To illustrate the amount of correction that\np-hacking might require, we calibrate the model using evidence from the medical\nsciences. In the calibrated model the robust critical value for any test\nstatistic is the classical critical value for the same test statistic with one\nfifth of the significance level."}, "http://arxiv.org/abs/2208.07898": {"title": "Collaborative causal inference on distributed data", "link": "http://arxiv.org/abs/2208.07898", "description": "In recent years, the development of technologies for causal inference with\nprivacy preservation of distributed data has gained considerable attention.\nMany existing methods for distributed data focus on resolving the lack of\nsubjects (samples) and can only reduce random errors in estimating treatment\neffects. In this study, we propose a data collaboration quasi-experiment\n(DC-QE) that resolves the lack of both subjects and covariates, reducing random\nerrors and biases in the estimation. Our method involves constructing\ndimensionality-reduced intermediate representations from private data from\nlocal parties, sharing intermediate representations instead of private data for\nprivacy preservation, estimating propensity scores from the shared intermediate\nrepresentations, and finally, estimating the treatment effects from propensity\nscores. Through numerical experiments on both artificial and real-world data,\nwe confirm that our method leads to better estimation results than individual\nanalyses. While dimensionality reduction loses some information in the private\ndata and causes performance degradation, we observe that sharing intermediate\nrepresentations with many parties to resolve the lack of subjects and\ncovariates sufficiently improves performance to overcome the degradation caused\nby dimensionality reduction. Although external validity is not necessarily\nguaranteed, our results suggest that DC-QE is a promising method. With the\nwidespread use of our method, intermediate representations can be published as\nopen data to help researchers find causalities and accumulate a knowledge base."}, "http://arxiv.org/abs/2304.07034": {"title": "Recursive Neyman Algorithm for Optimum Sample Allocation under Box Constraints on Sample Sizes in Strata", "link": "http://arxiv.org/abs/2304.07034", "description": "The optimum sample allocation in stratified sampling is one of the basic\nissues of survey methodology. It is a procedure of dividing the overall sample\nsize into strata sample sizes in such a way that for given sampling designs in\nstrata the variance of the stratified $\\pi$ estimator of the population total\n(or mean) for a given study variable assumes its minimum. In this work, we\nconsider the optimum allocation of a sample, under lower and upper bounds\nimposed jointly on sample sizes in strata. We are concerned with the variance\nfunction of some generic form that, in particular, covers the case of the\nsimple random sampling without replacement in strata. The goal of this paper is\ntwofold. First, we establish (using the Karush-Kuhn-Tucker conditions) a\ngeneric form of the optimal solution, the so-called optimality conditions.\nSecond, based on the established optimality conditions, we derive an efficient\nrecursive algorithm, named RNABOX, which solves the allocation problem under\nstudy. The RNABOX can be viewed as a generalization of the classical recursive\nNeyman allocation algorithm, a popular tool for optimum allocation when only\nupper bounds are imposed on sample strata-sizes. We implement RNABOX in R as a\npart of our package stratallo which is available from the Comprehensive R\nArchive Network (CRAN) repository."}, "http://arxiv.org/abs/2305.12366": {"title": "A Quantile Shift Approach To Main Effects And Interactions In A 2-By-2 Design", "link": "http://arxiv.org/abs/2305.12366", "description": "When comparing two independent groups, shift functions are basically\ntechniques that compare multiple quantiles rather than a single measure of\nlocation, the goal being to get a more detailed understanding of how the\ndistributions differ. Various versions have been proposed and studied. This\npaper deals with extensions of these methods to main effects and interactions\nin a between-by-between, 2-by-2 design. Two approaches are studied, one that\ncompares the deciles of the distributions, and one that has a certain\nconnection to the Wilcoxon-Mann-Whitney method. For both methods, we propose an\nimplementation using the Harrell-Davis quantile estimator, used in conjunction\nwith a percentile bootstrap approach. We report results of simulations of false\nand true positive rates."}, "http://arxiv.org/abs/2306.05415": {"title": "Causal normalizing flows: from theory to practice", "link": "http://arxiv.org/abs/2306.05415", "description": "In this work, we deepen on the use of normalizing flows for causal reasoning.\nSpecifically, we first leverage recent results on non-linear ICA to show that\ncausal models are identifiable from observational data given a causal ordering,\nand thus can be recovered using autoregressive normalizing flows (NFs). Second,\nwe analyze different design and learning choices for causal normalizing flows\nto capture the underlying causal data-generating process. Third, we describe\nhow to implement the do-operator in causal NFs, and thus, how to answer\ninterventional and counterfactual questions. Finally, in our experiments, we\nvalidate our design and training choices through a comprehensive ablation\nstudy; compare causal NFs to other approaches for approximating causal models;\nand empirically demonstrate that causal NFs can be used to address real-world\nproblems, where the presence of mixed discrete-continuous data and partial\nknowledge on the causal graph is the norm. The code for this work can be found\nat https://github.com/psanch21/causal-flows."}, "http://arxiv.org/abs/2307.13917": {"title": "BayesDAG: Gradient-Based Posterior Inference for Causal Discovery", "link": "http://arxiv.org/abs/2307.13917", "description": "Bayesian causal discovery aims to infer the posterior distribution over\ncausal models from observed data, quantifying epistemic uncertainty and\nbenefiting downstream tasks. However, computational challenges arise due to\njoint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and\nnonlinear functions. Despite recent progress towards efficient posterior\ninference over DAGs, existing methods are either limited to variational\ninference on node permutation matrices for linear causal models, leading to\ncompromised inference accuracy, or continuous relaxation of adjacency matrices\nconstrained by a DAG regularizer, which cannot ensure resulting graphs are\nDAGs. In this work, we introduce a scalable Bayesian causal discovery framework\nbased on a combination of stochastic gradient Markov Chain Monte Carlo\n(SG-MCMC) and Variational Inference (VI) that overcomes these limitations. Our\napproach directly samples DAGs from the posterior without requiring any DAG\nregularization, simultaneously draws function parameter samples and is\napplicable to both linear and nonlinear causal models. To enable our approach,\nwe derive a novel equivalence to the permutation-based DAG learning, which\nopens up possibilities of using any relaxed gradient estimator defined over\npermutations. To our knowledge, this is the first framework applying\ngradient-based MCMC sampling for causal discovery. Empirical evaluation on\nsynthetic and real-world datasets demonstrate our approach's effectiveness\ncompared to state-of-the-art baselines."}, "http://arxiv.org/abs/2312.05319": {"title": "Hyperbolic Network Latent Space Model with Learnable Curvature", "link": "http://arxiv.org/abs/2312.05319", "description": "Network data is ubiquitous in various scientific disciplines, including\nsociology, economics, and neuroscience. Latent space models are often employed\nin network data analysis, but the geometric effect of latent space curvature\nremains a significant, unresolved issue. In this work, we propose a hyperbolic\nnetwork latent space model with a learnable curvature parameter. We\ntheoretically justify that learning the optimal curvature is essential to\nminimizing the embedding error across all hyperbolic embedding methods beyond\nnetwork latent space models. A maximum-likelihood estimation strategy,\nemploying manifold gradient optimization, is developed, and we establish the\nconsistency and convergence rates for the maximum-likelihood estimators, both\nof which are technically challenging due to the non-linearity and non-convexity\nof the hyperbolic distance metric. We further demonstrate the geometric effect\nof latent space curvature and the superior performance of the proposed model\nthrough extensive simulation studies and an application using a Facebook\nfriendship network."}, "http://arxiv.org/abs/2312.05345": {"title": "A General Estimation Framework for Multi-State Markov Processes with Flexible Specification of the Transition Intensities", "link": "http://arxiv.org/abs/2312.05345", "description": "When interest lies in the progression of a disease rather than on a single\noutcome, non-homogeneous multi-state Markov models constitute a natural and\npowerful modelling approach. Constant monitoring of a phenomenon of interest is\noften unfeasible, hence leading to an intermittent observation scheme. This\nsetting is challenging and existing models and their implementations do not yet\nallow for flexible enough specifications that can fully exploit the information\ncontained in the data. To widen significantly the scope of multi-state Markov\nmodels, we propose a closed-form expression for the local curvature information\nof a key quantity, the transition probability matrix. Such development allows\none to model any type of multi-state Markov process, where the transition\nintensities are flexibly specified as functions of additive predictors.\nParameter estimation is carried out through a carefully structured, stable\npenalised likelihood approach. The methodology is exemplified via two case\nstudies that aim at modelling the onset of cardiac allograft vasculopathy, and\ncognitive decline. To support applicability and reproducibility, all developed\ntools are implemented in the R package flexmsm."}, "http://arxiv.org/abs/2312.05365": {"title": "Product Centered Dirichlet Processes for Dependent Clustering", "link": "http://arxiv.org/abs/2312.05365", "description": "While there is an immense literature on Bayesian methods for clustering, the\nmultiview case has received little attention. This problem focuses on obtaining\ndistinct but statistically dependent clusterings in a common set of entities\nfor different data types. For example, clustering patients into subgroups with\nsubgroup membership varying according to the domain of the patient variables. A\nchallenge is how to model the across-view dependence between the partitions of\npatients into subgroups. The complexities of the partition space make standard\nmethods to model dependence, such as correlation, infeasible. In this article,\nwe propose CLustering with Independence Centering (CLIC), a clustering prior\nthat uses a single parameter to explicitly model dependence between clusterings\nacross views. CLIC is induced by the product centered Dirichlet process (PCDP),\na novel hierarchical prior that bridges between independent and equivalent\npartitions. We show appealing theoretic properties, provide a finite\napproximation and prove its accuracy, present a marginal Gibbs sampler for\nposterior computation, and derive closed form expressions for the marginal and\njoint partition distributions for the CLIC model. On synthetic data and in an\napplication to epidemiology, CLIC accurately characterizes view-specific\npartitions while providing inference on the dependence level."}, "http://arxiv.org/abs/2312.05372": {"title": "Rational Kriging", "link": "http://arxiv.org/abs/2312.05372", "description": "This article proposes a new kriging that has a rational form. It is shown\nthat the generalized least squares estimate of the mean from rational kriging\nis much more well behaved than that from ordinary kriging. Parameter estimation\nand uncertainty quantification for rational kriging are proposed using a\nGaussian process framework. Its potential applications in emulation and\ncalibration of computer models are also discussed."}, "http://arxiv.org/abs/2312.05400": {"title": "Generalized difference-in-differences", "link": "http://arxiv.org/abs/2312.05400", "description": "We propose a new method for estimating causal effects in longitudinal/panel\ndata settings that we call generalized difference-in-differences. Our approach\nunifies two alternative approaches in these settings: ignorability estimators\n(e.g., synthetic controls) and difference-in-differences (DiD) estimators. We\npropose a new identifying assumption -- a stable bias assumption -- which\ngeneralizes the conditional parallel trends assumption in DiD, leading to the\nproposed generalized DiD framework. This change gives generalized DiD\nestimators the flexibility of ignorability estimators while maintaining the\nrobustness to unobserved confounding of DiD. We also show how ignorability and\nDiD estimators are special cases of generalized DiD. We then propose\ninfluence-function based estimators of the observed data functional, allowing\nthe use of double/debiased machine learning for estimation. We also show how\ngeneralized DiD easily extends to include clustered treatment assignment and\nstaggered adoption settings, and we discuss how the framework can facilitate\nestimation of other treatment effects beyond the average treatment effect on\nthe treated. Finally, we provide simulations which show that generalized DiD\noutperforms ignorability and DiD estimators when their identifying assumptions\nare not met, while being competitive with these special cases when their\nidentifying assumptions are met."}, "http://arxiv.org/abs/2312.05404": {"title": "Disentangled Latent Representation Learning for Tackling the Confounding M-Bias Problem in Causal Inference", "link": "http://arxiv.org/abs/2312.05404", "description": "In causal inference, it is a fundamental task to estimate the causal effect\nfrom observational data. However, latent confounders pose major challenges in\ncausal inference in observational data, for example, confounding bias and\nM-bias. Recent data-driven causal effect estimators tackle the confounding bias\nproblem via balanced representation learning, but assume no M-bias in the\nsystem, thus they fail to handle the M-bias. In this paper, we identify a\nchallenging and unsolved problem caused by a variable that leads to confounding\nbias and M-bias simultaneously. To address this problem with co-occurring\nM-bias and confounding bias, we propose a novel Disentangled Latent\nRepresentation learning framework for learning latent representations from\nproxy variables for unbiased Causal effect Estimation (DLRCE) from\nobservational data. Specifically, DLRCE learns three sets of latent\nrepresentations from the measured proxy variables to adjust for the confounding\nbias and M-bias. Extensive experiments on both synthetic and three real-world\ndatasets demonstrate that DLRCE significantly outperforms the state-of-the-art\nestimators in the case of the presence of both confounding bias and M-bias."}, "http://arxiv.org/abs/2312.05411": {"title": "Deep Bayes Factors", "link": "http://arxiv.org/abs/2312.05411", "description": "The is no other model or hypothesis verification tool in Bayesian statistics\nthat is as widely used as the Bayes factor. We focus on generative models that\nare likelihood-free and, therefore, render the computation of Bayes factors\n(marginal likelihood ratios) far from obvious. We propose a deep learning\nestimator of the Bayes factor based on simulated data from two competing models\nusing the likelihood ratio trick. This estimator is devoid of summary\nstatistics and obviates some of the difficulties with ABC model choice. We\nestablish sufficient conditions for consistency of our Deep Bayes Factor\nestimator as well as its consistency as a model selection tool. We investigate\nthe performance of our estimator on various examples using a wide range of\nquality metrics related to estimation and model decision accuracy. After\ntraining, our deep learning approach enables rapid evaluations of the Bayes\nfactor estimator at any fictional data arriving from either hypothesized model,\nnot just the observed data $Y_0$. This allows us to inspect entire Bayes factor\ndistributions under the two models and to quantify the relative location of the\nBayes factor evaluated at $Y_0$ in light of these distributions. Such tail area\nevaluations are not possible for Bayes factor estimators tailored to $Y_0$. We\nfind the performance of our Deep Bayes Factors competitive with existing MCMC\ntechniques that require the knowledge of the likelihood function. We also\nconsider variants for posterior or intrinsic Bayes factors estimation. We\ndemonstrate the usefulness of our approach on a relatively high-dimensional\nreal data example about determining cognitive biases."}, "http://arxiv.org/abs/2312.05523": {"title": "Functional Data Analysis: An Introduction and Recent Developments", "link": "http://arxiv.org/abs/2312.05523", "description": "Functional data analysis (FDA) is a statistical framework that allows for the\nanalysis of curves, images, or functions on higher dimensional domains. The\ngoals of FDA, such as descriptive analyses, classification, and regression, are\ngenerally the same as for statistical analyses of scalar-valued or multivariate\ndata, but FDA brings additional challenges due to the high- and infinite\ndimensionality of observations and parameters, respectively. This paper\nprovides an introduction to FDA, including a description of the most common\nstatistical analysis techniques, their respective software implementations, and\nsome recent developments in the field. The paper covers fundamental concepts\nsuch as descriptives and outliers, smoothing, amplitude and phase variation,\nand functional principal component analysis. It also discusses functional\nregression, statistical inference with functional data, functional\nclassification and clustering, and machine learning approaches for functional\ndata analysis. The methods discussed in this paper are widely applicable in\nfields such as medicine, biophysics, neuroscience, and chemistry, and are\nincreasingly relevant due to the widespread use of technologies that allow for\nthe collection of functional data. Sparse functional data methods are also\nrelevant for longitudinal data analysis. All presented methods are demonstrated\nusing available software in R by analyzing a data set on human motion and motor\ncontrol. To facilitate the understanding of the methods, their implementation,\nand hands-on application, the code for these practical examples is made\navailable on Github: https://github.com/davidruegamer/FDA_tutorial ."}, "http://arxiv.org/abs/2312.05590": {"title": "Gradient Tracking for High Dimensional Federated Optimization", "link": "http://arxiv.org/abs/2312.05590", "description": "In this paper, we study the (decentralized) distributed optimization problem\nwith high-dimensional sparse structure. Building upon the FedDA algorithm, we\npropose a (Decentralized) FedDA-GT algorithm, which combines the\n\\textbf{gradient tracking} technique. It is able to eliminate the heterogeneity\namong different clients' objective functions while ensuring a dimension-free\nconvergence rate. Compared to the vanilla FedDA approach, (D)FedDA-GT can\nsignificantly reduce the communication complexity, from ${O}(s^2\\log\nd/\\varepsilon^{3/2})$ to a more efficient ${O}(s^2\\log d/\\varepsilon)$. In\ncases where strong convexity is applicable, we introduce a multistep mechanism\nresulting in the Multistep ReFedDA-GT algorithm, a minor modified version of\nFedDA-GT. This approach achieves an impressive communication complexity of\n${O}\\left(s\\log d \\log \\frac{1}{\\varepsilon}\\right)$ through repeated calls to\nthe ReFedDA-GT algorithm. Finally, we conduct numerical experiments,\nillustrating that our proposed algorithms enjoy the dual advantage of being\ndimension-free and heterogeneity-free."}, "http://arxiv.org/abs/2312.05593": {"title": "Economic Forecasts Using Many Noises", "link": "http://arxiv.org/abs/2312.05593", "description": "This paper addresses a key question in economic forecasting: does pure noise\ntruly lack predictive power? Economists typically conduct variable selection to\neliminate noises from predictors. Yet, we prove a compelling result that in\nmost economic forecasts, the inclusion of noises in predictions yields greater\nbenefits than its exclusion. Furthermore, if the total number of predictors is\nnot sufficiently large, intentionally adding more noises yields superior\nforecast performance, outperforming benchmark predictors relying on dimension\nreduction. The intuition lies in economic predictive signals being densely\ndistributed among regression coefficients, maintaining modest forecast bias\nwhile diversifying away overall variance, even when a significant proportion of\npredictors constitute pure noises. One of our empirical demonstrations shows\nthat intentionally adding 300~6,000 pure noises to the Welch and Goyal (2008)\ndataset achieves a noteworthy 10% out-of-sample R square accuracy in\nforecasting the annual U.S. equity premium. The performance surpasses the\nmajority of sophisticated machine learning models."}, "http://arxiv.org/abs/2312.05682": {"title": "On Valid Multivariate Generalizations of the Confluent Hypergeometric Covariance Function", "link": "http://arxiv.org/abs/2312.05682", "description": "Modeling of multivariate random fields through Gaussian processes calls for\nthe construction of valid cross-covariance functions describing the dependence\nbetween any two component processes at different spatial locations. The\nrequired validity conditions often present challenges that lead to complicated\nrestrictions on the parameter space. The purpose of this paper is to present a\nsimplified techniques for establishing multivariate validity for the\nrecently-introduced Confluent Hypergeometric (CH) class of covariance\nfunctions. Specifically, we use multivariate mixtures to present both\nsimplified and comprehensive conditions for validity, based on results on\nconditionally negative semidefinite matrices and the Schur product theorem. In\naddition, we establish the spectral density of the CH covariance and use this\nto construct valid multivariate models as well as propose new\ncross-covariances. We show that our proposed approach leads to valid\nmultivariate cross-covariance models that inherit the desired marginal\nproperties of the CH model and outperform the multivariate Mat\\'ern model in\nout-of-sample prediction under slowly-decaying correlation of the underlying\nmultivariate random field. We also establish properties of multivariate CH\nmodels, including equivalence of Gaussian measures, and demonstrate their use\nin modeling a multivariate oceanography data set consisting of temperature,\nsalinity and oxygen, as measured by autonomous floats in the Southern Ocean."}, "http://arxiv.org/abs/2312.05700": {"title": "Influence Analysis with Panel Data", "link": "http://arxiv.org/abs/2312.05700", "description": "The presence of units with extreme values in the dependent and/or independent\nvariables (i.e., vertical outliers, leveraged data) has the potential to\nseverely bias regression coefficients and/or standard errors. This is common\nwith short panel data because the researcher cannot advocate asymptotic theory.\nExample include cross-country studies, cell-group analyses, and field or\nlaboratory experimental studies, where the researcher is forced to use few\ncross-sectional observations repeated over time due to the structure of the\ndata or research design. Available diagnostic tools may fail to properly detect\nthese anomalies, because they are not designed for panel data. In this paper,\nwe formalise statistical measures for panel data models with fixed effects to\nquantify the degree of leverage and outlyingness of units, and the joint and\nconditional influences of pairs of units. We first develop a method to visually\ndetect anomalous units in a panel data set, and identify their type. Second, we\ninvestigate the effect of these units on LS estimates, and on other units'\ninfluence on the estimated parameters. To illustrate and validate the proposed\nmethod, we use a synthetic data set contaminated with different types of\nanomalous units. We also provide an empirical example."}, "http://arxiv.org/abs/2312.05718": {"title": "Feasible contact tracing", "link": "http://arxiv.org/abs/2312.05718", "description": "Contact tracing is one of the most important tools for preventing the spread\nof infectious diseases, but as the experience of COVID-19 showed, it is also\nnext-to-impossible to implement when the disease is spreading rapidly. We show\nhow to substantially improve the efficiency of contact tracing by combining\nstandard microeconomic tools that measure heterogeneity in how infectious a\nsick person is with ideas from machine learning about sequential optimization.\nOur contributions are twofold. First, we incorporate heterogeneity in\nindividual infectiousness in a multi-armed bandit to establish optimal\nalgorithms. At the heart of this strategy is a focus on learning. In the\ntypical conceptualization of contact tracing, contacts of an infected person\nare tested to find more infections. Under a learning-first framework, however,\ncontacts of infected persons are tested to ascertain whether the infected\nperson is likely to be a \"high infector\" and to find additional infections only\nif it is likely to be highly fruitful. Second, we demonstrate using three\nadministrative contact tracing datasets from India and Pakistan during COVID-19\nthat this strategy improves efficiency. Using our algorithm, we find 80% of\ninfections with just 40% of contacts while current approaches test twice as\nmany contacts to identify the same number of infections. We further show that a\nsimple strategy that can be easily implemented in the field performs at nearly\noptimal levels, allowing for, what we call, feasible contact tracing. These\nresults are immediately transferable to contact tracing in any epidemic."}, "http://arxiv.org/abs/2312.05756": {"title": "A quantitative fusion strategy of stock picking and timing based on Particle Swarm Optimized-Back Propagation Neural Network and Multivariate Gaussian-Hidden Markov Model", "link": "http://arxiv.org/abs/2312.05756", "description": "In recent years, machine learning (ML) has brought effective approaches and\nnovel techniques to economic decision, investment forecasting, and risk\nmanagement, etc., coping the variable and intricate nature of economic and\nfinancial environments. For the investment in stock market, this research\nintroduces a pioneering quantitative fusion model combining stock timing and\npicking strategy by leveraging the Multivariate Gaussian-Hidden Markov Model\n(MGHMM) and Back Propagation Neural Network optimized by Particle Swarm\n(PSO-BPNN). After the information coefficients (IC) between fifty-two factors\nthat have been winsorized, neutralized and standardized and the return of CSI\n300 index are calculated, a given amount of factors that rank ahead are choose\nto be candidate factors heading for the input of PSO-BPNN after dimension\nreduction by Principal Component Analysis (PCA), followed by a certain amount\nof constituent stocks outputted. Subsequently, we conduct the prediction and\ntrading on the basis of the screening stocks and stock market state outputted\nby MGHMM trained using inputting CSI 300 index data after Box-Cox\ntransformation, bespeaking eximious performance during the period of past four\nyears. Ultimately, some conventional forecast and trading methods are compared\nwith our strategy in Chinese stock market. Our fusion strategy incorporating\nstock picking and timing presented in this article provide a innovative\ntechnique for financial analysis."}, "http://arxiv.org/abs/2312.05757": {"title": "Towards Human-like Perception: Learning Structural Causal Model in Heterogeneous Graph", "link": "http://arxiv.org/abs/2312.05757", "description": "Heterogeneous graph neural networks have become popular in various domains.\nHowever, their generalizability and interpretability are limited due to the\ndiscrepancy between their inherent inference flows and human reasoning logic or\nunderlying causal relationships for the learning problem. This study introduces\na novel solution, HG-SCM (Heterogeneous Graph as Structural Causal Model). It\ncan mimic the human perception and decision process through two key steps:\nconstructing intelligible variables based on semantics derived from the graph\nschema and automatically learning task-level causal relationships among these\nvariables by incorporating advanced causal discovery techniques. We compared\nHG-SCM to seven state-of-the-art baseline models on three real-world datasets,\nunder three distinct and ubiquitous out-of-distribution settings. HG-SCM\nachieved the highest average performance rank with minimal standard deviation,\nsubstantiating its effectiveness and superiority in terms of both predictive\npower and generalizability. Additionally, the visualization and analysis of the\nauto-learned causal diagrams for the three tasks aligned well with domain\nknowledge and human cognition, demonstrating prominent interpretability.\nHG-SCM's human-like nature and its enhanced generalizability and\ninterpretability make it a promising solution for special scenarios where\ntransparency and trustworthiness are paramount."}, "http://arxiv.org/abs/2312.05802": {"title": "Enhancing Scalability in Bayesian Nonparametric Factor Analysis of Spatiotemporal Data", "link": "http://arxiv.org/abs/2312.05802", "description": "This manuscript puts forward novel practicable spatiotemporal Bayesian factor\nanalysis frameworks computationally feasible for moderate to large data. Our\nmodels exhibit significantly enhanced computational scalability and storage\nefficiency, deliver high overall modeling performances, and possess powerful\ninferential capabilities for adequately predicting outcomes at future time\npoints or new spatial locations and satisfactorily clustering spatial locations\ninto regions with similar temporal trajectories, a frequently encountered\ncrucial task. We integrate on top of a baseline separable factor model with\ntemporally dependent latent factors and spatially dependent factor loadings\nunder a probit stick breaking process (PSBP) prior a new slice sampling\nalgorithm that permits unknown varying numbers of spatial mixture components\nacross all factors and guarantees them to be non-increasing through the MCMC\niterations, thus considerably enhancing model flexibility, efficiency, and\nscalability. We further introduce a novel spatial latent nearest-neighbor\nGaussian process (NNGP) prior and new sequential updating algorithms for the\nspatially varying latent variables in the PSBP prior, thereby attaining high\nspatial scalability. The markedly accelerated posterior sampling and spatial\nprediction as well as the great modeling and inferential performances of our\nmodels are substantiated by our simulation experiments."}, "http://arxiv.org/abs/2312.05898": {"title": "Dynamic Spatiotemporal ARCH Models: Small and Large Sample Results", "link": "http://arxiv.org/abs/2312.05898", "description": "This paper explores the estimation of a dynamic spatiotemporal autoregressive\nconditional heteroscedasticity (ARCH) model. The log-volatility term in this\nmodel can depend on (i) the spatial lag of the log-squared outcome variable,\n(ii) the time-lag of the log-squared outcome variable, (iii) the spatiotemporal\nlag of the log-squared outcome variable, (iv) exogenous variables, and (v) the\nunobserved heterogeneity across regions and time, i.e., the regional and time\nfixed effects. We examine the small and large sample properties of two\nquasi-maximum likelihood estimators and a generalized method of moments\nestimator for this model. We first summarize the theoretical properties of\nthese estimators and then compare their finite sample properties through Monte\nCarlo simulations."}, "http://arxiv.org/abs/2312.05974": {"title": "Learning the Causal Structure of Networked Dynamical Systems under Latent Nodes and Structured Noise", "link": "http://arxiv.org/abs/2312.05974", "description": "This paper considers learning the hidden causal network of a linear networked\ndynamical system (NDS) from the time series data at some of its nodes --\npartial observability. The dynamics of the NDS are driven by colored noise that\ngenerates spurious associations across pairs of nodes, rendering the problem\nmuch harder. To address the challenge of noise correlation and partial\nobservability, we assign to each pair of nodes a feature vector computed from\nthe time series data of observed nodes. The feature embedding is engineered to\nyield structural consistency: there exists an affine hyperplane that\nconsistently partitions the set of features, separating the feature vectors\ncorresponding to connected pairs of nodes from those corresponding to\ndisconnected pairs. The causal inference problem is thus addressed via\nclustering the designed features. We demonstrate with simple baseline\nsupervised methods the competitive performance of the proposed causal inference\nmechanism under broad connectivity regimes and noise correlation levels,\nincluding a real world network. Further, we devise novel technical guarantees\nof structural consistency for linear NDS under the considered regime."}, "http://arxiv.org/abs/2312.05985": {"title": "Fused Extended Two-Way Fixed Effects for Difference-in-Differences with Staggered Adoptions", "link": "http://arxiv.org/abs/2312.05985", "description": "To address the bias of the canonical two-way fixed effects estimator for\ndifference-in-differences under staggered adoptions, Wooldridge (2021) proposed\nthe extended two-way fixed effects estimator, which adds many parameters.\nHowever, this reduces efficiency. Restricting some of these parameters to be\nequal helps, but ad hoc restrictions may reintroduce bias. We propose a machine\nlearning estimator with a single tuning parameter, fused extended two-way fixed\neffects (FETWFE), that enables automatic data-driven selection of these\nrestrictions. We prove that under an appropriate sparsity assumption FETWFE\nidentifies the correct restrictions with probability tending to one. We also\nprove the consistency, asymptotic normality, and oracle efficiency of FETWFE\nfor two classes of heterogeneous marginal treatment effect estimators under\neither conditional or marginal parallel trends, and we prove consistency for\ntwo classes of conditional average treatment effects under conditional parallel\ntrends. We demonstrate FETWFE in simulation studies and an empirical\napplication."}, "http://arxiv.org/abs/2312.06018": {"title": "A Multivariate Polya Tree Model for Meta-Analysis with Event Time Distributions", "link": "http://arxiv.org/abs/2312.06018", "description": "We develop a non-parametric Bayesian prior for a family of random probability\nmeasures by extending the Polya tree ($PT$) prior to a joint prior for a set of\nprobability measures $G_1,\\dots,G_n$, suitable for meta-analysis with event\ntime outcomes. In the application to meta-analysis $G_i$ is the event time\ndistribution specific to study $i$. The proposed model defines a regression on\nstudy-specific covariates by introducing increased correlation for any pair of\nstudies with similar characteristics. The desired multivariate $PT$ model is\nconstructed by introducing a hierarchical prior on the conditional splitting\nprobabilities in the $PT$ construction for each of the $G_i$. The hierarchical\nprior replaces the independent beta priors for the splitting probability in the\n$PT$ construction with a Gaussian process prior for corresponding (logit)\nsplitting probabilities across all studies. The Gaussian process is indexed by\nstudy-specific covariates, introducing the desired dependence with increased\ncorrelation for similar studies. The main feature of the proposed construction\nis (conditionally) conjugate posterior updating with commonly reported\ninference summaries for event time data. The construction is motivated by a\nmeta-analysis over cancer immunotherapy studies."}, "http://arxiv.org/abs/2312.06028": {"title": "Rejoinder to Discussion of \"A Tale of Two Datasets: Representativeness and Generalisability of Inference for Samples of Networks''", "link": "http://arxiv.org/abs/2312.06028", "description": "This rejoinder responds to discussions by of Caimo, Niezink, and\nSchweinberger and Fritz of ''A Tale of Two Datasets: Representativeness and\nGeneralisability of Inference for Samples of Networks'' by Krivitsky, Coletti,\nand Hens, all published in the Journal of the American Statistical Association\nin 2023."}, "http://arxiv.org/abs/2312.06098": {"title": "Mixture Matrix-valued Autoregressive Model", "link": "http://arxiv.org/abs/2312.06098", "description": "Time series of matrix-valued data are increasingly available in various areas\nincluding economics, finance, social science, etc. These data may shed light on\nthe inter-dynamical relationships between two sets of attributes, for instance\ncountries and economic indices. The matrix autoregressive (MAR) model provides\na parsimonious approach for analyzing such data. However, the MAR model, being\na linear model with parametric constraints, cannot capture the nonlinear\npatterns in the data, such as regime shifts in the dynamics. We propose a\nmixture matrix autoregressive (MMAR) model for analyzing potential regime\nshifts in the dynamics between two attributes, for instance, due to recession\nvs. blooming, or quiet period vs. pandemic. We propose an EM algorithm for\nmaximum likelihood estimation. We derive some theoretical properties of the\nproposed method including consistency and asymptotic distribution, and\nillustrate its performance via simulations and real applications."}, "http://arxiv.org/abs/2312.06155": {"title": "Illustrating the structures of bias from immortal time using directed acyclic graphs", "link": "http://arxiv.org/abs/2312.06155", "description": "Immortal time is a period of follow-up during which death or the study\noutcome cannot occur by design. Bias from immortal time has been increasingly\nrecognized in epidemiologic studies. However, it remains unclear how immortal\ntime arises and what the structures of bias from immortal time are. Here, we\nuse an example \"Do Nobel Prize winners live longer than less recognized\nscientists?\" to illustrate that immortal time arises from using postbaseline\ninformation to define exposure or eligibility. We use time-varying directed\nacyclic graphs (DAGs) to present the structures of bias from immortal time as\nthe key sources of bias, that is confounding and selection bias. We explain\nthat excluding immortal time from the follow-up does not fully address the\nbias, and that the presence of competing risks can worsen the bias. We also\ndiscuss how the structures of bias from immortal time are shared by different\nstudy designs in pharmacoepidemiology and provide solutions, where possible, to\naddress the bias."}, "http://arxiv.org/abs/2312.06159": {"title": "Could dropping a few cells change the takeaways from differential expression?", "link": "http://arxiv.org/abs/2312.06159", "description": "Differential expression (DE) plays a fundamental role toward illuminating the\nmolecular mechanisms driving a difference between groups (e.g., due to\ntreatment or disease). While any analysis is run on particular cells/samples,\nthe intent is to generalize to future occurrences of the treatment or disease.\nImplicitly, this step is justified by assuming that present and future samples\nare independent and identically distributed from the same population. Though\nthis assumption is always false, we hope that any deviation from the assumption\nis small enough that A) conclusions of the analysis still hold and B) standard\ntools like standard error, significance, and power still reflect\ngeneralizability. Conversely, we might worry about these deviations, and\nreliance on standard tools, if conclusions could be substantively changed by\ndropping a very small fraction of data. While checking every small fraction is\ncomputationally intractable, recent work develops an approximation to identify\nwhen such an influential subset exists. Building on this work, we develop a\nmetric for dropping-data robustness of DE; namely, we cast the analysis in a\nform suitable to the approximation, extend the approximation to models with\ndata-dependent hyperparameters, and extend the notion of a data point from a\nsingle cell to a pseudobulk observation. We then overcome the inherent\nnon-differentiability of gene set enrichment analysis to develop an additional\napproximation for the robustness of top gene sets. We assess robustness of DE\nfor published single-cell RNA-seq data and discover that 1000s of genes can\nhave their results flipped by dropping &lt;1% of the data, including 100s that are\nsensitive to dropping a single cell (0.07%). Surprisingly, this non-robustness\nextends to high-level takeaways; half of the top 10 gene sets can be changed by\ndropping 1-2% of cells, and 2/10 can be changed by dropping a single cell."}, "http://arxiv.org/abs/2312.06204": {"title": "Multilayer Network Regression with Eigenvector Centrality and Community Structure", "link": "http://arxiv.org/abs/2312.06204", "description": "Centrality measures and community structures play a pivotal role in the\nanalysis of complex networks. To effectively model the impact of the network on\nour variable of interest, it is crucial to integrate information from the\nmultilayer network, including the interlayer correlations of network data. In\nthis study, we introduce a two-stage regression model that leverages the\neigenvector centrality and network community structure of fourth-order\ntensor-like multilayer networks. Initially, we utilize the eigenvector\ncentrality of multilayer networks, a method that has found extensive\napplication in prior research. Subsequently, we amalgamate the network\ncommunity structure to construct the community component centrality and\nindividual component centrality of nodes, which are then incorporated into the\nregression model. Furthermore, we establish the asymptotic properties of the\nleast squares estimates of the regression model coefficients. Our proposed\nmethod is employed to analyze data from the European airport network and The\nWorld Input-Output Database (WIOD), demonstrating its practical applicability\nand effectiveness."}, "http://arxiv.org/abs/2312.06265": {"title": "Type I Error Rates are Not Usually Inflated", "link": "http://arxiv.org/abs/2312.06265", "description": "The inflation of Type I error rates is thought to be one of the causes of the\nreplication crisis. Questionable research practices such as p-hacking are\nthought to inflate Type I error rates above their nominal level, leading to\nunexpectedly high levels of false positives in the literature and,\nconsequently, unexpectedly low replication rates. In this article, I offer an\nalternative view. I argue that questionable and other research practices do not\nusually inflate relevant Type I error rates. I begin with an introduction to\nType I error rates that distinguishes them from theoretical errors. I then\nillustrate my argument with respect to model misspecification, multiple\ntesting, selective inference, forking paths, exploratory analyses, p-hacking,\noptional stopping, double dipping, and HARKing. In each case, I demonstrate\nthat relevant Type I error rates are not usually inflated above their nominal\nlevel, and in the rare cases that they are, the inflation is easily identified\nand resolved. I conclude that the replication crisis may be explained, at least\nin part, by researchers' misinterpretation of statistical errors and their\nunderestimation of theoretical errors."}, "http://arxiv.org/abs/2312.06289": {"title": "A graphical framework for interpretable correlation matrix models", "link": "http://arxiv.org/abs/2312.06289", "description": "In this work, we present a new approach for constructing models for\ncorrelation matrices with a user-defined graphical structure. The graphical\nstructure makes correlation matrices interpretable and avoids the quadratic\nincrease of parameters as a function of the dimension. We suggest an automatic\napproach to define a prior using a natural sequence of simpler models within\nthe Penalized Complexity framework for the unknown parameters in these models.\n\nWe illustrate this approach with three applications: a multivariate linear\nregression of four biomarkers, a multivariate disease mapping, and a\nmultivariate longitudinal joint modelling. Each application underscores our\nmethod's intuitive appeal, signifying a substantial advancement toward a more\ncohesive and enlightening model that facilitates a meaningful interpretation of\ncorrelation matrices."}, "http://arxiv.org/abs/2312.06334": {"title": "Scoring multilevel regression and postratification based population and subpopulation estimates", "link": "http://arxiv.org/abs/2312.06334", "description": "Multilevel regression and poststratification (MRP) has been used extensively\nto adjust convenience and low-response surveys to make population and\nsubpopulation estimates. For this method, model validation is particularly\nimportant, but recent work has suggested that simple aggregation of individual\nprediction errors does not give a good measure of the error of the population\nestimate. In this manuscript we provide a clear explanation for why this\noccurs, propose two scoring metrics designed specifically for this problem, and\ndemonstrate their use in three different ways. We demonstrate that these\nscoring metrics correctly order models when compared to true goodness of\nestimate, although they do underestimate the magnitude of the score."}, "http://arxiv.org/abs/2312.06415": {"title": "Practicable Power Curve Approximation for Bioequivalence with Unequal Variances", "link": "http://arxiv.org/abs/2312.06415", "description": "Two-group (bio)equivalence tests assess whether two drug formulations provide\nsimilar therapeutic effects. These studies are often conducted using two\none-sided t-tests, where the test statistics jointly follow a bivariate\nt-distribution with singular covariance matrix. Unless the two groups of data\nare assumed to have equal variances, the degrees of freedom for this bivariate\nt-distribution are noninteger and unknown a priori. This makes it difficult to\nanalytically find sample sizes that yield desired power for the study using an\nautomated process. Popular free software for bioequivalence study design does\nnot accommodate the comparison of two groups with unequal variances, and\ncertain paid software solutions that make this accommodation produce unstable\nresults. We propose a novel simulation-based method that uses Sobol' sequences\nand root-finding algorithms to quickly and accurately approximate the power\ncurve for two-group bioequivalence tests with unequal variances. We also\nillustrate that caution should be exercised when assuming automated methods for\npower estimation are robust to arbitrary bioequivalence designs. Our methods\nfor sample size determination mitigate this lack of robustness and are widely\napplicable to equivalence and noninferiority tests facilitated via parallel and\ncrossover designs. All methods proposed in this work can be implemented using\nthe dent package in R."}, "http://arxiv.org/abs/2312.06437": {"title": "Posterior Ramifications of Prior Dependence Structures", "link": "http://arxiv.org/abs/2312.06437", "description": "In fully Bayesian analyses, prior distributions are specified before\nobserving data. Prior elicitation methods transfigure prior information into\nquantifiable prior distributions. Recently, methods that leverage copulas have\nbeen proposed to accommodate more flexible dependence structures when eliciting\nmultivariate priors. The resulting priors have been framed as suitable\ncandidates for Bayesian analysis. We prove that under broad conditions, the\nposterior cannot retain many of these flexible prior dependence structures as\ndata are observed. However, these flexible copula-based priors are useful for\ndesign purposes. Because correctly specifying the dependence structure a priori\ncan be difficult, we consider how the choice of prior copula impacts the\nposterior distribution in terms of convergence of the posterior mode. We also\nmake recommendations regarding prior dependence specification for posterior\nanalyses that streamline the prior elicitation process."}, "http://arxiv.org/abs/2312.06465": {"title": "A New Projection Pursuit Index for Big Data", "link": "http://arxiv.org/abs/2312.06465", "description": "Visualization of extremely large datasets in static or dynamic form is a huge\nchallenge because most traditional methods cannot deal with big data problems.\nA new visualization method for big data is proposed based on Projection\nPursuit, Guided Tour and Data Nuggets methods, that will help display\ninteresting hidden structures such as clusters, outliers, and other nonlinear\nstructures in big data. The Guided Tour is a dynamic graphical tool for\nhigh-dimensional data combining Projection Pursuit and Grand Tour methods. It\ndisplays a dynamic sequence of low-dimensional projections obtained by using\nProjection Pursuit (PP) index functions to navigate the data space. Different\nPP indices have been developed to detect interesting structures of multivariate\ndata but there are computational problems for big data using the original\nguided tour with these indices. A new PP index is developed to be computable\nfor big data, with the help of a data compression method called Data Nuggets\nthat reduces large datasets while maintaining the original data structure.\nSimulation studies are conducted and a real large dataset is used to illustrate\nthe proposed methodology. Static and dynamic graphical tools for big data can\nbe developed based on the proposed PP index to detect nonlinear structures."}, "http://arxiv.org/abs/2312.06478": {"title": "Prediction De-Correlated Inference", "link": "http://arxiv.org/abs/2312.06478", "description": "Leveraging machine-learning methods to predict outcomes on some unlabeled\ndatasets and then using these pseudo-outcomes in subsequent statistical\ninference is common in modern data analysis. Inference in this setting is often\ncalled post-prediction inference. We propose a novel, assumption-lean framework\nfor inference under post-prediction setting, called \\emph{Prediction\nDe-Correlated inference} (PDC). Our approach can automatically adapt to any\nblack-box machine-learning model and consistently outperforms supervised\nmethods. The PDC framework also offers easy extensibility for accommodating\nmultiple predictive models. Both numerical results and real-world data analysis\nsupport our theoretical results."}, "http://arxiv.org/abs/2312.06547": {"title": "KF-PLS: Optimizing Kernel Partial Least-Squares (K-PLS) with Kernel Flows", "link": "http://arxiv.org/abs/2312.06547", "description": "Partial Least-Squares (PLS) Regression is a widely used tool in chemometrics\nfor performing multivariate regression. PLS is a bi-linear method that has a\nlimited capacity of modelling non-linear relations between the predictor\nvariables and the response. Kernel PLS (K-PLS) has been introduced for\nmodelling non-linear predictor-response relations. In K-PLS, the input data is\nmapped via a kernel function to a Reproducing Kernel Hilbert space (RKH), where\nthe dependencies between the response and the input matrix are assumed to be\nlinear. K-PLS is performed in the RKH space between the kernel matrix and the\ndependent variable. Most available studies use fixed kernel parameters. Only a\nfew studies have been conducted on optimizing the kernel parameters for K-PLS.\nIn this article, we propose a methodology for the kernel function optimization\nbased on Kernel Flows (KF), a technique developed for Gaussian process\nregression (GPR). The results are illustrated with four case studies. The case\nstudies represent both numerical examples and real data used in classification\nand regression tasks. K-PLS optimized with KF, called KF-PLS in this study, is\nshown to yield good results in all illustrated scenarios. The paper presents\ncross-validation studies and hyperparameter analysis of the KF methodology when\napplied to K-PLS."}, "http://arxiv.org/abs/2312.06605": {"title": "Statistical Inference on Latent Space Models for Network Data", "link": "http://arxiv.org/abs/2312.06605", "description": "Latent space models are powerful statistical tools for modeling and\nunderstanding network data. While the importance of accounting for uncertainty\nin network analysis has been well recognized, the current literature\npredominantly focuses on point estimation and prediction, leaving the\nstatistical inference of latent space models an open question. This work aims\nto fill this gap by providing a general framework to analyze the theoretical\nproperties of the maximum likelihood estimators. In particular, we establish\nthe uniform consistency and asymptotic distribution results for the latent\nspace models under different edge types and link functions. Furthermore, the\nproposed framework enables us to generalize our results to the dependent-edge\nand sparse scenarios. Our theories are supported by simulation studies and have\nthe potential to be applied in downstream inferences, such as link prediction\nand network testing problems."}, "http://arxiv.org/abs/2312.06616": {"title": "The built environment and induced transport CO2 emissions: A double machine learning approach to account for residential self-selection", "link": "http://arxiv.org/abs/2312.06616", "description": "Understanding why travel behavior differs between residents of urban centers\nand suburbs is key to sustainable urban planning. Especially in light of rapid\nurban growth, identifying housing locations that minimize travel demand and\ninduced CO2 emissions is crucial to mitigate climate change. While the built\nenvironment plays an important role, the precise impact on travel behavior is\nobfuscated by residential self-selection. To address this issue, we propose a\ndouble machine learning approach to obtain unbiased, spatially-explicit\nestimates of the effect of the built environment on travel-related CO2\nemissions for each neighborhood by controlling for residential self-selection.\nWe examine how socio-demographics and travel-related attitudes moderate the\neffect and how it decomposes across the 5Ds of the built environment. Based on\na case study for Berlin and the travel diaries of 32,000 residents, we find\nthat the built environment causes household travel-related CO2 emissions to\ndiffer by a factor of almost two between central and suburban neighborhoods in\nBerlin. To highlight the practical importance for urban climate mitigation, we\nevaluate current plans for 64,000 new residential units in terms of total\ninduced transport CO2 emissions. Our findings underscore the significance of\nspatially differentiated compact development to decarbonize the transport\nsector."}, "http://arxiv.org/abs/2110.12722": {"title": "Functional instrumental variable regression with an application to estimating the impact of immigration on native wages", "link": "http://arxiv.org/abs/2110.12722", "description": "Functional linear regression gets its popularity as a statistical tool to\nstudy the relationship between function-valued response and exogenous\nexplanatory variables. However, in practice, it is hard to expect that the\nexplanatory variables of interest are perfectly exogenous, due to, for example,\nthe presence of omitted variables and measurement error. Despite its empirical\nrelevance, it was not until recently that this issue of endogeneity was studied\nin the literature on functional regression, and the development in this\ndirection does not seem to sufficiently meet practitioners' needs; for example,\nthis issue has been discussed with paying particular attention on consistent\nestimation and thus distributional properties of the proposed estimators still\nremain to be further explored. To fill this gap, this paper proposes new\nconsistent FPCA-based instrumental variable estimators and develops their\nasymptotic properties in detail. Simulation experiments under a wide range of\nsettings show that the proposed estimators perform considerably well. We apply\nour methodology to estimate the impact of immigration on native wages."}, "http://arxiv.org/abs/2111.10715": {"title": "Confidences in Hypotheses", "link": "http://arxiv.org/abs/2111.10715", "description": "This article introduces a broadly-applicable new method of statistical\nanalysis called hypotheses assessment. It is a frequentist procedure designed\nto answer the question: Given the sample evidence and assuming one of two\nhypotheses is true, what is the relative plausibility of each hypothesis? Our\naim is to determine frequentist confidences in the hypotheses that are relevant\nto the data at hand and are as powerful as the particular application allows.\nHypotheses assessments complement hypothesis tests because providing\nconfidences in the hypotheses in addition to test results can better inform\napplied researchers about the strength of evidence provided by the data. For\nsimple hypotheses, the method produces minimum and maximum confidences in each\nhypothesis. The composite case is more complex, and we introduce two\nconventions to aid with understanding the strength of evidence. Assessments are\nqualitatively different from hypothesis testing and confidence interval\noutcomes, and thus fill a gap in the statistician's toolkit."}, "http://arxiv.org/abs/2204.03343": {"title": "Binary Spatial Random Field Reconstruction from Non-Gaussian Inhomogeneous Time-series Observations", "link": "http://arxiv.org/abs/2204.03343", "description": "We develop a new model for spatial random field reconstruction of a\nbinary-valued spatial phenomenon. In our model, sensors are deployed in a\nwireless sensor network across a large geographical region. Each sensor\nmeasures a non-Gaussian inhomogeneous temporal process which depends on the\nspatial phenomenon. Two types of sensors are employed: one collects point\nobservations at specific time points, while the other collects integral\nobservations over time intervals. Subsequently, the sensors transmit these\ntime-series observations to a Fusion Center (FC), and the FC infers the spatial\nphenomenon from these observations. We show that the resulting posterior\npredictive distribution is intractable and develop a tractable two-step\nprocedure to perform inference. Firstly, we develop algorithms to perform\napproximate Likelihood Ratio Tests on the time-series observations, compressing\nthem to a single bit for both point sensors and integral sensors. Secondly,\nonce the compressed observations are transmitted to the FC, we utilize a\nSpatial Best Linear Unbiased Estimator (S-BLUE) to reconstruct the binary\nspatial random field at any desired spatial location. The performance of the\nproposed approach is studied using simulation. We further illustrate the\neffectiveness of our method using a weather dataset from the National\nEnvironment Agency (NEA) of Singapore with fields including temperature and\nrelative humidity."}, "http://arxiv.org/abs/2210.08964": {"title": "PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting", "link": "http://arxiv.org/abs/2210.08964", "description": "This paper presents a new perspective on time series forecasting. In existing\ntime series forecasting methods, the models take a sequence of numerical values\nas input and yield numerical values as output. The existing SOTA models are\nlargely based on the Transformer architecture, modified with multiple encoding\nmechanisms to incorporate the context and semantics around the historical data.\nInspired by the successes of pre-trained language foundation models, we pose a\nquestion about whether these models can also be adapted to solve time-series\nforecasting. Thus, we propose a new forecasting paradigm: prompt-based time\nseries forecasting (PromptCast). In this novel task, the numerical input and\noutput are transformed into prompts and the forecasting task is framed in a\nsentence-to-sentence manner, making it possible to directly apply language\nmodels for forecasting purposes. To support and facilitate the research of this\ntask, we also present a large-scale dataset (PISA) that includes three\nreal-world forecasting scenarios. We evaluate different SOTA numerical-based\nforecasting methods and language generation models. The benchmark results with\nvarious forecasting settings demonstrate the proposed PromptCast with language\ngeneration models is a promising research direction. Additionally, in\ncomparison to conventional numerical-based forecasting, PromptCast shows a much\nbetter generalization ability under the zero-shot setting."}, "http://arxiv.org/abs/2211.13715": {"title": "Trust Your $\\nabla$: Gradient-based Intervention Targeting for Causal Discovery", "link": "http://arxiv.org/abs/2211.13715", "description": "Inferring causal structure from data is a challenging task of fundamental\nimportance in science. Observational data are often insufficient to identify a\nsystem's causal structure uniquely. While conducting interventions (i.e.,\nexperiments) can improve the identifiability, such samples are usually\nchallenging and expensive to obtain. Hence, experimental design approaches for\ncausal discovery aim to minimize the number of interventions by estimating the\nmost informative intervention target. In this work, we propose a novel\nGradient-based Intervention Targeting method, abbreviated GIT, that 'trusts'\nthe gradient estimator of a gradient-based causal discovery framework to\nprovide signals for the intervention acquisition function. We provide extensive\nexperiments in simulated and real-world datasets and demonstrate that GIT\nperforms on par with competitive baselines, surpassing them in the low-data\nregime."}, "http://arxiv.org/abs/2303.02951": {"title": "The (Surprising) Sample Optimality of Greedy Procedures for Large-Scale Ranking and Selection", "link": "http://arxiv.org/abs/2303.02951", "description": "Ranking and selection (R&amp;S) aims to select the best alternative with the\nlargest mean performance from a finite set of alternatives. Recently,\nconsiderable attention has turned towards the large-scale R&amp;S problem which\ninvolves a large number of alternatives. Ideal large-scale R&amp;S procedures\nshould be sample optimal, i.e., the total sample size required to deliver an\nasymptotically non-zero probability of correct selection (PCS) grows at the\nminimal order (linear order) in the number of alternatives, $k$. Surprisingly,\nwe discover that the na\\\"ive greedy procedure, which keeps sampling the\nalternative with the largest running average, performs strikingly well and\nappears sample optimal. To understand this discovery, we develop a new\nboundary-crossing perspective and prove that the greedy procedure is sample\noptimal for the scenarios where the best mean maintains at least a positive\nconstant away from all other means as $k$ increases. We further show that the\nderived PCS lower bound is asymptotically tight for the slippage configuration\nof means with a common variance. For other scenarios, we consider the\nprobability of good selection and find that the result depends on the growth\nbehavior of the number of good alternatives: if it remains bounded as $k$\nincreases, the sample optimality still holds; otherwise, the result may change.\nMoreover, we propose the explore-first greedy procedures by adding an\nexploration phase to the greedy procedure. The procedures are proven to be\nsample optimal and consistent under the same assumptions. Last, we numerically\ninvestigate the performance of our greedy procedures in solving large-scale R&amp;S\nproblems."}, "http://arxiv.org/abs/2303.07490": {"title": "Comparing the Robustness of Simple Network Scale-Up Method (NSUM) Estimators", "link": "http://arxiv.org/abs/2303.07490", "description": "The network scale-up method (NSUM) is a cost-effective approach to estimating\nthe size or prevalence of a group of people that is hard to reach through a\nstandard survey. The basic NSUM involves two steps: estimating respondents'\ndegrees by one of various methods (in this paper we focus on the probe group\nmethod which uses the number of people a respondent knows in various groups of\nknown size), and estimating the prevalence of the hard-to-reach population of\ninterest using respondents' estimated degrees and the number of people they\nreport knowing in the hard-to-reach group. Each of these two steps involves\ntaking either an average of ratios or a ratio of averages. Using the ratio of\naverages for each step has so far been the most common approach. However, we\npresent theoretical arguments that using the average of ratios at the second,\nprevalence-estimation step often has lower mean squared error when the random\nmixing assumption is violated, which seems likely in practice; this estimator\nwhich uses the ratio of averages for degree estimates and the average of ratios\nfor prevalence was proposed early in NSUM development but has largely been\nunexplored and unused. Simulation results using an example network data set\nalso support these findings. Based on this theoretical and empirical evidence,\nwe suggest that future surveys that use a simple estimator may want to use this\nmixed estimator, and estimation methods based on this estimator may produce new\nimprovements."}, "http://arxiv.org/abs/2307.11084": {"title": "GeoCoDA: Recognizing and Validating Structural Processes in Geochemical Data", "link": "http://arxiv.org/abs/2307.11084", "description": "Geochemical data are compositional in nature and are subject to the problems\ntypically associated with data that are restricted to the real non-negative\nnumber space with constant-sum constraint, that is, the simplex. Geochemistry\ncan be considered a proxy for mineralogy, comprised of atomically ordered\nstructures that define the placement and abundance of elements in the mineral\nlattice structure. Based on the innovative contributions of John Aitchison, who\nintroduced the logratio transformation into compositional data analysis, this\ncontribution provides a systematic workflow for assessing geochemical data in a\nsimple and efficient way, such that significant geochemical (mineralogical)\nprocesses can be recognized and validated. This workflow, called GeoCoDA and\npresented here in the form of a tutorial, enables the recognition of processes\nfrom which models can be constructed based on the associations of elements that\nreflect mineralogy. Both the original compositional values and their\ntransformation to logratios are considered. These models can reflect\nrock-forming processes, metamorphism, alteration and ore mineralization.\nMoreover, machine learning methods, both unsupervised and supervised, applied\nto an optimized set of subcompositions of the data, provide a systematic,\naccurate, efficient and defensible approach to geochemical data analysis. The\nworkflow is illustrated on lithogeochemical data from exploration of the Star\nkimberlite, consisting of a series of eruptions with five recognized phases."}, "http://arxiv.org/abs/2307.15681": {"title": "A Continuous-Time Dynamic Factor Model for Intensive Longitudinal Data Arising from Mobile Health Studies", "link": "http://arxiv.org/abs/2307.15681", "description": "Intensive longitudinal data (ILD) collected in mobile health (mHealth)\nstudies contain rich information on multiple outcomes measured frequently over\ntime that have the potential to capture short-term and long-term dynamics.\nMotivated by an mHealth study of smoking cessation in which participants\nself-report the intensity of many emotions multiple times per day, we describe\na dynamic factor model that summarizes the ILD as a low-dimensional,\ninterpretable latent process. This model consists of two submodels: (i) a\nmeasurement submodel--a factor model--that summarizes the multivariate\nlongitudinal outcome as lower-dimensional latent variables and (ii) a\nstructural submodel--an Ornstein-Uhlenbeck (OU) stochastic process--that\ncaptures the temporal dynamics of the multivariate latent process in continuous\ntime. We derive a closed-form likelihood for the marginal distribution of the\noutcome and the computationally-simpler sparse precision matrix for the OU\nprocess. We propose a block coordinate descent algorithm for estimation.\nFinally, we apply our method to the mHealth data to summarize the dynamics of\n18 different emotions as two latent processes. These latent processes are\ninterpreted by behavioral scientists as the psychological constructs of\npositive and negative affect and are key in understanding vulnerability to\nlapsing back to tobacco use among smokers attempting to quit."}, "http://arxiv.org/abs/2308.09562": {"title": "Outlier detection for mixed-type data: A novel approach", "link": "http://arxiv.org/abs/2308.09562", "description": "Outlier detection can serve as an extremely important tool for researchers\nfrom a wide range of fields. From the sectors of banking and marketing to the\nsocial sciences and healthcare sectors, outlier detection techniques are very\nuseful for identifying subjects that exhibit different and sometimes peculiar\nbehaviours. When the data set available to the researcher consists of both\ndiscrete and continuous variables, outlier detection presents unprecedented\nchallenges. In this paper we propose a novel method that detects outlying\nobservations in settings of mixed-type data, while reducing the required user\ninteraction and providing general guidelines for selecting suitable\nhyperparameter values. The methodology developed is being assessed through a\nseries of simulations on data sets with varying characteristics and achieves\nvery good performance levels. Our method demonstrates a high capacity for\ndetecting the majority of outliers while minimising the number of falsely\ndetected non-outlying observations. The ideas and techniques outlined in the\npaper can be used either as a pre-processing step or in tandem with other data\nmining and machine learning algorithms for developing novel approaches to\nchallenging research problems."}, "http://arxiv.org/abs/2308.15986": {"title": "Sensitivity Analysis for Causal Effects in Observational Studies with Multivalued Treatments", "link": "http://arxiv.org/abs/2308.15986", "description": "One of the fundamental challenges in drawing causal inferences from\nobservational studies is that the assumption of no unmeasured confounding is\nnot testable from observed data. Therefore, assessing sensitivity to this\nassumption's violation is important to obtain valid causal conclusions in\nobservational studies. Although several sensitivity analysis frameworks are\navailable in the casual inference literature, very few of them are applicable\nto observational studies with multivalued treatments. To address this issue, we\npropose a sensitivity analysis framework for performing sensitivity analysis in\nmultivalued treatment settings. Within this framework, a general class of\nadditive causal estimands has been proposed. We demonstrate that the estimation\nof the causal estimands under the proposed sensitivity model can be performed\nvery efficiently. Simulation results show that the proposed framework performs\nwell in terms of bias of the point estimates and coverage of the confidence\nintervals when there is sufficient overlap in the covariate distributions. We\nillustrate the application of our proposed method by conducting an\nobservational study that estimates the causal effect of fish consumption on\nblood mercury levels."}, "http://arxiv.org/abs/2309.01536": {"title": "perms: Likelihood-free estimation of marginal likelihoods for binary response data in Python and R", "link": "http://arxiv.org/abs/2309.01536", "description": "In Bayesian statistics, the marginal likelihood (ML) is the key ingredient\nneeded for model comparison and model averaging. Unfortunately, estimating MLs\naccurately is notoriously difficult, especially for models where posterior\nsimulation is not possible. Recently, Christensen (2023) introduced the concept\nof permutation counting, which can accurately estimate MLs of models for\nexchangeable binary responses. Such data arise in a multitude of statistical\nproblems, including binary classification, bioassay and sensitivity testing.\nPermutation counting is entirely likelihood-free and works for any model from\nwhich a random sample can be generated, including nonparametric models. Here we\npresent perms, a package implementing permutation counting. As a result of\nextensive optimisation efforts, perms is computationally efficient and able to\nhandle large data problems. It is available as both an R package and a Python\nlibrary. A broad gallery of examples illustrating its usage is provided, which\nincludes both standard parametric binary classification and novel applications\nof nonparametric models, such as changepoint analysis. We also cover the\ndetails of the implementation of perms and illustrate its computational speed\nvia a simple simulation study."}, "http://arxiv.org/abs/2312.06669": {"title": "An Association Test Based on Kernel-Based Neural Networks for Complex Genetic Association Analysis", "link": "http://arxiv.org/abs/2312.06669", "description": "The advent of artificial intelligence, especially the progress of deep neural\nnetworks, is expected to revolutionize genetic research and offer unprecedented\npotential to decode the complex relationships between genetic variants and\ndisease phenotypes, which could mark a significant step toward improving our\nunderstanding of the disease etiology. While deep neural networks hold great\npromise for genetic association analysis, limited research has been focused on\ndeveloping neural-network-based tests to dissect complex genotype-phenotype\nassociations. This complexity arises from the opaque nature of neural networks\nand the absence of defined limiting distributions. We have previously developed\na kernel-based neural network model (KNN) that synergizes the strengths of\nlinear mixed models with conventional neural networks. KNN adopts a\ncomputationally efficient minimum norm quadratic unbiased estimator (MINQUE)\nalgorithm and uses KNN structure to capture the complex relationship between\nlarge-scale sequencing data and a disease phenotype of interest. In the KNN\nframework, we introduce a MINQUE-based test to assess the joint association of\ngenetic variants with the phenotype, which considers non-linear and\nnon-additive effects and follows a mixture of chi-square distributions. We also\nconstruct two additional tests to evaluate and interpret linear and\nnon-linear/non-additive genetic effects, including interaction effects. Our\nsimulations show that our method consistently controls the type I error rate\nunder various conditions and achieves greater power than a commonly used\nsequence kernel association test (SKAT), especially when involving non-linear\nand interaction effects. When applied to real data from the UK Biobank, our\napproach identified genes associated with hippocampal volume, which can be\nfurther replicated and evaluated for their role in the pathogenesis of\nAlzheimer's disease."}, "http://arxiv.org/abs/2312.06820": {"title": "Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and In-context Learning", "link": "http://arxiv.org/abs/2312.06820", "description": "Microsoft Windows Feedback Hub is designed to receive customer feedback on a\nwide variety of subjects including critical topics such as power and battery.\nFeedback is one of the most effective ways to have a grasp of users' experience\nwith Windows and its ecosystem. However, the sheer volume of feedback received\nby Feedback Hub makes it immensely challenging to diagnose the actual cause of\nreported issues. To better understand and triage issues, we leverage Double\nMachine Learning (DML) to associate users' feedback with telemetry signals. One\nof the main challenges we face in the DML pipeline is the necessity of domain\nknowledge for model design (e.g., causal graph), which sometimes is either not\navailable or hard to obtain. In this work, we take advantage of reasoning\ncapabilities in Large Language Models (LLMs) to generate a prior model that\nwhich to some extent compensates for the lack of domain knowledge and could be\nused as a heuristic for measuring feedback informativeness. Our LLM-based\napproach is able to extract previously known issues, uncover new bugs, and\nidentify sequences of events that lead to a bug, while minimizing out-of-domain\noutputs."}, "http://arxiv.org/abs/2312.06883": {"title": "Adaptive Experiments Toward Learning Treatment Effect Heterogeneity", "link": "http://arxiv.org/abs/2312.06883", "description": "Understanding treatment effect heterogeneity has become an increasingly\npopular task in various fields, as it helps design personalized advertisements\nin e-commerce or targeted treatment in biomedical studies. However, most of the\nexisting work in this research area focused on either analyzing observational\ndata based on strong causal assumptions or conducting post hoc analyses of\nrandomized controlled trial data, and there has been limited effort dedicated\nto the design of randomized experiments specifically for uncovering treatment\neffect heterogeneity. In the manuscript, we develop a framework for designing\nand analyzing response adaptive experiments toward better learning treatment\neffect heterogeneity. Concretely, we provide response adaptive experimental\ndesign frameworks that sequentially revise the data collection mechanism\naccording to the accrued evidence during the experiment. Such design strategies\nallow for the identification of subgroups with the largest treatment effects\nwith enhanced statistical efficiency. The proposed frameworks not only unify\nadaptive enrichment designs and response-adaptive randomization designs but\nalso complement A/B test designs in e-commerce and randomized trial designs in\nclinical settings. We demonstrate the merit of our design with theoretical\njustifications and in simulation studies with synthetic e-commerce and clinical\ntrial data."}, "http://arxiv.org/abs/2312.07175": {"title": "Instrumental Variable Estimation for Causal Inference in Longitudinal Data with Time-Dependent Latent Confounders", "link": "http://arxiv.org/abs/2312.07175", "description": "Causal inference from longitudinal observational data is a challenging\nproblem due to the difficulty in correctly identifying the time-dependent\nconfounders, especially in the presence of latent time-dependent confounders.\nInstrumental variable (IV) is a powerful tool for addressing the latent\nconfounders issue, but the traditional IV technique cannot deal with latent\ntime-dependent confounders in longitudinal studies. In this work, we propose a\nnovel Time-dependent Instrumental Factor Model (TIFM) for time-varying causal\neffect estimation from data with latent time-dependent confounders. At each\ntime-step, the proposed TIFM method employs the Recurrent Neural Network (RNN)\narchitecture to infer latent IV, and then uses the inferred latent IV factor\nfor addressing the confounding bias caused by the latent time-dependent\nconfounders. We provide a theoretical analysis for the proposed TIFM method\nregarding causal effect estimation in longitudinal data. Extensive evaluation\nwith synthetic datasets demonstrates the effectiveness of TIFM in addressing\ncausal effect estimation over time. We further apply TIFM to a climate dataset\nto showcase the potential of the proposed method in tackling real-world\nproblems."}, "http://arxiv.org/abs/2312.07177": {"title": "Fast Meta-Analytic Approximations for Relational Event Models: Applications to Data Streams and Multilevel Data", "link": "http://arxiv.org/abs/2312.07177", "description": "Large relational-event history data stemming from large networks are becoming\nincreasingly available due to recent technological developments (e.g. digital\ncommunication, online databases, etc). This opens many new doors to learning\nabout complex interaction behavior between actors in temporal social networks.\nThe relational event model has become the gold standard for relational event\nhistory analysis. Currently, however, the main bottleneck to fit relational\nevents models is of computational nature in the form of memory storage\nlimitations and computational complexity. Relational event models are therefore\nmainly used for relatively small data sets while larger, more interesting\ndatasets, including multilevel data structures and relational event data\nstreams, cannot be analyzed on standard desktop computers. This paper addresses\nthis problem by developing approximation algorithms based on meta-analysis\nmethods that can fit relational event models significantly faster while\navoiding the computational issues. In particular, meta-analytic approximations\nare proposed for analyzing streams of relational event data and multilevel\nrelational event data and potentially of combinations thereof. The accuracy and\nthe statistical properties of the methods are assessed using numerical\nsimulations. Furthermore, real-world data are used to illustrate the potential\nof the methodology to study social interaction behavior in an organizational\nnetwork and interaction behavior among political actors. The algorithms are\nimplemented in a publicly available R package 'remx'."}, "http://arxiv.org/abs/2312.07262": {"title": "Robust Bayesian graphical modeling using $\\gamma$-divergence", "link": "http://arxiv.org/abs/2312.07262", "description": "Gaussian graphical model is one of the powerful tools to analyze conditional\nindependence between two variables for multivariate Gaussian-distributed\nobservations. When the dimension of data is moderate or high, penalized\nlikelihood methods such as the graphical lasso are useful to detect significant\nconditional independence structures. However, the estimates are affected by\noutliers due to the Gaussian assumption. This paper proposes a novel robust\nposterior distribution for inference of Gaussian graphical models using the\n$\\gamma$-divergence which is one of the robust divergences. In particular, we\nfocus on the Bayesian graphical lasso by assuming the Laplace-type prior for\nelements of the inverse covariance matrix. The proposed posterior distribution\nmatches its maximum a posteriori estimate with the minimum $\\gamma$-divergence\nestimate provided by the frequentist penalized method. We show that the\nproposed method satisfies the posterior robustness which is a kind of measure\nof robustness in the Bayesian analysis. The property means that the information\nof outliers is automatically ignored in the posterior distribution as long as\nthe outliers are extremely large, which also provides theoretical robustness of\npoint estimate for the existing frequentist method. A sufficient condition for\nthe posterior propriety of the proposed posterior distribution is also shown.\nFurthermore, an efficient posterior computation algorithm via the weighted\nBayesian bootstrap method is proposed. The performance of the proposed method\nis illustrated through simulation studies and real data analysis."}, "http://arxiv.org/abs/2312.07320": {"title": "Convergence rates of non-stationary and deep Gaussian process regression", "link": "http://arxiv.org/abs/2312.07320", "description": "The focus of this work is the convergence of non-stationary and deep Gaussian\nprocess regression. More precisely, we follow a Bayesian approach to regression\nor interpolation, where the prior placed on the unknown function $f$ is a\nnon-stationary or deep Gaussian process, and we derive convergence rates of the\nposterior mean to the true function $f$ in terms of the number of observed\ntraining points. In some cases, we also show convergence of the posterior\nvariance to zero. The only assumption imposed on the function $f$ is that it is\nan element of a certain reproducing kernel Hilbert space, which we in\nparticular cases show to be norm-equivalent to a Sobolev space. Our analysis\nincludes the case of estimated hyper-parameters in the covariance kernels\nemployed, both in an empirical Bayes' setting and the particular hierarchical\nsetting constructed through deep Gaussian processes. We consider the settings\nof noise-free or noisy observations on deterministic or random training points.\nWe establish general assumptions sufficient for the convergence of deep\nGaussian process regression, along with explicit examples demonstrating the\nfulfilment of these assumptions. Specifically, our examples require that the\nH\\\"older or Sobolev norms of the penultimate layer are bounded almost surely."}, "http://arxiv.org/abs/2312.07479": {"title": "Convex Parameter Estimation of Perturbed Multivariate Generalized Gaussian Distributions", "link": "http://arxiv.org/abs/2312.07479", "description": "The multivariate generalized Gaussian distribution (MGGD), also known as the\nmultivariate exponential power (MEP) distribution, is widely used in signal and\nimage processing. However, estimating MGGD parameters, which is required in\npractical applications, still faces specific theoretical challenges. In\nparticular, establishing convergence properties for the standard fixed-point\napproach when both the distribution mean and the scatter (or the precision)\nmatrix are unknown is still an open problem. In robust estimation, imposing\nclassical constraints on the precision matrix, such as sparsity, has been\nlimited by the non-convexity of the resulting cost function. This paper tackles\nthese issues from an optimization viewpoint by proposing a convex formulation\nwith well-established convergence properties. We embed our analysis in a noisy\nscenario where robustness is induced by modelling multiplicative perturbations.\nThe resulting framework is flexible as it combines a variety of regularizations\nfor the precision matrix, the mean and model perturbations. This paper presents\nproof of the desired theoretical properties, specifies the conditions\npreserving these properties for different regularization choices and designs a\ngeneral proximal primal-dual optimization strategy. The experiments show a more\naccurate precision and covariance matrix estimation with similar performance\nfor the mean vector parameter compared to Tyler's M-estimator. In a\nhigh-dimensional setting, the proposed method outperforms the classical GLASSO,\none of its robust extensions, and the regularized Tyler's estimator."}, "http://arxiv.org/abs/2312.07520": {"title": "Estimating Counterfactual Matrix Means with Short Panel Data", "link": "http://arxiv.org/abs/2312.07520", "description": "We develop a more flexible approach for identifying and estimating average\ncounterfactual outcomes when several but not all possible outcomes are observed\nfor each unit in a large cross section. Such settings include event studies and\nstudies of outcomes of \"matches\" between agents of two types, e.g. workers and\nfirms or people and places. When outcomes are generated by a factor model that\nallows for low-dimensional unobserved confounders, our method yields\nconsistent, asymptotically normal estimates of counterfactual outcome means\nunder asymptotics that fix the number of outcomes as the cross section grows\nand general outcome missingness patterns, including those not accommodated by\nexisting methods. Our method is also computationally efficient, requiring only\na single eigendecomposition of a particular aggregation of any factor estimates\nconstructed using subsets of units with the same observed outcomes. In a\nsemi-synthetic simulation study based on matched employer-employee data, our\nmethod performs favorably compared to a Two-Way-Fixed-Effects-model-based\nestimator."}, "http://arxiv.org/abs/2205.04324": {"title": "On a wider class of prior distributions for graphical models", "link": "http://arxiv.org/abs/2205.04324", "description": "Gaussian graphical models are useful tools for conditional independence\nstructure inference of multivariate random variables. Unfortunately, Bayesian\ninference of latent graph structures is challenging due to exponential growth\nof $\\mathcal{G}_n$, the set of all graphs in $n$ vertices. One approach that\nhas been proposed to tackle this problem is to limit search to subsets of\n$\\mathcal{G}_n$. In this paper, we study subsets that are vector subspaces with\nthe cycle space $\\mathcal{C}_n$ as main example. We propose a novel prior on\n$\\mathcal{C}_n$ based on linear combinations of cycle basis elements and\npresent its theoretical properties. Using this prior, we implement a Markov\nchain Monte Carlo algorithm, and show that (i) posterior edge inclusion\nestimates computed with our technique are comparable to estimates from the\nstandard technique despite searching a smaller graph space, and (ii) the vector\nspace perspective enables straightforward implementation of MCMC algorithms."}, "http://arxiv.org/abs/2211.16362": {"title": "Score-based calibration testing for multivariate forecast distributions", "link": "http://arxiv.org/abs/2211.16362", "description": "Calibration tests based on the probability integral transform (PIT) are\nroutinely used to assess the quality of univariate distributional forecasts.\nHowever, PIT-based calibration tests for multivariate distributional forecasts\nface various challenges. We propose two new types of tests based on proper\nscoring rules, which overcome these challenges. They arise from a general\nframework for calibration testing in the multivariate case, introduced in this\nwork. The new tests have good size and power properties in simulations and\nsolve various problems of existing tests. We apply the tests to forecast\ndistributions for macroeconomic and financial time series data."}, "http://arxiv.org/abs/2301.10468": {"title": "Model selection-based estimation for generalized additive models using mixtures of g-priors: Towards systematization", "link": "http://arxiv.org/abs/2301.10468", "description": "We consider the estimation of generalized additive models using basis\nexpansions coupled with Bayesian model selection. Although Bayesian model\nselection is an intuitively appealing tool for regression splines, its use has\ntraditionally been limited to Gaussian additive regression because of the\navailability of a tractable form of the marginal model likelihood. We extend\nthe method to encompass the exponential family of distributions using the\nLaplace approximation to the likelihood. Although the approach exhibits success\nwith any Gaussian-type prior distribution, there remains a lack of consensus\nregarding the best prior distribution for nonparametric regression through\nmodel selection. We observe that the classical unit information prior\ndistribution for variable selection may not be well-suited for nonparametric\nregression using basis expansions. Instead, our investigation reveals that\nmixtures of g-priors are more suitable. We consider various mixtures of\ng-priors to evaluate the performance in estimating generalized additive models.\nFurthermore, we conduct a comparative analysis of several priors for knots to\nidentify the most practically effective strategy. Our extensive simulation\nstudies demonstrate the superiority of model selection-based approaches over\nother Bayesian methods."}, "http://arxiv.org/abs/2302.02482": {"title": "Continuously Indexed Graphical Models", "link": "http://arxiv.org/abs/2302.02482", "description": "Let $X = \\{X_{u}\\}_{u \\in U}$ be a real-valued Gaussian process indexed by a\nset $U$. It can be thought of as an undirected graphical model with every\nrandom variable $X_{u}$ serving as a vertex. We characterize this graph in\nterms of the covariance of $X$ through its reproducing kernel property. Unlike\nother characterizations in the literature, our characterization does not\nrestrict the index set $U$ to be finite or countable, and hence can be used to\nmodel the intrinsic dependence structure of stochastic processes in continuous\ntime/space. Consequently, this characterization is not in terms of the zero\nentries of an inverse covariance. This poses novel challenges for the problem\nof recovery of the dependence structure from a sample of independent\nrealizations of $X$, also known as structure estimation. We propose a\nmethodology that circumvents these issues, by targeting the recovery of the\nunderlying graph up to a finite resolution, which can be arbitrarily fine and\nis limited only by the available sample size. The recovery is shown to be\nconsistent so long as the graph is sufficiently regular in an appropriate\nsense. We derive corresponding convergence rates and finite sample guarantees.\nOur methodology is illustrated by means of a simulation study and two data\nanalyses."}, "http://arxiv.org/abs/2305.04634": {"title": "Neural Likelihood Surfaces for Spatial Processes with Computationally Intensive or Intractable Likelihoods", "link": "http://arxiv.org/abs/2305.04634", "description": "In spatial statistics, fast and accurate parameter estimation, coupled with a\nreliable means of uncertainty quantification, can be challenging when fitting a\nspatial process to real-world data because the likelihood function might be\nslow to evaluate or wholly intractable. In this work, we propose using\nconvolutional neural networks to learn the likelihood function of a spatial\nprocess. Through a specifically designed classification task, our neural\nnetwork implicitly learns the likelihood function, even in situations where the\nexact likelihood is not explicitly available. Once trained on the\nclassification task, our neural network is calibrated using Platt scaling which\nimproves the accuracy of the neural likelihood surfaces. To demonstrate our\napproach, we compare neural likelihood surfaces and the resulting maximum\nlikelihood estimates and approximate confidence regions with the equivalent for\nexact or approximate likelihood for two different spatial processes: a Gaussian\nprocess and a Brown-Resnick process which have computationally intensive and\nintractable likelihoods, respectively. We conclude that our method provides\nfast and accurate parameter estimation with a reliable method of uncertainty\nquantification in situations where standard methods are either undesirably slow\nor inaccurate. The method is applicable to any spatial process on a grid from\nwhich fast simulations are available."}, "http://arxiv.org/abs/2308.09112": {"title": "REACT to NHST: Sensible conclusions to meaningful hypotheses", "link": "http://arxiv.org/abs/2308.09112", "description": "While Null Hypothesis Significance Testing (NHST) remains a widely used\nstatistical tool, it suffers from several shortcomings, such as conflating\nstatistical and practical significance, sensitivity to sample size, and the\ninability to distinguish between accepting the null hypothesis and failing to\nreject it. Recent efforts have focused on developing alternatives to NHST to\naddress these issues. Despite these efforts, conventional NHST remains dominant\nin scientific research due to its simplicity and perceived ease of\ninterpretation. Our work presents a novel alternative to NHST that is just as\naccessible and intuitive: REACT. It not only tackles the shortcomings of NHST\nbut also offers additional advantages over existing alternatives. For instance,\nREACT is easily applicable to multiparametric hypotheses and does not require\nstringent significance-level corrections when conducting multiple tests. We\nillustrate the practical utility of REACT through real-world data examples,\nusing criteria aligned with common research practices to distinguish between\nthe absence of evidence and evidence of absence."}, "http://arxiv.org/abs/2309.04926": {"title": "Testing for Stationary or Persistent Coefficient Randomness in Predictive Regressions", "link": "http://arxiv.org/abs/2309.04926", "description": "This study considers tests for coefficient randomness in predictive\nregressions. Our focus is on how tests for coefficient randomness are\ninfluenced by the persistence of random coefficient. We find that when the\nrandom coefficient is stationary, or I(0), Nyblom's (1989) LM test loses its\noptimality (in terms of power), which is established against the alternative of\nintegrated, or I(1), random coefficient. We demonstrate this by constructing\ntests that are more powerful than the LM test when random coefficient is\nstationary, although these tests are dominated in terms of power by the LM test\nwhen random coefficient is integrated. This implies that the best test for\ncoefficient randomness differs from context to context, and practitioners\nshould take into account the persistence of potentially random coefficient and\nchoose from several tests accordingly. We apply tests for coefficient constancy\nto real data. The results mostly reverse the conclusion of an earlier empirical\nstudy."}, "http://arxiv.org/abs/2309.05482": {"title": "A conformal test of linear models via permutation-augmented regressions", "link": "http://arxiv.org/abs/2309.05482", "description": "Permutation tests are widely recognized as robust alternatives to tests based\non normal theory. Random permutation tests have been frequently employed to\nassess the significance of variables in linear models. Despite their widespread\nuse, existing random permutation tests lack finite-sample and assumption-free\nguarantees for controlling type I error in partial correlation tests. To\naddress this ongoing challenge, we have developed a conformal test through\npermutation-augmented regressions, which we refer to as PALMRT. PALMRT not only\nachieves power competitive with conventional methods but also provides reliable\ncontrol of type I errors at no more than $2\\alpha$, given any targeted level\n$\\alpha$, for arbitrary fixed designs and error distributions. We have\nconfirmed this through extensive simulations.\n\nCompared to the cyclic permutation test (CPT) and residual permutation test\n(RPT), which also offer theoretical guarantees, PALMRT does not compromise as\nmuch on power or set stringent requirements on the sample size, making it\nsuitable for diverse biomedical applications. We further illustrate the\ndifferences in a long-Covid study where PALMRT validated key findings\npreviously identified using the t-test after multiple corrections, while both\nCPT and RPT suffered from a drastic loss of power and failed to identify any\ndiscoveries. We endorse PALMRT as a robust and practical hypothesis test in\nscientific research for its superior error control, power preservation, and\nsimplicity. An R package for PALMRT is available at\n\\url{https://github.com/LeyingGuan/PairedRegression}."}, "http://arxiv.org/abs/2309.13001": {"title": "Joint $p$-Values for Higher-Powered Bayesian Model Checking with Frequentist Guarantees", "link": "http://arxiv.org/abs/2309.13001", "description": "We introduce a joint posterior $p$-value, an extension of the posterior\npredictive $p$-value for multiple test statistics, designed to address\nlimitations of existing Bayesian $p$-values in the setting of continuous model\nexpansion. In particular, we show that the posterior predictive $p$-value, as\nwell as its sampled variant, become more conservative as the parameter\ndimension grows, and we demonstrate the ability of the joint $p$-value to\novercome this problem in cases where we can select test statistics that are\nnegatively associated under the posterior. We validate these conclusions with a\npair of simulation examples in which the joint $p$-value achieves substantial\ngains to power with only a modest increase in computational cost."}, "http://arxiv.org/abs/2312.07610": {"title": "Interpretational errors in statistical causal inference", "link": "http://arxiv.org/abs/2312.07610", "description": "We formalize an interpretational error that is common in statistical causal\ninference, termed identity slippage. This formalism is used to describe\nhistorically-recognized fallacies, and analyse a fast-growing literature in\nstatistics and applied fields. We conducted a systematic review of natural\nlanguage claims in the literature on stochastic mediation parameters, and\ndocumented extensive evidence of identity slippage in applications. This\nframework for error detection is applicable whenever policy decisions depend on\nthe accurate interpretation of statistical results, which is nearly always the\ncase. Therefore, broad awareness of identity slippage will aid statisticians in\nthe successful translation of data into public good."}, "http://arxiv.org/abs/2312.07616": {"title": "Evaluating the Alignment of a Data Analysis between Analyst and Audience", "link": "http://arxiv.org/abs/2312.07616", "description": "A challenge that data analysts face is building a data analysis that is\nuseful for a given consumer. Previously, we defined a set of principles for\ndescribing data analyses that can be used to create a data analysis and to\ncharacterize the variation between analyses. Here, we introduce a concept that\nwe call the alignment of a data analysis between the data analyst and a\nconsumer. We define a successfully aligned data analysis as the matching of\nprinciples between the analyst and the consumer for whom the analysis is\ndeveloped. In this paper, we propose a statistical model for evaluating the\nalignment of a data analysis and describe some of its properties. We argue that\nthis framework provides a language for characterizing alignment and can be used\nas a guide for practicing data scientists and students in data science courses\nfor how to build better data analyses."}, "http://arxiv.org/abs/2312.07619": {"title": "Estimating Causal Impacts of Scaling a Voluntary Policy Intervention", "link": "http://arxiv.org/abs/2312.07619", "description": "Evaluations often inform future program implementation decisions. However,\nthe implementation context may differ, sometimes substantially, from the\nevaluation study context. This difference leads to uncertainty regarding the\nrelevance of evaluation findings to future decisions. Voluntary interventions\npose another challenge to generalizability, as we do not know precisely who\nwill volunteer for the intervention in the future. We present a novel approach\nfor estimating target population average treatment effects among the treated by\ngeneralizing results from an observational study to projected volunteers within\nthe target population (the treated group). Our estimation approach can\naccommodate flexible outcome regression estimators such as Bayesian Additive\nRegression Trees (BART) and Bayesian Causal Forests (BCF). Our generalizability\napproach incorporates uncertainty regarding target population treatment status\ninto the posterior credible intervals to better reflect the uncertainty of\nscaling a voluntary intervention. In a simulation based on real data, we\ndemonstrate that these flexible estimators (BCF and BART) improve performance\nover estimators that rely on parametric regressions. We use our approach to\nestimate impacts of scaling up Comprehensive Primary Care Plus, a health care\npayment model intended to improve quality and efficiency of primary care, and\nwe demonstrate the promise of scaling to a targeted subgroup of practices."}, "http://arxiv.org/abs/2312.07697": {"title": "A Class of Computational Methods to Reduce Selection Bias when Designing Phase 3 Clinical Trials", "link": "http://arxiv.org/abs/2312.07697", "description": "When designing confirmatory Phase 3 studies, one usually evaluates one or\nmore efficacious and safe treatment option(s) based on data from previous\nstudies. However, several retrospective research articles reported the\nphenomenon of ``diminished treatment effect in Phase 3'' based on many case\nstudies. Even under basic assumptions, it was shown that the commonly used\nestimator could substantially overestimate the efficacy of selected group(s).\nAs alternatives, we propose a class of computational methods to reduce\nestimation bias and mean squared error (MSE) with a broader scope of multiple\ntreatment groups and flexibility to accommodate summary results by group as\ninput. Based on simulation studies and a real data example, we provide\npractical implementation guidance for this class of methods under different\nscenarios. For more complicated problems, our framework can serve as a starting\npoint with additional layers built in. Proposed methods can also be widely\napplied to other selection problems."}, "http://arxiv.org/abs/2312.07704": {"title": "Distribution of the elemental regression weights with t-distributed co-variate measurement errors", "link": "http://arxiv.org/abs/2312.07704", "description": "In this article, a heuristic approach is used to determined the best\napproximate distribution of $\\dfrac{Y_1}{Y_1 + Y_2}$, given that $Y_1,Y_2$ are\nindependent, and each of $Y_1$ and $Y$ is distributed as the\n$\\mathcal{F}$-distribution with common denominator degrees of freedom. The\nproposed approximate distribution is subject to graphical comparisons and\ndistributional tests. The proposed distribution is used to derive the\ndistribution of the elemental regression weight $\\omega_E$, where $E$ is the\nelemental regression set."}, "http://arxiv.org/abs/2312.07727": {"title": "Two-sample inference for sparse functional data", "link": "http://arxiv.org/abs/2312.07727", "description": "We propose a novel test procedure for comparing mean functions across two\ngroups within the reproducing kernel Hilbert space (RKHS) framework. Our\nproposed method is adept at handling sparsely and irregularly sampled\nfunctional data when observation times are random for each subject.\nConventional approaches, that are built upon functional principal components\nanalysis, usually assume homogeneous covariance structure across groups.\nNonetheless, justifying this assumption in real-world scenarios can be\nchallenging. To eliminate the need for a homogeneous covariance structure, we\nfirst develop the functional Bahadur representation for the mean estimator\nunder the RKHS framework; this representation naturally leads to the desirable\npointwise limiting distributions. Moreover, we establish weak convergence for\nthe mean estimator, allowing us to construct a test statistic for the mean\ndifference. Our method is easily implementable and outperforms some\nconventional tests in controlling type I errors across various settings. We\ndemonstrate the finite sample performance of our approach through extensive\nsimulations and two real-world applications."}, "http://arxiv.org/abs/2312.07741": {"title": "Robust Functional Principal Component Analysis for Non-Euclidean Random Objects", "link": "http://arxiv.org/abs/2312.07741", "description": "Functional data analysis offers a diverse toolkit of statistical methods\ntailored for analyzing samples of real-valued random functions. Recently,\nsamples of time-varying random objects, such as time-varying networks, have\nbeen increasingly encountered in modern data analysis. These data structures\nrepresent elements within general metric spaces that lack local or global\nlinear structures, rendering traditional functional data analysis methods\ninapplicable. Moreover, the existing methodology for time-varying random\nobjects does not work well in the presence of outlying objects. In this paper,\nwe propose a robust method for analysing time-varying random objects. Our\nmethod employs pointwise Fr\\'{e}chet medians and then constructs pointwise\ndistance trajectories between the individual time courses and the sample\nFr\\'{e}chet medians. This representation effectively transforms time-varying\nobjects into functional data. A novel robust approach to functional principal\ncomponent analysis based on a Winsorized U-statistic estimator of the\ncovariance structure is introduced. The proposed robust analysis of these\ndistance trajectories is able to identify key features of time-varying objects\nand is useful for downstream analysis. To illustrate the efficacy of our\napproach, numerical studies focusing on dynamic networks are conducted. The\nresults indicate that the proposed method exhibits good all-round performance\nand surpasses the existing approach in terms of robustness, showcasing its\nsuperior performance in handling time-varying objects data."}, "http://arxiv.org/abs/2312.07775": {"title": "On the construction of stationary processes and random fields", "link": "http://arxiv.org/abs/2312.07775", "description": "We propose a new method to construct a stationary process and random field\nwith a given convex, decreasing covariance function and any one-dimensional\nmarginal distribution. The result is a new class of stationary processes and\nrandom fields. The construction method provides a simple, unified approach for\na wide range of covariance functions and any one-dimensional marginal\ndistributions, and it allows a new way to model dependence structures in a\nstationary process/random field as its dependence structure is induced by the\ncorrelation structure of a few disjoint sets in the support set of the marginal\ndistribution."}, "http://arxiv.org/abs/2312.07792": {"title": "Differentially private projection-depth-based medians", "link": "http://arxiv.org/abs/2312.07792", "description": "We develop $(\\epsilon,\\delta)$-differentially private projection-depth-based\nmedians using the propose-test-release (PTR) and exponential mechanisms. Under\ngeneral conditions on the input parameters and the population measure, (e.g. we\ndo not assume any moment bounds), we quantify the probability the test in PTR\nfails, as well as the cost of privacy via finite sample deviation bounds. We\ndemonstrate our main result on the canonical projection-depth-based median. In\nthe Gaussian setting, we show that the resulting deviation bound matches the\nknown lower bound for private Gaussian mean estimation, up to a polynomial\nfunction of the condition number of the covariance matrix. In the Cauchy\nsetting, we show that the ``outlier error amplification'' effect resulting from\nthe heavy tails outweighs the cost of privacy. This result is then verified via\nnumerical simulations. Additionally, we present results on general PTR\nmechanisms and a uniform concentration result on the projected spacings of\norder statistics."}, "http://arxiv.org/abs/2312.07829": {"title": "How to Select Covariates for Imputation-Based Regression Calibration Method -- A Causal Perspective", "link": "http://arxiv.org/abs/2312.07829", "description": "In this paper, we identify the criteria for the selection of the minimal and\nmost efficient covariate adjustment sets for the regression calibration method\ndeveloped by Carroll, Rupert and Stefanski (CRS, 1992), used to correct bias\ndue to continuous exposure measurement error. We utilize directed acyclic\ngraphs to illustrate how subject matter knowledge can aid in the selection of\nsuch adjustment sets. Valid measurement error correction requires the\ncollection of data on any (1) common causes of true exposure and outcome and\n(2) common causes of measurement error and outcome, in both the main study and\nvalidation study. For the CRS regression calibration method to be valid,\nresearchers need to minimally adjust for covariate set (1) in both the\nmeasurement error model (MEM) and the outcome model and adjust for covariate\nset (2) at least in the MEM. In practice, we recommend including the minimal\ncovariate adjustment set in both the MEM and the outcome model. In contrast\nwith the regression calibration method developed by Rosner, Spiegelman and\nWillet, it is valid and more efficient to adjust for correlates of the true\nexposure or of measurement error that are not risk factors in the MEM only\nunder CRS method. We applied the proposed covariate selection approach to the\nHealth Professional Follow-up Study, examining the effect of fiber intake on\ncardiovascular incidence. In this study, we demonstrated potential issues with\na data-driven approach to building the MEM that is agnostic to the structural\nassumptions. We extend the originally proposed estimators to settings where\neffect modification by a covariate is allowed. Finally, we caution against the\nuse of the regression calibration method to calibrate the true nutrition intake\nusing biomarkers."}, "http://arxiv.org/abs/2312.07873": {"title": "Causal Integration of Multiple Cancer Cohorts with High-Dimensional Confounders: Bayesian Propensity Score Estimation", "link": "http://arxiv.org/abs/2312.07873", "description": "Comparative meta-analyses of patient groups by integrating multiple\nobservational studies rely on estimated propensity scores (PSs) to mitigate\nconfounder imbalances. However, PS estimation grapples with the theoretical and\npractical challenges posed by high-dimensional confounders. Motivated by an\nintegrative analysis of breast cancer patients across seven medical centers,\nthis paper tackles the challenges associated with integrating multiple\nobservational datasets and offering nationally interpretable results. The\nproposed inferential technique, called Bayesian Motif Submatrices for\nConfounders (B-MSMC), addresses the curse of dimensionality by a hybrid of\nBayesian and frequentist approaches. B-MSMC uses nonparametric Bayesian\n``Chinese restaurant\" processes to eliminate redundancy in the high-dimensional\nconfounders and discover latent motifs or lower-dimensional structure. With\nthese motifs as potential predictors, standard regression techniques can be\nutilized to accurately infer the PSs and facilitate causal group comparisons.\nSimulations and meta-analysis of the motivating cancer investigation\ndemonstrate the efficacy of our proposal in high-dimensional causal inference\nby integrating multiple observational studies; using different weighting\nmethods, we apply the B-MSMC approach to efficiently address confounding when\nintegrating observational health studies with high-dimensional confounders."}, "http://arxiv.org/abs/2312.07882": {"title": "A non-parametric approach for estimating consumer valuation distributions using second price auctions", "link": "http://arxiv.org/abs/2312.07882", "description": "We focus on online second price auctions, where bids are made sequentially,\nand the winning bidder pays the maximum of the second-highest bid and a seller\nspecified reserve price. For many such auctions, the seller does not see all\nthe bids or the total number of bidders accessing the auction, and only\nobserves the current selling prices throughout the course of the auction. We\ndevelop a novel non-parametric approach to estimate the underlying consumer\nvaluation distribution based on this data. Previous non-parametric approaches\nin the literature only use the final selling price and assume knowledge of the\ntotal number of bidders. The resulting estimate, in particular, can be used by\nthe seller to compute the optimal profit-maximizing price for the product. Our\napproach is free of tuning parameters, and we demonstrate its computational and\nstatistical efficiency in a variety of simulation settings, and also on an Xbox\n7-day auction dataset on eBay."}, "http://arxiv.org/abs/2312.08040": {"title": "Markov's Equality and Post-hoc (Anytime) Valid Inference", "link": "http://arxiv.org/abs/2312.08040", "description": "We present Markov's equality: a tight version of Markov's inequality, that\ndoes not impose further assumptions on the on the random variable. We show that\nthis equality, as well as Markov's inequality and its randomized improvement,\nare directly implied by a set of deterministic inequalities. We apply Markov's\nequality to show that standard tests based on $e$-values and $e$-processes are\npost-hoc (anytime) valid: the tests remain valid, even if the level $\\alpha$ is\nselected after observing the data. In fact, we show that this property\ncharacterizes $e$-values and $e$-processes."}, "http://arxiv.org/abs/2312.08150": {"title": "Active learning with biased non-response to label requests", "link": "http://arxiv.org/abs/2312.08150", "description": "Active learning can improve the efficiency of training prediction models by\nidentifying the most informative new labels to acquire. However, non-response\nto label requests can impact active learning's effectiveness in real-world\ncontexts. We conceptualise this degradation by considering the type of\nnon-response present in the data, demonstrating that biased non-response is\nparticularly detrimental to model performance. We argue that this sort of\nnon-response is particularly likely in contexts where the labelling process, by\nnature, relies on user interactions. To mitigate the impact of biased\nnon-response, we propose a cost-based correction to the sampling strategy--the\nUpper Confidence Bound of the Expected Utility (UCB-EU)--that can, plausibly,\nbe applied to any active learning algorithm. Through experiments, we\ndemonstrate that our method successfully reduces the harm from labelling\nnon-response in many settings. However, we also characterise settings where the\nnon-response bias in the annotations remains detrimental under UCB-EU for\nparticular sampling methods and data generating processes. Finally, we evaluate\nour method on a real-world dataset from e-commerce platform Taobao. We show\nthat UCB-EU yields substantial performance improvements to conversion models\nthat are trained on clicked impressions. Most generally, this research serves\nto both better conceptualise the interplay between types of non-response and\nmodel improvements via active learning, and to provide a practical, easy to\nimplement correction that helps mitigate model degradation."}, "http://arxiv.org/abs/2312.08169": {"title": "Efficiency of Multivariate Tests in Trials in Progressive Supranuclear Palsy", "link": "http://arxiv.org/abs/2312.08169", "description": "Measuring disease progression in clinical trials for testing novel treatments\nfor multifaceted diseases as Progressive Supranuclear Palsy (PSP), remains\nchallenging. In this study we assess a range of statistical approaches to\ncompare outcomes measured by the items of the Progressive Supranuclear Palsy\nRating Scale (PSPRS). We consider several statistical approaches, including sum\nscores, as an FDA-recommended version of the PSPRS, multivariate tests, and\nanalysis approaches based on multiple comparisons of the individual items. We\npropose two novel approaches which measure disease status based on Item\nResponse Theory models. We assess the performance of these tests in an\nextensive simulation study and illustrate their use with a re-analysis of the\nABBV-8E12 clinical trial. Furthermore, we discuss the impact of the\nFDA-recommended scoring of item scores on the power of the statistical tests.\nWe find that classical approaches as the PSPRS sum score demonstrate moderate\nto high power when treatment effects are consistent across the individual\nitems. The tests based on Item Response Theory models yield the highest power\nwhen the simulated data are generated from an IRT model. The multiple testing\nbased approaches have a higher power in settings where the treatment effect is\nlimited to certain domains or items. The FDA-recommended item rescoring tends\nto decrease the simulated power. The study shows that there is no\none-size-fits-all testing procedure for evaluating treatment effects using\nPSPRS items; the optimal method varies based on the specific effect size\npatterns. The efficiency of the PSPRS sum score, while generally robust and\nstraightforward to apply, varies depending on the effect sizes' patterns\nencountered and more powerful alternatives are available in specific settings.\nThese findings can have important implications for the design of future\nclinical trials in PSP."}, "http://arxiv.org/abs/2011.06663": {"title": "Patient Recruitment Using Electronic Health Records Under Selection Bias: a Two-phase Sampling Framework", "link": "http://arxiv.org/abs/2011.06663", "description": "Electronic health records (EHRs) are increasingly recognized as a\ncost-effective resource for patient recruitment in clinical research. However,\nhow to optimally select a cohort from millions of individuals to answer a\nscientific question of interest remains unclear. Consider a study to estimate\nthe mean or mean difference of an expensive outcome. Inexpensive auxiliary\ncovariates predictive of the outcome may often be available in patients' health\nrecords, presenting an opportunity to recruit patients selectively which may\nimprove efficiency in downstream analyses. In this paper, we propose a\ntwo-phase sampling design that leverages available information on auxiliary\ncovariates in EHR data. A key challenge in using EHR data for multi-phase\nsampling is the potential selection bias, because EHR data are not necessarily\nrepresentative of the target population. Extending existing literature on\ntwo-phase sampling design, we derive an optimal two-phase sampling method that\nimproves efficiency over random sampling while accounting for the potential\nselection bias in EHR data. We demonstrate the efficiency gain from our\nsampling design via simulation studies and an application to evaluating the\nprevalence of hypertension among US adults leveraging data from the Michigan\nGenomics Initiative, a longitudinal biorepository in Michigan Medicine."}, "http://arxiv.org/abs/2201.05102": {"title": "Space-time extremes of severe US thunderstorm environments", "link": "http://arxiv.org/abs/2201.05102", "description": "Severe thunderstorms cause substantial economic and human losses in the\nUnited States. Simultaneous high values of convective available potential\nenergy (CAPE) and storm relative helicity (SRH) are favorable to severe\nweather, and both they and the composite variable\n$\\mathrm{PROD}=\\sqrt{\\mathrm{CAPE}} \\times \\mathrm{SRH}$ can be used as\nindicators of severe thunderstorm activity. Their extremal spatial dependence\nexhibits temporal non-stationarity due to seasonality and large-scale\natmospheric signals such as El Ni\\~no-Southern Oscillation (ENSO). In order to\ninvestigate this, we introduce a space-time model based on a max-stable,\nBrown--Resnick, field whose range depends on ENSO and on time through a tensor\nproduct spline. We also propose a max-stability test based on empirical\nlikelihood and the bootstrap. The marginal and dependence parameters must be\nestimated separately owing to the complexity of the model, and we develop a\nbootstrap-based model selection criterion that accounts for the marginal\nuncertainty when choosing the dependence model. In the case study, the\nout-sample performance of our model is good. We find that extremes of PROD,\nCAPE and SRH are generally more localized in summer and, in some regions, less\nlocalized during El Ni\\~no and La Ni\\~na events, and give meteorological\ninterpretations of these phenomena."}, "http://arxiv.org/abs/2202.07277": {"title": "Exploiting deterministic algorithms to perform global sensitivity analysis for continuous-time Markov chain compartmental models with application to epidemiology", "link": "http://arxiv.org/abs/2202.07277", "description": "In this paper, we propose a generic approach to perform global sensitivity\nanalysis (GSA) for compartmental models based on continuous-time Markov chains\n(CTMC). This approach enables a complete GSA for epidemic models, in which not\nonly the effects of uncertain parameters such as epidemic parameters\n(transmission rate, mean sojourn duration in compartments) are quantified, but\nalso those of intrinsic randomness and interactions between the two. The main\nstep in our approach is to build a deterministic representation of the\nunderlying continuous-time Markov chain by controlling the latent variables\nmodeling intrinsic randomness. Then, model output can be written as a\ndeterministic function of both uncertain parameters and controlled latent\nvariables, so that it becomes possible to compute standard variance-based\nsensitivity indices, e.g. the so-called Sobol' indices. However, different\nsimulation algorithms lead to different representations. We exhibit in this\nwork three different representations for CTMC stochastic compartmental models\nand discuss the results obtained by implementing and comparing GSAs based on\neach of these representations on a SARS-CoV-2 epidemic model."}, "http://arxiv.org/abs/2210.08589": {"title": "Anytime-Valid Linear Models and Regression Adjusted Causal Inference in Randomized Experiments", "link": "http://arxiv.org/abs/2210.08589", "description": "Linear regression adjustment is commonly used to analyse randomised\ncontrolled experiments due to its efficiency and robustness against model\nmisspecification. Current testing and interval estimation procedures leverage\nthe asymptotic distribution of such estimators to provide Type-I error and\ncoverage guarantees that hold only at a single sample size. Here, we develop\nthe theory for the anytime-valid analogues of such procedures, enabling linear\nregression adjustment in the sequential analysis of randomised experiments. We\nfirst provide sequential $F$-tests and confidence sequences for the parametric\nlinear model, which provide time-uniform Type-I error and coverage guarantees\nthat hold for all sample sizes. We then relax all linear model parametric\nassumptions in randomised designs and provide nonparametric model-free\nsequential tests and confidence sequences for treatment effects. This formally\nallows experiments to be continuously monitored for significance, stopped\nearly, and safeguards against statistical malpractices in data collection. A\nparticular feature of our results is their simplicity. Our test statistics and\nconfidence sequences all emit closed-form expressions, which are functions of\nstatistics directly available from a standard linear regression table. We\nillustrate our methodology with the sequential analysis of software A/B\nexperiments at Netflix, performing regression adjustment with pre-treatment\noutcomes."}, "http://arxiv.org/abs/2212.02505": {"title": "Shared Differential Clustering across Single-cell RNA Sequencing Datasets with the Hierarchical Dirichlet Process", "link": "http://arxiv.org/abs/2212.02505", "description": "Single-cell RNA sequencing (scRNA-seq) is powerful technology that allows\nresearchers to understand gene expression patterns at the single-cell level.\nHowever, analysing scRNA-seq data is challenging due to issues and biases in\ndata collection. In this work, we construct an integrated Bayesian model that\nsimultaneously addresses normalization, imputation and batch effects and also\nnonparametrically clusters cells into groups across multiple datasets. A Gibbs\nsampler based on a finite-dimensional approximation of the HDP is developed for\nposterior inference."}, "http://arxiv.org/abs/2305.14118": {"title": "Notes on Causation, Comparison, and Regression", "link": "http://arxiv.org/abs/2305.14118", "description": "Comparison and contrast are the basic means to unveil causation and learn\nwhich treatments work. To build good comparison groups, randomized\nexperimentation is key, yet often infeasible. In such non-experimental\nsettings, we illustrate and discuss diagnostics to assess how well the common\nlinear regression approach to causal inference approximates desirable features\nof randomized experiments, such as covariate balance, study representativeness,\ninterpolated estimation, and unweighted analyses. We also discuss alternative\nregression modeling, weighting, and matching approaches and argue they should\nbe given strong consideration in empirical work."}, "http://arxiv.org/abs/2312.08391": {"title": "Performance of capture-recapture population size estimators under covariate information", "link": "http://arxiv.org/abs/2312.08391", "description": "Capture-recapture methods for estimating the total size of elusive\npopulations are widely-used, however, due to the choice of estimator impacting\nupon the results and conclusions made, the question of performance of each\nestimator is raised. Motivated by an application of the estimators which allow\ncovariate information to meta-analytic data focused on the prevalence rate of\ncompleted suicide after bariatric surgery, where studies with no completed\nsuicides did not occur, this paper explores the performance of the estimators\nthrough use of a simulation study. The simulation study addresses the\nperformance of the Horvitz-Thompson, generalised Chao and generalised Zelterman\nestimators, in addition to performance of the analytical approach to variance\ncomputation. Given that the estimators vary in their dependence on\ndistributional assumptions, additional simulations are utilised to address the\nquestion of the impact outliers have on performance and inference."}, "http://arxiv.org/abs/2312.08530": {"title": "Using Model-Assisted Calibration Methods to Improve Efficiency of Regression Analyses with Two-Phase Samples under Complex Survey Designs", "link": "http://arxiv.org/abs/2312.08530", "description": "Two-phase sampling designs are frequently employed in epidemiological studies\nand large-scale health surveys. In such designs, certain variables are\nexclusively collected within a second-phase random subsample of the initial\nfirst-phase sample, often due to factors such as high costs, response burden,\nor constraints on data collection or measurement assessment. Consequently,\nsecond-phase sample estimators can be inefficient due to the diminished sample\nsize. Model-assisted calibration methods have been widely used to improve the\nefficiency of second-phase estimators. However, none of the existing methods\nhave considered the complexities arising from the intricate sample designs\npresent in both first- and second-phase samples in regression analyses. This\npaper proposes to calibrate the sample weights for the second-phase subsample\nto the weighted first-phase sample based on influence functions of regression\ncoefficients for a prediction of the covariate of interest, which can be\ncomputed for the entire first-phase sample. We establish the consistency of the\nproposed calibration estimation and provide variance estimation. Empirical\nevidence underscores the robustness of calibration on influence functions\ncompared to the imputation method, which can be sensitive to misspecified\nprediction models for the variable only collected in the second phase. Examples\nusing data from the National Health and Nutrition Examination Survey are\nprovided."}, "http://arxiv.org/abs/2312.08570": {"title": "(Re-)reading Sklar (1959) -- A personal view on Sklar's theorem", "link": "http://arxiv.org/abs/2312.08570", "description": "Some personal thoughts on Sklar's theorem after reading the original paper\n(Sklar, 1059) in French."}, "http://arxiv.org/abs/2312.08587": {"title": "Bayesian Tensor Modeling for Image-based Classification of Alzheimer's Disease", "link": "http://arxiv.org/abs/2312.08587", "description": "Tensor-based representations are being increasingly used to represent complex\ndata types such as imaging data, due to their appealing properties such as\ndimension reduction and the preservation of spatial information. Recently,\nthere is a growing literature on using Bayesian scalar-on-tensor regression\ntechniques that use tensor-based representations for high-dimensional and\nspatially distributed covariates to predict continuous outcomes. However\nsurprisingly, there is limited development on corresponding Bayesian\nclassification methods relying on tensor-valued covariates. Standard approaches\nthat vectorize the image are not desirable due to the loss of spatial\nstructure, and alternate methods that use extracted features from the image in\nthe predictive model may suffer from information loss. We propose a novel data\naugmentation-based Bayesian classification approach relying on tensor-valued\ncovariates, with a focus on imaging predictors. We propose two data\naugmentation schemes, one resulting in a support vector machine (SVM)\nclassifier, and another yielding a logistic regression classifier. While both\ntypes of classifiers have been proposed independently in literature, our\ncontribution is to extend such existing methodology to accommodate\nhigh-dimensional tensor valued predictors that involve low rank decompositions\nof the coefficient matrix while preserving the spatial information in the\nimage. An efficient Markov chain Monte Carlo (MCMC) algorithm is developed for\nimplementing these methods. Simulation studies show significant improvements in\nclassification accuracy and parameter estimation compared to routinely used\nclassification methods. We further illustrate our method in a neuroimaging\napplication using cortical thickness MRI data from Alzheimer's Disease\nNeuroimaging Initiative, with results displaying better classification accuracy\nthroughout several classification tasks."}, "http://arxiv.org/abs/2312.08670": {"title": "Temporal-Spatial Entropy Balancing for Causal Continuous Treatment-Effect Estimation", "link": "http://arxiv.org/abs/2312.08670", "description": "In the field of intracity freight transportation, changes in order volume are\nsignificantly influenced by temporal and spatial factors. When building subsidy\nand pricing strategies, predicting the causal effects of these strategies on\norder volume is crucial. In the process of calculating causal effects,\nconfounding variables can have an impact. Traditional methods to control\nconfounding variables handle data from a holistic perspective, which cannot\nensure the precision of causal effects in specific temporal and spatial\ndimensions. However, temporal and spatial dimensions are extremely critical in\nthe logistics field, and this limitation may directly affect the precision of\nsubsidy and pricing strategies. To address these issues, this study proposes a\ntechnique based on flexible temporal-spatial grid partitioning. Furthermore,\nbased on the flexible grid partitioning technique, we further propose a\ncontinuous entropy balancing method in the temporal-spatial domain, which named\nTS-EBCT (Temporal-Spatial Entropy Balancing for Causal Continue Treatments).\nThe method proposed in this paper has been tested on two simulation datasets\nand two real datasets, all of which have achieved excellent performance. In\nfact, after applying the TS-EBCT method to the intracity freight transportation\nfield, the prediction accuracy of the causal effect has been significantly\nimproved. It brings good business benefits to the company's subsidy and pricing\nstrategies."}, "http://arxiv.org/abs/2312.08838": {"title": "Bayesian Fused Lasso Modeling for Binary Data", "link": "http://arxiv.org/abs/2312.08838", "description": "L1-norm regularized logistic regression models are widely used for analyzing\ndata with binary response. In those analyses, fusing regression coefficients is\nuseful for detecting groups of variables. This paper proposes a binomial\nlogistic regression model with Bayesian fused lasso. Assuming a Laplace prior\non regression coefficients and differences between adjacent regression\ncoefficients enables us to perform variable selection and variable fusion\nsimultaneously in the Bayesian framework. We also propose assuming a horseshoe\nprior on the differences to improve the flexibility of variable fusion. The\nGibbs sampler is derived to estimate the parameters by a hierarchical\nexpression of priors and a data-augmentation method. Using simulation studies\nand real data analysis, we compare the proposed methods with the existing\nmethod."}, "http://arxiv.org/abs/2206.00560": {"title": "Learning common structures in a collection of networks", "link": "http://arxiv.org/abs/2206.00560", "description": "Let a collection of networks represent interactions within several (social or\necological) systems. We pursue two objectives: identifying similarities in the\ntopological structures that are held in common between the networks and\nclustering the collection into sub-collections of structurally homogeneous\nnetworks. We tackle these two questions with a probabilistic model based\napproach. We propose an extension of the Stochastic Block Model (SBM) adapted\nto the joint modeling of a collection of networks. The networks in the\ncollection are assumed to be independent realizations of SBMs. The common\nconnectivity structure is imposed through the equality of some parameters. The\nmodel parameters are estimated with a variational Expectation-Maximization (EM)\nalgorithm. We derive an ad-hoc penalized likelihood criterion to select the\nnumber of blocks and to assess the adequacy of the consensus found between the\nstructures of the different networks. This same criterion can also be used to\ncluster networks on the basis of their connectivity structure. It thus provides\na partition of the collection into subsets of structurally homogeneous\nnetworks. The relevance of our proposition is assessed on two collections of\necological networks. First, an application to three stream food webs reveals\nthe homogeneity of their structures and the correspondence between groups of\nspecies in different ecosystems playing equivalent ecological roles. Moreover,\nthe joint analysis allows a finer analysis of the structure of smaller\nnetworks. Second, we cluster 67 food webs according to their connectivity\nstructures and demonstrate that five mesoscale structures are sufficient to\ndescribe this collection."}, "http://arxiv.org/abs/2207.08933": {"title": "Change point detection in high dimensional data with U-statistics", "link": "http://arxiv.org/abs/2207.08933", "description": "We consider the problem of detecting distributional changes in a sequence of\nhigh dimensional data. Our approach combines two separate statistics stemming\nfrom $L_p$ norms whose behavior is similar under $H_0$ but potentially\ndifferent under $H_A$, leading to a testing procedure that that is flexible\nagainst a variety of alternatives. We establish the asymptotic distribution of\nour proposed test statistics separately in cases of weakly dependent and\nstrongly dependent coordinates as $\\min\\{N,d\\}\\to\\infty$, where $N$ denotes\nsample size and $d$ is the dimension, and establish consistency of testing and\nestimation procedures in high dimensions under one-change alternative settings.\nComputational studies in single and multiple change point scenarios demonstrate\nour method can outperform other nonparametric approaches in the literature for\ncertain alternatives in high dimensions. We illustrate our approach though an\napplication to Twitter data concerning the mentions of U.S. Governors."}, "http://arxiv.org/abs/2303.04416": {"title": "Inference on Optimal Dynamic Policies via Softmax Approximation", "link": "http://arxiv.org/abs/2303.04416", "description": "Estimating optimal dynamic policies from offline data is a fundamental\nproblem in dynamic decision making. In the context of causal inference, the\nproblem is known as estimating the optimal dynamic treatment regime. Even\nthough there exists a plethora of methods for estimation, constructing\nconfidence intervals for the value of the optimal regime and structural\nparameters associated with it is inherently harder, as it involves non-linear\nand non-differentiable functionals of unknown quantities that need to be\nestimated. Prior work resorted to sub-sample approaches that can deteriorate\nthe quality of the estimate. We show that a simple soft-max approximation to\nthe optimal treatment regime, for an appropriately fast growing temperature\nparameter, can achieve valid inference on the truly optimal regime. We\nillustrate our result for a two-period optimal dynamic regime, though our\napproach should directly extend to the finite horizon case. Our work combines\ntechniques from semi-parametric inference and $g$-estimation, together with an\nappropriate triangular array central limit theorem, as well as a novel analysis\nof the asymptotic influence and asymptotic bias of softmax approximations."}, "http://arxiv.org/abs/2303.17642": {"title": "Change Point Detection on A Separable Model for Dynamic Networks", "link": "http://arxiv.org/abs/2303.17642", "description": "This paper studies the change point detection problem in time series of\nnetworks, with the Separable Temporal Exponential-family Random Graph Model\n(STERGM). We consider a sequence of networks generated from a piecewise\nconstant distribution that is altered at unknown change points in time.\nDetection of the change points can identify the discrepancies in the underlying\ndata generating processes and facilitate downstream dynamic network analysis.\nMoreover, the STERGM that focuses on network statistics is a flexible model to\nfit dynamic networks with both dyadic and temporal dependence. We propose a new\nestimator derived from the Alternating Direction Method of Multipliers (ADMM)\nand the Group Fused Lasso to simultaneously detect multiple time points, where\nthe parameters of STERGM have changed. We also provide a Bayesian information\ncriterion for model selection to assist the detection. Our experiments show\ngood performance of the proposed method on both simulated and real data.\nLastly, we develop an R package CPDstergm to implement our method."}, "http://arxiv.org/abs/2310.01198": {"title": "Likelihood Based Inference for ARMA Models", "link": "http://arxiv.org/abs/2310.01198", "description": "Autoregressive moving average (ARMA) models are frequently used to analyze\ntime series data. Despite the popularity of these models, algorithms for\nfitting ARMA models have weaknesses that are not well known. We provide a\nsummary of parameter estimation via maximum likelihood and discuss common\npitfalls that may lead to sub-optimal parameter estimates. We propose a random\nrestart algorithm for parameter estimation that frequently yields higher\nlikelihoods than traditional maximum likelihood estimation procedures. We then\ninvestigate the parameter uncertainty of maximum likelihood estimates, and\npropose the use of profile confidence intervals as a superior alternative to\nintervals derived from the Fisher's information matrix. Through a series of\nsimulation studies, we demonstrate the efficacy of our proposed algorithm and\nthe improved nominal coverage of profile confidence intervals compared to the\nnormal approximation based on Fisher's Information."}, "http://arxiv.org/abs/2312.09303": {"title": "A Physics Based Surrogate Model in Bayesian Uncertainty Quantification involving Elliptic PDEs", "link": "http://arxiv.org/abs/2312.09303", "description": "The paper addresses Bayesian inferences in inverse problems with uncertainty\nquantification involving a computationally expensive forward map associated\nwith solving a partial differential equations. To mitigate the computational\ncost, the paper proposes a new surrogate model informed by the physics of the\nproblem, specifically when the forward map involves solving a linear elliptic\npartial differential equation. The study establishes the consistency of the\nposterior distribution for this surrogate model and demonstrates its\neffectiveness through numerical examples with synthetic data. The results\nindicate a substantial improvement in computational speed, reducing the\nprocessing time from several months with the exact forward map to a few\nminutes, while maintaining negligible loss of accuracy in the posterior\ndistribution."}, "http://arxiv.org/abs/2312.09356": {"title": "Sparsity meets correlation in Gaussian sequence model", "link": "http://arxiv.org/abs/2312.09356", "description": "We study estimation of an $s$-sparse signal in the $p$-dimensional Gaussian\nsequence model with equicorrelated observations and derive the minimax rate. A\nnew phenomenon emerges from correlation, namely the rate scales with respect to\n$p-2s$ and exhibits a phase transition at $p-2s \\asymp \\sqrt{p}$. Correlation\nis shown to be a blessing provided it is sufficiently strong, and the critical\ncorrelation level exhibits a delicate dependence on the sparsity level. Due to\ncorrelation, the minimax rate is driven by two subproblems: estimation of a\nlinear functional (the average of the signal) and estimation of the signal's\n$(p-1)$-dimensional projection onto the orthogonal subspace. The\nhigh-dimensional projection is estimated via sparse regression and the linear\nfunctional is cast as a robust location estimation problem. Existing robust\nestimators turn out to be suboptimal, and we show a kernel mode estimator with\na widening bandwidth exploits the Gaussian character of the data to achieve the\noptimal estimation rate."}, "http://arxiv.org/abs/2312.09422": {"title": "Joint Alignment of Multivariate Quasi-Periodic Functional Data Using Deep Learning", "link": "http://arxiv.org/abs/2312.09422", "description": "The joint alignment of multivariate functional data plays an important role\nin various fields such as signal processing, neuroscience and medicine,\nincluding the statistical analysis of data from wearable devices. Traditional\nmethods often ignore the phase variability and instead focus on the variability\nin the observed amplitude. We present a novel method for joint alignment of\nmultivariate quasi-periodic functions using deep neural networks, decomposing,\nbut retaining all the information in the data by preserving both phase and\namplitude variability. Our proposed neural network uses a special activation of\nthe output that builds on the unit simplex transformation, and we utilize a\nloss function based on the Fisher-Rao metric to train our model. Furthermore,\nour method is unsupervised and can provide an optimal common template function\nas well as subject-specific templates. We demonstrate our method on two\nsimulated datasets and one real example, comprising data from 12-lead 10s\nelectrocardiogram recordings."}, "http://arxiv.org/abs/2312.09604": {"title": "Inferring Causality from Time Series data based on Structural Causal Model and its application to Neural Connectomics", "link": "http://arxiv.org/abs/2312.09604", "description": "Inferring causation from time series data is of scientific interest in\ndifferent disciplines, particularly in neural connectomics. While different\napproaches exist in the literature with parametric modeling assumptions, we\nfocus on a non-parametric model for time series satisfying a Markovian\nstructural causal model with stationary distribution and without concurrent\neffects. We show that the model structure can be used to its advantage to\nobtain an elegant algorithm for causal inference from time series based on\nconditional dependence tests, coined Causal Inference in Time Series (CITS)\nalgorithm. We describe Pearson's partial correlation and Hilbert-Schmidt\ncriterion as candidates for such conditional dependence tests that can be used\nin CITS for the Gaussian and non-Gaussian settings, respectively. We prove the\nmathematical guarantee of the CITS algorithm in recovering the true causal\ngraph, under standard mixing conditions on the underlying time series. We also\nconduct a comparative evaluation of performance of CITS with other existing\nmethodologies in simulated datasets. We then describe the utlity of the\nmethodology in neural connectomics -- in inferring causal functional\nconnectivity from time series of neural activity, and demonstrate its\napplication to a real neurobiological dataset of electro-physiological\nrecordings from the mouse visual cortex recorded by Neuropixel probes."}, "http://arxiv.org/abs/2312.09607": {"title": "Variational excess risk bound for general state space models", "link": "http://arxiv.org/abs/2312.09607", "description": "In this paper, we consider variational autoencoders (VAE) for general state\nspace models. We consider a backward factorization of the variational\ndistributions to analyze the excess risk associated with VAE. Such backward\nfactorizations were recently proposed to perform online variational learning\nand to obtain upper bounds on the variational estimation error. When\nindependent trajectories of sequences are observed and under strong mixing\nassumptions on the state space model and on the variational distribution, we\nprovide an oracle inequality explicit in the number of samples and in the\nlength of the observation sequences. We then derive consequences of this\ntheoretical result. In particular, when the data distribution is given by a\nstate space model, we provide an upper bound for the Kullback-Leibler\ndivergence between the data distribution and its estimator and between the\nvariational posterior and the estimated state space posterior\ndistributions.Under classical assumptions, we prove that our results can be\napplied to Gaussian backward kernels built with dense and recurrent neural\nnetworks."}, "http://arxiv.org/abs/2312.09633": {"title": "Natural gradient Variational Bayes without matrix inversion", "link": "http://arxiv.org/abs/2312.09633", "description": "This paper presents an approach for efficiently approximating the inverse of\nFisher information, a key component in variational Bayes inference. A notable\naspect of our approach is the avoidance of analytically computing the Fisher\ninformation matrix and its explicit inversion. Instead, we introduce an\niterative procedure for generating a sequence of matrices that converge to the\ninverse of Fisher information. The natural gradient variational Bayes algorithm\nwithout matrix inversion is provably convergent and achieves a convergence rate\nof order O(log s/s), with s the number of iterations. We also obtain a central\nlimit theorem for the iterates. Our algorithm exhibits versatility, making it\napplicable across a diverse array of variational Bayes domains, including\nGaussian approximation and normalizing flow Variational Bayes. We offer a range\nof numerical examples to demonstrate the efficiency and reliability of the\nproposed variational Bayes method."}, "http://arxiv.org/abs/2312.09698": {"title": "Smoothing for age-period-cohort models: a comparison between splines and random process", "link": "http://arxiv.org/abs/2312.09698", "description": "Age-Period-Cohort (APC) models are well used in the context of modelling\nhealth and demographic data to produce smooth estimates of each time trend.\nWhen smoothing in the context of APC models, there are two main schools,\nfrequentist using penalised smoothing splines, and Bayesian using random\nprocesses with little crossover between them. In this article, we clearly lay\nout the theoretical link between the two schools, provide examples using\nsimulated and real data to highlight similarities and difference, and help a\ngeneral APC user understand potentially inaccessible theory from functional\nanalysis. As intuition suggests, both approaches lead to comparable and almost\nidentical in-sample predictions, but random processes within a Bayesian\napproach might be beneficial for out-of-sample prediction as the sources of\nuncertainty are captured in a more complete way."}, "http://arxiv.org/abs/2312.09758": {"title": "Diagnosing and Rectifying Fake OOD Invariance: A Restructured Causal Approach", "link": "http://arxiv.org/abs/2312.09758", "description": "Invariant representation learning (IRL) encourages the prediction from\ninvariant causal features to labels de-confounded from the environments,\nadvancing the technical roadmap of out-of-distribution (OOD) generalization.\nDespite spotlights around, recent theoretical results verified that some causal\nfeatures recovered by IRLs merely pretend domain-invariantly in the training\nenvironments but fail in unseen domains. The \\emph{fake invariance} severely\nendangers OOD generalization since the trustful objective can not be diagnosed\nand existing causal surgeries are invalid to rectify. In this paper, we review\na IRL family (InvRat) under the Partially and Fully Informative Invariant\nFeature Structural Causal Models (PIIF SCM /FIIF SCM) respectively, to certify\ntheir weaknesses in representing fake invariant features, then, unify their\ncausal diagrams to propose ReStructured SCM (RS-SCM). RS-SCM can ideally\nrebuild the spurious and the fake invariant features simultaneously. Given\nthis, we further develop an approach based on conditional mutual information\nwith respect to RS-SCM, then rigorously rectify the spurious and fake invariant\neffects. It can be easily implemented by a small feature selection subnet\nintroduced in the IRL family, which is alternatively optimized to achieve our\ngoal. Experiments verified the superiority of our approach to fight against the\nfake invariant issue across a variety of OOD generalization benchmarks."}, "http://arxiv.org/abs/2312.09777": {"title": "Weyl formula and thermodynamics of geometric flow", "link": "http://arxiv.org/abs/2312.09777", "description": "We study the Weyl formula for the asymptotic number of eigenvalues of the\nLaplace-Beltrami operator with Dirichlet boundary condition on a Riemannian\nmanifold in the context of geometric flows. Assuming the eigenvalues to be the\nenergies of some associated statistical system, we show that geometric flows\nare directly related with the direction of increasing entropy chosen. For a\nclosed Riemannian manifold we obtain a volume preserving flow of geometry being\nequivalent to the increment of Gibbs entropy function derived from the spectrum\nof Laplace-Beltrami operator. Resemblance with Arnowitt, Deser, and Misner\n(ADM) formalism of gravity is also noted by considering open Riemannian\nmanifolds, directly equating the geometric flow parameter and the direction of\nincreasing entropy as time direction."}, "http://arxiv.org/abs/2312.09825": {"title": "Extreme value methods for estimating rare events in Utopia", "link": "http://arxiv.org/abs/2312.09825", "description": "To capture the extremal behaviour of complex environmental phenomena in\npractice, flexible techniques for modelling tail behaviour are required. In\nthis paper, we introduce a variety of such methods, which were used by the\nLancopula Utopiversity team to tackle the data challenge of the 2023 Extreme\nValue Analysis Conference. This data challenge was split into four sections,\nlabelled C1-C4. Challenges C1 and C2 comprise univariate problems, where the\ngoal is to estimate extreme quantiles for a non-stationary time series\nexhibiting several complex features. We propose a flexible modelling technique,\nbased on generalised additive models, with diagnostics indicating generally\ngood performance for the observed data. Challenges C3 and C4 concern\nmultivariate problems where the focus is on estimating joint extremal\nprobabilities. For challenge C3, we propose an extension of available models in\nthe multivariate literature and use this framework to estimate extreme\nprobabilities in the presence of non-stationary dependence. Finally, for\nchallenge C4, which concerns a 50 dimensional random vector, we employ a\nclustering technique to achieve dimension reduction and use a conditional\nmodelling approach to estimate extremal probabilities across independent groups\nof variables."}, "http://arxiv.org/abs/2312.09862": {"title": "Wasserstein-based Minimax Estimation of Dependence in Multivariate Regularly Varying Extremes", "link": "http://arxiv.org/abs/2312.09862", "description": "We study minimax risk bounds for estimators of the spectral measure in\nmultivariate linear factor models, where observations are linear combinations\nof regularly varying latent factors. Non-asymptotic convergence rates are\nderived for the multivariate Peak-over-Threshold estimator in terms of the\n$p$-th order Wasserstein distance, and information-theoretic lower bounds for\nthe minimax risks are established. The convergence rate of the estimator is\nshown to be minimax optimal under a class of Pareto-type models analogous to\nthe standard class used in the setting of one-dimensional observations known as\nthe Hall-Welsh class. When the estimator is minimax inefficient, a novel\ntwo-step estimator is introduced and demonstrated to attain the minimax lower\nbound. Our analysis bridges the gaps in understanding trade-offs between\nestimation bias and variance in multivariate extreme value theory."}, "http://arxiv.org/abs/2312.09884": {"title": "Investigating the heterogeneity of \"study twins\"", "link": "http://arxiv.org/abs/2312.09884", "description": "Meta-analyses are commonly performed based on random-effects models, while in\ncertain cases one might also argue in favour of a common-effect model. One such\ncase may be given by the example of two \"study twins\" that are performed\naccording to a common (or at least very similar) protocol. Here we investigate\nthe particular case of meta-analysis of a pair of studies, e.g. summarizing the\nresults of two confirmatory clinical trials in phase III of a clinical\ndevelopment programme. Thereby, we focus on the question of to what extent\nhomogeneity or heterogeneity may be discernible, and include an empirical\ninvestigation of published (\"twin\") pairs of studies. A pair of estimates from\ntwo studies only provides very little evidence on homogeneity or heterogeneity\nof effects, and ad-hoc decision criteria may often be misleading."}, "http://arxiv.org/abs/2312.09900": {"title": "Integral Fractional Ornstein-Uhlenbeck Process Model for Animal Movement", "link": "http://arxiv.org/abs/2312.09900", "description": "Modeling the trajectories of animals is challenging due to the complexity of\ntheir behaviors, the influence of unpredictable environmental factors,\nindividual variability, and the lack of detailed data on their movements.\nAdditionally, factors such as migration, hunting, reproduction, and social\ninteractions add additional layers of complexity when attempting to accurately\nforecast their movements. In the literature, various models exits that aim to\nstudy animal telemetry, by modeling the velocity of the telemetry, the\ntelemetry itself or both processes jointly through a Markovian process. In this\nwork, we propose to model the velocity of each coordinate axis for animal\ntelemetry data as a fractional Ornstein-Uhlenbeck (fOU) process. Then, the\nintegral fOU process models position data in animal telemetry. Compared to\ntraditional methods, the proposed model is flexible in modeling long-range\nmemory. The Hurst parameter $H \\in (0,1)$ is a crucial parameter in integral\nfOU process, as it determines the degree of dependence or long-range memory.\nThe integral fOU process is nonstationary process. In addition, a higher Hurst\nparameter ($H &gt; 0.5$) indicates a stronger memory, leading to trajectories with\ntransient trends, while a lower Hurst parameter ($H &lt; 0.5$) implies a weaker\nmemory, resulting in trajectories with recurring trends. When H = 0.5, the\nprocess reduces to a standard integral Ornstein-Uhlenbeck process. We develop a\nfast simulation algorithm of telemetry trajectories using an approach via\nfinite-dimensional distributions. We also develop a maximum likelihood method\nfor parameter estimation and its performance is examined by simulation studies.\nFinally, we present a telemetry application of Fin Whales that disperse over\nthe Gulf of Mexico."}, "http://arxiv.org/abs/2312.10002": {"title": "On the Invertibility of Euler Integral Transforms with Hyperplanes and Quadric Hypersurfaces", "link": "http://arxiv.org/abs/2312.10002", "description": "The Euler characteristic transform (ECT) is an integral transform used widely\nin topological data analysis. Previous efforts by Curry et al. and Ghrist et\nal. have independently shown that the ECT is injective on all compact definable\nsets. In this work, we study the invertibility of the ECT on definable sets\nthat aren't necessarily compact, resulting in a complete classification of\nconstructible functions that the Euler characteristic transform is not\ninjective on. We then introduce the quadric Euler characteristic transform\n(QECT) as a natural generalization of the ECT by detecting definable shapes\nwith quadric hypersurfaces rather than hyperplanes. We also discuss some\ncriteria for the invertibility of QECT."}, "http://arxiv.org/abs/2108.01327": {"title": "Distributed Inference for Tail Risk", "link": "http://arxiv.org/abs/2108.01327", "description": "For measuring tail risk with scarce extreme events, extreme value analysis is\noften invoked as the statistical tool to extrapolate to the tail of a\ndistribution. The presence of large datasets benefits tail risk analysis by\nproviding more observations for conducting extreme value analysis. However,\nlarge datasets can be stored distributedly preventing the possibility of\ndirectly analyzing them. In this paper, we introduce a comprehensive set of\ntools for examining the asymptotic behavior of tail empirical and quantile\nprocesses in the setting where data is distributed across multiple sources, for\ninstance, when data are stored on multiple machines. Utilizing these tools, one\ncan establish the oracle property for most distributed estimators in extreme\nvalue statistics in a straightforward way. The main theoretical challenge\narises when the number of machines diverges to infinity. The number of machines\nresembles the role of dimensionality in high dimensional statistics. We provide\nvarious examples to demonstrate the practicality and value of our proposed\ntoolkit."}, "http://arxiv.org/abs/2206.04133": {"title": "Bayesian multivariate logistic regression for superiority and inferiority decision-making under observable treatment heterogeneity", "link": "http://arxiv.org/abs/2206.04133", "description": "The effects of treatments may differ between persons with different\ncharacteristics. Addressing such treatment heterogeneity is crucial to\ninvestigate whether patients with specific characteristics are likely to\nbenefit from a new treatment. The current paper presents a novel Bayesian\nmethod for superiority decision-making in the context of randomized controlled\ntrials with multivariate binary responses and heterogeneous treatment effects.\nThe framework is based on three elements: a) Bayesian multivariate logistic\nregression analysis with a P\\'olya-Gamma expansion; b) a transformation\nprocedure to transfer obtained regression coefficients to a more intuitive\nmultivariate probability scale (i.e., success probabilities and the differences\nbetween them); and c) a compatible decision procedure for treatment comparison\nwith prespecified decision error rates. Procedures for a priori sample size\nestimation under a non-informative prior distribution are included. A numerical\nevaluation demonstrated that decisions based on a priori sample size estimation\nresulted in anticipated error rates among the trial population as well as\nsubpopulations. Further, average and conditional treatment effect parameters\ncould be estimated unbiasedly when the sample was large enough. Illustration\nwith the International Stroke Trial dataset revealed a trend towards\nheterogeneous effects among stroke patients: Something that would have remained\nundetected when analyses were limited to average treatment effects."}, "http://arxiv.org/abs/2210.06448": {"title": "Debiased inference for a covariate-adjusted regression function", "link": "http://arxiv.org/abs/2210.06448", "description": "In this article, we study nonparametric inference for a covariate-adjusted\nregression function. This parameter captures the average association between a\ncontinuous exposure and an outcome after adjusting for other covariates. In\nparticular, under certain causal conditions, this parameter corresponds to the\naverage outcome had all units been assigned to a specific exposure level, known\nas the causal dose-response curve. We propose a debiased local linear estimator\nof the covariate-adjusted regression function, and demonstrate that our\nestimator converges pointwise to a mean-zero normal limit distribution. We use\nthis result to construct asymptotically valid confidence intervals for function\nvalues and differences thereof. In addition, we use approximation results for\nthe distribution of the supremum of an empirical process to construct\nasymptotically valid uniform confidence bands. Our methods do not require\nundersmoothing, permit the use of data-adaptive estimators of nuisance\nfunctions, and our estimator attains the optimal rate of convergence for a\ntwice differentiable function. We illustrate the practical performance of our\nestimator using numerical studies and an analysis of the effect of air\npollution exposure on cardiovascular mortality."}, "http://arxiv.org/abs/2308.12506": {"title": "General Covariance-Based Conditions for Central Limit Theorems with Dependent Triangular Arrays", "link": "http://arxiv.org/abs/2308.12506", "description": "We present a general central limit theorem with simple, easy-to-check\ncovariance-based sufficient conditions for triangular arrays of random vectors\nwhen all variables could be interdependent. The result is constructed from\nStein's method, but the conditions are distinct from related work. We show that\nthese covariance conditions nest standard assumptions studied in the literature\nsuch as $M$-dependence, mixing random fields, non-mixing autoregressive\nprocesses, and dependency graphs, which themselves need not imply each other.\nThis permits researchers to work with high-level but intuitive conditions based\non overall correlation instead of more complicated and restrictive conditions\nsuch as strong mixing in random fields that may not have any obvious\nmicro-foundation. As examples of the implications, we show how the theorem\nimplies asymptotic normality in estimating: treatment effects with spillovers\nin more settings than previously admitted, covariance matrices, processes with\nglobal dependencies such as epidemic spread and information diffusion, and\nspatial process with Mat\\'{e}rn dependencies."}, "http://arxiv.org/abs/2312.10176": {"title": "Spectral estimation for spatial point processes and random fields", "link": "http://arxiv.org/abs/2312.10176", "description": "Spatial data can come in a variety of different forms, but two of the most\ncommon generating models for such observations are random fields and point\nprocesses. Whilst it is known that spectral analysis can unify these two\ndifferent data forms, specific methodology for the related estimation is yet to\nbe developed. In this paper, we solve this problem by extending multitaper\nestimation, to estimate the spectral density matrix function for multivariate\nspatial data, where processes can be any combination of either point processes\nor random fields. We discuss finite sample and asymptotic theory for the\nproposed estimators, as well as specific details on the implementation,\nincluding how to perform estimation on non-rectangular domains and the correct\nimplementation of multitapering for processes sampled in different ways, e.g.\ncontinuously vs on a regular grid."}, "http://arxiv.org/abs/2312.10234": {"title": "Targeted Machine Learning for Average Causal Effect Estimation Using the Front-Door Functional", "link": "http://arxiv.org/abs/2312.10234", "description": "Evaluating the average causal effect (ACE) of a treatment on an outcome often\ninvolves overcoming the challenges posed by confounding factors in\nobservational studies. A traditional approach uses the back-door criterion,\nseeking adjustment sets to block confounding paths between treatment and\noutcome. However, this method struggles with unmeasured confounders. As an\nalternative, the front-door criterion offers a solution, even in the presence\nof unmeasured confounders between treatment and outcome. This method relies on\nidentifying mediators that are not directly affected by these confounders and\nthat completely mediate the treatment's effect. Here, we introduce novel\nestimation strategies for the front-door criterion based on the targeted\nminimum loss-based estimation theory. Our estimators work across diverse\nscenarios, handling binary, continuous, and multivariate mediators. They\nleverage data-adaptive machine learning algorithms, minimizing assumptions and\nensuring key statistical properties like asymptotic linearity,\ndouble-robustness, efficiency, and valid estimates within the target parameter\nspace. We establish conditions under which the nuisance functional estimations\nensure the root n-consistency of ACE estimators. Our numerical experiments show\nthe favorable finite sample performance of the proposed estimators. We\ndemonstrate the applicability of these estimators to analyze the effect of\nearly stage academic performance on future yearly income using data from the\nFinnish Social Science Data Archive."}, "http://arxiv.org/abs/2312.10388": {"title": "The Causal Impact of Credit Lines on Spending Distributions", "link": "http://arxiv.org/abs/2312.10388", "description": "Consumer credit services offered by e-commerce platforms provide customers\nwith convenient loan access during shopping and have the potential to stimulate\nsales. To understand the causal impact of credit lines on spending, previous\nstudies have employed causal estimators, based on direct regression (DR),\ninverse propensity weighting (IPW), and double machine learning (DML) to\nestimate the treatment effect. However, these estimators do not consider the\nnotion that an individual's spending can be understood and represented as a\ndistribution, which captures the range and pattern of amounts spent across\ndifferent orders. By disregarding the outcome as a distribution, valuable\ninsights embedded within the outcome distribution might be overlooked. This\npaper develops a distribution-valued estimator framework that extends existing\nreal-valued DR-, IPW-, and DML-based estimators to distribution-valued\nestimators within Rubin's causal framework. We establish their consistency and\napply them to a real dataset from a large e-commerce platform. Our findings\nreveal that credit lines positively influence spending across all quantiles;\nhowever, as credit lines increase, consumers allocate more to luxuries (higher\nquantiles) than necessities (lower quantiles)."}, "http://arxiv.org/abs/2312.10435": {"title": "Uncertainty Quantification in Heterogeneous Treatment Effect Estimation with Gaussian-Process-Based Partially Linear Model", "link": "http://arxiv.org/abs/2312.10435", "description": "Estimating heterogeneous treatment effects across individuals has attracted\ngrowing attention as a statistical tool for performing critical\ndecision-making. We propose a Bayesian inference framework that quantifies the\nuncertainty in treatment effect estimation to support decision-making in a\nrelatively small sample size setting. Our proposed model places Gaussian\nprocess priors on the nonparametric components of a semiparametric model called\na partially linear model. This model formulation has three advantages. First,\nwe can analytically compute the posterior distribution of a treatment effect\nwithout relying on the computationally demanding posterior approximation.\nSecond, we can guarantee that the posterior distribution concentrates around\nthe true one as the sample size goes to infinity. Third, we can incorporate\nprior knowledge about a treatment effect into the prior distribution, improving\nthe estimation efficiency. Our experimental results show that even in the small\nsample size setting, our method can accurately estimate the heterogeneous\ntreatment effects and effectively quantify its estimation uncertainty."}, "http://arxiv.org/abs/2312.10487": {"title": "The Dynamic Triple Gamma Prior as a Shrinkage Process Prior for Time-Varying Parameter Models", "link": "http://arxiv.org/abs/2312.10487", "description": "Many current approaches to shrinkage within the time-varying parameter\nframework assume that each state is equipped with only one innovation variance\nfor all time points. Sparsity is then induced by shrinking this variance\ntowards zero. We argue that this is not sufficient if the states display large\njumps or structural changes, something which is often the case in time series\nanalysis. To remedy this, we propose the dynamic triple gamma prior, a\nstochastic process that has a well-known triple gamma marginal form, while\nstill allowing for autocorrelation. Crucially, the triple gamma has many\ninteresting limiting and special cases (including the horseshoe shrinkage\nprior) which can also be chosen as the marginal distribution. Not only is the\nmarginal form well understood, we further derive many interesting properties of\nthe dynamic triple gamma, which showcase its dynamic shrinkage characteristics.\nWe develop an efficient Markov chain Monte Carlo algorithm to sample from the\nposterior and demonstrate the performance through sparse covariance modeling\nand forecasting of the returns of the components of the EURO STOXX 50 index."}, "http://arxiv.org/abs/2312.10499": {"title": "Censored extreme value estimation", "link": "http://arxiv.org/abs/2312.10499", "description": "A novel and comprehensive methodology designed to tackle the challenges posed\nby extreme values in the context of random censorship is introduced. The main\nfocus is the analysis of integrals based on the product-limit estimator of\nnormalized top-order statistics, denoted extreme Kaplan--Meier integrals. These\nintegrals allow for transparent derivation of various important asymptotic\ndistributional properties, offering an alternative approach to conventional\nplug-in estimation methods. Notably, this methodology demonstrates robustness\nand wide applicability within the scope of max-domains of attraction. An\nadditional noteworthy by-product is the extension of residual estimation of\nextremes to encompass all max-domains of attraction, which is of independent\ninterest."}, "http://arxiv.org/abs/2312.10541": {"title": "Random Measures, ANOVA Models and Quantifying Uncertainty in Randomized Controlled Trials", "link": "http://arxiv.org/abs/2312.10541", "description": "This short paper introduces a novel approach to global sensitivity analysis,\ngrounded in the variance-covariance structure of random variables derived from\nrandom measures. The proposed methodology facilitates the application of\ninformation-theoretic rules for uncertainty quantification, offering several\nadvantages. Specifically, the approach provides valuable insights into the\ndecomposition of variance within discrete subspaces, similar to the standard\nANOVA analysis. To illustrate this point, the method is applied to datasets\nobtained from the analysis of randomized controlled trials on evaluating the\nefficacy of the COVID-19 vaccine and assessing clinical endpoints in a lung\ncancer study."}, "http://arxiv.org/abs/2312.10548": {"title": "Analysis of composition on the original scale of measurement", "link": "http://arxiv.org/abs/2312.10548", "description": "In current applied research the most-used route to an analysis of composition\nis through log-ratios -- that is, contrasts among log-transformed measurements.\nHere we argue instead for a more direct approach, using a statistical model for\nthe arithmetic mean on the original scale of measurement. Central to the\napproach is a general variance-covariance function, derived by assuming\nmultiplicative measurement error. Quasi-likelihood analysis of logit models for\ncomposition is then a general alternative to the use of multivariate linear\nmodels for log-ratio transformed measurements, and it has important advantages.\nThese include robustness to secondary aspects of model specification, stability\nwhen there are zero-valued or near-zero measurements in the data, and more\ndirect interpretation. The usual efficiency property of quasi-likelihood\nestimation applies even when the error covariance matrix is unspecified. We\nalso indicate how the derived variance-covariance function can be used, instead\nof the variance-covariance matrix of log-ratios, with more general multivariate\nmethods for the analysis of composition. A specific feature is that the notion\nof `null correlation' -- for compositional measurements on their original scale\n-- emerges naturally."}, "http://arxiv.org/abs/2312.10563": {"title": "Mediation Analysis with Mendelian Randomization and Efficient Multiple GWAS Integration", "link": "http://arxiv.org/abs/2312.10563", "description": "Mediation analysis is a powerful tool for studying causal pathways between\nexposure, mediator, and outcome variables of interest. While classical\nmediation analysis using observational data often requires strong and sometimes\nunrealistic assumptions, such as unconfoundedness, Mendelian Randomization (MR)\navoids unmeasured confounding bias by employing genetic variants as\ninstrumental variables. We develop a novel MR framework for mediation analysis\nwith genome-wide associate study (GWAS) summary data, and provide solid\nstatistical guarantees. Our framework efficiently integrates information stored\nin three independent GWAS summary data and mitigates the commonly encountered\nwinner's curse and measurement error bias (a.k.a. instrument selection and weak\ninstrument bias) in MR. As a result, our framework provides valid statistical\ninference for both direct and mediation effects with enhanced statistical\nefficiency. As part of this endeavor, we also demonstrate that the concept of\nwinner's curse bias in mediation analysis with MR and summary data is more\ncomplex than previously documented in the classical two-sample MR literature,\nrequiring special treatments to address such a bias issue. Through our\ntheoretical investigations, we show that the proposed method delivers\nconsistent and asymptotically normally distributed causal effect estimates. We\nillustrate the finite-sample performance of our approach through simulation\nexperiments and a case study."}, "http://arxiv.org/abs/2312.10569": {"title": "Interpretable Causal Inference for Analyzing Wearable, Sensor, and Distributional Data", "link": "http://arxiv.org/abs/2312.10569", "description": "Many modern causal questions ask how treatments affect complex outcomes that\nare measured using wearable devices and sensors. Current analysis approaches\nrequire summarizing these data into scalar statistics (e.g., the mean), but\nthese summaries can be misleading. For example, disparate distributions can\nhave the same means, variances, and other statistics. Researchers can overcome\nthe loss of information by instead representing the data as distributions. We\ndevelop an interpretable method for distributional data analysis that ensures\ntrustworthy and robust decision-making: Analyzing Distributional Data via\nMatching After Learning to Stretch (ADD MALTS). We (i) provide analytical\nguarantees of the correctness of our estimation strategy, (ii) demonstrate via\nsimulation that ADD MALTS outperforms other distributional data analysis\nmethods at estimating treatment effects, and (iii) illustrate ADD MALTS'\nability to verify whether there is enough cohesion between treatment and\ncontrol units within subpopulations to trustworthily estimate treatment\neffects. We demonstrate ADD MALTS' utility by studying the effectiveness of\ncontinuous glucose monitors in mitigating diabetes risks."}, "http://arxiv.org/abs/2312.10570": {"title": "Adversarially Balanced Representation for Continuous Treatment Effect Estimation", "link": "http://arxiv.org/abs/2312.10570", "description": "Individual treatment effect (ITE) estimation requires adjusting for the\ncovariate shift between populations with different treatments, and deep\nrepresentation learning has shown great promise in learning a balanced\nrepresentation of covariates. However the existing methods mostly consider the\nscenario of binary treatments. In this paper, we consider the more practical\nand challenging scenario in which the treatment is a continuous variable (e.g.\ndosage of a medication), and we address the two main challenges of this setup.\nWe propose the adversarial counterfactual regression network (ACFR) that\nadversarially minimizes the representation imbalance in terms of KL divergence,\nand also maintains the impact of the treatment value on the outcome prediction\nby leveraging an attention mechanism. Theoretically we demonstrate that ACFR\nobjective function is grounded in an upper bound on counterfactual outcome\nprediction error. Our experimental evaluation on semi-synthetic datasets\ndemonstrates the empirical superiority of ACFR over a range of state-of-the-art\nmethods."}, "http://arxiv.org/abs/2312.10573": {"title": "Random Forest Variable Importance-based Selection Algorithm in Class Imbalance Problem", "link": "http://arxiv.org/abs/2312.10573", "description": "Random Forest is a machine learning method that offers many advantages,\nincluding the ability to easily measure variable importance. Class balancing\ntechnique is a well-known solution to deal with class imbalance problem.\nHowever, it has not been actively studied on RF variable importance. In this\npaper, we study the effect of class balancing on RF variable importance. Our\nsimulation results show that over-sampling is effective in correctly measuring\nvariable importance in class imbalanced situations with small sample size,\nwhile under-sampling fails to differentiate important and non-informative\nvariables. We then propose a variable selection algorithm that utilizes RF\nvariable importance and its confidence interval. Through an experimental study\nusing many real and artificial datasets, we demonstrate that our proposed\nalgorithm efficiently selects an optimal feature set, leading to improved\nprediction performance in class imbalance problem."}, "http://arxiv.org/abs/2312.10596": {"title": "A maximin optimal approach for model-free sampling designs in two-phase studies", "link": "http://arxiv.org/abs/2312.10596", "description": "Data collection costs can vary widely across variables in data science tasks.\nTwo-phase designs are often employed to save data collection costs. In\ntwo-phase studies, inexpensive variables are collected for all subjects in the\nfirst phase, and expensive variables are measured for a subset of subjects in\nthe second phase based on a predetermined sampling rule. The estimation\nefficiency under two-phase designs relies heavily on the sampling rule.\nExisting literature primarily focuses on designing sampling rules for\nestimating a scalar parameter in some parametric models or some specific\nestimating problems. However, real-world scenarios are usually model-unknown\nand involve two-phase designs for model-free estimation of a scalar or\nmulti-dimensional parameter. This paper proposes a maximin criterion to design\nan optimal sampling rule based on semiparametric efficiency bounds. The\nproposed method is model-free and applicable to general estimating problems.\nThe resulting sampling rule can minimize the semiparametric efficiency bound\nwhen the parameter is scalar and improve the bound for every component when the\nparameter is multi-dimensional. Simulation studies demonstrate that the\nproposed designs reduce the variance of the resulting estimator in various\nsettings. The implementation of the proposed design is illustrated in a real\ndata analysis."}, "http://arxiv.org/abs/2312.10607": {"title": "Bayesian Model Selection via Mean-Field Variational Approximation", "link": "http://arxiv.org/abs/2312.10607", "description": "This article considers Bayesian model selection via mean-field (MF)\nvariational approximation. Towards this goal, we study the non-asymptotic\nproperties of MF inference under the Bayesian framework that allows latent\nvariables and model mis-specification. Concretely, we show a Bernstein\nvon-Mises (BvM) theorem for the variational distribution from MF under possible\nmodel mis-specification, which implies the distributional convergence of MF\nvariational approximation to a normal distribution centering at the maximal\nlikelihood estimator (within the specified model). Motivated by the BvM\ntheorem, we propose a model selection criterion using the evidence lower bound\n(ELBO), and demonstrate that the model selected by ELBO tends to asymptotically\nagree with the one selected by the commonly used Bayesian information criterion\n(BIC) as sample size tends to infinity. Comparing to BIC, ELBO tends to incur\nsmaller approximation error to the log-marginal likelihood (a.k.a. model\nevidence) due to a better dimension dependence and full incorporation of the\nprior information. Moreover, we show the geometric convergence of the\ncoordinate ascent variational inference (CAVI) algorithm under the parametric\nmodel framework, which provides a practical guidance on how many iterations one\ntypically needs to run when approximating the ELBO. These findings demonstrate\nthat variational inference is capable of providing a computationally efficient\nalternative to conventional approaches in tasks beyond obtaining point\nestimates, which is also empirically demonstrated by our extensive numerical\nexperiments."}, "http://arxiv.org/abs/2312.10618": {"title": "Sparse Learning and Class Probability Estimation with Weighted Support Vector Machines", "link": "http://arxiv.org/abs/2312.10618", "description": "Classification and probability estimation have broad applications in modern\nmachine learning and data science applications, including biology, medicine,\nengineering, and computer science. The recent development of a class of\nweighted Support Vector Machines (wSVMs) has shown great values in robustly\npredicting the class probability and classification for various problems with\nhigh accuracy. The current framework is based on the $\\ell^2$-norm regularized\nbinary wSVMs optimization problem, which only works with dense features and has\npoor performance at sparse features with redundant noise in most real\napplications. The sparse learning process requires a prescreen of the important\nvariables for each binary wSVMs for accurately estimating pairwise conditional\nprobability. In this paper, we proposed novel wSVMs frameworks that incorporate\nautomatic variable selection with accurate probability estimation for sparse\nlearning problems. We developed efficient algorithms for effective variable\nselection for solving either the $\\ell^1$-norm or elastic net regularized\nbinary wSVMs optimization problems. The binary class probability is then\nestimated either by the $\\ell^2$-norm regularized wSVMs framework with selected\nvariables or by elastic net regularized wSVMs directly. The two-step approach\nof $\\ell^1$-norm followed by $\\ell^2$-norm wSVMs show a great advantage in both\nautomatic variable selection and reliable probability estimators with the most\nefficient time. The elastic net regularized wSVMs offer the best performance in\nterms of variable selection and probability estimation with the additional\nadvantage of variable grouping in the compensation of more computation time for\nhigh dimensional problems. The proposed wSVMs-based sparse learning methods\nhave wide applications and can be further extended to $K$-class problems\nthrough ensemble learning."}, "http://arxiv.org/abs/2312.10675": {"title": "Visualization and Assessment of Copula Symmetry", "link": "http://arxiv.org/abs/2312.10675", "description": "Visualization and assessment of copula structures are crucial for accurately\nunderstanding and modeling the dependencies in multivariate data analysis. In\nthis paper, we introduce an innovative method that employs functional boxplots\nand rank-based testing procedures to evaluate copula symmetry. This approach is\nspecifically designed to assess key characteristics such as reflection\nsymmetry, radial symmetry, and joint symmetry. We first construct test\nfunctions for each specific property and then investigate the asymptotic\nproperties of their empirical estimators. We demonstrate that the functional\nboxplot of these sample test functions serves as an informative visualization\ntool of a given copula structure, effectively measuring the departure from zero\nof the test function. Furthermore, we introduce a nonparametric testing\nprocedure to assess the significance of deviations from symmetry, ensuring the\naccuracy and reliability of our visualization method. Through extensive\nsimulation studies involving various copula models, we demonstrate the\neffectiveness of our testing approach. Finally, we apply our visualization and\ntesting techniques to two real-world datasets: a nutritional habits survey with\nfive variables and wind speed data from three locations in Saudi Arabia."}, "http://arxiv.org/abs/2312.10690": {"title": "M-Estimation in Censored Regression Model using Instrumental Variables under Endogeneity", "link": "http://arxiv.org/abs/2312.10690", "description": "We propose and study M-estimation to estimate the parameters in the censored\nregression model in the presence of endogeneity, i.e., the Tobit model. In the\ncourse of this study, we follow two-stage procedures: the first stage consists\nof applying control function procedures to address the issue of endogeneity\nusing instrumental variables, and the second stage applies the M-estimation\ntechnique to estimate the unknown parameters involved in the model. The large\nsample properties of the proposed estimators are derived and analyzed. The\nfinite sample properties of the estimators are studied through Monte Carlo\nsimulation and a real data application related to women's labor force\nparticipation."}, "http://arxiv.org/abs/2312.10695": {"title": "Nonparametric Strategy Test", "link": "http://arxiv.org/abs/2312.10695", "description": "We present a nonparametric statistical test for determining whether an agent\nis following a given mixed strategy in a repeated strategic-form game given\nsamples of the agent's play. This involves two components: determining whether\nthe agent's frequencies of pure strategies are sufficiently close to the target\nfrequencies, and determining whether the pure strategies selected are\nindependent between different game iterations. Our integrated test involves\napplying a chi-squared goodness of fit test for the first component and a\ngeneralized Wald-Wolfowitz runs test for the second component. The results from\nboth tests are combined using Bonferroni correction to produce a complete test\nfor a given significance level $\\alpha.$ We applied the test to publicly\navailable data of human rock-paper-scissors play. The data consists of 50\niterations of play for 500 human players. We test with a null hypothesis that\nthe players are following a uniform random strategy independently at each game\niteration. Using a significance level of $\\alpha = 0.05$, we conclude that 305\n(61%) of the subjects are following the target strategy."}, "http://arxiv.org/abs/2312.10706": {"title": "Margin-closed regime-switching multivariate time series models", "link": "http://arxiv.org/abs/2312.10706", "description": "A regime-switching multivariate time series model which is closed under\nmargins is built. The model imposes a restriction on all lower-dimensional\nsub-processes to follow a regime-switching process sharing the same latent\nregime sequence and having the same Markov order as the original process. The\nmargin-closed regime-switching model is constructed by considering the\nmultivariate margin-closed Gaussian VAR($k$) dependence as a copula within each\nregime, and builds dependence between observations in different regimes by\nrequiring the first observation in the new regime to depend on the last\nobservation in the previous regime. The property of closure under margins\nallows inference on the latent regimes based on lower-dimensional selected\nsub-processes and estimation of univariate parameters from univariate\nsub-processes, and enables the use of multi-stage estimation procedure for the\nmodel. The parsimonious dependence structure of the model also avoids a large\nnumber of parameters under the regime-switching setting. The proposed model is\napplied to a macroeconomic data set to infer the latent business cycle and\ncompared with the relevant benchmark."}, "http://arxiv.org/abs/2312.10796": {"title": "Two sample test for covariance matrices in ultra-high dimension", "link": "http://arxiv.org/abs/2312.10796", "description": "In this paper, we propose a new test for testing the equality of two\npopulation covariance matrices in the ultra-high dimensional setting that the\ndimension is much larger than the sizes of both of the two samples. Our\nproposed methodology relies on a data splitting procedure and a comparison of a\nset of well selected eigenvalues of the sample covariance matrices on the split\ndata sets. Compared to the existing methods, our methodology is adaptive in the\nsense that (i). it does not require specific assumption (e.g., comparable or\nbalancing, etc.) on the sizes of two samples; (ii). it does not need\nquantitative or structural assumptions of the population covariance matrices;\n(iii). it does not need the parametric distributions or the detailed knowledge\nof the moments of the two populations. Theoretically, we establish the\nasymptotic distributions of the statistics used in our method and conduct the\npower analysis. We justify that our method is powerful under very weak\nalternatives. We conduct extensive numerical simulations and show that our\nmethod significantly outperforms the existing ones both in terms of size and\npower. Analysis of two real data sets is also carried out to demonstrate the\nusefulness and superior performance of our proposed methodology. An\n$\\texttt{R}$ package $\\texttt{UHDtst}$ is developed for easy implementation of\nour proposed methodology."}, "http://arxiv.org/abs/2312.10814": {"title": "Scalable Design with Posterior-Based Operating Characteristics", "link": "http://arxiv.org/abs/2312.10814", "description": "To design trustworthy Bayesian studies, criteria for posterior-based\noperating characteristics - such as power and the type I error rate - are often\ndefined in clinical, industrial, and corporate settings. These posterior-based\noperating characteristics are typically assessed by exploring sampling\ndistributions of posterior probabilities via simulation. In this paper, we\npropose a scalable method to determine optimal sample sizes and decision\ncriteria that leverages large-sample theory to explore sampling distributions\nof posterior probabilities in a targeted manner. This targeted exploration\napproach prompts consistent sample size recommendations with fewer simulation\nrepetitions than standard methods. We repurpose the posterior probabilities\ncomputed in that approach to efficiently investigate various sample sizes and\ndecision criteria using contour plots."}, "http://arxiv.org/abs/2312.10894": {"title": "Effectiveness of Constant Stepsize in Markovian LSA and Statistical Inference", "link": "http://arxiv.org/abs/2312.10894", "description": "In this paper, we study the effectiveness of using a constant stepsize in\nstatistical inference via linear stochastic approximation (LSA) algorithms with\nMarkovian data. After establishing a Central Limit Theorem (CLT), we outline an\ninference procedure that uses averaged LSA iterates to construct confidence\nintervals (CIs). Our procedure leverages the fast mixing property of\nconstant-stepsize LSA for better covariance estimation and employs\nRichardson-Romberg (RR) extrapolation to reduce the bias induced by constant\nstepsize and Markovian data. We develop theoretical results for guiding\nstepsize selection in RR extrapolation, and identify several important settings\nwhere the bias provably vanishes even without extrapolation. We conduct\nextensive numerical experiments and compare against classical inference\napproaches. Our results show that using a constant stepsize enjoys easy\nhyperparameter tuning, fast convergence, and consistently better CI coverage,\nespecially when data is limited."}, "http://arxiv.org/abs/2312.10920": {"title": "Domain adaption and physical constrains transfer learning for shale gas production", "link": "http://arxiv.org/abs/2312.10920", "description": "Effective prediction of shale gas production is crucial for strategic\nreservoir development. However, in new shale gas blocks, two main challenges\nare encountered: (1) the occurrence of negative transfer due to insufficient\ndata, and (2) the limited interpretability of deep learning (DL) models. To\ntackle these problems, we propose a novel transfer learning methodology that\nutilizes domain adaptation and physical constraints. This methodology\neffectively employs historical data from the source domain to reduce negative\ntransfer from the data distribution perspective, while also using physical\nconstraints to build a robust and reliable prediction model that integrates\nvarious types of data. The methodology starts by dividing the production data\nfrom the source domain into multiple subdomains, thereby enhancing data\ndiversity. It then uses Maximum Mean Discrepancy (MMD) and global average\ndistance measures to decide on the feasibility of transfer. Through domain\nadaptation, we integrate all transferable knowledge, resulting in a more\ncomprehensive target model. Lastly, by incorporating drilling, completion, and\ngeological data as physical constraints, we develop a hybrid model. This model,\na combination of a multi-layer perceptron (MLP) and a Transformer\n(Transformer-MLP), is designed to maximize interpretability. Experimental\nvalidation in China's southwestern region confirms the method's effectiveness."}, "http://arxiv.org/abs/2312.10926": {"title": "A Random Effects Model-based Method of Moments Estimation of Causal Effect in Mendelian Randomization Studies", "link": "http://arxiv.org/abs/2312.10926", "description": "Recent advances in genotyping technology have delivered a wealth of genetic\ndata, which is rapidly advancing our understanding of the underlying genetic\narchitecture of complex diseases. Mendelian Randomization (MR) leverages such\ngenetic data to estimate the causal effect of an exposure factor on an outcome\nfrom observational studies. In this paper, we utilize genetic correlations to\nsummarize information on a large set of genetic variants associated with the\nexposure factor. Our proposed approach is a generalization of the MR-inverse\nvariance weighting (IVW) approach where we can accommodate many weak and\npleiotropic effects. Our approach quantifies the variation explained by all\nvalid instrumental variables (IVs) instead of estimating the individual effects\nand thus could accommodate weak IVs. This is particularly useful for performing\nMR estimation in small studies, or minority populations where the selection of\nvalid IVs is unreliable and thus has a large influence on the MR estimation.\nThrough simulation and real data analysis, we demonstrate that our approach\nprovides a robust alternative to the existing MR methods. We illustrate the\nrobustness of our proposed approach under the violation of MR assumptions and\ncompare the performance with several existing approaches."}, "http://arxiv.org/abs/2312.10958": {"title": "Large-sample properties of multiple imputation estimators for parameters of logistic regression with covariates missing at random separately or simultaneously", "link": "http://arxiv.org/abs/2312.10958", "description": "We consider logistic regression including two sets of discrete or categorical\ncovariates that are missing at random (MAR) separately or simultaneously. We\nexamine the asymptotic properties of two multiple imputation (MI) estimators,\ngiven in the study of Lee at al. (2023), for the parameters of the logistic\nregression model with both sets of discrete or categorical covariates that are\nMAR separately or simultaneously. The proposed estimated asymptotic variances\nof the two MI estimators address a limitation observed with Rubin's type\nestimated variances, which lead to underestimate the variances of the two MI\nestimators (Rubin, 1987). Simulation results demonstrate that our two proposed\nMI methods outperform the complete-case, semiparametric inverse probability\nweighting, random forest MI using chained equations, and stochastic\napproximation of expectation-maximization methods. To illustrate the\nmethodology's practical application, we provide a real data example from a\nsurvey conducted in the Feng Chia night market in Taichung City, Taiwan."}, "http://arxiv.org/abs/2312.11001": {"title": "A Versatile Causal Discovery Framework to Allow Causally-Related Hidden Variables", "link": "http://arxiv.org/abs/2312.11001", "description": "Most existing causal discovery methods rely on the assumption of no latent\nconfounders, limiting their applicability in solving real-life problems. In\nthis paper, we introduce a novel, versatile framework for causal discovery that\naccommodates the presence of causally-related hidden variables almost\neverywhere in the causal network (for instance, they can be effects of observed\nvariables), based on rank information of covariance matrix over observed\nvariables. We start by investigating the efficacy of rank in comparison to\nconditional independence and, theoretically, establish necessary and sufficient\nconditions for the identifiability of certain latent structural patterns.\nFurthermore, we develop a Rank-based Latent Causal Discovery algorithm, RLCD,\nthat can efficiently locate hidden variables, determine their cardinalities,\nand discover the entire causal structure over both measured and hidden ones. We\nalso show that, under certain graphical conditions, RLCD correctly identifies\nthe Markov Equivalence Class of the whole latent causal graph asymptotically.\nExperimental results on both synthetic and real-world personality data sets\ndemonstrate the efficacy of the proposed approach in finite-sample cases."}, "http://arxiv.org/abs/2312.11054": {"title": "Detection of Model-based Planted Pseudo-cliques in Random Dot Product Graphs by the Adjacency Spectral Embedding and the Graph Encoder Embedding", "link": "http://arxiv.org/abs/2312.11054", "description": "In this paper, we explore the capability of both the Adjacency Spectral\nEmbedding (ASE) and the Graph Encoder Embedding (GEE) for capturing an embedded\npseudo-clique structure in the random dot product graph setting. In both theory\nand experiments, we demonstrate that this pairing of model and methods can\nyield worse results than the best existing spectral clique detection methods,\ndemonstrating at once the methods' potential inability to capture even modestly\nsized pseudo-cliques and the methods' robustness to the model contamination\ngiving rise to the pseudo-clique structure. To further enrich our analysis, we\nalso consider the Variational Graph Auto-Encoder (VGAE) model in our simulation\nand real data experiments."}, "http://arxiv.org/abs/2312.11108": {"title": "Multiple change point detection in functional data with applications to biomechanical fatigue data", "link": "http://arxiv.org/abs/2312.11108", "description": "Injuries to the lower extremity joints are often debilitating, particularly\nfor professional athletes. Understanding the onset of stressful conditions on\nthese joints is therefore important in order to ensure prevention of injuries\nas well as individualised training for enhanced athletic performance. We study\nthe biomechanical joint angles from the hip, knee and ankle for runners who are\nexperiencing fatigue. The data is cyclic in nature and densely collected by\nbody worn sensors, which makes it ideal to work with in the functional data\nanalysis (FDA) framework.\n\nWe develop a new method for multiple change point detection for functional\ndata, which improves the state of the art with respect to at least two novel\naspects. First, the curves are compared with respect to their maximum absolute\ndeviation, which leads to a better interpretation of local changes in the\nfunctional data compared to classical $L^2$-approaches. Secondly, as slight\naberrations are to be often expected in a human movement data, our method will\nnot detect arbitrarily small changes but hunts for relevant changes, where\nmaximum absolute deviation between the curves exceeds a specified threshold,\nsay $\\Delta &gt;0$. We recover multiple changes in a long functional time series\nof biomechanical knee angle data, which are larger than the desired threshold\n$\\Delta$, allowing us to identify changes purely due to fatigue. In this work,\nwe analyse data from both controlled indoor as well as from an uncontrolled\noutdoor (marathon) setting."}, "http://arxiv.org/abs/2312.11136": {"title": "Identification of complier and noncomplier average causal effects in the presence of latent missing-at-random (LMAR) outcomes: a unifying view and choices of assumptions", "link": "http://arxiv.org/abs/2312.11136", "description": "The study of treatment effects is often complicated by noncompliance and\nmissing data. In the one-sided noncompliance setting where of interest are the\ncomplier and noncomplier average causal effects (CACE and NACE), we address\noutcome missingness of the \\textit{latent missing at random} type (LMAR, also\nknown as \\textit{latent ignorability}). That is, conditional on covariates and\ntreatment assigned, the missingness may depend on compliance type. Within the\ninstrumental variable (IV) approach to noncompliance, methods have been\nproposed for handling LMAR outcome that additionally invoke an exclusion\nrestriction type assumption on missingness, but no solution has been proposed\nfor when a non-IV approach is used. This paper focuses on effect identification\nin the presence of LMAR outcome, with a view to flexibly accommodate different\nprincipal identification approaches. We show that under treatment assignment\nignorability and LMAR only, effect nonidentifiability boils down to a set of\ntwo connected mixture equations involving unidentified stratum-specific\nresponse probabilities and outcome means. This clarifies that (except for a\nspecial case) effect identification generally requires two additional\nassumptions: a \\textit{specific missingness mechanism} assumption and a\n\\textit{principal identification} assumption. This provides a template for\nidentifying effects based on separate choices of these assumptions. We consider\na range of specific missingness assumptions, including those that have appeared\nin the literature and some new ones. Incidentally, we find an issue in the\nexisting assumptions, and propose a modification of the assumptions to avoid\nthe issue. Results under different assumptions are illustrated using data from\nthe Baltimore Experience Corps Trial."}, "http://arxiv.org/abs/2312.11137": {"title": "Random multiplication versus random sum: auto-regressive-like models with integer-valued random inputs", "link": "http://arxiv.org/abs/2312.11137", "description": "A common approach to analyze count time series is to fit models based on\nrandom sum operators. As an alternative, this paper introduces time series\nmodels based on a random multiplication operator, which is simply the\nmultiplication of a variable operand by an integer-valued random coefficient,\nwhose mean is the constant operand. Such operation is endowed into\nauto-regressive-like models with integer-valued random inputs, addressed as\nRMINAR. Two special variants are studied, namely the N0-valued random\ncoefficient auto-regressive model and the N0-valued random coefficient\nmultiplicative error model. Furthermore, Z-valued extensions are considered.\nThe dynamic structure of the proposed models is studied in detail. In\nparticular, their corresponding solutions are everywhere strictly stationary\nand ergodic, a fact that is not common neither in the literature on\ninteger-valued time series models nor real-valued random coefficient\nauto-regressive models. Therefore, the parameters of the RMINAR model are\nestimated using a four-stage weighted least squares estimator, with consistency\nand asymptotic normality established everywhere in the parameter space.\nFinally, the new RMINAR models are illustrated with some simulated and\nempirical examples."}, "http://arxiv.org/abs/2312.11178": {"title": "Deinterleaving RADAR emitters with optimal transport distances", "link": "http://arxiv.org/abs/2312.11178", "description": "Detection and identification of emitters provide vital information for\ndefensive strategies in electronic intelligence. Based on a received signal\ncontaining pulses from an unknown number of emitters, this paper introduces an\nunsupervised methodology for deinterleaving RADAR signals based on a\ncombination of clustering algorithms and optimal transport distances. The first\nstep involves separating the pulses with a clustering algorithm under the\nconstraint that the pulses of two different emitters cannot belong to the same\ncluster. Then, as the emitters exhibit complex behavior and can be represented\nby several clusters, we propose a hierarchical clustering algorithm based on an\noptimal transport distance to merge these clusters. A variant is also\ndeveloped, capable of handling more complex signals. Finally, the proposed\nmethodology is evaluated on simulated data provided through a realistic\nsimulator. Results show that the proposed methods are capable of deinterleaving\ncomplex RADAR signals."}, "http://arxiv.org/abs/2312.11319": {"title": "Uncertainty Quantification for Data-Driven Change-Point Learning via Cross-Validation", "link": "http://arxiv.org/abs/2312.11319", "description": "Accurately detecting multiple change-points is critical for various\napplications, but determining the optimal number of change-points remains a\nchallenge. Existing approaches based on information criteria attempt to balance\ngoodness-of-fit and model complexity, but their performance varies depending on\nthe model. Recently, data-driven selection criteria based on cross-validation\nhas been proposed, but these methods can be prone to slight overfitting in\nfinite samples. In this paper, we introduce a method that controls the\nprobability of overestimation and provides uncertainty quantification for\nlearning multiple change-points via cross-validation. We frame this problem as\na sequence of model comparison problems and leverage high-dimensional\ninferential procedures. We demonstrate the effectiveness of our approach\nthrough experiments on finite-sample data, showing superior uncertainty\nquantification for overestimation compared to existing methods. Our approach\nhas broad applicability and can be used in diverse change-point models."}, "http://arxiv.org/abs/2312.11323": {"title": "UniForCE: The Unimodality Forest Method for Clustering and Estimation of the Number of Clusters", "link": "http://arxiv.org/abs/2312.11323", "description": "Estimating the number of clusters k while clustering the data is a\nchallenging task. An incorrect cluster assumption indicates that the number of\nclusters k gets wrongly estimated. Consequently, the model fitting becomes less\nimportant. In this work, we focus on the concept of unimodality and propose a\nflexible cluster definition called locally unimodal cluster. A locally unimodal\ncluster extends for as long as unimodality is locally preserved across pairs of\nsubclusters of the data. Then, we propose the UniForCE method for locally\nunimodal clustering. The method starts with an initial overclustering of the\ndata and relies on the unimodality graph that connects subclusters forming\nunimodal pairs. Such pairs are identified using an appropriate statistical\ntest. UniForCE identifies maximal locally unimodal clusters by computing a\nspanning forest in the unimodality graph. Experimental results on both real and\nsynthetic datasets illustrate that the proposed methodology is particularly\nflexible and robust in discovering regular and highly complex cluster shapes.\nMost importantly, it automatically provides an adequate estimation of the\nnumber of clusters."}, "http://arxiv.org/abs/2312.11393": {"title": "Assessing Estimation Uncertainty under Model Misspecification", "link": "http://arxiv.org/abs/2312.11393", "description": "Model misspecification is ubiquitous in data analysis because the\ndata-generating process is often complex and mathematically intractable.\nTherefore, assessing estimation uncertainty and conducting statistical\ninference under a possibly misspecified working model is unavoidable. In such a\ncase, classical methods such as bootstrap and asymptotic theory-based inference\nfrequently fail since they rely heavily on the model assumptions. In this\narticle, we provide a new bootstrap procedure, termed local residual bootstrap,\nto assess estimation uncertainty under model misspecification for generalized\nlinear models. By resampling the residuals from the neighboring observations,\nwe can approximate the sampling distribution of the statistic of interest\naccurately. Instead of relying on the score equations, the proposed method\ndirectly recreates the response variables so that we can easily conduct\nstandard error estimation, confidence interval construction, hypothesis\ntesting, and model evaluation and selection. It performs similarly to classical\nbootstrap when the model is correctly specified and provides a more accurate\nassessment of uncertainty under model misspecification, offering data analysts\nan easy way to guard against the impact of misspecified models. We establish\ndesirable theoretical properties, such as the bootstrap validity, for the\nproposed method using the surrogate residuals. Numerical results and real data\nanalysis further demonstrate the superiority of the proposed method."}, "http://arxiv.org/abs/2312.11437": {"title": "Clustering Consistency of General Nonparametric Classification Methods in Cognitive Diagnosis", "link": "http://arxiv.org/abs/2312.11437", "description": "Cognitive diagnosis models have been popularly used in fields such as\neducation, psychology, and social sciences. While parametric likelihood\nestimation is a prevailing method for fitting cognitive diagnosis models,\nnonparametric methodologies are attracting increasing attention due to their\nease of implementation and robustness, particularly when sample sizes are\nrelatively small. However, existing clustering consistency results of the\nnonparametric estimation methods often rely on certain restrictive conditions,\nwhich may not be easily satisfied in practice. In this article, the clustering\nconsistency of the general nonparametric classification method is reestablished\nunder weaker and more practical conditions."}, "http://arxiv.org/abs/1811.11603": {"title": "Distribution Regression with Sample Selection, with an Application to Wage Decompositions in the UK", "link": "http://arxiv.org/abs/1811.11603", "description": "We develop a distribution regression model under endogenous sample selection.\nThis model is a semi-parametric generalization of the Heckman selection model.\nIt accommodates much richer effects of the covariates on outcome distribution\nand patterns of heterogeneity in the selection process, and allows for drastic\ndepartures from the Gaussian error structure, while maintaining the same level\ntractability as the classical model. The model applies to continuous, discrete\nand mixed outcomes. We provide identification, estimation, and inference\nmethods, and apply them to obtain wage decomposition for the UK. Here we\ndecompose the difference between the male and female wage distributions into\ncomposition, wage structure, selection structure, and selection sorting\neffects. After controlling for endogenous employment selection, we still find\nsubstantial gender wage gap -- ranging from 21% to 40% throughout the (latent)\noffered wage distribution that is not explained by composition. We also uncover\npositive sorting for single men and negative sorting for married women that\naccounts for a substantive fraction of the gender wage gap at the top of the\ndistribution."}, "http://arxiv.org/abs/2204.10359": {"title": "Boundary Adaptive Local Polynomial Conditional Density Estimators", "link": "http://arxiv.org/abs/2204.10359", "description": "We begin by introducing a class of conditional density estimators based on\nlocal polynomial techniques. The estimators are boundary adaptive and easy to\nimplement. We then study the (pointwise and) uniform statistical properties of\nthe estimators, offering characterizations of both probability concentration\nand distributional approximation. In particular, we establish uniform\nconvergence rates in probability and valid Gaussian distributional\napproximations for the Studentized t-statistic process. We also discuss\nimplementation issues such as consistent estimation of the covariance function\nfor the Gaussian approximation, optimal integrated mean squared error bandwidth\nselection, and valid robust bias-corrected inference. We illustrate the\napplicability of our results by constructing valid confidence bands and\nhypothesis tests for both parametric specification and shape constraints,\nexplicitly characterizing their approximation errors. A companion R software\npackage implementing our main results is provided."}, "http://arxiv.org/abs/2301.03747": {"title": "Semiparametric Regression for Spatial Data via Deep Learning", "link": "http://arxiv.org/abs/2301.03747", "description": "In this work, we propose a deep learning-based method to perform\nsemiparametric regression analysis for spatially dependent data. To be\nspecific, we use a sparsely connected deep neural network with rectified linear\nunit (ReLU) activation function to estimate the unknown regression function\nthat describes the relationship between response and covariates in the presence\nof spatial dependence. Under some mild conditions, the estimator is proven to\nbe consistent, and the rate of convergence is determined by three factors: (1)\nthe architecture of neural network class, (2) the smoothness and (intrinsic)\ndimension of true mean function, and (3) the magnitude of spatial dependence.\nOur method can handle well large data set owing to the stochastic gradient\ndescent optimization algorithm. Simulation studies on synthetic data are\nconducted to assess the finite sample performance, the results of which\nindicate that the proposed method is capable of picking up the intricate\nrelationship between response and covariates. Finally, a real data analysis is\nprovided to demonstrate the validity and effectiveness of the proposed method."}, "http://arxiv.org/abs/2301.10059": {"title": "Oncology clinical trial design planning based on a multistate model that jointly models progression-free and overall survival endpoints", "link": "http://arxiv.org/abs/2301.10059", "description": "When planning an oncology clinical trial, the usual approach is to assume\nproportional hazards and even an exponential distribution for time-to-event\nendpoints. Often, besides the gold-standard endpoint overall survival (OS),\nprogression-free survival (PFS) is considered as a second confirmatory\nendpoint. We use a survival multistate model to jointly model these two\nendpoints and find that neither exponential distribution nor proportional\nhazards will typically hold for both endpoints simultaneously. The multistate\nmodel provides a stochastic process approach to model the dependency of such\nendpoints neither requiring latent failure times nor explicit dependency\nmodelling such as copulae. We use the multistate model framework to simulate\nclinical trials with endpoints OS and PFS and show how design planning\nquestions can be answered using this approach. In particular, non-proportional\nhazards for at least one of the endpoints are naturally modelled as well as\ntheir dependency to improve planning. We consider an oncology trial on\nnon-small-cell lung cancer as a motivating example from which we derive\nrelevant trial design questions. We then illustrate how clinical trial design\ncan be based on simulations from a multistate model. Key applications are\nco-primary endpoints and group-sequential designs. Simulations for these\napplications show that the standard simplifying approach may very well lead to\nunderpowered or overpowered clinical trials. Our approach is quite general and\ncan be extended to more complex trial designs, further endpoints, and other\ntherapeutic areas. An R package is available on CRAN."}, "http://arxiv.org/abs/2302.09694": {"title": "Disentangled Representation for Causal Mediation Analysis", "link": "http://arxiv.org/abs/2302.09694", "description": "Estimating direct and indirect causal effects from observational data is\ncrucial to understanding the causal mechanisms and predicting the behaviour\nunder different interventions. Causal mediation analysis is a method that is\noften used to reveal direct and indirect effects. Deep learning shows promise\nin mediation analysis, but the current methods only assume latent confounders\nthat affect treatment, mediator and outcome simultaneously, and fail to\nidentify different types of latent confounders (e.g., confounders that only\naffect the mediator or outcome). Furthermore, current methods are based on the\nsequential ignorability assumption, which is not feasible for dealing with\nmultiple types of latent confounders. This work aims to circumvent the\nsequential ignorability assumption and applies the piecemeal deconfounding\nassumption as an alternative. We propose the Disentangled Mediation Analysis\nVariational AutoEncoder (DMAVAE), which disentangles the representations of\nlatent confounders into three types to accurately estimate the natural direct\neffect, natural indirect effect and total effect. Experimental results show\nthat the proposed method outperforms existing methods and has strong\ngeneralisation ability. We further apply the method to a real-world dataset to\nshow its potential application."}, "http://arxiv.org/abs/2302.13511": {"title": "Extrapolated cross-validation for randomized ensembles", "link": "http://arxiv.org/abs/2302.13511", "description": "Ensemble methods such as bagging and random forests are ubiquitous in various\nfields, from finance to genomics. Despite their prevalence, the question of the\nefficient tuning of ensemble parameters has received relatively little\nattention. This paper introduces a cross-validation method, ECV (Extrapolated\nCross-Validation), for tuning the ensemble and subsample sizes in randomized\nensembles. Our method builds on two primary ingredients: initial estimators for\nsmall ensemble sizes using out-of-bag errors and a novel risk extrapolation\ntechnique that leverages the structure of prediction risk decomposition. By\nestablishing uniform consistency of our risk extrapolation technique over\nensemble and subsample sizes, we show that ECV yields $\\delta$-optimal (with\nrespect to the oracle-tuned risk) ensembles for squared prediction risk. Our\ntheory accommodates general ensemble predictors, only requires mild moment\nassumptions, and allows for high-dimensional regimes where the feature\ndimension grows with the sample size. As a practical case study, we employ ECV\nto predict surface protein abundances from gene expressions in single-cell\nmultiomics using random forests. In comparison to sample-split cross-validation\nand $K$-fold cross-validation, ECV achieves higher accuracy avoiding sample\nsplitting. At the same time, its computational cost is considerably lower owing\nto the use of the risk extrapolation technique. Additional numerical results\nvalidate the finite-sample accuracy of ECV for several common ensemble\npredictors under a computational constraint on the maximum ensemble size."}, "http://arxiv.org/abs/2304.01944": {"title": "A Statistical Approach to Ecological Modeling by a New Similarity Index", "link": "http://arxiv.org/abs/2304.01944", "description": "Similarity index is an important scientific tool frequently used to determine\nwhether different pairs of entities are similar with respect to some prefixed\ncharacteristics. Some standard measures of similarity index include Jaccard\nindex, S{\\o}rensen-Dice index, and Simpson's index. Recently, a better index\n($\\hat{\\alpha}$) for the co-occurrence and/or similarity has been developed,\nand this measure really outperforms and gives theoretically supported\nreasonable predictions. However, the measure $\\hat{\\alpha}$ is not data\ndependent. In this article we propose a new measure of similarity which depends\nstrongly on the data before introducing randomness in prevalence. Then, we\npropose a new method of randomization which changes the whole pattern of\nresults. Before randomization our measure is similar to the Jaccard index,\nwhile after randomization it is close to $\\hat{\\alpha}$. We consider the\npopular ecological dataset from the Tuscan Archipelago, Italy; and compare the\nperformance of the proposed index to other measures. Since our proposed index\nis data dependent, it has some interesting properties which we illustrate in\nthis article through numerical studies."}, "http://arxiv.org/abs/2305.04587": {"title": "Replication of \"null results\" -- Absence of evidence or evidence of absence?", "link": "http://arxiv.org/abs/2305.04587", "description": "In several large-scale replication projects, statistically non-significant\nresults in both the original and the replication study have been interpreted as\na \"replication success\". Here we discuss the logical problems with this\napproach: Non-significance in both studies does not ensure that the studies\nprovide evidence for the absence of an effect and \"replication success\" can\nvirtually always be achieved if the sample sizes are small enough. In addition,\nthe relevant error rates are not controlled. We show how methods, such as\nequivalence testing and Bayes factors, can be used to adequately quantify the\nevidence for the absence of an effect and how they can be applied in the\nreplication setting. Using data from the Reproducibility Project: Cancer\nBiology, the Experimental Philosophy Replicability Project, and the\nReproducibility Project: Psychology we illustrate that many original and\nreplication studies with \"null results\" are in fact inconclusive. We conclude\nthat it is important to also replicate studies with statistically\nnon-significant results, but that they should be designed, analyzed, and\ninterpreted appropriately."}, "http://arxiv.org/abs/2306.02235": {"title": "Learning Linear Causal Representations from Interventions under General Nonlinear Mixing", "link": "http://arxiv.org/abs/2306.02235", "description": "We study the problem of learning causal representations from unknown, latent\ninterventions in a general setting, where the latent distribution is Gaussian\nbut the mixing function is completely general. We prove strong identifiability\nresults given unknown single-node interventions, i.e., without having access to\nthe intervention targets. This generalizes prior works which have focused on\nweaker classes, such as linear maps or paired counterfactual data. This is also\nthe first instance of causal identifiability from non-paired interventions for\ndeep neural network embeddings. Our proof relies on carefully uncovering the\nhigh-dimensional geometric structure present in the data distribution after a\nnon-linear density transformation, which we capture by analyzing quadratic\nforms of precision matrices of the latent distributions. Finally, we propose a\ncontrastive algorithm to identify the latent variables in practice and evaluate\nits performance on various tasks."}, "http://arxiv.org/abs/2309.09367": {"title": "ForLion: A New Algorithm for D-optimal Designs under General Parametric Statistical Models with Mixed Factors", "link": "http://arxiv.org/abs/2309.09367", "description": "In this paper, we address the problem of designing an experiment with both\ndiscrete and continuous factors under fairly general parametric statistical\nmodels. We propose a new algorithm, named ForLion, to search for optimal\ndesigns under the D-criterion. The algorithm performs an exhaustive search in a\ndesign space with mixed factors while keeping high efficiency and reducing the\nnumber of distinct experimental settings. Its optimality is guaranteed by the\ngeneral equivalence theorem. We demonstrate its superiority over\nstate-of-the-art design algorithms using real-life experiments under\nmultinomial logistic models (MLM) and generalized linear models (GLM). Our\nsimulation studies show that the ForLion algorithm could reduce the number of\nexperimental settings by 25% or improve the relative efficiency of the designs\nby 17.5% on average. Our algorithm can help the experimenters reduce the time\ncost, the usage of experimental devices, and thus the total cost of their\nexperiments while preserving high efficiencies of the designs."}, "http://arxiv.org/abs/2312.11573": {"title": "Estimation of individual causal effects in network setup for multiple treatments", "link": "http://arxiv.org/abs/2312.11573", "description": "We study the problem of estimation of Individual Treatment Effects (ITE) in\nthe context of multiple treatments and networked observational data. Leveraging\nthe network information, we aim to utilize hidden confounders that may not be\ndirectly accessible in the observed data, thereby enhancing the practical\napplicability of the strong ignorability assumption. To achieve this, we first\nemploy Graph Convolutional Networks (GCN) to learn a shared representation of\nthe confounders. Then, our approach utilizes separate neural networks to infer\npotential outcomes for each treatment. We design a loss function as a weighted\ncombination of two components: representation loss and Mean Squared Error (MSE)\nloss on the factual outcomes. To measure the representation loss, we extend\nexisting metrics such as Wasserstein and Maximum Mean Discrepancy (MMD) from\nthe binary treatment setting to the multiple treatments scenario. To validate\nthe effectiveness of our proposed methodology, we conduct a series of\nexperiments on the benchmark datasets such as BlogCatalog and Flickr. The\nexperimental results consistently demonstrate the superior performance of our\nmodels when compared to baseline methods."}, "http://arxiv.org/abs/2312.11582": {"title": "Shapley-PC: Constraint-based Causal Structure Learning with Shapley Values", "link": "http://arxiv.org/abs/2312.11582", "description": "Causal Structure Learning (CSL), amounting to extracting causal relations\namong the variables in a dataset, is widely perceived as an important step\ntowards robust and transparent models. Constraint-based CSL leverages\nconditional independence tests to perform causal discovery. We propose\nShapley-PC, a novel method to improve constraint-based CSL algorithms by using\nShapley values over the possible conditioning sets to decide which variables\nare responsible for the observed conditional (in)dependences. We prove\nsoundness and asymptotic consistency and demonstrate that it can outperform\nstate-of-the-art constraint-based, search-based and functional causal\nmodel-based methods, according to standard metrics in CSL."}, "http://arxiv.org/abs/2312.11710": {"title": "Real-time monitoring with RCA models", "link": "http://arxiv.org/abs/2312.11710", "description": "We propose a family of weighted statistics based on the CUSUM process of the\nWLS residuals for the online detection of changepoints in a Random Coefficient\nAutoregressive model, using both the standard CUSUM and the Page-CUSUM process.\nWe derive the asymptotics under the null of no changepoint for all possible\nweighing schemes, including the case of the standardised CUSUM, for which we\nderive a Darling-Erdos-type limit theorem; our results guarantee the\nprocedure-wise size control under both an open-ended and a closed-ended\nmonitoring. In addition to considering the standard RCA model with no\ncovariates, we also extend our results to the case of exogenous regressors. Our\nresults can be applied irrespective of (and with no prior knowledge required as\nto) whether the observations are stationary or not, and irrespective of whether\nthey change into a stationary or nonstationary regime. Hence, our methodology\nis particularly suited to detect the onset, or the collapse, of a bubble or an\nepidemic. Our simulations show that our procedures, especially when\nstandardising the CUSUM process, can ensure very good size control and short\ndetection delays. We complement our theory by studying the online detection of\nbreaks in epidemiological and housing prices series."}, "http://arxiv.org/abs/2312.11926": {"title": "Big Learning Expectation Maximization", "link": "http://arxiv.org/abs/2312.11926", "description": "Mixture models serve as one fundamental tool with versatile applications.\nHowever, their training techniques, like the popular Expectation Maximization\n(EM) algorithm, are notoriously sensitive to parameter initialization and often\nsuffer from bad local optima that could be arbitrarily worse than the optimal.\nTo address the long-lasting bad-local-optima challenge, we draw inspiration\nfrom the recent ground-breaking foundation models and propose to leverage their\nunderlying big learning principle to upgrade the EM. Specifically, we present\nthe Big Learning EM (BigLearn-EM), an EM upgrade that simultaneously performs\njoint, marginal, and orthogonally transformed marginal matchings between data\nand model distributions. Through simulated experiments, we empirically show\nthat the BigLearn-EM is capable of delivering the optimal with high\nprobability; comparisons on benchmark clustering datasets further demonstrate\nits effectiveness and advantages over existing techniques. The code is\navailable at\nhttps://github.com/YulaiCong/Big-Learning-Expectation-Maximization."}, "http://arxiv.org/abs/2312.11927": {"title": "Empowering Dual-Level Graph Self-Supervised Pretraining with Motif Discovery", "link": "http://arxiv.org/abs/2312.11927", "description": "While self-supervised graph pretraining techniques have shown promising\nresults in various domains, their application still experiences challenges of\nlimited topology learning, human knowledge dependency, and incompetent\nmulti-level interactions. To address these issues, we propose a novel solution,\nDual-level Graph self-supervised Pretraining with Motif discovery (DGPM), which\nintroduces a unique dual-level pretraining structure that orchestrates\nnode-level and subgraph-level pretext tasks. Unlike prior approaches, DGPM\nautonomously uncovers significant graph motifs through an edge pooling module,\naligning learned motif similarities with graph kernel-based similarities. A\ncross-matching task enables sophisticated node-motif interactions and novel\nrepresentation learning. Extensive experiments on 15 datasets validate DGPM's\neffectiveness and generalizability, outperforming state-of-the-art methods in\nunsupervised representation learning and transfer learning settings. The\nautonomously discovered motifs demonstrate the potential of DGPM to enhance\nrobustness and interpretability."}, "http://arxiv.org/abs/2312.11934": {"title": "Identification of Causal Structure with Latent Variables Based on Higher Order Cumulants", "link": "http://arxiv.org/abs/2312.11934", "description": "Causal discovery with latent variables is a crucial but challenging task.\nDespite the emergence of numerous methods aimed at addressing this challenge,\nthey are not fully identified to the structure that two observed variables are\ninfluenced by one latent variable and there might be a directed edge in\nbetween. Interestingly, we notice that this structure can be identified through\nthe utilization of higher-order cumulants. By leveraging the higher-order\ncumulants of non-Gaussian data, we provide an analytical solution for\nestimating the causal coefficients or their ratios. With the estimated (ratios\nof) causal coefficients, we propose a novel approach to identify the existence\nof a causal edge between two observed variables subject to latent variable\ninfluence. In case when such a causal edge exits, we introduce an asymmetry\ncriterion to determine the causal direction. The experimental results\ndemonstrate the effectiveness of our proposed method."}, "http://arxiv.org/abs/2312.11991": {"title": "Outcomes truncated by death in RCTs: a simulation study on the survivor average causal effect", "link": "http://arxiv.org/abs/2312.11991", "description": "Continuous outcome measurements truncated by death present a challenge for\nthe estimation of unbiased treatment effects in randomized controlled trials\n(RCTs). One way to deal with such situations is to estimate the survivor\naverage causal effect (SACE), but this requires making non-testable\nassumptions. Motivated by an ongoing RCT in very preterm infants with\nintraventricular hemorrhage, we performed a simulation study to compare a SACE\nestimator with complete case analysis (CCA, benchmark for a biased analysis)\nand an analysis after multiple imputation of missing outcomes. We set up 9\nscenarios combining positive, negative and no treatment effect on the outcome\n(cognitive development) and on survival at 2 years of age. Treatment effect\nestimates from all methods were compared in terms of bias, mean squared error\nand coverage with regard to two estimands: the treatment effect on the outcome\nused in the simulation and the SACE, which was derived by simulation of both\npotential outcomes per patient. Despite targeting different estimands\n(principal stratum estimand, hypothetical estimand), the SACE-estimator and\nmultiple imputation gave similar estimates of the treatment effect and\nefficiently reduced the bias compared to CCA. Also, both methods were\nrelatively robust to omission of one covariate in the analysis, and thus\nviolation of relevant assumptions. Although the SACE is not without\ncontroversy, we find it useful if mortality is inherent to the study\npopulation. Some degree of violation of the required assumptions is almost\ncertain, but may be acceptable in practice."}, "http://arxiv.org/abs/2312.12008": {"title": "How to develop, externally validate, and update multinomial prediction models", "link": "http://arxiv.org/abs/2312.12008", "description": "Multinomial prediction models (MPMs) have a range of potential applications\nacross healthcare where the primary outcome of interest has multiple nominal or\nordinal categories. However, the application of MPMs is scarce, which may be\ndue to the added methodological complexities that they bring. This article\nprovides a guide of how to develop, externally validate, and update MPMs. Using\na previously developed and validated MPM for treatment outcomes in rheumatoid\narthritis as an example, we outline guidance and recommendations for producing\na clinical prediction model, using multinomial logistic regression. This\narticle is intended to supplement existing general guidance on prediction model\nresearch. This guide is split into three parts: 1) Outcome definition and\nvariable selection, 2) Model development, and 3) Model evaluation (including\nperformance assessment, internal and external validation, and model\nrecalibration). We outline how to evaluate and interpret the predictive\nperformance of MPMs. R code is provided. We recommend the application of MPMs\nin clinical settings where the prediction of a nominal polytomous outcome is of\ninterest. Future methodological research could focus on MPM-specific\nconsiderations for variable selection and sample size criteria for external\nvalidation."}, "http://arxiv.org/abs/2312.12106": {"title": "Conditional autoregressive models fused with random forests to improve small-area spatial prediction", "link": "http://arxiv.org/abs/2312.12106", "description": "In areal unit data with missing or suppressed data, it desirable to create\nmodels that are able to predict observations that are not available.\nTraditional statistical methods achieve this through Bayesian hierarchical\nmodels that can capture the unexplained residual spatial autocorrelation\nthrough conditional autoregressive (CAR) priors, such that they can make\npredictions at geographically related spatial locations. In contrast, typical\nmachine learning approaches such as random forests ignore this residual\nautocorrelation, and instead base predictions on complex non-linear\nfeature-target relationships. In this paper, we propose CAR-Forest, a novel\nspatial prediction algorithm that combines the best features of both approaches\nby fusing them together. By iteratively refitting a random forest combined with\na Bayesian CAR model in one algorithm, CAR-Forest can incorporate flexible\nfeature-target relationships while still accounting for the residual spatial\nautocorrelation. Our results, based on a Scottish housing price data set, show\nthat CAR-Forest outperforms Bayesian CAR models, random forests, and the\nstate-of-the-art hybrid approach, geographically weighted random forest,\nproviding a state-of-the-art framework for small-area spatial prediction."}, "http://arxiv.org/abs/2312.12149": {"title": "Bayesian and minimax estimators of loss", "link": "http://arxiv.org/abs/2312.12149", "description": "We study the problem of loss estimation that involves for an observable $X\n\\sim f_{\\theta}$ the choice of a first-stage estimator $\\hat{\\gamma}$ of\n$\\gamma(\\theta)$, incurred loss $L=L(\\theta, \\hat{\\gamma})$, and the choice of\na second-stage estimator $\\hat{L}$ of $L$. We consider both: (i) a sequential\nversion where the first-stage estimate and loss are fixed and optimization is\nperformed at the second-stage level, and (ii) a simultaneous version with a\nRukhin-type loss function designed for the evaluation of $(\\hat{\\gamma},\n\\hat{L})$ as an estimator of $(\\gamma, L)$.\n\nWe explore various Bayesian solutions and provide minimax estimators for both\nsituations (i) and (ii). The analysis is carried out for several probability\nmodels, including multivariate normal models $N_d(\\theta, \\sigma^2 I_d)$ with\nboth known and unknown $\\sigma^2$, Gamma, univariate and multivariate Poisson,\nand negative binomial models, and relates to different choices of the\nfirst-stage and second-stage losses. The minimax findings are achieved by\nidentifying least favourable of sequence of priors and depend critically on\nparticular Bayesian solution properties, namely situations where the\nsecond-stage estimator $\\hat{L}(x)$ is constant as a function of $x$."}, "http://arxiv.org/abs/2312.12206": {"title": "Identification of Causal Structure in the Presence of Missing Data with Additive Noise Model", "link": "http://arxiv.org/abs/2312.12206", "description": "Missing data are an unavoidable complication frequently encountered in many\ncausal discovery tasks. When a missing process depends on the missing values\nthemselves (known as self-masking missingness), the recovery of the joint\ndistribution becomes unattainable, and detecting the presence of such\nself-masking missingness remains a perplexing challenge. Consequently, due to\nthe inability to reconstruct the original distribution and to discern the\nunderlying missingness mechanism, simply applying existing causal discovery\nmethods would lead to wrong conclusions. In this work, we found that the recent\nadvances additive noise model has the potential for learning causal structure\nunder the existence of the self-masking missingness. With this observation, we\naim to investigate the identification problem of learning causal structure from\nmissing data under an additive noise model with different missingness\nmechanisms, where the `no self-masking missingness' assumption can be\neliminated appropriately. Specifically, we first elegantly extend the scope of\nidentifiability of causal skeleton to the case with weak self-masking\nmissingness (i.e., no other variable could be the cause of self-masking\nindicators except itself). We further provide the sufficient and necessary\nidentification conditions of the causal direction under additive noise model\nand show that the causal structure can be identified up to an IN-equivalent\npattern. We finally propose a practical algorithm based on the above\ntheoretical results on learning the causal skeleton and causal direction.\nExtensive experiments on synthetic and real data demonstrate the efficiency and\neffectiveness of the proposed algorithms."}, "http://arxiv.org/abs/2312.12287": {"title": "A Criterion for Multivariate Regionalization of Spatial Data", "link": "http://arxiv.org/abs/2312.12287", "description": "The modifiable areal unit problem in geography or the change-of-support (COS)\nproblem in statistics demonstrates that the interpretation of spatial (or\nspatio-temporal) data analysis is affected by the choice of resolutions or\ngeographical units used in the study. The ecological fallacy is one famous\nexample of this phenomenon. Here we investigate the ecological fallacy\nassociated with the COS problem for multivariate spatial data with the goal of\nproviding a data-driven discretization criterion for the domain of interest\nthat minimizes aggregation errors. The discretization is based on a novel\nmultiscale metric, called the Multivariate Criterion for Aggregation Error\n(MVCAGE). Such multi-scale representations of an underlying multivariate\nprocess are often formulated in terms of basis expansions. We show that a\nparticularly useful basis expansion in this context is the multivariate\nKarhunen-Lo`eve expansion (MKLE). We use the MKLE to build the MVCAGE loss\nfunction and use it within the framework of spatial clustering algorithms to\nperform optimal spatial aggregation. We demonstrate the effectiveness of our\napproach through simulation and through regionalization of county-level income\nand hospital quality data over the United States and prediction of ocean color\nin the coastal Gulf of Alaska."}, "http://arxiv.org/abs/2312.12357": {"title": "Modeling non-linear Effects with Neural Networks in Relational Event Models", "link": "http://arxiv.org/abs/2312.12357", "description": "Dynamic networks offer an insight of how relational systems evolve. However,\nmodeling these networks efficiently remains a challenge, primarily due to\ncomputational constraints, especially as the number of observed events grows.\nThis paper addresses this issue by introducing the Deep Relational Event\nAdditive Model (DREAM) as a solution to the computational challenges presented\nby modeling non-linear effects in Relational Event Models (REMs). DREAM relies\non Neural Additive Models to model non-linear effects, allowing each effect to\nbe captured by an independent neural network. By strategically trading\ncomputational complexity for improved memory management and leveraging the\ncomputational capabilities of Graphic Processor Units (GPUs), DREAM efficiently\ncaptures complex non-linear relationships within data. This approach\ndemonstrates the capability of DREAM in modeling dynamic networks and scaling\nto larger networks. Comparisons with traditional REM approaches showcase DREAM\nsuperior computational efficiency. The model potential is further demonstrated\nby an examination of the patent citation network, which contains nearly 8\nmillion nodes and 100 million events."}, "http://arxiv.org/abs/2312.12361": {"title": "Improved multifidelity Monte Carlo estimators based on normalizing flows and dimensionality reduction techniques", "link": "http://arxiv.org/abs/2312.12361", "description": "We study the problem of multifidelity uncertainty propagation for\ncomputationally expensive models. In particular, we consider the general\nsetting where the high-fidelity and low-fidelity models have a dissimilar\nparameterization both in terms of number of random inputs and their probability\ndistributions, which can be either known in closed form or provided through\nsamples. We derive novel multifidelity Monte Carlo estimators which rely on a\nshared subspace between the high-fidelity and low-fidelity models where the\nparameters follow the same probability distribution, i.e., a standard Gaussian.\nWe build the shared space employing normalizing flows to map different\nprobability distributions into a common one, together with linear and nonlinear\ndimensionality reduction techniques, active subspaces and autoencoders,\nrespectively, which capture the subspaces where the models vary the most. We\nthen compose the existing low-fidelity model with these transformations and\nconstruct modified models with an increased correlation with the high-fidelity,\nwhich therefore yield multifidelity Monte Carlo estimators with reduced\nvariance. A series of numerical experiments illustrate the properties and\nadvantages of our approaches."}, "http://arxiv.org/abs/2312.12396": {"title": "A change-point random partition model for large spatio-temporal datasets", "link": "http://arxiv.org/abs/2312.12396", "description": "Spatio-temporal areal data can be seen as a collection of time series which\nare spatially correlated, according to a specific neighboring structure.\nMotivated by a dataset on mobile phone usage in the Metropolitan area of Milan,\nItaly, we propose a semi-parametric hierarchical Bayesian model allowing for\ntime-varying as well as spatial model-based clustering. To accommodate for\nchanging patterns over work hours and weekdays/weekends, we incorporate a\ntemporal change-point component that allows the specification of different\nhierarchical structures across time points. The model features a random\npartition prior that incorporates the desired spatial features and encourages\nco-clustering based on areal proximity. We explore properties of the model by\nway of extensive simulation studies from which we collect valuable information.\nFinally, we discuss the application to the motivating data, where the main goal\nis to spatially cluster population patterns of mobile phone usage."}, "http://arxiv.org/abs/2009.04710": {"title": "Robust Clustering with Normal Mixture Models: A Pseudo $\\beta$-Likelihood Approach", "link": "http://arxiv.org/abs/2009.04710", "description": "As in other estimation scenarios, likelihood based estimation in the normal\nmixture set-up is highly non-robust against model misspecification and presence\nof outliers (apart from being an ill-posed optimization problem). A robust\nalternative to the ordinary likelihood approach for this estimation problem is\nproposed which performs simultaneous estimation and data clustering and leads\nto subsequent anomaly detection. To invoke robustness, the methodology based on\nthe minimization of the density power divergence (or alternatively, the\nmaximization of the $\\beta$-likelihood) is utilized under suitable constraints.\nAn iteratively reweighted least squares approach has been followed in order to\ncompute the proposed estimators for the component means (or equivalently\ncluster centers) and component dispersion matrices which leads to simultaneous\ndata clustering. Some exploratory techniques are also suggested for anomaly\ndetection, a problem of great importance in the domain of statistics and\nmachine learning. The proposed method is validated with simulation studies\nunder different set-ups; it performs competitively or better compared to the\npopular existing methods like K-medoids, TCLUST, trimmed K-means and MCLUST,\nespecially when the mixture components (i.e., the clusters) share regions with\nsignificant overlap or outlying clusters exist with small but non-negligible\nweights (particularly in higher dimensions). Two real datasets are also used to\nillustrate the performance of the newly proposed method in comparison with\nothers along with an application in image processing. The proposed method\ndetects the clusters with lower misclassification rates and successfully points\nout the outlying (anomalous) observations from these datasets."}, "http://arxiv.org/abs/2202.02416": {"title": "Generalized Causal Tree for Uplift Modeling", "link": "http://arxiv.org/abs/2202.02416", "description": "Uplift modeling is crucial in various applications ranging from marketing and\npolicy-making to personalized recommendations. The main objective is to learn\noptimal treatment allocations for a heterogeneous population. A primary line of\nexisting work modifies the loss function of the decision tree algorithm to\nidentify cohorts with heterogeneous treatment effects. Another line of work\nestimates the individual treatment effects separately for the treatment group\nand the control group using off-the-shelf supervised learning algorithms. The\nformer approach that directly models the heterogeneous treatment effect is\nknown to outperform the latter in practice. However, the existing tree-based\nmethods are mostly limited to a single treatment and a single control use case,\nexcept for a handful of extensions to multiple discrete treatments. In this\npaper, we propose a generalization of tree-based approaches to tackle multiple\ndiscrete and continuous-valued treatments. We focus on a generalization of the\nwell-known causal tree algorithm due to its desirable statistical properties,\nbut our generalization technique can be applied to other tree-based approaches\nas well. The efficacy of our proposed method is demonstrated using experiments\nand real data examples."}, "http://arxiv.org/abs/2204.10426": {"title": "Marginal Structural Illness-Death Models for Semi-Competing Risks Data", "link": "http://arxiv.org/abs/2204.10426", "description": "The three state illness death model has been established as a general\napproach for regression analysis of semi competing risks data. For\nobservational data the marginal structural models (MSM) are a useful tool,\nunder the potential outcomes framework to define and estimate parameters with\ncausal interpretations. In this paper we introduce a class of marginal\nstructural illness death models for the analysis of observational semi\ncompeting risks data. We consider two specific such models, the Markov illness\ndeath MSM and the frailty based Markov illness death MSM. For interpretation\npurposes, risk contrasts under the MSMs are defined. Inference under the\nillness death MSM can be carried out using estimating equations with inverse\nprobability weighting, while inference under the frailty based illness death\nMSM requires a weighted EM algorithm. We study the inference procedures under\nboth MSMs using extensive simulations, and apply them to the analysis of mid\nlife alcohol exposure on late life cognitive impairment as well as mortality\nusing the Honolulu Asia Aging Study data set. The R codes developed in this\nwork have been implemented in the R package semicmprskcoxmsm that is publicly\navailable on CRAN."}, "http://arxiv.org/abs/2301.06098": {"title": "A novel method and comparison of methods for constructing Markov bridges", "link": "http://arxiv.org/abs/2301.06098", "description": "In this study, we address the central issue of statistical inference for\nMarkov jump processes using discrete time observations. The primary problem at\nhand is to accurately estimate the infinitesimal generator of a Markov jump\nprocess, a critical task in various applications. To tackle this problem, we\nbegin by reviewing established methods for generating sample paths from a\nMarkov jump process conditioned to endpoints, known as Markov bridges.\nAdditionally, we introduce a novel algorithm grounded in the concept of\ntime-reversal, which serves as our main contribution. Our proposed method is\nthen employed to estimate the infinitesimal generator of a Markov jump process.\nTo achieve this, we use a combination of Markov Chain Monte Carlo techniques\nand the Monte Carlo Expectation-Maximization algorithm. The results obtained\nfrom our approach demonstrate its effectiveness in providing accurate parameter\nestimates. To assess the efficacy of our proposed method, we conduct a\ncomprehensive comparative analysis with existing techniques (Bisection,\nUniformization, Direct, Rejection, and Modified Rejection), taking into\nconsideration both speed and accuracy. Notably, our method stands out as the\nfastest among the alternatives while maintaining high levels of precision."}, "http://arxiv.org/abs/2305.06262": {"title": "Flexible cost-penalized Bayesian model selection: developing inclusion paths with an application to diagnosis of heart disease", "link": "http://arxiv.org/abs/2305.06262", "description": "We propose a Bayesian model selection approach that allows medical\npractitioners to select among predictor variables while taking their respective\ncosts into account. Medical procedures almost always incur costs in time and/or\nmoney. These costs might exceed their usefulness for modeling the outcome of\ninterest. We develop Bayesian model selection that uses flexible model priors\nto penalize costly predictors a priori and select a subset of predictors useful\nrelative to their costs. Our approach (i) gives the practitioner control over\nthe magnitude of cost penalization, (ii) enables the prior to scale well with\nsample size, and (iii) enables the creation of our proposed inclusion path\nvisualization, which can be used to make decisions about individual candidate\npredictors using both probabilistic and visual tools. We demonstrate the\neffectiveness of our inclusion path approach and the importance of being able\nto adjust the magnitude of the prior's cost penalization through a dataset\npertaining to heart disease diagnosis in patients at the Cleveland Clinic\nFoundation, where several candidate predictors with various costs were recorded\nfor patients, and through simulated data."}, "http://arxiv.org/abs/2305.19139": {"title": "Estimating excess mortality in high-income countries during the COVID-19 pandemic", "link": "http://arxiv.org/abs/2305.19139", "description": "Quantifying the number of deaths caused by the COVID-19 crisis has been an\nongoing challenge for scientists, and no golden standard to do so has yet been\nestablished. We propose a principled approach to calculate age-adjusted yearly\nexcess mortality, and apply it to obtain estimates and uncertainty bounds for\n30 countries with publicly available data. The results uncover remarkable\nvariation in pandemic outcomes across different countries. We further compare\nour findings with existing estimates published in other major scientific\noutlets, highlighting the importance of proper age adjustment to obtain\nunbiased figures."}, "http://arxiv.org/abs/2312.12477": {"title": "Survey on Trustworthy Graph Neural Networks: From A Causal Perspective", "link": "http://arxiv.org/abs/2312.12477", "description": "Graph Neural Networks (GNNs) have emerged as powerful representation learning\ntools for capturing complex dependencies within diverse graph-structured data.\nDespite their success in a wide range of graph mining tasks, GNNs have raised\nserious concerns regarding their trustworthiness, including susceptibility to\ndistribution shift, biases towards certain populations, and lack of\nexplainability. Recently, integrating causal learning techniques into GNNs has\nsparked numerous ground-breaking studies since most of the trustworthiness\nissues can be alleviated by capturing the underlying data causality rather than\nsuperficial correlations. In this survey, we provide a comprehensive review of\nrecent research efforts on causality-inspired GNNs. Specifically, we first\npresent the key trustworthy risks of existing GNN models through the lens of\ncausality. Moreover, we introduce a taxonomy of Causality-Inspired GNNs\n(CIGNNs) based on the type of causal learning capability they are equipped\nwith, i.e., causal reasoning and causal representation learning. Besides, we\nsystematically discuss typical methods within each category and demonstrate how\nthey mitigate trustworthiness risks. Finally, we summarize useful resources and\ndiscuss several future directions, hoping to shed light on new research\nopportunities in this emerging field. The representative papers, along with\nopen-source data and codes, are available in\nhttps://github.com/usail-hkust/Causality-Inspired-GNNs."}, "http://arxiv.org/abs/2312.12638": {"title": "Using Exact Tests from Algebraic Statistics in Sparse Multi-way Analyses: An Application to Analyzing Differential Item Functioning", "link": "http://arxiv.org/abs/2312.12638", "description": "Asymptotic goodness-of-fit methods in contingency table analysis can struggle\nwith sparse data, especially in multi-way tables where it can be infeasible to\nmeet sample size requirements for a robust application of distributional\nassumptions. However, algebraic statistics provides exact alternatives to these\nclassical asymptotic methods that remain viable even with sparse data. We apply\nthese methods to a context in psychometrics and education research that leads\nnaturally to multi-way contingency tables: the analysis of differential item\nfunctioning (DIF). We explain concretely how to apply the exact methods of\nalgebraic statistics to DIF analysis using the R package algstat, and we\ncompare their performance to that of classical asymptotic methods."}, "http://arxiv.org/abs/2312.12641": {"title": "Matching via Distance Profiles", "link": "http://arxiv.org/abs/2312.12641", "description": "In this paper, we introduce and study matching methods based on distance\nprofiles. For the matching of point clouds, the proposed method is easily\nimplementable by solving a linear program, circumventing the computational\nobstacles of quadratic matching. Also, we propose and analyze a flexible way to\nexecute location-to-location matching using distance profiles. Moreover, we\nprovide a statistical estimation error analysis in the context of\nlocation-to-location matching using empirical process theory. Furthermore, we\napply our method to a certain model and show its noise stability by\ncharacterizing conditions on the noise level for the matching to be successful.\nLastly, we demonstrate the performance of the proposed method and compare it\nwith some existing methods using synthetic and real data."}, "http://arxiv.org/abs/2312.12645": {"title": "Revisiting the effect of greediness on the efficacy of exchange algorithms for generating exact optimal experimental designs", "link": "http://arxiv.org/abs/2312.12645", "description": "Coordinate exchange (CEXCH) is a popular algorithm for generating exact\noptimal experimental designs. The authors of CEXCH advocated for a highly\ngreedy implementation - one that exchanges and optimizes single element\ncoordinates of the design matrix. We revisit the effect of greediness on CEXCHs\nefficacy for generating highly efficient designs. We implement the\nsingle-element CEXCH (most greedy), a design-row (medium greedy) optimization\nexchange, and particle swarm optimization (PSO; least greedy) on 21 exact\nresponse surface design scenarios, under the $D$- and $I-$criterion, which have\nwell-known optimal designs that have been reproduced by several researchers. We\nfound essentially no difference in performance of the most greedy CEXCH and the\nmedium greedy CEXCH. PSO did exhibit better efficacy for generating $D$-optimal\ndesigns, and for most $I$-optimal designs than CEXCH, but not to a strong\ndegree under our parametrization. This work suggests that further investigation\nof the greediness dimension and its effect on CEXCH efficacy on a wider suite\nof models and criterion is warranted."}, "http://arxiv.org/abs/2312.12678": {"title": "Causal Discovery for fMRI data: Challenges, Solutions, and a Case Study", "link": "http://arxiv.org/abs/2312.12678", "description": "Designing studies that apply causal discovery requires navigating many\nresearcher degrees of freedom. This complexity is exacerbated when the study\ninvolves fMRI data. In this paper we (i) describe nine challenges that occur\nwhen applying causal discovery to fMRI data, (ii) discuss the space of\ndecisions that need to be made, (iii) review how a recent case study made those\ndecisions, (iv) and identify existing gaps that could potentially be solved by\nthe development of new methods. Overall, causal discovery is a promising\napproach for analyzing fMRI data, and multiple successful applications have\nindicated that it is superior to traditional fMRI functional connectivity\nmethods, but current causal discovery methods for fMRI leave room for\nimprovement."}, "http://arxiv.org/abs/2312.12708": {"title": "Gradient flows for empirical Bayes in high-dimensional linear models", "link": "http://arxiv.org/abs/2312.12708", "description": "Empirical Bayes provides a powerful approach to learning and adapting to\nlatent structure in data. Theory and algorithms for empirical Bayes have a rich\nliterature for sequence models, but are less understood in settings where\nlatent variables and data interact through more complex designs. In this work,\nwe study empirical Bayes estimation of an i.i.d. prior in Bayesian linear\nmodels, via the nonparametric maximum likelihood estimator (NPMLE). We\nintroduce and study a system of gradient flow equations for optimizing the\nmarginal log-likelihood, jointly over the prior and posterior measures in its\nGibbs variational representation using a smoothed reparametrization of the\nregression coefficients. A diffusion-based implementation yields a Langevin\ndynamics MCEM algorithm, where the prior law evolves continuously over time to\noptimize a sequence-model log-likelihood defined by the coordinates of the\ncurrent Langevin iterate. We show consistency of the NPMLE as $n, p \\rightarrow\n\\infty$ under mild conditions, including settings of random sub-Gaussian\ndesigns when $n \\asymp p$. In high noise, we prove a uniform log-Sobolev\ninequality for the mixing of Langevin dynamics, for possibly misspecified\npriors and non-log-concave posteriors. We then establish polynomial-time\nconvergence of the joint gradient flow to a near-NPMLE if the marginal negative\nlog-likelihood is convex in a sub-level set of the initialization."}, "http://arxiv.org/abs/2312.12710": {"title": "Semiparametric Copula Estimation for Spatially Correlated Multivariate Mixed Outcomes: Analyzing Visual Sightings of Fin Whales from Line Transect Survey", "link": "http://arxiv.org/abs/2312.12710", "description": "Multivariate data having both continuous and discrete variables is known as\nmixed outcomes and has widely appeared in a variety of fields such as ecology,\nepidemiology, and climatology. In order to understand the probability structure\nof multivariate data, the estimation of the dependence structure among mixed\noutcomes is very important. However, when location information is equipped with\nmultivariate data, the spatial correlation should be adequately taken into\naccount; otherwise, the estimation of the dependence structure would be\nseverely biased. To solve this issue, we propose a semiparametric Bayesian\ninference for the dependence structure among mixed outcomes while eliminating\nspatial correlation. To this end, we consider a hierarchical spatial model\nbased on the rank likelihood and a latent multivariate Gaussian process. We\ndevelop an efficient algorithm for computing the posterior using the Markov\nChain Monte Carlo. We also provide a scalable implementation of the model using\nthe nearest-neighbor Gaussian process under large spatial datasets. We conduct\na simulation study to validate our proposed procedure and demonstrate that the\nprocedure successfully accounts for spatial correlation and correctly infers\nthe dependence structure among outcomes. Furthermore, the procedure is applied\nto a real example collected during an international synoptic krill survey in\nthe Scotia Sea of the Antarctic Peninsula, which includes sighting data of fin\nwhales (Balaenoptera physalus), and the relevant oceanographic data."}, "http://arxiv.org/abs/2312.12741": {"title": "Locally Optimal Fixed-Budget Best Arm Identification in Two-Armed Gaussian Bandits with Unknown Variances", "link": "http://arxiv.org/abs/2312.12741", "description": "We address the problem of best arm identification (BAI) with a fixed budget\nfor two-armed Gaussian bandits. In BAI, given multiple arms, we aim to find the\nbest arm, an arm with the highest expected reward, through an adaptive\nexperiment. Kaufmann et al. (2016) develops a lower bound for the probability\nof misidentifying the best arm. They also propose a strategy, assuming that the\nvariances of rewards are known, and show that it is asymptotically optimal in\nthe sense that its probability of misidentification matches the lower bound as\nthe budget approaches infinity. However, an asymptotically optimal strategy is\nunknown when the variances are unknown. For this open issue, we propose a\nstrategy that estimates variances during an adaptive experiment and draws arms\nwith a ratio of the estimated standard deviations. We refer to this strategy as\nthe Neyman Allocation (NA)-Augmented Inverse Probability weighting (AIPW)\nstrategy. We then demonstrate that this strategy is asymptotically optimal by\nshowing that its probability of misidentification matches the lower bound when\nthe budget approaches infinity, and the gap between the expected rewards of two\narms approaches zero (small-gap regime). Our results suggest that under the\nworst-case scenario characterized by the small-gap regime, our strategy, which\nemploys estimated variance, is asymptotically optimal even when the variances\nare unknown."}, "http://arxiv.org/abs/2312.12786": {"title": "Heterogeneous Transfer Learning for Building High-Dimensional Generalized Linear Models with Disparate Datasets", "link": "http://arxiv.org/abs/2312.12786", "description": "Development of comprehensive prediction models are often of great interest in\nmany disciplines of science, but datasets with information on all desired\nfeatures typically have small sample sizes. In this article, we describe a\ntransfer learning approach for building high-dimensional generalized linear\nmodels using data from a main study that has detailed information on all\npredictors, and from one or more external studies that have ascertained a more\nlimited set of predictors. We propose using the external dataset(s) to build\nreduced model(s) and then transfer the information on underlying parameters for\nthe analysis of the main study through a set of calibration equations, while\naccounting for the study-specific effects of certain design variables. We then\nuse a generalized method of moment (GMM) with penalization for parameter\nestimation and develop highly scalable algorithms for fitting models taking\nadvantage of the popular glmnet package. We further show that the use of\nadaptive-Lasso penalty leads to the oracle property of underlying parameter\nestimates and thus leads to convenient post-selection inference procedures. We\nconduct extensive simulation studies to investigate both predictive performance\nand post-selection inference properties of the proposed method. Finally, we\nillustrate a timely application of the proposed method for the development of\nrisk prediction models for five common diseases using the UK Biobank study,\ncombining baseline information from all study participants (500K) and recently\nreleased high-throughout proteomic data (# protein = 1500) on a subset (50K) of\nthe participants."}, "http://arxiv.org/abs/2312.12823": {"title": "Detecting Multiple Change-Points in Distributional Sequences Derived from Structural Health Monitoring Data: An Application to Bridge Damage Detection", "link": "http://arxiv.org/abs/2312.12823", "description": "Detecting damage in important structures using monitored data is a\nfundamental task of structural health monitoring, which is very important for\nthe structures' safety and life-cycle management. Based on the statistical\npattern recognition paradigm, damage detection can be achieved by detecting\nchanges in distribution of properly extracted damage-sensitive features (DSFs).\nThis can be naturally formulated as a distributional change-point detection\nproblem. A good change-point detector for damage detection should be scalable\nto large DSF datasets, applicable to different types of changes and able to\ncontrol the false-positive indication rate. To address these challenges, we\npropose a new distributional change-point detection method for damage\ndetection. We embed the elements of a DSF distributional sequence into the\nWasserstein space and develop a MOSUM-type multiple change-point detector based\non Fr\\'echet statistics. Theoretical properties are also established. Extensive\nsimulation studies demonstrate the superiority of our proposal against other\ncompetitors in addressing the aforementioned practical requirements. We apply\nour method to the cable-tension measurements monitored from a long-span\ncable-stayed bridge for cable damage detection. We conduct a comprehensive\nchange-point analysis for the extracted DSF data, and find some interesting\npatterns from the detected changes, which provides important insights into the\ndamage of the cable system."}, "http://arxiv.org/abs/2312.12844": {"title": "Causal Discovery under Identifiable Heteroscedastic Noise Model", "link": "http://arxiv.org/abs/2312.12844", "description": "Capturing the underlying structural causal relations represented by Directed\nAcyclic Graphs (DAGs) has been a fundamental task in various AI disciplines.\nCausal DAG learning via the continuous optimization framework has recently\nachieved promising performance in terms of both accuracy and efficiency.\nHowever, most methods make strong assumptions of homoscedastic noise, i.e.,\nexogenous noises have equal variances across variables, observations, or even\nboth. The noises in real data usually violate both assumptions due to the\nbiases introduced by different data collection processes. To address the issue\nof heteroscedastic noise, we introduce relaxed and implementable sufficient\nconditions, proving the identifiability of a general class of SEM subject to\nthese conditions. Based on the identifiable general SEM, we propose a novel\nformulation for DAG learning that accounts for the variation in noise variance\nacross variables and observations. We then propose an effective two-phase\niterative DAG learning algorithm to address the increasing optimization\ndifficulties and to learn a causal DAG from data with heteroscedastic variable\nnoise under varying variance. We show significant empirical gains of the\nproposed approaches over state-of-the-art methods on both synthetic data and\nreal data."}, "http://arxiv.org/abs/2312.12952": {"title": "High-dimensional sparse classification using exponential weighting with empirical hinge loss", "link": "http://arxiv.org/abs/2312.12952", "description": "In this study, we address the problem of high-dimensional binary\nclassification. Our proposed solution involves employing an aggregation\ntechnique founded on exponential weights and empirical hinge loss. Through the\nemployment of a suitable sparsity-inducing prior distribution, we demonstrate\nthat our method yields favorable theoretical results on predictions and\nmisclassification error. The efficiency of our procedure is achieved through\nthe utilization of Langevin Monte Carlo, a gradient-based sampling approach. To\nillustrate the effectiveness of our approach, we conduct comparisons with the\nlogistic Lasso on both simulated and a real dataset. Our method frequently\ndemonstrates superior performance compared to the logistic Lasso."}, "http://arxiv.org/abs/2312.12966": {"title": "Rank-based Bayesian clustering via covariate-informed Mallows mixtures", "link": "http://arxiv.org/abs/2312.12966", "description": "Data in the form of rankings, ratings, pair comparisons or clicks are\nfrequently collected in diverse fields, from marketing to politics, to\nunderstand assessors' individual preferences. Combining such preference data\nwith features associated with the assessors can lead to a better understanding\nof the assessors' behaviors and choices. The Mallows model is a popular model\nfor rankings, as it flexibly adapts to different types of preference data, and\nthe previously proposed Bayesian Mallows Model (BMM) offers a computationally\nefficient framework for Bayesian inference, also allowing capturing the users'\nheterogeneity via a finite mixture. We develop a Bayesian Mallows-based finite\nmixture model that performs clustering while also accounting for\nassessor-related features, called the Bayesian Mallows model with covariates\n(BMMx). BMMx is based on a similarity function that a priori favours the\naggregation of assessors into a cluster when their covariates are similar,\nusing the Product Partition models (PPMx) proposal. We present two approaches\nto measure the covariate similarity: one based on a novel deterministic\nfunction measuring the covariates' goodness-of-fit to the cluster, and one\nbased on an augmented model as in PPMx. We investigate the performance of BMMx\nin both simulation experiments and real-data examples, showing the method's\npotential for advancing the understanding of assessor preferences and behaviors\nin different applications."}, "http://arxiv.org/abs/2312.13018": {"title": "Sample Design and Cross-sectional Weights of the Brazilian PCSVDF-Mulher Study (Waves 2016 and 2017): Integrating a Refreshment Sample with an Ongoing Longitudinal Sample to Calculate IPV Prevalence", "link": "http://arxiv.org/abs/2312.13018", "description": "Addressing unit non-response between waves of longitudinal studies\n(attrition) by means of sampling design in weighting has moved from an approach\nfocused on participant retention or modern missing data analysis procedures to\nan approach based on the availability of supplemental samples, either\ncollecting refreshment or replacement samples on an ongoing larger sample. We\nimplement a strategy for calculating individual cross-sectional weights and\napply them to the 2016 and 2017 waves of the PCSVDF-Mulher (Pesquisa de\nCondi\\c{c}\\~oes Socioecon\\^omicas e Viol\\^encia Dom\\'estica e Familiar contra a\nMulher - Survey of Socioeconomic Conditions and Domestic and Family Violence\nagainst Women), a large ($\\approx 10,000$), household interdisciplinary\nlongitudinal data set in Brazil to study intimate partner violence (IPV), its\ncauses and consequences. We developed a set of weights that combines a\nrefreshment sample collected in 2017 with the ongoing longitudinal sample\nstarted in 2016. Armed with this set of individual weights, we calculated IPV\nprevalence for nine capital cities in Brazil for the years 2016 and 2017. As\nfar as we know, this is the first attempt to calculate cross-sectional weights\nwith the aid of supplemental samples applied to a population representative\nsample study focused on IPV. Our analysis produced a set of weights whose\ncomparison to unweighted designs shows clearly neglected trends in the\nliterature on IPV measurement. Indeed, one of our key findings pointed out to\nthe fact that, even in well-designed longitudinal household surveys, the\nindiscriminate use of unweighted designs to calculate IPV prevalence might\nartificially and inadvertently inflate their values, which in turn might bring\ndistortions and considerable political, social, budgetary, and scientific\nimplications."}, "http://arxiv.org/abs/2312.13044": {"title": "Particle Gibbs for Likelihood-Free Inference of State Space Models with Application to Stochastic Volatility", "link": "http://arxiv.org/abs/2312.13044", "description": "State space models (SSMs) are widely used to describe dynamic systems.\nHowever, when the likelihood of the observations is intractable, parameter\ninference for SSMs cannot be easily carried out using standard Markov chain\nMonte Carlo or sequential Monte Carlo methods. In this paper, we propose a\nparticle Gibbs sampler as a general strategy to handle SSMs with intractable\nlikelihoods in the approximate Bayesian computation (ABC) setting. The proposed\nsampler incorporates a conditional auxiliary particle filter, which can help\nmitigate the weight degeneracy often encountered in ABC. To illustrate the\nmethodology, we focus on a classic stochastic volatility model (SVM) used in\nfinance and econometrics for analyzing and interpreting volatility. Simulation\nstudies demonstrate the accuracy of our sampler for SVM parameter inference,\ncompared to existing particle Gibbs samplers based on the conditional bootstrap\nfilter. As a real data application, we apply the proposed sampler for fitting\nan SVM to S&amp;P 500 Index time-series data during the 2008 financial crisis."}, "http://arxiv.org/abs/2312.13097": {"title": "Power calculation for cross-sectional stepped wedge cluster randomized trials with a time-to-event endpoint", "link": "http://arxiv.org/abs/2312.13097", "description": "A popular design choice in public health and implementation science research,\nstepped wedge cluster randomized trials (SW-CRTs) are a form of randomized\ntrial whereby clusters are progressively transitioned from control to\nintervention, and the timing of transition is randomized for each cluster. An\nimportant task at the design stage is to ensure that the planned trial has\nsufficient power to observe a clinically meaningful effect size. While methods\nfor determining study power have been well-developed for SW-CRTs with\ncontinuous and binary outcomes, limited methods for power calculation are\navailable for SW-CRTs with censored time-to-event outcomes. In this article, we\npropose a stratified marginal Cox model to account for secular trend in\ncross-sectional SW-CRTs, and derive an explicit expression of the robust\nsandwich variance to facilitate power calculations without the need for\ncomputationally intensive simulations. Power formulas based on both the Wald\nand robust score tests are developed and compared via simulation, generally\ndemonstrating superiority of robust score procedures in different finite-sample\nscenarios. Finally, we illustrate our methods using a SW-CRT testing the effect\nof a new electronic reminder system on time to catheter removal in hospital\nsettings. We also offer an R Shiny application to facilitate sample size and\npower calculations using our proposed methods."}, "http://arxiv.org/abs/2312.13148": {"title": "Partially factorized variational inference for high-dimensional mixed models", "link": "http://arxiv.org/abs/2312.13148", "description": "While generalized linear mixed models (GLMMs) are a fundamental tool in\napplied statistics, many specifications -- such as those involving categorical\nfactors with many levels or interaction terms -- can be computationally\nchallenging to estimate due to the need to compute or approximate\nhigh-dimensional integrals. Variational inference (VI) methods are a popular\nway to perform such computations, especially in the Bayesian context. However,\nnaive VI methods can provide unreliable uncertainty quantification. We show\nthat this is indeed the case in the GLMM context, proving that standard VI\n(i.e. mean-field) dramatically underestimates posterior uncertainty in\nhigh-dimensions. We then show how appropriately relaxing the mean-field\nassumption leads to VI methods whose uncertainty quantification does not\ndeteriorate in high-dimensions, and whose total computational cost scales\nlinearly with the number of parameters and observations. Our theoretical and\nnumerical results focus on GLMMs with Gaussian or binomial likelihoods, and\nrely on connections to random graph theory to obtain sharp high-dimensional\nasymptotic analysis. We also provide generic results, which are of independent\ninterest, relating the accuracy of variational inference to the convergence\nrate of the corresponding coordinate ascent variational inference (CAVI)\nalgorithm for Gaussian targets. Our proposed partially-factorized VI (PF-VI)\nmethodology for GLMMs is implemented in the R package vglmer, see\nhttps://github.com/mgoplerud/vglmer . Numerical results with simulated and real\ndata examples illustrate the favourable computation cost versus accuracy\ntrade-off of PF-VI."}, "http://arxiv.org/abs/2312.13168": {"title": "Learning Bayesian networks: a copula approach for mixed-type data", "link": "http://arxiv.org/abs/2312.13168", "description": "Estimating dependence relationships between variables is a crucial issue in\nmany applied domains, such as medicine, social sciences and psychology. When\nseveral variables are entertained, these can be organized into a network which\nencodes their set of conditional dependence relations. Typically however, the\nunderlying network structure is completely unknown or can be partially drawn\nonly; accordingly it should be learned from the available data, a process known\nas structure learning. In addition, data arising from social and psychological\nstudies are often of different types, as they can include categorical, discrete\nand continuous measurements. In this paper we develop a novel Bayesian\nmethodology for structure learning of directed networks which applies to mixed\ndata, i.e. possibly containing continuous, discrete, ordinal and binary\nvariables simultaneously. Whenever available, our method can easily incorporate\nknown dependence structures among variables represented by paths or edge\ndirections that can be postulated in advance based on the specific problem\nunder consideration. We evaluate the proposed method through extensive\nsimulation studies, with appreciable performances in comparison with current\nstate-of-the-art alternative methods. Finally, we apply our methodology to\nwell-being data from a social survey promoted by the United Nations, and mental\nhealth data collected from a cohort of medical students."}, "http://arxiv.org/abs/2312.13195": {"title": "Principal Component Copulas for Capital Modelling", "link": "http://arxiv.org/abs/2312.13195", "description": "We introduce a class of copulas that we call Principal Component Copulas.\nThis class intends to combine the strong points of copula-based techniques with\nprincipal component-based models, which results in flexibility when modelling\ntail dependence along the most important directions in multivariate data. The\nproposed techniques have conceptual similarities and technical differences with\nthe increasingly popular class of factor copulas. Such copulas can generate\ncomplex dependence structures and also perform well in high dimensions. We show\nthat Principal Component Copulas give rise to practical and technical\nadvantages compared to other techniques. We perform a simulation study and\napply the copula to multivariate return data. The copula class offers the\npossibility to avoid the curse of dimensionality when estimating very large\ncopula models and it performs particularly well on aggregate measures of tail\nrisk, which is of importance for capital modeling."}, "http://arxiv.org/abs/2103.07066": {"title": "Finding Subgroups with Significant Treatment Effects", "link": "http://arxiv.org/abs/2103.07066", "description": "Researchers often run resource-intensive randomized controlled trials (RCTs)\nto estimate the causal effects of interventions on outcomes of interest. Yet\nthese outcomes are often noisy, and estimated overall effects can be small or\nimprecise. Nevertheless, we may still be able to produce reliable evidence of\nthe efficacy of an intervention by finding subgroups with significant effects.\nIn this paper, we propose a machine-learning method that is specifically\noptimized for finding such subgroups in noisy data. Unlike available methods\nfor personalized treatment assignment, our tool is fundamentally designed to\ntake significance testing into account: it produces a subgroup that is chosen\nto maximize the probability of obtaining a statistically significant positive\ntreatment effect. We provide a computationally efficient implementation using\ndecision trees and demonstrate its gain over selecting subgroups based on\npositive (estimated) treatment effects. Compared to standard tree-based\nregression and classification tools, this approach tends to yield higher power\nin detecting subgroups affected by the treatment."}, "http://arxiv.org/abs/2202.02249": {"title": "Functional Mixtures-of-Experts", "link": "http://arxiv.org/abs/2202.02249", "description": "We consider the statistical analysis of heterogeneous data for prediction in\nsituations where the observations include functions, typically time series. We\nextend the modeling with Mixtures-of-Experts (ME), as a framework of choice in\nmodeling heterogeneity in data for prediction with vectorial observations, to\nthis functional data analysis context. We first present a new family of ME\nmodels, named functional ME (FME) in which the predictors are potentially noisy\nobservations, from entire functions. Furthermore, the data generating process\nof the predictor and the real response, is governed by a hidden discrete\nvariable representing an unknown partition. Second, by imposing sparsity on\nderivatives of the underlying functional parameters via Lasso-like\nregularizations, we provide sparse and interpretable functional representations\nof the FME models called iFME. We develop dedicated expectation--maximization\nalgorithms for Lasso-like (EM-Lasso) regularized maximum-likelihood parameter\nestimation strategies to fit the models. The proposed models and algorithms are\nstudied in simulated scenarios and in applications to two real data sets, and\nthe obtained results demonstrate their performance in accurately capturing\ncomplex nonlinear relationships and in clustering the heterogeneous regression\ndata."}, "http://arxiv.org/abs/2203.14511": {"title": "Statistical Inference for Heterogeneous Treatment Effects Discovered by Generic Machine Learning in Randomized Experiments", "link": "http://arxiv.org/abs/2203.14511", "description": "Researchers are increasingly turning to machine learning (ML) algorithms to\ninvestigate causal heterogeneity in randomized experiments. Despite their\npromise, ML algorithms may fail to accurately ascertain heterogeneous treatment\neffects under practical settings with many covariates and small sample size. In\naddition, the quantification of estimation uncertainty remains a challenge. We\ndevelop a general approach to statistical inference for heterogeneous treatment\neffects discovered by a generic ML algorithm. We apply the Neyman's repeated\nsampling framework to a common setting, in which researchers use an ML\nalgorithm to estimate the conditional average treatment effect and then divide\nthe sample into several groups based on the magnitude of the estimated effects.\nWe show how to estimate the average treatment effect within each of these\ngroups, and construct a valid confidence interval. In addition, we develop\nnonparametric tests of treatment effect homogeneity across groups, and\nrank-consistency of within-group average treatment effects. The validity of our\nmethodology does not rely on the properties of ML algorithms because it is\nsolely based on the randomization of treatment assignment and random sampling\nof units. Finally, we generalize our methodology to the cross-fitting procedure\nby accounting for the additional uncertainty induced by the random splitting of\ndata."}, "http://arxiv.org/abs/2206.10323": {"title": "What Makes Forest-Based Heterogeneous Treatment Effect Estimators Work?", "link": "http://arxiv.org/abs/2206.10323", "description": "Estimation of heterogeneous treatment effects (HTE) is of prime importance in\nmany disciplines, ranging from personalized medicine to economics among many\nothers. Random forests have been shown to be a flexible and powerful approach\nto HTE estimation in both randomized trials and observational studies. In\nparticular \"causal forests\", introduced by Athey, Tibshirani and Wager (2019),\nalong with the R implementation in package grf were rapidly adopted. A related\napproach, called \"model-based forests\", that is geared towards randomized\ntrials and simultaneously captures effects of both prognostic and predictive\nvariables, was introduced by Seibold, Zeileis and Hothorn (2018) along with a\nmodular implementation in the R package model4you.\n\nHere, we present a unifying view that goes beyond the theoretical motivations\nand investigates which computational elements make causal forests so successful\nand how these can be blended with the strengths of model-based forests. To do\nso, we show that both methods can be understood in terms of the same parameters\nand model assumptions for an additive model under L2 loss. This theoretical\ninsight allows us to implement several flavors of \"model-based causal forests\"\nand dissect their different elements in silico.\n\nThe original causal forests and model-based forests are compared with the new\nblended versions in a benchmark study exploring both randomized trials and\nobservational settings. In the randomized setting, both approaches performed\nakin. If confounding was present in the data generating process, we found local\ncentering of the treatment indicator with the corresponding propensities to be\nthe main driver for good performance. Local centering of the outcome was less\nimportant, and might be replaced or enhanced by simultaneous split selection\nwith respect to both prognostic and predictive effects."}, "http://arxiv.org/abs/2208.06729": {"title": "Optimal Recovery for Causal Inference", "link": "http://arxiv.org/abs/2208.06729", "description": "Problems in causal inference can be fruitfully addressed using signal\nprocessing techniques. As an example, it is crucial to successfully quantify\nthe causal effects of an intervention to determine whether the intervention\nachieved desired outcomes. We present a new geometric signal processing\napproach to classical synthetic control called ellipsoidal optimal recovery\n(EOpR), for estimating the unobservable outcome of a treatment unit. EOpR\nprovides policy evaluators with both worst-case and typical outcomes to help in\ndecision making. It is an approximation-theoretic technique that relates to the\ntheory of principal components, which recovers unknown observations given a\nlearned signal class and a set of known observations. We show EOpR can improve\npre-treatment fit and mitigate bias of the post-treatment estimate relative to\nother methods in causal inference. Beyond recovery of the unit of interest, an\nadvantage of EOpR is that it produces worst-case limits over the estimates\nproduced. We assess our approach on artificially-generated data, on datasets\ncommonly used in the econometrics literature, and in the context of the\nCOVID-19 pandemic, showing better performance than baseline techniques"}, "http://arxiv.org/abs/2303.13616": {"title": "Estimating Maximal Symmetries of Regression Functions via Subgroup Lattices", "link": "http://arxiv.org/abs/2303.13616", "description": "We present a method for estimating the maximal symmetry of a continuous\nregression function. Knowledge of such a symmetry can be used to significantly\nimprove modelling by removing the modes of variation resulting from the\nsymmetries. Symmetry estimation is carried out using hypothesis testing for\ninvariance strategically over the subgroup lattice of a search group G acting\non the feature space. We show that the estimation of the unique maximal\ninvariant subgroup of G generalises useful tools from linear dimension\nreduction to a non linear context. We show that the estimation is consistent\nwhen the subgroup lattice chosen is finite, even when some of the subgroups\nthemselves are infinite. We demonstrate the performance of this estimator in\nsynthetic settings and apply the methods to two data sets: satellite\nmeasurements of the earth's magnetic field intensity; and the distribution of\nsunspots."}, "http://arxiv.org/abs/2305.12043": {"title": "SF-SFD: Stochastic Optimization of Fourier Coefficients to Generate Space-Filling Designs", "link": "http://arxiv.org/abs/2305.12043", "description": "Due to the curse of dimensionality, it is often prohibitively expensive to\ngenerate deterministic space-filling designs. On the other hand, when using\nna{\\\"i}ve uniform random sampling to generate designs cheaply, design points\ntend to concentrate in a small region of the design space. Although, it is\npreferable in these cases to utilize quasi-random techniques such as Sobol\nsequences and Latin hypercube designs over uniform random sampling in many\nsettings, these methods have their own caveats especially in high-dimensional\nspaces. In this paper, we propose a technique that addresses the fundamental\nissue of measure concentration by updating high-dimensional distribution\nfunctions to produce better space-filling designs. Then, we show that our\ntechnique can outperform Latin hypercube sampling and Sobol sequences by the\ndiscrepancy metric while generating moderately-sized space-filling samples for\nhigh-dimensional problems."}, "http://arxiv.org/abs/2306.03625": {"title": "Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy Learning", "link": "http://arxiv.org/abs/2306.03625", "description": "We propose a simple and general framework for nonparametric estimation of\nheterogeneous treatment effects under fairness constraints. Under standard\nregularity conditions, we show that the resulting estimators possess the double\nrobustness property. We use this framework to characterize the trade-off\nbetween fairness and the maximum welfare achievable by the optimal policy. We\nevaluate the methods in a simulation study and illustrate them in a real-world\ncase study."}, "http://arxiv.org/abs/2312.13331": {"title": "A Bayesian Spatial Berkson error approach to estimate small area opioid mortality rates accounting for population-at-risk uncertainty", "link": "http://arxiv.org/abs/2312.13331", "description": "Monitoring small-area geographical population trends in opioid mortality has\nlarge scale implications to informing preventative resource allocation. A\ncommon approach to obtain small area estimates of opioid mortality is to use a\nstandard disease mapping approach in which population-at-risk estimates are\ntreated as fixed and known. Assuming fixed populations ignores the uncertainty\nsurrounding small area population estimates, which may bias risk estimates and\nunder-estimate their associated uncertainties. We present a Bayesian Spatial\nBerkson Error (BSBE) model to incorporate population-at-risk uncertainty within\na disease mapping model. We compare the BSBE approach to the naive (treating\ndenominators as fixed) using simulation studies to illustrate potential bias\nresulting from this assumption. We show the application of the BSBE model to\nobtain 2020 opioid mortality risk estimates for 159 counties in GA accounting\nfor population-at-risk uncertainty. Utilizing our proposed approach will help\nto inform interventions in opioid related public health responses, policies,\nand resource allocation. Additionally, we provide a general framework to\nimprove in the estimation and mapping of health indicators."}, "http://arxiv.org/abs/2312.13416": {"title": "A new criterion for interpreting acoustic emission damage signals in condition monitoring based on the distribution of cluster onsets", "link": "http://arxiv.org/abs/2312.13416", "description": "Structural Health Monitoring (SHM) relies on non-destructive techniques such\nas Acoustic Emission (AE) which provide a large amount of data over the life of\nthe systems. The analysis of these data is often based on clustering in order\nto get insights about damage evolution. In order to evaluate clustering\nresults, current approaches include Clustering Validity Indices (CVI) which\nfavor compact and separable clusters. However, these shape-based criteria are\nnot specific to AE data and SHM. This paper proposes a new approach based on\nthe sequentiality of clusters onsets. For monitoring purposes, onsets indicate\nwhen potential damage occurs for the first time and allows to detect the\ninititation of the defects. The proposed CVI relies on the Kullback-Leibler\ndivergence and enables to incorporate prior on damage onsets when available.\nThree experiments on real-world data sets demonstrate the relevance of the\nproposed approach. The first benchmark concerns the detection of the loosening\nof bolted plates under vibration. The proposed onset-based CVI outperforms the\nstandard approach in terms of both cluster quality and accuracy in detecting\nchanges in loosening. The second application involves micro-drilling of hard\nmaterials using Electrical Discharge Machining. In this industrial application,\nit is demonstrated that the proposed CVI can be used to evaluate the electrode\nprogression until the reference depth which is essential to ensure structural\nintegrity. Lastly, the third application is about the damage monitoring in a\ncomposite/metal hybrid joint structure. As an important result, the timeline of\nclusters generated by the proposed CVI is used to draw a scenario that accounts\nfor the occurrence of slippage leading to a critical failure."}, "http://arxiv.org/abs/2312.13430": {"title": "Debiasing Sample Loadings and Scores in Exponential Family PCA for Sparse Count Data", "link": "http://arxiv.org/abs/2312.13430", "description": "Multivariate count data with many zeros frequently occur in a variety of\napplication areas such as text mining with a document-term matrix and cluster\nanalysis with microbiome abundance data. Exponential family PCA (Collins et\nal., 2001) is a widely used dimension reduction tool to understand and capture\nthe underlying low-rank structure of count data. It produces principal\ncomponent scores by fitting Poisson regression models with estimated loadings\nas covariates. This tends to result in extreme scores for sparse count data\nsignificantly deviating from true scores. We consider two major sources of bias\nin this estimation procedure and propose ways to reduce their effects. First,\nthe discrepancy between true loadings and their estimates under a limited\nsample size largely degrades the quality of score estimates. By treating\nestimated loadings as covariates with bias and measurement errors, we debias\nscore estimates, using the iterative bootstrap method for loadings and\nconsidering classical measurement error models. Second, the existence of MLE\nbias is often ignored in score estimation, but this bias could be removed\nthrough well-known MLE bias reduction methods. We demonstrate the effectiveness\nof the proposed bias correction procedure through experiments on both simulated\ndata and real data."}, "http://arxiv.org/abs/2312.13450": {"title": "Precise FWER Control for Gaussian Related Fields: Riding the SuRF to continuous land -- Part 1", "link": "http://arxiv.org/abs/2312.13450", "description": "The Gaussian Kinematic Formula (GKF) is a powerful and computationally\nefficient tool to perform statistical inference on random fields and became a\nwell-established tool in the analysis of neuroimaging data. Using realistic\nerror models, recent articles show that GKF based methods for \\emph{voxelwise\ninference} lead to conservative control of the familywise error rate (FWER) and\nfor cluster-size inference lead to inflated false positive rates. In this\nseries of articles we identify and resolve the main causes of these\nshortcomings in the traditional usage of the GKF for voxelwise inference. This\nfirst part removes the \\textit{good lattice assumption} and allows the data to\nbe non-stationary, yet still assumes the data to be Gaussian. The latter\nassumption is resolved in part 2, where we also demonstrate that our GKF based\nmethodology is non-conservative under realistic error models."}, "http://arxiv.org/abs/2312.13454": {"title": "MixEHR-SurG: a joint proportional hazard and guided topic model for inferring mortality-associated topics from electronic health records", "link": "http://arxiv.org/abs/2312.13454", "description": "Objective: To improve survival analysis using EHR data, we aim to develop a\nsupervised topic model called MixEHR-SurG to simultaneously integrate\nheterogeneous EHR data and model survival hazard.\n\nMaterials and Methods: Our technical contributions are three-folds: (1)\nintegrating EHR topic inference with Cox proportional hazards likelihood; (2)\ninferring patient-specific topic hyperparameters using the PheCode concepts\nsuch that each topic can be identified with exactly one PheCode-associated\nphenotype; (3) multi-modal survival topic inference. This leads to a highly\ninterpretable survival and guided topic model that can infer PheCode-specific\nphenotype topics associated with patient mortality. We evaluated MixEHR-G using\na simulated dataset and two real-world EHR datasets: the Quebec Congenital\nHeart Disease (CHD) data consisting of 8,211 subjects with 75,187 outpatient\nclaim data of 1,767 unique ICD codes; the MIMIC-III consisting of 1,458\nsubjects with multi-modal EHR records.\n\nResults: Compared to the baselines, MixEHR-G achieved a superior dynamic\nAUROC for mortality prediction, with a mean AUROC score of 0.89 in the\nsimulation dataset and a mean AUROC of 0.645 on the CHD dataset. Qualitatively,\nMixEHR-G associates severe cardiac conditions with high mortality risk among\nthe CHD patients after the first heart failure hospitalization and critical\nbrain injuries with increased mortality among the MIMIC-III patients after\ntheir ICU discharge.\n\nConclusion: The integration of the Cox proportional hazards model and EHR\ntopic inference in MixEHR-SurG led to not only competitive mortality prediction\nbut also meaningful phenotype topics for systematic survival analysis. The\nsoftware is available at GitHub: https://github.com/li-lab-mcgill/MixEHR-SurG."}, "http://arxiv.org/abs/2312.13460": {"title": "Hierarchical selection of genetic and gene by environment interaction effects in high-dimensional mixed models", "link": "http://arxiv.org/abs/2312.13460", "description": "Interactions between genes and environmental factors may play a key role in\nthe etiology of many common disorders. Several regularized generalized linear\nmodels (GLMs) have been proposed for hierarchical selection of gene by\nenvironment interaction (GEI) effects, where a GEI effect is selected only if\nthe corresponding genetic main effect is also selected in the model. However,\nnone of these methods allow to include random effects to account for population\nstructure, subject relatedness and shared environmental exposure. In this\npaper, we develop a unified approach based on regularized penalized\nquasi-likelihood (PQL) estimation to perform hierarchical selection of GEI\neffects in sparse regularized mixed models. We compare the selection and\nprediction accuracy of our proposed model with existing methods through\nsimulations under the presence of population structure and shared environmental\nexposure. We show that for all simulation scenarios, compared to other\npenalized methods, our proposed method enforced sparsity by controlling the\nnumber of false positives in the model while having the best predictive\nperformance. Finally, we apply our method to a real data application using the\nOrofacial Pain: Prospective Evaluation and Risk Assessment (OPPERA) study, and\nfound that our method retrieves previously reported significant loci."}, "http://arxiv.org/abs/2312.13482": {"title": "Spatially Adaptive Variable Screening in Presurgical fMRI Data Analysis", "link": "http://arxiv.org/abs/2312.13482", "description": "Accurate delineation of tumor-adjacent functional brain regions is essential\nfor planning function-preserving neurosurgery. Functional magnetic resonance\nimaging (fMRI) is increasingly used for presurgical counseling and planning.\nWhen analyzing presurgical fMRI data, false negatives are more dangerous to the\npatients than false positives because patients are more likely to experience\nsignificant harm from failing to identify functional regions and subsequently\nresecting critical tissues. In this paper, we propose a novel spatially\nadaptive variable screening procedure to enable effective control of false\nnegatives while leveraging the spatial structure of fMRI data. Compared to\nexisting statistical methods in fMRI data analysis, the new procedure directly\ncontrols false negatives at a desirable level and is completely data-driven.\nThe new method is also substantially different from existing false-negative\ncontrol procedures which do not take spatial information into account.\nNumerical examples show that the new method outperforms several\nstate-of-the-art methods in retaining signal voxels, especially the subtle ones\nat the boundaries of functional regions, while providing cleaner separation of\nfunctional regions from background noise. Such results could be valuable to\npreserve critical tissues in neurosurgery."}, "http://arxiv.org/abs/2312.13517": {"title": "An utopic adventure in the modelling of conditional univariate and multivariate extremes", "link": "http://arxiv.org/abs/2312.13517", "description": "The EVA 2023 data competition consisted of four challenges, ranging from\ninterval estimation for very high quantiles of univariate extremes conditional\non covariates, point estimation of unconditional return levels under a custom\nloss function, to estimation of the probabilities of tail events for low and\nhigh-dimensional multivariate data. We tackle these tasks by revisiting the\ncurrent and existing literature on conditional univariate and multivariate\nextremes. We propose new cross-validation methods for covariate-dependent\nmodels, validation metrics for exchangeable multivariate models, formulae for\nthe joint probability of exceedance for multivariate generalized Pareto vectors\nand a composition sampling algorithm for generating multivariate tail events\nfor the latter. We highlight overarching themes ranging from model validation\nat extremely high quantile levels to building custom estimation strategies that\nleverage model assumptions."}, "http://arxiv.org/abs/2312.13643": {"title": "Debiasing Welch's Method for Spectral Density Estimation", "link": "http://arxiv.org/abs/2312.13643", "description": "Welch's method provides an estimator of the power spectral density that is\nstatistically consistent. This is achieved by averaging over periodograms\ncalculated from overlapping segments of a time series. For a finite length time\nseries, while the variance of the estimator decreases as the number of segments\nincrease, the magnitude of the estimator's bias increases: a bias-variance\ntrade-off ensues when setting the segment number. We address this issue by\nproviding a a novel method for debiasing Welch's method which maintains the\ncomputational complexity and asymptotic consistency, and leads to improved\nfinite-sample performance. Theoretical results are given for fourth-order\nstationary processes with finite fourth-order moments and absolutely continuous\nfourth-order cumulant spectrum. The significant bias reduction is demonstrated\nwith numerical simulation and an application to real-world data, where several\nempirical metrics indicate our debiased estimator compares favourably to\nWelch's. Our estimator also permits irregular spacing over frequency and we\ndemonstrate how this may be employed for signal compression and further\nvariance reduction. Code accompanying this work is available in the R and\npython languages."}, "http://arxiv.org/abs/2312.13725": {"title": "Extreme Value Statistics for Analysing Simulated Environmental Extremes", "link": "http://arxiv.org/abs/2312.13725", "description": "We present the methods employed by team `Uniofbathtopia' as part of the Data\nChallenge organised for the 13th International Conference on Extreme Value\nAnalysis (EVA2023), including our winning entry for the third sub-challenge.\nOur approaches unite ideas from extreme value theory, which provides a\nstatistical framework for the estimation of probabilities/return levels\nassociated with rare events, with techniques from unsupervised statistical\nlearning, such as clustering and support identification. The methods are\ndemonstrated on the data provided for the Data Challenge -- environmental data\nsampled from the fantasy country of `Utopia' -- but the underlying assumptions\nand frameworks should apply in more general settings and applications."}, "http://arxiv.org/abs/2312.13875": {"title": "Best Arm Identification in Batched Multi-armed Bandit Problems", "link": "http://arxiv.org/abs/2312.13875", "description": "Recently multi-armed bandit problem arises in many real-life scenarios where\narms must be sampled in batches, due to limited time the agent can wait for the\nfeedback. Such applications include biological experimentation and online\nmarketing. The problem is further complicated when the number of arms is large\nand the number of batches is small. We consider pure exploration in a batched\nmulti-armed bandit problem. We introduce a general linear programming framework\nthat can incorporate objectives of different theoretical settings in best arm\nidentification. The linear program leads to a two-stage algorithm that can\nachieve good theoretical properties. We demonstrate by numerical studies that\nthe algorithm also has good performance compared to certain UCB-type or\nThompson sampling methods."}, "http://arxiv.org/abs/2312.13992": {"title": "Bayesian nonparametric boundary detection for income areal data", "link": "http://arxiv.org/abs/2312.13992", "description": "Recent discussions on the future of metropolitan cities underscore the\npivotal role of (social) equity, driven by demographic and economic trends.\nMore equal policies can foster and contribute to a city's economic success and\nsocial stability. In this work, we focus on identifying metropolitan areas with\ndistinct economic and social levels in the greater Los Angeles area, one of the\nmost diverse yet unequal areas in the United States. Utilizing American\nCommunity Survey data, we propose a Bayesian model for boundary detection based\non income distributions. The model identifies areas with significant income\ndisparities, offering actionable insights for policymakers to address social\nand economic inequalities. Our approach formalized as a Bayesian structural\nlearning framework, models areal densities through finite mixture models.\nEfficient posterior computation is facilitated by a transdimensional Markov\nChain Monte Carlo sampler. The methodology is validated via extensive\nsimulations and applied to the income distributions in the greater Los Angeles\narea. We identify several boundaries in the income distributions which can be\nexplained in light of other social dynamics such as crime rates and healthcare,\nshowing the usefulness of such an analysis to policymakers."}, "http://arxiv.org/abs/2312.14013": {"title": "Two-Stage Pseudo Maximum Likelihood Estimation of Semiparametric Copula-based Regression Models for Semi-Competing Risks Data", "link": "http://arxiv.org/abs/2312.14013", "description": "We propose a two-stage estimation procedure for a copula-based model with\nsemi-competing risks data, where the non-terminal event is subject to dependent\ncensoring by the terminal event, and both events are subject to independent\ncensoring. Under a copula-based model, the marginal survival functions of\nindividual event times are specified by semiparametric transformation models,\nand the dependence between the bivariate event times is specified by a\nparametric copula function. For the estimation procedure, in the first stage,\nthe parameters associated with the marginal of the terminal event are estimated\nonly using the corresponding observed outcomes, and in the second stage, the\nmarginal parameters for the non-terminal event time and the copula parameter\nare estimated via maximizing a pseudo-likelihood function based on the joint\ndistribution of the bivariate event times. We derived the asymptotic properties\nof the proposed estimator and provided an analytic variance estimator for\ninference. Through simulation studies, we showed that our approach leads to\nconsistent estimates with less computational cost and more robustness compared\nto the one-stage procedure developed in Chen (2012), where all parameters were\nestimated simultaneously. In addition, our approach demonstrates more desirable\nfinite-sample performances over another existing two-stage estimation method\nproposed in Zhu et al. (2021)."}, "http://arxiv.org/abs/2312.14086": {"title": "A Bayesian approach to functional regression: theory and computation", "link": "http://arxiv.org/abs/2312.14086", "description": "We propose a novel Bayesian methodology for inference in functional linear\nand logistic regression models based on the theory of reproducing kernel\nHilbert spaces (RKHS's). These models build upon the RKHS associated with the\ncovariance function of the underlying stochastic process, and can be viewed as\na finite-dimensional approximation to the classical functional regression\nparadigm. The corresponding functional model is determined by a function living\non a dense subspace of the RKHS of interest, which has a tractable parametric\nform based on linear combinations of the kernel. By imposing a suitable prior\ndistribution on this functional space, we can naturally perform data-driven\ninference via standard Bayes methodology, estimating the posterior distribution\nthrough Markov chain Monte Carlo (MCMC) methods. In this context, our\ncontribution is two-fold. First, we derive a theoretical result that guarantees\nposterior consistency in these models, based on an application of a classic\ntheorem of Doob to our RKHS setting. Second, we show that several prediction\nstrategies stemming from our Bayesian formulation are competitive against other\nusual alternatives in both simulations and real data sets, including a\nBayesian-motivated variable selection procedure."}, "http://arxiv.org/abs/2312.14130": {"title": "Adaptation using spatially distributed Gaussian Processes", "link": "http://arxiv.org/abs/2312.14130", "description": "We consider the accuracy of an approximate posterior distribution in\nnonparametric regression problems by combining posterior distributions computed\non subsets of the data defined by the locations of the independent variables.\nWe show that this approximate posterior retains the rate of recovery of the\nfull data posterior distribution, where the rate of recovery adapts to the\nsmoothness of the true regression function. As particular examples we consider\nGaussian process priors based on integrated Brownian motion and the Mat\\'ern\nkernel augmented with a prior on the length scale. Besides theoretical\nguarantees we present a numerical study of the methods both on synthetic and\nreal world data. We also propose a new aggregation technique, which numerically\noutperforms previous approaches."}, "http://arxiv.org/abs/2202.00824": {"title": "KSD Aggregated Goodness-of-fit Test", "link": "http://arxiv.org/abs/2202.00824", "description": "We investigate properties of goodness-of-fit tests based on the Kernel Stein\nDiscrepancy (KSD). We introduce a strategy to construct a test, called KSDAgg,\nwhich aggregates multiple tests with different kernels. KSDAgg avoids splitting\nthe data to perform kernel selection (which leads to a loss in test power), and\nrather maximises the test power over a collection of kernels. We provide\nnon-asymptotic guarantees on the power of KSDAgg: we show it achieves the\nsmallest uniform separation rate of the collection, up to a logarithmic term.\nFor compactly supported densities with bounded model score function, we derive\nthe rate for KSDAgg over restricted Sobolev balls; this rate corresponds to the\nminimax optimal rate over unrestricted Sobolev balls, up to an iterated\nlogarithmic term. KSDAgg can be computed exactly in practice as it relies\neither on a parametric bootstrap or on a wild bootstrap to estimate the\nquantiles and the level corrections. In particular, for the crucial choice of\nbandwidth of a fixed kernel, it avoids resorting to arbitrary heuristics (such\nas median or standard deviation) or to data splitting. We find on both\nsynthetic and real-world data that KSDAgg outperforms other state-of-the-art\nquadratic-time adaptive KSD-based goodness-of-fit testing procedures."}, "http://arxiv.org/abs/2209.06101": {"title": "Evaluating individualized treatment effect predictions: a model-based perspective on discrimination and calibration assessment", "link": "http://arxiv.org/abs/2209.06101", "description": "In recent years, there has been a growing interest in the prediction of\nindividualized treatment effects. While there is a rapidly growing literature\non the development of such models, there is little literature on the evaluation\nof their performance. In this paper, we aim to facilitate the validation of\nprediction models for individualized treatment effects. The estimands of\ninterest are defined as based on the potential outcomes framework, which\nfacilitates a comparison of existing and novel measures. In particular, we\nexamine existing measures of measures of discrimination for benefit (variations\nof the c-for-benefit), and propose model-based extensions to the treatment\neffect setting for discrimination and calibration metrics that have a strong\nbasis in outcome risk prediction. The main focus is on randomized trial data\nwith binary endpoints and on models that provide individualized treatment\neffect predictions and potential outcome predictions. We use simulated data to\nprovide insight into the characteristics of the examined discrimination and\ncalibration statistics under consideration, and further illustrate all methods\nin a trial of acute ischemic stroke treatment. The results show that the\nproposed model-based statistics had the best characteristics in terms of bias\nand accuracy. While resampling methods adjusted for the optimism of performance\nestimates in the development data, they had a high variance across replications\nthat limited their accuracy. Therefore, individualized treatment effect models\nare best validated in independent data. To aid implementation, a software\nimplementation of the proposed methods was made available in R."}, "http://arxiv.org/abs/2211.11400": {"title": "The Online Closure Principle", "link": "http://arxiv.org/abs/2211.11400", "description": "The closure principle is fundamental in multiple testing and has been used to\nderive many efficient procedures with familywise error rate control. However,\nit is often unsuitable for modern research, which involves flexible multiple\ntesting settings where not all hypotheses are known at the beginning of the\nevaluation. In this paper, we focus on online multiple testing where a possibly\ninfinite sequence of hypotheses is tested over time. At each step, it must be\ndecided on the current hypothesis without having any information about the\nhypotheses that have not been tested yet. Our main contribution is a general\nand stringent mathematical definition of online multiple testing and a new\nonline closure principle which ensures that the resulting closed procedure can\nbe applied in the online setting. We prove that any familywise error rate\ncontrolling online procedure can be derived by this online closure principle\nand provide admissibility results. In addition, we demonstrate how short-cuts\nof these online closed procedures can be obtained under a suitable consonance\nproperty."}, "http://arxiv.org/abs/2305.12616": {"title": "Conformal Prediction With Conditional Guarantees", "link": "http://arxiv.org/abs/2305.12616", "description": "We consider the problem of constructing distribution-free prediction sets\nwith finite-sample conditional guarantees. Prior work has shown that it is\nimpossible to provide exact conditional coverage universally in finite samples.\nThus, most popular methods only provide marginal coverage over the covariates.\nThis paper bridges this gap by defining a spectrum of problems that interpolate\nbetween marginal and conditional validity. We motivate these problems by\nreformulating conditional coverage as coverage over a class of covariate\nshifts. When the target class of shifts is finite dimensional, we show how to\nsimultaneously obtain exact finite sample coverage over all possible shifts.\nFor example, given a collection of protected subgroups, our algorithm outputs\nintervals with exact coverage over each group. For more flexible, infinite\ndimensional classes where exact coverage is impossible, we provide a simple\nprocedure for quantifying the gap between the coverage of our algorithm and the\ntarget level. Moreover, by tuning a single hyperparameter, we allow the\npractitioner to control the size of this gap across shifts of interest. Our\nmethods can be easily incorporated into existing split conformal inference\npipelines, and thus can be used to quantify the uncertainty of modern black-box\nalgorithms without distributional assumptions."}, "http://arxiv.org/abs/2305.16842": {"title": "Accounting statement analysis at industry level", "link": "http://arxiv.org/abs/2305.16842", "description": "Compositional data are contemporarily defined as positive vectors, the ratios\namong whose elements are of interest to the researcher. Financial statement\nanalysis by means of accounting ratios fulfils this definition to the letter.\nCompositional data analysis solves the major problems in statistical analysis\nof standard financial ratios at industry level, such as skewness,\nnon-normality, non-linearity and dependence of the results on the choice of\nwhich accounting figure goes to the numerator and to the denominator of the\nratio. In spite of this, compositional applications to financial statement\nanalysis are still rare. In this article, we present some transformations\nwithin compositional data analysis that are particularly useful for financial\nstatement analysis. We show how to compute industry or sub-industry means of\nstandard financial ratios from a compositional perspective. We show how to\nvisualise firms in an industry with a compositional biplot, to classify them\nwith compositional cluster analysis and to relate financial and non-financial\nindicators with compositional regression models. We show an application to the\naccounting statements of Spanish wineries using DuPont analysis, and a\nstep-by-step tutorial to the compositional freeware CoDaPack."}, "http://arxiv.org/abs/2306.12865": {"title": "Estimating dynamic treatment regimes for ordinal outcomes with household interference: Application in household smoking cessation", "link": "http://arxiv.org/abs/2306.12865", "description": "The focus of precision medicine is on decision support, often in the form of\ndynamic treatment regimes (DTRs), which are sequences of decision rules. At\neach decision point, the decision rules determine the next treatment according\nto the patient's baseline characteristics, the information on treatments and\nresponses accrued by that point, and the patient's current health status,\nincluding symptom severity and other measures. However, DTR estimation with\nordinal outcomes is rarely studied, and rarer still in the context of\ninterference - where one patient's treatment may affect another's outcome. In\nthis paper, we introduce the weighted proportional odds model (WPOM): a\nregression-based, approximate doubly-robust approach to single-stage DTR\nestimation for ordinal outcomes. This method also accounts for the possibility\nof interference between individuals sharing a household through the use of\ncovariate balancing weights derived from joint propensity scores. Examining\ndifferent types of balancing weights, we verify the approximate double\nrobustness of WPOM with our adjusted weights via simulation studies. We further\nextend WPOM to multi-stage DTR estimation with household interference, namely\ndWPOM (dynamic WPOM). Lastly, we demonstrate our proposed methodology in the\nanalysis of longitudinal survey data from the Population Assessment of Tobacco\nand Health study, which motivates this work. Furthermore, considering\ninterference, we provide optimal treatment strategies for households to achieve\nsmoking cessation of the pair in the household."}, "http://arxiv.org/abs/2307.16502": {"title": "Percolated stochastic block model via EM algorithm and belief propagation with non-backtracking spectra", "link": "http://arxiv.org/abs/2307.16502", "description": "Whereas Laplacian and modularity based spectral clustering is apt to dense\ngraphs, recent results show that for sparse ones, the non-backtracking spectrum\nis the best candidate to find assortative clusters of nodes. Here belief\npropagation in the sparse stochastic block model is derived with arbitrary\ngiven model parameters that results in a non-linear system of equations; with\nlinear approximation, the spectrum of the non-backtracking matrix is able to\nspecify the number $k$ of clusters. Then the model parameters themselves can be\nestimated by the EM algorithm. Bond percolation in the assortative model is\nconsidered in the following two senses: the within- and between-cluster edge\nprobabilities decrease with the number of nodes and edges coming into existence\nin this way are retained with probability $\\beta$. As a consequence, the\noptimal $k$ is the number of the structural real eigenvalues (greater than\n$\\sqrt{c}$, where $c$ is the average degree) of the non-backtracking matrix of\nthe graph. Assuming, these eigenvalues $\\mu_1 &gt;\\dots &gt; \\mu_k$ are distinct, the\nmultiple phase transitions obtained for $\\beta$ are $\\beta_i\n=\\frac{c}{\\mu_i^2}$; further, at $\\beta_i$ the number of detectable clusters is\n$i$, for $i=1,\\dots ,k$. Inflation-deflation techniques are also discussed to\nclassify the nodes themselves, which can be the base of the sparse spectral\nclustering."}, "http://arxiv.org/abs/2310.02679": {"title": "Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization", "link": "http://arxiv.org/abs/2310.02679", "description": "We tackle the problem of sampling from intractable high-dimensional density\nfunctions, a fundamental task that often appears in machine learning and\nstatistics. We extend recent sampling-based approaches that leverage controlled\nstochastic processes to model approximate samples from these target densities.\nThe main drawback of these approaches is that the training objective requires\nfull trajectories to compute, resulting in sluggish credit assignment issues\ndue to use of entire trajectories and a learning signal present only at the\nterminal time. In this work, we present Diffusion Generative Flow Samplers\n(DGFS), a sampling-based framework where the learning process can be tractably\nbroken down into short partial trajectory segments, via parameterizing an\nadditional \"flow function\". Our method takes inspiration from the theory\ndeveloped for generative flow networks (GFlowNets), allowing us to make use of\nintermediate learning signals. Through various challenging experiments, we\ndemonstrate that DGFS achieves more accurate estimates of the normalization\nconstant than closely-related prior methods."}, "http://arxiv.org/abs/2312.14333": {"title": "Behaviour Modelling of Social Animals via Causal Structure Discovery and Graph Neural Networks", "link": "http://arxiv.org/abs/2312.14333", "description": "Better understanding the natural world is a crucial task with a wide range of\napplications. In environments with close proximity between humans and animals,\nsuch as zoos, it is essential to better understand the causes behind animal\nbehaviour and what interventions are responsible for changes in their\nbehaviours. This can help to predict unusual behaviours, mitigate detrimental\neffects and increase the well-being of animals. There has been work on\nmodelling the dynamics behind swarms of birds and insects but the complex\nsocial behaviours of mammalian groups remain less explored. In this work, we\npropose a method to build behavioural models using causal structure discovery\nand graph neural networks for time series. We apply this method to a mob of\nmeerkats in a zoo environment and study its ability to predict future actions\nand model the behaviour distribution at an individual-level and at a group\nlevel. We show that our method can match and outperform standard deep learning\narchitectures and generate more realistic data, while using fewer parameters\nand providing increased interpretability."}, "http://arxiv.org/abs/2312.14416": {"title": "Joint Semi-Symmetric Tensor PCA for Integrating Multi-modal Populations of Networks", "link": "http://arxiv.org/abs/2312.14416", "description": "Multi-modal populations of networks arise in many scenarios including in\nlarge-scale multi-modal neuroimaging studies that capture both functional and\nstructural neuroimaging data for thousands of subjects. A major research\nquestion in such studies is how functional and structural brain connectivity\nare related and how they vary across the population. we develop a novel\nPCA-type framework for integrating multi-modal undirected networks measured on\nmany subjects. Specifically, we arrange these networks as semi-symmetric\ntensors, where each tensor slice is a symmetric matrix representing a network\nfrom an individual subject. We then propose a novel Joint, Integrative\nSemi-Symmetric Tensor PCA (JisstPCA) model, associated with an efficient\niterative algorithm, for jointly finding low-rank representations of two or\nmore networks across the same population of subjects. We establish one-step\nstatistical convergence of our separate low-rank network factors as well as the\nshared population factors to the true factors, with finite sample statistical\nerror bounds. Through simulation studies and a real data example for\nintegrating multi-subject functional and structural brain connectivity, we\nillustrate the advantages of our method for finding joint low-rank structures\nin multi-modal populations of networks."}, "http://arxiv.org/abs/2312.14420": {"title": "On eigenvalues of sample covariance matrices based on high dimensional compositional data", "link": "http://arxiv.org/abs/2312.14420", "description": "This paper studies the asymptotic spectral properties of the sample\ncovariance matrix for high dimensional compositional data, including the\nlimiting spectral distribution, the limit of extreme eigenvalues, and the\ncentral limit theorem for linear spectral statistics. All asymptotic results\nare derived under the high-dimensional regime where the data dimension\nincreases to infinity proportionally with the sample size. The findings reveal\nthat the limiting spectral distribution is the well-known Marchenko-Pastur law.\nThe largest (or smallest non-zero) eigenvalue converges almost surely to the\nleft (or right) endpoint of the limiting spectral distribution, respectively.\nMoreover, the linear spectral statistics demonstrate a Gaussian limit.\nSimulation experiments demonstrate the accuracy of theoretical results."}, "http://arxiv.org/abs/2312.14534": {"title": "Global Rank Sum Test: An Efficient Rank-Based Nonparametric Test for Large Scale Online Experiment", "link": "http://arxiv.org/abs/2312.14534", "description": "Online experiments are widely used for improving online services. While doing\nonline experiments, The student t-test is the most widely used hypothesis\ntesting technique. In practice, however, the normality assumption on which the\nt-test depends on may fail, which resulting in untrustworthy results. In this\npaper, we first discuss the question of when the t-test fails, and thus\nintroduce the rank-sum test. Next, in order to solve the difficulties while\nimplementing rank-sum test in large online experiment platforms, we proposed a\nglobal-rank-sum test method as an improvement for the traditional one. Finally,\nwe demonstrate that the global-rank-sum test is not only more accurate and has\nhigher statistical power than the t-test, but also more time efficient than the\ntraditional rank-sum test, which eventually makes it possible for large online\nexperiment platforms to use."}, "http://arxiv.org/abs/2312.14549": {"title": "A machine learning approach based on survival analysis for IBNR frequencies in non-life reserving", "link": "http://arxiv.org/abs/2312.14549", "description": "We introduce new approaches for forecasting IBNR (Incurred But Not Reported)\nfrequencies by leveraging individual claims data, which includes accident date,\nreporting delay, and possibly additional features for every reported claim. A\nkey element of our proposal involves computing development factors, which may\nbe influenced by both the accident date and other features. These development\nfactors serve as the basis for predictions. While we assume close to continuous\nobservations of accident date and reporting delay, the development factors can\nbe expressed at any level of granularity, such as months, quarters, or year and\npredictions across different granularity levels exhibit coherence. The\ncalculation of development factors relies on the estimation of a hazard\nfunction in reverse development time, and we present three distinct methods for\nestimating this function: the Cox proportional hazard model, a feed-forward\nneural network, and xgboost (eXtreme gradient boosting). In all three cases,\nestimation is based on the same partial likelihood that accommodates left\ntruncation and ties in the data. While the first case is a semi-parametric\nmodel that assumes in parts a log linear structure, the two machine learning\napproaches only assume that the baseline and the other factors are\nmultiplicatively separable. Through an extensive simulation study and\nreal-world data application, our approach demonstrates promising results. This\npaper comes with an accompanying R-package, $\\texttt{ReSurv}$, which can be\naccessed at \\url{https://github.com/edhofman/ReSurv}"}, "http://arxiv.org/abs/2312.14583": {"title": "Inference on the state process of periodically inhomogeneous hidden Markov models for animal behavior", "link": "http://arxiv.org/abs/2312.14583", "description": "Over the last decade, hidden Markov models (HMMs) have become increasingly\npopular in statistical ecology, where they constitute natural tools for\nstudying animal behavior based on complex sensor data. Corresponding analyses\nsometimes explicitly focus on - and in any case need to take into account -\nperiodic variation, for example by quantifying the activity distribution over\nthe daily cycle or seasonal variation such as migratory behavior. For HMMs\nincluding periodic components, we establish important mathematical properties\nthat allow for comprehensive statistical inference related to periodic\nvariation, thereby also providing guidance for model building and model\nchecking. Specifically, we derive the periodically varying unconditional state\ndistribution as well as the time-varying and overall state dwell-time\ndistributions - all of which are of key interest when the inferential focus\nlies on the dynamics of the state process. We use the associated novel\ninference and model-checking tools to investigate changes in the diel activity\npatterns of fruit flies in response to changing light conditions."}, "http://arxiv.org/abs/2312.14601": {"title": "Generalized Moment Estimators based on Stein Identities", "link": "http://arxiv.org/abs/2312.14601", "description": "For parameter estimation of continuous and discrete distributions, we propose\na generalization of the method of moments (MM), where Stein identities are\nutilized for improved estimation performance. The construction of these\nStein-type MM-estimators makes use of a weight function as implied by an\nappropriate form of the Stein identity. Our general approach as well as\npotential benefits thereof are first illustrated by the simple example of the\nexponential distribution. Afterward, we investigate the more sophisticated\ntwo-parameter inverse Gaussian distribution and the two-parameter\nnegative-binomial distribution in great detail, together with illustrative\nreal-world data examples. Given an appropriate choice of the respective weight\nfunctions, their Stein-MM estimators, which are defined by simple closed-form\nformulas and allow for closed-form asymptotic computations, exhibit a better\nperformance regarding bias and mean squared error than competing estimators."}, "http://arxiv.org/abs/2312.14689": {"title": "Mistaken identities lead to missed opportunities: Testing for mean differences in partially matched data", "link": "http://arxiv.org/abs/2312.14689", "description": "It is increasingly common to collect pre-post data with pseudonyms or\nself-constructed identifiers. On survey responses from sensitive populations,\nidentifiers may be made optional to encourage higher response rates. The\nability to match responses between pre- and post-intervention phases for every\nparticipant may be impossible in such applications, leaving practitioners with\na choice between the paired t-test on the matched samples and the two-sample\nt-test on all samples for evaluating mean differences. We demonstrate the\ninadequacies with both approaches, as the former test requires discarding\nunmatched data, while the latter test ignores correlation and assumes\nindependence. In cases with a subset of matched samples, an opportunity to\nachieve limited inference about the correlation exists. We propose a novel\ntechnique for such `partially matched' data, which we refer to as the\nQuantile-based t-test for correlated samples, to assess mean differences using\na conservative estimate of the correlation between responses based on the\nmatched subset. Critically, our approach does not discard unmatched samples,\nnor does it assume independence. Our results demonstrate that the proposed\nmethod yields nominal Type I error probability while affording more power than\nexisting approaches. Practitioners can readily adopt our approach with basic\nstatistical programming software."}, "http://arxiv.org/abs/2312.14719": {"title": "Nonhomogeneous hidden semi-Markov models for toroidal data", "link": "http://arxiv.org/abs/2312.14719", "description": "A nonhomogeneous hidden semi-Markov model is proposed to segment toroidal\ntime series according to a finite number of latent regimes and, simultaneously,\nestimate the influence of time-varying covariates on the process' survival\nunder each regime. The model is a mixture of toroidal densities, whose\nparameters depend on the evolution of a semi-Markov chain, which is in turn\nmodulated by time-varying covariates through a proportional hazards assumption.\nParameter estimates are obtained using an EM algorithm that relies on an\nefficient augmentation of the latent process. The proposal is illustrated on a\ntime series of wind and wave directions recorded during winter."}, "http://arxiv.org/abs/2312.14810": {"title": "Accelerating Bayesian Optimal Experimental Design with Derivative-Informed Neural Operators", "link": "http://arxiv.org/abs/2312.14810", "description": "We consider optimal experimental design (OED) for nonlinear Bayesian inverse\nproblems governed by large-scale partial differential equations (PDEs). For the\noptimality criteria of Bayesian OED, we consider both expected information gain\nand summary statistics including the trace and determinant of the information\nmatrix that involves the evaluation of the parameter-to-observable (PtO) map\nand its derivatives. However, it is prohibitive to compute and optimize these\ncriteria when the PDEs are very expensive to solve, the parameters to estimate\nare high-dimensional, and the optimization problem is combinatorial,\nhigh-dimensional, and non-convex. To address these challenges, we develop an\naccurate, scalable, and efficient computational framework to accelerate the\nsolution of Bayesian OED. In particular, the framework is developed based on\nderivative-informed neural operator (DINO) surrogates with proper dimension\nreduction techniques and a modified swapping greedy algorithm. We demonstrate\nthe high accuracy of the DINO surrogates in the computation of the PtO map and\nthe optimality criteria compared to high-fidelity finite element\napproximations. We also show that the proposed method is scalable with\nincreasing parameter dimensions. Moreover, we demonstrate that it achieves high\nefficiency with over 1000X speedup compared to a high-fidelity Bayesian OED\nsolution for a three-dimensional PDE example with tens of thousands of\nparameters, including both online evaluation and offline construction costs of\nthe surrogates."}, "http://arxiv.org/abs/2108.13010": {"title": "Piecewise monotone estimation in one-parameter exponential family", "link": "http://arxiv.org/abs/2108.13010", "description": "The problem of estimating a piecewise monotone sequence of normal means is\ncalled the nearly isotonic regression. For this problem, an efficient algorithm\nhas been devised by modifying the pool adjacent violators algorithm (PAVA). In\nthis study, we investigate estimation of a piecewise monotone parameter\nsequence for general one-parameter exponential families such as binomial,\nPoisson and chi-square. We develop an efficient algorithm based on the modified\nPAVA, which utilizes the duality between the natural and expectation\nparameters. We also provide a method for selecting the regularization parameter\nby using an information criterion. Simulation results demonstrate that the\nproposed method detects change-points in piecewise monotone parameter sequences\nin a data-driven manner. Applications to spectrum estimation, causal inference\nand discretization error quantification of ODE solvers are also presented."}, "http://arxiv.org/abs/2207.09943": {"title": "Efficient Bias Correction for Cross-section and Panel Data", "link": "http://arxiv.org/abs/2207.09943", "description": "Bias correction can often improve the finite sample performance of\nestimators. We show that the choice of bias correction method has no effect on\nthe higher-order variance of semiparametrically efficient parametric\nestimators, so long as the estimate of the bias is asymptotically linear. It is\nalso shown that bootstrap, jackknife, and analytical bias estimates are\nasymptotically linear for estimators with higher-order expansions of a standard\nform. In particular, we find that for a variety of estimators the\nstraightforward bootstrap bias correction gives the same higher-order variance\nas more complicated analytical or jackknife bias corrections. In contrast, bias\ncorrections that do not estimate the bias at the parametric rate, such as the\nsplit-sample jackknife, result in larger higher-order variances in the i.i.d.\nsetting we focus on. For both a cross-sectional MLE and a panel model with\nindividual fixed effects, we show that the split-sample jackknife has a\nhigher-order variance term that is twice as large as that of the\n`leave-one-out' jackknife."}, "http://arxiv.org/abs/2212.03131": {"title": "Explainability as statistical inference", "link": "http://arxiv.org/abs/2212.03131", "description": "A wide variety of model explanation approaches have been proposed in recent\nyears, all guided by very different rationales and heuristics. In this paper,\nwe take a new route and cast interpretability as a statistical inference\nproblem. We propose a general deep probabilistic model designed to produce\ninterpretable predictions. The model parameters can be learned via maximum\nlikelihood, and the method can be adapted to any predictor network architecture\nand any type of prediction problem. Our method is a case of amortized\ninterpretability models, where a neural network is used as a selector to allow\nfor fast interpretation at inference time. Several popular interpretability\nmethods are shown to be particular cases of regularised maximum likelihood for\nour general model. We propose new datasets with ground truth selection which\nallow for the evaluation of the features importance map. Using these datasets,\nwe show experimentally that using multiple imputation provides more reasonable\ninterpretations."}, "http://arxiv.org/abs/2302.08151": {"title": "Towards a universal representation of statistical dependence", "link": "http://arxiv.org/abs/2302.08151", "description": "Dependence is undoubtedly a central concept in statistics. Though, it proves\ndifficult to locate in the literature a formal definition which goes beyond the\nself-evident 'dependence = non-independence'. This absence has allowed the term\n'dependence' and its declination to be used vaguely and indiscriminately for\nqualifying a variety of disparate notions, leading to numerous incongruities.\nFor example, the classical Pearson's, Spearman's or Kendall's correlations are\nwidely regarded as 'dependence measures' of major interest, in spite of\nreturning 0 in some cases of deterministic relationships between the variables\nat play, evidently not measuring dependence at all. Arguing that research on\nsuch a fundamental topic would benefit from a slightly more rigid framework,\nthis paper suggests a general definition of the dependence between two random\nvariables defined on the same probability space. Natural enough for aligning\nwith intuition, that definition is still sufficiently precise for allowing\nunequivocal identification of a 'universal' representation of the dependence\nstructure of any bivariate distribution. Links between this representation and\nfamiliar concepts are highlighted, and ultimately, the idea of a dependence\nmeasure based on that universal representation is explored and shown to satisfy\nRenyi's postulates."}, "http://arxiv.org/abs/2305.00349": {"title": "Causal effects of intervening variables in settings with unmeasured confounding", "link": "http://arxiv.org/abs/2305.00349", "description": "We present new results on average causal effects in settings with unmeasured\nexposure-outcome confounding. Our results are motivated by a class of\nestimands, e.g., frequently of interest in medicine and public health, that are\ncurrently not targeted by standard approaches for average causal effects. We\nrecognize these estimands as queries about the average causal effect of an\nintervening variable. We anchor our introduction of these estimands in an\ninvestigation of the role of chronic pain and opioid prescription patterns in\nthe opioid epidemic, and illustrate how conventional approaches will lead\nunreplicable estimates with ambiguous policy implications. We argue that our\naltenative effects are replicable and have clear policy implications, and\nfurthermore are non-parametrically identified by the classical frontdoor\nformula. As an independent contribution, we derive a new semiparametric\nefficient estimator of the frontdoor formula with a uniform sample boundedness\nguarantee. This property is unique among previously-described estimators in its\nclass, and we demonstrate superior performance in finite-sample settings.\nTheoretical results are applied with data from the National Health and\nNutrition Examination Survey."}, "http://arxiv.org/abs/2305.05330": {"title": "Point and probabilistic forecast reconciliation for general linearly constrained multiple time series", "link": "http://arxiv.org/abs/2305.05330", "description": "Forecast reconciliation is the post-forecasting process aimed to revise a set\nof incoherent base forecasts into coherent forecasts in line with given data\nstructures. Most of the point and probabilistic regression-based forecast\nreconciliation results ground on the so called \"structural representation\" and\non the related unconstrained generalized least squares reconciliation formula.\nHowever, the structural representation naturally applies to genuine\nhierarchical/grouped time series, where the top- and bottom-level variables are\nuniquely identified. When a general linearly constrained multiple time series\nis considered, the forecast reconciliation is naturally expressed according to\na projection approach. While it is well known that the classic structural\nreconciliation formula is equivalent to its projection approach counterpart, so\nfar it is not completely understood if and how a structural-like reconciliation\nformula may be derived for a general linearly constrained multiple time series.\nSuch an expression would permit to extend reconciliation definitions, theorems\nand results in a straightforward manner. In this paper, we show that for\ngeneral linearly constrained multiple time series it is possible to express the\nreconciliation formula according to a \"structural-like\" approach that keeps\ndistinct free and constrained, instead of bottom and upper (aggregated),\nvariables, establish the probabilistic forecast reconciliation framework, and\napply these findings to obtain fully reconciled point and probabilistic\nforecasts for the aggregates of the Australian GDP from income and expenditure\nsides, and for the European Area GDP disaggregated by income, expenditure and\noutput sides and by 19 countries."}, "http://arxiv.org/abs/2312.15032": {"title": "Combining support for hypotheses over heterogeneous studies with Bayesian Evidence Synthesis: A simulation study", "link": "http://arxiv.org/abs/2312.15032", "description": "Scientific claims gain credibility by replicability, especially if\nreplication under different circumstances and varying designs yields equivalent\nresults. Aggregating results over multiple studies is, however, not\nstraightforward, and when the heterogeneity between studies increases,\nconventional methods such as (Bayesian) meta-analysis and Bayesian sequential\nupdating become infeasible. *Bayesian Evidence Synthesis*, built upon the\nfoundations of the Bayes factor, allows to aggregate support for conceptually\nsimilar hypotheses over studies, regardless of methodological differences. We\nassess the performance of Bayesian Evidence Synthesis over multiple effect and\nsample sizes, with a broad set of (inequality-constrained) hypotheses using\nMonte Carlo simulations, focusing explicitly on the complexity of the\nhypotheses under consideration. The simulations show that this method can\nevaluate complex (informative) hypotheses regardless of methodological\ndifferences between studies, and performs adequately if the set of studies\nconsidered has sufficient statistical power. Additionally, we pinpoint\nchallenging conditions that can lead to unsatisfactory results, and provide\nsuggestions on handling these situations. Ultimately, we show that Bayesian\nEvidence Synthesis is a promising tool that can be used when traditional\nresearch synthesis methods are not applicable due to insurmountable\nbetween-study heterogeneity."}, "http://arxiv.org/abs/2312.15055": {"title": "Deep Learning for Efficient GWAS Feature Selection", "link": "http://arxiv.org/abs/2312.15055", "description": "Genome-Wide Association Studies (GWAS) face unique challenges in the era of\nbig genomics data, particularly when dealing with ultra-high-dimensional\ndatasets where the number of genetic features significantly exceeds the\navailable samples. This paper introduces an extension to the feature selection\nmethodology proposed by Mirzaei et al. (2020), specifically tailored to tackle\nthe intricacies associated with ultra-high-dimensional GWAS data. Our extended\napproach enhances the original method by introducing a Frobenius norm penalty\ninto the student network, augmenting its capacity to adapt to scenarios\ncharacterized by a multitude of features and limited samples. Operating\nseamlessly in both supervised and unsupervised settings, our method employs two\nkey neural networks. The first leverages an autoencoder or supervised\nautoencoder for dimension reduction, extracting salient features from the\nultra-high-dimensional genomic data. The second network, a regularized\nfeed-forward model with a single hidden layer, is designed for precise feature\nselection. The introduction of the Frobenius norm penalty in the student\nnetwork significantly boosts the method's resilience to the challenges posed by\nultra-high-dimensional GWAS datasets. Experimental results showcase the\nefficacy of our approach in feature selection for GWAS data. The method not\nonly handles the inherent complexities of ultra-high-dimensional settings but\nalso demonstrates superior adaptability to the nuanced structures present in\ngenomics data. The flexibility and versatility of our proposed methodology are\nunderscored by its successful performance across a spectrum of experiments."}, "http://arxiv.org/abs/2312.15079": {"title": "Invariance-based Inference in High-Dimensional Regression with Finite-Sample Guarantees", "link": "http://arxiv.org/abs/2312.15079", "description": "In this paper, we develop invariance-based procedures for testing and\ninference in high-dimensional regression models. These procedures, also known\nas randomization tests, provide several important advantages. First, for the\nglobal null hypothesis of significance, our test is valid in finite samples. It\nis also simple to implement and comes with finite-sample guarantees on\nstatistical power. Remarkably, despite its simplicity, this testing idea has\nescaped the attention of earlier analytical work, which mainly concentrated on\ncomplex high-dimensional asymptotic methods. Under an additional assumption of\nGaussian design, we show that this test also achieves the minimax optimal rate\nagainst certain nonsparse alternatives, a type of result that is rare in the\nliterature. Second, for partial null hypotheses, we propose residual-based\ntests and derive theoretical conditions for their validity. These tests can be\nmade powerful by constructing the test statistic in a way that, first, selects\nthe important covariates (e.g., through Lasso) and then orthogonalizes the\nnuisance parameters. We illustrate our results through extensive simulations\nand applied examples. One consistent finding is that the strong finite-sample\nguarantees associated with our procedures result in added robustness when it\ncomes to handling multicollinearity and heavy-tailed covariates."}, "http://arxiv.org/abs/2312.15179": {"title": "Evaluating District-based Election Surveys with Synthetic Dirichlet Likelihood", "link": "http://arxiv.org/abs/2312.15179", "description": "In district-based multi-party elections, electors cast votes in their\nrespective districts. In each district, the party with maximum votes wins the\ncorresponding seat in the governing body. Election Surveys try to predict the\nelection outcome (vote shares and seat shares of parties) by querying a random\nsample of electors. However, the survey results are often inconsistent with the\nactual results, which could be due to multiple reasons. The aim of this work is\nto estimate a posterior distribution over the possible outcomes of the\nelection, given one or more survey results. This is achieved using a prior\ndistribution over vote shares, election models to simulate the complete\nelection from the vote share, and survey models to simulate survey results from\na complete election. The desired posterior distribution over the space of\npossible outcomes is constructed using Synthetic Dirichlet Likelihoods, whose\nparameters are estimated from Monte Carlo sampling of elections using the\nelection models. We further show the same approach can also use be used to\nevaluate the surveys - whether they were biased or not, based on the true\noutcome once it is known. Our work offers the first-ever probabilistic model to\nanalyze district-based election surveys. We illustrate our approach with\nextensive experiments on real and simulated data of district-based political\nelections in India."}, "http://arxiv.org/abs/2312.15205": {"title": "X-Vine Models for Multivariate Extremes", "link": "http://arxiv.org/abs/2312.15205", "description": "Regular vine sequences permit the organisation of variables in a random\nvector along a sequence of trees. Regular vine models have become greatly\npopular in dependence modelling as a way to combine arbitrary bivariate copulas\ninto higher-dimensional ones, offering flexibility, parsimony, and\ntractability. In this project, we use regular vine structures to decompose and\nconstruct the exponent measure density of a multivariate extreme value\ndistribution, or, equivalently, the tail copula density. Although these\ndensities pose theoretical challenges due to their infinite mass, their\nhomogeneity property offers simplifications. The theory sheds new light on\nexisting parametric families and facilitates the construction of new ones,\ncalled X-vines. Computations proceed via recursive formulas in terms of\nbivariate model components. We develop simulation algorithms for X-vine\nmultivariate Pareto distributions as well as methods for parameter estimation\nand model selection on the basis of threshold exceedances. The methods are\nillustrated by Monte Carlo experiments and a case study on US flight delay\ndata."}, "http://arxiv.org/abs/2312.15217": {"title": "Constructing a T-test for Value Function Comparison of Individualized Treatment Regimes in the Presence of Multiple Imputation for Missing Data", "link": "http://arxiv.org/abs/2312.15217", "description": "Optimal individualized treatment decision-making has improved health outcomes\nin recent years. The value function is commonly used to evaluate the goodness\nof an individualized treatment decision rule. Despite recent advances,\ncomparing value functions between different treatment decision rules or\nconstructing confidence intervals around value functions remains difficult. We\npropose a t-test based method applied to a test set that generates valid\np-values to compare value functions between a given pair of treatment decision\nrules when some of the data are missing. We demonstrate the ease in use of this\nmethod and evaluate its performance via simulation studies and apply it to the\nChina Health and Nutrition Survey data."}, "http://arxiv.org/abs/2312.15222": {"title": "Towards reaching a consensus in Bayesian trial designs: the case of 2-arm trials", "link": "http://arxiv.org/abs/2312.15222", "description": "Practical employment of Bayesian trial designs has been rare, in part due to\nthe regulators' requirement to calibrate such designs with an upper bound for\nType 1 error rate. This has led to an internally inconsistent hybrid\nmethodology, where important advantages from applying the Bayesian principles\nare lost. To present an alternative approach, we consider the prototype case of\na 2-arm superiority trial with binary outcomes. The design is adaptive,\napplying block randomization for treatment assignment and using error tolerance\ncriteria based on sequentially updated posterior probabilities, to conclude\nefficacy of the experimental treatment or futility of the trial. The method\nalso contains the option of applying a decision rule for terminating the trial\nearly if the predicted costs from continuing would exceed the corresponding\ngains. We propose that the control of Type 1 error rate be replaced by a\ncontrol of false discovery rate (FDR), a concept that lends itself to both\nfrequentist and Bayesian interpretations. Importantly, if the regulators and\nthe investigators can agree on a consensus distribution to represent their\nshared understanding on the effect sizes, the selected level of risk tolerance\nagainst false conclusions during the data analysis will also be a valid bound\nfor the FDR. The approach can lower the ultimately unnecessary barriers from\nthe practical application of Bayesian trial designs. This can lead to more\nflexible experimental protocols and more efficient use of trial data while\nstill effectively guarding against falsely claimed discoveries."}, "http://arxiv.org/abs/2312.15373": {"title": "A Multi-day Needs-based Modeling Approach for Activity and Travel Demand Analysis", "link": "http://arxiv.org/abs/2312.15373", "description": "This paper proposes a multi-day needs-based model for activity and travel\ndemand analysis. The model captures the multi-day dynamics in activity\ngeneration, which enables the modeling of activities with increased flexibility\nin time and space (e.g., e-commerce and remote working). As an enhancement to\nactivity-based models, the proposed model captures the underlying\ndecision-making process of activity generation by accounting for psychological\nneeds as the drivers of activities. The level of need satisfaction is modeled\nas a psychological inventory, whose utility is optimized via decisions on\nactivity participation, location, and duration. The utility includes both the\nbenefit in the inventory gained and the cost in time, monetary expense as well\nas maintenance of safety stock. The model includes two sub-models, a\nDeterministic Model that optimizes the utility of the inventory, and an\nEmpirical Model that accounts for heterogeneity and stochasticity. Numerical\nexperiments are conducted to demonstrate model scalability. A maximum\nlikelihood estimator is proposed, the properties of the log-likelihood function\nare examined and the recovery of true parameters is tested. This research\ncontributes to the literature on transportation demand models in the following\nthree aspects. First, it is arguably better grounded in psychological theory\nthan traditional models and allows the generation of activity patterns to be\npolicy-sensitive (while avoiding the need for ad hoc utility definitions).\nSecond, it contributes to the development of needs-based models with a\nnon-myopic approach to model multi-day activity patterns. Third, it proposes a\ntractable model formulation via problem reformulation and computational\nenhancements, which allows for maximum likelihood parameter estimation."}, "http://arxiv.org/abs/2312.15376": {"title": "Geodesic Optimal Transport Regression", "link": "http://arxiv.org/abs/2312.15376", "description": "Classical regression models do not cover non-Euclidean data that reside in a\ngeneral metric space, while the current literature on non-Euclidean regression\nby and large has focused on scenarios where either predictors or responses are\nrandom objects, i.e., non-Euclidean, but not both. In this paper we propose\ngeodesic optimal transport regression models for the case where both predictors\nand responses lie in a common geodesic metric space and predictors may include\nnot only one but also several random objects. This provides an extension of\nclassical multiple regression to the case where both predictors and responses\nreside in non-Euclidean metric spaces, a scenario that has not been considered\nbefore. It is based on the concept of optimal geodesic transports, which we\ndefine as an extension of the notion of optimal transports in distribution\nspaces to more general geodesic metric spaces, where we characterize optimal\ntransports as transports along geodesics. The proposed regression models cover\nthe relation between non-Euclidean responses and vectors of non-Euclidean\npredictors in many spaces of practical statistical interest. These include\none-dimensional distributions viewed as elements of the 2-Wasserstein space and\nmultidimensional distributions with the Fisher-Rao metric that are represented\nas data on the Hilbert sphere. Also included are data on finite-dimensional\nRiemannian manifolds, with an emphasis on spheres, covering directional and\ncompositional data, as well as data that consist of symmetric positive definite\nmatrices. We illustrate the utility of geodesic optimal transport regression\nwith data on summer temperature distributions and human mortality."}, "http://arxiv.org/abs/2312.15396": {"title": "PKBOIN-12: A Bayesian optimal interval Phase I/II design incorporating pharmacokinetics outcomes to find the optimal biological dose", "link": "http://arxiv.org/abs/2312.15396", "description": "Immunotherapies and targeted therapies have gained popularity due to their\npromising therapeutic effects across multiple treatment areas. The focus of\nearly phase dose-finding clinical trials has shifted from finding the maximum\ntolerated dose (MTD) to identifying the optimal biological dose (OBD), which\naims to balance the toxicity and efficacy outcomes, thereby optimizing the\nrisk-benefit trade-off. These trials often collect multiple pharmacokinetics\n(PK) outcomes to assess drug exposure, which has shown correlations with\ntoxicity and efficacy outcomes but has not been utilized in the current\ndose-finding designs for OBD selection. Moreover, PK outcomes are usually\navailable within days after initial treatment, much faster than toxicity and\nefficacy outcomes. To bridge this gap, we introduce the innovative\nmodel-assisted PKBOIN-12 design, which enhances the BOIN12 design by\nintegrating PK information into both the dose-finding algorithm and the final\nOBD determination process. We further extend PKBOIN-12 to the TITE-PKBOIN-12\ndesign to address the challenges of late-onset toxicity and efficacy outcomes.\nSimulation results demonstrate that the PKBOIN-12 design more effectively\nidentifies the OBD and allocates a greater number of patients to it than\nBOIN12. Additionally, PKBOIN-12 decreases the probability of selecting\ninefficacious doses as the OBD by excluding those with low drug exposure.\nComprehensive simulation studies and sensitivity analysis confirm the\nrobustness of both PKBOIN-12 and TITE-PKBOIN-12 designs in various scenarios."}, "http://arxiv.org/abs/2312.15469": {"title": "Efficient Estimation of the Central Mean Subspace via Smoothed Gradient Outer Products", "link": "http://arxiv.org/abs/2312.15469", "description": "We consider the problem of sufficient dimension reduction (SDR) for\nmulti-index models. The estimators of the central mean subspace in prior works\neither have slow (non-parametric) convergence rates, or rely on stringent\ndistributional conditions (e.g., the covariate distribution $P_{\\mathbf{X}}$\nbeing elliptical symmetric). In this paper, we show that a fast parametric\nconvergence rate of form $C_d \\cdot n^{-1/2}$ is achievable via estimating the\n\\emph{expected smoothed gradient outer product}, for a general class of\ndistribution $P_{\\mathbf{X}}$ admitting Gaussian or heavier distributions. When\nthe link function is a polynomial with a degree of at most $r$ and\n$P_{\\mathbf{X}}$ is the standard Gaussian, we show that the prefactor depends\non the ambient dimension $d$ as $C_d \\propto d^r$."}, "http://arxiv.org/abs/2312.15496": {"title": "A Simple Bias Reduction for Chatterjee's Correlation", "link": "http://arxiv.org/abs/2312.15496", "description": "Chatterjee's rank correlation coefficient $\\xi_n$ is an empirical index for\ndetecting functional dependencies between two variables $X$ and $Y$. It is an\nestimator for a theoretical quantity $\\xi$ that is zero for independence and\none if $Y$ is a measurable function of $X$. Based on an equivalent\ncharacterization of sorted numbers, we derive an upper bound for $\\xi_n$ and\nsuggest a simple normalization aimed at reducing its bias for small sample size\n$n$. In Monte Carlo simulations of various cases, the normalization reduced the\nbias in all cases. The mean squared error was reduced, too, for values of $\\xi$\ngreater than about 0.4. Moreover, we observed that confidence intervals for\n$\\xi$ based on bootstrapping $\\xi_n$ in the usual n-out-of-n way have a\ncoverage probability close to zero. This is remedied by an m-out-of-n bootstrap\nwithout replacement in combination with our normalization method."}, "http://arxiv.org/abs/2312.15611": {"title": "Inference of Dependency Knowledge Graph for Electronic Health Records", "link": "http://arxiv.org/abs/2312.15611", "description": "The effective analysis of high-dimensional Electronic Health Record (EHR)\ndata, with substantial potential for healthcare research, presents notable\nmethodological challenges. Employing predictive modeling guided by a knowledge\ngraph (KG), which enables efficient feature selection, can enhance both\nstatistical efficiency and interpretability. While various methods have emerged\nfor constructing KGs, existing techniques often lack statistical certainty\nconcerning the presence of links between entities, especially in scenarios\nwhere the utilization of patient-level EHR data is limited due to privacy\nconcerns. In this paper, we propose the first inferential framework for\nderiving a sparse KG with statistical guarantee based on the dynamic log-linear\ntopic model proposed by \\cite{arora2016latent}. Within this model, the KG\nembeddings are estimated by performing singular value decomposition on the\nempirical pointwise mutual information matrix, offering a scalable solution. We\nthen establish entrywise asymptotic normality for the KG low-rank estimator,\nenabling the recovery of sparse graph edges with controlled type I error. Our\nwork uniquely addresses the under-explored domain of statistical inference\nabout non-linear statistics under the low-rank temporal dependent models, a\ncritical gap in existing research. We validate our approach through extensive\nsimulation studies and then apply the method to real-world EHR data in\nconstructing clinical KGs and generating clinical feature embeddings."}, "http://arxiv.org/abs/2312.15624": {"title": "Negative Controls for Instrumental Variable Designs", "link": "http://arxiv.org/abs/2312.15624", "description": "Studies using instrumental variables (IV) often assess the validity of their\nidentification assumptions using falsification tests. However, these tests are\noften carried out in an ad-hoc manner, without theoretical foundations. In this\npaper, we establish a theoretical framework for negative control tests, the\npredominant category of falsification tests for IV designs. These tests are\nconditional independence tests between negative control variables and either\nthe IV or the outcome (e.g., examining the ``effect'' on the lagged outcome).\nWe introduce a formal definition for threats to IV exogeneity (alternative path\nvariables) and characterize the necessary conditions that proxy variables for\nsuch unobserved threats must meet to serve as negative controls. The theory\nhighlights prevalent errors in the implementation of negative control tests and\nhow they could be corrected. Our theory can also be used to design new\nfalsification tests by identifying appropriate negative control variables,\nincluding currently underutilized types, and suggesting alternative statistical\ntests. The theory shows that all negative control tests assess IV exogeneity.\nHowever, some commonly used tests simultaneously evaluate the 2SLS functional\nform assumptions. Lastly, we show that while negative controls are useful for\ndetecting biases in IV designs, their capacity to correct or quantify such\nbiases requires additional non-trivial assumptions."}, "http://arxiv.org/abs/2312.15781": {"title": "A Computational Note on the Graphical Ridge in High-dimension", "link": "http://arxiv.org/abs/2312.15781", "description": "This article explores the estimation of precision matrices in\nhigh-dimensional Gaussian graphical models. We address the challenge of\nimproving the accuracy of maximum likelihood-based precision estimation through\npenalization. Specifically, we consider an elastic net penalty, which\nincorporates both L1 and Frobenius norm penalties while accounting for the\ntarget matrix during estimation. To enhance precision matrix estimation, we\npropose a novel two-step estimator that combines the strengths of ridge and\ngraphical lasso estimators. Through this approach, we aim to improve overall\nestimation performance. Our empirical analysis demonstrates the superior\nefficiency of our proposed method compared to alternative approaches. We\nvalidate the effectiveness of our proposal through numerical experiments and\napplication on three real datasets. These examples illustrate the practical\napplicability and usefulness of our proposed estimator."}, "http://arxiv.org/abs/2312.15919": {"title": "Review on Causality Detection Based on Empirical Dynamic Modeling", "link": "http://arxiv.org/abs/2312.15919", "description": "In contemporary scientific research, understanding the distinction between\ncorrelation and causation is crucial. While correlation is a widely used\nanalytical standard, it does not inherently imply causation. This paper\naddresses the potential for misinterpretation in relying solely on correlation,\nespecially in the context of nonlinear dynamics. Despite the rapid development\nof various correlation research methodologies, including machine learning, the\nexploration into mining causal correlations between variables remains ongoing.\nEmpirical Dynamic Modeling (EDM) emerges as a data-driven framework for\nmodeling dynamic systems, distinguishing itself by eschewing traditional\nformulaic methods in data analysis. Instead, it reconstructs dynamic system\nbehavior directly from time series data. The fundamental premise of EDM is that\ndynamic systems can be conceptualized as processes where a set of states,\ngoverned by specific rules, evolve over time in a high-dimensional space. By\nreconstructing these evolving states, dynamic systems can be effectively\nmodeled. Using EDM, this paper explores the detection of causal relationships\nbetween variables within dynamic systems through their time series data. It\nposits that if variable X causes variable Y, then the information about X is\ninherent in Y and can be extracted from Y's data. This study begins by\nexamining the dialectical relationship between correlation and causation,\nemphasizing that correlation does not equate to causation, and the absence of\ncorrelation does not necessarily indicate a lack of causation."}, "http://arxiv.org/abs/2312.16037": {"title": "Critical nonlinear aspects of hopping transport for reconfigurable logic in disordered dopant networks", "link": "http://arxiv.org/abs/2312.16037", "description": "Nonlinear behavior in the hopping transport of interacting charges enables\nreconfigurable logic in disordered dopant network devices, where voltages\napplied at control electrodes tune the relation between voltages applied at\ninput electrodes and the current measured at an output electrode. From kinetic\nMonte Carlo simulations we analyze the critical nonlinear aspects of\nvariable-range hopping transport for realizing Boolean logic gates in these\ndevices on three levels. First, we quantify the occurrence of individual gates\nfor random choices of control voltages. We find that linearly inseparable gates\nsuch as the XOR gate are less likely to occur than linearly separable gates\nsuch as the AND gate, despite the fact that the number of different regions in\nthe multidimensional control voltage space for which AND or XOR gates occur is\ncomparable. Second, we use principal component analysis to characterize the\ndistribution of the output current vectors for the (00,10,01,11) logic input\ncombinations in terms of eigenvectors and eigenvalues of the output covariance\nmatrix. This allows a simple and direct comparison of the behavior of different\nsimulated devices and a comparison to experimental devices. Third, we quantify\nthe nonlinearity in the distribution of the output current vectors necessary\nfor realizing Boolean functionality by introducing three nonlinearity\nindicators. The analysis provides a physical interpretation of the effects of\nchanging the hopping distance and temperature and is used in a comparison with\ndata generated by a deep neural network trained on a physical device."}, "http://arxiv.org/abs/2312.16139": {"title": "Anomaly component analysis", "link": "http://arxiv.org/abs/2312.16139", "description": "At the crossway of machine learning and data analysis, anomaly detection aims\nat identifying observations that exhibit abnormal behaviour. Be it measurement\nerrors, disease development, severe weather, production quality default(s)\n(items) or failed equipment, financial frauds or crisis events, their on-time\nidentification and isolation constitute an important task in almost any area of\nindustry and science. While a substantial body of literature is devoted to\ndetection of anomalies, little attention is payed to their explanation. This is\nthe case mostly due to intrinsically non-supervised nature of the task and\nnon-robustness of the exploratory methods like principal component analysis\n(PCA).\n\nWe introduce a new statistical tool dedicated for exploratory analysis of\nabnormal observations using data depth as a score. Anomaly component analysis\n(shortly ACA) is a method that searches a low-dimensional data representation\nthat best visualises and explains anomalies. This low-dimensional\nrepresentation not only allows to distinguish groups of anomalies better than\nthe methods of the state of the art, but as well provides a -- linear in\nvariables and thus easily interpretable -- explanation for anomalies. In a\ncomparative simulation and real-data study, ACA also proves advantageous for\nanomaly analysis with respect to methods present in the literature."}, "http://arxiv.org/abs/2312.16160": {"title": "SymmPI: Predictive Inference for Data with Group Symmetries", "link": "http://arxiv.org/abs/2312.16160", "description": "Quantifying the uncertainty of predictions is a core problem in modern\nstatistics. Methods for predictive inference have been developed under a\nvariety of assumptions, often -- for instance, in standard conformal prediction\n-- relying on the invariance of the distribution of the data under special\ngroups of transformations such as permutation groups. Moreover, many existing\nmethods for predictive inference aim to predict unobserved outcomes in\nsequences of feature-outcome observations. Meanwhile, there is interest in\npredictive inference under more general observation models (e.g., for partially\nobserved features) and for data satisfying more general distributional\nsymmetries (e.g., rotationally invariant or coordinate-independent observations\nin physics). Here we propose SymmPI, a methodology for predictive inference\nwhen data distributions have general group symmetries in arbitrary observation\nmodels. Our methods leverage the novel notion of distributional equivariant\ntransformations, which process the data while preserving their distributional\ninvariances. We show that SymmPI has valid coverage under distributional\ninvariance and characterize its performance under distribution shift,\nrecovering recent results as special cases. We apply SymmPI to predict\nunobserved values associated to vertices in a network, where the distribution\nis unchanged under relabelings that keep the network structure unchanged. In\nseveral simulations in a two-layer hierarchical model, and in an empirical data\nanalysis example, SymmPI performs favorably compared to existing methods."}, "http://arxiv.org/abs/2312.16162": {"title": "Properties of Test Statistics for Nonparametric Cointegrating Regression Functions Based on Subsamples", "link": "http://arxiv.org/abs/2312.16162", "description": "Nonparametric cointegrating regression models have been extensively used in\nfinancial markets, stock prices, heavy traffic, climate data sets, and energy\nmarkets. Models with parametric regression functions can be more appealing in\npractice compared to non-parametric forms, but do result in potential\nfunctional misspecification. Thus, there exists a vast literature on developing\na model specification test for parametric forms of regression functions. In\nthis paper, we develop two test statistics which are applicable for the\nendogenous regressors driven by long memory and semi-long memory input shocks\nin the regression model. The limit distributions of the test statistics under\nthese two scenarios are complicated and cannot be effectively used in practice.\nTo overcome this difficulty, we use the subsampling method and compute the test\nstatistics on smaller blocks of the data to construct their empirical\ndistributions. Throughout, Monte Carlo simulation studies are used to\nillustrate the properties of test statistics. We also provide an empirical\nexample of relating gross domestic product to total output of carbon dioxide in\ntwo European countries."}, "http://arxiv.org/abs/1909.06307": {"title": "Multiscale Jump Testing and Estimation Under Complex Temporal Dynamics", "link": "http://arxiv.org/abs/1909.06307", "description": "We consider the problem of detecting jumps in an otherwise smoothly evolving\ntrend whilst the covariance and higher-order structures of the system can\nexperience both smooth and abrupt changes over time. The number of jump points\nis allowed to diverge to infinity with the jump sizes possibly shrinking to\nzero. The method is based on a multiscale application of an optimal jump-pass\nfilter to the time series, where the scales are dense between admissible lower\nand upper bounds. For a wide class of non-stationary time series models and\ntrend functions, the proposed method is shown to be able to detect all jump\npoints within a nearly optimal range with a prescribed probability\nasymptotically under mild conditions. For a time series of length $n$, the\ncomputational complexity of the proposed method is $O(n)$ for each scale and\n$O(n\\log^{1+\\epsilon} n)$ overall, where $\\epsilon$ is an arbitrarily small\npositive constant. Numerical studies show that the proposed jump testing and\nestimation method performs robustly and accurately under complex temporal\ndynamics."}, "http://arxiv.org/abs/1911.06583": {"title": "GET: Global envelopes in R", "link": "http://arxiv.org/abs/1911.06583", "description": "This work describes the R package GET that implements global envelopes for a\ngeneral set of $d$-dimensional vectors $T$ in various applications. A\n$100(1-\\alpha)$% global envelope is a band bounded by two vectors such that the\nprobability that $T$ falls outside this envelope in any of the $d$ points is\nequal to $\\alpha$. The term 'global' means that this probability is controlled\nsimultaneously for all the $d$ elements of the vectors. The global envelopes\ncan be employed for central regions of functional or multivariate data, for\ngraphical Monte Carlo and permutation tests where the test statistic is\nmultivariate or functional, and for global confidence and prediction bands.\nIntrinsic graphical interpretation property is introduced for global envelopes.\nThe global envelopes included in the GET package have this property, which\nparticularly helps to interpret test results, by providing a graphical\ninterpretation that shows the reasons of rejection of the tested hypothesis.\nExamples of different uses of global envelopes and their implementation in the\nGET package are presented, including global envelopes for single and several\none- or two-dimensional functions, Monte Carlo goodness-of-fit tests for simple\nand composite hypotheses, comparison of distributions, functional analysis of\nvariance, functional linear model, and confidence bands in polynomial\nregression."}, "http://arxiv.org/abs/2007.02192": {"title": "Tail-adaptive Bayesian shrinkage", "link": "http://arxiv.org/abs/2007.02192", "description": "Modern genomic studies are increasingly focused on discovering more and more\ninteresting genes associated with a health response. Traditional shrinkage\npriors are primarily designed to detect a handful of signals from tens of\nthousands of predictors in the so-called ultra-sparsity domain. However, they\nmay fail to identify signals when the degree of sparsity is moderate. Robust\nsparse estimation under diverse sparsity regimes relies on a tail-adaptive\nshrinkage property. In this property, the tail-heaviness of the prior adjusts\nadaptively, becoming larger or smaller as the sparsity level increases or\ndecreases, respectively, to accommodate more or fewer signals. In this study,\nwe propose a global-local-tail (GLT) Gaussian mixture distribution that ensures\nthis property. We examine the role of the tail-index of the prior in relation\nto the underlying sparsity level and demonstrate that the GLT posterior\ncontracts at the minimax optimal rate for sparse normal mean models. We apply\nboth the GLT prior and the Horseshoe prior to real data problems and simulation\nexamples. Our findings indicate that the varying tail rule based on the GLT\nprior offers advantages over a fixed tail rule based on the Horseshoe prior in\ndiverse sparsity regimes."}, "http://arxiv.org/abs/2010.05117": {"title": "Combining Observational and Experimental Data to Improve Efficiency Using Imperfect Instruments", "link": "http://arxiv.org/abs/2010.05117", "description": "Randomized controlled trials generate experimental variation that can\ncredibly identify causal effects, but often suffer from limited scale, while\nobservational datasets are large, but often violate desired identification\nassumptions. To improve estimation efficiency, I propose a method that\nleverages imperfect instruments - pretreatment covariates that satisfy the\nrelevance condition but may violate the exclusion restriction. I show that\nthese imperfect instruments can be used to derive moment restrictions that, in\ncombination with the experimental data, improve estimation efficiency. I\noutline estimators for implementing this strategy, and show that my methods can\nreduce variance by up to 50%; therefore, only half of the experimental sample\nis required to attain the same statistical precision. I apply my method to a\nsearch listing dataset from Expedia that studies the causal effect of search\nrankings on clicks, and show that the method can substantially improve the\nprecision."}, "http://arxiv.org/abs/2011.04833": {"title": "Handling time-dependent exposures and confounders when estimating attributable fractions -- bridging the gap between multistate and counterfactual modeling", "link": "http://arxiv.org/abs/2011.04833", "description": "The population-attributable fraction (PAF) expresses the proportion of events\nthat can be ascribed to a certain exposure in a certain population. It can be\nstrongly time-dependent because either exposure incidence or excess risk may\nchange over time. Competing events may moreover hinder the outcome of interest\nfrom being observed. Occurrence of either of these events may, in turn, prevent\nthe exposure of interest. Estimation approaches therefore need to carefully\naccount for the timing of events in such highly dynamic settings. The use of\nmultistate models has been widely encouraged to eliminate preventable yet\ncommon types of time-dependent bias. Even so, it has been pointed out that\nproposed multistate modeling approaches for PAF estimation fail to fully\neliminate such bias. In addition, assessing whether patients die from rather\nthan with a certain exposure not only requires adequate modeling of the timing\nof events but also of their confounding factors. While proposed multistate\nmodeling approaches for confounding adjustment may adequately accommodate\nbaseline imbalances, unlike g-methods, these proposals are not generally\nequipped to handle time-dependent confounding. However, the connection between\nmultistate modeling and g-methods (e.g. inverse probability of censoring\nweighting) for PAF estimation is not readily apparent. In this paper, we\nprovide a weighting-based characterization of both approaches to illustrate\nthis connection, to pinpoint current shortcomings of multistate modeling, and\nto enhance intuition into simple modifications to overcome these. R code is\nmade available to foster the uptake of g-methods for PAF estimation."}, "http://arxiv.org/abs/2211.00338": {"title": "Typical Yet Unlikely and Normally Abnormal: The Intuition Behind High-Dimensional Statistics", "link": "http://arxiv.org/abs/2211.00338", "description": "Normality, in the colloquial sense, has historically been considered an\naspirational trait, synonymous with ideality. The arithmetic average and, by\nextension, statistics including linear regression coefficients, have often been\nused to characterize normality, and are often used as a way to summarize\nsamples and identify outliers. We provide intuition behind the behavior of such\nstatistics in high dimensions, and demonstrate that even for datasets with a\nrelatively low number of dimensions, data start to exhibit a number of\npeculiarities which become severe as the number of dimensions increases. Whilst\nour main goal is to familiarize researchers with these peculiarities, we also\nshow that normality can be better characterized with `typicality', an\ninformation theoretic concept relating to entropy. An application of typicality\nto both synthetic and real-world data concerning political values reveals that\nin multi-dimensional space, to be `normal' is actually to be atypical. We\nbriefly explore the ramifications for outlier detection, demonstrating how\ntypicality, in contrast with the popular Mahalanobis distance, represents a\nviable method for outlier detection."}, "http://arxiv.org/abs/2212.12940": {"title": "Exact Selective Inference with Randomization", "link": "http://arxiv.org/abs/2212.12940", "description": "We introduce a pivot for exact selective inference with randomization. Not\nonly does our pivot lead to exact inference in Gaussian regression models, but\nit is also available in closed form. We reduce the problem of exact selective\ninference to a bivariate truncated Gaussian distribution. By doing so, we give\nup some power that is achieved with approximate maximum likelihood estimation\nin Panigrahi and Taylor (2022). Yet our pivot always produces narrower\nconfidence intervals than a closely related data splitting procedure. We\ninvestigate the trade-off between power and exact selective inference on\nsimulated datasets and an HIV drug resistance dataset."}, "http://arxiv.org/abs/2301.05580": {"title": "Randomization Test for the Specification of Interference Structure", "link": "http://arxiv.org/abs/2301.05580", "description": "This study considers testing the specification of spillover effects in causal\ninference. We focus on experimental settings in which the treatment assignment\nmechanism is known to researchers. We develop a new randomization test\nutilizing a hierarchical relationship between different exposures. Compared\nwith existing approaches, our approach is essentially applicable to any null\nexposure specifications and produces powerful test statistics without a priori\nknowledge of the true interference structure. As empirical illustrations, we\nrevisit two existing social network experiments: one on farmers' insurance\nadoption and the other on anti-conflict education programs."}, "http://arxiv.org/abs/2309.05025": {"title": "Simulating data from marginal structural models for a survival time outcome", "link": "http://arxiv.org/abs/2309.05025", "description": "Marginal structural models (MSMs) are often used to estimate causal effects\nof treatments on survival time outcomes from observational data when\ntime-dependent confounding may be present. They can be fitted using, e.g.,\ninverse probability of treatment weighting (IPTW). It is important to evaluate\nthe performance of statistical methods in different scenarios, and simulation\nstudies are a key tool for such evaluations. In such simulation studies, it is\ncommon to generate data in such a way that the model of interest is correctly\nspecified, but this is not always straightforward when the model of interest is\nfor potential outcomes, as is an MSM. Methods have been proposed for simulating\nfrom MSMs for a survival outcome, but these methods impose restrictions on the\ndata-generating mechanism. Here we propose a method that overcomes these\nrestrictions. The MSM can be a marginal structural logistic model for a\ndiscrete survival time or a Cox or additive hazards MSM for a continuous\nsurvival time. The hazard of the potential survival time can be conditional on\nbaseline covariates, and the treatment variable can be discrete or continuous.\nWe illustrate the use of the proposed simulation algorithm by carrying out a\nbrief simulation study. This study compares the coverage of confidence\nintervals calculated in two different ways for causal effect estimates obtained\nby fitting an MSM via IPTW."}, "http://arxiv.org/abs/2312.16177": {"title": "Learning to Infer Unobserved Behaviors: Estimating User's Preference for a Site over Other Sites", "link": "http://arxiv.org/abs/2312.16177", "description": "A site's recommendation system relies on knowledge of its users' preferences\nto offer relevant recommendations to them. These preferences are for attributes\nthat comprise items and content shown on the site, and are estimated from the\ndata of users' interactions with the site. Another form of users' preferences\nis material too, namely, users' preferences for the site over other sites,\nsince that shows users' base level propensities to engage with the site.\nEstimating users' preferences for the site, however, faces major obstacles\nbecause (a) the focal site usually has no data of its users' interactions with\nother sites; these interactions are users' unobserved behaviors for the focal\nsite; and (b) the Machine Learning literature in recommendation does not offer\na model of this situation. Even if (b) is resolved, the problem in (a) persists\nsince without access to data of its users' interactions with other sites, there\nis no ground truth for evaluation. Moreover, it is most useful when (c) users'\npreferences for the site can be estimated at the individual level, since the\nsite can then personalize recommendations to individual users. We offer a\nmethod to estimate individual user's preference for a focal site, under this\npremise. In particular, we compute the focal site's share of a user's online\nengagements without any data from other sites. We show an evaluation framework\nfor the model using only the focal site's data, allowing the site to test the\nmodel. We rely upon a Hierarchical Bayes Method and perform estimation in two\ndifferent ways - Markov Chain Monte Carlo and Stochastic Gradient with Langevin\nDynamics. Our results find good support for the approach to computing\npersonalized share of engagement and for its evaluation."}, "http://arxiv.org/abs/2312.16188": {"title": "The curious case of the test set AUROC", "link": "http://arxiv.org/abs/2312.16188", "description": "Whilst the size and complexity of ML models have rapidly and significantly\nincreased over the past decade, the methods for assessing their performance\nhave not kept pace. In particular, among the many potential performance\nmetrics, the ML community stubbornly continues to use (a) the area under the\nreceiver operating characteristic curve (AUROC) for a validation and test\ncohort (distinct from training data) or (b) the sensitivity and specificity for\nthe test data at an optimal threshold determined from the validation ROC.\nHowever, we argue that considering scores derived from the test ROC curve alone\ngives only a narrow insight into how a model performs and its ability to\ngeneralise."}, "http://arxiv.org/abs/2312.16241": {"title": "Analysis of Pleiotropy for Testosterone and Lipid Profiles in Males and Females", "link": "http://arxiv.org/abs/2312.16241", "description": "In modern scientific studies, it is often imperative to determine whether a\nset of phenotypes is affected by a single factor. If such an influence is\nidentified, it becomes essential to discern whether this effect is contingent\nupon categories such as sex or age group, and importantly, to understand\nwhether this dependence is rooted in purely non-environmental reasons. The\nexploration of such dependencies often involves studying pleiotropy, a\nphenomenon wherein a single genetic locus impacts multiple traits. This\nheightened interest in uncovering dependencies by pleiotropy is fueled by the\ngrowing accessibility of summary statistics from genome-wide association\nstudies (GWAS) and the establishment of thoroughly phenotyped sample\ncollections. This advancement enables a systematic and comprehensive\nexploration of the genetic connections among various traits and diseases.\nadditive genetic correlation illuminates the genetic connection between two\ntraits, providing valuable insights into the shared biological pathways and\nunderlying causal relationships between them. In this paper, we present a novel\nmethod to analyze such dependencies by studying additive genetic correlations\nbetween pairs of traits under consideration. Subsequently, we employ matrix\ncomparison techniques to discern and elucidate sex-specific or\nage-group-specific associations, contributing to a deeper understanding of the\nnuanced dependencies within the studied traits. Our proposed method is\ncomputationally handy and requires only GWAS summary statistics. We validate\nour method by applying it to the UK Biobank data and present the results."}, "http://arxiv.org/abs/2312.16260": {"title": "Multinomial Link Models", "link": "http://arxiv.org/abs/2312.16260", "description": "We propose a unified multinomial link model for analyzing categorical\nresponses. It not only covers the existing multinomial logistic models and\ntheir extensions as a special class, but also allows the observations with NA\nor Unknown responses to be incorporated as a special category in the data\nanalysis. We provide explicit formulae for computing the likelihood gradient\nand Fisher information matrix, as well as detailed algorithms for finding the\nmaximum likelihood estimates of the model parameters. Our algorithms solve the\ninfeasibility issue of existing statistical software on estimating parameters\nof cumulative link models. The applications to real datasets show that the\nproposed multinomial link models can fit the data significantly better, and the\ncorresponding data analysis may correct the misleading conclusions due to\nmissing data."}, "http://arxiv.org/abs/2312.16307": {"title": "Incentive-Aware Synthetic Control: Accurate Counterfactual Estimation via Incentivized Exploration", "link": "http://arxiv.org/abs/2312.16307", "description": "We consider a panel data setting in which one observes measurements of units\nover time, under different interventions. Our focus is on the canonical family\nof synthetic control methods (SCMs) which, after a pre-intervention time period\nwhen all units are under control, estimate counterfactual outcomes for test\nunits in the post-intervention time period under control by using data from\ndonor units who have remained under control for the entire post-intervention\nperiod. In order for the counterfactual estimate produced by synthetic control\nfor a test unit to be accurate, there must be sufficient overlap between the\noutcomes of the donor units and the outcomes of the test unit. As a result, a\ncanonical assumption in the literature on SCMs is that the outcomes for the\ntest units lie within either the convex hull or the linear span of the outcomes\nfor the donor units. However despite their ubiquity, such overlap assumptions\nmay not always hold, as is the case when, e.g., units select their own\ninterventions and different subpopulations of units prefer different\ninterventions a priori.\n\nWe shed light on this typically overlooked assumption, and we address this\nissue by incentivizing units with different preferences to take interventions\nthey would not normally consider. Specifically, we provide a SCM for\nincentivizing exploration in panel data settings which provides\nincentive-compatible intervention recommendations to units by leveraging tools\nfrom information design and online learning. Using our algorithm, we show how\nto obtain valid counterfactual estimates using SCMs without the need for an\nexplicit overlap assumption on the unit outcomes."}, "http://arxiv.org/abs/2312.16357": {"title": "Statistical monitoring of European cross-border physical electricity flows using novel temporal edge network processes", "link": "http://arxiv.org/abs/2312.16357", "description": "Conventional modelling of networks evolving in time focuses on capturing\nvariations in the network structure. However, the network might be static from\nthe origin or experience only deterministic, regulated changes in its\nstructure, providing either a physical infrastructure or a specified connection\narrangement for some other processes. Thus, to detect change in its\nexploitation, we need to focus on the processes happening on the network. In\nthis work, we present the concept of monitoring random Temporal Edge Network\n(TEN) processes that take place on the edges of a graph having a fixed\nstructure. Our framework is based on the Generalized Network Autoregressive\nstatistical models with time-dependent exogenous variables (GNARX models) and\nCumulative Sum (CUSUM) control charts. To demonstrate its effective detection\nof various types of change, we conduct a simulation study and monitor the\nreal-world data of cross-border physical electricity flows in Europe."}, "http://arxiv.org/abs/2312.16439": {"title": "Inferring the Effect of a Confounded Treatment by Calibrating Resistant Population's Variance", "link": "http://arxiv.org/abs/2312.16439", "description": "In a general set-up that allows unmeasured confounding, we show that the\nconditional average treatment effect on the treated can be identified as one of\ntwo possible values. Unlike existing causal inference methods, we do not\nrequire an exogenous source of variability in the treatment, e.g., an\ninstrument or another outcome unaffected by the treatment. Instead, we require\n(a) a nondeterministic treatment assignment, (b) that conditional variances of\nthe two potential outcomes are equal in the treatment group, and (c) a\nresistant population that was not exposed to the treatment or, if exposed, is\nunaffected by the treatment. Assumption (a) is commonly assumed in theoretical\nwork, while (b) holds under fairly general outcome models. For (c), which is a\nnew assumption, we show that a resistant population is often available in\npractice. We develop a large sample inference methodology and demonstrate our\nproposed method in a study of the effect of surface mining in central\nAppalachia on birth weight that finds a harmful effect."}, "http://arxiv.org/abs/2312.16489": {"title": "Best-of-Both-Worlds Linear Contextual Bandits", "link": "http://arxiv.org/abs/2312.16489", "description": "This study investigates the problem of $K$-armed linear contextual bandits,\nan instance of the multi-armed bandit problem, under an adversarial corruption.\nAt each round, a decision-maker observes an independent and identically\ndistributed context and then selects an arm based on the context and past\nobservations. After selecting an arm, the decision-maker incurs a loss\ncorresponding to the selected arm. The decision-maker aims to minimize the\ncumulative loss over the trial. The goal of this study is to develop a strategy\nthat is effective in both stochastic and adversarial environments, with\ntheoretical guarantees. We first formulate the problem by introducing a novel\nsetting of bandits with adversarial corruption, referred to as the contextual\nadversarial regime with a self-bounding constraint. We assume linear models for\nthe relationship between the loss and the context. Then, we propose a strategy\nthat extends the RealLinExp3 by Neu &amp; Olkhovskaya (2020) and the\nFollow-The-Regularized-Leader (FTRL). The regret of our proposed algorithm is\nshown to be upper-bounded by $O\\left(\\min\\left\\{\\frac{(\\log(T))^3}{\\Delta_{*}}\n+ \\sqrt{\\frac{C(\\log(T))^3}{\\Delta_{*}}},\\ \\\n\\sqrt{T}(\\log(T))^2\\right\\}\\right)$, where $T \\in\\mathbb{N}$ is the number of\nrounds, $\\Delta_{*} &gt; 0$ is the constant minimum gap between the best and\nsuboptimal arms for any context, and $C\\in[0, T] $ is an adversarial corruption\nparameter. This regret upper bound implies\n$O\\left(\\frac{(\\log(T))^3}{\\Delta_{*}}\\right)$ in a stochastic environment and\nby $O\\left( \\sqrt{T}(\\log(T))^2\\right)$ in an adversarial environment. We refer\nto our strategy as the Best-of-Both-Worlds (BoBW) RealFTRL, due to its\ntheoretical guarantees in both stochastic and adversarial regimes."}, "http://arxiv.org/abs/2312.16512": {"title": "Degrees-of-freedom penalized piecewise regression", "link": "http://arxiv.org/abs/2312.16512", "description": "Many popular piecewise regression models rely on minimizing a cost function\non the model fit with a linear penalty on the number of segments. However, this\npenalty does not take into account varying complexities of the model functions\non the segments potentially leading to overfitting when models with varying\ncomplexities, such as polynomials of different degrees, are used. In this work,\nwe enhance on this approach by instead using a penalty on the sum of the\ndegrees of freedom over all segments, called degrees-of-freedom penalized\npiecewise regression (DofPPR). We show that the solutions of the resulting\nminimization problem are unique for almost all input data in a least squares\nsetting. We develop a fast algorithm which does not only compute a minimizer\nbut also determines an optimal hyperparameter -- in the sense of rolling cross\nvalidation with the one standard error rule -- exactly. This eliminates manual\nhyperparameter selection. Our method supports optional user parameters for\nincorporating domain knowledge. We provide an open-source Python/Rust code for\nthe piecewise polynomial least squares case which can be extended to further\nmodels. We demonstrate the practical utility through a simulation study and by\napplications to real data. A constrained variant of the proposed method gives\nstate-of-the-art results in the Turing benchmark for unsupervised changepoint\ndetection."}, "http://arxiv.org/abs/2312.16544": {"title": "Hierarchical variable clustering based on the predictive strength between random vectors", "link": "http://arxiv.org/abs/2312.16544", "description": "A rank-invariant clustering of variables is introduced that is based on the\npredictive strength between groups of variables, i.e., two groups are assigned\na high similarity if the variables in the first group contain high predictive\ninformation about the behaviour of the variables in the other group and/or vice\nversa. The method presented here is model-free, dependence-based and does not\nrequire any distributional assumptions. Various general invariance and\ncontinuity properties are investigated, with special attention to those that\nare beneficial for the agglomerative hierarchical clustering procedure. A fully\nnon-parametric estimator is considered whose excellent performance is\ndemonstrated in several simulation studies and by means of real-data examples."}, "http://arxiv.org/abs/2312.16656": {"title": "Clustering Sets of Functional Data by Similarity in Law", "link": "http://arxiv.org/abs/2312.16656", "description": "We introduce a new clustering method for the classification of functional\ndata sets by their probabilistic law, that is, a procedure that aims to assign\ndata sets to the same cluster if and only if the data were generated with the\nsame underlying distribution. This method has the nice virtue of being\nnon-supervised and non-parametric, allowing for exploratory investigation with\nfew assumptions about the data. Rigorous finite bounds on the classification\nerror are given along with an objective heuristic that consistently selects the\nbest partition in a data-driven manner. Simulated data has been clustered with\nthis procedure to show the performance of the method with different parametric\nmodel classes of functional data."}, "http://arxiv.org/abs/2312.16734": {"title": "Selective Inference for Sparse Graphs via Neighborhood Selection", "link": "http://arxiv.org/abs/2312.16734", "description": "Neighborhood selection is a widely used method used for estimating the\nsupport set of sparse precision matrices, which helps determine the conditional\ndependence structure in undirected graphical models. However, reporting only\npoint estimates for the estimated graph can result in poor replicability\nwithout accompanying uncertainty estimates. In fields such as psychology, where\nthe lack of replicability is a major concern, there is a growing need for\nmethods that can address this issue. In this paper, we focus on the Gaussian\ngraphical model. We introduce a selective inference method to attach\nuncertainty estimates to the selected (nonzero) entries of the precision matrix\nand decide which of the estimated edges must be included in the graph. Our\nmethod provides an exact adjustment for the selection of edges, which when\nmultiplied with the Wishart density of the random matrix, results in valid\nselective inferences. Through the use of externally added randomization\nvariables, our adjustment is easy to compute, requiring us to calculate the\nprobability of a selection event, that is equivalent to a few sign constraints\nand that decouples across the nodewise regressions. Through simulations and an\napplication to a mobile health trial designed to study mental health, we\ndemonstrate that our selective inference method results in higher power and\nimproved estimation accuracy."}, "http://arxiv.org/abs/2312.16739": {"title": "A Bayesian functional PCA model with multilevel partition priors for group studies in neuroscience", "link": "http://arxiv.org/abs/2312.16739", "description": "The statistical analysis of group studies in neuroscience is particularly\nchallenging due to the complex spatio-temporal nature of the data, its multiple\nlevels and the inter-individual variability in brain responses. In this\nrespect, traditional ANOVA-based studies and linear mixed effects models\ntypically provide only limited exploration of the dynamic of the group brain\nactivity and variability of the individual responses potentially leading to\noverly simplistic conclusions and/or missing more intricate patterns. In this\nstudy we propose a novel method based on functional Principal Components\nAnalysis and Bayesian model-based clustering to simultaneously assess group\neffects and individual deviations over the most important temporal features in\nthe data. This method provides a thorough exploration of group differences and\nindividual deviations in neuroscientific group studies without compromising on\nthe spatio-temporal nature of the data. By means of a simulation study we\ndemonstrate that the proposed model returns correct classification in different\nclustering scenarios under low and high of noise levels in the data. Finally we\nconsider a case study using Electroencephalogram data recorded during an object\nrecognition task where our approach provides new insights into the underlying\nbrain mechanisms generating the data and their variability."}, "http://arxiv.org/abs/2312.16769": {"title": "Estimation and Inference for High-dimensional Multi-response Growth Curve Model", "link": "http://arxiv.org/abs/2312.16769", "description": "A growth curve model (GCM) aims to characterize how an outcome variable\nevolves, develops and grows as a function of time, along with other predictors.\nIt provides a particularly useful framework to model growth trend in\nlongitudinal data. However, the estimation and inference of GCM with a large\nnumber of response variables faces numerous challenges, and remains\nunderdeveloped. In this article, we study the high-dimensional\nmultivariate-response linear GCM, and develop the corresponding estimation and\ninference procedures. Our proposal is far from a straightforward extension, and\ninvolves several innovative components. Specifically, we introduce a Kronecker\nproduct structure, which allows us to effectively decompose a very large\ncovariance matrix, and to pool the correlated samples to improve the estimation\naccuracy. We devise a highly non-trivial multi-step estimation approach to\nestimate the individual covariance components separately and effectively. We\nalso develop rigorous statistical inference procedures to test both the global\neffects and the individual effects, and establish the size and power\nproperties, as well as the proper false discovery control. We demonstrate the\neffectiveness of the new method through both intensive simulations, and the\nanalysis of a longitudinal neuroimaging data for Alzheimer's disease."}, "http://arxiv.org/abs/2312.16887": {"title": "Automatic Scoring of Cognition Drawings: Assessing the Quality of Machine-Based Scores Against a Gold Standard", "link": "http://arxiv.org/abs/2312.16887", "description": "Figure drawing is often used as part of dementia screening protocols. The\nSurvey of Health Aging and Retirement in Europe (SHARE) has adopted three\ndrawing tests from Addenbrooke's Cognitive Examination III as part of its\nquestionnaire module on cognition. While the drawings are usually scored by\ntrained clinicians, SHARE uses the face-to-face interviewers who conduct the\ninterviews to score the drawings during fieldwork. This may pose a risk to data\nquality, as interviewers may be less consistent in their scoring and more\nlikely to make errors due to their lack of clinical training. This paper\ntherefore reports a first proof of concept and evaluates the feasibility of\nautomating scoring using deep learning. We train several different\nconvolutional neural network (CNN) models using about 2,000 drawings from the\n8th wave of the SHARE panel in Germany and the corresponding interviewer\nscores, as well as self-developed 'gold standard' scores. The results suggest\nthat this approach is indeed feasible. Compared to training on interviewer\nscores, models trained on the gold standard data improve prediction accuracy by\nabout 10 percentage points. The best performing model, ConvNeXt Base, achieves\nan accuracy of about 85%, which is 5 percentage points higher than the accuracy\nof the interviewers. While this is a promising result, the models still\nstruggle to score partially correct drawings, which are also problematic for\ninterviewers. This suggests that more and better training data is needed to\nachieve production-level prediction accuracy. We therefore discuss possible\nnext steps to improve the quality and quantity of training examples."}, "http://arxiv.org/abs/2312.16953": {"title": "Super Ensemble Learning Using the Highly-Adaptive-Lasso", "link": "http://arxiv.org/abs/2312.16953", "description": "We consider estimation of a functional parameter of a realistically modeled\ndata distribution based on independent and identically distributed\nobservations. Suppose that the true function is defined as the minimizer of the\nexpectation of a specified loss function over its parameter space. Estimators\nof the true function are provided, viewed as a data-adaptive coordinate\ntransformation for the true function. For any $J$-dimensional real valued\ncadlag function with finite sectional variation norm, we define a candidate\nensemble estimator as the mapping from the data into the composition of the\ncadlag function and the $J$ estimated functions. Using $V$-fold\ncross-validation, we define the cross-validated empirical risk of each cadlag\nfunction specific ensemble estimator. We then define the Meta Highly Adaptive\nLasso Minimum Loss Estimator (M-HAL-MLE) as the cadlag function that minimizes\nthis cross-validated empirical risk over all cadlag functions with a uniform\nbound on the sectional variation norm. For each of the $V$ training samples,\nthis yields a composition of the M-HAL-MLE ensemble and the $J$ estimated\nfunctions trained on the training sample. We can estimate the true function\nwith the average of these $V$ estimated functions, which we call the M-HAL\nsuper-learner. The M-HAL super-learner converges to the oracle estimator at a\nrate $n^{-2/3}$ (up till $\\log n$-factor) w.r.t. excess risk, where the oracle\nestimator minimizes the excess risk among all considered ensembles. The excess\nrisk of the oracle estimator and true function is generally second order. Under\nweak conditions on the $J$ candidate estimators, target features of the\nundersmoothed M-HAL super-learner are asymptotically linear estimators of the\ncorresponding target features of true function, with influence curve either the\nefficient influence curve, or potentially, a super-efficient influence curve."}, "http://arxiv.org/abs/2312.17015": {"title": "Regularized Exponentially Tilted Empirical Likelihood for Bayesian Inference", "link": "http://arxiv.org/abs/2312.17015", "description": "Bayesian inference with empirical likelihood faces a challenge as the\nposterior domain is a proper subset of the original parameter space due to the\nconvex hull constraint. We propose a regularized exponentially tilted empirical\nlikelihood to address this issue. Our method removes the convex hull constraint\nusing a novel regularization technique, incorporating a continuous exponential\nfamily distribution to satisfy a Kullback--Leibler divergence criterion. The\nregularization arises as a limiting procedure where pseudo-data are added to\nthe formulation of exponentially tilted empirical likelihood in a structured\nfashion. We show that this regularized exponentially tilted empirical\nlikelihood retains certain desirable asymptotic properties of (exponentially\ntilted) empirical likelihood and has improved finite sample performance.\nSimulation and data analysis demonstrate that the proposed method provides a\nsuitable pseudo-likelihood for Bayesian inference. The implementation of our\nmethod is available as the R package retel. Supplementary materials for this\narticle are available online."}, "http://arxiv.org/abs/2312.17047": {"title": "Inconsistency of cross-validation for structure learning in Gaussian graphical models", "link": "http://arxiv.org/abs/2312.17047", "description": "Despite numerous years of research into the merits and trade-offs of various\nmodel selection criteria, obtaining robust results that elucidate the behavior\nof cross-validation remains a challenging endeavor. In this paper, we highlight\nthe inherent limitations of cross-validation when employed to discern the\nstructure of a Gaussian graphical model. We provide finite-sample bounds on the\nprobability that the Lasso estimator for the neighborhood of a node within a\nGaussian graphical model, optimized using a prediction oracle, misidentifies\nthe neighborhood. Our results pertain to both undirected and directed acyclic\ngraphs, encompassing general, sparse covariance structures. To support our\ntheoretical findings, we conduct an empirical investigation of this\ninconsistency by contrasting our outcomes with other commonly used information\ncriteria through an extensive simulation study. Given that many algorithms\ndesigned to learn the structure of graphical models require hyperparameter\nselection, the precise calibration of this hyperparameter is paramount for\naccurately estimating the inherent structure. Consequently, our observations\nshed light on this widely recognized practical challenge."}, "http://arxiv.org/abs/2312.17061": {"title": "Bayesian Analysis of High Dimensional Vector Error Correction Model", "link": "http://arxiv.org/abs/2312.17061", "description": "Vector Error Correction Model (VECM) is a classic method to analyse\ncointegration relationships amongst multivariate non-stationary time series. In\nthis paper, we focus on high dimensional setting and seek for\nsample-size-efficient methodology to determine the level of cointegration. Our\ninvestigation centres at a Bayesian approach to analyse the cointegration\nmatrix, henceforth determining the cointegration rank. We design two algorithms\nand implement them on simulated examples, yielding promising results\nparticularly when dealing with high number of variables and relatively low\nnumber of observations. Furthermore, we extend this methodology to empirically\ninvestigate the constituents of the S&amp;P 500 index, where low-volatility\nportfolios can be found during both in-sample training and out-of-sample\ntesting periods."}, "http://arxiv.org/abs/2312.17065": {"title": "CluBear: A Subsampling Package for Interactive Statistical Analysis with Massive Data on A Single Machine", "link": "http://arxiv.org/abs/2312.17065", "description": "This article introduces CluBear, a Python-based open-source package for\ninteractive massive data analysis. The key feature of CluBear is that it\nenables users to conduct convenient and interactive statistical analysis of\nmassive data with only a traditional single-computer system. Thus, CluBear\nprovides a cost-effective solution when mining large-scale datasets. In\naddition, the CluBear package integrates many commonly used statistical and\ngraphical tools, which are useful for most commonly encountered data analysis\ntasks."}, "http://arxiv.org/abs/2312.17111": {"title": "Online Tensor Inference", "link": "http://arxiv.org/abs/2312.17111", "description": "Recent technological advances have led to contemporary applications that\ndemand real-time processing and analysis of sequentially arriving tensor data.\nTraditional offline learning, involving the storage and utilization of all data\nin each computational iteration, becomes impractical for high-dimensional\ntensor data due to its voluminous size. Furthermore, existing low-rank tensor\nmethods lack the capability for statistical inference in an online fashion,\nwhich is essential for real-time predictions and informed decision-making. This\npaper addresses these challenges by introducing a novel online inference\nframework for low-rank tensor learning. Our approach employs Stochastic\nGradient Descent (SGD) to enable efficient real-time data processing without\nextensive memory requirements, thereby significantly reducing computational\ndemands. We establish a non-asymptotic convergence result for the online\nlow-rank SGD estimator, nearly matches the minimax optimal rate of estimation\nerror in offline models that store all historical data. Building upon this\nfoundation, we propose a simple yet powerful online debiasing approach for\nsequential statistical inference in low-rank tensor learning. The entire online\nprocedure, covering both estimation and inference, eliminates the need for data\nsplitting or storing historical data, making it suitable for on-the-fly\nhypothesis testing. Given the sequential nature of our data collection,\ntraditional analyses relying on offline methods and sample splitting are\ninadequate. In our analysis, we control the sum of constructed\nsuper-martingales to ensure estimates along the entire solution path remain\nwithin the benign region. Additionally, a novel spectral representation tool is\nemployed to address statistical dependencies among iterative estimates,\nestablishing the desired asymptotic normality."}, "http://arxiv.org/abs/2312.17230": {"title": "Variable Neighborhood Searching Rerandomization", "link": "http://arxiv.org/abs/2312.17230", "description": "Rerandomization discards undesired treatment assignments to ensure covariate\nbalance in randomized experiments. However, rerandomization based on\nacceptance-rejection sampling is computationally inefficient, especially when\nnumerous independent assignments are required to perform randomization-based\nstatistical inference. Existing acceleration methods are suboptimal and are not\napplicable in structured experiments, including stratified experiments and\nexperiments with clusters. Based on metaheuristics in combinatorial\noptimization, we propose a novel variable neighborhood searching\nrerandomization(VNSRR) method to draw balanced assignments in various\nexperiments efficiently. We derive the unbiasedness and a lower bound for the\nvariance reduction of the treatment effect estimator under VNSRR. Simulation\nstudies and a real data example indicate that our method maintains the\nappealing statistical properties of rerandomization and can sample thousands of\ntreatment assignments within seconds, even in cases where existing methods\nrequire an hour to complete the task."}, "http://arxiv.org/abs/2003.13119": {"title": "Statistical Quantile Learning for Large, Nonlinear, and Additive Latent Variable Models", "link": "http://arxiv.org/abs/2003.13119", "description": "The studies of large-scale, high-dimensional data in fields such as genomics\nand neuroscience have injected new insights into science. Yet, despite\nadvances, they are confronting several challenges often simultaneously:\nnon-linearity, slow computation, inconsistency and uncertain convergence, and\nsmall sample sizes compared to high feature dimensions. Here, we propose a\nrelatively simple, scalable, and consistent nonlinear dimension reduction\nmethod that can potentially address these issues in unsupervised settings. We\ncall this method Statistical Quantile Learning (SQL) because, methodologically,\nit leverages on a quantile approximation of the latent variables and standard\nnonparametric techniques (sieve or penalyzed methods). By doing so, we show\nthat estimating the model originate from a convex assignment matching problem.\nTheoretically, we provide the asymptotic properties of SQL and its rates of\nconvergence. Operationally, SQL overcomes both the parametric restriction in\nnonlinear factor models in statistics and the difficulty of specifying\nhyperparameters and vanishing gradients in deep learning. Simulation studies\nassent the theory and reveal that SQL outperforms state-of-the-art statistical\nand machine learning methods. Compared to its linear competitors, SQL explains\nmore variance, yields better separation and explanation, and delivers more\naccurate outcome prediction when latent factors are used as predictors;\ncompared to its nonlinear competitors, SQL shows considerable advantage in\ninterpretability, ease of use and computations in high-dimensional\nsettings.Finally, we apply SQL to high-dimensional gene expression data\n(consisting of 20263 genes from 801 subjects), where the proposed method\nidentified latent factors predictive of five cancer types. The SQL package is\navailable at https://github.com/jbodelet/SQL."}, "http://arxiv.org/abs/2011.08174": {"title": "Policy design in experiments with unknown interference", "link": "http://arxiv.org/abs/2011.08174", "description": "This paper studies experimental designs for estimation and inference on\npolicies with spillover effects. Units are organized into a finite number of\nlarge clusters and interact in unknown ways within each cluster. First, we\nintroduce a single-wave experiment that, by varying the randomization across\ncluster pairs, estimates the marginal effect of a change in treatment\nprobabilities, taking spillover effects into account. Using the marginal\neffect, we propose a test for policy optimality. Second, we design a\nmultiple-wave experiment to estimate welfare-maximizing treatment rules. We\nprovide strong theoretical guarantees and an implementation in a large-scale\nfield experiment."}, "http://arxiv.org/abs/2102.06197": {"title": "Estimating a Directed Tree for Extremes", "link": "http://arxiv.org/abs/2102.06197", "description": "We propose a new method to estimate a root-directed spanning tree from\nextreme data. A prominent example is a river network, to be discovered from\nextreme flow measured at a set of stations. Our new algorithm utilizes\nqualitative aspects of a max-linear Bayesian network, which has been designed\nfor modelling causality in extremes. The algorithm estimates bivariate scores\nand returns a root-directed spanning tree. It performs extremely well on\nbenchmark data and new data. We prove that the new estimator is consistent\nunder a max-linear Bayesian network model with noise. We also assess its\nstrengths and limitations in a small simulation study."}, "http://arxiv.org/abs/2206.10108": {"title": "A Bayesian Nonparametric Approach for Identifying Differentially Abundant Taxa in Multigroup Microbiome Data with Covariates", "link": "http://arxiv.org/abs/2206.10108", "description": "Scientific studies in the last two decades have established the central role\nof the microbiome in disease and health. Differential abundance analysis seeks\nto identify microbial taxa associated with sample groups defined by a factor\nsuch as disease subtype, geographical region, or environmental condition. The\nresults, in turn, help clinical practitioners and researchers diagnose disease\nand develop treatments more effectively. However, microbiome data analysis is\nuniquely challenging due to high-dimensionality, sparsity, compositionally, and\ncollinearity. There is a critical need for unified statistical approaches for\ndifferential analysis in the presence of covariates. We develop a zero-inflated\nBayesian nonparametric (ZIBNP) methodology that meets these multipronged\nchallenges. The proposed technique flexibly adapts to the unique data\ncharacteristics, casts the high proportion of zeros in a censoring framework,\nand mitigates high-dimensionality and collinearity by utilizing the\ndimension-reducing property of the semiparametric Chinese restaurant process.\nAdditionally, the ZIBNP approach relates the microbiome sampling depths to\ninferential precision while accommodating the compositional nature of\nmicrobiome data. Through simulation studies and analyses of the CAnine\nMicrobiome during Parasitism (CAMP) and Global Gut microbiome datasets, we\ndemonstrate the accuracy of ZIBNP compared to established methods for\ndifferential abundance analysis in the presence of covariates."}, "http://arxiv.org/abs/2208.10910": {"title": "A flexible empirical Bayes approach to multiple linear regression and connections with penalized regression", "link": "http://arxiv.org/abs/2208.10910", "description": "We introduce a new empirical Bayes approach for large-scale multiple linear\nregression. Our approach combines two key ideas: (i) the use of flexible\n\"adaptive shrinkage\" priors, which approximate the nonparametric family of\nscale mixture of normal distributions by a finite mixture of normal\ndistributions; and (ii) the use of variational approximations to efficiently\nestimate prior hyperparameters and compute approximate posteriors. Combining\nthese two ideas results in fast and flexible methods, with computational speed\ncomparable to fast penalized regression methods such as the Lasso, and with\nsuperior prediction accuracy across a wide range of scenarios. Furthermore, we\nshow that the posterior mean from our method can be interpreted as solving a\npenalized regression problem, with the precise form of the penalty function\nbeing learned from the data by directly solving an optimization problem (rather\nthan being tuned by cross-validation). Our methods are implemented in an R\npackage, mr.ash.alpha, available from\nhttps://github.com/stephenslab/mr.ash.alpha"}, "http://arxiv.org/abs/2209.13117": {"title": "Consistent Covariance estimation for stratum imbalances under minimization method for covariate-adaptive randomization", "link": "http://arxiv.org/abs/2209.13117", "description": "Pocock and Simon's minimization method is a popular approach for\ncovariate-adaptive randomization in clinical trials. Valid statistical\ninference with data collected under the minimization method requires the\nknowledge of the limiting covariance matrix of within-stratum imbalances, whose\nexistence is only recently established. In this work, we propose a\nbootstrap-based estimator for this limit and establish its consistency, in\nparticular, by Le Cam's third lemma. As an application, we consider in\nsimulation studies adjustments to existing robust tests for treatment effects\nwith survival data by the proposed estimator. It shows that the adjusted tests\nachieve a size close to the nominal level, and unlike other designs, the robust\ntests without adjustment may have an asymptotic size inflation issue under the\nminimization method."}, "http://arxiv.org/abs/2209.15224": {"title": "Robust Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models", "link": "http://arxiv.org/abs/2209.15224", "description": "Unsupervised learning has been widely used in many real-world applications.\nOne of the simplest and most important unsupervised learning models is the\nGaussian mixture model (GMM). In this work, we study the multi-task learning\nproblem on GMMs, which aims to leverage potentially similar GMM parameter\nstructures among tasks to obtain improved learning performance compared to\nsingle-task learning. We propose a multi-task GMM learning procedure based on\nthe EM algorithm that not only can effectively utilize unknown similarity\nbetween related tasks but is also robust against a fraction of outlier tasks\nfrom arbitrary distributions. The proposed procedure is shown to achieve\nminimax optimal rate of convergence for both parameter estimation error and the\nexcess mis-clustering error, in a wide range of regimes. Moreover, we\ngeneralize our approach to tackle the problem of transfer learning for GMMs,\nwhere similar theoretical results are derived. Finally, we demonstrate the\neffectiveness of our methods through simulations and real data examples. To the\nbest of our knowledge, this is the first work studying multi-task and transfer\nlearning on GMMs with theoretical guarantees."}, "http://arxiv.org/abs/2212.06228": {"title": "LRD spectral analysis of multifractional functional time series on manifolds", "link": "http://arxiv.org/abs/2212.06228", "description": "This paper addresses the estimation of the second-order structure of a\nmanifold cross-time random field (RF) displaying spatially varying Long Range\nDependence (LRD), adopting the functional time series framework introduced in\nRuiz-Medina (2022). Conditions for the asymptotic unbiasedness of the\nintegrated periodogram operator in the Hilbert-Schmidt operator norm are\nderived beyond structural assumptions. Weak-consistent estimation of the\nlong-memory operator is achieved under a semiparametric functional spectral\nframework in the Gaussian context. The case where the projected manifold\nprocess can display Short Range Dependence (SRD) and LRD at different manifold\nscales is also analyzed. The performance of both estimation procedures is\nillustrated in the simulation study, in the context of multifractionally\nintegrated spherical functional autoregressive-moving average (SPHARMA(p,q))\nprocesses."}, "http://arxiv.org/abs/2301.01854": {"title": "Solving The Ordinary Least Squares in Closed Form, Without Inversion or Normalization", "link": "http://arxiv.org/abs/2301.01854", "description": "By connecting the LU factorization and the Gram-Schmidt orthogonalization\nwithout any normalization, closed-forms for the coefficients of the ordinary\nleast squares estimates are presented. Instead of using matrix inversion\nexplicitly, each of the coefficients is expressed and computed directly as a\nlinear combination of non-normalized Gram-Schmidt vectors and the original data\nmatrix and also in terms of the upper triangular factor from LU factorization.\nThe coefficients may computed iteratively using backward or forward algorithms\ngiven."}, "http://arxiv.org/abs/2312.17420": {"title": "Exact Consistency Tests for Gaussian Mixture Filters using Normalized Deviation Squared Statistics", "link": "http://arxiv.org/abs/2312.17420", "description": "We consider the problem of evaluating dynamic consistency in discrete time\nprobabilistic filters that approximate stochastic system state densities with\nGaussian mixtures. Dynamic consistency means that the estimated probability\ndistributions correctly describe the actual uncertainties. As such, the problem\nof consistency testing naturally arises in applications with regards to\nestimator tuning and validation. However, due to the general complexity of the\ndensity functions involved, straightforward approaches for consistency testing\nof mixture-based estimators have remained challenging to define and implement.\nThis paper derives a new exact result for Gaussian mixture consistency testing\nwithin the framework of normalized deviation squared (NDS) statistics. It is\nshown that NDS test statistics for generic multivariate Gaussian mixture models\nexactly follow mixtures of generalized chi-square distributions, for which\nefficient computational tools are available. The accuracy and utility of the\nresulting consistency tests are numerically demonstrated on static and dynamic\nmixture estimation examples."}, "http://arxiv.org/abs/2312.17480": {"title": "Detection of evolutionary shifts in variance under an Ornsten-Uhlenbeck model", "link": "http://arxiv.org/abs/2312.17480", "description": "1. Abrupt environmental changes can lead to evolutionary shifts in not only\nmean (optimal value), but also variance of descendants in trait evolution.\nThere are some methods to detect shifts in optimal value but few studies\nconsider shifts in variance. 2. We use a multi-optima and multi-variance OU\nprocess model to describe the trait evolution process with shifts in both\noptimal value and variance and provide analysis of how the covariance between\nspecies changes when shifts in variance occur along the path. 3. We propose a\nnew method to detect the shifts in both variance and optimal values based on\nminimizing the loss function with L1 penalty. We implement our method in a new\nR package, ShiVa (Detection of evolutionary shifts in variance). 4. We conduct\nsimulations to compare our method with the two methods considering only shifts\nin optimal values (l1ou; PhylogeneticEM). Our method shows strength in\npredictive ability and includes far fewer false positive shifts in optimal\nvalue compared to other methods when shifts in variance actually exist. When\nthere are only shifts in optimal value, our method performs similarly to other\nmethods. We applied our method to the cordylid data, ShiVa outperformed l1ou\nand phyloEM, exhibiting the highest log-likelihood and lowest BIC."}, "http://arxiv.org/abs/2312.17566": {"title": "Doublethink: simultaneous Bayesian-frequentist model-averaged hypothesis testing", "link": "http://arxiv.org/abs/2312.17566", "description": "Bayesian model-averaged hypothesis testing is an important technique in\nregression because it addresses the problem that the evidence one variable\ndirectly affects an outcome often depends on which other variables are included\nin the model. This problem is caused by confounding and mediation, and is\npervasive in big data settings with thousands of variables. However,\nmodel-averaging is under-utilized in fields, like epidemiology, where classical\nstatistical approaches dominate. Here we show that simultaneous Bayesian and\nfrequentist model-averaged hypothesis testing is possible in large samples, for\na family of priors. We show that Bayesian model-averaged regression is a closed\ntesting procedure, and use the theory of regular variation to derive\ninterchangeable posterior odds and $p$-values that jointly control the Bayesian\nfalse discovery rate (FDR), the frequentist type I error rate, and the\nfrequentist familywise error rate (FWER). These results arise from an\nasymptotic chi-squared distribution for the model-averaged deviance, under the\nnull hypothesis. We call the approach 'Doublethink'. In a related manuscript\n(Arning, Fryer and Wilson, 2024), we apply it to discovering direct risk\nfactors for COVID-19 hospitalization in UK Biobank, and we discuss its broader\nimplications for bridging the differences between Bayesian and frequentist\nhypothesis testing."}, "http://arxiv.org/abs/2312.17716": {"title": "Dependent Random Partitions by Shrinking Toward an Anchor", "link": "http://arxiv.org/abs/2312.17716", "description": "Although exchangeable processes from Bayesian nonparametrics have been used\nas a generating mechanism for random partition models, we deviate from this\nparadigm to explicitly incorporate clustering information in the formulation\nour random partition model. Our shrinkage partition distribution takes any\npartition distribution and shrinks its probability mass toward an anchor\npartition. We show how this provides a framework to model\nhierarchically-dependent and temporally-dependent random partitions. The\nshrinkage parameters control the degree of dependence, accommodating at its\nextremes both independence and complete equality. Since a priori knowledge of\nitems may vary, our formulation allows the degree of shrinkage toward the\nanchor to be item-specific. Our random partition model has a tractable\nnormalizing constant which allows for standard Markov chain Monte Carlo\nalgorithms for posterior sampling. We prove intuitive theoretical properties\nfor our distribution and compare it to related partition distributions. We show\nthat our model provides better out-of-sample fit in a real data application."}, "http://arxiv.org/abs/2210.00697": {"title": "A flexible model for correlated count data, with application to multi-condition differential expression analyses of single-cell RNA sequencing data", "link": "http://arxiv.org/abs/2210.00697", "description": "Detecting differences in gene expression is an important part of single-cell\nRNA sequencing experiments, and many statistical methods have been developed\nfor this aim. Most differential expression analyses focus on comparing\nexpression between two groups (e.g., treatment vs. control). But there is\nincreasing interest in multi-condition differential expression analyses in\nwhich expression is measured in many conditions, and the aim is to accurately\ndetect and estimate expression differences in all conditions. We show that\ndirectly modeling single-cell RNA-seq counts in all conditions simultaneously,\nwhile also inferring how expression differences are shared across conditions,\nleads to greatly improved performance for detecting and estimating expression\ndifferences compared to existing methods. We illustrate the potential of this\nnew approach by analyzing data from a single-cell experiment studying the\neffects of cytokine stimulation on gene expression. We call our new method\n\"Poisson multivariate adaptive shrinkage\", and it is implemented in an R\npackage available online at https://github.com/stephenslab/poisson.mash.alpha."}, "http://arxiv.org/abs/2211.13383": {"title": "A Non-Gaussian Bayesian Filter Using Power and Generalized Logarithmic Moments", "link": "http://arxiv.org/abs/2211.13383", "description": "In this paper, we aim to propose a consistent non-Gaussian Bayesian filter of\nwhich the system state is a continuous function. The distributions of the true\nsystem states, and those of the system and observation noises, are only assumed\nLebesgue integrable with no prior constraints on what function classes they\nfall within. This type of filter has significant merits in both theory and\npractice, which is able to ameliorate the curse of dimensionality for the\nparticle filter, a popular non-Gaussian Bayesian filter of which the system\nstate is parameterized by discrete particles and the corresponding weights. We\nfirst propose a new type of statistics, called the generalized logarithmic\nmoments. Together with the power moments, they are used to form a density\nsurrogate, parameterized as an analytic function, to approximate the true\nsystem state. The map from the parameters of the proposed density surrogate to\nboth the power moments and the generalized logarithmic moments is proved to be\na diffeomorphism, establishing the fact that there exists a unique density\nsurrogate which satisfies both moment conditions. This diffeomorphism also\nallows us to use gradient methods to treat the convex optimization problem in\ndetermining the parameters. Last but not least, simulation results reveal the\nadvantage of using both sets of moments for estimating mixtures of complicated\ntypes of functions. A robot localization simulation is also given, as an\nengineering application to validate the proposed filtering scheme."}, "http://arxiv.org/abs/2304.03476": {"title": "Generalizing the intention-to-treat effect of an active control against placebo from historical placebo-controlled trials to an active-controlled trial: A case study of the efficacy of daily oral TDF/FTC in the HPTN 084 study", "link": "http://arxiv.org/abs/2304.03476", "description": "In many clinical settings, an active-controlled trial design (e.g., a\nnon-inferiority or superiority design) is often used to compare an experimental\nmedicine to an active control (e.g., an FDA-approved, standard therapy). One\nprominent example is a recent phase 3 efficacy trial, HIV Prevention Trials\nNetwork Study 084 (HPTN 084), comparing long-acting cabotegravir, a new HIV\npre-exposure prophylaxis (PrEP) agent, to the FDA-approved daily oral tenofovir\ndisoproxil fumarate plus emtricitabine (TDF/FTC) in a population of\nheterosexual women in 7 African countries. One key complication of interpreting\nstudy results in an active-controlled trial like HPTN 084 is that the placebo\narm is not present and the efficacy of the active control (and hence the\nexperimental drug) compared to the placebo can only be inferred by leveraging\nother data sources. \\bz{In this article, we study statistical inference for the\nintention-to-treat (ITT) effect of the active control using relevant historical\nplacebo-controlled trials data under the potential outcomes (PO) framework}. We\nhighlight the role of adherence and unmeasured confounding, discuss in detail\nidentification assumptions and two modes of inference (point versus partial\nidentification), propose estimators under identification assumptions permitting\npoint identification, and lay out sensitivity analyses needed to relax\nidentification assumptions. We applied our framework to estimating the\nintention-to-treat effect of daily oral TDF/FTC versus placebo in HPTN 084\nusing data from an earlier Phase 3, placebo-controlled trial of daily oral\nTDF/FTC (Partners PrEP)."}, "http://arxiv.org/abs/2305.08284": {"title": "Model-based standardization using multiple imputation", "link": "http://arxiv.org/abs/2305.08284", "description": "When studying the association between treatment and a clinical outcome, a\nparametric multivariable model of the conditional outcome expectation is often\nused to adjust for covariates. The treatment coefficient of the outcome model\ntargets a conditional treatment effect. Model-based standardization is\ntypically applied to average the model predictions over the target covariate\ndistribution, and generate a covariate-adjusted estimate of the marginal\ntreatment effect. The standard approach to model-based standardization involves\nmaximum-likelihood estimation and use of the non-parametric bootstrap. We\nintroduce a novel, general-purpose, model-based standardization method based on\nmultiple imputation that is easily applicable when the outcome model is a\ngeneralized linear model. We term our proposed approach multiple imputation\nmarginalization (MIM). MIM consists of two main stages: the generation of\nsynthetic datasets and their analysis. MIM accommodates a Bayesian statistical\nframework, which naturally allows for the principled propagation of\nuncertainty, integrates the analysis into a probabilistic framework, and allows\nfor the incorporation of prior evidence. We conduct a simulation study to\nbenchmark the finite-sample performance of MIM in conjunction with a parametric\noutcome model. The simulations provide proof-of-principle in scenarios with\nbinary outcomes, continuous-valued covariates, a logistic outcome model and the\nmarginal log odds ratio as the target effect measure. When parametric modeling\nassumptions hold, MIM yields unbiased estimation in the target covariate\ndistribution, valid coverage rates, and similar precision and efficiency than\nthe standard approach to model-based standardization."}, "http://arxiv.org/abs/2401.00097": {"title": "Recursive identification with regularization and on-line hyperparameters estimation", "link": "http://arxiv.org/abs/2401.00097", "description": "This paper presents a regularized recursive identification algorithm with\nsimultaneous on-line estimation of both the model parameters and the algorithms\nhyperparameters. A new kernel is proposed to facilitate the algorithm\ndevelopment. The performance of this novel scheme is compared with that of the\nrecursive least-squares algorithm in simulation."}, "http://arxiv.org/abs/2401.00104": {"title": "Causal State Distillation for Explainable Reinforcement Learning", "link": "http://arxiv.org/abs/2401.00104", "description": "Reinforcement learning (RL) is a powerful technique for training intelligent\nagents, but understanding why these agents make specific decisions can be quite\nchallenging. This lack of transparency in RL models has been a long-standing\nproblem, making it difficult for users to grasp the reasons behind an agent's\nbehaviour. Various approaches have been explored to address this problem, with\none promising avenue being reward decomposition (RD). RD is appealing as it\nsidesteps some of the concerns associated with other methods that attempt to\nrationalize an agent's behaviour in a post-hoc manner. RD works by exposing\nvarious facets of the rewards that contribute to the agent's objectives during\ntraining. However, RD alone has limitations as it primarily offers insights\nbased on sub-rewards and does not delve into the intricate cause-and-effect\nrelationships that occur within an RL agent's neural model. In this paper, we\npresent an extension of RD that goes beyond sub-rewards to provide more\ninformative explanations. Our approach is centred on a causal learning\nframework that leverages information-theoretic measures for explanation\nobjectives that encourage three crucial properties of causal factors:\n\\emph{causal sufficiency}, \\emph{sparseness}, and \\emph{orthogonality}. These\nproperties help us distill the cause-and-effect relationships between the\nagent's states and actions or rewards, allowing for a deeper understanding of\nits decision-making processes. Our framework is designed to generate local\nexplanations and can be applied to a wide range of RL tasks with multiple\nreward channels. Through a series of experiments, we demonstrate that our\napproach offers more meaningful and insightful explanations for the agent's\naction selections."}, "http://arxiv.org/abs/2401.00139": {"title": "Is Knowledge All Large Language Models Needed for Causal Reasoning?", "link": "http://arxiv.org/abs/2401.00139", "description": "This paper explores the causal reasoning of large language models (LLMs) to\nenhance their interpretability and reliability in advancing artificial\nintelligence. Despite the proficiency of LLMs in a range of tasks, their\npotential for understanding causality requires further exploration. We propose\na novel causal attribution model that utilizes \"do-operators\" for constructing\ncounterfactual scenarios, allowing us to systematically quantify the influence\nof input numerical data and LLMs' pre-existing knowledge on their causal\nreasoning processes. Our newly developed experimental setup assesses LLMs'\nreliance on contextual information and inherent knowledge across various\ndomains. Our evaluation reveals that LLMs' causal reasoning ability depends on\nthe context and domain-specific knowledge provided, and supports the argument\nthat \"knowledge is, indeed, what LLMs principally require for sound causal\nreasoning\". On the contrary, in the absence of knowledge, LLMs still maintain a\ndegree of causal reasoning using the available numerical data, albeit with\nlimitations in the calculations."}, "http://arxiv.org/abs/2401.00196": {"title": "Bayesian principal stratification with longitudinal data and truncation by death", "link": "http://arxiv.org/abs/2401.00196", "description": "In many causal studies, outcomes are censored by death, in the sense that\nthey are neither observed nor defined for units who die. In such studies, the\nfocus is usually on the stratum of always survivors up to a single fixed time\ns. Building on a recent strand of the literature, we propose an extended\nframework for the analysis of longitudinal studies, where units can die at\ndifferent time points, and the main endpoints are observed and well defined\nonly up to the death time. We develop a Bayesian longitudinal principal\nstratification framework, where units are cross classified according to the\nlongitudinal death status. Under this framework, the focus is on causal effects\nfor the principal strata of units that would be alive up to a time point s\nirrespective of their treatment assignment, where these strata may vary as a\nfunction of s. We can get precious insights into the effects of treatment by\ninspecting the distribution of baseline characteristics within each\nlongitudinal principal stratum, and by investigating the time trend of both\nprincipal stratum membership and survivor-average causal effects. We illustrate\nour approach for the analysis of a longitudinal observational study aimed to\nassess, under the assumption of strong ignorability of treatment assignment,\nthe causal effects of a policy promoting start ups on firms survival and hiring\npolicy, where firms hiring status is censored by death."}, "http://arxiv.org/abs/2401.00245": {"title": "Alternative Approaches for Computing Highest-Density Regions", "link": "http://arxiv.org/abs/2401.00245", "description": "Many statistical problems require estimating a density function, say $f$,\nfrom data samples. In this work, for example, we are interested in\nhighest-density regions (HDRs), i.e., minimum volume sets that contain a given\nprobability. HDRs are typically computed using a density quantile approach,\nwhich, in the case of unknown densities, involves their estimation. This task\nturns out to be far from trivial, especially over increased dimensions and when\ndata are sparse and exhibit complex structures (e.g., multimodalities or\nparticular dependencies). We address this challenge by exploring alternative\napproaches to build HDRs that overcome direct (multivariate) density\nestimation. First, we generalize the density quantile method, currently\nimplementable on the basis of a consistent estimator of the density, to\n$neighbourhood$ measures, i.e., measures that preserve the order induced in the\nsample by $f$. Second, we discuss a number of suitable probabilistic- and\ndistance-based measures such as the $k$-nearest neighbourhood Euclidean\ndistance. Third, motivated by the ubiquitous role of $copula$ modeling in\nmodern statistics, we explore its use in the context of probabilistic-based\nmeasures. An extensive comparison among the introduced measures is provided,\nand their implications for computing HDRs in real-world problems are discussed."}, "http://arxiv.org/abs/2401.00255": {"title": "Adaptive Rank-based Tests for High Dimensional Mean Problems", "link": "http://arxiv.org/abs/2401.00255", "description": "The Wilcoxon signed-rank test and the Wilcoxon-Mann-Whitney test are commonly\nemployed in one sample and two sample mean tests for one-dimensional hypothesis\nproblems. For high-dimensional mean test problems, we calculate the asymptotic\ndistribution of the maximum of rank statistics for each variable and suggest a\nmax-type test. This max-type test is then merged with a sum-type test, based on\ntheir asymptotic independence offered by stationary and strong mixing\nassumptions. Our numerical studies reveal that this combined test demonstrates\nrobustness and superiority over other methods, especially for heavy-tailed\ndistributions."}, "http://arxiv.org/abs/2401.00257": {"title": "Assessing replication success via skeptical mixture priors", "link": "http://arxiv.org/abs/2401.00257", "description": "There is a growing interest in the analysis of replication studies of\noriginal findings across many disciplines. When testing a hypothesis for an\neffect size, two Bayesian approaches stand out for their principled use of the\nBayes factor (BF), namely the replication BF and the skeptical BF. In\nparticular, the latter BF is based on the skeptical prior, which represents the\nopinion of an investigator who is unconvinced by the original findings and\nwants to challenge them. We embrace the skeptical perspective, and elaborate a\nnovel mixture prior which incorporates skepticism while at the same time\ncontrolling for prior-data conflict within the original data. Consistency\nproperties of the resulting skeptical mixture BF are provided together with an\nextensive analysis of the main features of our proposal. Finally, we apply our\nmethodology to data from the Social Sciences Replication Project. In particular\nwe show that, for some case studies where prior-data conflict is an issue, our\nmethod uses a more realistic prior and leads to evidence-classification for\nreplication success which differs from the standard skeptical approach."}, "http://arxiv.org/abs/2401.00324": {"title": "Stratified distance space improves the efficiency of sequential samplers for approximate Bayesian computation", "link": "http://arxiv.org/abs/2401.00324", "description": "Approximate Bayesian computation (ABC) methods are standard tools for\ninferring parameters of complex models when the likelihood function is\nanalytically intractable. A popular approach to improving the poor acceptance\nrate of the basic rejection sampling ABC algorithm is to use sequential Monte\nCarlo (ABC SMC) to produce a sequence of proposal distributions adapting\ntowards the posterior, instead of generating values from the prior distribution\nof the model parameters. Proposal distribution for the subsequent iteration is\ntypically obtained from a weighted set of samples, often called particles, of\nthe current iteration of this sequence. Current methods for constructing these\nproposal distributions treat all the particles equivalently, regardless of the\ncorresponding value generated by the sampler, which may lead to inefficiency\nwhen propagating the information across iterations of the algorithm. To improve\nsampler efficiency, we introduce a modified approach called stratified distance\nABC SMC. Our algorithm stratifies particles based on their distance between the\ncorresponding synthetic and observed data, and then constructs distinct\nproposal distributions for all the strata. Taking into account the distribution\nof distances across the particle space leads to substantially improved\nacceptance rate of the rejection sampling. We further show that efficiency can\nbe gained by introducing a novel stopping rule for the sequential process based\non the stratified posterior samples and demonstrate these advances by several\nexamples."}, "http://arxiv.org/abs/2401.00354": {"title": "Estimation of the Emax model", "link": "http://arxiv.org/abs/2401.00354", "description": "This study focuses on the estimation of the Emax dose-response model, a\nwidely utilized framework in clinical trials, agriculture, and environmental\nexperiments. Existing challenges in obtaining maximum likelihood estimates\n(MLE) for model parameters are often ascribed to computational issues but, in\nreality, stem from the absence of MLE. Our contribution provides a new\nunderstanding and control of all the experimental situations that pratictioners\nmight face, guiding them in the estimation process. We derive the exact MLE for\na three-point experimental design and we identify the two scenarios where the\nMLE fails. To address these challenges, we propose utilizing Firth's modified\nscore, providing its analytical expression as a function of the experimental\ndesign. Through a simulation study, we demonstrate that, in one of the\nproblematic cases, the Firth modification yields a finite estimate. For the\nremaining case, we introduce a design-correction strategy akin to a hypothesis\ntest."}, "http://arxiv.org/abs/2401.00395": {"title": "Energetic Variational Gaussian Process Regression for Computer Experiments", "link": "http://arxiv.org/abs/2401.00395", "description": "The Gaussian process (GP) regression model is a widely employed surrogate\nmodeling technique for computer experiments, offering precise predictions and\nstatistical inference for the computer simulators that generate experimental\ndata. Estimation and inference for GP can be performed in both frequentist and\nBayesian frameworks. In this chapter, we construct the GP model through\nvariational inference, particularly employing the recently introduced energetic\nvariational inference method by Wang et al. (2021). Adhering to the GP model\nassumptions, we derive posterior distributions for its parameters. The\nenergetic variational inference approach bridges the Bayesian sampling and\noptimization and enables approximation of the posterior distributions and\nidentification of the posterior mode. By incorporating a normal prior on the\nmean component of the GP model, we also apply shrinkage estimation to the\nparameters, facilitating mean function variable selection. To showcase the\neffectiveness of our proposed GP model, we present results from three benchmark\nexamples."}, "http://arxiv.org/abs/2401.00461": {"title": "A Penalized Functional Linear Cox Regression Model for Spatially-defined Environmental Exposure with an Estimated Buffer Distance", "link": "http://arxiv.org/abs/2401.00461", "description": "In environmental health research, it is of interest to understand the effect\nof the neighborhood environment on health. Researchers have shown a protective\nassociation between green space around a person's residential address and\ndepression outcomes. In measuring exposure to green space, distance buffers are\noften used. However, buffer distances differ across studies. Typically, the\nbuffer distance is determined by researchers a priori. It is unclear how to\nidentify an appropriate buffer distance for exposure assessment. To address\ngeographic uncertainty problem for exposure assessment, we present a domain\nselection algorithm based on the penalized functional linear Cox regression\nmodel. The theoretical properties of our proposed method are studied and\nsimulation studies are conducted to evaluate finite sample performances of our\nmethod. The proposed method is illustrated in a study of associations of green\nspace exposure with depression and/or antidepressant use in the Nurses' Health\nStudy."}, "http://arxiv.org/abs/2401.00517": {"title": "Detecting Imprinting and Maternal Effects Using Monte Carlo Expectation Maximization Algorithm", "link": "http://arxiv.org/abs/2401.00517", "description": "Numerous statistical methods have been developed to explore genomic\nimprinting and maternal effects, which are causes of parent-of-origin patterns\nin complex human diseases. However, most of them either only model one of these\ntwo confounded epigenetic effects, or make strong yet unrealistic assumptions\nabout the population to avoid over-parameterization. A recent partial\nlikelihood method (LIME) can identify both epigenetic effects based on\ncase-control family data without those assumptions. Theoretical and empirical\nstudies have shown its validity and robustness. However, because LIME obtains\nparameter estimation by maximizing partial likelihood, it is interesting to\ncompare its efficiency with full likelihood maximizer. To overcome the\ndifficulty in over-parameterization when using full likelihood, in this study\nwe propose a Monte Carlo Expectation Maximization (MCEM) method to detect\nimprinting and maternal effects jointly. Those unknown mating type\nprobabilities, the nuisance parameters, can be considered as latent variables\nin EM algorithm. Monte Carlo samples are used to numerically approximate the\nexpectation function that cannot be solved algebraically. Our simulation\nresults show that though this MCEM algorithm takes longer computational time,\nand can give higher bias in some simulations compared to LIME, it can generally\ndetect both epigenetic effects with higher power and smaller standard error\nwhich demonstrates that it can be a good complement of LIME method."}, "http://arxiv.org/abs/2401.00520": {"title": "Monte Carlo Expectation-Maximization algorithm to detect imprinting and maternal effects for discordant sib-pair data", "link": "http://arxiv.org/abs/2401.00520", "description": "Numerous statistical methods have been developed to explore genomic\nimprinting and maternal effects, which are causes of parent-of-origin patterns\nin complex human diseases. Most of the methods, however, either only model one\nof these two confounded epigenetic effects, or make strong yet unrealistic\nassumptions about the population to avoid over-parameterization. A recent\npartial likelihood method (LIMEDSP ) can identify both epigenetic effects based\non discordant sibpair family data without those assumptions. Theoretical and\nempirical studies have shown its validity and robustness. As LIMEDSP method\nobtains parameter estimation by maximizing partial likelihood, it is\ninteresting to compare its efficiency with full likelihood maximizer. To\novercome the difficulty in over-parameterization when using full likelihood,\nthis study proposes a discordant sib-pair design based Monte Carlo Expectation\nMaximization (MCEMDSP ) method to detect imprinting and maternal effects\njointly. Those unknown mating type probabilities, the nuisance parameters, are\nconsidered as latent variables in EM algorithm. Monte Carlo samples are used to\nnumerically approximate the expectation function that cannot be solved\nalgebraically. Our simulation results show that though this MCEMDSP algorithm\ntakes longer computation time, it can generally detect both epigenetic effects\nwith higher power, which demonstrates that it can be a good complement of\nLIMEDSP method"}, "http://arxiv.org/abs/2401.00540": {"title": "Study Duration Prediction for Clinical Trials with Time-to-Event Endpoints Using Mixture Distributions Accounting for Heterogeneous Population", "link": "http://arxiv.org/abs/2401.00540", "description": "In the era of precision medicine, more and more clinical trials are now\ndriven or guided by biomarkers, which are patient characteristics objectively\nmeasured and evaluated as indicators of normal biological processes, pathogenic\nprocesses, or pharmacologic responses to therapeutic interventions. With the\noverarching objective to optimize and personalize disease management,\nbiomarker-guided clinical trials increase the efficiency by appropriately\nutilizing prognostic or predictive biomarkers in the design. However, the\nefficiency gain is often not quantitatively compared to the traditional\nall-comers design, in which a faster enrollment rate is expected (e.g. due to\nno restriction to biomarker positive patients) potentially leading to a shorter\nduration. To accurately predict biomarker-guided trial duration, we propose a\ngeneral framework using mixture distributions accounting for heterogeneous\npopulation. Extensive simulations are performed to evaluate the impact of\nheterogeneous population and the dynamics of biomarker characteristics and\ndisease on the study duration. Several influential parameters including median\nsurvival time, enrollment rate, biomarker prevalence and effect size are\nidentitied. Re-assessments of two publicly available trials are conducted to\nempirically validate the prediction accuracy and to demonstrate the practical\nutility. The R package \\emph{detest} is developed to implement the proposed\nmethod and is publicly available on CRAN."}, "http://arxiv.org/abs/2401.00566": {"title": "Change point analysis -- the empirical Hankel transform approach", "link": "http://arxiv.org/abs/2401.00566", "description": "In this study, we introduce the first-of-its-kind class of tests for\ndetecting change points in the distribution of a sequence of independent\nmatrix-valued random variables. The tests are constructed using the weighted\nsquare integral difference of the empirical orthogonal Hankel transforms. The\ntest statistics have a convenient closed-form expression, making them easy to\nimplement in practice. We present their limiting properties and demonstrate\ntheir quality through an extensive simulation study. We utilize these tests for\nchange point detection in cryptocurrency markets to showcase their practical\nuse. The detection of change points in this context can have various\napplications in constructing and analyzing novel trading systems."}, "http://arxiv.org/abs/2401.00568": {"title": "Extrapolation of Relative Treatment Effects using Change-point Survival Models", "link": "http://arxiv.org/abs/2401.00568", "description": "Introduction: Modelling of relative treatment effects is an important aspect\nto consider when extrapolating the long-term survival outcomes of treatments.\nFlexible parametric models offer the ability to accurately model the observed\ndata, however, the extrapolated relative treatment effects and subsequent\nsurvival function may lack face validity. Methods: We investigate the ability\nof change-point survival models to estimate changes in the relative treatment\neffects, specifically treatment delay, loss of treatment effects and converging\nhazards. These models are implemented using standard Bayesian statistical\nsoftware and propagate the uncertainty associate with all model parameters\nincluding the change-point location. A simulation study was conducted to assess\nthe predictive performance of these models compared with other parametric\nsurvival models. Change-point survival models were applied to three datasets,\ntwo of which were used in previous health technology assessments. Results:\nChange-point survival models typically provided improved extrapolated survival\npredictions, particularly when the changes in relative treatment effects are\nlarge. When applied to the real world examples they provided good fit to the\nobserved data while and in some situations produced more clinically plausible\nextrapolations than those generated by flexible spline models. Change-point\nmodels also provided support to a previously implemented modelling approach\nwhich was justified by visual inspection only and not goodness of fit to the\nobserved data. Conclusions: We believe change-point survival models offer the\nability to flexibly model observed data while also modelling and investigating\nclinically plausible scenarios with respect to the relative treatment effects."}, "http://arxiv.org/abs/2401.00624": {"title": "Semi-Confirmatory Factor Analysis for High-Dimensional Data with Interconnected Community Structures", "link": "http://arxiv.org/abs/2401.00624", "description": "We propose a novel data-driven semi-confirmatory factor analysis (SCFA) model\nthat addresses the absence of model specification and handles the estimation\nand inference tasks with high-dimensional data. Confirmatory factor analysis\n(CFA) is a prevalent and pivotal technique for statistically validating the\ncovariance structure of latent common factors derived from multiple observed\nvariables. In contrast to other factor analysis methods, CFA offers a flexible\ncovariance modeling approach for common factors, enhancing the interpretability\nof relationships between the common factors, as well as between common factors\nand observations. However, the application of classic CFA models faces dual\nbarriers: the lack of a prerequisite specification of \"non-zero loadings\" or\nfactor membership (i.e., categorizing the observations into distinct common\nfactors), and the formidable computational burden in high-dimensional scenarios\nwhere the number of observed variables surpasses the sample size. To bridge\nthese two gaps, we propose the SCFA model by integrating the underlying\nhigh-dimensional covariance structure of observed variables into the CFA model.\nAdditionally, we offer computationally efficient solutions (i.e., closed-form\nuniformly minimum variance unbiased estimators) and ensure accurate statistical\ninference through closed-form exact variance estimators for all model\nparameters and factor scores. Through an extensive simulation analysis\nbenchmarking against standard computational packages, SCFA exhibits superior\nperformance in estimating model parameters and recovering factor scores, while\nsubstantially reducing the computational load, across both low- and\nhigh-dimensional scenarios. It exhibits moderate robustness to model\nmisspecification. We illustrate the practical application of the SCFA model by\nconducting factor analysis on a high-dimensional gene expression dataset."}, "http://arxiv.org/abs/2401.00634": {"title": "A scalable two-stage Bayesian approach accounting for exposure measurement error in environmental epidemiology", "link": "http://arxiv.org/abs/2401.00634", "description": "Accounting for exposure measurement errors has been recognized as a crucial\nproblem in environmental epidemiology for over two decades. Bayesian\nhierarchical models offer a coherent probabilistic framework for evaluating\nassociations between environmental exposures and health effects, which take\ninto account exposure measurement errors introduced by uncertainty in the\nestimated exposure as well as spatial misalignment between the exposure and\nhealth outcome data. While two-stage Bayesian analyses are often regarded as a\ngood alternative to fully Bayesian analyses when joint estimation is not\nfeasible, there has been minimal research on how to properly propagate\nuncertainty from the first-stage exposure model to the second-stage health\nmodel, especially in the case of a large number of participant locations along\nwith spatially correlated exposures. We propose a scalable two-stage Bayesian\napproach, called a sparse multivariate normal (sparse MVN) prior approach,\nbased on the Vecchia approximation for assessing associations between exposure\nand health outcomes in environmental epidemiology. We compare its performance\nwith existing approaches through simulation. Our sparse MVN prior approach\nshows comparable performance with the fully Bayesian approach, which is a gold\nstandard but is impossible to implement in some cases. We investigate the\nassociation between source-specific exposures and pollutant (nitrogen dioxide\n(NO$_2$))-specific exposures and birth outcomes for 2012 in Harris County,\nTexas, using several approaches, including the newly developed method."}, "http://arxiv.org/abs/2401.00649": {"title": "Linear Model and Extensions", "link": "http://arxiv.org/abs/2401.00649", "description": "I developed the lecture notes based on my ``Linear Model'' course at the\nUniversity of California Berkeley over the past seven years. This book provides\nan intermediate-level introduction to the linear model. It balances rigorous\nproofs and heuristic arguments. This book provides R code to replicate all\nsimulation studies and case studies."}, "http://arxiv.org/abs/2401.00667": {"title": "Channelling Multimodality Through a Unimodalizing Transport: Warp-U Sampler and Stochastic Bridge Sampling", "link": "http://arxiv.org/abs/2401.00667", "description": "Monte Carlo integration is fundamental in scientific and statistical\ncomputation, but requires reliable samples from the target distribution, which\nposes a substantial challenge in the case of multi-modal distributions.\nExisting methods often involve time-consuming tuning, and typically lack\ntailored estimators for efficient use of the samples. This paper adapts the\nWarp-U transformation [Wang et al., 2022] to form multi-modal sampling strategy\ncalled Warp-U sampling. It constructs a stochastic map to transport a\nmulti-modal density into a uni-modal one, and subsequently inverts the\ntransport but with new stochasticity injected. For efficient use of the samples\nfor normalising constant estimation, we propose (i) an unbiased estimation\nscheme based coupled chains, where the Warp-U sampling is used to reduce the\ncoupling time; and (ii) a stochastic Warp-U bridge sampling estimator, which\nimproves its deterministic counterpart given in Wang et al. [2022]. Our overall\napproach requires less tuning and is easier to apply than common alternatives.\nTheoretically, we establish the ergodicity of our sampling algorithm and that\nour stochastic Warp-U bridge sampling estimator has greater (asymptotic)\nprecision per CPU second compared to the Warp-U bridge estimator of Wang et al.\n[2022] under practical conditions. The advantages and current limitations of\nour approach are demonstrated through simulation studies and an application to\nexoplanet detection."}, "http://arxiv.org/abs/2401.00800": {"title": "Factor Importance Ranking and Selection using Total Indices", "link": "http://arxiv.org/abs/2401.00800", "description": "Factor importance measures the impact of each feature on output prediction\naccuracy. Many existing works focus on the model-based importance, but an\nimportant feature in one learning algorithm may hold little significance in\nanother model. Hence, a factor importance measure ought to characterize the\nfeature's predictive potential without relying on a specific prediction\nalgorithm. Such algorithm-agnostic importance is termed as intrinsic importance\nin Williamson et al. (2023), but their estimator again requires model fitting.\nTo bypass the modeling step, we present the equivalence between predictiveness\npotential and total Sobol' indices from global sensitivity analysis, and\nintroduce a novel consistent estimator that can be directly estimated from\nnoisy data. Integrating with forward selection and backward elimination gives\nrise to FIRST, Factor Importance Ranking and Selection using Total (Sobol')\nindices. Extensive simulations are provided to demonstrate the effectiveness of\nFIRST on regression and binary classification problems, and a clear advantage\nover the state-of-the-art methods."}, "http://arxiv.org/abs/2401.00840": {"title": "Bayesian Effect Selection in Additive Models with an Application to Time-to-Event Data", "link": "http://arxiv.org/abs/2401.00840", "description": "Accurately selecting and estimating smooth functional effects in additive\nmodels with potentially many functions is a challenging task. We introduce a\nnovel Demmler-Reinsch basis expansion to model the functional effects that\nallows us to orthogonally decompose an effect into its linear and nonlinear\nparts. We show that our representation allows to consistently estimate both\nparts as opposed to commonly employed mixed model representations. Equipping\nthe reparameterized regression coefficients with normal beta prime spike and\nslab priors allows us to determine whether a continuous covariate has a linear,\na nonlinear or no effect at all. We provide new theoretical results for the\nprior and a compelling explanation for its superior Markov chain Monte Carlo\nmixing performance compared to the spike-and-slab group lasso. We establish an\nefficient posterior estimation scheme and illustrate our approach along effect\nselection on the hazard rate of a time-to-event response in the geoadditive Cox\nregression model in simulations and data on survival with leukemia."}, "http://arxiv.org/abs/2205.00605": {"title": "Cluster-based Regression using Variational Inference and Applications in Financial Forecasting", "link": "http://arxiv.org/abs/2205.00605", "description": "This paper describes an approach to simultaneously identify clusters and\nestimate cluster-specific regression parameters from the given data. Such an\napproach can be useful in learning the relationship between input and output\nwhen the regression parameters for estimating output are different in different\nregions of the input space. Variational Inference (VI), a machine learning\napproach to obtain posterior probability densities using optimization\ntechniques, is used to identify clusters of explanatory variables and\nregression parameters for each cluster. From these results, one can obtain both\nthe expected value and the full distribution of predicted output. Other\nadvantages of the proposed approach include the elegant theoretical solution\nand clear interpretability of results. The proposed approach is well-suited for\nfinancial forecasting where markets have different regimes (or clusters) with\ndifferent patterns and correlations of market changes in each regime. In\nfinancial applications, knowledge about such clusters can provide useful\ninsights about portfolio performance and identify the relative importance of\nvariables in different market regimes. An illustrative example of predicting\none-day S&amp;P change is considered to illustrate the approach and compare the\nperformance of the proposed approach with standard regression without clusters.\nDue to the broad applicability of the problem, its elegant theoretical\nsolution, and the computational efficiency of the proposed algorithm, the\napproach may be useful in a number of areas extending beyond the financial\ndomain."}, "http://arxiv.org/abs/2209.02008": {"title": "Parallel sampling of decomposable graphs using Markov chain on junction trees", "link": "http://arxiv.org/abs/2209.02008", "description": "Bayesian inference for undirected graphical models is mostly restricted to\nthe class of decomposable graphs, as they enjoy a rich set of properties making\nthem amenable to high-dimensional problems. While parameter inference is\nstraightforward in this setup, inferring the underlying graph is a challenge\ndriven by the computational difficulty in exploring the space of decomposable\ngraphs. This work makes two contributions to address this problem. First, we\nprovide sufficient and necessary conditions for when multi-edge perturbations\nmaintain decomposability of the graph. Using these, we characterize a simple\nclass of partitions that efficiently classify all edge perturbations by whether\nthey maintain decomposability. Second, we propose a novel parallel\nnon-reversible Markov chain Monte Carlo sampler for distributions over junction\ntree representations of the graph. At every step, the parallel sampler executes\nsimultaneously all edge perturbations within a partition. Through simulations,\nwe demonstrate the efficiency of our new edge perturbation conditions and class\nof partitions. We find that our parallel sampler yields improved mixing\nproperties in comparison to the single-move variate, and outperforms current\nstate-of-the-arts methods in terms of accuracy and computational efficiency.\nThe implementation of our work is available in the Python package parallelDG."}, "http://arxiv.org/abs/2302.00293": {"title": "A Survey of Methods, Challenges and Perspectives in Causality", "link": "http://arxiv.org/abs/2302.00293", "description": "Deep Learning models have shown success in a large variety of tasks by\nextracting correlation patterns from high-dimensional data but still struggle\nwhen generalizing out of their initial distribution. As causal engines aim to\nlearn mechanisms independent from a data distribution, combining Deep Learning\nwith Causality can have a great impact on the two fields. In this paper, we\nfurther motivate this assumption. We perform an extensive overview of the\ntheories and methods for Causality from different perspectives, with an\nemphasis on Deep Learning and the challenges met by the two domains. We show\nearly attempts to bring the fields together and the possible perspectives for\nthe future. We finish by providing a large variety of applications for\ntechniques from Causality."}, "http://arxiv.org/abs/2304.14954": {"title": "A Class of Dependent Random Distributions Based on Atom Skipping", "link": "http://arxiv.org/abs/2304.14954", "description": "We propose the Plaid Atoms Model (PAM), a novel Bayesian nonparametric model\nfor grouped data. Founded on an idea of `atom skipping', PAM is part of a\nwell-established category of models that generate dependent random\ndistributions and clusters across multiple groups. Atom skipping referrs to\nstochastically assigning 0 weights to atoms in an infinite mixture. Deploying\natom skipping across groups, PAM produces a dependent clustering pattern with\noverlapping and non-overlapping clusters across groups. As a result,\ninterpretable posterior inference is possible such as reporting the posterior\nprobability of a cluster being exclusive to a single group or shared among a\nsubset of groups. We discuss the theoretical properties of the proposed and\nrelated models. Minor extensions of the proposed model for multivariate or\ncount data are presented. Simulation studies and applications using real-world\ndatasets illustrate the performance of the new models with comparison to\nexisting models."}, "http://arxiv.org/abs/2305.09126": {"title": "Transfer Learning for Causal Effect Estimation", "link": "http://arxiv.org/abs/2305.09126", "description": "We present a Transfer Causal Learning (TCL) framework when target and source\ndomains share the same covariate/feature spaces, aiming to improve causal\neffect estimation accuracy in limited data. Limited data is very common in\nmedical applications, where some rare medical conditions, such as sepsis, are\nof interest. Our proposed method, named \\texttt{$\\ell_1$-TCL}, incorporates\n$\\ell_1$ regularized TL for nuisance models (e.g., propensity score model); the\nTL estimator of the nuisance parameters is plugged into downstream average\ncausal/treatment effect estimators (e.g., inverse probability weighted\nestimator). We establish non-asymptotic recovery guarantees for the\n\\texttt{$\\ell_1$-TCL} with generalized linear model (GLM) under the sparsity\nassumption in the high-dimensional setting, and demonstrate the empirical\nbenefits of \\texttt{$\\ell_1$-TCL} through extensive numerical simulation for\nGLM and recent neural network nuisance models. Our method is subsequently\nextended to real data and generates meaningful insights consistent with medical\nliterature, a case where all baseline methods fail."}, "http://arxiv.org/abs/2305.12789": {"title": "The Decaying Missing-at-Random Framework: Doubly Robust Causal Inference with Partially Labeled Data", "link": "http://arxiv.org/abs/2305.12789", "description": "In real-world scenarios, data collection limitations often result in\npartially labeled datasets, leading to difficulties in drawing reliable causal\ninferences. Traditional approaches in the semi-supervised (SS) and missing data\nliterature may not adequately handle these complexities, leading to biased\nestimates. To address these challenges, our paper introduces a novel decaying\nmissing-at-random (decaying MAR) framework. This framework tackles missing\noutcomes in high-dimensional settings and accounts for selection bias arising\nfrom the dependence of labeling probability on covariates. Notably, we relax\nthe need for a positivity condition, commonly required in the missing data\nliterature, and allow uniform decay of labeling propensity scores with sample\nsize, accommodating faster growth of unlabeled data. Our decaying MAR framework\nenables easy rate double-robust (DR) estimation of average treatment effects,\nsucceeding where other methods fail, even with correctly specified nuisance\nmodels. Additionally, it facilitates asymptotic normality under model\nmisspecification. To achieve this, we propose adaptive new targeted\nbias-reducing nuisance estimators and asymmetric cross-fitting, along with a\nnovel semi-parametric approach that fully leverages large volumes of unlabeled\ndata. Our approach requires weak sparsity conditions. Numerical results confirm\nour estimators' efficacy and versatility, addressing selection bias and model\nmisspecification."}, "http://arxiv.org/abs/2401.00872": {"title": "On discriminating between Libby-Novick generalized beta and Kumaraswamy distributions: theory and methods", "link": "http://arxiv.org/abs/2401.00872", "description": "In fitting a continuous bounded data, the generalized beta (and several\nvariants of this distribution) and the two-parameter Kumaraswamy (KW)\ndistributions are the two most prominent univariate continuous distributions\nthat come to our mind. There are some common features between these two rival\nprobability models and to select one of them in a practical situation can be of\ngreat interest. Consequently, in this paper, we discuss various methods of\nselection between the generalized beta proposed by Libby and Novick (1982)\n(LNGB) and the KW distributions, such as the criteria based on probability of\ncorrect selection which is an improvement over the likelihood ratio statistic\napproach, and also based on pseudo-distance measures. We obtain an\napproximation for the probability of correct selection under the hypotheses\nHLNGB and HKW , and select the model that maximizes it. However, our proposal\nis more appealing in the sense that we provide the comparison study for the\nLNGB distribution that subsumes both types of classical beta and exponentiated\ngenerators (see, for details, Cordeiro et al. 2014; Libby and Novick 1982)\nwhich can be a natural competitor of a two-parameter KW distribution in an\nappropriate scenario."}, "http://arxiv.org/abs/2401.00945": {"title": "A review of Monte Carlo-based versions of the EM algorithm", "link": "http://arxiv.org/abs/2401.00945", "description": "The EM algorithm is a powerful tool for maximum likelihood estimation with\nmissing data. In practice, the calculations required for the EM algorithm are\noften intractable. We review numerous methods to circumvent this\nintractability, all of which are based on Monte Carlo simulation. We focus our\nattention on the Monte Carlo EM (MCEM) algorithm and its various\nimplementations. We also discuss some related methods like stochastic\napproximation and Monte Carlo maximum likelihood. Generating the Monte Carlo\nsamples necessary for these methods is, in general, a hard problem. As such, we\nreview several simulation strategies which can be used to address this\nchallenge.\n\nGiven the wide range of methods available for approximating the EM, it can be\nchallenging to select which one to use. We review numerous comparisons between\nthese methods from a wide range of sources, and offer guidance on synthesizing\nthe findings. Finally, we give some directions for future research to fill\nimportant gaps in the existing literature on the MCEM algorithm and related\nmethods."}, "http://arxiv.org/abs/2401.00987": {"title": "Inverting estimating equations for causal inference on quantiles", "link": "http://arxiv.org/abs/2401.00987", "description": "The causal inference literature frequently focuses on estimating the mean of\nthe potential outcome, whereas the quantiles of the potential outcome may carry\nimportant additional information. We propose a universal approach, based on the\ninverse estimating equations, to generalize a wide class of causal inference\nsolutions from estimating the mean of the potential outcome to its quantiles.\nWe assume that an identifying moment function is available to identify the mean\nof the threshold-transformed potential outcome, based on which a convenient\nconstruction of the estimating equation of quantiles of potential outcome is\nproposed. In addition, we also give a general construction of the efficient\ninfluence functions of the mean and quantiles of potential outcomes, and\nidentify their connection. We motivate estimators for the quantile estimands\nwith the efficient influence function, and develop their asymptotic properties\nwhen either parametric models or data-adaptive machine learners are used to\nestimate the nuisance functions. A broad implication of our results is that one\ncan rework the existing result for mean causal estimands to facilitate causal\ninference on quantiles, rather than starting from scratch. Our results are\nillustrated by several examples."}, "http://arxiv.org/abs/2401.01064": {"title": "Robust Inference for Multiple Predictive Regressions with an Application on Bond Risk Premia", "link": "http://arxiv.org/abs/2401.01064", "description": "We propose a robust hypothesis testing procedure for the predictability of\nmultiple predictors that could be highly persistent. Our method improves the\npopular extended instrumental variable (IVX) testing (Phillips and Lee, 2013;\nKostakis et al., 2015) in that, besides addressing the two bias effects found\nin Hosseinkouchack and Demetrescu (2021), we find and deal with the\nvariance-enlargement effect. We show that two types of higher-order terms\ninduce these distortion effects in the test statistic, leading to significant\nover-rejection for one-sided tests and tests in multiple predictive\nregressions. Our improved IVX-based test includes three steps to tackle all the\nissues above regarding finite sample bias and variance terms. Thus, the test\nstatistics perform well in size control, while its power performance is\ncomparable with the original IVX. Monte Carlo simulations and an empirical\nstudy on the predictability of bond risk premia are provided to demonstrate the\neffectiveness of the newly proposed approach."}, "http://arxiv.org/abs/2401.01234": {"title": "Mixture cure semiparametric additive hazard models under partly interval censoring -- a penalized likelihood approach", "link": "http://arxiv.org/abs/2401.01234", "description": "Survival analysis can sometimes involve individuals who will not experience\nthe event of interest, forming what is known as the cured group. Identifying\nsuch individuals is not always possible beforehand, as they provide only\nright-censored data. Ignoring the presence of the cured group can introduce\nbias in the final model. This paper presents a method for estimating a\nsemiparametric additive hazards model that accounts for the cured fraction.\nUnlike regression coefficients in a hazard ratio model, those in an additive\nhazard model measure hazard differences. The proposed method uses a primal-dual\ninterior point algorithm to obtain constrained maximum penalized likelihood\nestimates of the model parameters, including the regression coefficients and\nthe baseline hazard, subject to certain non-negativity constraints."}, "http://arxiv.org/abs/2401.01264": {"title": "Multiple Randomization Designs: Estimation and Inference with Interference", "link": "http://arxiv.org/abs/2401.01264", "description": "Classical designs of randomized experiments, going back to Fisher and Neyman\nin the 1930s still dominate practice even in online experimentation. However,\nsuch designs are of limited value for answering standard questions in settings,\ncommon in marketplaces, where multiple populations of agents interact\nstrategically, leading to complex patterns of spillover effects. In this paper,\nwe discuss new experimental designs and corresponding estimands to account for\nand capture these complex spillovers. We derive the finite-sample properties of\ntractable estimators for main effects, direct effects, and spillovers, and\npresent associated central limit theorems."}, "http://arxiv.org/abs/2401.01294": {"title": "Efficient Sparse Least Absolute Deviation Regression with Differential Privacy", "link": "http://arxiv.org/abs/2401.01294", "description": "In recent years, privacy-preserving machine learning algorithms have\nattracted increasing attention because of their important applications in many\nscientific fields. However, in the literature, most privacy-preserving\nalgorithms demand learning objectives to be strongly convex and Lipschitz\nsmooth, which thus cannot cover a wide class of robust loss functions (e.g.,\nquantile/least absolute loss). In this work, we aim to develop a fast\nprivacy-preserving learning solution for a sparse robust regression problem.\nOur learning loss consists of a robust least absolute loss and an $\\ell_1$\nsparse penalty term. To fast solve the non-smooth loss under a given privacy\nbudget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE)\nalgorithm for least absolute deviation regression. Our algorithm achieves a\nfast estimation by reformulating the sparse LAD problem as a penalized least\nsquare estimation problem and adopts a three-stage noise injection to guarantee\nthe $(\\epsilon,\\delta)$-differential privacy. We show that our algorithm can\nachieve better privacy and statistical accuracy trade-off compared with the\nstate-of-the-art privacy-preserving regression algorithms. In the end, we\nconduct experiments to verify the efficiency of our proposed FRAPPE algorithm."}, "http://arxiv.org/abs/2112.07145": {"title": "Linear Discriminant Analysis with High-dimensional Mixed Variables", "link": "http://arxiv.org/abs/2112.07145", "description": "Datasets containing both categorical and continuous variables are frequently\nencountered in many areas, and with the rapid development of modern measurement\ntechnologies, the dimensions of these variables can be very high. Despite the\nrecent progress made in modelling high-dimensional data for continuous\nvariables, there is a scarcity of methods that can deal with a mixed set of\nvariables. To fill this gap, this paper develops a novel approach for\nclassifying high-dimensional observations with mixed variables. Our framework\nbuilds on a location model, in which the distributions of the continuous\nvariables conditional on categorical ones are assumed Gaussian. We overcome the\nchallenge of having to split data into exponentially many cells, or\ncombinations of the categorical variables, by kernel smoothing, and provide new\nperspectives for its bandwidth choice to ensure an analogue of Bochner's Lemma,\nwhich is different to the usual bias-variance tradeoff. We show that the two\nsets of parameters in our model can be separately estimated and provide\npenalized likelihood for their estimation. Results on the estimation accuracy\nand the misclassification rates are established, and the competitive\nperformance of the proposed classifier is illustrated by extensive simulation\nand real data studies."}, "http://arxiv.org/abs/2206.02164": {"title": "Estimating and Mitigating the Congestion Effect of Curbside Pick-ups and Drop-offs: A Causal Inference Approach", "link": "http://arxiv.org/abs/2206.02164", "description": "Curb space is one of the busiest areas in urban road networks. Especially in\nrecent years, the rapid increase of ride-hailing trips and commercial\ndeliveries has induced massive pick-ups/drop-offs (PUDOs), which occupy the\nlimited curb space that was designed and built decades ago. These PUDOs could\njam curbside utilization and disturb the mainline traffic flow, evidently\nleading to significant negative societal externalities. However, there is a\nlack of an analytical framework that rigorously quantifies and mitigates the\ncongestion effect of PUDOs in the system view, particularly with little data\nsupport and involvement of confounding effects. To bridge this research gap,\nthis paper develops a rigorous causal inference approach to estimate the\ncongestion effect of PUDOs on general regional networks. A causal graph is set\nto represent the spatio-temporal relationship between PUDOs and traffic speed,\nand a double and separated machine learning (DSML) method is proposed to\nquantify how PUDOs affect traffic congestion. Additionally, a re-routing\nformulation is developed and solved to encourage passenger walking and traffic\nflow re-routing to achieve system optimization. Numerical experiments are\nconducted using real-world data in the Manhattan area. On average, 100\nadditional units of PUDOs in a region could reduce the traffic speed by 3.70\nand 4.54 mph on weekdays and weekends, respectively. Re-routing trips with\nPUDOs on curb space could respectively reduce the system-wide total travel time\nby 2.44% and 2.12% in Midtown and Central Park on weekdays. Sensitivity\nanalysis is also conducted to demonstrate the effectiveness and robustness of\nthe proposed framework."}, "http://arxiv.org/abs/2208.00124": {"title": "Physical Parameter Calibration", "link": "http://arxiv.org/abs/2208.00124", "description": "Computer simulation models are widely used to study complex physical systems.\nA related fundamental topic is the inverse problem, also called calibration,\nwhich aims at learning about the values of parameters in the model based on\nobservations. In most real applications, the parameters have specific physical\nmeanings, and we call them physical parameters. To recognize the true\nunderlying physical system, we need to effectively estimate such parameters.\nHowever, existing calibration methods cannot do this well due to the model\nidentifiability problem. This paper proposes a semi-parametric model, called\nthe discrepancy decomposition model, to describe the discrepancy between the\nphysical system and the computer model. The proposed model possesses a clear\ninterpretation, and more importantly, it is identifiable under mild conditions.\nUnder this model, we present estimators of the physical parameters and the\ndiscrepancy, and then establish their asymptotic properties. Numerical examples\nshow that the proposed method can better estimate the physical parameters than\nexisting methods."}, "http://arxiv.org/abs/2302.01974": {"title": "Conic Sparsity: Estimation of Regression Parameters in Closed Convex Polyhedral Cones", "link": "http://arxiv.org/abs/2302.01974", "description": "Statistical problems often involve linear equality and inequality constraints\non model parameters. Direct estimation of parameters restricted to general\npolyhedral cones, particularly when one is interested in estimating low\ndimensional features, may be challenging. We use a dual form parameterization\nto characterize parameter vectors restricted to lower dimensional faces of\npolyhedral cones and use the characterization to define a notion of 'sparsity'\non such cones. We show that the proposed notion agrees with the usual notion of\nsparsity in the unrestricted case and prove the validity of the proposed\ndefinition as a measure of sparsity. The identifiable parameterization of the\nlower dimensional faces allows a generalization of popular spike-and-slab\npriors to a closed convex polyhedral cone. The prior measure utilizes the\ngeometry of the cone by defining a Markov random field over the adjacency graph\nof the extreme rays of the cone. We describe an efficient way of computing the\nposterior of the parameters in the restricted case. We illustrate the\nusefulness of the proposed methodology for imposing linear equality and\ninequality constraints by using wearables data from the National Health and\nNutrition Examination Survey (NHANES) actigraph study where the daily average\nactivity profiles of participants exhibit patterns that seem to obey such\nconstraints."}, "http://arxiv.org/abs/2401.01426": {"title": "Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference", "link": "http://arxiv.org/abs/2401.01426", "description": "Pearl's causal hierarchy establishes a clear separation between\nobservational, interventional, and counterfactual questions. Researchers\nproposed sound and complete algorithms to compute identifiable causal queries\nat a given level of the hierarchy using the causal structure and data from the\nlower levels of the hierarchy. However, most of these algorithms assume that we\ncan accurately estimate the probability distribution of the data, which is an\nimpractical assumption for high-dimensional variables such as images. On the\nother hand, modern generative deep learning architectures can be trained to\nlearn how to accurately sample from such high-dimensional distributions.\nEspecially with the recent rise of foundation models for images, it is\ndesirable to leverage pre-trained models to answer causal queries with such\nhigh-dimensional data. To address this, we propose a sequential training\nalgorithm that, given the causal structure and a pre-trained conditional\ngenerative model, can train a deep causal generative model, which utilizes the\npre-trained model and can provably sample from identifiable interventional and\ncounterfactual distributions. Our algorithm, called Modular-DCM, uses\nadversarial training to learn the network weights, and to the best of our\nknowledge, is the first algorithm that can make use of pre-trained models and\nprovably sample from any identifiable causal query in the presence of latent\nconfounders with high-dimensional data. We demonstrate the utility of our\nalgorithm using semi-synthetic and real-world datasets containing images as\nvariables in the causal structure."}, "http://arxiv.org/abs/2401.01500": {"title": "Log-concave Density Estimation with Independent Components", "link": "http://arxiv.org/abs/2401.01500", "description": "We propose a method for estimating a log-concave density on $\\mathbb R^d$\nfrom samples, under the assumption that there exists an orthogonal\ntransformation that makes the components of the random vector independent.\nWhile log-concave density estimation is hard both computationally and\nstatistically, the independent components assumption alleviates both issues,\nwhile still maintaining a large non-parametric class. We prove that under mild\nconditions, at most $\\tilde{\\mathcal{O}}(\\epsilon^{-4})$ samples (suppressing\nconstants and log factors) suffice for our proposed estimator to be within\n$\\epsilon$ of the original density in squared Hellinger distance. On the\ncomputational front, while the usual log-concave maximum likelihood estimate\ncan be obtained via a finite-dimensional convex program, it is slow to compute\n-- especially in higher dimensions. We demonstrate through numerical\nexperiments that our estimator can be computed efficiently, making it more\npractical to use."}, "http://arxiv.org/abs/2401.01665": {"title": "A Wild Bootstrap Procedure for the Identification of Optimal Groups in Singular Spectrum Analysis", "link": "http://arxiv.org/abs/2401.01665", "description": "A key step in separating signal from noise using Singular Spectrum Analysis\n(SSA) is grouping, which is often done subjectively. In this article a method\nwhich enables the identification of statistically significant groups for the\ngrouping step in SSA is presented. The proposed procedure provides a more\nobjective and reliable approach for separating noise from the main signal in\nSSA. We utilize the w- correlation and test if it close or equal to zero. A\nwild bootstrap approach is used to determine the distribution of the\nw-correlation. To identify an ideal number of groupings which leads to almost\nperfect separation of the noise and signal, a given number of groups are\ntested, necessitating accounting for multiplicity. The effectiveness of our\nmethod in identifying the best group is demonstrated through a simulation\nstudy, furthermore, we have applied the approach to real world data in the\ncontext of neuroimaging. This research provides a valuable contribution to the\nfield of SSA and offers important insights into the statistical properties of\nthe w-correlation distribution. The results obtained from the simulation\nstudies and analysis of real-world data demonstrate the effectiveness of the\nproposed approach in identifying the best groupings for SSA."}, "http://arxiv.org/abs/2401.01713": {"title": "Multiple testing of interval composite null hypotheses using randomized p-values", "link": "http://arxiv.org/abs/2401.01713", "description": "One class of statistical hypothesis testing procedures is the indisputable\nequivalence tests, whose main objective is to establish practical equivalence\nrather than the usual statistical significant difference. These hypothesis\ntests are prone in bioequivalence studies, where one would wish to show that,\nfor example, an existing drug and a new one under development have the same\ntherapeutic effect. In this article, we consider a two-stage randomized (RAND2)\np-value utilizing the uniformly most powerful (UMP) p-value in the first stage\nwhen multiple two-one-sided hypotheses are of interest. We investigate the\nbehavior of the distribution functions of the two p-values when there are\nchanges in the boundaries of the null or alternative hypothesis or when the\nchosen parameters are too close to these boundaries. We also consider the\nbehavior of the power functions to an increase in sample size. Specifically, we\ninvestigate the level of conservativity to the sample sizes to see if we\ncontrol the type I error rate when using either of the two p-values for any\nsample size. In multiple tests, we evaluate the performance of the two p-values\nin estimating the proportion of true null hypotheses. We conduct a family-wise\nerror rate control using an adaptive Bonferroni procedure with a plug-in\nestimator to account for the multiplicity that arises from the multiple\nhypotheses under consideration. We verify the various claims in this research\nusing simulation study and real-world data analysis."}, "http://arxiv.org/abs/2401.01806": {"title": "A complex meta-regression model to identify effective features of interventions from multi-arm, multi-follow-up trials", "link": "http://arxiv.org/abs/2401.01806", "description": "Network meta-analysis (NMA) combines evidence from multiple trials to compare\nthe effectiveness of a set of interventions. In public health research,\ninterventions are often complex, made up of multiple components or features.\nThis makes it difficult to define a common set of interventions on which to\nperform the analysis. One approach to this problem is component network\nmeta-analysis (CNMA) which uses a meta-regression framework to define each\nintervention as a subset of components whose individual effects combine\nadditively. In this paper, we are motivated by a systematic review of complex\ninterventions to prevent obesity in children. Due to considerable heterogeneity\nacross the trials, these interventions cannot be expressed as a subset of\ncomponents but instead are coded against a framework of characteristic\nfeatures. To analyse these data, we develop a bespoke CNMA-inspired model that\nallows us to identify the most important features of interventions. We define a\nmeta-regression model with covariates on three levels: intervention, study, and\nfollow-up time, as well as flexible interaction terms. By specifying different\nregression structures for trials with and without a control arm, we relax the\nassumption from previous CNMA models that a control arm is the absence of\nintervention components. Furthermore, we derive a correlation structure that\naccounts for trials with multiple intervention arms and multiple follow-up\ntimes. Although our model was developed for the specifics of the obesity data\nset, it has wider applicability to any set of complex interventions that can be\ncoded according to a set of shared features."}, "http://arxiv.org/abs/2401.01833": {"title": "Credible Distributions of Overall Ranking of Entities", "link": "http://arxiv.org/abs/2401.01833", "description": "Inference on overall ranking of a set of entities, such as chess players,\nsubpopulations or hospitals, is an important problem. Estimation of ranks based\non point estimates of means does not account for the uncertainty in those\nestimates. Treating estimated ranks without regard for uncertainty is\nproblematic. We propose a Bayesian solution. It is competitive with recent\nfrequentist methods, and more effective and informative, and is as easy to\nimplement as it is to compute the posterior means and variances of the entity\nmeans. Using credible sets, we created novel credible distributions for the\nrank vector of the entities. We evaluate the Bayesian procedure in terms of\naccuracy and stability in two applications and a simulation study. Frequentist\napproaches cannot take account of covariates, but the Bayesian method handles\nthem easily."}, "http://arxiv.org/abs/2401.01872": {"title": "Multiple Imputation of Hierarchical Nonlinear Time Series Data with an Application to School Enrollment Data", "link": "http://arxiv.org/abs/2401.01872", "description": "International comparisons of hierarchical time series data sets based on\nsurvey data, such as annual country-level estimates of school enrollment rates,\ncan suffer from large amounts of missing data due to differing coverage of\nsurveys across countries and across times. A popular approach to handling\nmissing data in these settings is through multiple imputation, which can be\nespecially effective when there is an auxiliary variable that is strongly\npredictive of and has a smaller amount of missing data than the variable of\ninterest. However, standard methods for multiple imputation of hierarchical\ntime series data can perform poorly when the auxiliary variable and the\nvariable of interest are have a nonlinear relationship. Performance of standard\nmultiple imputation methods can also suffer if the substantive analysis model\nof interest is uncongenial to the imputation model, which can be a common\noccurrence for social science data if the imputation phase is conducted\nindependently of the analysis phase. We propose a Bayesian method for multiple\nimputation of hierarchical nonlinear time series data that uses a sequential\ndecomposition of the joint distribution and incorporates smoothing splines to\naccount for nonlinear relationships between variables. We compare the proposed\nmethod with existing multiple imputation methods through a simulation study and\nan application to secondary school enrollment data. We find that the proposed\nmethod can lead to substantial performance increases for estimation of\nparameters in uncongenial analysis models and for prediction of individual\nmissing values."}, "http://arxiv.org/abs/2208.10962": {"title": "Prediction of good reaction coordinates and future evolution of MD trajectories using Regularized Sparse Autoencoders: A novel deep learning approach", "link": "http://arxiv.org/abs/2208.10962", "description": "Identifying reaction coordinates(RCs) is an active area of research, given\nthe crucial role RCs play in determining the progress of a chemical reaction.\nThe choice of the reaction coordinate is often based on heuristic knowledge.\nHowever, an essential criterion for the choice is that the coordinate should\ncapture both the reactant and product states unequivocally. Also, the\ncoordinate should be the slowest one so that all the other degrees of freedom\ncan easily equilibrate along the reaction coordinate. Also, the coordinate\nshould be the slowest one so that all the other degrees of freedom can easily\nequilibrate along the reaction coordinate. We used a regularised sparse\nautoencoder, an energy-based model, to discover a crucial set of reaction\ncoordinates. Along with discovering reaction coordinates, our model also\npredicts the evolution of a molecular dynamics(MD) trajectory. We showcased\nthat including sparsity enforcing regularisation helps in choosing a small but\nimportant set of reaction coordinates. We used two model systems to demonstrate\nour approach: alanine dipeptide system and proflavine and DNA system, which\nexhibited intercalation of proflavine into DNA minor groove in an aqueous\nenvironment. We model MD trajectory as a multivariate time series, and our\nlatent variable model performs the task of multi-step time series prediction.\nThis idea is inspired by the popular sparse coding approach - to represent each\ninput sample as a linear combination of few elements taken from a set of\nrepresentative patterns."}, "http://arxiv.org/abs/2308.01704": {"title": "Similarity-based Random Partition Distribution for Clustering Functional Data", "link": "http://arxiv.org/abs/2308.01704", "description": "Random partition distribution is a crucial tool for model-based clustering.\nThis study advances the field of random partition in the context of functional\nspatial data, focusing on the challenges posed by hourly population data across\nvarious regions and dates. We propose an extended generalized Dirichlet\nprocess, named the similarity-based generalized Dirichlet process (SGDP), to\naddress the limitations of simple random partition distributions (e.g., those\ninduced by the Dirichlet process), such as an overabundance of clusters. This\nmodel prevents producing excess clusters as well as incorporates pairwise\nsimilarity information to ensure a more accurate and meaningful grouping. The\ntheoretical properties of SGDP are studied. Then, SGDP is applied to a\nreal-world dataset of hourly population flows in 500$\\rm{m}^2$ meshes in the\ncentral part of Tokyo. In this empirical context, SGDP excelled at detecting\nmeaningful patterns in the data while accounting for spatial nuances. The\nresults underscore the adaptability and utility of the method, showcasing its\nprowess in revealing intricate spatiotemporal dynamics. This study's findings\ncontribute significantly to urban planning, transportation, and policy-making\nby providing a helpful tool for understanding population dynamics and their\nimplications."}, "http://arxiv.org/abs/2401.01949": {"title": "Adjacency Matrix Decomposition Clustering for Human Activity Data", "link": "http://arxiv.org/abs/2401.01949", "description": "Mobile apps and wearable devices accurately and continuously measure human\nactivity; patterns within this data can provide a wealth of information\napplicable to fields such as transportation and health. Despite the potential\nutility of this data, there has been limited development of analysis methods\nfor sequences of daily activities. In this paper, we propose a novel clustering\nmethod and cluster evaluation metric for human activity data that leverages an\nadjacency matrix representation to cluster the data without the calculation of\na distance matrix. Our technique is substantially faster than conventional\nmethods based on computing pairwise distances via sequence alignment algorithms\nand also enhances interpretability of results. We compare our method to\ndistance-based hierarchical clustering through simulation studies and an\napplication to data collected by Daynamica, an app that turns raw sensor data\ninto a daily summary of a user's activities. Among days that contain a large\nportion of time spent at home, our method distinguishes days that also contain\nfull-time work or multiple hours of travel, while hierarchical clustering\ngroups these days together. We also illustrate how the computational advantage\nof our method enables the analysis of longer sequences by clustering full weeks\nof activity data."}, "http://arxiv.org/abs/2401.01966": {"title": "Ethical considerations for data involving human gender and sex variables", "link": "http://arxiv.org/abs/2401.01966", "description": "The inclusion of human sex and gender data in statistical analysis invokes\nmultiple considerations for data collection, combination, analysis, and\ninterpretation. These considerations are not unique to variables representing\nsex and gender. However, considering the relevance of the ethical practice\nstandards for statistics and data science to sex and gender variables is\ntimely, with results that can be applied to other sociocultural variables.\nHistorically, human gender and sex have been categorized with a binary system.\nThis tradition persists mainly because it is easy, and not because it produces\nthe best scientific information. Binary classification simplifies combinations\nof older and newer data sets. However, this classification system eliminates\nthe ability for respondents to articulate their gender identity, conflates\ngender and sex, and also obscures potentially important differences by\ncollapsing across valid and authentic categories. This approach perpetuates\nhistorical inaccuracy, simplicity, and bias, while also limiting the\ninformation that emerges from analyses of human data. The approach also\nviolates multiple elements in the American Statistical Association (ASA)\nEthical Guidelines for Statistical Practice. Information that would be captured\nwith a nonbinary classification could be relevant to decisions about analysis\nmethods and to decisions based on otherwise expert statistical work.\nStatistical practitioners are increasingly concerned with inconsistent,\nuninformative, and even unethical data collection and analysis practices. This\npaper presents a historical introduction to the collection and analysis of\nhuman gender and sex data, offers a critique of a few common survey questioning\nmethods based on alignment with the ASA Ethical Guidelines, and considers the\nscope of ethical considerations for human gender and sex data from design\nthrough analysis and interpretation."}, "http://arxiv.org/abs/2401.01977": {"title": "Conformal causal inference for cluster randomized trials: model-robust inference without asymptotic approximations", "link": "http://arxiv.org/abs/2401.01977", "description": "In the analysis of cluster randomized trials, two typical features are that\nindividuals within a cluster are correlated and that the total number of\nclusters can sometimes be limited. While model-robust treatment effect\nestimators have been recently developed, their asymptotic theory requires the\nnumber of clusters to approach infinity, and one often has to empirically\nassess the applicability of those methods in finite samples. To address this\nchallenge, we propose a conformal causal inference framework that achieves the\ntarget coverage probability of treatment effects in finite samples without the\nneed for asymptotic approximations. Meanwhile, we prove that this framework is\ncompatible with arbitrary working models, including machine learning algorithms\nleveraging baseline covariates, possesses robustness against arbitrary\nmisspecification of working models, and accommodates a variety of\nwithin-cluster correlations. Under this framework, we offer efficient\nalgorithms to make inferences on treatment effects at both the cluster and\nindividual levels, applicable to user-specified covariate subgroups and two\ntypes of test data. Finally, we demonstrate our methods via simulations and a\nreal data application based on a cluster randomized trial for treating chronic\npain."}, "http://arxiv.org/abs/2401.02048": {"title": "Random Effect Restricted Mean Survival Time Model", "link": "http://arxiv.org/abs/2401.02048", "description": "The restricted mean survival time (RMST) model has been garnering attention\nas a way to provide a clinically intuitive measure: the mean survival time.\nRMST models, which use methods based on pseudo time-to-event values and inverse\nprobability censoring weighting, can adjust covariates. However, no approach\nhas yet been introduced that considers random effects for clusters. In this\npaper, we propose a new random-effect RMST. We present two methods of analysis\nthat consider variable effects by i) using a generalized mixed model with\npseudo-values and ii) integrating the estimated results from the inverse\nprobability censoring weighting estimating equations for each cluster. We\nevaluate our proposed methods through computer simulations. In addition, we\nanalyze the effect of a mother's age at birth on under-five deaths in India\nusing states as clusters."}, "http://arxiv.org/abs/2401.02154": {"title": "Disentangle Estimation of Causal Effects from Cross-Silo Data", "link": "http://arxiv.org/abs/2401.02154", "description": "Estimating causal effects among different events is of great importance to\ncritical fields such as drug development. Nevertheless, the data features\nassociated with events may be distributed across various silos and remain\nprivate within respective parties, impeding direct information exchange between\nthem. This, in turn, can result in biased estimations of local causal effects,\nwhich rely on the characteristics of only a subset of the covariates. To tackle\nthis challenge, we introduce an innovative disentangle architecture designed to\nfacilitate the seamless cross-silo transmission of model parameters, enriched\nwith causal mechanisms, through a combination of shared and private branches.\nBesides, we introduce global constraints into the equation to effectively\nmitigate bias within the various missing domains, thereby elevating the\naccuracy of our causal effect estimation. Extensive experiments conducted on\nnew semi-synthetic datasets show that our method outperforms state-of-the-art\nbaselines."}, "http://arxiv.org/abs/2401.02387": {"title": "Assessing Time Series Correlation Significance: A Parametric Approach with Application to Physiological Signals", "link": "http://arxiv.org/abs/2401.02387", "description": "Correlation coefficients play a pivotal role in quantifying linear\nrelationships between random variables. Yet, their application to time series\ndata is very challenging due to temporal dependencies. This paper introduces a\nnovel approach to estimate the statistical significance of correlation\ncoefficients in time series data, addressing the limitations of traditional\nmethods based on the concept of effective degrees of freedom (or effective\nsample size, ESS). These effective degrees of freedom represent the independent\nsample size that would yield comparable test statistics under the assumption of\nno temporal correlation. We propose to assume a parametric Gaussian form for\nthe autocorrelation function. We show that this assumption, motivated by a\nLaplace approximation, enables a simple estimator of the ESS that depends only\non the temporal derivatives of the time series. Through numerical experiments,\nwe show that the proposed approach yields accurate statistics while\nsignificantly reducing computational overhead. In addition, we evaluate the\nadequacy of our approach on real physiological signals, for assessing the\nconnectivity measures in electrophysiology and detecting correlated arm\nmovements in motion capture data. Our methodology provides a simple tool for\nresearchers working with time series data, enabling robust hypothesis testing\nin the presence of temporal dependencies."}, "http://arxiv.org/abs/1712.02195": {"title": "Fast approximations in the homogeneous Ising model for use in scene analysis", "link": "http://arxiv.org/abs/1712.02195", "description": "The Ising model is important in statistical modeling and inference in many\napplications, however its normalizing constant, mean number of active vertices\nand mean spin interaction -- quantities needed in inference -- are\ncomputationally intractable. We provide accurate approximations that make it\npossible to numerically calculate these quantities in the homogeneous case.\nSimulation studies indicate good performance of our approximation formulae that\nare scalable and unfazed by the size (number of nodes, degree of graph) of the\nMarkov Random Field. The practical import of our approximation formulae is\nillustrated in performing Bayesian inference in a functional Magnetic Resonance\nImaging activation detection experiment, and also in likelihood ratio testing\nfor anisotropy in the spatial patterns of yearly increases in pistachio tree\nyields."}, "http://arxiv.org/abs/2203.12179": {"title": "Targeted Function Balancing", "link": "http://arxiv.org/abs/2203.12179", "description": "This paper introduces Targeted Function Balancing (TFB), a covariate\nbalancing weights framework for estimating the average treatment effect of a\nbinary intervention. TFB first regresses an outcome on covariates, and then\nselects weights that balance functions (of the covariates) that are\nprobabilistically near the resulting regression function. This yields balance\nin the regression function's predicted values and the covariates, with the\nregression function's estimated variance determining how much balance in the\ncovariates is sufficient. Notably, TFB demonstrates that intentionally leaving\nimbalance in some covariates can increase efficiency without introducing bias,\nchallenging traditions that warn against imbalance in any variable.\nAdditionally, TFB is entirely defined by a regression function and its\nestimated variance, turning the problem of how best to balance the covariates\ninto how best to model the outcome. Kernel regularized least squares and the\nLASSO are considered as regression estimators. With the former, TFB contributes\nto the literature of kernel-based weights. As for the LASSO, TFB uses the\nregression function's estimated variance to prioritize balance in certain\ndimensions of the covariates, a feature that can be greatly exploited by\nchoosing a sparse regression estimator. This paper also introduces a balance\ndiagnostic, Targeted Function Imbalance, that may have useful applications."}, "http://arxiv.org/abs/2303.01572": {"title": "Transportability without positivity: a synthesis of statistical and simulation modeling", "link": "http://arxiv.org/abs/2303.01572", "description": "When estimating an effect of an action with a randomized or observational\nstudy, that study is often not a random sample of the desired target\npopulation. Instead, estimates from that study can be transported to the target\npopulation. However, transportability methods generally rely on a positivity\nassumption, such that all relevant covariate patterns in the target population\nare also observed in the study sample. Strict eligibility criteria,\nparticularly in the context of randomized trials, may lead to violations of\nthis assumption. Two common approaches to address positivity violations are\nrestricting the target population and restricting the relevant covariate set.\nAs neither of these restrictions are ideal, we instead propose a synthesis of\nstatistical and simulation models to address positivity violations. We propose\ncorresponding g-computation and inverse probability weighting estimators. The\nrestriction and synthesis approaches to addressing positivity violations are\ncontrasted with a simulation experiment and an illustrative example in the\ncontext of sexually transmitted infection testing uptake. In both cases, the\nproposed synthesis approach accurately addressed the original research question\nwhen paired with a thoughtfully selected simulation model. Neither of the\nrestriction approaches were able to accurately address the motivating question.\nAs public health decisions must often be made with imperfect target population\ninformation, model synthesis is a viable approach given a combination of\nempirical data and external information based on the best available knowledge."}, "http://arxiv.org/abs/2401.02529": {"title": "Simulation-based transition density approximation for the inference of SDE models", "link": "http://arxiv.org/abs/2401.02529", "description": "Stochastic Differential Equations (SDEs) serve as a powerful modeling tool in\nvarious scientific domains, including systems science, engineering, and\necological science. While the specific form of SDEs is typically known for a\ngiven problem, certain model parameters remain unknown. Efficiently inferring\nthese unknown parameters based on observations of the state in discrete time\nseries represents a vital practical subject. The challenge arises in nonlinear\nSDEs, where maximum likelihood estimation of parameters is generally unfeasible\ndue to the absence of closed-form expressions for transition and stationary\nprobability density functions of the states. In response to this limitation, we\npropose a novel two-step parameter inference mechanism. This approach involves\na global-search phase followed by a local-refining procedure. The global-search\nphase is dedicated to identifying the domain of high-value likelihood\nfunctions, while the local-refining procedure is specifically designed to\nenhance the surrogate likelihood within this localized domain. Additionally, we\npresent two simulation-based approximations for the transition density, aiming\nto efficiently or accurately approximate the likelihood function. Numerical\nexamples illustrate the efficacy of our proposed methodology in achieving\nposterior parameter estimation."}, "http://arxiv.org/abs/2401.02557": {"title": "Multivariate Functional Clustering with Variable Selection and Application to Sensor Data from Engineering Systems", "link": "http://arxiv.org/abs/2401.02557", "description": "Multi-sensor data that track system operating behaviors are widely available\nnowadays from various engineering systems. Measurements from each sensor over\ntime form a curve and can be viewed as functional data. Clustering of these\nmultivariate functional curves is important for studying the operating patterns\nof systems. One complication in such applications is the possible presence of\nsensors whose data do not contain relevant information. Hence it is desirable\nfor the clustering method to equip with an automatic sensor selection\nprocedure. Motivated by a real engineering application, we propose a functional\ndata clustering method that simultaneously removes noninformative sensors and\ngroups functional curves into clusters using informative sensors. Functional\nprincipal component analysis is used to transform multivariate functional data\ninto a coefficient matrix for data reduction. We then model the transformed\ndata by a Gaussian mixture distribution to perform model-based clustering with\nvariable selection. Three types of penalties, the individual, variable, and\ngroup penalties, are considered to achieve automatic variable selection.\nExtensive simulations are conducted to assess the clustering and variable\nselection performance of the proposed methods. The application of the proposed\nmethods to an engineering system with multiple sensors shows the promise of the\nmethods and reveals interesting patterns in the sensor data."}, "http://arxiv.org/abs/2401.02694": {"title": "Nonconvex High-Dimensional Time-Varying Coefficient Estimation for Noisy High-Frequency Observations", "link": "http://arxiv.org/abs/2401.02694", "description": "In this paper, we propose a novel high-dimensional time-varying coefficient\nestimator for noisy high-frequency observations. In high-frequency finance, we\noften observe that noises dominate a signal of an underlying true process.\nThus, we cannot apply usual regression procedures to analyze noisy\nhigh-frequency observations. To handle this issue, we first employ a smoothing\nmethod for the observed variables. However, the smoothed variables still\ncontain non-negligible noises. To manage these non-negligible noises and the\nhigh dimensionality, we propose a nonconvex penalized regression method for\neach local coefficient. This method produces consistent but biased local\ncoefficient estimators. To estimate the integrated coefficients, we propose a\ndebiasing scheme and obtain a debiased integrated coefficient estimator using\ndebiased local coefficient estimators. Then, to further account for the\nsparsity structure of the coefficients, we apply a thresholding scheme to the\ndebiased integrated coefficient estimator. We call this scheme the Thresholded\ndEbiased Nonconvex LASSO (TEN-LASSO) estimator. Furthermore, this paper\nestablishes the concentration properties of the TEN-LASSO estimator and\ndiscusses a nonconvex optimization algorithm."}, "http://arxiv.org/abs/2401.02735": {"title": "Shared active subspace for multivariate vector-valued functions", "link": "http://arxiv.org/abs/2401.02735", "description": "This paper proposes several approaches as baselines to compute a shared\nactive subspace for multivariate vector-valued functions. The goal is to\nminimize the deviation between the function evaluations on the original space\nand those on the reconstructed one. This is done either by manipulating the\ngradients or the symmetric positive (semi-)definite (SPD) matrices computed\nfrom the gradients of each component function so as to get a single structure\ncommon to all component functions. These approaches can be applied to any data\nirrespective of the underlying distribution unlike the existing vector-valued\napproach that is constrained to a normal distribution. We test the\neffectiveness of these methods on five optimization problems. The experiments\nshow that, in general, the SPD-level methods are superior to the gradient-level\nones, and are close to the vector-valued approach in the case of a normal\ndistribution. Interestingly, in most cases it suffices to take the sum of the\nSPD matrices to identify the best shared active subspace."}, "http://arxiv.org/abs/2401.02828": {"title": "Optimal prediction of positive-valued spatial processes: asymmetric power-divergence loss", "link": "http://arxiv.org/abs/2401.02828", "description": "This article studies the use of asymmetric loss functions for the optimal\nprediction of positive-valued spatial processes. We focus on the family of\npower-divergence loss functions due to its many convenient properties, such as\nits continuity, convexity, relationship to well known divergence measures, and\nthe ability to control the asymmetry and behaviour of the loss function via a\npower parameter. The properties of power-divergence loss functions, optimal\npower-divergence (OPD) spatial predictors, and related measures of uncertainty\nquantification are examined. In addition, we examine the notion of asymmetry in\nloss functions defined for positive-valued spatial processes and define an\nasymmetry measure that is applied to the power-divergence loss function and\nother common loss functions. The paper concludes with a spatial statistical\nanalysis of zinc measurements in the soil of a floodplain of the Meuse River,\nNetherlands, using OPD spatial prediction."}, "http://arxiv.org/abs/2401.02917": {"title": "Bayesian changepoint detection via logistic regression and the topological analysis of image series", "link": "http://arxiv.org/abs/2401.02917", "description": "We present a Bayesian method for multivariate changepoint detection that\nallows for simultaneous inference on the location of a changepoint and the\ncoefficients of a logistic regression model for distinguishing pre-changepoint\ndata from post-changepoint data. In contrast to many methods for multivariate\nchangepoint detection, the proposed method is applicable to data of mixed type\nand avoids strict assumptions regarding the distribution of the data and the\nnature of the change. The regression coefficients provide an interpretable\ndescription of a potentially complex change. For posterior inference, the model\nadmits a simple Gibbs sampling algorithm based on P\\'olya-gamma data\naugmentation. We establish conditions under which the proposed method is\nguaranteed to recover the true underlying changepoint. As a testing ground for\nour method, we consider the problem of detecting topological changes in time\nseries of images. We demonstrate that the proposed method, combined with a\nnovel topological feature embedding, performs well on both simulated and real\nimage data."}, "http://arxiv.org/abs/2401.02930": {"title": "Dagma-DCE: Interpretable, Non-Parametric Differentiable Causal Discovery", "link": "http://arxiv.org/abs/2401.02930", "description": "We introduce Dagma-DCE, an interpretable and model-agnostic scheme for\ndifferentiable causal discovery. Current non- or over-parametric methods in\ndifferentiable causal discovery use opaque proxies of ``independence'' to\njustify the inclusion or exclusion of a causal relationship. We show\ntheoretically and empirically that these proxies may be arbitrarily different\nthan the actual causal strength. Juxtaposed to existing differentiable causal\ndiscovery algorithms, \\textsc{Dagma-DCE} uses an interpretable measure of\ncausal strength to define weighted adjacency matrices. In a number of simulated\ndatasets, we show our method achieves state-of-the-art level performance. We\nadditionally show that \\textsc{Dagma-DCE} allows for principled thresholding\nand sparsity penalties by domain-experts. The code for our method is available\nopen-source at https://github.com/DanWaxman/DAGMA-DCE, and can easily be\nadapted to arbitrary differentiable models."}, "http://arxiv.org/abs/2401.02939": {"title": "Penalized Distributed Lag Interaction Model: Air Pollution, Birth Weight and Neighborhood Vulnerability", "link": "http://arxiv.org/abs/2401.02939", "description": "Maternal exposure to air pollution during pregnancy has a substantial public\nhealth impact. Epidemiological evidence supports an association between\nmaternal exposure to air pollution and low birth weight. A popular method to\nestimate this association while identifying windows of susceptibility is a\ndistributed lag model (DLM), which regresses an outcome onto exposure history\nobserved at multiple time points. However, the standard DLM framework does not\nallow for modification of the association between repeated measures of exposure\nand the outcome. We propose a distributed lag interaction model that allows\nmodification of the exposure-time-response associations across individuals by\nincluding an interaction between a continuous modifying variable and the\nexposure history. Our model framework is an extension of a standard DLM that\nuses a cross-basis, or bi-dimensional function space, to simultaneously\ndescribe both the modification of the exposure-response relationship and the\ntemporal structure of the exposure data. Through simulations, we showed that\nour model with penalization out-performs a standard DLM when the true\nexposure-time-response associations vary by a continuous variable. Using a\nColorado, USA birth cohort, we estimated the association between birth weight\nand ambient fine particulate matter air pollution modified by an area-level\nmetric of health and social adversities from Colorado EnviroScreen."}, "http://arxiv.org/abs/2401.02953": {"title": "Linked factor analysis", "link": "http://arxiv.org/abs/2401.02953", "description": "Factor models are widely used in the analysis of high-dimensional data in\nseveral fields of research. Estimating a factor model, in particular its\ncovariance matrix, from partially observed data vectors is very challenging. In\nthis work, we show that when the data are structurally incomplete, the factor\nmodel likelihood function can be decomposed into the product of the likelihood\nfunctions of multiple partial factor models relative to different subsets of\ndata. If these multiple partial factor models are linked together by common\nparameters, then we can obtain complete maximum likelihood estimates of the\nfactor model parameters and thereby the full covariance matrix. We call this\nframework Linked Factor Analysis (LINFA). LINFA can be used for covariance\nmatrix completion, dimension reduction, data completion, and graphical\ndependence structure recovery. We propose an efficient Expectation-Maximization\nalgorithm for maximum likelihood estimation, accelerated by a novel group\nvertex tessellation (GVT) algorithm which identifies a minimal partition of the\nvertex set to implement an efficient optimization in the maximization steps. We\nillustrate our approach in an extensive simulation study and in the analysis of\ncalcium imaging data collected from mouse visual cortex."}, "http://arxiv.org/abs/2203.12808": {"title": "Robustness Against Weak or Invalid Instruments: Exploring Nonlinear Treatment Models with Machine Learning", "link": "http://arxiv.org/abs/2203.12808", "description": "We discuss causal inference for observational studies with possibly invalid\ninstrumental variables. We propose a novel methodology called two-stage\ncurvature identification (TSCI) by exploring the nonlinear treatment model with\nmachine learning. {The first-stage machine learning enables improving the\ninstrumental variable's strength and adjusting for different forms of violating\nthe instrumental variable assumptions.} The success of TSCI requires the\ninstrumental variable's effect on treatment to differ from its violation form.\nA novel bias correction step is implemented to remove bias resulting from the\npotentially high complexity of machine learning. Our proposed \\texttt{TSCI}\nestimator is shown to be asymptotically unbiased and Gaussian even if the\nmachine learning algorithm does not consistently estimate the treatment model.\nFurthermore, we design a data-dependent method to choose the best among several\ncandidate violation forms. We apply TSCI to study the effect of education on\nearnings."}, "http://arxiv.org/abs/2208.14011": {"title": "Robust and Efficient Estimation in Ordinal Response Models using the Density Power Divergence", "link": "http://arxiv.org/abs/2208.14011", "description": "In real life, we frequently come across data sets that involve some\nindependent explanatory variable(s) generating a set of ordinal responses.\nThese ordinal responses may correspond to an underlying continuous latent\nvariable, which is linearly related to the covariate(s), and takes a particular\n(ordinal) label depending on whether this latent variable takes value in some\nsuitable interval specified by a pair of (unknown) cut-offs. The most efficient\nway of estimating the unknown parameters (i.e., the regression coefficients and\nthe cut-offs) is the method of maximum likelihood (ML). However, contamination\nin the data set either in the form of misspecification of ordinal responses, or\nthe unboundedness of the covariate(s), might destabilize the likelihood\nfunction to a great extent where the ML based methodology might lead to\ncompletely unreliable inferences. In this paper, we explore a minimum distance\nestimation procedure based on the popular density power divergence (DPD) to\nyield robust parameter estimates for the ordinal response model. This paper\nhighlights how the resulting estimator, namely the minimum DPD estimator\n(MDPDE), can be used as a practical robust alternative to the classical\nprocedures based on the ML. We rigorously develop several theoretical\nproperties of this estimator, and provide extensive simulations to substantiate\nthe theory developed."}, "http://arxiv.org/abs/2303.10018": {"title": "Efficient nonparametric estimation of Toeplitz covariance matrices", "link": "http://arxiv.org/abs/2303.10018", "description": "A new nonparametric estimator for Toeplitz covariance matrices is proposed.\nThis estimator is based on a data transformation that translates the problem of\nToeplitz covariance matrix estimation to the problem of mean estimation in an\napproximate Gaussian regression. The resulting Toeplitz covariance matrix\nestimator is positive definite by construction, fully data-driven and\ncomputationally very fast. Moreover, this estimator is shown to be minimax\noptimal under the spectral norm for a large class of Toeplitz matrices. These\nresults are readily extended to estimation of inverses of Toeplitz covariance\nmatrices. Also, an alternative version of the Whittle likelihood for the\nspectral density based on the Discrete Cosine Transform (DCT) is proposed. The\nmethod is implemented in the R package vstdct that accompanies the paper."}, "http://arxiv.org/abs/2308.11260": {"title": "A simplified spatial+ approach to mitigate spatial confounding in multivariate spatial areal models", "link": "http://arxiv.org/abs/2308.11260", "description": "Spatial areal models encounter the well-known and challenging problem of\nspatial confounding. This issue makes it arduous to distinguish between the\nimpacts of observed covariates and spatial random effects. Despite previous\nresearch and various proposed methods to tackle this problem, finding a\ndefinitive solution remains elusive. In this paper, we propose a simplified\nversion of the spatial+ approach that involves dividing the covariate into two\ncomponents. One component captures large-scale spatial dependence, while the\nother accounts for short-scale dependence. This approach eliminates the need to\nseparately fit spatial models for the covariates. We apply this method to\nanalyse two forms of crimes against women, namely rapes and dowry deaths, in\nUttar Pradesh, India, exploring their relationship with socio-demographic\ncovariates. To evaluate the performance of the new approach, we conduct\nextensive simulation studies under different spatial confounding scenarios. The\nresults demonstrate that the proposed method provides reliable estimates of\nfixed effects and posterior correlations between different responses."}, "http://arxiv.org/abs/2308.15596": {"title": "Double Probability Integral Transform Residuals for Regression Models with Discrete Outcomes", "link": "http://arxiv.org/abs/2308.15596", "description": "The assessment of regression models with discrete outcomes is challenging and\nhas many fundamental issues. With discrete outcomes, standard regression model\nassessment tools such as Pearson and deviance residuals do not follow the\nconventional reference distribution (normal) under the true model, calling into\nquestion the legitimacy of model assessment based on these tools. To fill this\ngap, we construct a new type of residuals for general discrete outcomes,\nincluding ordinal and count outcomes. The proposed residuals are based on two\nlayers of probability integral transformation. When at least one continuous\ncovariate is available, the proposed residuals closely follow a uniform\ndistribution (or a normal distribution after transformation) under the\ncorrectly specified model. One can construct visualizations such as QQ plots to\ncheck the overall fit of a model straightforwardly, and the shape of QQ plots\ncan further help identify possible causes of misspecification such as\noverdispersion. We provide theoretical justification for the proposed residuals\nby establishing their asymptotic properties. Moreover, in order to assess the\nmean structure and identify potential covariates, we develop an ordered curve\nas a supplementary tool, which is based on the comparison between the partial\nsum of outcomes and of fitted means. Through simulation, we demonstrate\nempirically that the proposed tools outperform commonly used residuals for\nvarious model assessment tasks. We also illustrate the workflow of model\nassessment using the proposed tools in data analysis."}, "http://arxiv.org/abs/2401.03072": {"title": "Optimal Nonparametric Inference on Network Effects with Dependent Edges", "link": "http://arxiv.org/abs/2401.03072", "description": "Testing network effects in weighted directed networks is a foundational\nproblem in econometrics, sociology, and psychology. Yet, the prevalent edge\ndependency poses a significant methodological challenge. Most existing methods\nare model-based and come with stringent assumptions, limiting their\napplicability. In response, we introduce a novel, fully nonparametric framework\nthat requires only minimal regularity assumptions. While inspired by recent\ndevelopments in $U$-statistic literature (<a href=\"https://export.arxiv.org/abs/1712.00771\">arXiv:1712.00771</a>, <a href=\"https://export.arxiv.org/abs/2004.06615\">arXiv:2004.06615</a>),\nour approach notably broadens their scopes. Specifically, we identified and\ncarefully addressed the challenge of indeterminate degeneracy in the test\nstatistics $-$ a problem that aforementioned tools do not handle. We\nestablished Berry-Esseen type bound for the accuracy of type-I error rate\ncontrol. Using original analysis, we also proved the minimax optimality of our\ntest's power. Simulations underscore the superiority of our method in\ncomputation speed, accuracy, and numerical robustness compared to competing\nmethods. We also applied our method to the U.S. faculty hiring network data and\ndiscovered intriguing findings."}, "http://arxiv.org/abs/2401.03081": {"title": "Shrinkage Estimation and Prediction for Joint Type-II Censored Data from Two Burr-XII Populations", "link": "http://arxiv.org/abs/2401.03081", "description": "The main objective of this paper is to apply linear and pretest shrinkage\nestimation techniques to estimating the parameters of two 2-parameter Burr-XII\ndistributions. Further more, predictions for future observations are made using\nboth classical and Bayesian methods within a joint type-II censoring scheme.\nThe efficiency of shrinkage estimates is compared to maximum likelihood and\nBayesian estimates obtained through the expectation-maximization algorithm and\nimportance sampling method, as developed by Akbari Bargoshadi et al. (2023) in\n\"Statistical inference under joint type-II censoring data from two Burr-XII\npopulations\" published in Communications in Statistics-Simulation and\nComputation\". For Bayesian estimations, both informative and non-informative\nprior distributions are considered. Additionally, various loss functions\nincluding squared error, linear-exponential, and generalized entropy are taken\ninto account. Approximate confidence, credible, and highest probability density\nintervals are calculated. To evaluate the performance of the estimation\nmethods, a Monte Carlo simulation study is conducted. Additionally, two real\ndatasets are utilized to illustrate the proposed methods."}, "http://arxiv.org/abs/2401.03084": {"title": "Finite sample performance of optimal treatment rule estimators with right-censored outcomes", "link": "http://arxiv.org/abs/2401.03084", "description": "Patient care may be improved by recommending treatments based on patient\ncharacteristics when there is treatment effect heterogeneity. Recently, there\nhas been a great deal of attention focused on the estimation of optimal\ntreatment rules that maximize expected outcomes. However, there has been\ncomparatively less attention given to settings where the outcome is\nright-censored, especially with regard to the practical use of estimators. In\nthis study, simulations were undertaken to assess the finite-sample performance\nof estimators for optimal treatment rules and estimators for the expected\noutcome under treatment rules. The simulations were motivated by the common\nsetting in biomedical and public health research where the data is\nobservational, survival times may be right-censored, and there is interest in\nestimating baseline treatment decisions to maximize survival probability. A\nvariety of outcome regression and direct search estimation methods were\ncompared for optimal treatment rule estimation across a range of simulation\nscenarios. Methods that flexibly model the outcome performed comparatively\nwell, including in settings where the treatment rule was non-linear. R code to\nreproduce this study's results are available on Github."}, "http://arxiv.org/abs/2401.03106": {"title": "Contrastive linear regression", "link": "http://arxiv.org/abs/2401.03106", "description": "Contrastive dimension reduction methods have been developed for case-control\nstudy data to identify variation that is enriched in the foreground (case) data\nX relative to the background (control) data Y. Here, we develop contrastive\nregression for the setting when there is a response variable r associated with\neach foreground observation. This situation occurs frequently when, for\nexample, the unaffected controls do not have a disease grade or intervention\ndosage but the affected cases have a disease grade or intervention dosage, as\nin autism severity, solid tumors stages, polyp sizes, or warfarin dosages. Our\ncontrastive regression model captures shared low-dimensional variation between\nthe predictors in the cases and control groups, and then explains the\ncase-specific response variables through the variance that remains in the\npredictors after shared variation is removed. We show that, in one\nsingle-nucleus RNA sequencing dataset on autism severity in postmortem brain\nsamples from donors with and without autism and in another single-cell RNA\nsequencing dataset on cellular differentiation in chronic rhinosinusitis with\nand without nasal polyps, our contrastive linear regression performs feature\nranking and identifies biologically-informative predictors associated with\nresponse that cannot be identified using other approaches"}, "http://arxiv.org/abs/2401.03123": {"title": "A least distance estimator for a multivariate regression model using deep neural networks", "link": "http://arxiv.org/abs/2401.03123", "description": "We propose a deep neural network (DNN) based least distance (LD) estimator\n(DNN-LD) for a multivariate regression problem, addressing the limitations of\nthe conventional methods. Due to the flexibility of a DNN structure, both\nlinear and nonlinear conditional mean functions can be easily modeled, and a\nmultivariate regression model can be realized by simply adding extra nodes at\nthe output layer. The proposed method is more efficient in capturing the\ndependency structure among responses than the least squares loss, and robust to\noutliers. In addition, we consider $L_1$-type penalization for variable\nselection, crucial in analyzing high-dimensional data. Namely, we propose what\nwe call (A)GDNN-LD estimator that enjoys variable selection and model\nestimation simultaneously, by applying the (adaptive) group Lasso penalty to\nweight parameters in the DNN structure. For the computation, we propose a\nquadratic smoothing approximation method to facilitate optimizing the\nnon-smooth objective function based on the least distance loss. The simulation\nstudies and a real data analysis demonstrate the promising performance of the\nproposed method."}, "http://arxiv.org/abs/2401.03206": {"title": "A Robbins--Monro Sequence That Can Exploit Prior Information For Faster Convergence", "link": "http://arxiv.org/abs/2401.03206", "description": "We propose a new method to improve the convergence speed of the Robbins-Monro\nalgorithm by introducing prior information about the target point into the\nRobbins-Monro iteration. We achieve the incorporation of prior information\nwithout the need of a -- potentially wrong -- regression model, which would\nalso entail additional constraints. We show that this prior-information\nRobbins-Monro sequence is convergent for a wide range of prior distributions,\neven wrong ones, such as Gaussian, weighted sum of Gaussians, e.g., in a kernel\ndensity estimate, as well as bounded arbitrary distribution functions greater\nthan zero. We furthermore analyse the sequence numerically to understand its\nperformance and the influence of parameters. The results demonstrate that the\nprior-information Robbins-Monro sequence converges faster than the standard\none, especially during the first steps, which are particularly important for\napplications where the number of function measurements is limited, and when the\nnoise of observing the underlying function is large. We finally propose a rule\nto select the parameters of the sequence."}, "http://arxiv.org/abs/2401.03268": {"title": "Adaptive Randomization Methods for Sequential Multiple Assignment Randomized Trials (SMARTs) via Thompson Sampling", "link": "http://arxiv.org/abs/2401.03268", "description": "Response-adaptive randomization (RAR) has been studied extensively in\nconventional, single-stage clinical trials, where it has been shown to yield\nethical and statistical benefits, especially in trials with many treatment\narms. However, RAR and its potential benefits are understudied in sequential\nmultiple assignment randomized trials (SMARTs), which are the gold-standard\ntrial design for evaluation of multi-stage treatment regimes. We propose a\nsuite of RAR algorithms for SMARTs based on Thompson Sampling (TS), a widely\nused RAR method in single-stage trials in which treatment randomization\nprobabilities are aligned with the estimated probability that the treatment is\noptimal. We focus on two common objectives in SMARTs: (i) comparison of the\nregimes embedded in the trial, and (ii) estimation of an optimal embedded\nregime. We develop valid post-study inferential procedures for treatment\nregimes under the proposed algorithms. This is nontrivial, as (even in\nsingle-stage settings) RAR can lead to nonnormal limiting distributions of\nestimators. Our algorithms are the first for RAR in multi-stage trials that\naccount for nonregularity in the estimand. Empirical studies based on\nreal-world SMARTs show that TS can improve in-trial subject outcomes without\nsacrificing efficiency for post-trial comparisons."}, "http://arxiv.org/abs/2401.03287": {"title": "Advancing Stepped Wedge Cluster Randomized Trials Analysis: Bayesian Hierarchical Penalized Spline Models for Immediate and Time-Varying Intervention Effects", "link": "http://arxiv.org/abs/2401.03287", "description": "Stepped wedge cluster randomized trials (SWCRTs) often face challenges with\npotential confounding by time trends. Traditional frequentist methods can fail\nto provide adequate coverage of the intervention's true effect using confidence\nintervals, whereas Bayesian approaches show potential for better coverage of\nintervention effects. However, Bayesian methods have seen limited development\nin SWCRTs. We propose two novel Bayesian hierarchical penalized spline models\nfor SWCRTs. The first model is for SWCRTs involving many clusters and time\nperiods, focusing on immediate intervention effects. To evaluate its efficacy,\nwe compared this model to traditional frequentist methods. We further developed\nthe model to estimate time-varying intervention effects. We conducted a\ncomparative analysis of this Bayesian spline model against an existing Bayesian\nmonotone effect curve model. The proposed models are applied in the Primary\nPalliative Care for Emergency Medicine stepped wedge trial to evaluate the\neffectiveness of primary palliative care intervention. Extensive simulations\nand a real-world application demonstrate the strengths of the proposed Bayesian\nmodels. The Bayesian immediate effect model consistently achieves near the\nfrequentist nominal coverage probability for true intervention effect,\nproviding more reliable interval estimations than traditional frequentist\nmodels, while maintaining high estimation accuracy. The proposed Bayesian\ntime-varying effect model exhibits advancements over the existing Bayesian\nmonotone effect curve model in terms of improved accuracy and reliability. To\nthe best of our knowledge, this is the first development of Bayesian\nhierarchical spline modeling for SWCRTs. The proposed models offer an accurate\nand robust analysis of intervention effects. Their application could lead to\neffective adjustments in intervention strategies."}, "http://arxiv.org/abs/2401.03326": {"title": "Optimal Adaptive SMART Designs with Binary Outcomes", "link": "http://arxiv.org/abs/2401.03326", "description": "In a sequential multiple-assignment randomized trial (SMART), a sequence of\ntreatments is given to a patient over multiple stages. In each stage,\nrandomization may be done to allocate patients to different treatment groups.\nEven though SMART designs are getting popular among clinical researchers, the\nmethodologies for adaptive randomization at different stages of a SMART are few\nand not sophisticated enough to handle the complexity of optimal allocation of\ntreatments at every stage of a trial. Lack of optimal allocation methodologies\ncan raise serious concerns about SMART designs from an ethical point of view.\nIn this work, we develop an optimal adaptive allocation procedure to minimize\nthe expected number of treatment failures for a SMART with a binary primary\noutcome. Issues related to optimal adaptive allocations are explored\ntheoretically with supporting simulations. The applicability of the proposed\nmethodology is demonstrated using a recently conducted SMART study named\nM-Bridge for developing universal and resource-efficient dynamic treatment\nregimes (DTRs) for incoming first-year college students as a bridge to\ndesirable treatments to address alcohol-related risks."}, "http://arxiv.org/abs/2401.03338": {"title": "Modelling pathwise uncertainty of Stochastic Differential Equations samplers via Probabilistic Numerics", "link": "http://arxiv.org/abs/2401.03338", "description": "Probabilistic ordinary differential equation (ODE) solvers have been\nintroduced over the past decade as uncertainty-aware numerical integrators.\nThey typically proceed by assuming a functional prior to the ODE solution,\nwhich is then queried on a grid to form a posterior distribution over the ODE\nsolution. As the queries span the integration interval, the approximate\nposterior solution then converges to the true deterministic one. Gaussian ODE\nfilters, in particular, have enjoyed a lot of attention due to their\ncomputational efficiency, the simplicity of their implementation, as well as\ntheir provable fast convergence rates. In this article, we extend the\nmethodology to stochastic differential equations (SDEs) and propose a\nprobabilistic simulator for SDEs. Our approach involves transforming the SDE\ninto a sequence of random ODEs using piecewise differentiable approximations of\nthe Brownian motion. We then apply probabilistic ODE solvers to the individual\nODEs, resulting in a pathwise probabilistic solution to the SDE\\@. We establish\nworst-case strong $1.5$ local and $1.0$ global convergence orders for a\nspecific instance of our method. We further show how we can marginalise the\nBrownian approximations, by incorporating its coefficients as part of the prior\nODE model, allowing for computing exact transition densities under our model.\nFinally, we numerically validate the theoretical findings, showcasing\nreasonable weak convergence properties in the marginalised version."}, "http://arxiv.org/abs/2401.03554": {"title": "False Discovery Rate and Localizing Power", "link": "http://arxiv.org/abs/2401.03554", "description": "False discovery rate (FDR) is commonly used for correction for multiple\ntesting in neuroimaging studies. However, when using two-tailed tests, making\ndirectional inferences about the results can lead to vastly inflated error\nrate, even approaching 100\\% in some cases. This happens because FDR only\nprovides weak control over the error rate, meaning that the proportion of error\nis guaranteed only globally over all tests, not within subsets, such as among\nthose in only one or another direction. Here we consider and evaluate different\nstrategies for FDR control with two-tailed tests, using both synthetic and real\nimaging data. Approaches that separate the tests by direction of the hypothesis\ntest, or by the direction of the resulting test statistic, more properly\ncontrol the directional error rate and preserve FDR benefits, albeit with a\ndoubled risk of errors under complete absence of signal. Strategies that\ncombine tests in both directions, or that use simple two-tailed p-values, can\nlead to invalid directional conclusions, even if these tests remain globally\nvalid. To enable valid thresholding for directional inference, we suggest that\nimaging software should allow the possibility that the user sets asymmetrical\nthresholds for the two sides of the statistical map. While FDR continues to be\na valid, powerful procedure for multiple testing correction, care is needed\nwhen making directional inferences for two-tailed tests, or more broadly, when\nmaking any localized inference."}, "http://arxiv.org/abs/2401.03633": {"title": "A Spatial-statistical model to analyse historical rutting data", "link": "http://arxiv.org/abs/2401.03633", "description": "Pavement rutting poses a significant challenge in flexible pavements,\nnecessitating costly asphalt resurfacing. To address this issue\ncomprehensively, we propose an advanced Bayesian hierarchical framework of\nlatent Gaussian models with spatial components. Our model provides a thorough\ndiagnostic analysis, pinpointing areas exhibiting unexpectedly high rutting\nrates. Incorporating spatial and random components, and important explanatory\nvariables like annual average daily traffic (traffic intensity), pavement type,\nrut depth and lane width, our proposed models account for and estimate the\ninfluence of these variables on rutting. This approach not only quantifies\nuncertainties and discerns locations at the highest risk of requiring\nmaintenance, but also uncover spatial dependencies in rutting\n(millimetre/year). We apply our models to a data set spanning eleven years\n(2010-2020). Our findings emphasize the systematic unexpected spatial rutting\neffect, where more of the rutting variability is explained by including spatial\ncomponents. Pavement type, in conjunction with traffic intensity, is also found\nto be the primary driver of rutting. Furthermore, the spatial dependencies\nuncovered reveal road sections experiencing more than 1 millimeter of rutting\nbeyond annual expectations. This leads to a halving of the expected pavement\nlifespan in these areas. Our study offers valuable insights, presenting maps\nindicating expected rutting, and identifying locations with accelerated rutting\nrates, resulting in a reduction in pavement life expectancy of at least 10\nyears."}, "http://arxiv.org/abs/2401.03649": {"title": "Bayes Factor of Zero Inflated Models under Jeffereys Prior", "link": "http://arxiv.org/abs/2401.03649", "description": "Microbiome omics data including 16S rRNA reveal intriguing dynamic\nassociations between the human microbiome and various disease states. Drastic\nchanges in microbiota can be associated with factors like diet, hormonal\ncycles, diseases, and medical interventions. Along with the identification of\nspecific bacteria taxa associated with diseases, recent advancements give\nevidence that metabolism, genetics, and environmental factors can model these\nmicrobial effects. However, the current analytic methods for integrating\nmicrobiome data are fully developed to address the main challenges of\nlongitudinal metagenomics data, such as high-dimensionality, intra-sample\ndependence, and zero-inflation of observed counts. Hence, we propose the Bayes\nfactor approach for model selection based on negative binomial, Poisson,\nzero-inflated negative binomial, and zero-inflated Poisson models with\nnon-informative Jeffreys prior. We find that both in simulation studies and\nreal data analysis, our Bayes factor remarkably outperform traditional Akaike\ninformation criterion and Vuong's test. A new R package BFZINBZIP has been\nintroduced to do simulation study and real data analysis to facilitate Bayesian\nmodel selection based on the Bayes factor."}, "http://arxiv.org/abs/2401.03693": {"title": "TAD-SIE: Sample Size Estimation for Clinical Randomized Controlled Trials using a Trend-Adaptive Design with a Synthetic-Intervention-Based Estimator", "link": "http://arxiv.org/abs/2401.03693", "description": "Phase-3 clinical trials provide the highest level of evidence on drug safety\nand effectiveness needed for market approval by implementing large randomized\ncontrolled trials (RCTs). However, 30-40% of these trials fail mainly because\nsuch studies have inadequate sample sizes, stemming from the inability to\nobtain accurate initial estimates of average treatment effect parameters. To\nremove this obstacle from the drug development cycle, we present a new\nalgorithm called Trend-Adaptive Design with a Synthetic-Intervention-Based\nEstimator (TAD-SIE) that appropriately powers a parallel-group trial, a\nstandard RCT design, by leveraging a state-of-the-art hypothesis testing\nstrategy and a novel trend-adaptive design (TAD). Specifically, TAD-SIE uses\nSECRETS (Subject-Efficient Clinical Randomized Controlled Trials using\nSynthetic Intervention) for hypothesis testing, which simulates a cross-over\ntrial in order to boost power; doing so, makes it easier for a trial to reach\ntarget power within trial constraints (e.g., sample size limits). To estimate\nsample sizes, TAD-SIE implements a new TAD tailored to SECRETS given that\nSECRETS violates assumptions under standard TADs. In addition, our TAD\novercomes the ineffectiveness of standard TADs by allowing sample sizes to be\nincreased across iterations without any condition while controlling\nsignificance level with futility stopping. On a real-world Phase-3 clinical RCT\n(i.e., a two-arm parallel-group superiority trial with an equal number of\nsubjects per arm), TAD-SIE reaches typical target operating points of 80% or\n90% power and 5% significance level in contrast to baseline algorithms that\nonly get at best 59% power and 4% significance level."}, "http://arxiv.org/abs/2401.03756": {"title": "Contextual Fixed-Budget Best Arm Identification: Adaptive Experimental Design with Policy Learning", "link": "http://arxiv.org/abs/2401.03756", "description": "Individualized treatment recommendation is a crucial task in evidence-based\ndecision-making. In this study, we formulate this task as a fixed-budget best\narm identification (BAI) problem with contextual information. In this setting,\nwe consider an adaptive experiment given multiple treatment arms. At each\nround, a decision-maker observes a context (covariate) that characterizes an\nexperimental unit and assigns the unit to one of the treatment arms. At the end\nof the experiment, the decision-maker recommends a treatment arm estimated to\nyield the highest expected outcome conditioned on a context (best treatment\narm). The effectiveness of this decision is measured in terms of the worst-case\nexpected simple regret (policy regret), which represents the largest difference\nbetween the conditional expected outcomes of the best and recommended treatment\narms given a context. Our initial step is to derive asymptotic lower bounds for\nthe worst-case expected simple regret, which also implies ideal treatment\nassignment rules. Following the lower bounds, we propose the Adaptive Sampling\n(AS)-Policy Learning recommendation (PL) strategy. Under this strategy, we\nrandomly assign a treatment arm with a ratio of a target assignment ratio at\neach round. At the end of the experiment, we train a policy, a function that\nrecommends a treatment arm given a context, by maximizing the counterfactual\nempirical policy value. Our results show that the AS-PL strategy is\nasymptotically minimax optimal, with its leading factor of expected simple\nregret converging with our established worst-case lower bound. This research\nhas broad implications in various domains, and in light of existing literature,\nour method can be perceived as an adaptive experimental design tailored for\npolicy learning, on-policy learning, or adaptive welfare maximization."}, "http://arxiv.org/abs/2401.03820": {"title": "Optimal Differentially Private PCA and Estimation for Spiked Covariance Matrices", "link": "http://arxiv.org/abs/2401.03820", "description": "Estimating a covariance matrix and its associated principal components is a\nfundamental problem in contemporary statistics. While optimal estimation\nprocedures have been developed with well-understood properties, the increasing\ndemand for privacy preservation introduces new complexities to this classical\nproblem. In this paper, we study optimal differentially private Principal\nComponent Analysis (PCA) and covariance estimation within the spiked covariance\nmodel.\n\nWe precisely characterize the sensitivity of eigenvalues and eigenvectors\nunder this model and establish the minimax rates of convergence for estimating\nboth the principal components and covariance matrix. These rates hold up to\nlogarithmic factors and encompass general Schatten norms, including spectral\nnorm, Frobenius norm, and nuclear norm as special cases.\n\nWe introduce computationally efficient differentially private estimators and\nprove their minimax optimality, up to logarithmic factors. Additionally,\nmatching minimax lower bounds are established. Notably, in comparison with\nexisting literature, our results accommodate a diverging rank, necessitate no\neigengap condition between distinct principal components, and remain valid even\nif the sample size is much smaller than the dimension."}, "http://arxiv.org/abs/2401.03834": {"title": "Simultaneous false discovery bounds for invariant causal prediction", "link": "http://arxiv.org/abs/2401.03834", "description": "Invariant causal prediction (ICP, Peters et al. (2016)) provides a novel way\nto identify causal predictors of a response by utilizing heterogeneous data\nfrom different environments. One advantage of ICP is that it guarantees to make\nno false causal discoveries with high probability. Such a guarantee, however,\ncan be too conservative in some applications, resulting in few or no\ndiscoveries. To address this, we propose simultaneous false discovery bounds\nfor ICP, which provides users with extra flexibility in exploring causal\npredictors and can extract more informative results. These additional\ninferences come for free, in the sense that they do not require additional\nassumptions, and the same information obtained by the original ICP is retained.\nWe demonstrate the practical usage of our method through simulations and a real\ndataset."}, "http://arxiv.org/abs/2401.03881": {"title": "Density regression via Dirichlet process mixtures of normal structured additive regression models", "link": "http://arxiv.org/abs/2401.03881", "description": "Within Bayesian nonparametrics, dependent Dirichlet process mixture models\nprovide a highly flexible approach for conducting inference about the\nconditional density function. However, several formulations of this class make\neither rather restrictive modelling assumptions or involve intricate algorithms\nfor posterior inference, thus preventing their widespread use. In response to\nthese challenges, we present a flexible, versatile, and computationally\ntractable model for density regression based on a single-weights dependent\nDirichlet process mixture of normal distributions model for univariate\ncontinuous responses. We assume an additive structure for the mean of each\nmixture component and incorporate the effects of continuous covariates through\nsmooth nonlinear functions. The key components of our modelling approach are\npenalised B-splines and their bivariate tensor product extension. Our proposed\nmethod also seamlessly accommodates parametric effects of categorical\ncovariates, linear effects of continuous covariates, interactions between\ncategorical and/or continuous covariates, varying coefficient terms, and random\neffects, which is why we refer our model as a Dirichlet process mixture of\nnormal structured additive regression models. A noteworthy feature of our\nmethod is its efficiency in posterior simulation through Gibbs sampling, as\nclosed-form full conditional distributions for all model parameters are\navailable. Results from a simulation study demonstrate that our approach\nsuccessfully recovers true conditional densities and other regression\nfunctionals in various challenging scenarios. Applications to a toxicology,\ndisease diagnosis, and agricultural study are provided and further underpin the\nbroad applicability of our modelling framework. An R package, \\texttt{DDPstar},\nimplementing the proposed method is publicly available at\n\\url{https://bitbucket.org/mxrodriguez/ddpstar}."}, "http://arxiv.org/abs/2401.03891": {"title": "Radius selection using kernel density estimation for the computation of nonlinear measures", "link": "http://arxiv.org/abs/2401.03891", "description": "When nonlinear measures are estimated from sampled temporal signals with\nfinite-length, a radius parameter must be carefully selected to avoid a poor\nestimation. These measures are generally derived from the correlation integral\nwhich quantifies the probability of finding neighbors, i.e. pair of points\nspaced by less than the radius parameter. While each nonlinear measure comes\nwith several specific empirical rules to select a radius value, we provide a\nsystematic selection method. We show that the optimal radius for nonlinear\nmeasures can be approximated by the optimal bandwidth of a Kernel Density\nEstimator (KDE) related to the correlation sum. The KDE framework provides\nnon-parametric tools to approximate a density function from finite samples\n(e.g. histograms) and optimal methods to select a smoothing parameter, the\nbandwidth (e.g. bin width in histograms). We use results from KDE to derive a\nclosed-form expression for the optimal radius. The latter is used to compute\nthe correlation dimension and to construct recurrence plots yielding an\nestimate of Kolmogorov-Sinai entropy. We assess our method through numerical\nexperiments on signals generated by nonlinear systems and experimental\nelectroencephalographic time series."}, "http://arxiv.org/abs/2401.04036": {"title": "A regularized MANOVA test for semicontinuous high-dimensional data", "link": "http://arxiv.org/abs/2401.04036", "description": "We propose a MANOVA test for semicontinuous data that is applicable also when\nthe dimensionality exceeds the sample size. The test statistic is obtained as a\nlikelihood ratio, where numerator and denominator are computed at the maxima of\npenalized likelihood functions under each hypothesis. Closed form solutions for\nthe regularized estimators allow us to avoid computational overheads. We derive\nthe null distribution using a permutation scheme. The power and level of the\nresulting test are evaluated in a simulation study. We illustrate the new\nmethodology with two original data analyses, one regarding microRNA expression\nin human blastocyst cultures, and another regarding alien plant species\ninvasion in the island of Socotra (Yemen)."}, "http://arxiv.org/abs/2401.04086": {"title": "A Priori Determination of the Pretest Probability", "link": "http://arxiv.org/abs/2401.04086", "description": "In this manuscript, we present various proposed methods estimate the\nprevalence of disease, a critical prerequisite for the adequate interpretation\nof screening tests. To address the limitations of these approaches, which\nrevolve primarily around their a posteriori nature, we introduce a novel method\nto estimate the pretest probability of disease, a priori, utilizing the Logit\nfunction from the logistic regression model. This approach is a modification of\nMcGee's heuristic, originally designed for estimating the posttest probability\nof disease. In a patient presenting with $n_\\theta$ signs or symptoms, the\nminimal bound of the pretest probability, $\\phi$, can be approximated by:\n\n$\\phi \\approx\n\\frac{1}{5}{ln\\left[\\displaystyle\\prod_{\\theta=1}^{i}\\kappa_\\theta\\right]}$\n\nwhere $ln$ is the natural logarithm, and $\\kappa_\\theta$ is the likelihood\nratio associated with the sign or symptom in question."}, "http://arxiv.org/abs/2107.00118": {"title": "Do we need to estimate the variance in robust mean estimation?", "link": "http://arxiv.org/abs/2107.00118", "description": "In this paper, we propose self-tuned robust estimators for estimating the\nmean of heavy-tailed distributions, which refer to distributions with only\nfinite variances. Our approach introduces a new loss function that considers\nboth the mean parameter and a robustification parameter. By jointly optimizing\nthe empirical loss function with respect to both parameters, the\nrobustification parameter estimator can automatically adapt to the unknown data\nvariance, and thus the self-tuned mean estimator can achieve optimal\nfinite-sample performance. Our method outperforms previous approaches in terms\nof both computational and asymptotic efficiency. Specifically, it does not\nrequire cross-validation or Lepski's method to tune the robustification\nparameter, and the variance of our estimator achieves the Cram\\'er-Rao lower\nbound. Project source code is available at\n\\url{https://github.com/statsle/automean}."}, "http://arxiv.org/abs/2107.11869": {"title": "Adaptive Estimation and Uniform Confidence Bands for Nonparametric Structural Functions and Elasticities", "link": "http://arxiv.org/abs/2107.11869", "description": "We introduce two data-driven procedures for optimal estimation and inference\nin nonparametric models using instrumental variables. The first is a\ndata-driven choice of sieve dimension for a popular class of sieve two-stage\nleast squares estimators. When implemented with this choice, estimators of both\nthe structural function $h_0$ and its derivatives (such as elasticities)\nconverge at the fastest possible (i.e., minimax) rates in sup-norm. The second\nis for constructing uniform confidence bands (UCBs) for $h_0$ and its\nderivatives. Our UCBs guarantee coverage over a generic class of\ndata-generating processes and contract at the minimax rate, possibly up to a\nlogarithmic factor. As such, our UCBs are asymptotically more efficient than\nUCBs based on the usual approach of undersmoothing. As an application, we\nestimate the elasticity of the intensive margin of firm exports in a\nmonopolistic competition model of international trade. Simulations illustrate\nthe good performance of our procedures in empirically calibrated designs. Our\nresults provide evidence against common parameterizations of the distribution\nof unobserved firm heterogeneity."}, "http://arxiv.org/abs/2201.06606": {"title": "Drift vs Shift: Decoupling Trends and Changepoint Analysis", "link": "http://arxiv.org/abs/2201.06606", "description": "We introduce a new approach for decoupling trends (drift) and changepoints\n(shifts) in time series. Our locally adaptive model-based approach for robustly\ndecoupling combines Bayesian trend filtering and machine learning based\nregularization. An over-parameterized Bayesian dynamic linear model (DLM) is\nfirst applied to characterize drift. Then a weighted penalized likelihood\nestimator is paired with the estimated DLM posterior distribution to identify\nshifts. We show how Bayesian DLMs specified with so-called shrinkage priors can\nprovide smooth estimates of underlying trends in the presence of complex noise\ncomponents. However, their inability to shrink exactly to zero inhibits direct\nchangepoint detection. In contrast, penalized likelihood methods are highly\neffective in locating changepoints. However, they require data with simple\npatterns in both signal and noise. The proposed decoupling approach combines\nthe strengths of both, i.e. the flexibility of Bayesian DLMs with the hard\nthresholding property of penalized likelihood estimators, to provide\nchangepoint analysis in complex, modern settings. The proposed framework is\noutlier robust and can identify a variety of changes, including in mean and\nslope. It is also easily extended for analysis of parameter shifts in\ntime-varying parameter models like dynamic regressions. We illustrate the\nflexibility and contrast the performance and robustness of our approach with\nseveral alternative methods across a wide range of simulations and application\nexamples."}, "http://arxiv.org/abs/2303.00471": {"title": "E-values for k-Sample Tests With Exponential Families", "link": "http://arxiv.org/abs/2303.00471", "description": "We develop and compare e-variables for testing whether $k$ samples of data\nare drawn from the same distribution, the alternative being that they come from\ndifferent elements of an exponential family. We consider the GRO (growth-rate\noptimal) e-variables for (1) a `small' null inside the same exponential family,\nand (2) a `large' nonparametric null, as well as (3) an e-variable arrived at\nby conditioning on the sum of the sufficient statistics. (2) and (3) are\nefficiently computable, and extend ideas from Turner et al. [2021] and Wald\n[1947] respectively from Bernoulli to general exponential families. We provide\ntheoretical and simulation-based comparisons of these e-variables in terms of\ntheir logarithmic growth rate, and find that for small effects all four\ne-variables behave surprisingly similarly; for the Gaussian location and\nPoisson families, e-variables (1) and (3) coincide; for Bernoulli, (1) and (2)\ncoincide; but in general, whether (2) or (3) grows faster under the alternative\nis family-dependent. We furthermore discuss algorithms for numerically\napproximating (1)."}, "http://arxiv.org/abs/2303.13850": {"title": "Towards Learning and Explaining Indirect Causal Effects in Neural Networks", "link": "http://arxiv.org/abs/2303.13850", "description": "Recently, there has been a growing interest in learning and explaining causal\neffects within Neural Network (NN) models. By virtue of NN architectures,\nprevious approaches consider only direct and total causal effects assuming\nindependence among input variables. We view an NN as a structural causal model\n(SCM) and extend our focus to include indirect causal effects by introducing\nfeedforward connections among input neurons. We propose an ante-hoc method that\ncaptures and maintains direct, indirect, and total causal effects during NN\nmodel training. We also propose an algorithm for quantifying learned causal\neffects in an NN model and efficient approximation strategies for quantifying\ncausal effects in high-dimensional data. Extensive experiments conducted on\nsynthetic and real-world datasets demonstrate that the causal effects learned\nby our ante-hoc method better approximate the ground truth effects compared to\nexisting methods."}, "http://arxiv.org/abs/2304.08242": {"title": "The Deep Latent Position Topic Model for Clustering and Representation of Networks with Textual Edges", "link": "http://arxiv.org/abs/2304.08242", "description": "Numerical interactions leading to users sharing textual content published by\nothers are naturally represented by a network where the individuals are\nassociated with the nodes and the exchanged texts with the edges. To understand\nthose heterogeneous and complex data structures, clustering nodes into\nhomogeneous groups as well as rendering a comprehensible visualisation of the\ndata is mandatory. To address both issues, we introduce Deep-LPTM, a\nmodel-based clustering strategy relying on a variational graph auto-encoder\napproach as well as a probabilistic model to characterise the topics of\ndiscussion. Deep-LPTM allows to build a joint representation of the nodes and\nof the edges in two embeddings spaces. The parameters are inferred using a\nvariational inference algorithm. We also introduce IC2L, a model selection\ncriterion specifically designed to choose models with relevant clustering and\nvisualisation properties. An extensive benchmark study on synthetic data is\nprovided. In particular, we find that Deep-LPTM better recovers the partitions\nof the nodes than the state-of-the art ETSBM and STBM. Eventually, the emails\nof the Enron company are analysed and visualisations of the results are\npresented, with meaningful highlights of the graph structure."}, "http://arxiv.org/abs/2305.05126": {"title": "Comparing Foundation Models using Data Kernels", "link": "http://arxiv.org/abs/2305.05126", "description": "Recent advances in self-supervised learning and neural network scaling have\nenabled the creation of large models, known as foundation models, which can be\neasily adapted to a wide range of downstream tasks. The current paradigm for\ncomparing foundation models involves evaluating them with aggregate metrics on\nvarious benchmark datasets. This method of model comparison is heavily\ndependent on the chosen evaluation metric, which makes it unsuitable for\nsituations where the ideal metric is either not obvious or unavailable. In this\nwork, we present a methodology for directly comparing the embedding space\ngeometry of foundation models, which facilitates model comparison without the\nneed for an explicit evaluation metric. Our methodology is grounded in random\ngraph theory and enables valid hypothesis testing of embedding similarity on a\nper-datum basis. Further, we demonstrate how our methodology can be extended to\nfacilitate population level model comparison. In particular, we show how our\nframework can induce a manifold of models equipped with a distance function\nthat correlates strongly with several downstream metrics. We remark on the\nutility of this population level model comparison as a first step towards a\ntaxonomic science of foundation models."}, "http://arxiv.org/abs/2305.18044": {"title": "Bayesian estimation of clustered dependence structures in functional neuroconnectivity", "link": "http://arxiv.org/abs/2305.18044", "description": "Motivated by the need to model the dependence between regions of interest in\nfunctional neuroconnectivity for efficient inference, we propose a new\nsampling-based Bayesian clustering approach for covariance structures of\nhigh-dimensional Gaussian outcomes. The key technique is based on a Dirichlet\nprocess that clusters covariance sub-matrices into independent groups of\noutcomes, thereby naturally inducing sparsity in the whole brain connectivity\nmatrix. A new split-merge algorithm is employed to achieve convergence of the\nMarkov chain that is shown empirically to recover both uniform and Dirichlet\npartitions with high accuracy. We investigate the empirical performance of the\nproposed method through extensive simulations. Finally, the proposed approach\nis used to group regions of interest into functionally independent groups in\nthe Autism Brain Imaging Data Exchange participants with autism spectrum\ndisorder and and co-occurring attention-deficit/hyperactivity disorder."}, "http://arxiv.org/abs/2306.12528": {"title": "Structured Learning in Time-dependent Cox Models", "link": "http://arxiv.org/abs/2306.12528", "description": "Cox models with time-dependent coefficients and covariates are widely used in\nsurvival analysis. In high-dimensional settings, sparse regularization\ntechniques are employed for variable selection, but existing methods for\ntime-dependent Cox models lack flexibility in enforcing specific sparsity\npatterns (i.e., covariate structures). We propose a flexible framework for\nvariable selection in time-dependent Cox models, accommodating complex\nselection rules. Our method can adapt to arbitrary grouping structures,\nincluding interaction selection, temporal, spatial, tree, and directed acyclic\ngraph structures. It achieves accurate estimation with low false alarm rates.\nWe develop the sox package, implementing a network flow algorithm for\nefficiently solving models with complex covariate structures. sox offers a\nuser-friendly interface for specifying grouping structures and delivers fast\ncomputation. Through examples, including a case study on identifying predictors\nof time to all-cause death in atrial fibrillation patients, we demonstrate the\npractical application of our method with specific selection rules."}, "http://arxiv.org/abs/2401.04156": {"title": "LASPATED: a Library for the Analysis of SPAtio-TEmporal Discrete data", "link": "http://arxiv.org/abs/2401.04156", "description": "We describe methods, tools, and a software library called LASPATED, available\non GitHub (at https://github.com/vguigues/) to fit models using spatio-temporal\ndata and space-time discretization. A video tutorial for this library is\navailable on YouTube. We consider two types of methods to estimate a\nnon-homogeneous Poisson process in space and time. The methods approximate the\narrival intensity function of the Poisson process by discretizing space and\ntime, and estimating arrival intensity as a function of subregion and time\ninterval. With such methods, it is typical that the dimension of the estimator\nis large relative to the amount of data, and therefore the performance of the\nestimator can be improved by using additional data. The first method uses\nadditional data to add a regularization term to the likelihood function for\ncalibrating the intensity of the Poisson process. The second method uses\nadditional data to estimate arrival intensity as a function of covariates. We\ndescribe a Python package to perform various types of space and time\ndiscretization. We also describe two packages for the calibration of the\nmodels, one in Matlab and one in C++. We demonstrate the advantages of our\nmethods compared to basic maximum likelihood estimation with simulated and real\ndata. The experiments with real data calibrate models of the arrival process of\nemergencies to be handled by the Rio de Janeiro emergency medical service."}, "http://arxiv.org/abs/2401.04190": {"title": "Is it possible to know cosmological fine-tuning?", "link": "http://arxiv.org/abs/2401.04190", "description": "Fine-tuning studies whether some physical parameters, or relevant ratios\nbetween them, are located within so-called life-permitting intervals of small\nprobability outside of which carbon-based life would not be possible. Recent\ndevelopments have found estimates of these probabilities that circumvent\nprevious concerns of measurability and selection bias. However, the question\nremains if fine-tuning can indeed be known. Using a mathematization of the\nepistemological concepts of learning and knowledge acquisition, we argue that\nmost examples that have been touted as fine-tuned cannot be formally assessed\nas such. Nevertheless, fine-tuning can be known when the physical parameter is\nseen as a random variable and it is supported in the nonnegative real line,\nprovided the size of the life-permitting interval is small in relation to the\nobserved value of the parameter."}, "http://arxiv.org/abs/2401.04240": {"title": "A New Cure Rate Model with Discrete and Multiple Exposures", "link": "http://arxiv.org/abs/2401.04240", "description": "Cure rate models are mostly used to study data arising from cancer clinical\ntrials. Its use in the context of infectious diseases has not been explored\nwell. In 2008, Tournoud and Ecochard first proposed a mechanistic formulation\nof cure rate model in the context of infectious diseases with multiple\nexposures to infection. However, they assumed a simple Poisson distribution to\ncapture the unobserved pathogens at each exposure time. In this paper, we\npropose a new cure rate model to study infectious diseases with discrete\nmultiple exposures to infection. Our formulation captures both over-dispersion\nand under-dispersion with respect to the count on pathogens at each time of\nexposure. We also propose a new estimation method based on the expectation\nmaximization algorithm to calculate the maximum likelihood estimates of the\nmodel parameters. We carry out a detailed Monte Carlo simulation study to\ndemonstrate the performance of the proposed model and estimation algorithm. The\nflexibility of our proposed model also allows us to carry out a model\ndiscrimination. For this purpose, we use both likelihood ratio test and\ninformation-based criteria. Finally, we illustrate our proposed model using a\nrecently collected data on COVID-19."}, "http://arxiv.org/abs/2401.04263": {"title": "Two-Step Targeted Minimum-Loss Based Estimation for Non-Negative Two-Part Outcomes", "link": "http://arxiv.org/abs/2401.04263", "description": "Non-negative two-part outcomes are defined as outcomes with a density\nfunction that have a zero point mass but are otherwise positive. Examples, such\nas healthcare expenditure and hospital length of stay, are common in healthcare\nutilization research. Despite the practical relevance of non-negative two-part\noutcomes, very few methods exist to leverage knowledge of their semicontinuity\nto achieve improved performance in estimating causal effects. In this paper, we\ndevelop a nonparametric two-step targeted minimum-loss based estimator (denoted\nas hTMLE) for non-negative two-part outcomes. We present methods for a general\nclass of interventions referred to as modified treatment policies, which can\naccommodate continuous, categorical, and binary exposures. The two-step TMLE\nuses a targeted estimate of the intensity component of the outcome to produce a\ntargeted estimate of the binary component of the outcome that may improve\nfinite sample efficiency. We demonstrate the efficiency gains achieved by the\ntwo-step TMLE with simulated examples and then apply it to a cohort of Medicaid\nbeneficiaries to estimate the effect of chronic pain and physical disability on\ndays' supply of opioids."}, "http://arxiv.org/abs/2401.04297": {"title": "Staged trees for discrete longitudinal data", "link": "http://arxiv.org/abs/2401.04297", "description": "In this paper we investigate the use of staged tree models for discrete\nlongitudinal data. Staged trees are a type of probabilistic graphical model for\nfinite sample space processes. They are a natural fit for longitudinal data\nbecause a temporal ordering is often implicitly assumed and standard methods\ncan be used for model selection and probability estimation. However, model\nselection methods perform poorly when the sample size is small relative to the\nsize of the graph and model interpretation is tricky with larger graphs. This\nis exacerbated by longitudinal data which is characterised by repeated\nobservations. To address these issues we propose two approaches: the\nlongitudinal staged tree with Markov assumptions which makes some initial\nconditional independence assumptions represented by a directed acyclic graph\nand marginal longitudinal staged trees which model certain margins of the data."}, "http://arxiv.org/abs/2401.04352": {"title": "Non-Deterministic Extension of Plasma Wind Tunnel Data Calibrated Model Predictions to Flight Conditions", "link": "http://arxiv.org/abs/2401.04352", "description": "This work proposes a novel approach for non-deterministic extension of\nexperimental data that considers structural model inadequacy for conditions\nother than the calibration scenario while simultaneously resolving any\nsignificant prior-data discrepancy with information extracted from flight\nmeasurements. This functionality is achieved through methodical utilization of\nmodel error emulators and Bayesian model averaging studies with available\nresponse data. The outlined approach does not require prior flight data\navailability and introduces straightforward mechanisms for their assimilation\nin future predictions. Application of the methodology is demonstrated herein by\nextending material performance data captured at the HyMETS facility to the MSL\nscenario, where the described process yields results that exhibit significantly\nimproved capacity for predictive uncertainty quantification studies. This work\nalso investigates limitations associated with straightforward uncertainty\npropagation procedures onto calibrated model predictions for the flight\nscenario and manages computational requirements with sensitivity analysis and\nsurrogate modeling techniques."}, "http://arxiv.org/abs/2401.04450": {"title": "Recanting twins: addressing intermediate confounding in mediation analysis", "link": "http://arxiv.org/abs/2401.04450", "description": "The presence of intermediate confounders, also called recanting witnesses, is\na fundamental challenge to the investigation of causal mechanisms in mediation\nanalysis, preventing the identification of natural path-specific effects.\nProposed alternative parameters (such as randomizational interventional\neffects) are problematic because they can be non-null even when there is no\nmediation for any individual in the population; i.e., they are not an average\nof underlying individual-level mechanisms. In this paper we develop a novel\nmethod for mediation analysis in settings with intermediate confounding, with\nguarantees that the causal parameters are summaries of the individual-level\nmechanisms of interest. The method is based on recently proposed ideas that\nview causality as the transfer of information, and thus replace recanting\nwitnesses by draws from their conditional distribution, what we call \"recanting\ntwins\". We show that, in the absence of intermediate confounding, recanting\ntwin effects recover natural path-specific effects. We present the assumptions\nrequired for identification of recanting twins effects under a standard\nstructural causal model, as well as the assumptions under which the recanting\ntwin identification formulas can be interpreted in the context of the recently\nproposed separable effects models. To estimate recanting-twin effects, we\ndevelop efficient semi-parametric estimators that allow the use of data driven\nmethods in the estimation of the nuisance parameters. We present numerical\nstudies of the methods using synthetic data, as well as an application to\nevaluate the role of new-onset anxiety and depressive disorder in explaining\nthe relationship between gabapentin/pregabalin prescription and incident opioid\nuse disorder among Medicaid beneficiaries with chronic pain."}, "http://arxiv.org/abs/2401.04490": {"title": "Testing similarity of parametric competing risks models for identifying potentially similar pathways in healthcare", "link": "http://arxiv.org/abs/2401.04490", "description": "The identification of similar patient pathways is a crucial task in\nhealthcare analytics. A flexible tool to address this issue are parametric\ncompeting risks models, where transition intensities may be specified by a\nvariety of parametric distributions, thus in particular being possibly\ntime-dependent. We assess the similarity between two such models by examining\nthe transitions between different health states. This research introduces a\nmethod to measure the maximum differences in transition intensities over time,\nleading to the development of a test procedure for assessing similarity. We\npropose a parametric bootstrap approach for this purpose and provide a proof to\nconfirm the validity of this procedure. The performance of our proposed method\nis evaluated through a simulation study, considering a range of sample sizes,\ndiffering amounts of censoring, and various thresholds for similarity. Finally,\nwe demonstrate the practical application of our approach with a case study from\nurological clinical routine practice, which inspired this research."}, "http://arxiv.org/abs/2401.04498": {"title": "Efficient designs for multivariate crossover trials", "link": "http://arxiv.org/abs/2401.04498", "description": "This article aims to study efficient/trace optimal designs for crossover\ntrials, with multiple response recorded from each subject in each time period.\nA multivariate fixed effect model is proposed with direct and carryover effects\ncorresponding to the multiple responses and the error dispersion matrix\nallowing for correlations to exist between and within responses. Two\ncorrelation structures, namely the proportional and the generalized markov\ncovariances are studied. The corresponding information matrices for direct\neffects under the two covariances are used to determine efficient designs.\nEfficiency of orthogonal array designs of Type $I$ and strength $2$ is\ninvestigated for the two covariance forms. To motivate the multivariate\ncrossover designs, a gene expression data in a $3 \\times 3$ framework is\nutilized."}, "http://arxiv.org/abs/2401.04603": {"title": "Skewed Pivot-Blend Modeling with Applications to Semicontinuous Outcomes", "link": "http://arxiv.org/abs/2401.04603", "description": "Skewness is a common occurrence in statistical applications. In recent years,\nvarious distribution families have been proposed to model skewed data by\nintroducing unequal scales based on the median or mode. However, we argue that\nthe point at which unbalanced scales occur may be at any quantile and cannot be\nreparametrized as an ordinary shift parameter in the presence of skewness. In\nthis paper, we introduce a novel skewed pivot-blend technique to create a\nskewed density family based on any continuous density, even those that are\nasymmetric and nonunimodal. Our framework enables the simultaneous estimation\nof scales, the pivotal point, and other location parameters, along with various\nextensions. We also introduce a skewed two-part model tailored for\nsemicontinuous outcomes, which identifies relevant variables across the entire\npopulation and mitigates the additional skewness induced by commonly used\ntransformations. Our theoretical analysis reveals the influence of skewness\nwithout assuming asymptotic conditions. Experiments on synthetic and real-life\ndata demonstrate the excellent performance of the proposed method."}, "http://arxiv.org/abs/2401.04686": {"title": "Weighted likelihood methods for robust fitting of wrapped models for $p$-torus data", "link": "http://arxiv.org/abs/2401.04686", "description": "We consider robust estimation of wrapped models to multivariate circular data\nthat are points on the surface of a $p$-torus based on the weighted likelihood\nmethodology.Robust model fitting is achieved by a set of weighted likelihood\nestimating equations, based on the computation of data dependent weights aimed\nto down-weight anomalous values, such as unexpected directions that do not\nshare the main pattern of the bulk of the data. Weighted likelihood estimating\nequations with weights evaluated on the torus orobtained after unwrapping the\ndata onto the Euclidean space are proposed and compared. Asymptotic properties\nand robustness features of the estimators under study have been studied,\nwhereas their finite sample behavior has been investigated by Monte Carlo\nnumerical experiment and real data examples."}, "http://arxiv.org/abs/2401.04689": {"title": "Efficient estimation for ergodic diffusion processes sampled at high frequency", "link": "http://arxiv.org/abs/2401.04689", "description": "A general theory of efficient estimation for ergodic diffusion processes\nsampled at high frequency with an infinite time horizon is presented. High\nfrequency sampling is common in many applications, with finance as a prominent\nexample. The theory is formulated in term of approximate martingale estimating\nfunctions and covers a large class of estimators including most of the\npreviously proposed estimators for diffusion processes. Easily checked\nconditions ensuring that an estimating function is an approximate martingale\nare derived, and general conditions ensuring consistency and asymptotic\nnormality of estimators are given. Most importantly, simple conditions are\ngiven that ensure rate optimality and efficiency. Rate optimal estimators of\nparameters in the diffusion coefficient converge faster than estimators of\ndrift coefficient parameters because they take advantage of the information in\nthe quadratic variation. The conditions facilitate the choice among the\nmultitude of estimators that have been proposed for diffusion models. Optimal\nmartingale estimating functions in the sense of Godambe and Heyde and their\nhigh frequency approximations are, under weak conditions, shown to satisfy the\nconditions for rate optimality and efficiency. This provides a natural feasible\nmethod of constructing explicit rate optimal and efficient estimating functions\nby solving a linear equation."}, "http://arxiv.org/abs/2401.04693": {"title": "Co-Clustering Multi-View Data Using the Latent Block Model", "link": "http://arxiv.org/abs/2401.04693", "description": "The Latent Block Model (LBM) is a prominent model-based co-clustering method,\nreturning parametric representations of each block cluster and allowing the use\nof well-grounded model selection methods. The LBM, while adapted in literature\nto handle different feature types, cannot be applied to datasets consisting of\nmultiple disjoint sets of features, termed views, for a common set of\nobservations. In this work, we introduce the multi-view LBM, extending the LBM\nmethod to multi-view data, where each view marginally follows an LBM. In the\ncase of two views, the dependence between them is captured by a cluster\nmembership matrix, and we aim to learn the structure of this matrix. We develop\na likelihood-based approach in which parameter estimation uses a stochastic EM\nalgorithm integrating a Gibbs sampler, and an ICL criterion is derived to\ndetermine the number of row and column clusters in each view. To motivate the\napplication of multi-view methods, we extend recent work developing hypothesis\ntests for the null hypothesis that clusters of observations in each view are\nindependent of each other. The testing procedure is integrated into the model\nestimation strategy. Furthermore, we introduce a penalty scheme to generate\nsparse row clusterings. We verify the performance of the developed algorithm\nusing synthetic datasets, and provide guidance for optimal parameter selection.\nFinally, the multi-view co-clustering method is applied to a complex genomics\ndataset, and is shown to provide new insights for high-dimension multi-view\nproblems."}, "http://arxiv.org/abs/2401.04723": {"title": "Spatio-temporal data fusion for the analysis of in situ and remote sensing data using the INLA-SPDE approach", "link": "http://arxiv.org/abs/2401.04723", "description": "We propose a Bayesian hierarchical model to address the challenge of spatial\nmisalignment in spatio-temporal data obtained from in situ and satellite\nsources. The model is fit using the INLA-SPDE approach, which provides\nefficient computation. Our methodology combines the different data sources in a\n\"fusion\"\" model via the construction of projection matrices in both spatial and\ntemporal domains. Through simulation studies, we demonstrate that the fusion\nmodel has superior performance in prediction accuracy across space and time\ncompared to standalone \"in situ\" and \"satellite\" models based on only in situ\nor satellite data, respectively. The fusion model also generally outperforms\nthe standalone models in terms of parameter inference. Such a modeling approach\nis motivated by environmental problems, and our specific focus is on the\nanalysis and prediction of harmful algae bloom (HAB) events, where the\nconvention is to conduct separate analyses based on either in situ samples or\nsatellite images. A real data analysis shows that the proposed model is a\nnecessary step towards a unified characterization of bloom dynamics and\nidentifying the key drivers of HAB events."}, "http://arxiv.org/abs/1807.00347": {"title": "Robust Inference Under Heteroskedasticity via the Hadamard Estimator", "link": "http://arxiv.org/abs/1807.00347", "description": "Drawing statistical inferences from large datasets in a model-robust way is\nan important problem in statistics and data science. In this paper, we propose\nmethods that are robust to large and unequal noise in different observational\nunits (i.e., heteroskedasticity) for statistical inference in linear\nregression. We leverage the Hadamard estimator, which is unbiased for the\nvariances of ordinary least-squares regression. This is in contrast to the\npopular White's sandwich estimator, which can be substantially biased in high\ndimensions. We propose to estimate the signal strength, noise level,\nsignal-to-noise ratio, and mean squared error via the Hadamard estimator. We\ndevelop a new degrees of freedom adjustment that gives more accurate confidence\nintervals than variants of White's sandwich estimator. Moreover, we provide\nconditions ensuring the estimator is well-defined, by studying a new random\nmatrix ensemble in which the entries of a random orthogonal projection matrix\nare squared. We also show approximate normality, using the second-order\nPoincare inequality. Our work provides improved statistical theory and methods\nfor linear regression in high dimensions."}, "http://arxiv.org/abs/2110.12235": {"title": "Adjusting for indirectly measured confounding using large-scale propensity scores", "link": "http://arxiv.org/abs/2110.12235", "description": "Confounding remains one of the major challenges to causal inference with\nobservational data. This problem is paramount in medicine, where we would like\nto answer causal questions from large observational datasets like electronic\nhealth records (EHRs) and administrative claims. Modern medical data typically\ncontain tens of thousands of covariates. Such a large set carries hope that\nmany of the confounders are directly measured, and further hope that others are\nindirectly measured through their correlation with measured covariates. How can\nwe exploit these large sets of covariates for causal inference? To help answer\nthis question, this paper examines the performance of the large-scale\npropensity score (LSPS) approach on causal analysis of medical data. We\ndemonstrate that LSPS may adjust for indirectly measured confounders by\nincluding tens of thousands of covariates that may be correlated with them. We\npresent conditions under which LSPS removes bias due to indirectly measured\nconfounders, and we show that LSPS may avoid bias when inadvertently adjusting\nfor variables (like colliders) that otherwise can induce bias. We demonstrate\nthe performance of LSPS with both simulated medical data and real medical data."}, "http://arxiv.org/abs/2208.07014": {"title": "Proximal Survival Analysis to Handle Dependent Right Censoring", "link": "http://arxiv.org/abs/2208.07014", "description": "Many epidemiological and clinical studies aim at analyzing a time-to-event\nendpoint. A common complication is right censoring. In some cases, it arises\nbecause subjects are still surviving after the study terminates or move out of\nthe study area, in which case right censoring is typically treated as\nindependent or non-informative. Such an assumption can be further relaxed to\nconditional independent censoring by leveraging possibly time-varying covariate\ninformation, if available, assuming censoring and failure time are independent\namong covariate strata. In yet other instances, events may be censored by other\ncompeting events like death and are associated with censoring possibly through\nprognoses. Realistically, measured covariates can rarely capture all such\nassociations with certainty. For such dependent censoring, often covariate\nmeasurements are at best proxies of underlying prognoses. In this paper, we\nestablish a nonparametric identification framework by formally admitting that\nconditional independent censoring may fail in practice and accounting for\ncovariate measurements as imperfect proxies of underlying association. The\nframework suggests adaptive estimators which we give generic assumptions under\nwhich they are consistent, asymptotically normal, and doubly robust. We\nillustrate our framework with concrete settings, where we examine the\nfinite-sample performance of our proposed estimators via a Monte-Carlo\nsimulation and apply them to the SEER-Medicare dataset."}, "http://arxiv.org/abs/2306.03384": {"title": "A Calibrated Data-Driven Approach for Small Area Estimation using Big Data", "link": "http://arxiv.org/abs/2306.03384", "description": "Where the response variable in a big data set is consistent with the variable\nof interest for small area estimation, the big data by itself can provide the\nestimates for small areas. These estimates are often subject to the coverage\nand measurement error bias inherited from the big data. However, if a\nprobability survey of the same variable of interest is available, the survey\ndata can be used as a training data set to develop an algorithm to impute for\nthe data missed by the big data and adjust for measurement errors. In this\npaper, we outline a methodology for such imputations based on an kNN algorithm\ncalibrated to an asymptotically design-unbiased estimate of the national total\nand illustrate the use of a training data set to estimate the imputation bias\nand the fixed - asymptotic bootstrap to estimate the variance of the small area\nhybrid estimator. We illustrate the methodology of this paper using a public\nuse data set and use it to compare the accuracy and precision of our hybrid\nestimator with the Fay-Harriot (FH) estimator. Finally, we also examine\nnumerically the accuracy and precision of the FH estimator when the auxiliary\nvariables used in the linking models are subject to under-coverage errors."}, "http://arxiv.org/abs/2306.06270": {"title": "Markov bases: a 25 year update", "link": "http://arxiv.org/abs/2306.06270", "description": "In this paper, we evaluate the challenges and best practices associated with\nthe Markov bases approach to sampling from conditional distributions. We\nprovide insights and clarifications after 25 years of the publication of the\nfundamental theorem for Markov bases by Diaconis and Sturmfels. In addition to\na literature review we prove three new results on the complexity of Markov\nbases in hierarchical models, relaxations of the fibers in log-linear models,\nand limitations of partial sets of moves in providing an irreducible Markov\nchain."}, "http://arxiv.org/abs/2309.02281": {"title": "s-ID: Causal Effect Identification in a Sub-Population", "link": "http://arxiv.org/abs/2309.02281", "description": "Causal inference in a sub-population involves identifying the causal effect\nof an intervention on a specific subgroup, which is distinguished from the\nwhole population through the influence of systematic biases in the sampling\nprocess. However, ignoring the subtleties introduced by sub-populations can\neither lead to erroneous inference or limit the applicability of existing\nmethods. We introduce and advocate for a causal inference problem in\nsub-populations (henceforth called s-ID), in which we merely have access to\nobservational data of the targeted sub-population (as opposed to the entire\npopulation). Existing inference problems in sub-populations operate on the\npremise that the given data distributions originate from the entire population,\nthus, cannot tackle the s-ID problem. To address this gap, we provide necessary\nand sufficient conditions that must hold in the causal graph for a causal\neffect in a sub-population to be identifiable from the observational\ndistribution of that sub-population. Given these conditions, we present a sound\nand complete algorithm for the s-ID problem."}, "http://arxiv.org/abs/2401.04753": {"title": "Dynamic Models Augmented by Hierarchical Data: An Application Of Estimating HIV Epidemics At Sub-National And Sub-Population Level", "link": "http://arxiv.org/abs/2401.04753", "description": "Dynamic models have been successfully used in producing estimates of HIV\nepidemics at the national level due to their epidemiological nature and their\nability to estimate prevalence, incidence, and mortality rates simultaneously.\nRecently, HIV interventions and policies have required more information at\nsub-national levels to support local planning, decision making and resource\nallocation. Unfortunately, many areas lack sufficient data for deriving stable\nand reliable results, and this is a critical technical barrier to more\nstratified estimates. One solution is to borrow information from other areas\nwithin the same country. However, directly assuming hierarchical structures\nwithin the HIV dynamic models is complicated and computationally\ntime-consuming. In this paper, we propose a simple and innovative way to\nincorporate hierarchical information into the dynamical systems by using\nauxiliary data. The proposed method efficiently uses information from multiple\nareas within each country without increasing the computational burden. As a\nresult, the new model improves predictive ability and uncertainty assessment."}, "http://arxiv.org/abs/2401.04771": {"title": "Network Layout Algorithm with Covariate Smoothing", "link": "http://arxiv.org/abs/2401.04771", "description": "Network science explores intricate connections among objects, employed in\ndiverse domains like social interactions, fraud detection, and disease spread.\nVisualization of networks facilitates conceptualizing research questions and\nforming scientific hypotheses. Networks, as mathematical high-dimensional\nobjects, require dimensionality reduction for (planar) visualization.\nVisualizing empirical networks present additional challenges. They often\ncontain false positive (spurious) and false negative (missing) edges.\nTraditional visualization methods don't account for errors in observation,\npotentially biasing interpretations. Moreover, contemporary network data\nincludes rich nodal attributes. However, traditional methods neglect these\nattributes when computing node locations. Our visualization approach aims to\nleverage nodal attribute richness to compensate for network data limitations.\nWe employ a statistical model estimating the probability of edge connections\nbetween nodes based on their covariates. We enhance the Fruchterman-Reingold\nalgorithm to incorporate estimated dyad connection probabilities, allowing\npractitioners to balance reliance on observed versus estimated edges. We\nexplore optimal smoothing levels, offering a natural way to include relevant\nnodal information in layouts. Results demonstrate the effectiveness of our\nmethod in achieving robust network visualization, providing insights for\nimproved analysis."}, "http://arxiv.org/abs/2401.04775": {"title": "Approximate Inference for Longitudinal Mechanistic HIV Contact Networks", "link": "http://arxiv.org/abs/2401.04775", "description": "Network models are increasingly used to study infectious disease spread.\nExponential Random Graph models have a history in this area, with scalable\ninference methods now available. An alternative approach uses mechanistic\nnetwork models. Mechanistic network models directly capture individual\nbehaviors, making them suitable for studying sexually transmitted diseases.\nCombining mechanistic models with Approximate Bayesian Computation allows\nflexible modeling using domain-specific interaction rules among agents,\navoiding network model oversimplifications. These models are ideal for\nlongitudinal settings as they explicitly incorporate network evolution over\ntime. We implemented a discrete-time version of a previously published\ncontinuous-time model of evolving contact networks for men who have sex with\nmen (MSM) and proposed an ABC-based approximate inference scheme for it. As\nexpected, we found that a two-wave longitudinal study design improves the\naccuracy of inference compared to a cross-sectional design. However, the gains\nin precision in collecting data twice, up to 18%, depend on the spacing of the\ntwo waves and are sensitive to the choice of summary statistics. In addition to\nmethodological developments, our results inform the design of future\nlongitudinal network studies in sexually transmitted diseases, specifically in\nterms of what data to collect from participants and when to do so."}, "http://arxiv.org/abs/2401.04778": {"title": "Generative neural networks for characteristic functions", "link": "http://arxiv.org/abs/2401.04778", "description": "In this work, we provide a simulation algorithm to simulate from a\n(multivariate) characteristic function, which is only accessible in a black-box\nformat. We construct a generative neural network, whose loss function exploits\na specific representation of the Maximum-Mean-Discrepancy metric to directly\nincorporate the targeted characteristic function. The construction is universal\nin the sense that it is independent of the dimension and that it does not\nrequire any assumptions on the given characteristic function. Furthermore,\nfinite sample guarantees on the approximation quality in terms of the\nMaximum-Mean Discrepancy metric are derived. The method is illustrated in a\nshort simulation study."}, "http://arxiv.org/abs/2401.04797": {"title": "Principal Component Analysis for Equation Discovery", "link": "http://arxiv.org/abs/2401.04797", "description": "Principal Component Analysis (PCA) is one of the most commonly used\nstatistical methods for data exploration, and for dimensionality reduction\nwherein the first few principal components account for an appreciable\nproportion of the variability in the data. Less commonly, attention is paid to\nthe last principal components because they do not account for an appreciable\nproportion of variability. However, this defining characteristic of the last\nprincipal components also qualifies them as combinations of variables that are\nconstant across the cases. Such constant-combinations are important because\nthey may reflect underlying laws of nature. In situations involving a large\nnumber of noisy covariates, the underlying law may not correspond to the last\nprincipal component, but rather to one of the last. Consequently, a criterion\nis required to identify the relevant eigenvector. In this paper, two examples\nare employed to demonstrate the proposed methodology; one from Physics,\ninvolving a small number of covariates, and another from Meteorology wherein\nthe number of covariates is in the thousands. It is shown that with an\nappropriate selection criterion, PCA can be employed to ``discover\" Kepler's\nthird law (in the former), and the hypsometric equation (in the latter)."}, "http://arxiv.org/abs/2401.04832": {"title": "Group lasso priors for Bayesian accelerated failure time models with left-truncated and interval-censored data", "link": "http://arxiv.org/abs/2401.04832", "description": "An important task in health research is to characterize time-to-event\noutcomes such as disease onset or mortality in terms of a potentially\nhigh-dimensional set of risk factors. For example, prospective cohort studies\nof Alzheimer's disease typically enroll older adults for observation over\nseveral decades to assess the long-term impact of genetic and other factors on\ncognitive decline and mortality. The accelerated failure time model is\nparticularly well-suited to such studies, structuring covariate effects as\n`horizontal' changes to the survival quantiles that conceptually reflect shifts\nin the outcome distribution due to lifelong exposures. However, this modeling\ntask is complicated by the enrollment of adults at differing ages, and\nintermittent followup visits leading to interval censored outcome information.\nMoreover, genetic and clinical risk factors are not only high-dimensional, but\ncharacterized by underlying grouping structure, such as by function or gene\nlocation. Such grouped high-dimensional covariates require shrinkage methods\nthat directly acknowledge this structure to facilitate variable selection and\nestimation. In this paper, we address these considerations directly by\nproposing a Bayesian accelerated failure time model with a group-structured\nlasso penalty, designed for left-truncated and interval-censored time-to-event\ndata. We develop a custom Markov chain Monte Carlo sampler for efficient\nestimation, and investigate the impact of various methods of penalty tuning and\nthresholding for variable selection. We present a simulation study examining\nthe performance of this method relative to models with an ordinary lasso\npenalty, and apply the proposed method to identify groups of predictive genetic\nand clinical risk factors for Alzheimer's disease in the Religious Orders Study\nand Memory and Aging Project (ROSMAP) prospective cohort studies of AD and\ndementia."}, "http://arxiv.org/abs/2401.04841": {"title": "Analysis of Compositional Data with Positive Correlations among Components using a Nested Dirichlet Distribution with Application to a Morris Water Maze Experiment", "link": "http://arxiv.org/abs/2401.04841", "description": "In a typical Morris water maze experiment, a mouse is placed in a circular\nwater tank and allowed to swim freely until it finds a platform, triggering a\nroute of escape from the tank. For reference purposes, the tank is divided into\nfour quadrants: the target quadrant where the trigger to escape resides, the\nopposite quadrant to the target, and two adjacent quadrants. Several response\nvariables can be measured: the amount of time that a mouse spends in different\nquadrants of the water tank, the number of times the mouse crosses from one\nquadrant to another, or how quickly a mouse triggers an escape from the tank.\nWhen considering time within each quadrant, it is hypothesized that normal mice\nwill spend smaller amounts of time in quadrants that do not contain the escape\nroute, while mice with an acquired or induced mental deficiency will spend\nequal time in all quadrants of the tank. Clearly, proportion of time in the\nquadrants must sum to one and are therefore statistically dependent; however,\nmost analyses of data from this experiment treat time in quadrants as\nstatistically independent. A recent paper introduced a hypothesis testing\nmethod that involves fitting such data to a Dirichlet distribution. While an\nimprovement over studies that ignore the compositional structure of the data,\nwe show that methodology is flawed. We introduce a two-sample test to detect\ndifferences in proportion of components for two independent groups where both\ngroups are from either a Dirichlet or nested Dirichlet distribution. This new\ntest is used to reanalyze the data from a previous study and come to a\ndifferent conclusion."}, "http://arxiv.org/abs/2401.04863": {"title": "Estimands and cumulative incidence function regression in clinical trials: some new results on interpretability and robustness", "link": "http://arxiv.org/abs/2401.04863", "description": "Regression analyses based on transformations of cumulative incidence\nfunctions are often adopted when modeling and testing for treatment effects in\nclinical trial settings involving competing and semi-competing risks. Common\nframeworks include the Fine-Gray model and models based on direct binomial\nregression. Using large sample theory we derive the limiting values of\ntreatment effect estimators based on such models when the data are generated\naccording to multiplicative intensity-based models, and show that the estimand\nis sensitive to several process features. The rejection rates of hypothesis\ntests based on cumulative incidence function regression models are also\nexamined for null hypotheses of different types, based on which a robustness\nproperty is established. In such settings supportive secondary analyses of\ntreatment effects are essential to ensure a full understanding of the nature of\ntreatment effects. An application to a palliative study of individuals with\nbreast cancer metastatic to bone is provided for illustration."}, "http://arxiv.org/abs/2401.05088": {"title": "Hybrid of node and link communities for graphon estimation", "link": "http://arxiv.org/abs/2401.05088", "description": "Networks serve as a tool used to examine the large-scale connectivity\npatterns in complex systems. Modelling their generative mechanism\nnonparametrically is often based on step-functions, such as the stochastic\nblock models. These models are capable of addressing two prominent topics in\nnetwork science: link prediction and community detection. However, such methods\noften have a resolution limit, making it difficult to separate small-scale\nstructures from noise. To arrive at a smoother representation of the network's\ngenerative mechanism, we explicitly trade variance for bias by smoothing blocks\nof edges based on stochastic equivalence. As such, we propose a different\nestimation method using a new model, which we call the stochastic shape model.\nTypically, analysis methods are based on modelling node or link communities. In\ncontrast, we take a hybrid approach, bridging the two notions of community.\nConsequently, we obtain a more parsimonious representation, enabling a more\ninterpretable and multiscale summary of the network structure. By considering\nmultiple resolutions, we trade bias and variance to ensure that our estimator\nis rate-optimal. We also examine the performance of our model through\nsimulations and applications to real network data."}, "http://arxiv.org/abs/2401.05124": {"title": "Nonparametric worst-case bounds for publication bias on the summary receiver operating characteristic curve", "link": "http://arxiv.org/abs/2401.05124", "description": "The summary receiver operating characteristic (SROC) curve has been\nrecommended as one important meta-analytical summary to represent the accuracy\nof a diagnostic test in the presence of heterogeneous cutoff values. However,\nselective publication of diagnostic studies for meta-analysis can induce\npublication bias (PB) on the estimate of the SROC curve. Several sensitivity\nanalysis methods have been developed to quantify PB on the SROC curve, and all\nthese methods utilize parametric selection functions to model the selective\npublication mechanism. The main contribution of this article is to propose a\nnew sensitivity analysis approach that derives the worst-case bounds for the\nSROC curve by adopting nonparametric selection functions under minimal\nassumptions. The estimation procedures of the worst-case bounds use the Monte\nCarlo method to obtain the SROC curves along with the corresponding area under\nthe curves in the worst case where the maximum possible PB under a range of\nmarginal selection probabilities is considered. We apply the proposed method to\na real-world meta-analysis to show that the worst-case bounds of the SROC\ncurves can provide useful insights for discussing the robustness of\nmeta-analytical findings on diagnostic test accuracy."}, "http://arxiv.org/abs/2401.05256": {"title": "Tests of Missing Completely At Random based on sample covariance matrices", "link": "http://arxiv.org/abs/2401.05256", "description": "We study the problem of testing whether the missing values of a potentially\nhigh-dimensional dataset are Missing Completely at Random (MCAR). We relax the\nproblem of testing MCAR to the problem of testing the compatibility of a\nsequence of covariance matrices, motivated by the fact that this procedure is\nfeasible when the dimension grows with the sample size. Tests of compatibility\ncan be used to test the feasibility of positive semi-definite matrix completion\nproblems with noisy observations, and thus our results may be of independent\ninterest. Our first contributions are to define a natural measure of the\nincompatibility of a sequence of correlation matrices, which can be\ncharacterised as the optimal value of a Semi-definite Programming (SDP)\nproblem, and to establish a key duality result allowing its practical\ncomputation and interpretation. By studying the concentration properties of the\nnatural plug-in estimator of this measure, we introduce novel hypothesis tests\nthat we prove have power against all distributions with incompatible covariance\nmatrices. The choice of critical values for our tests rely on a new\nconcentration inequality for the Pearson sample correlation matrix, which may\nbe of interest more widely. By considering key examples of missingness\nstructures, we demonstrate that our procedures are minimax rate optimal in\ncertain cases. We further validate our methodology with numerical simulations\nthat provide evidence of validity and power, even when data are heavy tailed."}, "http://arxiv.org/abs/2401.05281": {"title": "Asymptotic expected sensitivity function and its applications to nonparametric correlation estimators", "link": "http://arxiv.org/abs/2401.05281", "description": "We introduce a new type of influence function, the asymptotic expected\nsensitivity function, which is often equivalent to but mathematically more\ntractable than the traditional one based on the Gateaux derivative. To\nillustrate, we study the robustness of some important rank correlations,\nincluding Spearman's and Kendall's correlations, and the recently developed\nChatterjee's correlation."}, "http://arxiv.org/abs/2401.05315": {"title": "Multi-resolution filters via linear projection for large spatio-temporal datasets", "link": "http://arxiv.org/abs/2401.05315", "description": "Advances in compact sensing devices mounted on satellites have facilitated\nthe collection of large spatio-temporal datasets with coordinates. Since such\ndatasets are often incomplete and noisy, it is useful to create the prediction\nsurface of a spatial field. To this end, we consider an online filtering\ninference by using the Kalman filter based on linear Gaussian state-space\nmodels. However, the Kalman filter is impractically time-consuming when the\nnumber of locations in spatio-temporal datasets is large. To address this\nproblem, we propose a multi-resolution filter via linear projection (MRF-lp), a\nfast computation method for online filtering inference. In the MRF-lp, by\ncarrying out a multi-resolution approximation via linear projection (MRA-lp),\nthe forecast covariance matrix can be approximated while capturing both the\nlarge- and small-scale spatial variations. As a result of this approximation,\nour proposed MRF-lp preserves a block-sparse structure of some matrices\nappearing in the MRF-lp through time, which leads to the scalability of this\nalgorithm. Additionally, we discuss extensions of the MRF-lp to a nonlinear and\nnon-Gaussian case. Simulation studies and real data analysis for total\nprecipitable water vapor demonstrate that our proposed approach performs well\ncompared with the related methods."}, "http://arxiv.org/abs/2401.05330": {"title": "Hierarchical Causal Models", "link": "http://arxiv.org/abs/2401.05330", "description": "Scientists often want to learn about cause and effect from hierarchical data,\ncollected from subunits nested inside units. Consider students in schools,\ncells in patients, or cities in states. In such settings, unit-level variables\n(e.g. each school's budget) may affect subunit-level variables (e.g. the test\nscores of each student in each school) and vice versa. To address causal\nquestions with hierarchical data, we propose hierarchical causal models, which\nextend structural causal models and causal graphical models by adding inner\nplates. We develop a general graphical identification technique for\nhierarchical causal models that extends do-calculus. We find many situations in\nwhich hierarchical data can enable causal identification even when it would be\nimpossible with non-hierarchical data, that is, if we had only unit-level\nsummaries of subunit-level variables (e.g. the school's average test score,\nrather than each student's score). We develop estimation techniques for\nhierarchical causal models, using methods including hierarchical Bayesian\nmodels. We illustrate our results in simulation and via a reanalysis of the\nclassic \"eight schools\" study."}, "http://arxiv.org/abs/2010.02848": {"title": "Unified Robust Estimation", "link": "http://arxiv.org/abs/2010.02848", "description": "Robust estimation is primarily concerned with providing reliable parameter\nestimates in the presence of outliers. Numerous robust loss functions have been\nproposed in regression and classification, along with various computing\nalgorithms. In modern penalised generalised linear models (GLM), however, there\nis limited research on robust estimation that can provide weights to determine\nthe outlier status of the observations. This article proposes a unified\nframework based on a large family of loss functions, a composite of concave and\nconvex functions (CC-family). Properties of the CC-family are investigated, and\nCC-estimation is innovatively conducted via the iteratively reweighted convex\noptimisation (IRCO), which is a generalisation of the iteratively reweighted\nleast squares in robust linear regression. For robust GLM, the IRCO becomes the\niteratively reweighted GLM. The unified framework contains penalised estimation\nand robust support vector machine and is demonstrated with a variety of data\napplications."}, "http://arxiv.org/abs/2010.09335": {"title": "Statistical Models for Repeated Categorical Ratings: The R Package rater", "link": "http://arxiv.org/abs/2010.09335", "description": "A common problem in many disciplines is the need to assign a set of items\ninto categories or classes with known labels. This is often done by one or more\nexpert raters, or sometimes by an automated process. If these assignments or\n`ratings' are difficult to make accurately, a common tactic is to repeat them\nby different raters, or even by the same rater multiple times on different\noccasions. We present an R package `rater`, available on CRAN, that implements\nBayesian versions of several statistical models for analysis of repeated\ncategorical rating data. Inference is possible for the true underlying (latent)\nclass of each item, as well as the accuracy of each rater. The models are\nextensions of, and include, the Dawid-Skene model, and we implemented them\nusing the Stan probabilistic programming language. We illustrate the use of\n`rater` through a few examples. We also discuss in detail the techniques of\nmarginalisation and conditioning, which are necessary for these models but also\napply more generally to other models implemented in Stan."}, "http://arxiv.org/abs/2204.10969": {"title": "Combining Doubly Robust Methods and Machine Learning for Estimating Average Treatment Effects for Observational Real-world Data", "link": "http://arxiv.org/abs/2204.10969", "description": "Observational cohort studies are increasingly being used for comparative\neffectiveness research to assess the safety of therapeutics. Recently, various\ndoubly robust methods have been proposed for average treatment effect\nestimation by combining the treatment model and the outcome model via different\nvehicles, such as matching, weighting, and regression. The key advantage of\ndoubly robust estimators is that they require either the treatment model or the\noutcome model to be correctly specified to obtain a consistent estimator of\naverage treatment effects, and therefore lead to a more accurate and often more\nprecise inference. However, little work has been done to understand how doubly\nrobust estimators differ due to their unique strategies of using the treatment\nand outcome models and how machine learning techniques can be combined to boost\ntheir performance. Here we examine multiple popular doubly robust methods and\ncompare their performance using different treatment and outcome modeling via\nextensive simulations and a real-world application. We found that incorporating\nmachine learning with doubly robust estimators such as the targeted maximum\nlikelihood estimator gives the best overall performance. Practical guidance on\nhow to apply doubly robust estimators is provided."}, "http://arxiv.org/abs/2205.00259": {"title": "cubble: An R Package for Organizing and Wrangling Multivariate Spatio-temporal Data", "link": "http://arxiv.org/abs/2205.00259", "description": "Multivariate spatio-temporal data refers to multiple measurements taken\nacross space and time. For many analyses, spatial and time components can be\nseparately studied: for example, to explore the temporal trend of one variable\nfor a single spatial location, or to model the spatial distribution of one\nvariable at a given time. However for some studies, it is important to analyse\ndifferent aspects of the spatio-temporal data simultaneouly, like for instance,\ntemporal trends of multiple variables across locations. In order to facilitate\nthe study of different portions or combinations of spatio-temporal data, we\nintroduce a new data structure, cubble, with a suite of functions enabling easy\nslicing and dicing on the different components spatio-temporal components. The\nproposed cubble structure ensures that all the components of the data are easy\nto access and manipulate while providing flexibility for data analysis. In\naddition, cubble facilitates visual and numerical explorations of the data\nwhile easing data wrangling and modelling. The cubble structure and the\nfunctions provided in the cubble R package equip users with the capability to\nhandle hierarchical spatial and temporal structures. The cubble structure and\nthe tools implemented in the package are illustrated with different examples of\nAustralian climate data."}, "http://arxiv.org/abs/2301.08836": {"title": "Scalable Gaussian Process Inference with Stan", "link": "http://arxiv.org/abs/2301.08836", "description": "Gaussian processes (GPs) are sophisticated distributions to model functional\ndata. Whilst theoretically appealing, they are computationally cumbersome\nexcept for small datasets. We implement two methods for scaling GP inference in\nStan: First, a general sparse approximation using a directed acyclic dependency\ngraph; second, a fast, exact method for regularly spaced data modeled by GPs\nwith stationary kernels using the fast Fourier transform. Based on benchmark\nexperiments, we offer guidance for practitioners to decide between different\nmethods and parameterizations. We consider two real-world examples to\nillustrate the package. The implementation follows Stan's design and exposes\nperformant inference through a familiar interface. Full posterior inference for\nten thousand data points is feasible on a laptop in less than 20 seconds.\nDetails on how to get started using the popular interfaces cmdstanpy for Python\nand cmdstanr for R are provided."}, "http://arxiv.org/abs/2302.09103": {"title": "Multiple change-point detection for Poisson processes", "link": "http://arxiv.org/abs/2302.09103", "description": "The aim of change-point detection is to discover the changes in behavior that\nlie behind time sequence data. In this article, we study the case where the\ndata comes from an inhomogeneous Poisson process or a marked Poisson process.\nWe present a methodology for detecting multiple offline change-points based on\na minimum contrast estimator. In particular, we explain how to handle the\ncontinuous nature of the process with the available discrete observations. In\naddition, we select the appropriate number of regimes via a cross-validation\nprocedure which is really handy here due to the nature of the Poisson process.\nThrough experiments on simulated and real data sets, we demonstrate the\ninterest of the proposed method. The proposed method has been implemented in\nthe R package \\texttt{CptPointProcess} R."}, "http://arxiv.org/abs/2303.00531": {"title": "Parameter estimation for a hidden linear birth and death process with immigration", "link": "http://arxiv.org/abs/2303.00531", "description": "In this paper, we use a linear birth and death process with immigration to\nmodel infectious disease propagation when contamination stems from both\nperson-to-person contact and contact with the environment. Our aim is to\nestimate the parameters of the process. The main originality and difficulty\ncomes from the observation scheme. Counts of infected population are hidden.\nThe only data available are periodic cumulated new retired counts. Although\nvery common in epidemiology, this observation scheme is mathematically\nchallenging even for such a standard stochastic process. We first derive an\nanalytic expression of the unknown parameters as functions of well-chosen\ndiscrete time transition probabilities. Second, we extend and adapt the\nstandard Baum-Welch algorithm in order to estimate the said discrete time\ntransition probabilities in our hidden data framework. The performance of our\nestimators is illustrated both on synthetic data and real data of typhoid fever\nin Mayotte."}, "http://arxiv.org/abs/2306.04836": {"title": "$K$-Nearest-Neighbor Resampling for Off-Policy Evaluation in Stochastic Control", "link": "http://arxiv.org/abs/2306.04836", "description": "In this paper, we propose a novel $K$-nearest neighbor resampling procedure\nfor estimating the performance of a policy from historical data containing\nrealized episodes of a decision process generated under a different policy. We\nprovide statistical consistency results under weak conditions. In particular,\nwe avoid the common assumption of identically and independently distributed\ntransitions and rewards. Instead, our analysis allows for the sampling of\nentire episodes, as is common practice in most applications. To establish the\nconsistency in this setting, we generalize Stone's Theorem, a well-known result\nin nonparametric statistics on local averaging, to include episodic data and\nthe counterfactual estimation underlying off-policy evaluation (OPE). By\nfocusing on feedback policies that depend deterministically on the current\nstate in environments with continuous state-action spaces and system-inherent\nstochasticity effected by chosen actions, and relying on trajectory simulation\nsimilar to Monte Carlo methods, the proposed method is particularly well suited\nfor stochastic control environments. Compared to other OPE methods, our\nalgorithm does not require optimization, can be efficiently implemented via\ntree-based nearest neighbor search and parallelization, and does not explicitly\nassume a parametric model for the environment's dynamics. Numerical experiments\ndemonstrate the effectiveness of the algorithm compared to existing baselines\nin a variety of stochastic control settings, including a linear quadratic\nregulator, trade execution in limit order books, and online stochastic bin\npacking."}, "http://arxiv.org/abs/2401.05343": {"title": "Spectral Topological Data Analysis of Brain Signals", "link": "http://arxiv.org/abs/2401.05343", "description": "Topological data analysis (TDA) has become a powerful approach over the last\ntwenty years, mainly due to its ability to capture the shape and the geometry\ninherent in the data. Persistence homology, which is a particular tool in TDA,\nhas been demonstrated to be successful in analyzing functional brain\nconnectivity. One limitation of standard approaches is that they use\narbitrarily chosen threshold values for analyzing connectivity matrices. To\novercome this weakness, TDA provides a filtration of the weighted brain network\nacross a range of threshold values. However, current analyses of the\ntopological structure of functional brain connectivity primarily rely on overly\nsimplistic connectivity measures, such as the Pearson orrelation. These\nmeasures do not provide information about the specific oscillators that drive\ndependence within the brain network. Here, we develop a frequency-specific\napproach that utilizes coherence, a measure of dependence in the spectral\ndomain, to evaluate the functional connectivity of the brain. Our approach, the\nspectral TDA (STDA), has the ability to capture more nuanced and detailed\ninformation about the underlying brain networks. The proposed STDA method leads\nto a novel topological summary, the spectral landscape, which is a\n2D-generalization of the persistence landscape. Using the novel spectral\nlandscape, we analyze the EEG brain connectivity of patients with attention\ndeficit hyperactivity disorder (ADHD) and shed light on the frequency-specific\ndifferences in the topology of brain connectivity between the controls and ADHD\npatients."}, "http://arxiv.org/abs/2401.05414": {"title": "On the Three Demons in Causality in Finance: Time Resolution, Nonstationarity, and Latent Factors", "link": "http://arxiv.org/abs/2401.05414", "description": "Financial data is generally time series in essence and thus suffers from\nthree fundamental issues: the mismatch in time resolution, the time-varying\nproperty of the distribution - nonstationarity, and causal factors that are\nimportant but unknown/unobserved. In this paper, we follow a causal perspective\nto systematically look into these three demons in finance. Specifically, we\nreexamine these issues in the context of causality, which gives rise to a novel\nand inspiring understanding of how the issues can be addressed. Following this\nperspective, we provide systematic solutions to these problems, which hopefully\nwould serve as a foundation for future research in the area."}, "http://arxiv.org/abs/2401.05466": {"title": "Multidimensional Scaling for Interval Data: INTERSCAL", "link": "http://arxiv.org/abs/2401.05466", "description": "Standard multidimensional scaling takes as input a dissimilarity matrix of\ngeneral term $\\delta _{ij}$ which is a numerical value. In this paper we input\n$\\delta _{ij}=[\\underline{\\delta _{ij}},\\overline{\\delta _{ij}}]$ where\n$\\underline{\\delta _{ij}}$ and $\\overline{\\delta _{ij}}$ are the lower bound\nand the upper bound of the ``dissimilarity'' between the stimulus/object $S_i$\nand the stimulus/object $S_j$ respectively. As output instead of representing\neach stimulus/object on a factorial plane by a point, as in other\nmultidimensional scaling methods, in the proposed method each stimulus/object\nis visualized by a rectangle, in order to represent dissimilarity variation. We\ngeneralize the classical scaling method looking for a method that produces\nresults similar to those obtained by Tops Principal Components Analysis. Two\nexamples are presented to illustrate the effectiveness of the proposed method."}, "http://arxiv.org/abs/2401.05471": {"title": "Shrinkage linear regression for symbolic interval-valued variables", "link": "http://arxiv.org/abs/2401.05471", "description": "This paper proposes a new approach to fit a linear regression for symbolic\ninternal-valued variables, which improves both the Center Method suggested by\nBillard and Diday in \\cite{BillardDiday2000} and the Center and Range Method\nsuggested by Lima-Neto, E.A. and De Carvalho, F.A.T. in \\cite{Lima2008,\nLima2010}. Just in the Centers Method and the Center and Range Method, the new\nmethods proposed fit the linear regression model on the midpoints and in the\nhalf of the length of the intervals as an additional variable (ranges) assumed\nby the predictor variables in the training data set, but to make these fitments\nin the regression models, the methods Ridge Regression, Lasso, and Elastic Net\nproposed by Tibshirani, R. Hastie, T., and Zou H in \\cite{Tib1996,\nHastieZou2005} are used. The prediction of the lower and upper of the interval\nresponse (dependent) variable is carried out from their midpoints and ranges,\nwhich are estimated from the linear regression models with shrinkage generated\nin the midpoints and the ranges of the interval-valued predictors. Methods\npresented in this document are applied to three real data sets cardiologic\ninterval data set, Prostate interval data set and US Murder interval data set\nto then compare their performance and facility of interpretation regarding the\nCenter Method and the Center and Range Method. For this evaluation, the\nroot-mean-squared error and the correlation coefficient are used. Besides, the\nreader may use all the methods presented herein and verify the results using\nthe {\\tt RSDA} package written in {\\tt R} language, that can be downloaded and\ninstalled directly from {\\tt CRAN} \\cite{Rod2014}."}, "http://arxiv.org/abs/2401.05472": {"title": "INTERSTATIS: The STATIS method for interval valued data", "link": "http://arxiv.org/abs/2401.05472", "description": "The STATIS method, proposed by L'Hermier des Plantes and Escoufier, is used\nto analyze multiple data tables in which is very common that each of the tables\nhave information concerning the same set of individuals. The differences and\nsimilitudes between said tables are analyzed by means of a structure called the\n\\emph{compromise}. In this paper we present a new algorithm for applying the\nSTATIS method when the input consists of interval data. This proposal is based\non Moore's interval arithmetic and the Centers Method for Principal Component\nAnalysis with interval data, proposed by Cazes el al. \\cite{cazes1997}. In\naddition to presenting the INTERSTATIS method in an algorithmic way, an\nexecution example is shown, alongside the interpretation of its results."}, "http://arxiv.org/abs/2401.05473": {"title": "Pyramidal Clustering Algorithms in ISO-3D Project", "link": "http://arxiv.org/abs/2401.05473", "description": "Pyramidal clustering method generalizes hierarchies by allowing non-disjoint\nclasses at a given level instead of a partition. Moreover, the clusters of the\npyramid are intervals of a total order on the set being clustered. [Diday\n1984], [Bertrand, Diday 1990] and [Mfoumoune 1998] proposed algorithms to build\na pyramid starting with an arbitrary order of the individual. In this paper we\npresent two new algorithms name {\\tt CAPS} and {\\tt CAPSO}. {\\tt CAPSO} builds\na pyramid starting with an order given on the set of the individuals (or\nsymbolic objects) while {\\tt CAPS} finds this order. These two algorithms\nallows moreover to cluster more complex data than the tabular model allows to\nprocess, by considering variation on the values taken by the variables, in this\nway, our method produces a symbolic pyramid. Each cluster thus formed is\ndefined not only by the set of its elements (i.e. its extent) but also by a\nsymbolic object, which describes its properties (i.e. its intent). These two\nalgorithms were implemented in C++ and Java to the ISO-3D project."}, "http://arxiv.org/abs/2401.05517": {"title": "On Efficient Inference of Causal Effects with Multiple Mediators", "link": "http://arxiv.org/abs/2401.05517", "description": "This paper provides robust estimators and efficient inference of causal\neffects involving multiple interacting mediators. Most existing works either\nimpose a linear model assumption among the mediators or are restricted to\nhandle conditionally independent mediators given the exposure. To overcome\nthese limitations, we define causal and individual mediation effects in a\ngeneral setting, and employ a semiparametric framework to develop quadruply\nrobust estimators for these causal effects. We further establish the asymptotic\nnormality of the proposed estimators and prove their local semiparametric\nefficiencies. The proposed method is empirically validated via simulated and\nreal datasets concerning psychiatric disorders in trauma survivors."}, "http://arxiv.org/abs/2401.05556": {"title": "Assessing High-Order Links in Cardiovascular and Respiratory Networks via Static and Dynamic Information Measures", "link": "http://arxiv.org/abs/2401.05556", "description": "The network representation is becoming increasingly popular for the\ndescription of cardiovascular interactions based on the analysis of multiple\nsimultaneously collected variables. However, the traditional methods to assess\nnetwork links based on pairwise interaction measures cannot reveal high-order\neffects involving more than two nodes, and are not appropriate to infer the\nunderlying network topology. To address these limitations, here we introduce a\nframework which combines the assessment of high-order interactions with\nstatistical inference for the characterization of the functional links\nsustaining physiological networks. The framework develops information-theoretic\nmeasures quantifying how two nodes interact in a redundant or synergistic way\nwith the rest of the network, and employs these measures for reconstructing the\nfunctional structure of the network. The measures are implemented for both\nstatic and dynamic networks mapped respectively by random variables and random\nprocesses using plug-in and model-based entropy estimators. The validation on\ntheoretical and numerical simulated networks documents the ability of the\nframework to represent high-order interactions as networks and to detect\nstatistical structures associated to cascade, common drive and common target\neffects. The application to cardiovascular networks mapped by the beat-to-beat\nvariability of heart rate, respiration, arterial pressure, cardiac output and\nvascular resistance allowed noninvasive characterization of several mechanisms\nof cardiovascular control operating in resting state and during orthostatic\nstress. Our approach brings to new comprehensive assessment of physiological\ninteractions and complements existing strategies for the classification of\npathophysiological states."}, "http://arxiv.org/abs/2401.05728": {"title": "A General Method for Resampling Autocorrelated Spatial Data", "link": "http://arxiv.org/abs/2401.05728", "description": "Comparing spatial data sets is a ubiquitous task in data analysis, however\nthe presence of spatial autocorrelation means that standard estimates of\nvariance will be wrong and tend to over-estimate the statistical significance\nof correlations and other observations. While there are a number of existing\napproaches to this problem, none are ideal, requiring detailed analytical\ncalculations, which are hard to generalise or detailed knowledge of the data\ngenerating process, which may not be available. In this work we propose a\nresampling approach based on Tobler's Law. By resampling the data with fixed\nspatial autocorrelation, measured by Moran's I, we generate a more realistic\nnull model. Testing on real and synthetic data, we find that, as long as the\nspatial autocorrelation is not too strong, this approach works just as well as\nif we knew the data generating process."}, "http://arxiv.org/abs/2401.05784": {"title": "Covariance Function Estimation for High-Dimensional Functional Time Series with Dual Factor Structures", "link": "http://arxiv.org/abs/2401.05784", "description": "We propose a flexible dual functional factor model for modelling\nhigh-dimensional functional time series. In this model, a high-dimensional\nfully functional factor parametrisation is imposed on the observed functional\nprocesses, whereas a low-dimensional version (via series approximation) is\nassumed for the latent functional factors. We extend the classic principal\ncomponent analysis technique for the estimation of a low-rank structure to the\nestimation of a large covariance matrix of random functions that satisfies a\nnotion of (approximate) functional \"low-rank plus sparse\" structure; and\ngeneralise the matrix shrinkage method to functional shrinkage in order to\nestimate the sparse structure of functional idiosyncratic components. Under\nappropriate regularity conditions, we derive the large sample theory of the\ndeveloped estimators, including the consistency of the estimated factors and\nfunctional factor loadings and the convergence rates of the estimated matrices\nof covariance functions measured by various (functional) matrix norms.\nConsistent selection of the number of factors and a data-driven rule to choose\nthe shrinkage parameter are discussed. Simulation and empirical studies are\nprovided to demonstrate the finite-sample performance of the developed model\nand estimation methodology."}, "http://arxiv.org/abs/2401.05817": {"title": "Testing for similarity of multivariate mixed outcomes using generalised joint regression models with application to efficacy-toxicity responses", "link": "http://arxiv.org/abs/2401.05817", "description": "A common problem in clinical trials is to test whether the effect of an\nexplanatory variable on a response of interest is similar between two groups,\ne.g. patient or treatment groups. In this regard, similarity is defined as\nequivalence up to a pre-specified threshold that denotes an acceptable\ndeviation between the two groups. This issue is typically tackled by assessing\nif the explanatory variable's effect on the response is similar. This\nassessment is based on, for example, confidence intervals of differences or a\nsuitable distance between two parametric regression models. Typically, these\napproaches build on the assumption of a univariate continuous or binary outcome\nvariable. However, multivariate outcomes, especially beyond the case of\nbivariate binary response, remain underexplored. This paper introduces an\napproach based on a generalised joint regression framework exploiting the\nGaussian copula. Compared to existing methods, our approach accommodates\nvarious outcome variable scales, such as continuous, binary, categorical, and\nordinal, including mixed outcomes in multi-dimensional spaces. We demonstrate\nthe validity of this approach through a simulation study and an\nefficacy-toxicity case study, hence highlighting its practical relevance."}, "http://arxiv.org/abs/2401.05839": {"title": "Modelling physical activity profiles in COPD patients: a fully functional approach to variable domain functional regression models", "link": "http://arxiv.org/abs/2401.05839", "description": "Physical activity plays a significant role in the well-being of individuals\nwith Chronic obstructive Pulmonary Disease (COPD). Specifically, it has been\ndirectly associated with changes in hospitalization rates for these patients.\nHowever, previous investigations have primarily been conducted in a\ncross-sectional or longitudinal manner and have not considered a continuous\nperspective. Using the telEPOC program we use telemonitoring data to analyze\nthe impact of physical activity adopting a functional data approach. However,\nTraditional functional data methods, including functional regression models,\ntypically assume a consistent data domain. However, the data in the telEPOC\nprogram exhibits variable domains, presenting a challenge since the majority of\nfunctional data methods, are based on the fact that data are observed in the\nsame domain. To address this challenge, we introduce a novel fully functional\nmethodology tailored to variable domain functional data, eliminating the need\nfor data alignment, which can be computationally taxing. Although models\ndesigned for variable domain data are relatively scarce and may have inherent\nlimitations in their estimation methods, our approach circumvents these issues.\nWe substantiate the effectiveness of our methodology through a simulation\nstudy, comparing our results with those obtained using established\nmethodologies. Finally, we apply our methodology to analyze the impact of\nphysical activity in COPD patients using the telEPOC program's data. Software\nfor our method is available in the form of R code on request at\n\\url{https://github.com/Pavel-Hernadez-Amaro/V.D.F.R.M-new-estimation-approach.git}."}, "http://arxiv.org/abs/2401.05905": {"title": "Feasible pairwise pseudo-likelihood inference on spatial regressions in irregular lattice grids: the KD-T PL algorithm", "link": "http://arxiv.org/abs/2401.05905", "description": "Spatial regression models are central to the field of spatial statistics.\nNevertheless, their estimation in case of large and irregular gridded spatial\ndatasets presents considerable computational challenges. To tackle these\ncomputational problems, Arbia \\citep{arbia_2014_pairwise} introduced a\npseudo-likelihood approach (called pairwise likelihood, say PL) which required\nthe identification of pairs of observations that are internally correlated, but\nmutually conditionally uncorrelated. However, while the PL estimators enjoy\noptimal theoretical properties, their practical implementation when dealing\nwith data observed on irregular grids suffers from dramatic computational\nissues (connected with the identification of the pairs of observations) that,\nin most empirical cases, negatively counter-balance its advantages. In this\npaper we introduce an algorithm specifically designed to streamline the\ncomputation of the PL in large and irregularly gridded spatial datasets,\ndramatically simplifying the estimation phase. In particular, we focus on the\nestimation of Spatial Error models (SEM). Our proposed approach, efficiently\npairs spatial couples exploiting the KD tree data structure and exploits it to\nderive the closed-form expressions for fast parameter approximation. To\nshowcase the efficiency of our method, we provide an illustrative example using\nsimulated data, demonstrating the computational advantages if compared to a\nfull likelihood inference are not at the expenses of accuracy."}, "http://arxiv.org/abs/2401.06082": {"title": "Borrowing from historical control data in a Bayesian time-to-event model with flexible baseline hazard function", "link": "http://arxiv.org/abs/2401.06082", "description": "There is currently a focus on statistical methods which can use historical\ntrial information to help accelerate the discovery, development and delivery of\nmedicine. Bayesian methods can be constructed so that the borrowing is\n\"dynamic\" in the sense that the similarity of the data helps to determine how\nmuch information is used. In the time to event setting with one historical data\nset, a popular model for a range of baseline hazards is the piecewise\nexponential model where the time points are fixed and a borrowing structure is\nimposed on the model. Although convenient for implementation this approach\neffects the borrowing capability of the model. We propose a Bayesian model\nwhich allows the time points to vary and a dependency to be placed between the\nbaseline hazards. This serves to smooth the posterior baseline hazard improving\nboth model estimation and borrowing characteristics. We explore a variety of\nprior structures for the borrowing within our proposed model and assess their\nperformance against established approaches. We demonstrate that this leads to\nimproved type I error in the presence of prior data conflict and increased\npower. We have developed accompanying software which is freely available and\nenables easy implementation of the approach."}, "http://arxiv.org/abs/2401.06091": {"title": "A Closer Look at AUROC and AUPRC under Class Imbalance", "link": "http://arxiv.org/abs/2401.06091", "description": "In machine learning (ML), a widespread adage is that the area under the\nprecision-recall curve (AUPRC) is a superior metric for model comparison to the\narea under the receiver operating characteristic (AUROC) for binary\nclassification tasks with class imbalance. This paper challenges this notion\nthrough novel mathematical analysis, illustrating that AUROC and AUPRC can be\nconcisely related in probabilistic terms. We demonstrate that AUPRC, contrary\nto popular belief, is not superior in cases of class imbalance and might even\nbe a harmful metric, given its inclination to unduly favor model improvements\nin subpopulations with more frequent positive labels. This bias can\ninadvertently heighten algorithmic disparities. Prompted by these insights, a\nthorough review of existing ML literature was conducted, utilizing large\nlanguage models to analyze over 1.5 million papers from arXiv. Our\ninvestigation focused on the prevalence and substantiation of the purported\nAUPRC superiority. The results expose a significant deficit in empirical\nbacking and a trend of misattributions that have fuelled the widespread\nacceptance of AUPRC's supposed advantages. Our findings represent a dual\ncontribution: a significant technical advancement in understanding metric\nbehaviors and a stark warning about unchecked assumptions in the ML community.\nAll experiments are accessible at\nhttps://github.com/mmcdermott/AUC_is_all_you_need."}, "http://arxiv.org/abs/2011.04168": {"title": "Likelihood Inference for Possibly Non-Stationary Processes via Adaptive Overdifferencing", "link": "http://arxiv.org/abs/2011.04168", "description": "We make an observation that facilitates exact likelihood-based inference for\nthe parameters of the popular ARFIMA model without requiring stationarity by\nallowing the upper bound $\\bar{d}$ for the memory parameter $d$ to exceed\n$0.5$. We observe that estimating the parameters of a single non-stationary\nARFIMA model is equivalent to estimating the parameters of a sequence of\nstationary ARFIMA models, which allows for the use of existing methods for\nevaluating the likelihood for an invertible and stationary ARFIMA model. This\nenables improved inference because many standard methods perform poorly when\nestimates are close to the boundary of the parameter space. It also allows us\nto leverage the wealth of likelihood approximations that have been introduced\nfor estimating the parameters of a stationary process. We explore how\nestimation of the memory parameter $d$ depends on the upper bound $\\bar{d}$ and\nintroduce adaptive procedures for choosing $\\bar{d}$. Via simulations, we\nexamine the performance of our adaptive procedures for estimating the memory\nparameter when the true value is as large as $2.5$. Our adaptive procedures\nestimate the memory parameter well, can be used to obtain confidence intervals\nfor the memory parameter that achieve nominal coverage rates, and perform\nfavorably relative to existing alternatives."}, "http://arxiv.org/abs/2203.10118": {"title": "Bayesian Structural Learning with Parametric Marginals for Count Data: An Application to Microbiota Systems", "link": "http://arxiv.org/abs/2203.10118", "description": "High dimensional and heterogeneous count data are collected in various\napplied fields. In this paper, we look closely at high-resolution sequencing\ndata on the microbiome, which have enabled researchers to study the genomes of\nentire microbial communities. Revealing the underlying interactions between\nthese communities is of vital importance to learn how microbes influence human\nhealth. To perform structural learning from multivariate count data such as\nthese, we develop a novel Gaussian copula graphical model with two key\nelements. Firstly, we employ parametric regression to characterize the marginal\ndistributions. This step is crucial for accommodating the impact of external\ncovariates. Neglecting this adjustment could potentially introduce distortions\nin the inference of the underlying network of dependences. Secondly, we advance\na Bayesian structure learning framework, based on a computationally efficient\nsearch algorithm that is suited to high dimensionality. The approach returns\nsimultaneous inference of the marginal effects and of the dependence structure,\nincluding graph uncertainty estimates. A simulation study and a real data\nanalysis of microbiome data highlight the applicability of the proposed\napproach at inferring networks from multivariate count data in general, and its\nrelevance to microbiome analyses in particular. The proposed method is\nimplemented in the R package BDgraph."}, "http://arxiv.org/abs/2207.02986": {"title": "fabisearch: A Package for Change Point Detection in and Visualization of the Network Structure of Multivariate High-Dimensional Time Series in R", "link": "http://arxiv.org/abs/2207.02986", "description": "Change point detection is a commonly used technique in time series analysis,\ncapturing the dynamic nature in which many real-world processes function. With\nthe ever increasing troves of multivariate high-dimensional time series data,\nespecially in neuroimaging and finance, there is a clear need for scalable and\ndata-driven change point detection methods. Currently, change point detection\nmethods for multivariate high-dimensional data are scarce, with even less\navailable in high-level, easily accessible software packages. To this end, we\nintroduce the R package fabisearch, available on the Comprehensive R Archive\nNetwork (CRAN), which implements the factorized binary search (FaBiSearch)\nmethodology. FaBiSearch is a novel statistical method for detecting change\npoints in the network structure of multivariate high-dimensional time series\nwhich employs non-negative matrix factorization (NMF), an unsupervised\ndimension reduction and clustering technique. Given the high computational cost\nof NMF, we implement the method in C++ code and use parallelization to reduce\ncomputation time. Further, we also utilize a new binary search algorithm to\nefficiently identify multiple change points and provide a new method for\nnetwork estimation for data between change points. We show the functionality of\nthe package and the practicality of the method by applying it to a neuroimaging\nand a finance data set. Lastly, we provide an interactive, 3-dimensional,\nbrain-specific network visualization capability in a flexible, stand-alone\nfunction. This function can be conveniently used with any node coordinate\natlas, and nodes can be color coded according to community membership (if\napplicable). The output is an elegantly displayed network laid over a cortical\nsurface, which can be rotated in the 3-dimensional space."}, "http://arxiv.org/abs/2302.02718": {"title": "A Log-Linear Non-Parametric Online Changepoint Detection Algorithm based on Functional Pruning", "link": "http://arxiv.org/abs/2302.02718", "description": "Online changepoint detection aims to detect anomalies and changes in\nreal-time in high-frequency data streams, sometimes with limited available\ncomputational resources. This is an important task that is rooted in many\nreal-world applications, including and not limited to cybersecurity, medicine\nand astrophysics. While fast and efficient online algorithms have been recently\nintroduced, these rely on parametric assumptions which are often violated in\npractical applications. Motivated by data streams from the telecommunications\nsector, we build a flexible nonparametric approach to detect a change in the\ndistribution of a sequence. Our procedure, NP-FOCuS, builds a sequential\nlikelihood ratio test for a change in a set of points of the empirical\ncumulative density function of our data. This is achieved by keeping track of\nthe number of observations above or below those points. Thanks to functional\npruning ideas, NP-FOCuS has a computational cost that is log-linear in the\nnumber of observations and is suitable for high-frequency data streams. In\nterms of detection power, NP-FOCuS is seen to outperform current nonparametric\nonline changepoint techniques in a variety of settings. We demonstrate the\nutility of the procedure on both simulated and real data."}, "http://arxiv.org/abs/2304.10005": {"title": "Prediction under interventions: evaluation of counterfactual performance using longitudinal observational data", "link": "http://arxiv.org/abs/2304.10005", "description": "Predictions under interventions are estimates of what a person's risk of an\noutcome would be if they were to follow a particular treatment strategy, given\ntheir individual characteristics. Such predictions can give important input to\nmedical decision making. However, evaluating predictive performance of\ninterventional predictions is challenging. Standard ways of evaluating\npredictive performance do not apply when using observational data, because\nprediction under interventions involves obtaining predictions of the outcome\nunder conditions that are different to those that are observed for a subset of\nindividuals in the validation dataset. This work describes methods for\nevaluating counterfactual performance of predictions under interventions for\ntime-to-event outcomes. This means we aim to assess how well predictions would\nmatch the validation data if all individuals had followed the treatment\nstrategy under which predictions are made. We focus on counterfactual\nperformance evaluation using longitudinal observational data, and under\ntreatment strategies that involve sustaining a particular treatment regime over\ntime. We introduce an estimation approach using artificial censoring and\ninverse probability weighting which involves creating a validation dataset that\nmimics the treatment strategy under which predictions are made. We extend\nmeasures of calibration, discrimination (c-index and cumulative/dynamic AUCt)\nand overall prediction error (Brier score) to allow assessment of\ncounterfactual performance. The methods are evaluated using a simulation study,\nincluding scenarios in which the methods should detect poor performance.\nApplying our methods in the context of liver transplantation shows that our\nprocedure allows quantification of the performance of predictions supporting\ncrucial decisions on organ allocation."}, "http://arxiv.org/abs/2309.01334": {"title": "Average treatment effect on the treated, under lack of positivity", "link": "http://arxiv.org/abs/2309.01334", "description": "The use of propensity score (PS) methods has become ubiquitous in causal\ninference. At the heart of these methods is the positivity assumption.\nViolation of the positivity assumption leads to the presence of extreme PS\nweights when estimating average causal effects of interest, such as the average\ntreatment effect (ATE) or the average treatment effect on the treated (ATT),\nwhich renders invalid related statistical inference. To circumvent this issue,\ntrimming or truncating the extreme estimated PSs have been widely used.\nHowever, these methods require that we specify a priori a threshold and\nsometimes an additional smoothing parameter. While there are a number of\nmethods dealing with the lack of positivity when estimating ATE, surprisingly\nthere is no much effort in the same issue for ATT. In this paper, we first\nreview widely used methods, such as trimming and truncation in ATT. We\nemphasize the underlying intuition behind these methods to better understand\ntheir applications and highlight their main limitations. Then, we argue that\nthe current methods simply target estimands that are scaled ATT (and thus move\nthe goalpost to a different target of interest), where we specify the scale and\nthe target populations. We further propose a PS weight-based alternative for\nthe average causal effect on the treated, called overlap weighted average\ntreatment effect on the treated (OWATT). The appeal of our proposed method lies\nin its ability to obtain similar or even better results than trimming and\ntruncation while relaxing the constraint to choose a priori a threshold (or\neven specify a smoothing parameter). The performance of the proposed method is\nillustrated via a series of Monte Carlo simulations and a data analysis on\nracial disparities in health care expenditures."}, "http://arxiv.org/abs/2401.06261": {"title": "Prediction of causal genes at GWAS loci with pleiotropic gene regulatory effects using sets of correlated instrumental variables", "link": "http://arxiv.org/abs/2401.06261", "description": "Multivariate Mendelian randomization (MVMR) is a statistical technique that\nuses sets of genetic instruments to estimate the direct causal effects of\nmultiple exposures on an outcome of interest. At genomic loci with pleiotropic\ngene regulatory effects, that is, loci where the same genetic variants are\nassociated to multiple nearby genes, MVMR can potentially be used to predict\ncandidate causal genes. However, consensus in the field dictates that the\ngenetic instruments in MVMR must be independent, which is usually not possible\nwhen considering a group of candidate genes from the same locus.\n\nWe used causal inference theory to show that MVMR with correlated instruments\nsatisfies the instrumental set condition. This is a classical result by Brito\nand Pearl (2002) for structural equation models that guarantees the\nidentifiability of causal effects in situations where multiple exposures\ncollectively, but not individually, separate a set of instrumental variables\nfrom an outcome variable. Extensive simulations confirmed the validity and\nusefulness of these theoretical results even at modest sample sizes.\nImportantly, the causal effect estimates remain unbiased and their variance\nsmall when instruments are highly correlated.\n\nWe applied MVMR with correlated instrumental variable sets at risk loci from\ngenome-wide association studies (GWAS) for coronary artery disease using eQTL\ndata from the STARNET study. Our method predicts causal genes at twelve loci,\neach associated with multiple colocated genes in multiple tissues. However, the\nextensive degree of regulatory pleiotropy across tissues and the limited number\nof causal variants in each locus still require that MVMR is run on a\ntissue-by-tissue basis, and testing all gene-tissue pairs at a given locus in a\nsingle model to predict causal gene-tissue combinations remains infeasible."}, "http://arxiv.org/abs/2401.06264": {"title": "Exposure effects are policy relevant only under strong assumptions about the interference structure", "link": "http://arxiv.org/abs/2401.06264", "description": "Savje (2023) recommends misspecified exposure effects as a way to avoid\nstrong assumptions about interference when analyzing the results of an\nexperiment. In this discussion, we highlight a key limitation of Savje's\nrecommendation. Exposure effects are not generally useful for evaluating social\npolicies without the strong assumptions that Savje seeks to avoid.\n\nOur discussion is organized as follows. Section 2 summarizes our position,\nsection 3 provides a concrete example, and section 4 concludes. Proof of claims\nare in an appendix."}, "http://arxiv.org/abs/2401.06347": {"title": "Diagnostics for Regression Models with Semicontinuous Outcomes", "link": "http://arxiv.org/abs/2401.06347", "description": "Semicontinuous outcomes commonly arise in a wide variety of fields, such as\ninsurance claims, healthcare expenditures, rainfall amounts, and alcohol\nconsumption. Regression models, including Tobit, Tweedie, and two-part models,\nare widely employed to understand the relationship between semicontinuous\noutcomes and covariates. Given the potential detrimental consequences of model\nmisspecification, after fitting a regression model, it is of prime importance\nto check the adequacy of the model. However, due to the point mass at zero,\nstandard diagnostic tools for regression models (e.g., deviance and Pearson\nresiduals) are not informative for semicontinuous data. To bridge this gap, we\npropose a new type of residuals for semicontinuous outcomes that are applicable\nto general regression models. Under the correctly specified model, the proposed\nresiduals converge to being uniformly distributed, and when the model is\nmisspecified, they significantly depart from this pattern. In addition to\nin-sample validation, the proposed methodology can also be employed to evaluate\npredictive distributions. We demonstrate the effectiveness of the proposed tool\nusing health expenditure data from the US Medical Expenditure Panel Survey."}, "http://arxiv.org/abs/2401.06348": {"title": "A Fully Bayesian Approach for Comprehensive Mapping of Magnitude and Phase Brain Activation in Complex-Valued fMRI Data", "link": "http://arxiv.org/abs/2401.06348", "description": "Functional magnetic resonance imaging (fMRI) plays a crucial role in\nneuroimaging, enabling the exploration of brain activity through complex-valued\nsignals. These signals, composed of magnitude and phase, offer a rich source of\ninformation for understanding brain functions. Traditional fMRI analyses have\nlargely focused on magnitude information, often overlooking the potential\ninsights offered by phase data. In this paper, we propose a novel fully\nBayesian model designed for analyzing single-subject complex-valued fMRI\n(cv-fMRI) data. Our model, which we refer to as the CV-M&amp;P model, is\ndistinctive in its comprehensive utilization of both magnitude and phase\ninformation in fMRI signals, allowing for independent prediction of different\ntypes of activation maps. We incorporate Gaussian Markov random fields (GMRFs)\nto capture spatial correlations within the data, and employ image partitioning\nand parallel computation to enhance computational efficiency. Our model is\nrigorously tested through simulation studies, and then applied to a real\ndataset from a unilateral finger-tapping experiment. The results demonstrate\nthe model's effectiveness in accurately identifying brain regions activated in\nresponse to specific tasks, distinguishing between magnitude and phase\nactivation."}, "http://arxiv.org/abs/2401.06350": {"title": "Optimal estimation of the null distribution in large-scale inference", "link": "http://arxiv.org/abs/2401.06350", "description": "The advent of large-scale inference has spurred reexamination of conventional\nstatistical thinking. In a Gaussian model for $n$ many $z$-scores with at most\n$k &lt; \\frac{n}{2}$ nonnulls, Efron suggests estimating the location and scale\nparameters of the null distribution. Placing no assumptions on the nonnull\neffects, the statistical task can be viewed as a robust estimation problem.\nHowever, the best known robust estimators fail to be consistent in the regime\n$k \\asymp n$ which is especially relevant in large-scale inference. The failure\nof estimators which are minimax rate-optimal with respect to other formulations\nof robustness (e.g. Huber's contamination model) might suggest the\nimpossibility of consistent estimation in this regime and, consequently, a\nmajor weakness of Efron's suggestion. A sound evaluation of Efron's model thus\nrequires a complete understanding of consistency. We sharply characterize the\nregime of $k$ for which consistent estimation is possible and further establish\nthe minimax estimation rates. It is shown consistent estimation of the location\nparameter is possible if and only if $\\frac{n}{2} - k = \\omega(\\sqrt{n})$, and\nconsistent estimation of the scale parameter is possible in the entire regime\n$k &lt; \\frac{n}{2}$. Faster rates than those in Huber's contamination model are\nachievable by exploiting the Gaussian character of the data. The minimax upper\nbound is obtained by considering estimators based on the empirical\ncharacteristic function. The minimax lower bound involves constructing two\nmarginal distributions whose characteristic functions match on a wide interval\ncontaining zero. The construction notably differs from those in the literature\nby sharply capturing a scaling of $n-2k$ in the minimax estimation rate of the\nlocation."}, "http://arxiv.org/abs/2401.06383": {"title": "Decomposition with Monotone B-splines: Fitting and Testing", "link": "http://arxiv.org/abs/2401.06383", "description": "A univariate continuous function can always be decomposed as the sum of a\nnon-increasing function and a non-decreasing one. Based on this property, we\npropose a non-parametric regression method that combines two spline-fitted\nmonotone curves. We demonstrate by extensive simulations that, compared to\nstandard spline-fitting methods, the proposed approach is particularly\nadvantageous in high-noise scenarios. Several theoretical guarantees are\nestablished for the proposed approach. Additionally, we present statistics to\ntest the monotonicity of a function based on monotone decomposition, which can\nbetter control Type I error and achieve comparable (if not always higher) power\ncompared to existing methods. Finally, we apply the proposed fitting and\ntesting approaches to analyze the single-cell pseudotime trajectory datasets,\nidentifying significant biological insights for non-monotonically expressed\ngenes through Gene Ontology enrichment analysis. The source code implementing\nthe methodology and producing all results is accessible at\nhttps://github.com/szcf-weiya/MonotoneDecomposition.jl."}, "http://arxiv.org/abs/2401.06403": {"title": "Fourier analysis of spatial point processes", "link": "http://arxiv.org/abs/2401.06403", "description": "In this article, we develop comprehensive frequency domain methods for\nestimating and inferring the second-order structure of spatial point processes.\nThe main element here is on utilizing the discrete Fourier transform (DFT) of\nthe point pattern and its tapered counterpart. Under second-order stationarity,\nwe show that both the DFTs and the tapered DFTs are asymptotically jointly\nindependent Gaussian even when the DFTs share the same limiting frequencies.\nBased on these results, we establish an $\\alpha$-mixing central limit theorem\nfor a statistic formulated as a quadratic form of the tapered DFT. As\napplications, we derive the asymptotic distribution of the kernel spectral\ndensity estimator and establish a frequency domain inferential method for\nparametric stationary point processes. For the latter, the resulting model\nparameter estimator is computationally tractable and yields meaningful\ninterpretations even in the case of model misspecification. We investigate the\nfinite sample performance of our estimator through simulations, considering\nscenarios of both correctly specified and misspecified models. Furthermore, we\nextend our proposed DFT-based frequency domain methods to a class of\nnon-stationary spatial point processes."}, "http://arxiv.org/abs/2401.06446": {"title": "Increasing dimension asymptotics for two-way crossed mixed effect models", "link": "http://arxiv.org/abs/2401.06446", "description": "This paper presents asymptotic results for the maximum likelihood and\nrestricted maximum likelihood (REML) estimators within a two-way crossed mixed\neffect model as the sizes of the rows, columns, and cells tend to infinity.\nUnder very mild conditions which do not require the assumption of normality,\nthe estimators are proven to be asymptotically normal, possessing a structured\ncovariance matrix. The growth rate for the number of rows, columns, and cells\nis unrestricted, whether considered pairwise or collectively."}, "http://arxiv.org/abs/2401.06447": {"title": "A comprehensive framework for multi-fidelity surrogate modeling with noisy data: a gray-box perspective", "link": "http://arxiv.org/abs/2401.06447", "description": "Computer simulations (a.k.a. white-box models) are more indispensable than\never to model intricate engineering systems. However, computational models\nalone often fail to fully capture the complexities of reality. When physical\nexperiments are accessible though, it is of interest to enhance the incomplete\ninformation offered by computational models. Gray-box modeling is concerned\nwith the problem of merging information from data-driven (a.k.a. black-box)\nmodels and white-box (i.e., physics-based) models. In this paper, we propose to\nperform this task by using multi-fidelity surrogate models (MFSMs). A MFSM\nintegrates information from models with varying computational fidelity into a\nnew surrogate model. The multi-fidelity surrogate modeling framework we propose\nhandles noise-contaminated data and is able to estimate the underlying\nnoise-free high-fidelity function. Our methodology emphasizes on delivering\nprecise estimates of the uncertainty in its predictions in the form of\nconfidence and prediction intervals, by quantitatively incorporating the\ndifferent types of uncertainty that affect the problem, arising from\nmeasurement noise and from lack of knowledge due to the limited experimental\ndesign budget on both the high- and low-fidelity models. Applied to gray-box\nmodeling, our MFSM framework treats noisy experimental data as the\nhigh-fidelity and the white-box computational models as their low-fidelity\ncounterparts. The effectiveness of our methodology is showcased through\nsynthetic examples and a wind turbine application."}, "http://arxiv.org/abs/2401.06465": {"title": "Sanity Checks Revisited: An Exploration to Repair the Model Parameter Randomisation Test", "link": "http://arxiv.org/abs/2401.06465", "description": "The Model Parameter Randomisation Test (MPRT) is widely acknowledged in the\neXplainable Artificial Intelligence (XAI) community for its well-motivated\nevaluative principle: that the explanation function should be sensitive to\nchanges in the parameters of the model function. However, recent works have\nidentified several methodological caveats for the empirical interpretation of\nMPRT. To address these caveats, we introduce two adaptations to the original\nMPRT -- Smooth MPRT and Efficient MPRT, where the former minimises the impact\nthat noise has on the evaluation results through sampling and the latter\ncircumvents the need for biased similarity measurements by re-interpreting the\ntest through the explanation's rise in complexity, after full parameter\nrandomisation. Our experimental results demonstrate that these proposed\nvariants lead to improved metric reliability, thus enabling a more trustworthy\napplication of XAI methods."}, "http://arxiv.org/abs/2401.06557": {"title": "Treatment-Aware Hyperbolic Representation Learning for Causal Effect Estimation with Social Networks", "link": "http://arxiv.org/abs/2401.06557", "description": "Estimating the individual treatment effect (ITE) from observational data is a\ncrucial research topic that holds significant value across multiple domains.\nHow to identify hidden confounders poses a key challenge in ITE estimation.\nRecent studies have incorporated the structural information of social networks\nto tackle this challenge, achieving notable advancements. However, these\nmethods utilize graph neural networks to learn the representation of hidden\nconfounders in Euclidean space, disregarding two critical issues: (1) the\nsocial networks often exhibit a scalefree structure, while Euclidean embeddings\nsuffer from high distortion when used to embed such graphs, and (2) each\nego-centric network within a social network manifests a treatment-related\ncharacteristic, implying significant patterns of hidden confounders. To address\nthese issues, we propose a novel method called Treatment-Aware Hyperbolic\nRepresentation Learning (TAHyper). Firstly, TAHyper employs the hyperbolic\nspace to encode the social networks, thereby effectively reducing the\ndistortion of confounder representation caused by Euclidean embeddings.\nSecondly, we design a treatment-aware relationship identification module that\nenhances the representation of hidden confounders by identifying whether an\nindividual and her neighbors receive the same treatment. Extensive experiments\non two benchmark datasets are conducted to demonstrate the superiority of our\nmethod."}, "http://arxiv.org/abs/2401.06564": {"title": "Valid causal inference with unobserved confounding in high-dimensional settings", "link": "http://arxiv.org/abs/2401.06564", "description": "Various methods have recently been proposed to estimate causal effects with\nconfidence intervals that are uniformly valid over a set of data generating\nprocesses when high-dimensional nuisance models are estimated by\npost-model-selection or machine learning estimators. These methods typically\nrequire that all the confounders are observed to ensure identification of the\neffects. We contribute by showing how valid semiparametric inference can be\nobtained in the presence of unobserved confounders and high-dimensional\nnuisance models. We propose uncertainty intervals which allow for unobserved\nconfounding, and show that the resulting inference is valid when the amount of\nunobserved confounding is small relative to the sample size; the latter is\nformalized in terms of convergence rates. Simulation experiments illustrate the\nfinite sample properties of the proposed intervals and investigate an\nalternative procedure that improves the empirical coverage of the intervals\nwhen the amount of unobserved confounding is large. Finally, a case study on\nthe effect of smoking during pregnancy on birth weight is used to illustrate\nthe use of the methods introduced to perform a sensitivity analysis to\nunobserved confounding."}, "http://arxiv.org/abs/2401.06575": {"title": "A Weibull Mixture Cure Frailty Model for High-dimensional Covariates", "link": "http://arxiv.org/abs/2401.06575", "description": "A novel mixture cure frailty model is introduced for handling censored\nsurvival data. Mixture cure models are preferable when the existence of a cured\nfraction among patients can be assumed. However, such models are heavily\nunderexplored: frailty structures within cure models remain largely\nundeveloped, and furthermore, most existing methods do not work for\nhigh-dimensional datasets, when the number of predictors is significantly\nlarger than the number of observations. In this study, we introduce a novel\nextension of the Weibull mixture cure model that incorporates a frailty\ncomponent, employed to model an underlying latent population heterogeneity with\nrespect to the outcome risk. Additionally, high-dimensional covariates are\nintegrated into both the cure rate and survival part of the model, providing a\ncomprehensive approach to employ the model in the context of high-dimensional\nomics data. We also perform variable selection via an adaptive elastic net\npenalization, and propose a novel approach to inference using the\nexpectation-maximization (EM) algorithm. Extensive simulation studies are\nconducted across various scenarios to demonstrate the performance of the model,\nand results indicate that our proposed method outperforms competitor models. We\napply the novel approach to analyze RNAseq gene expression data from bulk\nbreast cancer patients included in The Cancer Genome Atlas (TCGA) database. A\nset of prognostic biomarkers is then derived from selected genes, and\nsubsequently validated via both functional enrichment analysis and comparison\nto the existing biological literature. Finally, a prognostic risk score index\nbased on the identified biomarkers is proposed and validated by exploring the\npatients' survival."}, "http://arxiv.org/abs/2401.06687": {"title": "Proximal Causal Inference With Text Data", "link": "http://arxiv.org/abs/2401.06687", "description": "Recent text-based causal methods attempt to mitigate confounding bias by\nincluding unstructured text data as proxies of confounding variables that are\npartially or imperfectly measured. These approaches assume analysts have\nsupervised labels of the confounders given text for a subset of instances, a\nconstraint that is not always feasible due to data privacy or cost. Here, we\naddress settings in which an important confounding variable is completely\nunobserved. We propose a new causal inference method that splits pre-treatment\ntext data, infers two proxies from two zero-shot models on the separate splits,\nand applies these proxies in the proximal g-formula. We prove that our\ntext-based proxy method satisfies identification conditions required by the\nproximal g-formula while other seemingly reasonable proposals do not. We\nevaluate our method in synthetic and semi-synthetic settings and find that it\nproduces estimates with low bias. This combination of proximal causal inference\nand zero-shot classifiers is novel (to our knowledge) and expands the set of\ntext-specific causal methods available to practitioners."}, "http://arxiv.org/abs/1905.11232": {"title": "Efficient posterior sampling for high-dimensional imbalanced logistic regression", "link": "http://arxiv.org/abs/1905.11232", "description": "High-dimensional data are routinely collected in many areas. We are\nparticularly interested in Bayesian classification models in which one or more\nvariables are imbalanced. Current Markov chain Monte Carlo algorithms for\nposterior computation are inefficient as $n$ and/or $p$ increase due to\nworsening time per step and mixing rates. One strategy is to use a\ngradient-based sampler to improve mixing while using data sub-samples to reduce\nper-step computational complexity. However, usual sub-sampling breaks down when\napplied to imbalanced data. Instead, we generalize piece-wise deterministic\nMarkov chain Monte Carlo algorithms to include importance-weighted and\nmini-batch sub-sampling. These approaches maintain the correct stationary\ndistribution with arbitrarily small sub-samples, and substantially outperform\ncurrent competitors. We provide theoretical support and illustrate gains in\nsimulated and real data applications."}, "http://arxiv.org/abs/2305.19481": {"title": "Bayesian Image Analysis in Fourier Space", "link": "http://arxiv.org/abs/2305.19481", "description": "Bayesian image analysis has played a large role over the last 40+ years in\nsolving problems in image noise-reduction, de-blurring, feature enhancement,\nand object detection. However, these problems can be complex and lead to\ncomputational difficulties, due to the modeled interdependence between spatial\nlocations. The Bayesian image analysis in Fourier space (BIFS) approach\nproposed here reformulates the conventional Bayesian image analysis paradigm\nfor continuous valued images as a large set of independent (but heterogeneous)\nprocesses over Fourier space. The original high-dimensional estimation problem\nin image space is thereby broken down into (trivially parallelizable)\nindependent one-dimensional problems in Fourier space. The BIFS approach leads\nto easy model specification with fast and direct computation, a wide range of\npossible prior characteristics, easy modeling of isotropy into the prior, and\nmodels that are effectively invariant to changes in image resolution."}, "http://arxiv.org/abs/2307.09404": {"title": "Continuous-time multivariate analysis", "link": "http://arxiv.org/abs/2307.09404", "description": "The starting point for much of multivariate analysis (MVA) is an $n\\times p$\ndata matrix whose $n$ rows represent observations and whose $p$ columns\nrepresent variables. Some multivariate data sets, however, may be best\nconceptualized not as $n$ discrete $p$-variate observations, but as $p$ curves\nor functions defined on a common time interval. We introduce a framework for\nextending techniques of multivariate analysis to such settings. The proposed\nframework rests on the assumption that the curves can be represented as linear\ncombinations of basis functions such as B-splines. This is formally identical\nto the Ramsay-Silverman representation of functional data; but whereas\nfunctional data analysis extends MVA to the case of observations that are\ncurves rather than vectors -- heuristically, $n\\times p$ data with $p$ infinite\n-- we are instead concerned with what happens when $n$ is infinite. We describe\nhow to translate the classical MVA methods of covariance and correlation\nestimation, principal component analysis, Fisher's linear discriminant\nanalysis, and $k$-means clustering to the continuous-time setting. We\nillustrate the methods with a novel perspective on a well-known Canadian\nweather data set, and with applications to neurobiological and environmetric\ndata. The methods are implemented in the publicly available R package\n\\texttt{ctmva}."}, "http://arxiv.org/abs/2401.06864": {"title": "Deep Learning With DAGs", "link": "http://arxiv.org/abs/2401.06864", "description": "Social science theories often postulate causal relationships among a set of\nvariables or events. Although directed acyclic graphs (DAGs) are increasingly\nused to represent these theories, their full potential has not yet been\nrealized in practice. As non-parametric causal models, DAGs require no\nassumptions about the functional form of the hypothesized relationships.\nNevertheless, to simplify the task of empirical evaluation, researchers tend to\ninvoke such assumptions anyway, even though they are typically arbitrary and do\nnot reflect any theoretical content or prior knowledge. Moreover, functional\nform assumptions can engender bias, whenever they fail to accurately capture\nthe complexity of the causal system under investigation. In this article, we\nintroduce causal-graphical normalizing flows (cGNFs), a novel approach to\ncausal inference that leverages deep neural networks to empirically evaluate\ntheories represented as DAGs. Unlike conventional approaches, cGNFs model the\nfull joint distribution of the data according to a DAG supplied by the analyst,\nwithout relying on stringent assumptions about functional form. In this way,\nthe method allows for flexible, semi-parametric estimation of any causal\nestimand that can be identified from the DAG, including total effects,\nconditional effects, direct and indirect effects, and path-specific effects. We\nillustrate the method with a reanalysis of Blau and Duncan's (1967) model of\nstatus attainment and Zhou's (2019) model of conditional versus controlled\nmobility. To facilitate adoption, we provide open-source software together with\na series of online tutorials for implementing cGNFs. The article concludes with\na discussion of current limitations and directions for future development."}, "http://arxiv.org/abs/2401.06904": {"title": "Non-collapsibility and Built-in Selection Bias of Hazard Ratio in Randomized Controlled Trials", "link": "http://arxiv.org/abs/2401.06904", "description": "Background: The hazard ratio of the Cox proportional hazards model is widely\nused in randomized controlled trials to assess treatment effects. However, two\nproperties of the hazard ratio including the non-collapsibility and built-in\nselection bias need to be further investigated. Methods: We conduct simulations\nto differentiate the non-collapsibility effect and built-in selection bias from\nthe difference between the marginal and the conditional hazard ratio.\nMeanwhile, we explore the performance of the Cox model with inverse probability\nof treatment weighting for covariate adjustment when estimating the marginal\nhazard ratio. The built-in selection bias is further assessed in the\nperiod-specific hazard ratio. Results: The conditional hazard ratio is a biased\nestimate of the marginal effect due to the non-collapsibility property. In\ncontrast, the hazard ratio estimated from the inverse probability of treatment\nweighting Cox model provides an unbiased estimate of the true marginal hazard\nratio. The built-in selection bias only manifests in the period-specific hazard\nratios even when the proportional hazards assumption is satisfied. The Cox\nmodel with inverse probability of treatment weighting can be used to account\nfor confounding bias and provide an unbiased effect under the randomized\ncontrolled trials setting when the parameter of interest is the marginal\neffect. Conclusions: We propose that the period-specific hazard ratios should\nalways be avoided due to the profound effects of built-in selection bias."}, "http://arxiv.org/abs/2401.06909": {"title": "Sensitivity Analysis for Matched Observational Studies with Continuous Exposures and Binary Outcomes", "link": "http://arxiv.org/abs/2401.06909", "description": "Matching is one of the most widely used study designs for adjusting for\nmeasured confounders in observational studies. However, unmeasured confounding\nmay exist and cannot be removed by matching. Therefore, a sensitivity analysis\nis typically needed to assess a causal conclusion's sensitivity to unmeasured\nconfounding. Sensitivity analysis frameworks for binary exposures have been\nwell-established for various matching designs and are commonly used in various\nstudies. However, unlike the binary exposure case, there still lacks valid and\ngeneral sensitivity analysis methods for continuous exposures, except in some\nspecial cases such as pair matching. To fill this gap in the binary outcome\ncase, we develop a sensitivity analysis framework for general matching designs\nwith continuous exposures and binary outcomes. First, we use probabilistic\nlattice theory to show our sensitivity analysis approach is\nfinite-population-exact under Fisher's sharp null. Second, we prove a novel\ndesign sensitivity formula as a powerful tool for asymptotically evaluating the\nperformance of our sensitivity analysis approach. Third, to allow effect\nheterogeneity with binary outcomes, we introduce a framework for conducting\nasymptotically exact inference and sensitivity analysis on generalized\nattributable effects with binary outcomes via mixed-integer programming.\nFourth, for the continuous outcomes case, we show that conducting an\nasymptotically exact sensitivity analysis in matched observational studies when\nboth the exposures and outcomes are continuous is generally NP-hard, except in\nsome special cases such as pair matching. As a real data application, we apply\nour new methods to study the effect of early-life lead exposure on juvenile\ndelinquency. We also develop a publicly available R package for implementation\nof the methods in this work."}, "http://arxiv.org/abs/2401.06919": {"title": "Pseudo-Empirical Likelihood Methods for Causal Inference", "link": "http://arxiv.org/abs/2401.06919", "description": "Causal inference problems have remained an important research topic over the\npast several decades due to their general applicability in assessing a\ntreatment effect in many different real-world settings. In this paper, we\npropose two inferential procedures on the average treatment effect (ATE)\nthrough a two-sample pseudo-empirical likelihood (PEL) approach. The first\nprocedure uses the estimated propensity scores for the formulation of the PEL\nfunction, and the resulting maximum PEL estimator of the ATE is equivalent to\nthe inverse probability weighted estimator discussed in the literature. Our\nfocus in this scenario is on the PEL ratio statistic and establishing its\ntheoretical properties. The second procedure incorporates outcome regression\nmodels for PEL inference through model-calibration constraints, and the\nresulting maximum PEL estimator of the ATE is doubly robust. Our main\ntheoretical result in this case is the establishment of the asymptotic\ndistribution of the PEL ratio statistic. We also propose a bootstrap method for\nconstructing PEL ratio confidence intervals for the ATE to bypass the scaling\nconstant which is involved in the asymptotic distribution of the PEL ratio\nstatistic but is very difficult to calculate. Finite sample performances of our\nproposed methods with comparisons to existing ones are investigated through\nsimulation studies."}, "http://arxiv.org/abs/2401.06925": {"title": "Modeling Latent Selection with Structural Causal Models", "link": "http://arxiv.org/abs/2401.06925", "description": "Selection bias is ubiquitous in real-world data, and can lead to misleading\nresults if not dealt with properly. We introduce a conditioning operation on\nStructural Causal Models (SCMs) to model latent selection from a causal\nperspective. We show that the conditioning operation transforms an SCM with the\npresence of an explicit latent selection mechanism into an SCM without such\nselection mechanism, which partially encodes the causal semantics of the\nselected subpopulation according to the original SCM. Furthermore, we show that\nthis conditioning operation preserves the simplicity, acyclicity, and linearity\nof SCMs, and commutes with marginalization. Thanks to these properties,\ncombined with marginalization and intervention, the conditioning operation\noffers a valuable tool for conducting causal reasoning tasks within causal\nmodels where latent details have been abstracted away. We demonstrate by\nexample how classical results of causal inference can be generalized to include\nselection bias and how the conditioning operation helps with modeling of\nreal-world problems."}, "http://arxiv.org/abs/2401.06990": {"title": "Graphical Principal Component Analysis of Multivariate Functional Time Series", "link": "http://arxiv.org/abs/2401.06990", "description": "In this paper, we consider multivariate functional time series with a two-way\ndependence structure: a serial dependence across time points and a graphical\ninteraction among the multiple functions within each time point. We develop the\nnotion of dynamic weak separability, a more general condition than those\nassumed in literature, and use it to characterize the two-way structure in\nmultivariate functional time series. Based on the proposed weak separability,\nwe develop a unified framework for functional graphical models and dynamic\nprincipal component analysis, and further extend it to optimally reconstruct\nsignals from contaminated functional data using graphical-level information. We\ninvestigate asymptotic properties of the resulting estimators and illustrate\nthe effectiveness of our proposed approach through extensive simulations. We\napply our method to hourly air pollution data that were collected from a\nmonitoring network in China."}, "http://arxiv.org/abs/2401.07000": {"title": "Counterfactual Slope and Its Applications to Social Stratification", "link": "http://arxiv.org/abs/2401.07000", "description": "This paper addresses two prominent theses in social stratification research,\nthe great equalizer thesis and Mare's (1980) school transition thesis. Both\ntheses are premised on a descriptive regularity: the association between\nsocioeconomic background and an outcome variable changes when conditioning on\nan intermediate treatment. The interpretation of this descriptive regularity is\ncomplicated by social actors' differential selection into treatment based on\ntheir potential outcomes under treatment. In particular, if the descriptive\nregularity is driven by selection, then the theses do not have a substantive\ninterpretation. We propose a set of novel counterfactual slope estimands, which\ncapture the two theses under the hypothetical scenario where differential\nselection into treatment is eliminated. Thus, we use the counterfactual slopes\nto construct selection-free tests for the two theses. Compared with the\nexisting literature, we are the first to provide explicit, nonparametric, and\ncausal estimands, which enable us to conduct principled selection-free tests.\nWe develop efficient and robust estimators by deriving the efficient influence\nfunctions of the estimands. We apply our framework to a nationally\nrepresentative dataset in the United States and re-evaluate the two theses.\nFindings from our selection-free tests show that the descriptive regularity of\nthe two theses is misleading for substantive interpretations."}, "http://arxiv.org/abs/2401.07018": {"title": "Graphical models for cardinal paired comparisons data", "link": "http://arxiv.org/abs/2401.07018", "description": "Graphical models for cardinal paired comparison data with and without\ncovariates are rigorously analyzed. Novel, graph--based, necessary and\nsufficient conditions which guarantee strong consistency, asymptotic normality\nand the exponential convergence of the estimated ranks are emphasized. A\ncomplete theory for models with covariates is laid out. In particular\nconditions under which covariates can be safely omitted from the model are\nprovided. The methodology is employed in the analysis of both finite and\ninfinite sets of ranked items specifically in the case of large sparse\ncomparison graphs. The proposed methods are explored by simulation and applied\nto the ranking of teams in the National Basketball Association (NBA)."}, "http://arxiv.org/abs/2401.07152": {"title": "Inference for Synthetic Controls via Refined Placebo Tests", "link": "http://arxiv.org/abs/2401.07152", "description": "The synthetic control method is often applied to problems with one treated\nunit and a small number of control units. A common inferential task in this\nsetting is to test null hypotheses regarding the average treatment effect on\nthe treated. Inference procedures that are justified asymptotically are often\nunsatisfactory due to (1) small sample sizes that render large-sample\napproximation fragile and (2) simplification of the estimation procedure that\nis implemented in practice. An alternative is permutation inference, which is\nrelated to a common diagnostic called the placebo test. It has provable Type-I\nerror guarantees in finite samples without simplification of the method, when\nthe treatment is uniformly assigned. Despite this robustness, the placebo test\nsuffers from low resolution since the null distribution is constructed from\nonly $N$ reference estimates, where $N$ is the sample size. This creates a\nbarrier for statistical inference at a common level like $\\alpha = 0.05$,\nespecially when $N$ is small. We propose a novel leave-two-out procedure that\nbypasses this issue, while still maintaining the same finite-sample Type-I\nerror guarantee under uniform assignment for a wide range of $N$. Unlike the\nplacebo test whose Type-I error always equals the theoretical upper bound, our\nprocedure often achieves a lower unconditional Type-I error than theory\nsuggests; this enables useful inference in the challenging regime when $\\alpha\n&lt; 1/N$. Empirically, our procedure achieves a higher power when the effect size\nis reasonably large and a comparable power otherwise. We generalize our\nprocedure to non-uniform assignments and show how to conduct sensitivity\nanalysis. From a methodological perspective, our procedure can be viewed as a\nnew type of randomization inference different from permutation or rank-based\ninference, which is particularly effective in small samples."}, "http://arxiv.org/abs/2401.07221": {"title": "Type I multivariate P\\'olya-Aeppli distributions with applications", "link": "http://arxiv.org/abs/2401.07221", "description": "An extensive body of literature exists that specifically addresses the\nunivariate case of zero-inflated count models. In contrast, research pertaining\nto multivariate models is notably less developed. We proposed two new\nparsimonious multivariate models which can be used to model correlated\nmultivariate overdispersed count data. Furthermore, for different parameter\nsettings and sample sizes, various simulations are performed. In conclusion, we\ndemonstrated the performance of the newly proposed multivariate candidates on\ntwo benchmark datasets, which surpasses that of several alternative approaches."}, "http://arxiv.org/abs/2401.07231": {"title": "Use of Prior Knowledge to Discover Causal Additive Models with Unobserved Variables and its Application to Time Series Data", "link": "http://arxiv.org/abs/2401.07231", "description": "This paper proposes two methods for causal additive models with unobserved\nvariables (CAM-UV). CAM-UV assumes that the causal functions take the form of\ngeneralized additive models and that latent confounders are present. First, we\npropose a method that leverages prior knowledge for efficient causal discovery.\nThen, we propose an extension of this method for inferring causality in time\nseries data. The original CAM-UV algorithm differs from other existing causal\nfunction models in that it does not seek the causal order between observed\nvariables, but rather aims to identify the causes for each observed variable.\nTherefore, the first proposed method in this paper utilizes prior knowledge,\nsuch as understanding that certain variables cannot be causes of specific\nothers. Moreover, by incorporating the prior knowledge that causes precedes\ntheir effects in time, we extend the first algorithm to the second method for\ncausal discovery in time series data. We validate the first proposed method by\nusing simulated data to demonstrate that the accuracy of causal discovery\nincreases as more prior knowledge is accumulated. Additionally, we test the\nsecond proposed method by comparing it with existing time series causal\ndiscovery methods, using both simulated data and real-world data."}, "http://arxiv.org/abs/2401.07259": {"title": "Inference for multivariate extremes via a semi-parametric angular-radial model", "link": "http://arxiv.org/abs/2401.07259", "description": "The modelling of multivariate extreme events is important in a wide variety\nof applications, including flood risk analysis, metocean engineering and\nfinancial modelling. A wide variety of statistical techniques have been\nproposed in the literature; however, many such methods are limited in the forms\nof dependence they can capture, or make strong parametric assumptions about\ndata structures. In this article, we introduce a novel inference framework for\nmultivariate extremes based on a semi-parametric angular-radial model. This\nmodel overcomes the limitations of many existing approaches and provides a\nunified paradigm for assessing joint tail behaviour. Alongside inferential\ntools, we also introduce techniques for assessing uncertainty and goodness of\nfit. Our proposed technique is tested on simulated data sets alongside observed\nmetocean time series', with results indicating generally good performance."}, "http://arxiv.org/abs/2401.07267": {"title": "Inference for high-dimensional linear expectile regression with de-biased method", "link": "http://arxiv.org/abs/2401.07267", "description": "In this paper, we address the inference problem in high-dimensional linear\nexpectile regression. We transform the expectile loss into a\nweighted-least-squares form and apply a de-biased strategy to establish\nWald-type tests for multiple constraints within a regularized framework.\nSimultaneously, we construct an estimator for the pseudo-inverse of the\ngeneralized Hessian matrix in high dimension with general amenable regularizers\nincluding Lasso and SCAD, and demonstrate its consistency through a new proof\ntechnique. We conduct simulation studies and real data applications to\ndemonstrate the efficacy of our proposed test statistic in both homoscedastic\nand heteroscedastic scenarios."}, "http://arxiv.org/abs/2401.07294": {"title": "Multilevel Metamodels: A Novel Approach to Enhance Efficiency and Generalizability in Monte Carlo Simulation Studies", "link": "http://arxiv.org/abs/2401.07294", "description": "Metamodels, or the regression analysis of Monte Carlo simulation (MCS)\nresults, provide a powerful tool to summarize MCS findings. However, an as of\nyet unexplored approach is the use of multilevel metamodels (MLMM) that better\naccount for the dependent data structure of MCS results that arises from\nfitting multiple models to the same simulated data set. In this study, we\narticulate the theoretical rationale for the MLMM and illustrate how it can\ndramatically improve efficiency over the traditional regression approach,\nbetter account for complex MCS designs, and provide new insights into the\ngeneralizability of MCS findings."}, "http://arxiv.org/abs/2401.07344": {"title": "Robust Genomic Prediction and Heritability Estimation using Density Power Divergence", "link": "http://arxiv.org/abs/2401.07344", "description": "This manuscript delves into the intersection of genomics and phenotypic\nprediction, focusing on the statistical innovation required to navigate the\ncomplexities introduced by noisy covariates and confounders. The primary\nemphasis is on the development of advanced robust statistical models tailored\nfor genomic prediction from single nucleotide polymorphism (SNP) data collected\nfrom genome-wide association studies (GWAS) in plant and animal breeding and\nmulti-field trials. The manuscript explores the limitations of traditional\nmarker-assisted recurrent selection, highlighting the significance of\nincorporating all estimated effects of marker loci into the statistical\nframework and aiming to reduce the high dimensionality of GWAS data while\npreserving critical information. This paper introduces a new robust statistical\nframework for genomic prediction, employing one-stage and two-stage linear\nmixed model analyses along with utilizing the popular robust minimum density\npower divergence estimator (MDPDE) to estimate genetic effects on phenotypic\ntraits. The study illustrates the superior performance of the proposed\nMDPDE-based genomic prediction and associated heritability estimation\nprocedures over existing competitors through extensive empirical experiments on\nartificial datasets and application to a real-life maize breeding dataset. The\nresults showcase the robustness and accuracy of the proposed MDPDE-based\napproaches, especially in the presence of data contamination, emphasizing their\npotential applications in improving breeding programs and advancing genomic\nprediction of phenotyping traits."}, "http://arxiv.org/abs/2401.07365": {"title": "Sequential permutation testing by betting", "link": "http://arxiv.org/abs/2401.07365", "description": "We develop an anytime-valid permutation test, where the dataset is fixed and\nthe permutations are sampled sequentially one by one, with the objective of\nsaving computational resources by sampling fewer permutations and stopping\nearly. The core technical advance is the development of new test martingales\n(nonnegative martingales with initial value one) for testing exchangeability\nagainst a very particular alternative. These test martingales are constructed\nusing new and simple betting strategies that smartly bet on the relative ranks\nof permuted test statistics. The betting strategy is guided by the derivation\nof a simple log-optimal betting strategy, and displays excellent power in\npractice. In contrast to a well-known method by Besag and Clifford, our method\nyields a valid e-value or a p-value at any stopping time, and with particular\nstopping rules, it yields computational gains under both the null and the\nalternative without compromising power."}, "http://arxiv.org/abs/2401.07400": {"title": "Gaussian Processes for Time Series with Lead-Lag Effects with applications to biology data", "link": "http://arxiv.org/abs/2401.07400", "description": "Investigating the relationship, particularly the lead-lag effect, between\ntime series is a common question across various disciplines, especially when\nuncovering biological process. However, analyzing time series presents several\nchallenges. Firstly, due to technical reasons, the time points at which\nobservations are made are not at uniform inintervals. Secondly, some lead-lag\neffects are transient, necessitating time-lag estimation based on a limited\nnumber of time points. Thirdly, external factors also impact these time series,\nrequiring a similarity metric to assess the lead-lag relationship. To counter\nthese issues, we introduce a model grounded in the Gaussian process, affording\nthe flexibility to estimate lead-lag effects for irregular time series. In\naddition, our method outputs dissimilarity scores, thereby broadening its\napplications to include tasks such as ranking or clustering multiple pair-wise\ntime series when considering their strength of lead-lag effects with external\nfactors. Crucially, we offer a series of theoretical proofs to substantiate the\nvalidity of our proposed kernels and the identifiability of kernel parameters.\nOur model demonstrates advances in various simulations and real-world\napplications, particularly in the study of dynamic chromatin interactions,\ncompared to other leading methods."}, "http://arxiv.org/abs/2401.07401": {"title": "Design-Based Estimation and Central Limit Theorems for Local Average Treatment Effects for RCTs", "link": "http://arxiv.org/abs/2401.07401", "description": "There is a growing literature on design-based methods to estimate average\ntreatment effects for randomized controlled trials (RCTs) using the\nunderpinnings of experiments. In this article, we build on these methods to\nconsider design-based regression estimators for the local average treatment\neffect (LATE) estimand for RCTs with treatment noncompliance. We prove new\nfinite-population central limit theorems for a range of designs, including\nblocked and clustered RCTs, allowing for baseline covariates to improve\nprecision. We discuss consistent variance estimators based on model residuals\nand conduct simulations that show the estimators yield confidence interval\ncoverage near nominal levels. We demonstrate the methods using data from a\nprivate school voucher RCT in New York City USA."}, "http://arxiv.org/abs/2401.07421": {"title": "A Bayesian Approach to Modeling Variance of Intensive Longitudinal Biomarker Data as a Predictor of Health Outcomes", "link": "http://arxiv.org/abs/2401.07421", "description": "Intensive longitudinal biomarker data are increasingly common in scientific\nstudies that seek temporally granular understanding of the role of behavioral\nand physiological factors in relation to outcomes of interest. Intensive\nlongitudinal biomarker data, such as those obtained from wearable devices, are\noften obtained at a high frequency typically resulting in several hundred to\nthousand observations per individual measured over minutes, hours, or days.\nOften in longitudinal studies, the primary focus is on relating the means of\nbiomarker trajectories to an outcome, and the variances are treated as nuisance\nparameters, although they may also be informative for the outcomes. In this\npaper, we propose a Bayesian hierarchical model to jointly model a\ncross-sectional outcome and the intensive longitudinal biomarkers. To model the\nvariability of biomarkers and deal with the high intensity of data, we develop\nsubject-level cubic B-splines and allow the sharing of information across\nindividuals for both the residual variability and the random effects\nvariability. Then different levels of variability are extracted and\nincorporated into an outcome submodel for inferential and predictive purposes.\nWe demonstrate the utility of the proposed model via an application involving\nbio-monitoring of hertz-level heart rate information from a study on social\nstress."}, "http://arxiv.org/abs/2401.07445": {"title": "GACE: Learning Graph-Based Cross-Page Ads Embedding For Click-Through Rate Prediction", "link": "http://arxiv.org/abs/2401.07445", "description": "Predicting click-through rate (CTR) is the core task of many ads online\nrecommendation systems, which helps improve user experience and increase\nplatform revenue. In this type of recommendation system, we often encounter two\nmain problems: the joint usage of multi-page historical advertising data and\nthe cold start of new ads. In this paper, we proposed GACE, a graph-based\ncross-page ads embedding generation method. It can warm up and generate the\nrepresentation embedding of cold-start and existing ads across various pages.\nSpecifically, we carefully build linkages and a weighted undirected graph model\nconsidering semantic and page-type attributes to guide the direction of feature\nfusion and generation. We designed a variational auto-encoding task as\npre-training module and generated embedding representations for new and old ads\nbased on this task. The results evaluated in the public dataset AliEC from\nRecBole and the real-world industry dataset from Alipay show that our GACE\nmethod is significantly superior to the SOTA method. In the online A/B test,\nthe click-through rate on three real-world pages from Alipay has increased by\n3.6%, 2.13%, and 3.02%, respectively. Especially in the cold-start task, the\nCTR increased by 9.96%, 7.51%, and 8.97%, respectively."}, "http://arxiv.org/abs/2401.07522": {"title": "Balancing the edge effect and dimension of spectral spatial statistics under irregular sampling with applications to isotropy testing", "link": "http://arxiv.org/abs/2401.07522", "description": "We investigate distributional properties of a class of spectral spatial\nstatistics under irregular sampling of a random field that is defined on\n$\\mathbb{R}^d$, and use this to obtain a test for isotropy. Within this\ncontext, edge effects are well-known to create a bias in classical estimators\ncommonly encountered in the analysis of spatial data. This bias increases with\ndimension $d$ and, for $d&gt;1$, can become non-negligible in the limiting\ndistribution of such statistics to the extent that a nondegenerate distribution\ndoes not exist. We provide a general theory for a class of (integrated)\nspectral statistics that enables to 1) significantly reduce this bias and 2)\nthat ensures that asymptotically Gaussian limits can be derived for $d \\le 3$\nfor appropriately tapered versions of such statistics. We use this to address\nsome crucial gaps in the literature, and demonstrate that tapering with a\nsufficiently smooth function is necessary to achieve such results. Our findings\nspecifically shed a new light on a recent result in Subba Rao (2018a). Our\ntheory then is used to propose a novel test for isotropy. In contrast to most\nof the literature, which validates this assumption on a finite number of\nspatial locations (or a finite number of Fourier frequencies), we develop a\ntest for isotropy on the full spatial domain by means of its characterization\nin the frequency domain. More precisely, we derive an explicit expression for\nthe minimum $L^2$-distance between the spectral density of the random field and\nits best approximation by a spectral density of an isotropic process. We prove\nasymptotic normality of an estimator of this quantity in the mixed increasing\ndomain framework and use this result to derive an asymptotic level\n$\\alpha$-test."}, "http://arxiv.org/abs/2401.07562": {"title": "Probabilistic Richardson Extrapolation", "link": "http://arxiv.org/abs/2401.07562", "description": "For over a century, extrapolation methods have provided a powerful tool to\nimprove the convergence order of a numerical method. However, these tools are\nnot well-suited to modern computer codes, where multiple continua are\ndiscretised and convergence orders are not easily analysed. To address this\nchallenge we present a probabilistic perspective on Richardson extrapolation, a\npoint of view that unifies classical extrapolation methods with modern\nmulti-fidelity modelling, and handles uncertain convergence orders by allowing\nthese to be statistically estimated. The approach is developed using Gaussian\nprocesses, leading to Gauss-Richardson Extrapolation (GRE). Conditions are\nestablished under which extrapolation using the conditional mean achieves a\npolynomial (or even an exponential) speed-up compared to the original numerical\nmethod. Further, the probabilistic formulation unlocks the possibility of\nexperimental design, casting the selection of fidelities as a continuous\noptimisation problem which can then be (approximately) solved. A case-study\ninvolving a computational cardiac model demonstrates that practical gains in\naccuracy can be achieved using the GRE method."}, "http://arxiv.org/abs/2401.07625": {"title": "Statistics in Survey Sampling", "link": "http://arxiv.org/abs/2401.07625", "description": "Survey sampling theory and methods are introduced. Sampling designs and\nestimation methods are carefully discussed as a textbook for survey sampling.\nTopics includes Horvitz-Thompson estimation, simple random sampling, stratified\nsampling, cluster sampling, ratio estimation, regression estimation, variance\nestimation, two-phase sampling, and nonresponse adjustment methods."}, "http://arxiv.org/abs/2401.07724": {"title": "A non-parametric estimator for Archimedean copulas under flexible censoring scenarios and an application to claims reserving", "link": "http://arxiv.org/abs/2401.07724", "description": "With insurers benefiting from ever-larger amounts of data of increasing\ncomplexity, we explore a data-driven method to model dependence within\nmultilevel claims in this paper. More specifically, we start from a\nnon-parametric estimator for Archimedean copula generators introduced by Genest\nand Rivest (1993), and we extend it to diverse flexible censoring scenarios\nusing techniques derived from survival analysis. We implement a graphical\nselection procedure for copulas that we validate using goodness-of-fit methods\napplied to complete, single-censored, and double-censored bivariate data. We\nillustrate the performance of our model with multiple simulation studies. We\nthen apply our methodology to a recent Canadian automobile insurance dataset\nwhere we seek to model the dependence between the activation delays of\ncorrelated coverages. We show that our model performs quite well in selecting\nthe best-fitted copula for the data at hand, especially when the dataset is\nlarge, and that the results can then be used as part of a larger claims\nreserving methodology."}, "http://arxiv.org/abs/2401.07767": {"title": "Estimation of the genetic Gaussian network using GWAS summary data", "link": "http://arxiv.org/abs/2401.07767", "description": "Genetic Gaussian network of multiple phenotypes constructed through the\ngenetic correlation matrix is informative for understanding their biological\ndependencies. However, its interpretation may be challenging because the\nestimated genetic correlations are biased due to estimation errors and\nhorizontal pleiotropy inherent in GWAS summary statistics. Here we introduce a\nnovel approach called Estimation of Genetic Graph (EGG), which eliminates the\nestimation error bias and horizontal pleiotropy bias with the same techniques\nused in multivariable Mendelian randomization. The genetic network estimated by\nEGG can be interpreted as representing shared common biological contributions\nbetween phenotypes, conditional on others, and even as indicating the causal\ncontributions. We use both simulations and real data to demonstrate the\nsuperior efficacy of our novel method in comparison with the traditional\nnetwork estimators. R package EGG is available on\nhttps://github.com/harryyiheyang/EGG."}, "http://arxiv.org/abs/2401.07820": {"title": "Posterior shrinkage towards linear subspaces", "link": "http://arxiv.org/abs/2401.07820", "description": "It is common to hold prior beliefs that are not characterized by points in\nthe parameter space but instead are relational in nature and can be described\nby a linear subspace. While some previous work has been done to account for\nsuch prior beliefs, the focus has primarily been on point estimators within a\nregression framework. We argue, however, that prior beliefs about parameters\nought to be encoded into the prior distribution rather than in the formation of\na point estimator. In this way, the prior beliefs help shape \\textit{all}\ninference. Through exponential tilting, we propose a fully generalizable method\nof taking existing prior information from, e.g., a pilot study, and combining\nit with additional prior beliefs represented by parameters lying on a linear\nsubspace. We provide computationally efficient algorithms for posterior\ninference that, once inference is made using a non-tilted prior, does not\ndepend on the sample size. We illustrate our proposed approach on an\nantihypertensive clinical trial dataset where we shrink towards a power law\ndose-response relationship, and on monthly influenza and pneumonia data where\nwe shrink moving average lag parameters towards smoothness. Software to\nimplement the proposed approach is provided in the R package \\verb+SUBSET+\navailable on GitHub."}, "http://arxiv.org/abs/2401.08159": {"title": "Reluctant Interaction Modeling in Generalized Linear Models", "link": "http://arxiv.org/abs/2401.08159", "description": "While including pairwise interactions in a regression model can better\napproximate response surface, fitting such an interaction model is a well-known\ndifficult problem. In particular, analyzing contemporary high-dimensional\ndatasets often leads to extremely large-scale interaction modeling problem,\nwhere the challenge is posed to identify important interactions among millions\nor even billions of candidate interactions. While several methods have recently\nbeen proposed to tackle this challenge, they are mostly designed by (1)\nassuming the hierarchy assumption among the important interactions and (or) (2)\nfocusing on the case in linear models with interactions and (sub)Gaussian\nerrors. In practice, however, neither of these two building blocks has to hold.\nIn this paper, we propose an interaction modeling framework in generalized\nlinear models (GLMs) which is free of any assumptions on hierarchy. We develop\na non-trivial extension of the reluctance interaction selection principle to\nthe GLMs setting, where a main effect is preferred over an interaction if all\nelse is equal. Our proposed method is easy to implement, and is highly scalable\nto large-scale datasets. Theoretically, we demonstrate that it possesses\nscreening consistency under high-dimensional setting. Numerical studies on\nsimulated datasets and a real dataset show that the proposed method does not\nsacrifice statistical performance in the presence of significant computational\ngain."}, "http://arxiv.org/abs/2401.08172": {"title": "On GEE for Mean-Variance-Correlation Models: Variance Estimation and Model Selection", "link": "http://arxiv.org/abs/2401.08172", "description": "Generalized estimating equations (GEE) are of great importance in analyzing\nclustered data without full specification of multivariate distributions. A\nrecent approach jointly models the mean, variance, and correlation coefficients\nof clustered data through three sets of regressions (Luo and Pan, 2022). We\nobserve that these estimating equations, however, are a special case of those\nof Yan and Fine (2004) which further allows the variance to depend on the mean\nthrough a variance function. The proposed variance estimators may be incorrect\nfor the variance and correlation parameters because of a subtle dependence\ninduced by the nested structure of the estimating equations. We characterize\nmodel settings where their variance estimation is invalid and show the variance\nestimators in Yan and Fine (2004) correctly account for such dependence. In\naddition, we introduce a novel model selection criterion that enables the\nsimultaneous selection of the mean-scale-correlation model. The sandwich\nvariance estimator and the proposed model selection criterion are tested by\nseveral simulation studies and real data analysis, which validate its\neffectiveness in variance estimation and model selection. Our work also extends\nthe R package geepack with the flexibility to apply different working\ncovariance matrices for the variance and correlation structures."}, "http://arxiv.org/abs/2401.08173": {"title": "Simultaneous Change Point Detection and Identification for High Dimensional Linear Models", "link": "http://arxiv.org/abs/2401.08173", "description": "In this article, we consider change point inference for high dimensional\nlinear models. For change point detection, given any subgroup of variables, we\npropose a new method for testing the homogeneity of corresponding regression\ncoefficients across the observations. Under some regularity conditions, the\nproposed new testing procedure controls the type I error asymptotically and is\npowerful against sparse alternatives and enjoys certain optimality. For change\npoint identification, an argmax based change point estimator is proposed which\nis shown to be consistent for the true change point location. Moreover,\ncombining with the binary segmentation technique, we further extend our new\nmethod for detecting and identifying multiple change points. Extensive\nnumerical studies justify the validity of our new method and an application to\nthe Alzheimer's disease data analysis further demonstrate its competitive\nperformance."}, "http://arxiv.org/abs/2401.08175": {"title": "Bayesian Kriging Approaches for Spatial Functional Data", "link": "http://arxiv.org/abs/2401.08175", "description": "Functional kriging approaches have been developed to predict the curves at\nunobserved spatial locations. However, most existing approaches are based on\nvariogram fittings rather than constructing hierarchical statistical models.\nTherefore, it is challenging to analyze the relationships between functional\nvariables, and uncertainty quantification of the model is not trivial. In this\nmanuscript, we propose a Bayesian framework for spatial function-on-function\nregression. However, inference for the proposed model has computational and\ninferential challenges because the model needs to account for within and\nbetween-curve dependencies. Furthermore, high-dimensional and spatially\ncorrelated parameters can lead to the slow mixing of Markov chain Monte Carlo\nalgorithms. To address these issues, we first utilize a basis transformation\napproach to simplify the covariance and apply projection methods for dimension\nreduction. We also develop a simultaneous band score for the proposed model to\ndetect the significant region in the regression function. We apply the methods\nto simulated and real datasets, including data on particulate matter in Japan\nand mobility data in South Korea. The proposed method is computationally\nefficient and provides accurate estimations and predictions."}, "http://arxiv.org/abs/2401.08224": {"title": "Differentially Private Estimation of CATE in Adaptive Experiment", "link": "http://arxiv.org/abs/2401.08224", "description": "Adaptive experiment is widely adopted to estimate conditional average\ntreatment effect (CATE) in clinical trials and many other scenarios. While the\nprimary goal in experiment is to maximize estimation accuracy, due to the\nimperative of social welfare, it's also crucial to provide treatment with\nsuperior outcomes to patients, which is measured by regret in contextual bandit\nframework. These two objectives often lead to contrast optimal allocation\nmechanism. Furthermore, privacy concerns arise in clinical scenarios containing\nsensitive data like patients health records. Therefore, it's essential for the\ntreatment allocation mechanism to incorporate robust privacy protection\nmeasures. In this paper, we investigate the tradeoff between loss of social\nwelfare and statistical power in contextual bandit experiment. We propose a\nmatched upper and lower bound for the multi-objective optimization problem, and\nthen adopt the concept of Pareto optimality to mathematically characterize the\noptimality condition. Furthermore, we propose differentially private algorithms\nwhich still matches the lower bound, showing that privacy is \"almost free\".\nAdditionally, we derive the asymptotic normality of the estimator, which is\nessential in statistical inference and hypothesis testing."}, "http://arxiv.org/abs/2401.08303": {"title": "A Bayesian multivariate model with temporal dependence on random partition of areal data", "link": "http://arxiv.org/abs/2401.08303", "description": "More than half of the world's population is exposed to the risk of\nmosquito-borne diseases, which leads to millions of cases and hundreds of\nthousands of deaths every year. Analyzing this type of data is often complex\nand poses several interesting challenges, mainly due to the vast geographic\narea, the peculiar temporal behavior, and the potential correlation between\ninfections. Motivation stems from the analysis of tropical diseases data,\nnamely, the number of cases of two arboviruses, dengue and chikungunya,\ntransmitted by the same mosquito, for all the 145 microregions in Southeast\nBrazil from 2018 to 2022. As a contribution to the literature on multivariate\ndisease data, we develop a flexible Bayesian multivariate spatio-temporal model\nwhere temporal dependence is defined for areal clusters. The model features a\nprior distribution for the random partition of areal data that incorporates\nneighboring information, thus encouraging maps with few contiguous clusters and\ndiscouraging clusters with disconnected areas. The model also incorporates an\nautoregressive structure and terms related to seasonal patterns into temporal\ncomponents that are disease and cluster-specific. It also considers a\nmultivariate directed acyclic graph autoregressive structure to accommodate\nspatial and inter-disease dependence, facilitating the interpretation of\nspatial correlation. We explore properties of the model by way of simulation\nstudies and show results that prove our proposal compares well to competing\nalternatives. Finally, we apply the model to the motivating dataset with a\ntwofold goal: clustering areas where the temporal trend of certain diseases are\nsimilar, and exploring the potential existence of temporal and/or spatial\ncorrelation between two diseases transmitted by the same mosquito."}, "http://arxiv.org/abs/2006.13850": {"title": "Global Sensitivity and Domain-Selective Testing for Functional-Valued Responses: An Application to Climate Economy Models", "link": "http://arxiv.org/abs/2006.13850", "description": "Understanding the dynamics and evolution of climate change and associated\nuncertainties is key for designing robust policy actions. Computer models are\nkey tools in this scientific effort, which have now reached a high level of\nsophistication and complexity. Model auditing is needed in order to better\nunderstand their results, and to deal with the fact that such models are\nincreasingly opaque with respect to their inner workings. Current techniques\nsuch as Global Sensitivity Analysis (GSA) are limited to dealing either with\nmultivariate outputs, stochastic ones, or finite-change inputs. This limits\ntheir applicability to time-varying variables such as future pathways of\ngreenhouse gases. To provide additional semantics in the analysis of a model\nensemble, we provide an extension of GSA methodologies tackling the case of\nstochastic functional outputs with finite change inputs. To deal with finite\nchange inputs and functional outputs, we propose an extension of currently\navailable GSA methodologies while we deal with the stochastic part by\nintroducing a novel, domain-selective inferential technique for sensitivity\nindices. Our method is explored via a simulation study that shows its\nrobustness and efficacy in detecting sensitivity patterns. We apply it to real\nworld data, where its capabilities can provide to practitioners and\npolicymakers additional information about the time dynamics of sensitivity\npatterns, as well as information about robustness."}, "http://arxiv.org/abs/2107.00527": {"title": "Distribution-Free Prediction Bands for Multivariate Functional Time Series: an Application to the Italian Gas Market", "link": "http://arxiv.org/abs/2107.00527", "description": "Uncertainty quantification in forecasting represents a topic of great\nimportance in energy trading, as understanding the status of the energy market\nwould enable traders to directly evaluate the impact of their own offers/bids.\nTo this end, we propose a scalable procedure that outputs closed-form\nsimultaneous prediction bands for multivariate functional response variables in\na time series setting, which is able to guarantee performance bounds in terms\nof unconditional coverage and asymptotic exactness, both under some conditions.\nAfter evaluating its performance on synthetic data, the method is used to build\nmultivariate prediction bands for daily demand and offer curves in the Italian\ngas market."}, "http://arxiv.org/abs/2110.13017": {"title": "Nested $\\hat R$: Assessing the convergence of Markov chain Monte Carlo when running many short chains", "link": "http://arxiv.org/abs/2110.13017", "description": "Recent developments in Markov chain Monte Carlo (MCMC) algorithms allow us to\nrun thousands of chains in parallel almost as quickly as a single chain, using\nhardware accelerators such as GPUs. While each chain still needs to forget its\ninitial point during a warmup phase, the subsequent sampling phase can be\nshorter than in classical settings, where we run only a few chains. To\ndetermine if the resulting short chains are reliable, we need to assess how\nclose the Markov chains are to their stationary distribution after warmup. The\npotential scale reduction factor $\\widehat R$ is a popular convergence\ndiagnostic but unfortunately can require a long sampling phase to work well. We\npresent a nested design to overcome this challenge and a generalization called\nnested $\\widehat R$. This new diagnostic works under conditions similar to\n$\\widehat R$ and completes the workflow for GPU-friendly samplers. In addition,\nthe proposed nesting provides theoretical insights into the utility of\n$\\widehat R$, in both classical and short-chains regimes."}, "http://arxiv.org/abs/2111.10718": {"title": "The R2D2 Prior for Generalized Linear Mixed Models", "link": "http://arxiv.org/abs/2111.10718", "description": "In Bayesian analysis, the selection of a prior distribution is typically done\nby considering each parameter in the model. While this can be convenient, in\nmany scenarios it may be desirable to place a prior on a summary measure of the\nmodel instead. In this work, we propose a prior on the model fit, as measured\nby a Bayesian coefficient of determination ($R^2)$, which then induces a prior\non the individual parameters. We achieve this by placing a beta prior on $R^2$\nand then deriving the induced prior on the global variance parameter for\ngeneralized linear mixed models. We derive closed-form expressions in many\nscenarios and present several approximation strategies when an analytic form is\nnot possible and/or to allow for easier computation. In these situations, we\nsuggest approximating the prior by using a generalized beta prime distribution\nand provide a simple default prior construction scheme. This approach is quite\nflexible and can be easily implemented in standard Bayesian software. Lastly,\nwe demonstrate the performance of the method on simulated and real-world data,\nwhere the method particularly shines in high-dimensional settings, as well as\nmodeling random effects."}, "http://arxiv.org/abs/2206.05337": {"title": "Integrating complex selection rules into the latent overlapping group Lasso for constructing coherent prediction models", "link": "http://arxiv.org/abs/2206.05337", "description": "The construction of coherent prediction models holds great importance in\nmedical research as such models enable health researchers to gain deeper\ninsights into disease epidemiology and clinicians to identify patients at\nhigher risk of adverse outcomes. One commonly employed approach to developing\nprediction models is variable selection through penalized regression\ntechniques. Integrating natural variable structures into this process not only\nenhances model interpretability but can also %increase the likelihood of\nrecovering the true underlying model and boost prediction accuracy. However, a\nchallenge lies in determining how to effectively integrate potentially complex\nselection dependencies into the penalized regression. In this work, we\ndemonstrate how to represent selection dependencies mathematically, provide\nalgorithms for deriving the complete set of potential models, and offer a\nstructured approach for integrating complex rules into variable selection\nthrough the latent overlapping group Lasso. To illustrate our methodology, we\napplied these techniques to construct a coherent prediction model for major\nbleeding in hypertensive patients recently hospitalized for atrial fibrillation\nand subsequently prescribed oral anticoagulants. In this application, we\naccount for a proxy of anticoagulant adherence and its interaction with dosage\nand the type of oral anticoagulants in addition to drug-drug interactions."}, "http://arxiv.org/abs/2206.08756": {"title": "Tensor-on-Tensor Regression: Riemannian Optimization, Over-parameterization, Statistical-computational Gap, and Their Interplay", "link": "http://arxiv.org/abs/2206.08756", "description": "We study the tensor-on-tensor regression, where the goal is to connect tensor\nresponses to tensor covariates with a low Tucker rank parameter tensor/matrix\nwithout the prior knowledge of its intrinsic rank. We propose the Riemannian\ngradient descent (RGD) and Riemannian Gauss-Newton (RGN) methods and cope with\nthe challenge of unknown rank by studying the effect of rank\nover-parameterization. We provide the first convergence guarantee for the\ngeneral tensor-on-tensor regression by showing that RGD and RGN respectively\nconverge linearly and quadratically to a statistically optimal estimate in both\nrank correctly-parameterized and over-parameterized settings. Our theory\nreveals an intriguing phenomenon: Riemannian optimization methods naturally\nadapt to over-parameterization without modifications to their implementation.\nWe also prove the statistical-computational gap in scalar-on-tensor regression\nby a direct low-degree polynomial argument. Our theory demonstrates a \"blessing\nof statistical-computational gap\" phenomenon: in a wide range of scenarios in\ntensor-on-tensor regression for tensors of order three or higher, the\ncomputationally required sample size matches what is needed by moderate rank\nover-parameterization when considering computationally feasible estimators,\nwhile there are no such benefits in the matrix settings. This shows moderate\nrank over-parameterization is essentially \"cost-free\" in terms of sample size\nin tensor-on-tensor regression of order three or higher. Finally, we conduct\nsimulation studies to show the advantages of our proposed methods and to\ncorroborate our theoretical findings."}, "http://arxiv.org/abs/2209.01396": {"title": "Small Study Regression Discontinuity Designs: Density Inclusive Study Size Metric and Performance", "link": "http://arxiv.org/abs/2209.01396", "description": "Regression discontinuity (RD) designs are popular quasi-experimental studies\nin which treatment assignment depends on whether the value of a running\nvariable exceeds a cutoff. RD designs are increasingly popular in educational\napplications due to the prevalence of cutoff-based interventions. In such\napplications sample sizes can be relatively small or there may be sparsity\naround the cutoff. We propose a metric, density inclusive study size (DISS),\nthat characterizes the size of an RD study better than overall sample size by\nincorporating the density of the running variable. We show the usefulness of\nthis metric in a Monte Carlo simulation study that compares the operating\ncharacteristics of popular nonparametric RD estimation methods in small\nstudies. We also apply the DISS metric and RD estimation methods to school\naccountability data from the state of Indiana."}, "http://arxiv.org/abs/2212.06108": {"title": "Tandem clustering with invariant coordinate selection", "link": "http://arxiv.org/abs/2212.06108", "description": "For multivariate data, tandem clustering is a well-known technique aiming to\nimprove cluster identification through initial dimension reduction.\nNevertheless, the usual approach using principal component analysis (PCA) has\nbeen criticized for focusing solely on inertia so that the first components do\nnot necessarily retain the structure of interest for clustering. To address\nthis limitation, a new tandem clustering approach based on invariant coordinate\nselection (ICS) is proposed. By jointly diagonalizing two scatter matrices, ICS\nis designed to find structure in the data while providing affine invariant\ncomponents. Certain theoretical results have been previously derived and\nguarantee that under some elliptical mixture models, the group structure can be\nhighlighted on a subset of the first and/or last components. However, ICS has\ngarnered minimal attention within the context of clustering. Two challenges\nassociated with ICS include choosing the pair of scatter matrices and selecting\nthe components to retain. For effective clustering purposes, it is demonstrated\nthat the best scatter pairs consist of one scatter matrix capturing the\nwithin-cluster structure and another capturing the global structure. For the\nformer, local shape or pairwise scatters are of great interest, as is the\nminimum covariance determinant (MCD) estimator based on a carefully chosen\nsubset size that is smaller than usual. The performance of ICS as a dimension\nreduction method is evaluated in terms of preserving the cluster structure in\nthe data. In an extensive simulation study and empirical applications with\nbenchmark data sets, various combinations of scatter matrices as well as\ncomponent selection criteria are compared in situations with and without\noutliers. Overall, the new approach of tandem clustering with ICS shows\npromising results and clearly outperforms the PCA-based approach."}, "http://arxiv.org/abs/2212.07687": {"title": "Networks of reinforced stochastic processes: probability of asymptotic polarization and related general results", "link": "http://arxiv.org/abs/2212.07687", "description": "In a network of reinforced stochastic processes, for certain values of the\nparameters, all the agents' inclinations synchronize and converge almost surely\ntoward a certain random variable. The present work aims at clarifying when the\nagents can asymptotically polarize, i.e. when the common limit inclination can\ntake the extreme values, 0 or 1, with probability zero, strictly positive, or\nequal to one. Moreover, we present a suitable technique to estimate this\nprobability that, along with the theoretical results, has been framed in the\nmore general setting of a class of martingales taking values in [0, 1] and\nfollowing a specific dynamics."}, "http://arxiv.org/abs/2303.14226": {"title": "Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions", "link": "http://arxiv.org/abs/2303.14226", "description": "Consider a setting where there are $N$ heterogeneous units and $p$\ninterventions. Our goal is to learn unit-specific potential outcomes for any\ncombination of these $p$ interventions, i.e., $N \\times 2^p$ causal parameters.\nChoosing a combination of interventions is a problem that naturally arises in a\nvariety of applications such as factorial design experiments, recommendation\nengines, combination therapies in medicine, conjoint analysis, etc. Running $N\n\\times 2^p$ experiments to estimate the various parameters is likely expensive\nand/or infeasible as $N$ and $p$ grow. Further, with observational data there\nis likely confounding, i.e., whether or not a unit is seen under a combination\nis correlated with its potential outcome under that combination. To address\nthese challenges, we propose a novel latent factor model that imposes structure\nacross units (i.e., the matrix of potential outcomes is approximately rank\n$r$), and combinations of interventions (i.e., the coefficients in the Fourier\nexpansion of the potential outcomes is approximately $s$ sparse). We establish\nidentification for all $N \\times 2^p$ parameters despite unobserved\nconfounding. We propose an estimation procedure, Synthetic Combinations, and\nestablish it is finite-sample consistent and asymptotically normal under\nprecise conditions on the observation pattern. Our results imply consistent\nestimation given $\\text{poly}(r) \\times \\left( N + s^2p\\right)$ observations,\nwhile previous methods have sample complexity scaling as $\\min(N \\times s^2p, \\\n\\ \\text{poly(r)} \\times (N + 2^p))$. We use Synthetic Combinations to propose a\ndata-efficient experimental design. Empirically, Synthetic Combinations\noutperforms competing approaches on a real-world dataset on movie\nrecommendations. Lastly, we extend our analysis to do causal inference where\nthe intervention is a permutation over $p$ items (e.g., rankings)."}, "http://arxiv.org/abs/2303.17478": {"title": "A Bayesian Dirichlet Auto-Regressive Moving Average Model for Forecasting Lead Times", "link": "http://arxiv.org/abs/2303.17478", "description": "Lead time data is compositional data found frequently in the hospitality\nindustry. Hospitality businesses earn fees each day, however these fees cannot\nbe recognized until later. For business purposes, it is important to understand\nand forecast the distribution of future fees for the allocation of resources,\nfor business planning, and for staffing. Motivated by 5 years of daily fees\ndata, we propose a new class of Bayesian time series models, a Bayesian\nDirichlet Auto-Regressive Moving Average (B-DARMA) model for compositional time\nseries, modeling the proportion of future fees that will be recognized in 11\nconsecutive 30 day windows and 1 last consecutive 35 day window. Each day's\ncompositional datum is modeled as Dirichlet distributed given the mean and a\nscale parameter. The mean is modeled with a Vector Autoregressive Moving\nAverage process after transforming with an additive log ratio link function and\ndepends on previous compositional data, previous compositional parameters and\ndaily covariates. The B-DARMA model offers solutions to data analyses of large\ncompositional vectors and short or long time series, offers efficiency gains\nthrough choice of priors, provides interpretable parameters for inference, and\nmakes reasonable forecasts."}, "http://arxiv.org/abs/2306.17361": {"title": "iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models", "link": "http://arxiv.org/abs/2306.17361", "description": "Structural causal models (SCMs) are widely used in various disciplines to\nrepresent causal relationships among variables in complex systems.\nUnfortunately, the underlying causal structure is often unknown, and estimating\nit from data remains a challenging task. In many situations, however, the end\ngoal is to localize the changes (shifts) in the causal mechanisms between\nrelated datasets instead of learning the full causal structure of the\nindividual datasets. Some applications include root cause analysis, analyzing\ngene regulatory network structure changes between healthy and cancerous\nindividuals, or explaining distribution shifts. This paper focuses on\nidentifying the causal mechanism shifts in two or more related datasets over\nthe same set of variables -- without estimating the entire DAG structure of\neach SCM. Prior work under this setting assumed linear models with Gaussian\nnoises; instead, in this work we assume that each SCM belongs to the more\ngeneral class of nonlinear additive noise models (ANMs). A key technical\ncontribution of this work is to show that the Jacobian of the score function\nfor the mixture distribution allows for the identification of shifts under\ngeneral non-parametric functional mechanisms. Once the shifted variables are\nidentified, we leverage recent work to estimate the structural differences, if\nany, for the shifted variables. Experiments on synthetic and real-world data\nare provided to showcase the applicability of this approach. Code implementing\nthe proposed method is open-source and publicly available at\nhttps://github.com/kevinsbello/iSCAN."}, "http://arxiv.org/abs/2307.10841": {"title": "A criterion and incremental design construction for simultaneous kriging predictions", "link": "http://arxiv.org/abs/2307.10841", "description": "In this paper, we further investigate the problem of selecting a set of\ndesign points for universal kriging, which is a widely used technique for\nspatial data analysis. Our goal is to select the design points in order to make\nsimultaneous predictions of the random variable of interest at a finite number\nof unsampled locations with maximum precision. Specifically, we consider as\nresponse a correlated random field given by a linear model with an unknown\nparameter vector and a spatial error correlation structure. We propose a new\ndesign criterion that aims at simultaneously minimizing the variation of the\nprediction errors at various points. We also present various efficient\ntechniques for incrementally building designs for that criterion scaling well\nfor high dimensions. Thus the method is particularly suitable for big data\napplications in areas of spatial data analysis such as mining, hydrogeology,\nnatural resource monitoring, and environmental sciences or equivalently for any\ncomputer simulation experiments. We have demonstrated the effectiveness of the\nproposed designs through two illustrative examples: one by simulation and\nanother based on real data from Upper Austria."}, "http://arxiv.org/abs/2307.15404": {"title": "Information-based Preprocessing of PLC Data for Automatic Behavior Modeling", "link": "http://arxiv.org/abs/2307.15404", "description": "Cyber-physical systems (CPS) offer immense optimization potential for\nmanufacturing processes through the availability of multivariate time series\ndata of actors and sensors. Based on automated analysis software, the\ndeployment of adaptive and responsive measures is possible for time series\ndata. Due to the complex and dynamic nature of modern manufacturing, analysis\nand modeling often cannot be entirely automated. Even machine- or deep learning\napproaches often depend on a priori expert knowledge and labelling. In this\npaper, an information-based data preprocessing approach is proposed. By\napplying statistical methods including variance and correlation analysis, an\napproximation of the sampling rate in event-based systems and the utilization\nof spectral analysis, knowledge about the underlying manufacturing processes\ncan be gained prior to modeling. The paper presents, how statistical analysis\nenables the pruning of a dataset's least important features and how the\nsampling rate approximation approach sets the base for further data analysis\nand modeling. The data's underlying periodicity, originating from the cyclic\nnature of an automated manufacturing process, will be detected by utilizing the\nfast Fourier transform. This information-based preprocessing method will then\nbe validated for process time series data of cyber-physical systems'\nprogrammable logic controllers (PLC)."}, "http://arxiv.org/abs/2308.05205": {"title": "Dynamic survival analysis: modelling the hazard function via ordinary differential equations", "link": "http://arxiv.org/abs/2308.05205", "description": "The hazard function represents one of the main quantities of interest in the\nanalysis of survival data. We propose a general approach for parametrically\nmodelling the dynamics of the hazard function using systems of autonomous\nordinary differential equations (ODEs). This modelling approach can be used to\nprovide qualitative and quantitative analyses of the evolution of the hazard\nfunction over time. Our proposal capitalises on the extensive literature of\nODEs which, in particular, allow for establishing basic rules or laws on the\ndynamics of the hazard function via the use of autonomous ODEs. We show how to\nimplement the proposed modelling framework in cases where there is an analytic\nsolution to the system of ODEs or where an ODE solver is required to obtain a\nnumerical solution. We focus on the use of a Bayesian modelling approach, but\nthe proposed methodology can also be coupled with maximum likelihood\nestimation. A simulation study is presented to illustrate the performance of\nthese models and the interplay of sample size and censoring. Two case studies\nusing real data are presented to illustrate the use of the proposed approach\nand to highlight the interpretability of the corresponding models. We conclude\nwith a discussion on potential extensions of our work and strategies to include\ncovariates into our framework."}, "http://arxiv.org/abs/2309.14658": {"title": "Improvements on Scalable Stochastic Bayesian Inference Methods for Multivariate Hawkes Process", "link": "http://arxiv.org/abs/2309.14658", "description": "Multivariate Hawkes Processes (MHPs) are a class of point processes that can\naccount for complex temporal dynamics among event sequences. In this work, we\nstudy the accuracy and computational efficiency of three classes of algorithms\nwhich, while widely used in the context of Bayesian inference, have rarely been\napplied in the context of MHPs: stochastic gradient expectation-maximization,\nstochastic gradient variational inference and stochastic gradient Langevin\nMonte Carlo. An important contribution of this paper is a novel approximation\nto the likelihood function that allows us to retain the computational\nadvantages associated with conjugate settings while reducing approximation\nerrors associated with the boundary effects. The comparisons are based on\nvarious simulated scenarios as well as an application to the study the risk\ndynamics in the Standard &amp; Poor's 500 intraday index prices among its 11\nsectors."}, "http://arxiv.org/abs/2401.08606": {"title": "Forking paths in financial economics", "link": "http://arxiv.org/abs/2401.08606", "description": "We argue that spanning large numbers of degrees of freedom in empirical\nanalysis allows better characterizations of effects and thus improves the\ntrustworthiness of conclusions. Our ideas are illustrated in three studies:\nequity premium prediction, asset pricing anomalies and risk premia estimation.\nIn the first, we find that each additional degree of freedom in the protocol\nexpands the average range of $t$-statistics by at least 30%. In the second, we\nshow that resorting to forking paths instead of bootstrapping in multiple\ntesting raises the bar of significance for anomalies: at the 5% confidence\nlevel, the threshold for bootstrapped statistics is 4.5, whereas with paths, it\nis at least 8.2, a bar much higher than those currently used in the literature.\nIn our third application, we reveal the importance of particular steps in the\nestimation of premia. In addition, we use paths to corroborate prior findings\nin the three topics. We document heterogeneity in our ability to replicate\nprior studies: some conclusions seem robust, others do not align with the paths\nwe were able to generate."}, "http://arxiv.org/abs/2401.08626": {"title": "Validation and Comparison of Non-Stationary Cognitive Models: A Diffusion Model Application", "link": "http://arxiv.org/abs/2401.08626", "description": "Cognitive processes undergo various fluctuations and transient states across\ndifferent temporal scales. Superstatistics are emerging as a flexible framework\nfor incorporating such non-stationary dynamics into existing cognitive model\nclasses. In this work, we provide the first experimental validation of\nsuperstatistics and formal comparison of four non-stationary diffusion decision\nmodels in a specifically designed perceptual decision-making task. Task\ndifficulty and speed-accuracy trade-off were systematically manipulated to\ninduce expected changes in model parameters. To validate our models, we assess\nwhether the inferred parameter trajectories align with the patterns and\nsequences of the experimental manipulations. To address computational\nchallenges, we present novel deep learning techniques for amortized Bayesian\nestimation and comparison of models with time-varying parameters. Our findings\nindicate that transition models incorporating both gradual and abrupt parameter\nshifts provide the best fit to the empirical data. Moreover, we find that the\ninferred parameter trajectories closely mirror the sequence of experimental\nmanipulations. Posterior re-simulations further underscore the ability of the\nmodels to faithfully reproduce critical data patterns. Accordingly, our results\nsuggest that the inferred non-stationary dynamics may reflect actual changes in\nthe targeted psychological constructs. We argue that our initial experimental\nvalidation paves the way for the widespread application of superstatistics in\ncognitive modeling and beyond."}, "http://arxiv.org/abs/2401.08702": {"title": "Do We Really Even Need Data?", "link": "http://arxiv.org/abs/2401.08702", "description": "As artificial intelligence and machine learning tools become more accessible,\nand scientists face new obstacles to data collection (e.g. rising costs,\ndeclining survey response rates), researchers increasingly use predictions from\npre-trained algorithms as outcome variables. Though appealing for financial and\nlogistical reasons, using standard tools for inference can misrepresent the\nassociation between independent variables and the outcome of interest when the\ntrue, unobserved outcome is replaced by a predicted value. In this paper, we\ncharacterize the statistical challenges inherent to this so-called\n``post-prediction inference'' problem and elucidate three potential sources of\nerror: (i) the relationship between predicted outcomes and their true,\nunobserved counterparts, (ii) robustness of the machine learning model to\nresampling or uncertainty about the training data, and (iii) appropriately\npropagating not just bias but also uncertainty from predictions into the\nultimate inference procedure. We also contrast the framework for\npost-prediction inference with classical work spanning several related fields,\nincluding survey sampling, missing data, and semi-supervised learning. This\ncontrast elucidates the role of design in both classical and modern inference\nproblems."}, "http://arxiv.org/abs/2401.08875": {"title": "DCRMTA: Unbiased Causal Representation for Multi-touch Attribution", "link": "http://arxiv.org/abs/2401.08875", "description": "Multi-touch attribution (MTA) currently plays a pivotal role in achieving a\nfair estimation of the contributions of each advertising touchpoint to-wards\nconversion behavior, deeply influencing budget allocation and advertising\nrecommenda-tion. Traditional multi-touch attribution methods initially build a\nconversion prediction model, an-ticipating learning the inherent relationship\nbe-tween touchpoint sequences and user purchasing behavior through historical\ndata. Based on this, counterfactual touchpoint sequences are con-structed from\nthe original sequence subset, and conversions are estimated using the\nprediction model, thus calculating advertising contributions. A covert\nassumption of these methods is the un-biased nature of conversion prediction\nmodels. However, due to confounding variables factors arising from user\npreferences and internet recom-mendation mechanisms such as homogenization of\nad recommendations resulting from past shop-ping records, bias can easily occur\nin conversion prediction models trained on observational data. This paper\nredefines the causal effect of user fea-tures on conversions and proposes a\nnovel end-to-end approach, Deep Causal Representation for MTA (DCRMTA). Our\nmodel while eliminating confounding variables, extracts features with causal\nrelations to conversions from users. Fur-thermore, Extensive experiments on\nboth synthet-ic and real-world Criteo data demonstrate DCRMTA's superior\nperformance in converting prediction across varying data distributions, while\nalso effectively attributing value across dif-ferent advertising channels"}, "http://arxiv.org/abs/2401.08941": {"title": "A Powerful and Precise Feature-level Filter using Group Knockoffs", "link": "http://arxiv.org/abs/2401.08941", "description": "Selecting important features that have substantial effects on the response\nwith provable type-I error rate control is a fundamental concern in statistics,\nwith wide-ranging practical applications. Existing knockoff filters, although\nshown to provide theoretical guarantee on false discovery rate (FDR) control,\noften struggle to strike a balance between high power and precision in\npinpointing important features when there exist large groups of strongly\ncorrelated features. To address this challenge, we develop a new filter using\ngroup knockoffs to achieve both powerful and precise selection of important\nfeatures. Via experiments of simulated data and analysis of a real Alzheimer's\ndisease genetic dataset, it is found that the proposed filter can not only\ncontrol the proportion of false discoveries but also identify important\nfeatures with comparable power and greater precision than the existing group\nknockoffs filter."}, "http://arxiv.org/abs/2401.09379": {"title": "Merging uncertainty sets via majority vote", "link": "http://arxiv.org/abs/2401.09379", "description": "Given $K$ uncertainty sets that are arbitrarily dependent -- for example,\nconfidence intervals for an unknown parameter obtained with $K$ different\nestimators, or prediction sets obtained via conformal prediction based on $K$\ndifferent algorithms on shared data -- we address the question of how to\nefficiently combine them in a black-box manner to produce a single uncertainty\nset. We present a simple and broadly applicable majority vote procedure that\nproduces a merged set with nearly the same error guarantee as the input sets.\nWe then extend this core idea in a few ways: we show that weighted averaging\ncan be a powerful way to incorporate prior information, and a simple\nrandomization trick produces strictly smaller merged sets without altering the\ncoverage guarantee. Along the way, we prove an intriguing result that R\\\"uger's\ncombination rules (eg: twice the median of dependent p-values is a p-value) can\nbe strictly improved with randomization. When deployed in online settings, we\nshow how the exponential weighted majority algorithm can be employed in order\nto learn a good weighting over time. We then combine this method with adaptive\nconformal inference to deliver a simple conformal online model aggregation\n(COMA) method for nonexchangeable data."}, "http://arxiv.org/abs/2401.09381": {"title": "Modelling clusters in network time series with an application to presidential elections in the USA", "link": "http://arxiv.org/abs/2401.09381", "description": "Network time series are becoming increasingly relevant in the study of\ndynamic processes characterised by a known or inferred underlying network\nstructure. Generalised Network Autoregressive (GNAR) models provide a\nparsimonious framework for exploiting the underlying network, even in the\nhigh-dimensional setting. We extend the GNAR framework by introducing the\n$\\textit{community}$-$\\alpha$ GNAR model that exploits prior knowledge and/or\nexogenous variables for identifying and modelling dynamic interactions across\ncommunities in the underlying network. We further analyse the dynamics of\n$\\textit{Red, Blue}$ and $\\textit{Swing}$ states throughout presidential\nelections in the USA. Our analysis shows that dynamics differ among the\nstate-wise clusters."}, "http://arxiv.org/abs/2401.09401": {"title": "PERMUTOOLS: A MATLAB Package for Multivariate Permutation Testing", "link": "http://arxiv.org/abs/2401.09401", "description": "Statistical hypothesis testing and effect size measurement are routine parts\nof quantitative research. Advancements in computer processing power have\ngreatly improved the capability of statistical inference through the\navailability of resampling methods. However, many of the statistical practices\nused today are based on traditional, parametric methods that rely on\nassumptions about the underlying population. These assumptions may not always\nbe valid, leading to inaccurate results and misleading interpretations.\nPermutation testing, on the other hand, generates the sampling distribution\nempirically by permuting the observed data, providing distribution-free\nhypothesis testing. Furthermore, this approach lends itself to a powerful\nmethod for multiple comparison correction - known as max correction - which is\nless prone to type II errors than conventional correction methods. Parametric\nmethods have also traditionally been utilized for estimating the confidence\ninterval of various test statistics and effect size measures. However, these\ntoo can be estimated empirically using permutation or bootstrapping techniques.\nWhilst resampling methods are generally considered preferable, many popular\nprogramming languages and statistical software packages lack efficient\nimplementations. Here, we introduce PERMUTOOLS, a MATLAB package for\nmultivariate permutation testing and effect size measurement."}, "http://arxiv.org/abs/2008.03073": {"title": "From the power law to extreme value mixture distributions", "link": "http://arxiv.org/abs/2008.03073", "description": "The power law is useful in describing count phenomena such as network degrees\nand word frequencies. With a single parameter, it captures the main feature\nthat the frequencies are linear on the log-log scale. Nevertheless, there have\nbeen criticisms of the power law, for example that a threshold needs to be\npre-selected without its uncertainty quantified, that the power law is simply\ninadequate, and that subsequent hypothesis tests are required to determine\nwhether the data could have come from the power law. We propose a modelling\nframework that combines two different generalisations of the power law, namely\nthe generalised Pareto distribution and the Zipf-polylog distribution, to\nresolve these issues. The proposed mixture distributions are shown to fit the\ndata well and quantify the threshold uncertainty in a natural way. A model\nselection step embedded in the Bayesian inference algorithm further answers the\nquestion whether the power law is adequate."}, "http://arxiv.org/abs/2211.11884": {"title": "Parameter Estimation in Nonlinear Multivariate Stochastic Differential Equations Based on Splitting Schemes", "link": "http://arxiv.org/abs/2211.11884", "description": "Surprisingly, general estimators for nonlinear continuous time models based\non stochastic differential equations are yet lacking. Most applications still\nuse the Euler-Maruyama discretization, despite many proofs of its bias. More\nsophisticated methods, such as Kessler's Gaussian approximation, Ozak's Local\nLinearization, A\\\"it-Sahalia's Hermite expansions, or MCMC methods, lack a\nstraightforward implementation, do not scale well with increasing model\ndimension or can be numerically unstable. We propose two efficient and\neasy-to-implement likelihood-based estimators based on the Lie-Trotter (LT) and\nthe Strang (S) splitting schemes. We prove that S has $L^p$ convergence rate of\norder 1, a property already known for LT. We show that the estimators are\nconsistent and asymptotically efficient under the less restrictive one-sided\nLipschitz assumption. A numerical study on the 3-dimensional stochastic Lorenz\nsystem complements our theoretical findings. The simulation shows that the S\nestimator performs the best when measured on precision and computational speed\ncompared to the state-of-the-art."}, "http://arxiv.org/abs/2212.00703": {"title": "Data Integration Via Analysis of Subspaces (DIVAS)", "link": "http://arxiv.org/abs/2212.00703", "description": "Modern data collection in many data paradigms, including bioinformatics,\noften incorporates multiple traits derived from different data types (i.e.\nplatforms). We call this data multi-block, multi-view, or multi-omics data. The\nemergent field of data integration develops and applies new methods for\nstudying multi-block data and identifying how different data types relate and\ndiffer. One major frontier in contemporary data integration research is\nmethodology that can identify partially-shared structure between\nsub-collections of data types. This work presents a new approach: Data\nIntegration Via Analysis of Subspaces (DIVAS). DIVAS combines new insights in\nangular subspace perturbation theory with recent developments in matrix signal\nprocessing and convex-concave optimization into one algorithm for exploring\npartially-shared structure. Based on principal angles between subspaces, DIVAS\nprovides built-in inference on the results of the analysis, and is effective\neven in high-dimension-low-sample-size (HDLSS) situations."}, "http://arxiv.org/abs/2301.05636": {"title": "Improving Power by Conditioning on Less in Post-selection Inference for Changepoints", "link": "http://arxiv.org/abs/2301.05636", "description": "Post-selection inference has recently been proposed as a way of quantifying\nuncertainty about detected changepoints. The idea is to run a changepoint\ndetection algorithm, and then re-use the same data to perform a test for a\nchange near each of the detected changes. By defining the p-value for the test\nappropriately, so that it is conditional on the information used to choose the\ntest, this approach will produce valid p-values. We show how to improve the\npower of these procedures by conditioning on less information. This gives rise\nto an ideal selective p-value that is intractable but can be approximated by\nMonte Carlo. We show that for any Monte Carlo sample size, this procedure\nproduces valid p-values, and empirically that noticeable increase in power is\npossible with only very modest Monte Carlo sample sizes. Our procedure is easy\nto implement given existing post-selection inference methods, as we just need\nto generate perturbations of the data set and re-apply the post-selection\nmethod to each of these. On genomic data consisting of human GC content, our\nprocedure increases the number of significant changepoints that are detected\nfrom e.g. 17 to 27, when compared to existing methods."}, "http://arxiv.org/abs/2308.08958": {"title": "Linear Regression with Weak Exogeneity", "link": "http://arxiv.org/abs/2308.08958", "description": "This paper studies linear time series regressions with many regressors. Weak\nexogeneity is the most used identifying assumption in time series. Weak\nexogeneity requires the structural error to have zero conditional expectation\ngiven the present and past regressor values, allowing errors to correlate with\nfuture regressor realizations. We show that weak exogeneity in time series\nregressions with many controls may produce substantial biases and even render\nthe least squares (OLS) estimator inconsistent. The bias arises in settings\nwith many regressors because the normalized OLS design matrix remains\nasymptotically random and correlates with the regression error when only weak\n(but not strict) exogeneity holds. This bias's magnitude increases with the\nnumber of regressors and their average autocorrelation. To address this issue,\nwe propose an innovative approach to bias correction that yields a new\nestimator with improved properties relative to OLS. We establish consistency\nand conditional asymptotic Gaussianity of this new estimator and provide a\nmethod for inference."}, "http://arxiv.org/abs/2309.03969": {"title": "Estimating the prevalance of indirect effects and other spillovers", "link": "http://arxiv.org/abs/2309.03969", "description": "In settings where interference between units is possible, we define the\nprevalence of indirect effects to be the number of units who are affected by\nthe treatment of others. This quantity does not fully identify an indirect\neffect, but may be used to show whether such effects are widely prevalent.\nGiven a randomized experiment with binary-valued outcomes, methods are\npresented for conservative point estimation and one-sided interval estimation.\nNo assumptions beyond randomization of treatment are required, allowing for\nusage in settings where models or assumptions on interference might be\nquestionable. To show asymptotic coverage of our intervals in settings not\ncovered by existing results, we provide a central limit theorem that combines\nlocal dependence and sampling without replacement. Consistency and minimax\nproperties of the point estimator are shown as well. The approach is\ndemonstrated on an experiment in which students were treated for a highly\ntransmissible parasitic infection, for which we find that a significant\nfraction of students were affected by the treatment of schools other than their\nown."}, "http://arxiv.org/abs/2401.09559": {"title": "Asymptotic Online FWER Control for Dependent Test Statistics", "link": "http://arxiv.org/abs/2401.09559", "description": "In online multiple testing, an a priori unknown number of hypotheses are\ntested sequentially, i.e. at each time point a test decision for the current\nhypothesis has to be made using only the data available so far. Although many\npowerful test procedures have been developed for online error control in recent\nyears, most of them are designed solely for independent or at most locally\ndependent test statistics. In this work, we provide a new framework for\nderiving online multiple test procedures which ensure asymptotical (with\nrespect to the sample size) control of the familywise error rate (FWER),\nregardless of the dependence structure between test statistics. In this\ncontext, we give a few concrete examples of such test procedures and discuss\ntheir properties. Furthermore, we conduct a simulation study in which the type\nI error control of these test procedures is also confirmed for a finite sample\nsize and a gain in power is indicated."}, "http://arxiv.org/abs/2401.09641": {"title": "Functional Linear Non-Gaussian Acyclic Model for Causal Discovery", "link": "http://arxiv.org/abs/2401.09641", "description": "In causal discovery, non-Gaussianity has been used to characterize the\ncomplete configuration of a Linear Non-Gaussian Acyclic Model (LiNGAM),\nencompassing both the causal ordering of variables and their respective\nconnection strengths. However, LiNGAM can only deal with the finite-dimensional\ncase. To expand this concept, we extend the notion of variables to encompass\nvectors and even functions, leading to the Functional Linear Non-Gaussian\nAcyclic Model (Func-LiNGAM). Our motivation stems from the desire to identify\ncausal relationships in brain-effective connectivity tasks involving, for\nexample, fMRI and EEG datasets. We demonstrate why the original LiNGAM fails to\nhandle these inherently infinite-dimensional datasets and explain the\navailability of functional data analysis from both empirical and theoretical\nperspectives. {We establish theoretical guarantees of the identifiability of\nthe causal relationship among non-Gaussian random vectors and even random\nfunctions in infinite-dimensional Hilbert spaces.} To address the issue of\nsparsity in discrete time points within intrinsic infinite-dimensional\nfunctional data, we propose optimizing the coordinates of the vectors using\nfunctional principal component analysis. Experimental results on synthetic data\nverify the ability of the proposed framework to identify causal relationships\namong multivariate functions using the observed samples. For real data, we\nfocus on analyzing the brain connectivity patterns derived from fMRI data."}, "http://arxiv.org/abs/2401.09696": {"title": "Rejection Sampling with Vertical Weighted Strips", "link": "http://arxiv.org/abs/2401.09696", "description": "A number of distributions that arise in statistical applications can be\nexpressed in the form of a weighted density: the product of a base density and\na nonnegative weight function. Generating variates from such a distribution may\nbe nontrivial and can involve an intractable normalizing constant. Rejection\nsampling may be used to generate exact draws, but requires formulation of a\nsuitable proposal distribution. To be practically useful, the proposal must\nboth be convenient to sample from and not reject candidate draws too\nfrequently. A well-known approach to design a proposal involves decomposing the\ntarget density into a finite mixture, whose components may correspond to a\npartition of the support. This work considers such a construction that focuses\non majorization of the weight function. This approach may be applicable when\nassumptions for adaptive rejection sampling and related algorithms are not met.\nAn upper bound for the rejection probability based on this construction can be\nexpressed to evaluate the efficiency of the proposal before sampling. A method\nto partition the support is considered where regions are bifurcated based on\ntheir contribution to the bound. Examples based on the von Mises Fisher\ndistribution and Gaussian Process regression are provided to illustrate the\nmethod."}, "http://arxiv.org/abs/2401.09715": {"title": "Fast Variational Inference of Latent Space Models for Dynamic Networks Using Bayesian P-Splines", "link": "http://arxiv.org/abs/2401.09715", "description": "Latent space models (LSMs) are often used to analyze dynamic (time-varying)\nnetworks that evolve in continuous time. Existing approaches to Bayesian\ninference for these models rely on Markov chain Monte Carlo algorithms, which\ncannot handle modern large-scale networks. To overcome this limitation, we\nintroduce a new prior for continuous-time LSMs based on Bayesian P-splines that\nallows the posterior to adapt to the dimension of the latent space and the\ntemporal variation in each latent position. We propose a stochastic variational\ninference algorithm to estimate the model parameters. We use stochastic\noptimization to subsample both dyads and observed time points to design a fast\nalgorithm that is linear in the number of edges in the dynamic network.\nFurthermore, we establish non-asymptotic error bounds for point estimates\nderived from the variational posterior. To our knowledge, this is the first\nsuch result for Bayesian estimators of continuous-time LSMs. Lastly, we use the\nmethod to analyze a large data set of international conflicts consisting of\n4,456,095 relations from 2018 to 2022."}, "http://arxiv.org/abs/2401.09719": {"title": "Kernel-based multi-marker tests of association based on the accelerated failure time model", "link": "http://arxiv.org/abs/2401.09719", "description": "Kernel-based multi-marker tests for survival outcomes use primarily the Cox\nmodel to adjust for covariates. The proportional hazards assumption made by the\nCox model could be unrealistic, especially in the long-term follow-up. We\ndevelop a suite of novel multi-marker survival tests for genetic association\nbased on the accelerated failure time model, which is a popular alternative to\nthe Cox model due to its direct physical interpretation. The tests are based on\nthe asymptotic distributions of their test statistics and are thus\ncomputationally efficient. The association tests can account for the\nheterogeneity of genetic effects across sub-populations/individuals to increase\nthe power. All the new tests can deal with competing risks and left truncation.\nMoreover, we develop small-sample corrections to the tests to improve their\naccuracy under small samples. Extensive numerical experiments show that the new\ntests perform very well in various scenarios. An application to a genetic\ndataset of Alzheimer's disease illustrates the tests' practical utility."}, "http://arxiv.org/abs/2401.09816": {"title": "Jackknife empirical likelihood ratio test for testing the equality of semivariance", "link": "http://arxiv.org/abs/2401.09816", "description": "Semivariance is a measure of the dispersion of all observations that fall\nabove the mean or target value of a random variable and it plays an important\nrole in life-length, actuarial and income studies. In this paper, we develop a\nnew non-parametric test for equality of upper semi-variance. We use the\nU-statistic theory to derive the test statistic and then study the asymptotic\nproperties of the test statistic. We also develop a jackknife empirical\nlikelihood (JEL) ratio test for equality of upper Semivariance. Extensive Monte\nCarlo simulation studies are carried out to validate the performance of the\nproposed JEL-based test. We illustrate the test procedure using real data."}, "http://arxiv.org/abs/2401.09994": {"title": "Bayesian modeling of spatial ordinal data from health surveys", "link": "http://arxiv.org/abs/2401.09994", "description": "Health surveys allow exploring health indicators that are of great value from\na public health point of view and that cannot normally be studied from regular\nhealth registries. These indicators are usually coded as ordinal variables and\nmay depend on covariates associated with individuals. In this paper, we propose\na Bayesian individual-level model for small-area estimation of survey-based\nhealth indicators. A categorical likelihood is used at the first level of the\nmodel hierarchy to describe the ordinal data, and spatial dependence among\nsmall areas is taken into account by using a conditional autoregressive (CAR)\ndistribution. Post-stratification of the results of the proposed\nindividual-level model allows extrapolating the results to any administrative\nareal division, even for small areas. We apply this methodology to the analysis\nof the Health Survey of the Region of Valencia (Spain) of 2016 to describe the\ngeographical distribution of a self-perceived health indicator of interest in\nthis region."}, "http://arxiv.org/abs/2401.10010": {"title": "A global kernel estimator for partially linear varying coefficient additive hazards models", "link": "http://arxiv.org/abs/2401.10010", "description": "In biomedical studies, we are often interested in the association between\ndifferent types of covariates and the times to disease events. Because the\nrelationship between the covariates and event times is often complex, standard\nsurvival models that assume a linear covariate effect are inadequate. A\nflexible class of models for capturing complex interaction effects among types\nof covariates is the varying coefficient models, where the effects of a type of\ncovariates can be modified by another type of covariates. In this paper, we\nstudy kernel-based estimation methods for varying coefficient additive hazards\nmodels. Unlike many existing kernel-based methods that use a local neighborhood\nof subjects for the estimation of the varying coefficient function, we propose\na novel global approach that is generally more efficient. We establish\ntheoretical properties of the proposed estimators and demonstrate their\nsuperior performance compared with existing local methods through large-scale\nsimulation studies. To illustrate the proposed method, we provide an\napplication to a motivating cancer genomic study."}, "http://arxiv.org/abs/2401.10057": {"title": "A method for characterizing disease emergence curves from paired pathogen detection and serology data", "link": "http://arxiv.org/abs/2401.10057", "description": "Wildlife disease surveillance programs and research studies track infection\nand identify risk factors for wild populations, humans, and agriculture. Often,\nseveral types of samples are collected from individuals to provide more\ncomplete information about an animal's infection history. Methods that jointly\nanalyze multiple data streams to study disease emergence and drivers of\ninfection via epidemiological process models remain underdeveloped.\nJoint-analysis methods can more thoroughly analyze all available data, more\nprecisely quantifying epidemic processes, outbreak status, and risks. We\ncontribute a paired data modeling approach that analyzes multiple samples from\nindividuals. We use \"characterization maps\" to link paired data to\nepidemiological processes through a hierarchical statistical observation model.\nOur approach can provide both Bayesian and frequentist estimates of\nepidemiological parameters and state. We motivate our approach through the need\nto use paired pathogen and antibody detection tests to estimate parameters and\ninfection trajectories for the widely applicable susceptible, infectious,\nrecovered (SIR) model. We contribute general formulas to link characterization\nmaps to arbitrary process models and datasets and an extended SIR model that\nbetter accommodates paired data. We find via simulation that paired data can\nmore efficiently estimate SIR parameters than unpaired data, requiring samples\nfrom 5-10 times fewer individuals. We then study SARS-CoV-2 in wild\nWhite-tailed deer (Odocoileus virginianus) from three counties in the United\nStates. Estimates for average infectious times corroborate captive animal\nstudies. Our methods use general statistical theory to let applications extend\nbeyond the SIR model we consider, and to more complicated examples of paired\ndata."}, "http://arxiv.org/abs/2401.10124": {"title": "Lower Ricci Curvature for Efficient Community Detection", "link": "http://arxiv.org/abs/2401.10124", "description": "This study introduces the Lower Ricci Curvature (LRC), a novel, scalable, and\nscale-free discrete curvature designed to enhance community detection in\nnetworks. Addressing the computational challenges posed by existing\ncurvature-based methods, LRC offers a streamlined approach with linear\ncomputational complexity, making it well-suited for large-scale network\nanalysis. We further develop an LRC-based preprocessing method that effectively\naugments popular community detection algorithms. Through comprehensive\nsimulations and applications on real-world datasets, including the NCAA\nfootball league network, the DBLP collaboration network, the Amazon product\nco-purchasing network, and the YouTube social network, we demonstrate the\nefficacy of our method in significantly improving the performance of various\ncommunity detection algorithms."}, "http://arxiv.org/abs/2401.10180": {"title": "Generalized Decomposition Priors on R2", "link": "http://arxiv.org/abs/2401.10180", "description": "The adoption of continuous shrinkage priors in high-dimensional linear models\nhas gained momentum, driven by their theoretical and practical advantages. One\nof these shrinkage priors is the R2D2 prior, which comes with intuitive\nhyperparameters and well understood theoretical properties. The core idea is to\nspecify a prior on the percentage of explained variance $R^2$ and to conduct a\nDirichlet decomposition to distribute the explained variance among all the\nregression terms of the model. Due to the properties of the Dirichlet\ndistribution, the competition among variance components tends to gravitate\ntowards negative dependence structures, fully determined by the individual\ncomponents' means. Yet, in reality, specific coefficients or groups may compete\ndifferently for the total variability than the Dirichlet would allow for. In\nthis work we address this limitation by proposing a generalization of the R2D2\nprior, which we term the Generalized Decomposition R2 (GDR2) prior.\n\nOur new prior provides great flexibility in expressing dependency structures\nas well as enhanced shrinkage properties. Specifically, we explore the\ncapabilities of variance decomposition via logistic normal distributions.\nThrough extensive simulations and real-world case studies, we demonstrate that\nGDR2 priors yield strongly improved out-of-sample predictive performance and\nparameter recovery compared to R2D2 priors with similar hyper-parameter\nchoices."}, "http://arxiv.org/abs/2401.10193": {"title": "tinyVAST: R package with an expressive interface to specify lagged and simultaneous effects in multivariate spatio-temporal models", "link": "http://arxiv.org/abs/2401.10193", "description": "Multivariate spatio-temporal models are widely applicable, but specifying\ntheir structure is complicated and may inhibit wider use. We introduce the R\npackage tinyVAST from two viewpoints: the software user and the statistician.\nFrom the user viewpoint, tinyVAST adapts a widely used formula interface to\nspecify generalized additive models, and combines this with arguments to\nspecify spatial and spatio-temporal interactions among variables. These\ninteractions are specified using arrow notation (from structural equation\nmodels), or an extended arrow-and-lag notation that allows simultaneous,\nlagged, and recursive dependencies among variables over time. The user also\nspecifies a spatial domain for areal (gridded), continuous (point-count), or\nstream-network data. From the statistician viewpoint, tinyVAST constructs\nsparse precision matrices representing multivariate spatio-temporal variation,\nand parameters are estimated by specifying a generalized linear mixed model\n(GLMM). This expressive interface encompasses vector autoregressive, empirical\northogonal functions, spatial factor analysis, and ARIMA models. To\ndemonstrate, we fit to data from two survey platforms sampling corals, sponges,\nrockfishes, and flatfishes in the Gulf of Alaska and Aleutian Islands. We then\ncompare eight alternative model structures using different assumptions about\nhabitat drivers and survey detectability. Model selection suggests that\ntowed-camera and bottom trawl gears have spatial variation in detectability but\nsample the same underlying density of flatfishes and rockfishes, and that\nrockfishes are positively associated with sponges while flatfishes are\nnegatively associated with corals. We conclude that tinyVAST can be used to\ntest complicated dependencies representing alternative structural assumptions\nfor research and real-world policy evaluation."}, "http://arxiv.org/abs/2401.10196": {"title": "Functional Conditional Gaussian Graphical Models", "link": "http://arxiv.org/abs/2401.10196", "description": "Functional data has become a commonly encountered data type. In this paper,\nwe contribute to the literature on functional graphical modelling by extending\nthe notion of conditional Gaussian Graphical models and proposing a\ndouble-penalized estimator by which to recover the edge-set of the\ncorresponding graph. Penalty parameters play a crucial role in determining the\nprecision matrices for the response variables and the regression matrices. The\nperformance and model selection process in the proposed framework are\ninvestigated using information criteria. Moreover, we propose a novel version\nof the Kullback-Leibler cross-validation designed for conditional joint\nGaussian Graphical Models. The evaluation of model performance is done in terms\nof Kullback-Leibler divergence and graph recovery power."}, "http://arxiv.org/abs/2206.02250": {"title": "Frequency Domain Statistical Inference for High-Dimensional Time Series", "link": "http://arxiv.org/abs/2206.02250", "description": "Analyzing time series in the frequency domain enables the development of\npowerful tools for investigating the second-order characteristics of\nmultivariate processes. Parameters like the spectral density matrix and its\ninverse, the coherence or the partial coherence, encode comprehensively the\ncomplex linear relations between the component processes of the multivariate\nsystem. In this paper, we develop inference procedures for such parameters in a\nhigh-dimensional, time series setup. Towards this goal, we first focus on the\nderivation of consistent estimators of the coherence and, more importantly, of\nthe partial coherence which possess manageable limiting distributions that are\nsuitable for testing purposes. Statistical tests of the hypothesis that the\nmaximum over frequencies of the coherence, respectively, of the partial\ncoherence, do not exceed a prespecified threshold value are developed. Our\napproach allows for testing hypotheses for individual coherences and/or partial\ncoherences as well as for multiple testing of large sets of such parameters. In\nthe latter case, a consistent procedure to control the false discovery rate is\ndeveloped. The finite sample performance of the inference procedures introduced\nis investigated by means of simulations and applications to the construction of\ngraphical interaction models for brain connectivity based on EEG data are\npresented."}, "http://arxiv.org/abs/2206.09754": {"title": "Guided structure learning of DAGs for count data", "link": "http://arxiv.org/abs/2206.09754", "description": "In this paper, we tackle structure learning of Directed Acyclic Graphs\n(DAGs), with the idea of exploiting available prior knowledge of the domain at\nhand to guide the search of the best structure. In particular, we assume to\nknow the topological ordering of variables in addition to the given data. We\nstudy a new algorithm for learning the structure of DAGs, proving its\ntheoretical consistence in the limit of infinite observations. Furthermore, we\nexperimentally compare the proposed algorithm to a number of popular\ncompetitors, in order to study its behavior in finite samples."}, "http://arxiv.org/abs/2211.08784": {"title": "The robusTest package: two-sample tests revisited", "link": "http://arxiv.org/abs/2211.08784", "description": "The R package robusTest offers corrected versions of several common tests in\nbivariate statistics. We point out the limitations of these tests in their\nclassical versions, some of which are well known such as robustness or\ncalibration problems, and provide simple alternatives that can be easily used\ninstead. The classical tests and theirs robust alternatives are compared\nthrough a small simulation study. The latter emphasizes the superiority of\nrobust versions of the test of interest. Finally, an illustration of\ncorrelation's tests on a real data set is also provided."}, "http://arxiv.org/abs/2303.13598": {"title": "Bootstrap-Assisted Inference for Generalized Grenander-type Estimators", "link": "http://arxiv.org/abs/2303.13598", "description": "Westling and Carone (2020) proposed a framework for studying the large sample\ndistributional properties of generalized Grenander-type estimators, a versatile\nclass of nonparametric estimators of monotone functions. The limiting\ndistribution of those estimators is representable as the left derivative of the\ngreatest convex minorant of a Gaussian process whose covariance kernel can be\ncomplicated and whose monomial mean can be of unknown order (when the degree of\nflatness of the function of interest is unknown). The standard nonparametric\nbootstrap is unable to consistently approximate the large sample distribution\nof the generalized Grenander-type estimators even if the monomial order of the\nmean is known, making statistical inference a challenging endeavour in\napplications. To address this inferential problem, we present a\nbootstrap-assisted inference procedure for generalized Grenander-type\nestimators. The procedure relies on a carefully crafted, yet automatic,\ntransformation of the estimator. Moreover, our proposed method can be made\n``flatness robust'' in the sense that it can be made adaptive to the (possibly\nunknown) degree of flatness of the function of interest. The method requires\nonly the consistent estimation of a single scalar quantity, for which we\npropose an automatic procedure based on numerical derivative estimation and the\ngeneralized jackknife. Under random sampling, our inference method can be\nimplemented using a computationally attractive exchangeable bootstrap\nprocedure. We illustrate our methods with examples and we also provide a small\nsimulation study. The development of formal results is made possible by some\ntechnical results that may be of independent interest."}, "http://arxiv.org/abs/2304.05527": {"title": "Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box", "link": "http://arxiv.org/abs/2304.05527", "description": "Automatic differentiation variational inference (ADVI) offers fast and\neasy-to-use posterior approximation in multiple modern probabilistic\nprogramming languages. However, its stochastic optimizer lacks clear\nconvergence criteria and requires tuning parameters. Moreover, ADVI inherits\nthe poor posterior uncertainty estimates of mean-field variational Bayes\n(MFVB). We introduce \"deterministic ADVI\" (DADVI) to address these issues.\nDADVI replaces the intractable MFVB objective with a fixed Monte Carlo\napproximation, a technique known in the stochastic optimization literature as\nthe \"sample average approximation\" (SAA). By optimizing an approximate but\ndeterministic objective, DADVI can use off-the-shelf second-order optimization,\nand, unlike standard mean-field ADVI, is amenable to more accurate posterior\ncovariances via linear response (LR). In contrast to existing worst-case\ntheory, we show that, on certain classes of common statistical problems, DADVI\nand the SAA can perform well with relatively few samples even in very high\ndimensions, though we also show that such favorable results cannot extend to\nvariational approximations that are too expressive relative to mean-field ADVI.\nWe show on a variety of real-world problems that DADVI reliably finds good\nsolutions with default settings (unlike ADVI) and, together with LR\ncovariances, is typically faster and more accurate than standard ADVI."}, "http://arxiv.org/abs/2401.10233": {"title": "Likelihood-ratio inference on differences in quantiles", "link": "http://arxiv.org/abs/2401.10233", "description": "Quantiles can represent key operational and business metrics, but the\ncomputational challenges associated with inference has hampered their adoption\nin online experimentation. One-sample confidence intervals are trivial to\nconstruct; however, two-sample inference has traditionally required\nbootstrapping or a density estimator. This paper presents a new two-sample\ndifference-in-quantile hypothesis test and confidence interval based on a\nlikelihood-ratio test statistic. A conservative version of the test does not\ninvolve a density estimator; a second version of the test, which uses a density\nestimator, yields confidence intervals very close to the nominal coverage\nlevel. It can be computed using only four order statistics from each sample."}, "http://arxiv.org/abs/2401.10235": {"title": "Semi-parametric local variable selection under misspecification", "link": "http://arxiv.org/abs/2401.10235", "description": "Local variable selection aims to discover localized effects by assessing the\nimpact of covariates on outcomes within specific regions defined by other\ncovariates. We outline some challenges of local variable selection in the\npresence of non-linear relationships and model misspecification. Specifically,\nwe highlight a potential drawback of common semi-parametric methods: even\nslight model misspecification can result in a high rate of false positives. To\naddress these shortcomings, we propose a methodology based on orthogonal cut\nsplines that achieves consistent local variable selection in high-dimensional\nscenarios. Our approach offers simplicity, handles both continuous and discrete\ncovariates, and provides theory for high-dimensional covariates and model\nmisspecification. We discuss settings with either independent or dependent\ndata. Our proposal allows including adjustment covariates that do not undergo\nselection, enhancing flexibility in modeling complex scenarios. We illustrate\nits application in simulation studies with both independent and functional\ndata, as well as with two real datasets. One dataset evaluates salary gaps\nassociated with discrimination factors at different ages, while the other\nexamines the effects of covariates on brain activation over time. The approach\nis implemented in the R package mombf."}, "http://arxiv.org/abs/2401.10269": {"title": "Robust Multi-Sensor Multi-Target Tracking Using Possibility Labeled Multi-Bernoulli Filter", "link": "http://arxiv.org/abs/2401.10269", "description": "With the increasing complexity of multiple target tracking scenes, a single\nsensor may not be able to effectively monitor a large number of targets.\nTherefore, it is imperative to extend the single-sensor technique to\nMulti-Sensor Multi-Target Tracking (MSMTT) for enhanced functionality. Typical\nMSMTT methods presume complete randomness of all uncertain components, and\ntherefore effective solutions such as the random finite set filter and\ncovariance intersection method have been derived to conduct the MSMTT task.\nHowever, the presence of epistemic uncertainty, arising from incomplete\ninformation, is often disregarded within the context of MSMTT. This paper\ndevelops an innovative possibility Labeled Multi-Bernoulli (LMB) Filter based\non the labeled Uncertain Finite Set (UFS) theory. The LMB filter inherits the\nhigh robustness of the possibility generalized labeled multi-Bernoulli filter\nwith simplified computational complexity. The fusion of LMB UFSs is derived and\nadapted to develop a robust MSMTT scheme. Simulation results corroborate the\nsuperior performance exhibited by the proposed approach in comparison to\ntypical probabilistic methods."}, "http://arxiv.org/abs/2401.10275": {"title": "Symbolic principal plane with Duality Centers Method", "link": "http://arxiv.org/abs/2401.10275", "description": "In \\cite{ref11} and \\cite{ref3}, the authors proposed the Centers and the\nVertices Methods to extend the well known principal components analysis method\nto a particular kind of symbolic objects characterized by multi--valued\nvariables of interval type. Nevertheless the authors use the classical circle\nof correlation to represent the variables. The correlation between the\nvariables and the principal components are not symbolic, because they compute\nthe standard correlations between the midpoints of variables and the midpoints\nof the principal components. It is well known that in standard principal\ncomponent analysis we may compute the correlation between the variables and the\nprincipal components using the duality relations starting from the coordinates\nof the individuals in the principal plane, also we can compute the coordinates\nof the individuals in the principal plane using duality relations starting from\nthe correlation between the variables and the principal components. In this\npaper we propose a new method to compute the symbolic correlation circle using\nduality relations in the case of interval-valued variables. Besides, the reader\nmay use all the methods presented herein and verify the results using the {\\tt\nRSDA} package written in {\\tt R} language, that can be downloaded and installed\ndirectly from {\\tt CRAN} \\cite{Rod2014}."}, "http://arxiv.org/abs/2401.10276": {"title": "Correspondence Analysis for Symbolic Multi--Valued Variables", "link": "http://arxiv.org/abs/2401.10276", "description": "This paper sets a proposal of a new method and two new algorithms for\nCorrespondence Analysis when we have Symbolic Multi--Valued Variables (SymCA).\nIn our method, there are two multi--valued variables $X$ and $Y$, that is to\nsay, the modality that takes the variables for a given individual is a finite\nset formed by the possible modalities taken for the variables in a given\nindividual, that which allows to apply the Correspondence Analysis to multiple\nselection questionnaires. Then, starting from all the possible classic\ncontingency tables an interval contingency table can be built, which will be\nthe point of departure of the proposed method."}, "http://arxiv.org/abs/2401.10495": {"title": "Causal Layering via Conditional Entropy", "link": "http://arxiv.org/abs/2401.10495", "description": "Causal discovery aims to recover information about an unobserved causal graph\nfrom the observable data it generates. Layerings are orderings of the variables\nwhich place causes before effects. In this paper, we provide ways to recover\nlayerings of a graph by accessing the data via a conditional entropy oracle,\nwhen distributions are discrete. Our algorithms work by repeatedly removing\nsources or sinks from the graph. Under appropriate assumptions and\nconditioning, we can separate the sources or sinks from the remainder of the\nnodes by comparing their conditional entropy to the unconditional entropy of\ntheir noise. Our algorithms are provably correct and run in worst-case\nquadratic time. The main assumptions are faithfulness and injective noise, and\neither known noise entropies or weakly monotonically increasing noise entropies\nalong directed paths. In addition, we require one of either a very mild\nextension of faithfulness, or strictly monotonically increasing noise\nentropies, or expanding noise injectivity to include an additional single\nargument in the structural functions."}, "http://arxiv.org/abs/2401.10592": {"title": "Bayesian sample size determination using robust commensurate priors with interpretable discrepancy weights", "link": "http://arxiv.org/abs/2401.10592", "description": "Randomized controlled clinical trials provide the gold standard for evidence\ngeneration in relation to the effectiveness of a new treatment in medical\nresearch. Relevant information from previous studies may be desirable to\nincorporate in the design of a new trial, with the Bayesian paradigm providing\na coherent framework to formally incorporate prior knowledge. Many established\nmethods involve the use of a discounting factor, sometimes related to a measure\nof `similarity' between historical sources and the new trial. However, it is\noften the case that the sample size is highly nonlinear in those discounting\nfactors. This hinders communication with subject-matter experts to elicit\nsensible values for borrowing strength at the trial design stage.\n\nFocusing on a sample size formula that can incorporate historical data from\nmultiple sources, we propose a linearization technique such that the sample\nsize changes evenly over values of the discounting factors (hereafter referred\nto as `weights'). Our approach leads to interpretable weights that directly\nrepresent the dissimilarity between historical and new trial data on the\nprobability scale, and could therefore facilitate easier elicitation of expert\nopinion on their values.\n\nInclusion of historical data in the design of clinical trials is not common\npractice. Part of the reason might be difficulty in interpretability of\ndiscrepancy parameters. We hope our work will help to bridge this gap and\nencourage uptake of these innovative methods.\n\nKeywords: Bayesian sample size determination; Commensurate priors; Historical\nborrowing; Prior aggregation; Uniform shrinkage."}, "http://arxiv.org/abs/2401.10796": {"title": "Reliability analysis for data-driven noisy models using active learning", "link": "http://arxiv.org/abs/2401.10796", "description": "Reliability analysis aims at estimating the failure probability of an\nengineering system. It often requires multiple runs of a limit-state function,\nwhich usually relies on computationally intensive simulations. Traditionally,\nthese simulations have been considered deterministic, i.e., running them\nmultiple times for a given set of input parameters always produces the same\noutput. However, this assumption does not always hold, as many studies in the\nliterature report non-deterministic computational simulations (also known as\nnoisy models). In such cases, running the simulations multiple times with the\nsame input will result in different outputs. Similarly, data-driven models that\nrely on real-world data may also be affected by noise. This characteristic\nposes a challenge when performing reliability analysis, as many classical\nmethods, such as FORM and SORM, are tailored to deterministic models. To bridge\nthis gap, this paper provides a novel methodology to perform reliability\nanalysis on models contaminated by noise. In such cases, noise introduces\nlatent uncertainty into the reliability estimator, leading to an incorrect\nestimation of the real underlying reliability index, even when using Monte\nCarlo simulation. To overcome this challenge, we propose the use of denoising\nregression-based surrogate models within an active learning reliability\nanalysis framework. Specifically, we combine Gaussian process regression with a\nnoise-aware learning function to efficiently estimate the probability of\nfailure of the underlying noise-free model. We showcase the effectiveness of\nthis methodology on standard benchmark functions and a finite element model of\na realistic structural frame."}, "http://arxiv.org/abs/2401.10824": {"title": "The trivariate wrapped Cauchy copula -- a multi-purpose model for angular data", "link": "http://arxiv.org/abs/2401.10824", "description": "In this paper, we will present a new flexible distribution for\nthree-dimensional angular data, or data on the three-dimensional torus. Our\ntrivariate wrapped Cauchy copula has the following benefits: (i) simple form of\ndensity, (ii) adjustable degree of dependence between every pair of variables,\n(iii) interpretable and well-estimable parameters, (iv) well-known conditional\ndistributions, (v) a simple data generating mechanism, (vi) unimodality.\nMoreover, our construction allows for linear marginals, implying that our\ncopula can also model cylindrical data. Parameter estimation via maximum\nlikelihood is explained, a comparison with the competitors in the existing\nliterature is given, and two real datasets are considered, one concerning\nprotein dihedral angles and another about data obtained by a buoy in the\nAdriatic Sea."}, "http://arxiv.org/abs/2401.10867": {"title": "Learning Optimal Dynamic Treatment Regimes from Longitudinal Data", "link": "http://arxiv.org/abs/2401.10867", "description": "Studies often report estimates of the average treatment effect. While the ATE\nsummarizes the effect of a treatment on average, it does not provide any\ninformation about the effect of treatment within any individual. A treatment\nstrategy that uses an individual's information to tailor treatment to maximize\nbenefit is known as an optimal dynamic treatment rule. Treatment, however, is\ntypically not limited to a single point in time; consequently, learning an\noptimal rule for a time-varying treatment may involve not just learning the\nextent to which the comparative treatments' benefits vary across the\ncharacteristics of individuals, but also learning the extent to which the\ncomparative treatments' benefits vary as relevant circumstances evolve within\nan individual. The goal of this paper is to provide a tutorial for estimating\nODTR from longitudinal observational and clinical trial data for applied\nresearchers. We describe an approach that uses a doubly-robust unbiased\ntransformation of the conditional average treatment effect. We then learn a\ntime-varying ODTR for when to increase buprenorphine-naloxone dose to minimize\nreturn-to-regular-opioid-use among patients with opioid use disorder. Our\nanalysis highlights the utility of ODTRs in the context of sequential decision\nmaking: the learned ODTR outperforms a clinically defined strategy."}, "http://arxiv.org/abs/2401.10869": {"title": "Variable selection for partially linear additive models", "link": "http://arxiv.org/abs/2401.10869", "description": "Among semiparametric regression models, partially linear additive models\nprovide a useful tool to include additive nonparametric components as well as a\nparametric component, when explaining the relationship between the response and\na set of explanatory variables. This paper concerns such models under sparsity\nassumptions for the covariates included in the linear component. Sparse\ncovariates are frequent in regression problems where the task of variable\nselection is usually of interest. As in other settings, outliers either in the\nresiduals or in the covariates involved in the linear component have a harmful\neffect. To simultaneously achieve model selection for the parametric component\nof the model and resistance to outliers, we combine preliminary robust\nestimators of the additive component, robust linear $MM-$regression estimators\nwith a penalty such as SCAD on the coefficients in the parametric part. Under\nmild assumptions, consistency results and rates of convergence for the proposed\nestimators are derived. A Monte Carlo study is carried out to compare, under\ndifferent models and contamination schemes, the performance of the robust\nproposal with its classical counterpart. The obtained results show the\nadvantage of using the robust approach. Through the analysis of a real data\nset, we also illustrate the benefits of the proposed procedure."}, "http://arxiv.org/abs/2103.06347": {"title": "Factorized Binary Search: change point detection in the network structure of multivariate high-dimensional time series", "link": "http://arxiv.org/abs/2103.06347", "description": "Functional magnetic resonance imaging (fMRI) time series data presents a\nunique opportunity to understand the behavior of temporal brain connectivity,\nand models that uncover the complex dynamic workings of this organ are of keen\ninterest in neuroscience. We are motivated to develop accurate change point\ndetection and network estimation techniques for high-dimensional whole-brain\nfMRI data. To this end, we introduce factorized binary search (FaBiSearch), a\nnovel change point detection method in the network structure of multivariate\nhigh-dimensional time series in order to understand the large-scale\ncharacterizations and dynamics of the brain. FaBiSearch employs non-negative\nmatrix factorization, an unsupervised dimension reduction technique, and a new\nbinary search algorithm to identify multiple change points. In addition, we\npropose a new method for network estimation for data between change points. We\nseek to understand the dynamic mechanism of the brain, particularly for two\nfMRI data sets. The first is a resting-state fMRI experiment, where subjects\nare scanned over three visits. The second is a task-based fMRI experiment,\nwhere subjects read Chapter 9 of Harry Potter and the Sorcerer's Stone. For the\nresting-state data set, we examine the test-retest behavior of dynamic\nfunctional connectivity, while for the task-based data set, we explore network\ndynamics during the reading and whether change points across subjects coincide\nwith key plot twists in the story. Further, we identify hub nodes in the brain\nnetwork and examine their dynamic behavior. Finally, we make all the methods\ndiscussed available in the R package fabisearch on CRAN."}, "http://arxiv.org/abs/2207.07020": {"title": "Estimating sparse direct effects in multivariate regression with the spike-and-slab LASSO", "link": "http://arxiv.org/abs/2207.07020", "description": "The Gaussian chain graph model simultaneously parametrizes (i) the direct\neffects of $p$ predictors on $q$ outcomes and (ii) the residual partial\ncovariances between pairs of outcomes. We introduce a new method for fitting\nsparse Gaussian chain graph models with spike-and-slab LASSO (SSL) priors. We\ndevelop an Expectation Conditional Maximization algorithm to obtain sparse\nestimates of the $p \\times q$ matrix of direct effects and the $q \\times q$\nresidual precision matrix. Our algorithm iteratively solves a sequence of\npenalized maximum likelihood problems with self-adaptive penalties that\ngradually filter out negligible regression coefficients and partial\ncovariances. Because it adaptively penalizes individual model parameters, our\nmethod is seen to outperform fixed-penalty competitors on simulated data. We\nestablish the posterior contraction rate for our model, buttressing our\nmethod's excellent empirical performance with strong theoretical guarantees.\nUsing our method, we estimated the direct effects of diet and residence type on\nthe composition of the gut microbiome of elderly adults."}, "http://arxiv.org/abs/2301.00040": {"title": "Optimization-based Sensitivity Analysis for Unmeasured Confounding using Partial Correlations", "link": "http://arxiv.org/abs/2301.00040", "description": "Causal inference necessarily relies upon untestable assumptions; hence, it is\ncrucial to assess the robustness of obtained results to violations of\nidentification assumptions. However, such sensitivity analysis is only\noccasionally undertaken in practice, as many existing methods only apply to\nrelatively simple models and their results are often difficult to interpret. We\ntake a more flexible approach to sensitivity analysis and view it as a\nconstrained stochastic optimization problem. This work focuses on sensitivity\nanalysis for a linear causal effect when an unmeasured confounder and a\npotential instrument are present. We show how the bias of the OLS and TSLS\nestimands can be expressed in terms of partial correlations. Leveraging the\nalgebraic rules that relate different partial correlations, practitioners can\nspecify intuitive sensitivity models which bound the bias. We further show that\nthe heuristic \"plug-in\" sensitivity interval may not have any confidence\nguarantees; instead, we propose a bootstrap approach to construct sensitivity\nintervals which perform well in numerical simulations. We illustrate the\nproposed methods with a real study on the causal effect of education on\nearnings and provide user-friendly visualization tools."}, "http://arxiv.org/abs/2307.00048": {"title": "Learned harmonic mean estimation of the marginal likelihood with normalizing flows", "link": "http://arxiv.org/abs/2307.00048", "description": "Computing the marginal likelihood (also called the Bayesian model evidence)\nis an important task in Bayesian model selection, providing a principled\nquantitative way to compare models. The learned harmonic mean estimator solves\nthe exploding variance problem of the original harmonic mean estimation of the\nmarginal likelihood. The learned harmonic mean estimator learns an importance\nsampling target distribution that approximates the optimal distribution. While\nthe approximation need not be highly accurate, it is critical that the\nprobability mass of the learned distribution is contained within the posterior\nin order to avoid the exploding variance problem. In previous work a bespoke\noptimization problem is introduced when training models in order to ensure this\nproperty is satisfied. In the current article we introduce the use of\nnormalizing flows to represent the importance sampling target distribution. A\nflow-based model is trained on samples from the posterior by maximum likelihood\nestimation. Then, the probability density of the flow is concentrated by\nlowering the variance of the base distribution, i.e. by lowering its\n\"temperature\", ensuring its probability mass is contained within the posterior.\nThis approach avoids the need for a bespoke optimisation problem and careful\nfine tuning of parameters, resulting in a more robust method. Moreover, the use\nof normalizing flows has the potential to scale to high dimensional settings.\nWe present preliminary experiments demonstrating the effectiveness of the use\nof flows for the learned harmonic mean estimator. The harmonic code\nimplementing the learned harmonic mean, which is publicly available, has been\nupdated to now support normalizing flows."}, "http://arxiv.org/abs/2401.11001": {"title": "Reply to Comment by Schilling on their paper \"Optimal and fast confidence intervals for hypergeometric successes\"", "link": "http://arxiv.org/abs/2401.11001", "description": "A response to a letter to the editor by Schilling regarding Bartroff, Lorden,\nand Wang (\"Optimal and fast confidence intervals for hypergeometric successes\"\n2022, <a href=\"https://export.arxiv.org/abs/2109.05624\">arXiv:2109.05624</a>)"}, "http://arxiv.org/abs/2401.11046": {"title": "Information Based Inference in Models with Set-Valued Predictions and Misspecification", "link": "http://arxiv.org/abs/2401.11046", "description": "This paper proposes an information-based inference method for partially\nidentified parameters in incomplete models that is valid both when the model is\ncorrectly specified and when it is misspecified. Key features of the method\nare: (i) it is based on minimizing a suitably defined Kullback-Leibler\ninformation criterion that accounts for incompleteness of the model and\ndelivers a non-empty pseudo-true set; (ii) it is computationally tractable;\n(iii) its implementation is the same for both correctly and incorrectly\nspecified models; (iv) it exploits all information provided by variation in\ndiscrete and continuous covariates; (v) it relies on Rao's score statistic,\nwhich is shown to be asymptotically pivotal."}, "http://arxiv.org/abs/2401.11070": {"title": "Efficient Data Reduction Strategies for Big Data and High-Dimensional LASSO Regressions", "link": "http://arxiv.org/abs/2401.11070", "description": "The IBOSS approach proposed by Wang et al. (2019) selects the most\ninformative subset of n points. It assumes that the ordinary least squares\nmethod is used and requires that the number of variables, p, is not large.\nHowever, in many practical problems, p is very large and penalty-based model\nfitting methods such as LASSO is used. We study the big data problems, in which\nboth n and p are large. In the first part, we focus on reduction in data\npoints. We develop theoretical results showing that the IBOSS type of approach\ncan be applicable to penalty-based regressions such as LASSO. In the second\npart, we consider the situations where p is extremely large. We propose a\ntwo-step approach that involves first reducing the number of variables and then\nreducing the number of data points. Two separate algorithms are developed,\nwhose performances are studied through extensive simulation studies. Compared\nto existing methods including well-known split-and-conquer approach, the\nproposed methods enjoy advantages in terms of estimation accuracy, prediction\naccuracy, and computation time."}, "http://arxiv.org/abs/2401.11075": {"title": "Estimating the Hawkes process from a discretely observed sample path", "link": "http://arxiv.org/abs/2401.11075", "description": "The Hawkes process is a widely used model in many areas, such as\n\nfinance, seismology, neuroscience, epidemiology, and social\n\nsciences. Estimation of the Hawkes process from continuous\n\nobservations of a sample path is relatively straightforward using\n\neither the maximum likelihood or other methods. However, estimating\n\nthe parameters of a Hawkes process from observations of a sample\n\npath at discrete time points only is challenging due to the\n\nintractability of the likelihood with such data. In this work, we\n\nintroduce a method to estimate the Hawkes process from a discretely\n\nobserved sample path. The method takes advantage of a state-space\n\nrepresentation of the incomplete data problem and use the sequential\n\nMonte Carlo (aka particle filtering) to approximate the likelihood\n\nfunction. As an estimator of the likelihood function the SMC\n\napproximation is unbiased, and therefore it can be used together\n\nwith the Metropolis-Hastings algorithm to construct Markov Chains to\n\napproximate the likelihood distribution, or more generally, the\n\nposterior distribution of model parameters. The performance of the\n\nmethodology is assessed using simulation experiments and compared\n\nwith other recently published methods. The proposed estimator is\n\nfound to have a smaller mean square error than the two benchmark\n\nestimators. The proposed method has the additional advantage that\n\nconfidence intervals for the parameters are easily available. We\n\napply the proposed estimator to the analysis of weekly count data on\n\nmeasles cases in Tokyo Japan and compare the results to those by\n\none of the benchmark methods."}, "http://arxiv.org/abs/2401.11119": {"title": "Constraint-based measures of shift and relative shift for discrete frequency distributions", "link": "http://arxiv.org/abs/2401.11119", "description": "Comparisons of frequency distributions often invoke the concept of shift to\ndescribe directional changes in properties such as the mean. In the present\nstudy, we sought to define shift as a property in and of itself. Specifically,\nwe define distributional shift (DS) as the concentration of frequencies away\nfrom the discrete class having the greatest value (e.g., the right-most bin of\na histogram). We derive a measure of DS using the normalized sum of\nexponentiated cumulative frequencies. We then define relative distributional\nshift (RDS) as the difference in DS between two distributions, revealing the\nmagnitude and direction by which one distribution is concentrated to lesser or\ngreater discrete classes relative to another. We find that RDS is highly\nrelated to popular measures that, while based on the comparison of frequency\ndistributions, do not explicitly consider shift. While RDS provides a useful\ncomplement to other comparative measures, DS allows shift to be quantified as a\nproperty of individual distributions, similar in concept to a statistical\nmoment."}, "http://arxiv.org/abs/2401.11128": {"title": "Regularized Estimation of Sparse Spectral Precision Matrices", "link": "http://arxiv.org/abs/2401.11128", "description": "Spectral precision matrix, the inverse of a spectral density matrix, is an\nobject of central interest in frequency-domain analysis of multivariate time\nseries. Estimation of spectral precision matrix is a key step in calculating\npartial coherency and graphical model selection of stationary time series. When\nthe dimension of a multivariate time series is moderate to large, traditional\nestimators of spectral density matrices such as averaged periodograms tend to\nbe severely ill-conditioned, and one needs to resort to suitable regularization\nstrategies involving optimization over complex variables.\n\nIn this work, we propose complex graphical Lasso (CGLASSO), an\n$\\ell_1$-penalized estimator of spectral precision matrix based on local\nWhittle likelihood maximization. We develop fast $\\textit{pathwise coordinate\ndescent}$ algorithms for implementing CGLASSO on large dimensional time series\ndata sets. At its core, our algorithmic development relies on a ring\nisomorphism between complex and real matrices that helps map a number of\noptimization problems over complex variables to similar optimization problems\nover real variables. This finding may be of independent interest and more\nbroadly applicable for high-dimensional statistical analysis with\ncomplex-valued data. We also present a complete non-asymptotic theory of our\nproposed estimator which shows that consistent estimation is possible in\nhigh-dimensional regime as long as the underlying spectral precision matrix is\nsuitably sparse. We compare the performance of CGLASSO with competing\nalternatives on simulated data sets, and use it to construct partial coherence\nnetwork among brain regions from a real fMRI data set."}, "http://arxiv.org/abs/2401.11263": {"title": "Estimating heterogeneous treatment effect from survival outcomes via (orthogonal) censoring unbiased learning", "link": "http://arxiv.org/abs/2401.11263", "description": "Methods for estimating heterogeneous treatment effects (HTE) from\nobservational data have largely focused on continuous or binary outcomes, with\nless attention paid to survival outcomes and almost none to settings with\ncompeting risks. In this work, we develop censoring unbiased transformations\n(CUTs) for survival outcomes both with and without competing risks.After\nconverting time-to-event outcomes using these CUTs, direct application of HTE\nlearners for continuous outcomes yields consistent estimates of heterogeneous\ncumulative incidence effects, total effects, and separable direct effects. Our\nCUTs enable application of a much larger set of state of the art HTE learners\nfor censored outcomes than had previously been available, especially in\ncompeting risks settings. We provide generic model-free learner-specific oracle\ninequalities bounding the finite-sample excess risk. The oracle efficiency\nresults depend on the oracle selector and estimated nuisance functions from all\nsteps involved in the transformation. We demonstrate the empirical performance\nof the proposed methods in simulation studies."}, "http://arxiv.org/abs/2401.11265": {"title": "Assessing the Competitiveness of Matrix-Free Block Likelihood Estimation in Spatial Models", "link": "http://arxiv.org/abs/2401.11265", "description": "In geostatistics, block likelihood offers a balance between statistical\naccuracy and computational efficiency when estimating covariance functions.\nThis balance is reached by dividing the sample into blocks and computing a\nweighted sum of (sub) log-likelihoods corresponding to pairs of blocks.\nPractitioners often choose block sizes ranging from hundreds to a few thousand\nobservations, inherently involving matrix-based implementations. An\nalternative, residing at the opposite end of this methodological spectrum,\ntreats each observation as a block, resulting in the matrix-free pairwise\nlikelihood method. We propose an additional alternative within this broad\nmethodological landscape, systematically constructing blocks of size two and\nmerging pairs of blocks through conditioning. Importantly, our method\nstrategically avoids large-sized blocks, facilitating explicit calculations\nthat ultimately do not rely on matrix computations. Studies with both simulated\nand real data validate the effectiveness of our approach, on one hand\ndemonstrating its superiority over pairwise likelihood, and on the other,\nchallenging the intuitive notion that employing matrix-based versions\nuniversally lead to better statistical performance."}, "http://arxiv.org/abs/2401.11272": {"title": "Asymptotics for non-degenerate multivariate $U$-statistics with estimated nuisance parameters under the null and local alternative hypotheses", "link": "http://arxiv.org/abs/2401.11272", "description": "The large-sample behavior of non-degenerate multivariate $U$-statistics of\narbitrary degree is investigated under the assumption that their kernel depends\non parameters that can be estimated consistently. Mild regularity conditions\nare given which guarantee that once properly normalized, such statistics are\nasymptotically multivariate Gaussian both under the null hypothesis and\nsequences of local alternatives. The work of Randles (1982, Ann. Statist.) is\nextended in three ways: the data and the kernel values can be multivariate\nrather than univariate, the limiting behavior under local alternatives is\nstudied for the first time, and the effect of knowing some of the nuisance\nparameters is quantified. These results can be applied to a broad range of\ngoodness-of-fit testing contexts, as shown in one specific example."}, "http://arxiv.org/abs/2401.11278": {"title": "Handling incomplete outcomes and covariates in cluster-randomized trials: doubly-robust estimation, efficiency considerations, and sensitivity analysis", "link": "http://arxiv.org/abs/2401.11278", "description": "In cluster-randomized trials, missing data can occur in various ways,\nincluding missing values in outcomes and baseline covariates at the individual\nor cluster level, or completely missing information for non-participants. Among\nthe various types of missing data in CRTs, missing outcomes have attracted the\nmost attention. However, no existing method comprehensively addresses all the\naforementioned types of missing data simultaneously due to their complexity.\nThis gap in methodology may lead to confusion and potential pitfalls in the\nanalysis of CRTs. In this article, we propose a doubly-robust estimator for a\nvariety of estimands that simultaneously handles missing outcomes under a\nmissing-at-random assumption, missing covariates with the missing-indicator\nmethod (with no constraint on missing covariate distributions), and missing\ncluster-population sizes via a uniform sampling framework. Furthermore, we\nprovide three approaches to improve precision by choosing the optimal weights\nfor intracluster correlation, leveraging machine learning, and modeling the\npropensity score for treatment assignment. To evaluate the impact of violated\nmissing data assumptions, we additionally propose a sensitivity analysis that\nmeasures when missing data alter the conclusion of treatment effect estimation.\nSimulation studies and data applications both show that our proposed method is\nvalid and superior to the existing methods."}, "http://arxiv.org/abs/2401.11327": {"title": "Measuring hierarchically-organized interactions in dynamic networks through spectral entropy rates: theory, estimation, and illustrative application to physiological networks", "link": "http://arxiv.org/abs/2401.11327", "description": "Recent advances in signal processing and information theory are boosting the\ndevelopment of new approaches for the data-driven modelling of complex network\nsystems. In the fields of Network Physiology and Network Neuroscience where the\nsignals of interest are often rich of oscillatory content, the spectral\nrepresentation of network systems is essential to ascribe the analyzed\ninteractions to specific oscillations with physiological meaning. In this\ncontext, the present work formalizes a coherent framework which integrates\nseveral information dynamics approaches to quantify node-specific, pairwise and\nhigher-order interactions in network systems. The framework establishes a\nhierarchical organization of interactions of different order using measures of\nentropy rate, mutual information rate and O-information rate, to quantify\nrespectively the dynamics of individual nodes, the links between pairs of\nnodes, and the redundant/synergistic hyperlinks between groups of nodes. All\nmeasures are formulated in the time domain, and then expanded to the spectral\ndomain to obtain frequency-specific information. The practical computation of\nall measures is favored presenting a toolbox that implements their parametric\nand non-parametric estimation, and includes approaches to assess their\nstatistical significance. The framework is illustrated first using theoretical\nexamples where the properties of the measures are displayed in benchmark\nsimulated network systems, and then applied to representative examples of\nmultivariate time series in the context of Network Neuroscience and Network\nPhysiology."}, "http://arxiv.org/abs/2401.11328": {"title": "A Hierarchical Decision-Based Maintenance for a Complex Modular System Driven by the { MoMA} Algorithm", "link": "http://arxiv.org/abs/2401.11328", "description": "This paper presents a maintenance policy for a modular system formed by K\nindependent modules (n-subsystems) subjected to environmental conditions\n(shocks). For the modeling of this complex system, the use of the\nMatrix-Analytical Method (MAM) is proposed under a layered approach according\nto its hierarchical structure. Thus, the operational state of the system (top\nlayer) depends on the states of the modules (middle layer), which in turn\ndepend on the states of their components (bottom layer). This allows a detailed\ndescription of the system operation to plan maintenance actions appropriately\nand optimally. We propose a hierarchical decision-based maintenance strategy\nwith periodic inspections as follows: at the time of the inspection, the\ncondition of the system is first evaluated. If intervention is necessary, the\nmodules are then checked to make individual decisions based on their states,\nand so on. Replacement or repair will be carried out as appropriate. An\noptimization problem is formulated as a function of the length of the\ninspection period and the intervention cost incurred over the useful life of\nthe system. Our method shows the advantages, providing compact and\nimplementable expressions. The model is illustrated on a submarine Electrical\nControl Unit (ECU)."}, "http://arxiv.org/abs/2401.11346": {"title": "Estimating Default Probability and Correlation using Stan", "link": "http://arxiv.org/abs/2401.11346", "description": "This work has the objective of estimating default probabilities and\ncorrelations of credit portfolios given default rate information through a\nBayesian framework using Stan. We use Vasicek's single factor credit model to\nestablish the theoretical framework for the behavior of the default rates, and\nuse NUTS Markov Chain Monte Carlo to estimate the parameters. We compare the\nBayesian estimates with classical estimates such as moments estimators and\nmaximum likelihood estimates. We apply the methodology both to simulated data\nand to corporate default rates, and perform inferences through Bayesian methods\nin order to exhibit the advantages of such a framework. We perform default\nforecasting and exhibit the importance of an adequate estimation of default\ncorrelations, and exhibit the advantage of using Stan to perform sampling\nregarding prior choice."}, "http://arxiv.org/abs/2401.11352": {"title": "Geometric Insights and Empirical Observations on Covariate Adjustment and Stratified Randomization in Randomized Clinical Trials", "link": "http://arxiv.org/abs/2401.11352", "description": "The statistical efficiency of randomized clinical trials can be improved by\nincorporating information from baseline covariates (i.e., pre-treatment patient\ncharacteristics). This can be done in the design stage using a\ncovariate-adaptive randomization scheme such as stratified (permutated block)\nrandomization, or in the analysis stage through covariate adjustment. This\narticle provides a geometric perspective on covariate adjustment and stratified\nrandomization in a unified framework where all regular, asymptotically linear\nestimators are identified as augmented estimators. From this perspective,\ncovariate adjustment can be viewed as an effort to approximate the optimal\naugmentation function, and stratified randomization aims to improve a given\napproximation by projecting it into an affine subspace containing the optimal\naugmentation function. The efficiency benefit of stratified randomization is\nasymptotically equivalent to making full use of stratum information in\ncovariate adjustment, which can be achieved using a simple calibration\nprocedure. Simulation results indicate that stratified randomization is clearly\nbeneficial to unadjusted estimators and much less so to adjusted ones and that\ncalibration is an effective way to recover the efficiency benefit of stratified\nrandomization without actually performing stratified randomization. These\ninsights and observations are illustrated using real clinical trial data."}, "http://arxiv.org/abs/2401.11354": {"title": "Squared Wasserstein-2 Distance for Efficient Reconstruction of Stochastic Differential Equations", "link": "http://arxiv.org/abs/2401.11354", "description": "We provide an analysis of the squared Wasserstein-2 ($W_2$) distance between\ntwo probability distributions associated with two stochastic differential\nequations (SDEs). Based on this analysis, we propose the use of a squared $W_2$\ndistance-based loss functions in the \\textit{reconstruction} of SDEs from noisy\ndata. To demonstrate the practicality of our Wasserstein distance-based loss\nfunctions, we performed numerical experiments that demonstrate the efficiency\nof our method in reconstructing SDEs that arise across a number of\napplications."}, "http://arxiv.org/abs/2401.11359": {"title": "The Exact Risks of Reference Panel-based Regularized Estimators", "link": "http://arxiv.org/abs/2401.11359", "description": "Reference panel-based estimators have become widely used in genetic\nprediction of complex traits due to their ability to address data privacy\nconcerns and reduce computational and communication costs. These estimators\nestimate the covariance matrix of predictors using an external reference panel,\ninstead of relying solely on the original training data. In this paper, we\ninvestigate the performance of reference panel-based $L_1$ and $L_2$\nregularized estimators within a unified framework based on approximate message\npassing (AMP). We uncover several key factors that influence the accuracy of\nreference panel-based estimators, including the sample sizes of the training\ndata and reference panels, the signal-to-noise ratio, the underlying sparsity\nof the signal, and the covariance matrix among predictors. Our findings reveal\nthat, even when the sample size of the reference panel matches that of the\ntraining data, reference panel-based estimators tend to exhibit lower accuracy\ncompared to traditional regularized estimators. Furthermore, we observe that\nthis performance gap widens as the amount of training data increases,\nhighlighting the importance of constructing large-scale reference panels to\nmitigate this issue. To support our theoretical analysis, we develop a novel\nnon-separable matrix AMP framework capable of handling the complexities\nintroduced by a general covariance matrix and the additional randomness\nassociated with a reference panel. We validate our theoretical results through\nextensive simulation studies and real data analyses using the UK Biobank\ndatabase."}, "http://arxiv.org/abs/2401.11368": {"title": "When exposure affects subgroup membership: Framing relevant causal questions in perinatal epidemiology and beyond", "link": "http://arxiv.org/abs/2401.11368", "description": "Perinatal epidemiology often aims to evaluate exposures on infant outcomes.\nWhen the exposure affects the composition of people who give birth to live\ninfants (e.g., by affecting fertility, behavior, or birth outcomes), this \"live\nbirth process\" mediates the exposure effect on infant outcomes. Causal\nestimands previously proposed for this setting include the total exposure\neffect on composite birth and infant outcomes, controlled direct effects (e.g.,\nenforcing birth), and principal stratum direct effects. Using perinatal HIV\ntransmission in the SEARCH Study as a motivating example, we present two\nalternative causal estimands: 1) conditional total effects; and 2) conditional\nstochastic direct effects, formulated under a hypothetical intervention to draw\nmediator values from some distribution (possibly conditional on covariates).\nThe proposed conditional total effect includes impacts of an intervention that\noperate by changing the types of people who have a live birth and the timing of\nbirths. The proposed conditional stochastic direct effects isolate the effect\nof an exposure on infant outcomes excluding any impacts through this live birth\nprocess. In SEARCH, this approach quantifies the impact of a universal testing\nand treatment intervention on infant HIV-free survival absent any effect of the\nintervention on the live birth process, within a clearly defined target\npopulation of women of reproductive age with HIV at study baseline. Our\napproach has implications for the evaluation of intervention effects in\nperinatal epidemiology broadly, and whenever causal effects within a subgroup\nare of interest and exposure affects membership in the subgroup."}, "http://arxiv.org/abs/2401.11380": {"title": "MoMA: Model-based Mirror Ascent for Offline Reinforcement Learning", "link": "http://arxiv.org/abs/2401.11380", "description": "Model-based offline reinforcement learning methods (RL) have achieved\nstate-of-the-art performance in many decision-making problems thanks to their\nsample efficiency and generalizability. Despite these advancements, existing\nmodel-based offline RL approaches either focus on theoretical studies without\ndeveloping practical algorithms or rely on a restricted parametric policy\nspace, thus not fully leveraging the advantages of an unrestricted policy space\ninherent to model-based methods. To address this limitation, we develop MoMA, a\nmodel-based mirror ascent algorithm with general function approximations under\npartial coverage of offline data. MoMA distinguishes itself from existing\nliterature by employing an unrestricted policy class. In each iteration, MoMA\nconservatively estimates the value function by a minimization procedure within\na confidence set of transition models in the policy evaluation step, then\nupdates the policy with general function approximations instead of\ncommonly-used parametric policy classes in the policy improvement step. Under\nsome mild assumptions, we establish theoretical guarantees of MoMA by proving\nan upper bound on the suboptimality of the returned policy. We also provide a\npractically implementable, approximate version of the algorithm. The\neffectiveness of MoMA is demonstrated via numerical studies."}, "http://arxiv.org/abs/2401.11507": {"title": "Redundant multiple testing corrections: The fallacy of using family-based error rates to make inferences about individual hypotheses", "link": "http://arxiv.org/abs/2401.11507", "description": "During multiple testing, researchers often adjust their alpha level to\ncontrol the familywise error rate for a statistical inference about a joint\nunion alternative hypothesis (e.g., \"H1 or H2\"). However, in some cases, they\ndo not make this inference and instead make separate inferences about each of\nthe individual hypotheses that comprise the joint hypothesis (e.g., H1 and H2).\nFor example, a researcher might use a Bonferroni correction to adjust their\nalpha level from the conventional level of 0.050 to 0.025 when testing H1 and\nH2, find a significant result for H1 (p &lt; 0.025) and not for H2 (p &gt; .0.025),\nand so claim support for H1 and not for H2. However, these separate individual\ninferences do not require an alpha adjustment. Only a statistical inference\nabout the union alternative hypothesis \"H1 or H2\" requires an alpha adjustment\nbecause it is based on \"at least one\" significant result among the two tests,\nand so it depends on the familywise error rate. When a researcher corrects\ntheir alpha level during multiple testing but does not make an inference about\nthe union alternative hypothesis, their correction is redundant. In the present\narticle, I discuss this redundant correction problem, including its associated\nloss of statistical power and its potential causes vis-\\`a-vis error rate\nconfusions and the alpha adjustment ritual. I also provide three illustrations\nof redundant corrections from recent psychology studies. I conclude that\nredundant corrections represent a symptom of statisticism, and I call for a\nmore nuanced and context-specific approach to multiple testing corrections."}, "http://arxiv.org/abs/2401.11515": {"title": "Geometry-driven Bayesian Inference for Ultrametric Covariance Matrices", "link": "http://arxiv.org/abs/2401.11515", "description": "Ultrametric matrices arise as covariance matrices in latent tree models for\nmultivariate data with hierarchically correlated components. As a parameter\nspace in a model, the set of ultrametric matrices is neither convex nor a\nsmooth manifold, and focus in literature has hitherto mainly been restricted to\nestimation through projections and relaxation-based techniques. Leveraging the\nlink between an ultrametric matrix and a rooted tree, we equip the set of\nultrametric matrices with a convenient geometry based on the well-known\ngeometry of phylogenetic trees, whose attractive properties (e.g. unique\ngeodesics and Fr\\'{e}chet means) the set of ultrametric matrices inherits. This\nresults in a novel representation of an ultrametric matrix by coordinates of\nthe tree space, which we then use to define a class of Markovian and consistent\nprior distributions on the set of ultrametric matrices in a Bayesian model, and\ndevelop an efficient algorithm to sample from the posterior distribution that\ngenerates updates by making intrinsic local moves along geodesics within the\nset of ultrametric matrices. In simulation studies, our proposed algorithm\nrestores the underlying matrices with posterior samples that recover the tree\ntopology with a high frequency of true topology and generate element-wise\ncredible intervals with a high nominal coverage rate. We use the proposed\nalgorithm on the pre-clinical cancer data to investigate the mechanism\nsimilarity by constructing the underlying treatment tree and identify\ntreatments with high mechanism similarity also target correlated pathways in\nbiological literature."}, "http://arxiv.org/abs/2401.11537": {"title": "Addressing researcher degrees of freedom through minP adjustment", "link": "http://arxiv.org/abs/2401.11537", "description": "When different researchers study the same research question using the same\ndataset they may obtain different and potentially even conflicting results.\nThis is because there is often substantial flexibility in researchers'\nanalytical choices, an issue also referred to as ''researcher degrees of\nfreedom''. Combined with selective reporting of the smallest p-value or largest\neffect, researcher degrees of freedom may lead to an increased rate of false\npositive and overoptimistic results. In this paper, we address this issue by\nformalizing the multiplicity of analysis strategies as a multiple testing\nproblem. As the test statistics of different analysis strategies are usually\nhighly dependent, a naive approach such as the Bonferroni correction is\ninappropriate because it leads to an unacceptable loss of power. Instead, we\npropose using the ''minP'' adjustment method, which takes potential test\ndependencies into account and approximates the underlying null distribution of\nthe minimal p-value through a permutation-based procedure. This procedure is\nknown to achieve more power than simpler approaches while ensuring a weak\ncontrol of the family-wise error rate. We illustrate our approach for\naddressing researcher degrees of freedom by applying it to a study on the\nimpact of perioperative paO2 on post-operative complications after\nneurosurgery. A total of 48 analysis strategies are considered and adjusted\nusing the minP procedure. This approach allows to selectively report the result\nof the analysis strategy yielding the most convincing evidence, while\ncontrolling the type 1 error -- and thus the risk of publishing false positive\nresults that may not be replicable."}, "http://arxiv.org/abs/2401.11540": {"title": "A new flexible class of kernel-based tests of independence", "link": "http://arxiv.org/abs/2401.11540", "description": "Spherical and hyperspherical data are commonly encountered in diverse applied\nresearch domains, underscoring the vital task of assessing independence within\nsuch data structures. In this context, we investigate the properties of test\nstatistics relying on distance correlation measures originally introduced for\nthe energy distance, and generalize the concept to strongly negative definite\nkernel-based distances. An important benefit of employing this method lies in\nits versatility across diverse forms of directional data, enabling the\nexamination of independence among vectors of varying types. The applicability\nof tests is demonstrated on several real datasets."}, "http://arxiv.org/abs/2401.11646": {"title": "Nonparametric Estimation via Variance-Reduced Sketching", "link": "http://arxiv.org/abs/2401.11646", "description": "Nonparametric models are of great interest in various scientific and\nengineering disciplines. Classical kernel methods, while numerically robust and\nstatistically sound in low-dimensional settings, become inadequate in\nhigher-dimensional settings due to the curse of dimensionality. In this paper,\nwe introduce a new framework called Variance-Reduced Sketching (VRS),\nspecifically designed to estimate density functions and nonparametric\nregression functions in higher dimensions with a reduced curse of\ndimensionality. Our framework conceptualizes multivariable functions as\ninfinite-size matrices, and facilitates a new sketching technique motivated by\nnumerical linear algebra literature to reduce the variance in estimation\nproblems. We demonstrate the robust numerical performance of VRS through a\nseries of simulated experiments and real-world data applications. Notably, VRS\nshows remarkable improvement over existing neural network estimators and\nclassical kernel methods in numerous density estimation and nonparametric\nregression models. Additionally, we offer theoretical justifications for VRS to\nsupport its ability to deliver nonparametric estimation with a reduced curse of\ndimensionality."}, "http://arxiv.org/abs/2401.11789": {"title": "Stein EWMA Control Charts for Count Processes", "link": "http://arxiv.org/abs/2401.11789", "description": "The monitoring of serially independent or autocorrelated count processes is\nconsidered, having a Poisson or (negative) binomial marginal distribution under\nin-control conditions. Utilizing the corresponding Stein identities,\nexponentially weighted moving-average (EWMA) control charts are constructed,\nwhich can be flexibly adapted to uncover zero inflation, over- or\nunderdispersion. The proposed Stein EWMA charts' performance is investigated by\nsimulations, and their usefulness is demonstrated by a real-world data example\nfrom health surveillance."}, "http://arxiv.org/abs/2401.11804": {"title": "Regression Copulas for Multivariate Responses", "link": "http://arxiv.org/abs/2401.11804", "description": "We propose a novel distributional regression model for a multivariate\nresponse vector based on a copula process over the covariate space. It uses the\nimplicit copula of a Gaussian multivariate regression, which we call a\n``regression copula''. To allow for large covariate vectors their coefficients\nare regularized using a novel multivariate extension of the horseshoe prior.\nBayesian inference and distributional predictions are evaluated using efficient\nvariational inference methods, allowing application to large datasets. An\nadvantage of the approach is that the marginal distributions of the response\nvector can be estimated separately and accurately, resulting in predictive\ndistributions that are marginally-calibrated. Two substantive applications of\nthe methodology highlight its efficacy in multivariate modeling. The first is\nthe econometric modeling and prediction of half-hourly regional Australian\nelectricity prices. Here, our approach produces more accurate distributional\nforecasts than leading benchmark methods. The second is the evaluation of\nmultivariate posteriors in likelihood-free inference (LFI) of a model for tree\nspecies abundance data, extending a previous univariate regression copula LFI\nmethod. In both applications, we demonstrate that our new approach exhibits a\ndesirable marginal calibration property."}, "http://arxiv.org/abs/2401.11827": {"title": "Flexible Models for Simple Longitudinal Data", "link": "http://arxiv.org/abs/2401.11827", "description": "We propose a new method for estimating subject-specific mean functions from\nlongitudinal data. We aim to do this in a flexible manner (without restrictive\nassumptions about the shape of the subject-specific mean functions), while\nexploiting similarities in the mean functions between different subjects.\nFunctional principal components analysis fulfils both requirements, and methods\nfor functional principal components analysis have been developed for\nlongitudinal data. However, we find that these existing methods sometimes give\nfitted mean functions which are more complex than needed to provide a good fit\nto the data. We develop a new penalised likelihood approach to flexibly model\nlongitudinal data, with a penalty term to control the balance between fit to\nthe data and smoothness of the subject-specific mean curves. We run simulation\nstudies to demonstrate that the new method substantially improves the quality\nof inference relative to existing methods across a range of examples, and apply\nthe method to data on changes in body composition in adolescent girls."}, "http://arxiv.org/abs/2401.11842": {"title": "Subgroup analysis methods for time-to-event outcomes in heterogeneous randomized controlled trials", "link": "http://arxiv.org/abs/2401.11842", "description": "Non-significant randomized control trials can hide subgroups of good\nresponders to experimental drugs, thus hindering subsequent development.\nIdentifying such heterogeneous treatment effects is key for precision medicine\nand many post-hoc analysis methods have been developed for that purpose. While\nseveral benchmarks have been carried out to identify the strengths and\nweaknesses of these methods, notably for binary and continuous endpoints,\nsimilar systematic empirical evaluation of subgroup analysis for time-to-event\nendpoints are lacking. This work aims to fill this gap by evaluating several\nsubgroup analysis algorithms in the context of time-to-event outcomes, by means\nof three different research questions: Is there heterogeneity? What are the\nbiomarkers responsible for such heterogeneity? Who are the good responders to\ntreatment? In this context, we propose a new synthetic and semi-synthetic data\ngeneration process that allows one to explore a wide range of heterogeneity\nscenarios with precise control on the level of heterogeneity. We provide an\nopen source Python package, available on Github, containing our generation\nprocess and our comprehensive benchmark framework. We hope this package will be\nuseful to the research community for future investigations of heterogeneity of\ntreatment effects and subgroup analysis methods benchmarking."}, "http://arxiv.org/abs/2401.11885": {"title": "Bootstrap prediction regions for daily curves of electricity demand and price using functional data", "link": "http://arxiv.org/abs/2401.11885", "description": "The aim of this paper is to compute one-day-ahead prediction regions for\ndaily curves of electricity demand and price. Three model-based procedures to\nconstruct general prediction regions are proposed, all of them using bootstrap\nalgorithms. The first proposed method considers any $L_p$ norm for functional\ndata to measure the distance between curves, the second one is designed to take\ndifferent variabilities along the curve into account, and the third one takes\nadvantage of the notion of depth of a functional data. The regression model\nwith functional response on which our proposed prediction regions are based is\nrather general: it allows to include both endogenous and exogenous functional\nvariables, as well as exogenous scalar variables; in addition, the effect of\nsuch variables on the response one is modeled in a parametric, nonparametric or\nsemi-parametric way. A comparative study is carried out to analyse the\nperformance of these prediction regions for the electricity market of mainland\nSpain, in year 2012. This work extends and complements the methods and results\nin Aneiros et al. (2016) (focused on curve prediction) and Vilar et al. (2018)\n(focused on prediction intervals), which use the same database as here."}, "http://arxiv.org/abs/2401.11948": {"title": "The Ensemble Kalman Filter for Dynamic Inverse Problems", "link": "http://arxiv.org/abs/2401.11948", "description": "In inverse problems, the goal is to estimate unknown model parameters from\nnoisy observational data. Traditionally, inverse problems are solved under the\nassumption of a fixed forward operator describing the observation model. In\nthis article, we consider the extension of this approach to situations where we\nhave a dynamic forward model, motivated by applications in scientific\ncomputation and engineering. We specifically consider this extension for a\nderivative-free optimizer, the ensemble Kalman inversion (EKI). We introduce\nand justify a new methodology called dynamic-EKI, which is a particle-based\nmethod with a changing forward operator. We analyze our new method, presenting\nresults related to the control of our particle system through its covariance\nstructure. This analysis includes moment bounds and an ensemble collapse, which\nare essential for demonstrating a convergence result. We establish convergence\nin expectation and validate our theoretical findings through experiments with\ndynamic-EKI applied to a 2D Darcy flow partial differential equation."}, "http://arxiv.org/abs/2401.12031": {"title": "Multi-objective optimisation using expected quantile improvement for decision making in disease outbreaks", "link": "http://arxiv.org/abs/2401.12031", "description": "Optimization under uncertainty is important in many applications,\nparticularly to inform policy and decision making in areas such as public\nhealth. A key source of uncertainty arises from the incorporation of\nenvironmental variables as inputs into computational models or simulators. Such\nvariables represent uncontrollable features of the optimization problem and\nreliable decision making must account for the uncertainty they propagate to the\nsimulator outputs. Often, multiple, competing objectives are defined from these\noutputs such that the final optimal decision is a compromise between different\ngoals.\n\nHere, we present emulation-based optimization methodology for such problems\nthat extends expected quantile improvement (EQI) to address multi-objective\noptimization. Focusing on the practically important case of two objectives, we\nuse a sequential design strategy to identify the Pareto front of optimal\nsolutions. Uncertainty from the environmental variables is integrated out using\nMonte Carlo samples from the simulator. Interrogation of the expected output\nfrom the simulator is facilitated by use of (Gaussian process) emulators. The\nmethodology is demonstrated on an optimization problem from public health\ninvolving the dispersion of anthrax spores across a spatial terrain.\nEnvironmental variables include meteorological features that impact the\ndispersion, and the methodology identifies the Pareto front even when there is\nconsiderable input uncertainty."}, "http://arxiv.org/abs/2401.12084": {"title": "Temporal Aggregation for the Synthetic Control Method", "link": "http://arxiv.org/abs/2401.12084", "description": "The synthetic control method (SCM) is a popular approach for estimating the\nimpact of a treatment on a single unit with panel data. Two challenges arise\nwith higher frequency data (e.g., monthly versus yearly): (1) achieving\nexcellent pre-treatment fit is typically more challenging; and (2) overfitting\nto noise is more likely. Aggregating data over time can mitigate these problems\nbut can also destroy important signal. In this paper, we bound the bias for SCM\nwith disaggregated and aggregated outcomes and give conditions under which\naggregating tightens the bounds. We then propose finding weights that balance\nboth disaggregated and aggregated series."}, "http://arxiv.org/abs/2401.12126": {"title": "Biological species delimitation based on genetic and spatial dissimilarity: a comparative study", "link": "http://arxiv.org/abs/2401.12126", "description": "The delimitation of biological species, i.e., deciding which individuals\nbelong to the same species and whether and how many different species are\nrepresented in a data set, is key to the conservation of biodiversity. Much\nexisting work uses only genetic data for species delimitation, often employing\nsome kind of cluster analysis. This can be misleading, because geographically\ndistant groups of individuals can be genetically quite different even if they\nbelong to the same species. This paper investigates the problem of testing\nwhether two potentially separated groups of individuals can belong to a single\nspecies or not based on genetic and spatial data. Various approaches are\ncompared (some of which already exist in the literature) based on simulated\nmetapopulations generated with SLiM and GSpace - two software packages that can\nsimulate spatially-explicit genetic data at an individual level. Approaches\ninvolve partial Mantel testing, maximum likelihood mixed-effects models with a\npopulation effect, and jackknife-based homogeneity tests. A key challenge is\nthat most tests perform on genetic and geographical distance data, violating\nstandard independence assumptions. Simulations showed that partial Mantel tests\nand mixed-effects models have larger power than jackknife-based methods, but\ntend to display type-I-error rates slightly above the significance level.\nMoreover, a multiple regression model neglecting the dependence in the\ndissimilarities did not show inflated type-I-error rate. An application on\nbrassy ringlets concludes the paper."}, "http://arxiv.org/abs/1412.0367": {"title": "Bayesian nonparametric modeling for mean residual life regression", "link": "http://arxiv.org/abs/1412.0367", "description": "The mean residual life function is a key functional for a survival\ndistribution. It has practically useful interpretation as the expected\nremaining lifetime given survival up to a particular time point, and it also\ncharacterizes the survival distribution. However, it has received limited\nattention in terms of inference methods under a probabilistic modeling\nframework. In this paper, we seek to provide general inference methodology for\nmean residual life regression. Survival data often include a set of predictor\nvariables for the survival response distribution, and in many cases it is\nnatural to include the covariates as random variables into the modeling. We\nthus propose a Dirichlet process mixture modeling approach for the joint\nstochastic mechanism of the covariates and survival responses. This approach\nimplies a flexible model structure for the mean residual life of the\nconditional response distribution, allowing general shapes for mean residual\nlife as a function of covariates given a specific time point, as well as a\nfunction of time given particular values of the covariate vector. To expand the\nscope of the modeling framework, we extend the mixture model to incorporate\ndependence across experimental groups, such as treatment and control groups.\nThis extension is built from a dependent Dirichlet process prior for the\ngroup-specific mixing distributions, with common locations and weights that\nvary across groups through latent bivariate beta distributed random variables.\nWe develop properties of the proposed regression models, and discuss methods\nfor prior specification and posterior inference. The different components of\nthe methodology are illustrated with simulated data sets. Moreover, the\nmodeling approach is applied to a data set comprising right censored survival\ntimes of patients with small cell lung cancer."}, "http://arxiv.org/abs/2110.00314": {"title": "Confounder importance learning for treatment effect inference", "link": "http://arxiv.org/abs/2110.00314", "description": "We address modelling and computational issues for multiple treatment effect\ninference under many potential confounders. A primary issue relates to\npreventing harmful effects from omitting relevant covariates (under-selection),\nwhile not running into over-selection issues that introduce substantial\nvariance and a bias related to the non-random over-inclusion of covariates. We\npropose a novel empirical Bayes framework for Bayesian model averaging that\nlearns from data the extent to which the inclusion of key covariates should be\nencouraged, specifically those highly associated to the treatments. A key\nchallenge is computational. We develop fast algorithms, including an\nExpectation-Propagation variational approximation and simple stochastic\ngradient optimization algorithms, to learn the hyper-parameters from data. Our\nframework uses widely-used ingredients and largely existing software, and it is\nimplemented within the R package mombf featured on CRAN. This work is motivated\nby and is illustrated in two applications. The first is the association between\nsalary variation and discriminatory factors. The second, that has been debated\nin previous works, is the association between abortion policies and crime. Our\napproach provides insights that differ from previous analyses especially in\nsituations with weaker treatment effects."}, "http://arxiv.org/abs/2201.12865": {"title": "Extremal Random Forests", "link": "http://arxiv.org/abs/2201.12865", "description": "Classical methods for quantile regression fail in cases where the quantile of\ninterest is extreme and only few or no training data points exceed it.\nAsymptotic results from extreme value theory can be used to extrapolate beyond\nthe range of the data, and several approaches exist that use linear regression,\nkernel methods or generalized additive models. Most of these methods break down\nif the predictor space has more than a few dimensions or if the regression\nfunction of extreme quantiles is complex. We propose a method for extreme\nquantile regression that combines the flexibility of random forests with the\ntheory of extrapolation. Our extremal random forest (ERF) estimates the\nparameters of a generalized Pareto distribution, conditional on the predictor\nvector, by maximizing a local likelihood with weights extracted from a quantile\nrandom forest. We penalize the shape parameter in this likelihood to regularize\nits variability in the predictor space. Under general domain of attraction\nconditions, we show consistency of the estimated parameters in both the\nunpenalized and penalized case. Simulation studies show that our ERF\noutperforms both classical quantile regression methods and existing regression\napproaches from extreme value theory. We apply our methodology to extreme\nquantile prediction for U.S. wage data."}, "http://arxiv.org/abs/2203.00144": {"title": "The Concordance Index decomposition: A measure for a deeper understanding of survival prediction models", "link": "http://arxiv.org/abs/2203.00144", "description": "The Concordance Index (C-index) is a commonly used metric in Survival\nAnalysis for evaluating the performance of a prediction model. In this paper,\nwe propose a decomposition of the C-index into a weighted harmonic mean of two\nquantities: one for ranking observed events versus other observed events, and\nthe other for ranking observed events versus censored cases. This decomposition\nenables a finer-grained analysis of the relative strengths and weaknesses\nbetween different survival prediction methods. The usefulness of this\ndecomposition is demonstrated through benchmark comparisons against classical\nmodels and state-of-the-art methods, together with the new variational\ngenerative neural-network-based method (SurVED) proposed in this paper. The\nperformance of the models is assessed using four publicly available datasets\nwith varying levels of censoring. Using the C-index decomposition and synthetic\ncensoring, the analysis shows that deep learning models utilize the observed\nevents more effectively than other models. This allows them to keep a stable\nC-index in different censoring levels. In contrast to such deep learning\nmethods, classical machine learning models deteriorate when the censoring level\ndecreases due to their inability to improve on ranking the events versus other\nevents."}, "http://arxiv.org/abs/2211.01799": {"title": "Statistical Inference for Scale Mixture Models via Mellin Transform Approach", "link": "http://arxiv.org/abs/2211.01799", "description": "This paper deals with statistical inference for the scale mixture models. We\nstudy an estimation approach based on the Mellin -- Stieltjes transform that\ncan be applied to both discrete and absolute continuous mixing distributions.\nThe accuracy of the corresponding estimate is analysed in terms of its expected\npointwise error. As an important technical result, we prove the analogue of the\nBerry -- Esseen inequality for the Mellin transforms. The proposed statistical\napproach is illustrated by numerical examples."}, "http://arxiv.org/abs/2211.16155": {"title": "High-Dimensional Block Diagonal Covariance Structure Detection Using Singular Vectors", "link": "http://arxiv.org/abs/2211.16155", "description": "The assumption of independent subvectors arises in many aspects of\nmultivariate analysis. In most real-world applications, however, we lack prior\nknowledge about the number of subvectors and the specific variables within each\nsubvector. Yet, testing all these combinations is not feasible. For example,\nfor a data matrix containing 15 variables, there are already 1 382 958 545\npossible combinations. Given that zero correlation is a necessary condition for\nindependence, independent subvectors exhibit a block diagonal covariance\nmatrix. This paper focuses on the detection of such block diagonal covariance\nstructures in high-dimensional data and therefore also identifies uncorrelated\nsubvectors. Our nonparametric approach exploits the fact that the structure of\nthe covariance matrix is mirrored by the structure of its eigenvectors.\nHowever, the true block diagonal structure is masked by noise in the sample\ncase. To address this problem, we propose to use sparse approximations of the\nsample eigenvectors to reveal the sparse structure of the population\neigenvectors. Notably, the right singular vectors of a data matrix with an\noverall mean of zero are identical to the sample eigenvectors of its covariance\nmatrix. Using sparse approximations of these singular vectors instead of the\neigenvectors makes the estimation of the covariance matrix obsolete. We\ndemonstrate the performance of our method through simulations and provide real\ndata examples. Supplementary materials for this article are available online."}, "http://arxiv.org/abs/2303.05659": {"title": "A marginal structural model for normal tissue complication probability", "link": "http://arxiv.org/abs/2303.05659", "description": "The goal of radiation therapy for cancer is to deliver prescribed radiation\ndose to the tumor while minimizing dose to the surrounding healthy tissues. To\nevaluate treatment plans, the dose distribution to healthy organs is commonly\nsummarized as dose-volume histograms (DVHs). Normal tissue complication\nprobability (NTCP) modelling has centered around making patient-level risk\npredictions with features extracted from the DVHs, but few have considered\nadapting a causal framework to evaluate the safety of alternative treatment\nplans. We propose causal estimands for NTCP based on deterministic and\nstochastic interventions, as well as propose estimators based on marginal\nstructural models that impose bivariable monotonicity between dose, volume, and\ntoxicity risk. The properties of these estimators are studied through\nsimulations, and their use is illustrated in the context of radiotherapy\ntreatment of anal canal cancer patients."}, "http://arxiv.org/abs/2305.04141": {"title": "Geostatistical capture-recapture models", "link": "http://arxiv.org/abs/2305.04141", "description": "Methods for population estimation and inference have evolved over the past\ndecade to allow for the incorporation of spatial information when using\ncapture-recapture study designs. Traditional approaches to specifying spatial\ncapture-recapture (SCR) models often rely on an individual-based detection\nfunction that decays as a detection location is farther from an individual's\nactivity center. Traditional SCR models are intuitive because they incorporate\nmechanisms of animal space use based on their assumptions about activity\ncenters. We modify the SCR model to accommodate a wide range of space use\npatterns, including for those individuals that may exhibit traditional\nelliptical utilization distributions. Our approach uses underlying Gaussian\nprocesses to characterize the space use of individuals. This allows us to\naccount for multimodal and other complex space use patterns that may arise due\nto movement. We refer to this class of models as geostatistical\ncapture-recapture (GCR) models. We adapt a recursive computing strategy to fit\nGCR models to data in stages, some of which can be parallelized. This technique\nfacilitates implementation and leverages modern multicore and distributed\ncomputing environments. We demonstrate the application of GCR models by\nanalyzing both simulated data and a data set involving capture histories of\nsnowshoe hares in central Colorado, USA."}, "http://arxiv.org/abs/2308.03355": {"title": "Nonparametric Bayes multiresolution testing for high-dimensional rare events", "link": "http://arxiv.org/abs/2308.03355", "description": "In a variety of application areas, there is interest in assessing evidence of\ndifferences in the intensity of event realizations between groups. For example,\nin cancer genomic studies collecting data on rare variants, the focus is on\nassessing whether and how the variant profile changes with the disease subtype.\nMotivated by this application, we develop multiresolution nonparametric Bayes\ntests for differential mutation rates across groups. The multiresolution\napproach yields fast and accurate detection of spatial clusters of rare\nvariants, and our nonparametric Bayes framework provides great flexibility for\nmodeling the intensities of rare variants. Some theoretical properties are also\nassessed, including weak consistency of our Dirichlet Process-Poisson-Gamma\nmixture over multiple resolutions. Simulation studies illustrate excellent\nsmall sample properties relative to competitors, and we apply the method to\ndetect rare variants related to common variable immunodeficiency from whole\nexome sequencing data on 215 patients and over 60,027 control subjects."}, "http://arxiv.org/abs/2309.15316": {"title": "Leveraging Neural Networks to Profile Health Care Providers with Application to Medicare Claims", "link": "http://arxiv.org/abs/2309.15316", "description": "Encompassing numerous nationwide, statewide, and institutional initiatives in\nthe United States, provider profiling has evolved into a major health care\nundertaking with ubiquitous applications, profound implications, and\nhigh-stakes consequences. In line with such a significant profile, the\nliterature has accumulated a number of developments dedicated to enhancing the\nstatistical paradigm of provider profiling. Tackling wide-ranging profiling\nissues, these methods typically adjust for risk factors using linear\npredictors. While this approach is simple, it can be too restrictive to\ncharacterize complex and dynamic factor-outcome associations in certain\ncontexts. One such example arises from evaluating dialysis facilities treating\nMedicare beneficiaries with end-stage renal disease. It is of primary interest\nto consider how the coronavirus disease (COVID-19) affected 30-day unplanned\nreadmissions in 2020. The impact of COVID-19 on the risk of readmission varied\ndramatically across pandemic phases. To efficiently capture the variation while\nprofiling facilities, we develop a generalized partially linear model (GPLM)\nthat incorporates a neural network. Considering provider-level clustering, we\nimplement the GPLM as a stratified sampling-based stochastic optimization\nalgorithm that features accelerated convergence. Furthermore, an exact test is\ndesigned to identify under- and over-performing facilities, with an\naccompanying funnel plot to visualize profiles. The advantages of the proposed\nmethods are demonstrated through simulation experiments and profiling dialysis\nfacilities using 2020 Medicare claims from the United States Renal Data System."}, "http://arxiv.org/abs/2401.12309": {"title": "Interpreting Event-Studies from Recent Difference-in-Differences Methods", "link": "http://arxiv.org/abs/2401.12309", "description": "This note discusses the interpretation of event-study plots produced by\nrecent difference-in-differences methods. I show that even when specialized to\nthe case of non-staggered treatment timing, the default plots produced by\nsoftware for three of the most popular recent methods (de Chaisemartin and\nD'Haultfoeuille, 2020; Callaway and SantAnna, 2021; Borusyak, Jaravel and\nSpiess, 2024) do not match those of traditional two-way fixed effects (TWFE)\nevent-studies: the new methods may show a kink or jump at the time of treatment\neven when the TWFE event-study shows a straight line. This difference stems\nfrom the fact that the new methods construct the pre-treatment coefficients\nasymmetrically from the post-treatment coefficients. As a result, visual\nheuristics for analyzing TWFE event-study plots should not be immediately\napplied to those from these methods. I conclude with practical recommendations\nfor constructing and interpreting event-study plots when using these methods."}, "http://arxiv.org/abs/2401.12369": {"title": "SubgroupTE: Advancing Treatment Effect Estimation with Subgroup Identification", "link": "http://arxiv.org/abs/2401.12369", "description": "Precise estimation of treatment effects is crucial for evaluating\nintervention effectiveness. While deep learning models have exhibited promising\nperformance in learning counterfactual representations for treatment effect\nestimation (TEE), a major limitation in most of these models is that they treat\nthe entire population as a homogeneous group, overlooking the diversity of\ntreatment effects across potential subgroups that have varying treatment\neffects. This limitation restricts the ability to precisely estimate treatment\neffects and provide subgroup-specific treatment recommendations. In this paper,\nwe propose a novel treatment effect estimation model, named SubgroupTE, which\nincorporates subgroup identification in TEE. SubgroupTE identifies\nheterogeneous subgroups with different treatment responses and more precisely\nestimates treatment effects by considering subgroup-specific causal effects. In\naddition, SubgroupTE iteratively optimizes subgrouping and treatment effect\nestimation networks to enhance both estimation and subgroup identification.\nComprehensive experiments on the synthetic and semi-synthetic datasets exhibit\nthe outstanding performance of SubgroupTE compared with the state-of-the-art\nmodels on treatment effect estimation. Additionally, a real-world study\ndemonstrates the capabilities of SubgroupTE in enhancing personalized treatment\nrecommendations for patients with opioid use disorder (OUD) by advancing\ntreatment effect estimation with subgroup identification."}, "http://arxiv.org/abs/2401.12420": {"title": "Rank-based estimators of global treatment effects for cluster randomized trials with multiple endpoints", "link": "http://arxiv.org/abs/2401.12420", "description": "Cluster randomization trials commonly employ multiple endpoints. When a\nsingle summary of treatment effects across endpoints is of primary interest,\nglobal hypothesis testing/effect estimation methods represent a common analysis\nstrategy. However, specification of the joint distribution required by these\nmethods is non-trivial, particularly when endpoint properties differ. We\ndevelop rank-based interval estimators for a global treatment effect referred\nto as the \"global win probability,\" or the probability that a treatment\nindividual responds better than a control individual on average. Using\nendpoint-specific ranks among the combined sample and within each arm, each\nindividual-level observation is converted to a \"win fraction\" which quantifies\nthe proportion of wins experienced over every observation in the comparison\narm. An individual's multiple observations are then replaced by a single\n\"global win fraction,\" constructed by averaging win fractions across endpoints.\nA linear mixed model is applied directly to the global win fractions to recover\npoint, variance, and interval estimates of the global win probability adjusted\nfor clustering. Simulation demonstrates our approach performs well concerning\ncoverage and type I error, and methods are easily implemented using standard\nsoftware. A case study using publicly available data is provided with\ncorresponding R and SAS code."}, "http://arxiv.org/abs/2401.12640": {"title": "Multilevel network meta-regression for general likelihoods: synthesis of individual and aggregate data with applications to survival analysis", "link": "http://arxiv.org/abs/2401.12640", "description": "Network meta-analysis combines aggregate data (AgD) from multiple randomised\ncontrolled trials, assuming that any effect modifiers are balanced across\npopulations. Individual patient data (IPD) meta-regression is the ``gold\nstandard'' method to relax this assumption, however IPD are frequently only\navailable in a subset of studies. Multilevel network meta-regression (ML-NMR)\nextends IPD meta-regression to incorporate AgD studies whilst avoiding\naggregation bias, but currently requires the aggregate-level likelihood to have\na known closed form. Notably, this prevents application to time-to-event\noutcomes.\n\nWe extend ML-NMR to individual-level likelihoods of any form, by integrating\nthe individual-level likelihood function over the AgD covariate distributions\nto obtain the respective marginal likelihood contributions. We illustrate with\ntwo examples of time-to-event outcomes, showing the performance of ML-NMR in a\nsimulated comparison with little loss of precision from a full IPD analysis,\nand demonstrating flexible modelling of baseline hazards using cubic M-splines\nwith synthetic data on newly diagnosed multiple myeloma.\n\nML-NMR is a general method for synthesising individual and aggregate level\ndata in networks of all sizes. Extension to general likelihoods, including for\nsurvival outcomes, greatly increases the applicability of the method. R and\nStan code is provided, and the methods are implemented in the multinma R\npackage."}, "http://arxiv.org/abs/2401.12697": {"title": "A Computationally Efficient Approach to False Discovery Rate Control and Power Maximisation via Randomisation and Mirror Statistic", "link": "http://arxiv.org/abs/2401.12697", "description": "Simultaneously performing variable selection and inference in\nhigh-dimensional regression models is an open challenge in statistics and\nmachine learning. The increasing availability of vast amounts of variables\nrequires the adoption of specific statistical procedures to accurately select\nthe most important predictors in a high-dimensional space, while controlling\nthe False Discovery Rate (FDR) arising from the underlying multiple hypothesis\ntesting. In this paper we propose the joint adoption of the Mirror Statistic\napproach to FDR control, coupled with outcome randomisation to maximise the\nstatistical power of the variable selection procedure. Through extensive\nsimulations we show how our proposed strategy allows to combine the benefits of\nthe two techniques. The Mirror Statistic is a flexible method to control FDR,\nwhich only requires mild model assumptions, but requires two sets of\nindependent regression coefficient estimates, usually obtained after splitting\nthe original dataset. Outcome randomisation is an alternative to Data\nSplitting, that allows to generate two independent outcomes, which can then be\nused to estimate the coefficients that go into the construction of the Mirror\nStatistic. The combination of these two approaches provides increased testing\npower in a number of scenarios, such as highly correlated covariates and high\npercentages of active variables. Moreover, it is scalable to very\nhigh-dimensional problems, since the algorithm has a low memory footprint and\nonly requires a single run on the full dataset, as opposed to iterative\nalternatives such as Multiple Data Splitting."}, "http://arxiv.org/abs/2401.12753": {"title": "Optimal Confidence Bands for Shape-restricted Regression in Multidimensions", "link": "http://arxiv.org/abs/2401.12753", "description": "In this paper, we propose and study construction of confidence bands for\nshape-constrained regression functions when the predictor is multivariate. In\nparticular, we consider the continuous multidimensional white noise model given\nby $d Y(\\mathbf{t}) = n^{1/2} f(\\mathbf{t}) \\,d\\mathbf{t} + d W(\\mathbf{t})$,\nwhere $Y$ is the observed stochastic process on $[0,1]^d$ ($d\\ge 1$), $W$ is\nthe standard Brownian sheet on $[0,1]^d$, and $f$ is the unknown function of\ninterest assumed to belong to a (shape-constrained) function class, e.g.,\ncoordinate-wise monotone functions or convex functions. The constructed\nconfidence bands are based on local kernel averaging with bandwidth chosen\nautomatically via a multivariate multiscale statistic. The confidence bands\nhave guaranteed coverage for every $n$ and for every member of the underlying\nfunction class. Under monotonicity/convexity constraints on $f$, the proposed\nconfidence bands automatically adapt (in terms of width) to the global and\nlocal (H\\\"{o}lder) smoothness and intrinsic dimensionality of the unknown $f$;\nthe bands are also shown to be optimal in a certain sense. These bands have\n(almost) parametric ($n^{-1/2}$) widths when the underlying function has\n``low-complexity'' (e.g., piecewise constant/affine)."}, "http://arxiv.org/abs/2401.12776": {"title": "Sub-model aggregation for scalable eigenvector spatial filtering: Application to spatially varying coefficient modeling", "link": "http://arxiv.org/abs/2401.12776", "description": "This study proposes a method for aggregating/synthesizing global and local\nsub-models for fast and flexible spatial regression modeling. Eigenvector\nspatial filtering (ESF) was used to model spatially varying coefficients and\nspatial dependence in the residuals by sub-model, while the generalized\nproduct-of-experts method was used to aggregate these sub-models. The major\nadvantages of the proposed method are as follows: (i) it is highly scalable for\nlarge samples in terms of accuracy and computational efficiency; (ii) it is\neasily implemented by estimating sub-models independently first and\naggregating/averaging them thereafter; and (iii) likelihood-based inference is\navailable because the marginal likelihood is available in closed-form. The\naccuracy and computational efficiency of the proposed method are confirmed\nusing Monte Carlo simulation experiments. This method was then applied to\nresidential land price analysis in Japan. The results demonstrate the\nusefulness of this method for improving the interpretability of spatially\nvarying coefficients. The proposed method is implemented in an R package\nspmoran (version 0.3.0 or later)."}, "http://arxiv.org/abs/2401.12827": {"title": "Distributed Empirical Likelihood Inference With or Without Byzantine Failures", "link": "http://arxiv.org/abs/2401.12827", "description": "Empirical likelihood is a very important nonparametric approach which is of\nwide application. However, it is hard and even infeasible to calculate the\nempirical log-likelihood ratio statistic with massive data. The main challenge\nis the calculation of the Lagrange multiplier. This motivates us to develop a\ndistributed empirical likelihood method by calculating the Lagrange multiplier\nin a multi-round distributed manner. It is shown that the distributed empirical\nlog-likelihood ratio statistic is asymptotically standard chi-squared under\nsome mild conditions. The proposed algorithm is communication-efficient and\nachieves the desired accuracy in a few rounds. Further, the distributed\nempirical likelihood method is extended to the case of Byzantine failures. A\nmachine selection algorithm is developed to identify the worker machines\nwithout Byzantine failures such that the distributed empirical likelihood\nmethod can be applied. The proposed methods are evaluated by numerical\nsimulations and illustrated with an analysis of airline on-time performance\nstudy and a surface climate analysis of Yangtze River Economic Belt."}, "http://arxiv.org/abs/2401.12836": {"title": "Empirical Likelihood Inference over Decentralized Networks", "link": "http://arxiv.org/abs/2401.12836", "description": "As a nonparametric statistical inference approach, empirical likelihood has\nbeen found very useful in numerous occasions. However, it encounters serious\ncomputational challenges when applied directly to the modern massive dataset.\nThis article studies empirical likelihood inference over decentralized\ndistributed networks, where the data are locally collected and stored by\ndifferent nodes. To fully utilize the data, this article fuses Lagrange\nmultipliers calculated in different nodes by employing a penalization\ntechnique. The proposed distributed empirical log-likelihood ratio statistic\nwith Lagrange multipliers solved by the penalized function is asymptotically\nstandard chi-squared under regular conditions even for a divergent machine\nnumber. Nevertheless, the optimization problem with the fused penalty is still\nhard to solve in the decentralized distributed network. To address the problem,\ntwo alternating direction method of multipliers (ADMM) based algorithms are\nproposed, which both have simple node-based implementation schemes.\nTheoretically, this article establishes convergence properties for proposed\nalgorithms, and further proves the linear convergence of the second algorithm\nin some specific network structures. The proposed methods are evaluated by\nnumerical simulations and illustrated with analyses of census income and Ford\ngobike datasets."}, "http://arxiv.org/abs/2401.12865": {"title": "Gridsemble: Selective Ensembling for False Discovery Rates", "link": "http://arxiv.org/abs/2401.12865", "description": "In this paper, we introduce Gridsemble, a data-driven selective ensembling\nalgorithm for estimating local false discovery rates (fdr) in large-scale\nmultiple hypothesis testing. Existing methods for estimating fdr often yield\ndifferent conclusions, yet the unobservable nature of fdr values prevents the\nuse of traditional model selection. There is limited guidance on choosing a\nmethod for a given dataset, making this an arbitrary decision in practice.\nGridsemble circumvents this challenge by ensembling a subset of methods with\nweights based on their estimated performances, which are computed on synthetic\ndatasets generated to mimic the observed data while including ground truth. We\ndemonstrate through simulation studies and an experimental application that\nthis method outperforms three popular R software packages with their default\nparameter values$\\unicode{x2014}$common choices given the current landscape.\nWhile our applications are in the context of high throughput transcriptomics,\nwe emphasize that Gridsemble is applicable to any use of large-scale multiple\nhypothesis testing, an approach that is utilized in many fields. We believe\nthat Gridsemble will be a useful tool for computing reliable estimates of fdr\nand for improving replicability in the presence of multiple hypotheses by\neliminating the need for an arbitrary choice of method. Gridsemble is\nimplemented in an open-source R software package available on GitHub at\njennalandy/gridsemblefdr."}, "http://arxiv.org/abs/2401.12905": {"title": "Estimating the construct validity of Principal Components Analysis", "link": "http://arxiv.org/abs/2401.12905", "description": "In many scientific disciplines, the features of interest cannot be observed\ndirectly, so must instead be inferred from observed behaviour. Latent variable\nanalyses are increasingly employed to systematise these inferences, and\nPrincipal Components Analysis (PCA) is perhaps the simplest and most popular of\nthese methods. Here, we examine how the assumptions that we are prepared to\nentertain, about the latent variable system, mediate the likelihood that\nPCA-derived components will capture the true sources of variance underlying\ndata. As expected, we find that this likelihood is excellent in the best case,\nand robust to empirically reasonable levels of measurement noise, but best-case\nperformance is also: (a) not robust to violations of the method's more\nprominent assumptions, of linearity and orthogonality; and also (b) requires\nthat other subtler assumptions be made, such as that the latent variables\nshould have varying importance, and that weights relating latent variables to\nobserved data have zero mean. Neither variance explained, nor replication in\nindependent samples, could reliably predict which (if any) PCA-derived\ncomponents will capture true sources of variance in data. We conclude by\ndescribing a procedure to fit these inferences more directly to empirical data,\nand use it to find that components derived via PCA from two different empirical\nneuropsychological datasets, are less likely to have meaningful referents in\nthe brain than we hoped."}, "http://arxiv.org/abs/2401.12911": {"title": "Pretraining and the Lasso", "link": "http://arxiv.org/abs/2401.12911", "description": "Pretraining is a popular and powerful paradigm in machine learning. As an\nexample, suppose one has a modest-sized dataset of images of cats and dogs, and\nplans to fit a deep neural network to classify them from the pixel features.\nWith pretraining, we start with a neural network trained on a large corpus of\nimages, consisting of not just cats and dogs but hundreds of other image types.\nThen we fix all of the network weights except for the top layer (which makes\nthe final classification) and train (or \"fine tune\") those weights on our\ndataset. This often results in dramatically better performance than the network\ntrained solely on our smaller dataset.\n\nIn this paper, we ask the question \"Can pretraining help the lasso?\". We\ndevelop a framework for the lasso in which an overall model is fit to a large\nset of data, and then fine-tuned to a specific task on a smaller dataset. This\nlatter dataset can be a subset of the original dataset, but does not need to\nbe. We find that this framework has a wide variety of applications, including\nstratified models, multinomial targets, multi-response models, conditional\naverage treatment estimation and even gradient boosting.\n\nIn the stratified model setting, the pretrained lasso pipeline estimates the\ncoefficients common to all groups at the first stage, and then group specific\ncoefficients at the second \"fine-tuning\" stage. We show that under appropriate\nassumptions, the support recovery rate of the common coefficients is superior\nto that of the usual lasso trained only on individual groups. This separate\nidentification of common and individual coefficients can also be useful for\nscientific understanding."}, "http://arxiv.org/abs/2401.12924": {"title": "Performance Analysis of Support Vector Machine (SVM) on Challenging Datasets for Forest Fire Detection", "link": "http://arxiv.org/abs/2401.12924", "description": "This article delves into the analysis of performance and utilization of\nSupport Vector Machines (SVMs) for the critical task of forest fire detection\nusing image datasets. With the increasing threat of forest fires to ecosystems\nand human settlements, the need for rapid and accurate detection systems is of\nutmost importance. SVMs, renowned for their strong classification capabilities,\nexhibit proficiency in recognizing patterns associated with fire within images.\nBy training on labeled data, SVMs acquire the ability to identify distinctive\nattributes associated with fire, such as flames, smoke, or alterations in the\nvisual characteristics of the forest area. The document thoroughly examines the\nuse of SVMs, covering crucial elements like data preprocessing, feature\nextraction, and model training. It rigorously evaluates parameters such as\naccuracy, efficiency, and practical applicability. The knowledge gained from\nthis study aids in the development of efficient forest fire detection systems,\nenabling prompt responses and improving disaster management. Moreover, the\ncorrelation between SVM accuracy and the difficulties presented by\nhigh-dimensional datasets is carefully investigated, demonstrated through a\nrevealing case study. The relationship between accuracy scores and the\ndifferent resolutions used for resizing the training datasets has also been\ndiscussed in this article. These comprehensive studies result in a definitive\noverview of the difficulties faced and the potential sectors requiring further\nimprovement and focus."}, "http://arxiv.org/abs/2401.12937": {"title": "Are the Signs of Factor Loadings Arbitrary in Confirmatory Factor Analysis? Problems and Solutions", "link": "http://arxiv.org/abs/2401.12937", "description": "The replication crisis in social and behavioral sciences has raised concerns\nabout the reliability and validity of empirical studies. While research in the\nliterature has explored contributing factors to this crisis, the issues related\nto analytical tools have received less attention. This study focuses on a\nwidely used analytical tool - confirmatory factor analysis (CFA) - and\ninvestigates one issue that is typically overlooked in practice: accurately\nestimating factor-loading signs. Incorrect loading signs can distort the\nrelationship between observed variables and latent factors, leading to\nunreliable or invalid results in subsequent analyses. Our study aims to\ninvestigate and address the estimation problem of factor-loading signs in CFA\nmodels. Based on an empirical demonstration and Monte Carlo simulation studies,\nwe found current methods have drawbacks in estimating loading signs. To address\nthis problem, three solutions are proposed and proven to work effectively. The\napplications of these solutions are discussed and elaborated."}, "http://arxiv.org/abs/2401.12967": {"title": "Measure transport with kernel mean embeddings", "link": "http://arxiv.org/abs/2401.12967", "description": "Kalman filters constitute a scalable and robust methodology for approximate\nBayesian inference, matching first and second order moments of the target\nposterior. To improve the accuracy in nonlinear and non-Gaussian settings, we\nextend this principle to include more or different characteristics, based on\nkernel mean embeddings (KMEs) of probability measures into their corresponding\nHilbert spaces. Focusing on the continuous-time setting, we develop a family of\ninteracting particle systems (termed $\\textit{KME-dynamics}$) that bridge\nbetween the prior and the posterior, and that include the Kalman-Bucy filter as\na special case. A variant of KME-dynamics has recently been derived from an\noptimal transport perspective by Maurais and Marzouk, and we expose further\nconnections to (kernelised) diffusion maps, leading to a variational\nformulation of regression type. Finally, we conduct numerical experiments on\ntoy examples and the Lorenz-63 model, the latter of which show particular\npromise for a hybrid modification (called Kalman-adjusted KME-dynamics)."}, "http://arxiv.org/abs/2110.05475": {"title": "Bayesian hidden Markov models for latent variable labeling assignments in conflict research: application to the role ceasefires play in conflict dynamics", "link": "http://arxiv.org/abs/2110.05475", "description": "A crucial challenge for solving problems in conflict research is in\nleveraging the semi-supervised nature of the data that arise. Observed response\ndata such as counts of battle deaths over time indicate latent processes of\ninterest such as intensity and duration of conflicts, but defining and labeling\ninstances of these unobserved processes requires nuance and imprecision. The\navailability of such labels, however, would make it possible to study the\neffect of intervention-related predictors - such as ceasefires - directly on\nconflict dynamics (e.g., latent intensity) rather than through an intermediate\nproxy like observed counts of battle deaths. Motivated by this problem and the\nnew availability of the ETH-PRIO Civil Conflict Ceasefires data set, we propose\na Bayesian autoregressive (AR) hidden Markov model (HMM) framework as a\nsufficiently flexible machine learning approach for semi-supervised regime\nlabeling with uncertainty quantification. We motivate our approach by\nillustrating the way it can be used to study the role that ceasefires play in\nshaping conflict dynamics. This ceasefires data set is the first systematic and\nglobally comprehensive data on ceasefires, and our work is the first to analyze\nthis new data and to explore the effect of ceasefires on conflict dynamics in a\ncomprehensive and cross-country manner."}, "http://arxiv.org/abs/2112.14946": {"title": "A causal inference framework for spatial confounding", "link": "http://arxiv.org/abs/2112.14946", "description": "Recently, addressing spatial confounding has become a major topic in spatial\nstatistics. However, the literature has provided conflicting definitions, and\nmany proposed definitions do not address the issue of confounding as it is\nunderstood in causal inference. We define spatial confounding as the existence\nof an unmeasured causal confounder with a spatial structure. We present a\ncausal inference framework for nonparametric identification of the causal\neffect of a continuous exposure on an outcome in the presence of spatial\nconfounding. We propose double machine learning (DML), a procedure in which\nflexible models are used to regress both the exposure and outcome variables on\nconfounders to arrive at a causal estimator with favorable robustness\nproperties and convergence rates, and we prove that this approach is consistent\nand asymptotically normal under spatial dependence. As far as we are aware,\nthis is the first approach to spatial confounding that does not rely on\nrestrictive parametric assumptions (such as linearity, effect homogeneity, or\nGaussianity) for both identification and estimation. We demonstrate the\nadvantages of the DML approach analytically and in simulations. We apply our\nmethods and reasoning to a study of the effect of fine particulate matter\nexposure during pregnancy on birthweight in California."}, "http://arxiv.org/abs/2210.08228": {"title": "Nonparametric Estimation of Mediation Effects with A General Treatment", "link": "http://arxiv.org/abs/2210.08228", "description": "To investigate causal mechanisms, causal mediation analysis decomposes the\ntotal treatment effect into the natural direct and indirect effects. This paper\nexamines the estimation of the direct and indirect effects in a general\ntreatment effect model, where the treatment can be binary, multi-valued,\ncontinuous, or a mixture. We propose generalized weighting estimators with\nweights estimated by solving an expanding set of equations. Under some\nsufficient conditions, we show that the proposed estimators are consistent and\nasymptotically normal. Specifically, when the treatment is discrete, the\nproposed estimators attain the semiparametric efficiency bounds. Meanwhile,\nwhen the treatment is continuous, the convergence rates of the proposed\nestimators are slower than $N^{-1/2}$; however, they are still more efficient\nthan that constructed from the true weighting function. A simulation study\nreveals that our estimators exhibit a satisfactory finite-sample performance,\nwhile an application shows their practical value"}, "http://arxiv.org/abs/2210.13843": {"title": "GLS under Monotone Heteroskedasticity", "link": "http://arxiv.org/abs/2210.13843", "description": "The generalized least square (GLS) is one of the most basic tools in\nregression analyses. A major issue in implementing the GLS is estimation of the\nconditional variance function of the error term, which typically requires a\nrestrictive functional form assumption for parametric estimation or smoothing\nparameters for nonparametric estimation. In this paper, we propose an\nalternative approach to estimate the conditional variance function under\nnonparametric monotonicity constraints by utilizing the isotonic regression\nmethod. Our GLS estimator is shown to be asymptotically equivalent to the\ninfeasible GLS estimator with knowledge of the conditional error variance, and\ninvolves only some tuning to trim boundary observations, not only for point\nestimation but also for interval estimation or hypothesis testing. Our analysis\nextends the scope of the isotonic regression method by showing that the\nisotonic estimates, possibly with generated variables, can be employed as first\nstage estimates to be plugged in for semiparametric objects. Simulation studies\nillustrate excellent finite sample performances of the proposed method. As an\nempirical example, we revisit Acemoglu and Restrepo's (2017) study on the\nrelationship between an aging population and economic growth to illustrate how\nour GLS estimator effectively reduces estimation errors."}, "http://arxiv.org/abs/2212.11398": {"title": "Grace periods in comparative effectiveness studies of sustained treatments", "link": "http://arxiv.org/abs/2212.11398", "description": "Researchers are often interested in estimating the effect of sustained use of\na treatment on a health outcome. However, adherence to strict treatment\nprotocols can be challenging for individuals in practice and, when\nnon-adherence is expected, estimates of the effect of sustained use may not be\nuseful for decision making. As an alternative, more relaxed treatment protocols\nwhich allow for periods of time off treatment (i.e. grace periods) have been\nconsidered in pragmatic randomized trials and observational studies. In this\narticle, we consider the interpretation, identification, and estimation of\ntreatment strategies which include grace periods. We contrast natural grace\nperiod strategies which allow individuals the flexibility to take treatment as\nthey would naturally do, with stochastic grace period strategies in which the\ninvestigator specifies the distribution of treatment utilization. We estimate\nthe effect of initiation of a thiazide diuretic or an angiotensin-converting\nenzyme inhibitor in hypertensive individuals under various strategies which\ninclude grace periods."}, "http://arxiv.org/abs/2301.02739": {"title": "Rank-transformed subsampling: inference for multiple data splitting and exchangeable p-values", "link": "http://arxiv.org/abs/2301.02739", "description": "Many testing problems are readily amenable to randomised tests such as those\nemploying data splitting. However despite their usefulness in principle,\nrandomised tests have obvious drawbacks. Firstly, two analyses of the same\ndataset may lead to different results. Secondly, the test typically loses power\nbecause it does not fully utilise the entire sample. As a remedy to these\ndrawbacks, we study how to combine the test statistics or p-values resulting\nfrom multiple random realisations such as through random data splits. We\ndevelop rank-transformed subsampling as a general method for delivering large\nsample inference about the combined statistic or p-value under mild\nassumptions. We apply our methodology to a wide range of problems, including\ntesting unimodality in high-dimensional data, testing goodness-of-fit of\nparametric quantile regression models, testing no direct effect in a\nsequentially randomised trial and calibrating cross-fit double machine learning\nconfidence intervals. In contrast to existing p-value aggregation schemes that\ncan be highly conservative, our method enjoys type-I error control that\nasymptotically approaches the nominal level. Moreover, compared to using the\nordinary subsampling, we show that our rank transform can remove the\nfirst-order bias in approximating the null under alternatives and greatly\nimprove power."}, "http://arxiv.org/abs/2303.07706": {"title": "On the Utility of Equal Batch Sizes for Inference in Stochastic Gradient Descent", "link": "http://arxiv.org/abs/2303.07706", "description": "Stochastic gradient descent (SGD) is an estimation tool for large data\nemployed in machine learning and statistics. Due to the Markovian nature of the\nSGD process, inference is a challenging problem. An underlying asymptotic\nnormality of the averaged SGD (ASGD) estimator allows for the construction of a\nbatch-means estimator of the asymptotic covariance matrix. Instead of the usual\nincreasing batch-size strategy employed in ASGD, we propose a memory efficient\nequal batch-size strategy and show that under mild conditions, the estimator is\nconsistent. A key feature of the proposed batching technique is that it allows\nfor bias-correction of the variance, at no cost to memory. Since joint\ninference for high dimensional problems may be undesirable, we present\nmarginal-friendly simultaneous confidence intervals, and show through an\nexample how covariance estimators of ASGD can be employed in improved\npredictions."}, "http://arxiv.org/abs/2307.04225": {"title": "Copula-like inference for discrete bivariate distributions with rectangular supports", "link": "http://arxiv.org/abs/2307.04225", "description": "After reviewing a large body of literature on the modeling of bivariate\ndiscrete distributions with finite support, \\cite{Gee20} made a compelling case\nfor the use of $I$-projections in the sense of \\cite{Csi75} as a sound way to\nattempt to decompose a bivariate probability mass function (p.m.f.) into its\ntwo univariate margins and a bivariate p.m.f.\\ with uniform margins playing the\nrole of a discrete copula. From a practical perspective, the necessary\n$I$-projections on Fr\\'echet classes can be carried out using the iterative\nproportional fitting procedure (IPFP), also known as Sinkhorn's algorithm or\nmatrix scaling in the literature. After providing conditions under which a\nbivariate p.m.f.\\ can be decomposed in the aforementioned sense, we\ninvestigate, for starting bivariate p.m.f.s with rectangular supports,\nnonparametric and parametric estimation procedures as well as goodness-of-fit\ntests for the underlying discrete copula. Related asymptotic results are\nprovided and build upon a differentiability result for $I$-projections on\nFr\\'echet classes which can be of independent interest. Theoretical results are\ncomplemented by finite-sample experiments and a data example."}, "http://arxiv.org/abs/2309.08783": {"title": "Quantifying predictive uncertainty of aphasia severity in stroke patients with sparse heteroscedastic Bayesian high-dimensional regression", "link": "http://arxiv.org/abs/2309.08783", "description": "Sparse linear regression methods for high-dimensional data commonly assume\nthat residuals have constant variance, which can be violated in practice. For\nexample, Aphasia Quotient (AQ) is a critical measure of language impairment and\ninforms treatment decisions, but it is challenging to measure in stroke\npatients. It is of interest to use high-resolution T2 neuroimages of brain\ndamage to predict AQ. However, sparse regression models show marked evidence of\nheteroscedastic error even after transformations are applied. This violation of\nthe homoscedasticity assumption can lead to bias in estimated coefficients,\nprediction intervals (PI) with improper length, and increased type I errors.\nBayesian heteroscedastic linear regression models relax the homoscedastic error\nassumption but can enforce restrictive prior assumptions on parameters, and\nmany are computationally infeasible in the high-dimensional setting. This paper\nproposes estimating high-dimensional heteroscedastic linear regression models\nusing a heteroscedastic partitioned empirical Bayes Expectation Conditional\nMaximization (H-PROBE) algorithm. H-PROBE is a computationally efficient\nmaximum a posteriori estimation approach that requires minimal prior\nassumptions and can incorporate covariates hypothesized to impact\nheterogeneity. We apply this method by using high-dimensional neuroimages to\npredict and provide PIs for AQ that accurately quantify predictive uncertainty.\nOur analysis demonstrates that H-PROBE can provide narrower PI widths than\nstandard methods without sacrificing coverage. Narrower PIs are clinically\nimportant for determining the risk of moderate to severe aphasia. Additionally,\nthrough extensive simulation studies, we exhibit that H-PROBE results in\nsuperior prediction, variable selection, and predictive inference compared to\nalternative methods."}, "http://arxiv.org/abs/2401.13009": {"title": "Comparative Study of Causal Discovery Methods for Cyclic Models with Hidden Confounders", "link": "http://arxiv.org/abs/2401.13009", "description": "Nowadays, the need for causal discovery is ubiquitous. A better understanding\nof not just the stochastic dependencies between parts of a system, but also the\nactual cause-effect relations, is essential for all parts of science. Thus, the\nneed for reliable methods to detect causal directions is growing constantly. In\nthe last 50 years, many causal discovery algorithms have emerged, but most of\nthem are applicable only under the assumption that the systems have no feedback\nloops and that they are causally sufficient, i.e. that there are no unmeasured\nsubsystems that can affect multiple measured variables. This is unfortunate\nsince those restrictions can often not be presumed in practice. Feedback is an\nintegral feature of many processes, and real-world systems are rarely\ncompletely isolated and fully measured. Fortunately, in recent years, several\ntechniques, that can cope with cyclic, causally insufficient systems, have been\ndeveloped. And with multiple methods available, a practical application of\nthose algorithms now requires knowledge of the respective strengths and\nweaknesses. Here, we focus on the problem of causal discovery for sparse linear\nmodels which are allowed to have cycles and hidden confounders. We have\nprepared a comprehensive and thorough comparative study of four causal\ndiscovery techniques: two versions of the LLC method [10] and two variants of\nthe ASP-based algorithm [11]. The evaluation investigates the performance of\nthose techniques for various experiments with multiple interventional setups\nand different dataset sizes."}, "http://arxiv.org/abs/2401.13010": {"title": "Bartholomew's trend test -- approximated by a multiple contrast test", "link": "http://arxiv.org/abs/2401.13010", "description": "Bartholomew's trend test belongs to the broad class of isotonic regression\nmodels, specifically with a single qualitative factor, e.g. dose levels. Using\nthe approximation of the ANOVA F-test by the maximum contrast test against\ngrand mean and pool-adjacent-violator estimates under order restriction, an\neasier to use approximation is proposed."}, "http://arxiv.org/abs/2401.13045": {"title": "Assessment of Sports Concussion in Female Athletes: A Role for Neuroinformatics?", "link": "http://arxiv.org/abs/2401.13045", "description": "Over the past decade, the intricacies of sports-related concussions among\nfemale athletes have become readily apparent. Traditional clinical methods for\ndiagnosing concussions suffer limitations when applied to female athletes,\noften failing to capture subtle changes in brain structure and function.\nAdvanced neuroinformatics techniques and machine learning models have become\ninvaluable assets in this endeavor. While these technologies have been\nextensively employed in understanding concussion in male athletes, there\nremains a significant gap in our comprehension of their effectiveness for\nfemale athletes. With its remarkable data analysis capacity, machine learning\noffers a promising avenue to bridge this deficit. By harnessing the power of\nmachine learning, researchers can link observed phenotypic neuroimaging data to\nsex-specific biological mechanisms, unraveling the mysteries of concussions in\nfemale athletes. Furthermore, embedding methods within machine learning enable\nexamining brain architecture and its alterations beyond the conventional\nanatomical reference frame. In turn, allows researchers to gain deeper insights\ninto the dynamics of concussions, treatment responses, and recovery processes.\nTo guarantee that female athletes receive the optimal care they deserve,\nresearchers must employ advanced neuroimaging techniques and sophisticated\nmachine-learning models. These tools enable an in-depth investigation of the\nunderlying mechanisms responsible for concussion symptoms stemming from\nneuronal dysfunction in female athletes. This paper endeavors to address the\ncrucial issue of sex differences in multimodal neuroimaging experimental design\nand machine learning approaches within female athlete populations, ultimately\nensuring that they receive the tailored care they require when facing the\nchallenges of concussions."}, "http://arxiv.org/abs/2401.13090": {"title": "Variational Estimation for Multidimensional Generalized Partial Credit Model", "link": "http://arxiv.org/abs/2401.13090", "description": "Multidimensional item response theory (MIRT) models have generated increasing\ninterest in the psychometrics literature. Efficient approaches for estimating\nMIRT models with dichotomous responses have been developed, but constructing an\nequally efficient and robust algorithm for polytomous models has received\nlimited attention. To address this gap, this paper presents a novel Gaussian\nvariational estimation algorithm for the multidimensional generalized partial\ncredit model (MGPCM). The proposed algorithm demonstrates both fast and\naccurate performance, as illustrated through a series of simulation studies and\ntwo real data analyses."}, "http://arxiv.org/abs/2401.13094": {"title": "On cross-validated estimation of skew normal model", "link": "http://arxiv.org/abs/2401.13094", "description": "Skew normal model suffers from inferential drawbacks, namely singular Fisher\ninformation in the vicinity of symmetry and diverging of maximum likelihood\nestimation. To address the above drawbacks, Azzalini and Arellano-Valle (2013)\nintroduced maximum penalised likelihood estimation (MPLE) by subtracting a\npenalty function from the log-likelihood function with a pre-specified penalty\ncoefficient. Here, we propose a cross-validated MPLE to improve its performance\nwhen the underlying model is close to symmetry. We develop a theory for MPLE,\nwhere an asymptotic rate for the cross-validated penalty coefficient is\nderived. We further show that the proposed cross-validated MPLE is\nasymptotically efficient under certain conditions. In simulation studies and a\nreal data application, we demonstrate that the proposed estimator can\noutperform the conventional MPLE when the model is close to symmetry."}, "http://arxiv.org/abs/2401.13208": {"title": "Assessing Influential Observations in Pain Prediction using fMRI Data", "link": "http://arxiv.org/abs/2401.13208", "description": "Influential diagnosis is an integral part of data analysis, of which most\nexisting methodological frameworks presume a deterministic submodel and are\ndesigned for low-dimensional data (i.e., the number of predictors p smaller\nthan the sample size n). However, the stochastic selection of a submodel from\nhigh-dimensional data where p exceeds n has become ubiquitous. Thus, methods\nfor identifying observations that could exert undue influence on the choice of\na submodel can play an important role in this setting. To date, discussion of\nthis topic has been limited, falling short in two domains: (i) constrained\nability to detect multiple influential points, and (ii) applicability only in\nrestrictive settings. After describing the problem, we characterize and\nformalize the concept of influential observations on variable selection. Then,\nwe propose a generalized diagnostic measure, extended from an available metric\naccommodating different model selectors and multiple influential observations,\nthe asymptotic distribution of which is subsequently establish large p, thus\nproviding guidelines to ascertain influential observations. A high-dimensional\nclustering procedure is further incorporated into our proposed scheme to detect\nmultiple influential points. Simulation is conducted to assess the performances\nof various diagnostic approaches. The proposed procedure further demonstrates\nits value in improving predictive power when analyzing thermal-stimulated pain\nbased on fMRI data."}, "http://arxiv.org/abs/2401.13379": {"title": "An Ising Similarity Regression Model for Modeling Multivariate Binary Data", "link": "http://arxiv.org/abs/2401.13379", "description": "Understanding the dependence structure between response variables is an\nimportant component in the analysis of correlated multivariate data. This\narticle focuses on modeling dependence structures in multivariate binary data,\nmotivated by a study aiming to understand how patterns in different U.S.\nsenators' votes are determined by similarities (or lack thereof) in their\nattributes, e.g., political parties and social network profiles. To address\nsuch a research question, we propose a new Ising similarity regression model\nwhich regresses pairwise interaction coefficients in the Ising model against a\nset of similarity measures available/constructed from covariates. Model\nselection approaches are further developed through regularizing the\npseudo-likelihood function with an adaptive lasso penalty to enable the\nselection of relevant similarity measures. We establish estimation and\nselection consistency of the proposed estimator under a general setting where\nthe number of similarity measures and responses tend to infinity. Simulation\nstudy demonstrates the strong finite sample performance of the proposed\nestimator in terms of parameter estimation and similarity selection. Applying\nthe Ising similarity regression model to a dataset of roll call voting records\nof 100 U.S. senators, we are able to quantify how similarities in senators'\nparties, businessman occupations and social network profiles drive their voting\nassociations."}, "http://arxiv.org/abs/2401.13665": {"title": "Entrywise Inference for Causal Panel Data: A Simple and Instance-Optimal Approach", "link": "http://arxiv.org/abs/2401.13665", "description": "In causal inference with panel data under staggered adoption, the goal is to\nestimate and derive confidence intervals for potential outcomes and treatment\neffects. We propose a computationally efficient procedure, involving only\nsimple matrix algebra and singular value decomposition. We derive\nnon-asymptotic bounds on the entrywise error, establishing its proximity to a\nsuitably scaled Gaussian variable. Despite its simplicity, our procedure turns\nout to be instance-optimal, in that our theoretical scaling matches a local\ninstance-wise lower bound derived via a Bayesian Cram\\'{e}r-Rao argument. Using\nour insights, we develop a data-driven procedure for constructing entrywise\nconfidence intervals with pre-specified coverage guarantees. Our analysis is\nbased on a general inferential toolbox for the SVD algorithm applied to the\nmatrix denoising model, which might be of independent interest."}, "http://arxiv.org/abs/2201.05828": {"title": "Adaptive procedures for directional false discovery rate control", "link": "http://arxiv.org/abs/2201.05828", "description": "In multiple hypothesis testing, it is well known that adaptive procedures can\nenhance power via incorporating information about the number of true nulls\npresent. Under independence, we establish that two adaptive false discovery\nrate (FDR) methods, upon augmenting sign declarations, also offer directional\nfalse discovery rate (FDR$_\\text{dir}$) control in the strong sense. Such\nFDR$_\\text{dir}$ controlling properties are appealing because adaptive\nprocedures have the greatest potential to reap substantial gain in power when\nthe underlying parameter configurations contain little to no true nulls, which\nare precisely settings where the FDR$_\\text{dir}$ is an arguably more\nmeaningful error rate to be controlled than the FDR."}, "http://arxiv.org/abs/2212.02306": {"title": "Robust multiple method comparison and transformation", "link": "http://arxiv.org/abs/2212.02306", "description": "A generalization of Passing-Bablok regression is proposed for comparing\nmultiple measurement methods simultaneously. Possible applications include\nassay migration studies or interlaboratory trials. When comparing only two\nmethods, the method reduces to the usual Passing-Bablok estimator. It is close\nin spirit to reduced major axis regression, which is, however, not robust. To\nobtain a robust estimator, the major axis is replaced by the (hyper-)spherical\nmedian axis. The method is shown to reduce to the usual Passing-Bablok\nestimator if only two methods are compared. This technique has been applied to\ncompare SARS-CoV-2 serological tests, bilirubin in neonates, and an in vitro\ndiagnostic test using different instruments, sample preparations, and reagent\nlots. In addition, plots similar to the well-known Bland-Altman plots have been\ndeveloped to represent the variance structure."}, "http://arxiv.org/abs/2305.15936": {"title": "Learning DAGs from Data with Few Root Causes", "link": "http://arxiv.org/abs/2305.15936", "description": "We present a novel perspective and algorithm for learning directed acyclic\ngraphs (DAGs) from data generated by a linear structural equation model (SEM).\nFirst, we show that a linear SEM can be viewed as a linear transform that, in\nprior work, computes the data from a dense input vector of random valued root\ncauses (as we will call them) associated with the nodes. Instead, we consider\nthe case of (approximately) few root causes and also introduce noise in the\nmeasurement of the data. Intuitively, this means that the DAG data is produced\nby few data-generating events whose effect percolates through the DAG. We prove\nidentifiability in this new setting and show that the true DAG is the global\nminimizer of the $L^0$-norm of the vector of root causes. For data with few\nroot causes, with and without noise, we show superior performance compared to\nprior DAG learning methods."}, "http://arxiv.org/abs/2307.02096": {"title": "Adaptive multi-stage integration schemes for Hamiltonian Monte Carlo", "link": "http://arxiv.org/abs/2307.02096", "description": "Hamiltonian Monte Carlo (HMC) is a powerful tool for Bayesian statistical\ninference due to its potential to rapidly explore high dimensional state space,\navoiding the random walk behavior typical of many Markov Chain Monte Carlo\nsamplers. The proper choice of the integrator of the Hamiltonian dynamics is\nkey to the efficiency of HMC. It is becoming increasingly clear that\nmulti-stage splitting integrators are a good alternative to the Verlet method,\ntraditionally used in HMC. Here we propose a principled way of finding optimal,\nproblem-specific integration schemes (in terms of the best conservation of\nenergy for harmonic forces/Gaussian targets) within the families of 2- and\n3-stage splitting integrators. The method, which we call Adaptive Integration\nApproach for statistics, or s-AIA, uses a multivariate Gaussian model and\nsimulation data obtained at the HMC burn-in stage to identify a system-specific\ndimensional stability interval and assigns the most appropriate 2-/3-stage\nintegrator for any user-chosen simulation step size within that interval. s-AIA\nhas been implemented in the in-house software package HaiCS without introducing\ncomputational overheads in the simulations. The efficiency of the s-AIA\nintegrators and their impact on the HMC accuracy, sampling performance and\nconvergence are discussed in comparison with known fixed-parameter multi-stage\nsplitting integrators (including Verlet). Numerical experiments on well-known\nstatistical models show that the adaptive schemes reach the best possible\nperformance within the family of 2-, 3-stage splitting schemes."}, "http://arxiv.org/abs/2401.13777": {"title": "Revisiting the memoryless property -- testing for the Pareto type I distribution", "link": "http://arxiv.org/abs/2401.13777", "description": "We propose new goodness-of-fit tests for the Pareto type I distribution.\nThese tests are based on a multiplicative version of the memoryless property\nwhich characterises this distribution. We present the results of a Monte Carlo\npower study demonstrating that the proposed tests are powerful compared to\nexisting tests. As a result of independent interest, we demonstrate that tests\nspecifically developed for the Pareto type I distribution substantially\noutperform tests for exponentiality applied to log-transformed data (since\nPareto type I distributed values can be transformed to exponentiality via a\nsimple log-transformation). Specifically, the newly proposed tests based on the\nmultiplicative memoryless property of the Pareto distribution substantially\noutperform a test based on the memoryless property of the exponential\ndistribution. The practical use of tests is illustrated by testing the\nhypothesis that two sets of observed golfers' earnings (those of the PGA and\nLIV tours) are realised from Pareto distributions."}, "http://arxiv.org/abs/2401.13787": {"title": "Bayesian Analysis of the Beta Regression Model Subject to Linear Inequality Restrictions with Application", "link": "http://arxiv.org/abs/2401.13787", "description": "ReRecent studies in machine learning are based on models in which parameters\nor state variables are bounded restricted. These restrictions are from prior\ninformation to ensure the validity of scientific theories or structural\nconsistency based on physical phenomena. The valuable information contained in\nthe restrictions must be considered during the estimation process to improve\nestimation accuracy. Many researchers have focused on linear regression models\nsubject to linear inequality restrictions, but generalized linear models have\nreceived little attention. In this paper, the parameters of beta Bayesian\nregression models subjected to linear inequality restrictions are estimated.\nThe proposed Bayesian restricted estimator, which is demonstrated by simulated\nstudies, outperforms ordinary estimators. Even in the presence of\nmulticollinearity, it outperforms the ridge estimator in terms of the standard\ndeviation and the mean squared error. The results confirm that the proposed\nBayesian restricted estimator makes sparsity in parameter estimating without\nusing the regularization penalty. Finally, a real data set is analyzed by the\nnew proposed Bayesian estimation method."}, "http://arxiv.org/abs/2401.13820": {"title": "A Bayesian hierarchical mixture cure modelling framework to utilize multiple survival datasets for long-term survivorship estimates: A case study from previously untreated metastatic melanoma", "link": "http://arxiv.org/abs/2401.13820", "description": "Time to an event of interest over a lifetime is a central measure of the\nclinical benefit of an intervention used in a health technology assessment\n(HTA). Within the same trial multiple end-points may also be considered. For\nexample, overall and progression-free survival time for different drugs in\noncology studies. A common challenge is when an intervention is only effective\nfor some proportion of the population who are not clinically identifiable.\nTherefore, latent group membership as well as separate survival models for\ngroups identified need to be estimated. However, follow-up in trials may be\nrelatively short leading to substantial censoring. We present a general\nBayesian hierarchical framework that can handle this complexity by exploiting\nthe similarity of cure fractions between end-points; accounting for the\ncorrelation between them and improving the extrapolation beyond the observed\ndata. Assuming exchangeability between cure fractions facilitates the borrowing\nof information between end-points. We show the benefits of using our approach\nwith a motivating example, the CheckMate 067 phase 3 trial consisting of\npatients with metastatic melanoma treated with first line therapy."}, "http://arxiv.org/abs/2401.13890": {"title": "Discrete Hawkes process with flexible residual distribution and filtered historical simulation", "link": "http://arxiv.org/abs/2401.13890", "description": "We introduce a new model which can be considered as a extended version of the\nHawkes process in a discrete sense. This model enables the integration of\nvarious residual distributions while preserving the fundamental properties of\nthe original Hawkes process. The rich nature of this model enables a filtered\nhistorical simulation which incorporate the properties of original time series\nmore accurately. The process naturally extends to multi-variate models with\neasy implementations of estimation and simulation. We investigate the effect of\nflexible residual distribution on estimation of high frequency financial data\ncompared with the Hawkes process."}, "http://arxiv.org/abs/2401.13929": {"title": "Reinforcement Learning with Hidden Markov Models for Discovering Decision-Making Dynamics", "link": "http://arxiv.org/abs/2401.13929", "description": "Major depressive disorder (MDD) presents challenges in diagnosis and\ntreatment due to its complex and heterogeneous nature. Emerging evidence\nindicates that reward processing abnormalities may serve as a behavioral marker\nfor MDD. To measure reward processing, patients perform computer-based\nbehavioral tasks that involve making choices or responding to stimulants that\nare associated with different outcomes. Reinforcement learning (RL) models are\nfitted to extract parameters that measure various aspects of reward processing\nto characterize how patients make decisions in behavioral tasks. Recent\nfindings suggest the inadequacy of characterizing reward learning solely based\non a single RL model; instead, there may be a switching of decision-making\nprocesses between multiple strategies. An important scientific question is how\nthe dynamics of learning strategies in decision-making affect the reward\nlearning ability of individuals with MDD. Motivated by the probabilistic reward\ntask (PRT) within the EMBARC study, we propose a novel RL-HMM framework for\nanalyzing reward-based decision-making. Our model accommodates learning\nstrategy switching between two distinct approaches under a hidden Markov model\n(HMM): subjects making decisions based on the RL model or opting for random\nchoices. We account for continuous RL state space and allow time-varying\ntransition probabilities in the HMM. We introduce a computationally efficient\nEM algorithm for parameter estimation and employ a nonparametric bootstrap for\ninference. We apply our approach to the EMBARC study to show that MDD patients\nare less engaged in RL compared to the healthy controls, and engagement is\nassociated with brain activities in the negative affect circuitry during an\nemotional conflict task."}, "http://arxiv.org/abs/2401.13943": {"title": "Is the age pension in Australia sustainable and fair? Evidence from forecasting the old-age dependency ratio using the Hamilton-Perry model", "link": "http://arxiv.org/abs/2401.13943", "description": "The age pension aims to assist eligible elderly Australians meet specific age\nand residency criteria in maintaining basic living standards. In designing\nefficient pension systems, government policymakers seek to satisfy the\nexpectations of the overall aging population in Australia. However, the\npopulation's unique demographic characteristics at the state and territory\nlevel are often overlooked due to the lack of available data. We use the\nHamilton-Perry model, which requires minimum input, to model and forecast the\nevolution of age-specific populations at the state level. We also integrate the\nobtained sub-national demographic information to determine sustainable pension\nages up to 2051. We also investigate pension welfare distribution in all states\nand territories to identify disadvantaged residents under the current pension\nsystem. Using the sub-national mortality data for Australia from 1971 to 2021\nobtained from AHMD (2023), we implement the Hamilton-Perry model with the help\nof functional time series forecasting techniques. With forecasts of\nage-specific population sizes for each state and territory, we compute the old\nage dependency ratio to determine the nationwide sustainable pension age."}, "http://arxiv.org/abs/2401.13975": {"title": "Sparse signal recovery and source localization via covariance learning", "link": "http://arxiv.org/abs/2401.13975", "description": "In the Multiple Measurements Vector (MMV) model, measurement vectors are\nconnected to unknown, jointly sparse signal vectors through a linear regression\nmodel employing a single known measurement matrix (or dictionary). Typically,\nthe number of atoms (columns of the dictionary) is greater than the number\nmeasurements and the sparse signal recovery problem is generally ill-posed. In\nthis paper, we treat the signals and measurement noise as independent Gaussian\nrandom vectors with unknown signal covariance matrix and noise variance,\nrespectively, and derive fixed point (FP) equation for solving the likelihood\nequation for signal powers, thereby enabling the recovery of the sparse signal\nsupport (sources with non-zero variances). Two practical algorithms, a block\ncoordinate descent (BCD) and a cyclic coordinate descent (CCD) algorithms, that\nleverage on the FP characterization of the likelihood equation are then\nproposed. Additionally, a greedy pursuit method, analogous to popular\nsimultaneous orthogonal matching pursuit (OMP), is introduced. Our numerical\nexamples demonstrate effectiveness of the proposed covariance learning (CL)\nalgorithms both in classic sparse signal recovery as well as in\ndirection-of-arrival (DOA) estimation problems where they perform favourably\ncompared to the state-of-the-art algorithms under a broad variety of settings."}, "http://arxiv.org/abs/2401.14052": {"title": "Testing Alpha in High Dimensional Linear Factor Pricing Models with Dependent Observations", "link": "http://arxiv.org/abs/2401.14052", "description": "In this study, we introduce three distinct testing methods for testing alpha\nin high dimensional linear factor pricing model that deals with dependent data.\nThe first method is a sum-type test procedure, which exhibits high performance\nwhen dealing with dense alternatives. The second method is a max-type test\nprocedure, which is particularly effective for sparse alternatives. For a\nbroader range of alternatives, we suggest a Cauchy combination test procedure.\nThis is predicated on the asymptotic independence of the sum-type and max-type\ntest statistics. Both simulation studies and practical data application\ndemonstrate the effectiveness of our proposed methods when handling dependent\nobservations."}, "http://arxiv.org/abs/2401.14094": {"title": "ODC and ROC curves, comparison curves, and stochastic dominance", "link": "http://arxiv.org/abs/2401.14094", "description": "We discuss two novel approaches to the classical two-sample problem. Our\nstarting point are properly standardized and combined, very popular in several\nareas of statistics and data analysis, ordinal dominance and receiver\ncharacteristic curves, denoted by ODC and ROC, respectively. The proposed new\ncurves are termed the comparison curves. Their estimates, being weighted rank\nprocesses on (0,1), form the basis of inference. These weighted processes are\nintuitive, well-suited for visual inspection of data at hand, and are also\nuseful for constructing some formal inferential procedures. They can be applied\nto several variants of two-sample problem. Their use can help to improve some\nexisting procedures both in terms of power and the ability to identify the\nsources of departures from the postulated model. To simplify interpretation of\nfinite sample results we restrict attention to values of the processes on a\nfinite grid of points. This results in the so-called bar plots (B-plots) which\nreadably summarize the information contained in the data. What is more, we show\nthat B-plots along with adjusted simultaneous acceptance regions provide\nprincipled information about where the model departs from the data. This leads\nto a framework which facilitates identification of regions with locally\nsignificant differences.\n\nWe show an implementation of the considered techniques to a standard\nstochastic dominance testing problem. Some min-type statistics are introduced\nand investigated. A simulation study compares two tests pertinent to the\ncomparison curves to well-established tests in the literature and demonstrates\nthe strong and competitive performance of the former in many typical\nsituations. Some real data applications illustrate simplicity and practical\nusefulness of the proposed approaches. A range of other applications of\nconsidered weighted processes is briefly discussed too."}, "http://arxiv.org/abs/2401.14122": {"title": "On a Novel Skewed Generalized t Distribution: Properties, Estimations and its Applications", "link": "http://arxiv.org/abs/2401.14122", "description": "With the progress of information technology, large amounts of asymmetric,\nleptokurtic and heavy-tailed data are arising in various fields, such as\nfinance, engineering, genetics and medicine. It is very challenging to model\nthose kinds of data, especially for extremely skewed data, accompanied by very\nhigh kurtosis or heavy tails. In this paper, we propose a class of novel skewed\ngeneralized t distribution (SkeGTD) as a scale mixture of skewed generalized\nnormal. The proposed SkeGTD has excellent adaptiveness to various data, because\nof its capability of allowing for a large range of skewness and kurtosis and\nits compatibility of the separated location, scale, skewness and shape\nparameters. We investigate some important properties of this family of\ndistributions. The maximum likelihood estimation, L-moments estimation and\ntwo-step estimation for the SkeGTD are explored. To illustrate the usefulness\nof the proposed methodology, we present simulation studies and analyze two real\ndatasets."}, "http://arxiv.org/abs/2401.14294": {"title": "Heteroscedasticity-aware stratified sampling to improve uplift modeling", "link": "http://arxiv.org/abs/2401.14294", "description": "In many business applications, including online marketing and customer churn\nprevention, randomized controlled trials (RCT's) are conducted to investigate\non the effect of specific treatment (coupon offers, advertisement\nmailings,...). Such RCT's allow for the estimation of average treatment effects\nas well as the training of (uplift) models for the heterogeneity of treatment\neffects between individuals. The problem with these RCT's is that they are\ncostly and this cost increases with the number of individuals included into the\nRCT. For this reason, there is research how to conduct experiments involving a\nsmall number of individuals while still obtaining precise treatment effect\nestimates. We contribute to this literature a heteroskedasticity-aware\nstratified sampling (HS) scheme, which leverages the fact that different\nindividuals have different noise levels in their outcome and precise treatment\neffect estimation requires more observations from the \"high-noise\" individuals\nthan from the \"low-noise\" individuals. By theory as well as by empirical\nexperiments, we demonstrate that our HS-sampling yields significantly more\nprecise estimates of the ATE, improves uplift models and makes their evaluation\nmore reliable compared to RCT data sampled completely randomly. Due to the\nrelative ease of application and the significant benefits, we expect\nHS-sampling to be valuable in many real-world applications."}, "http://arxiv.org/abs/2401.14338": {"title": "Case-crossover designs and overdispersion with application in air pollution epidemiology", "link": "http://arxiv.org/abs/2401.14338", "description": "Over the last three decades, case-crossover designs have found many\napplications in health sciences, especially in air pollution epidemiology. They\nare typically used, in combination with partial likelihood techniques, to\ndefine a conditional logistic model for the responses, usually health outcomes,\nconditional on the exposures. Despite the fact that conditional logistic models\nhave been shown equivalent, in typical air pollution epidemiology setups, to\nspecific instances of the well-known Poisson time series model, it is often\nclaimed that they cannot allow for overdispersion. This paper clarifies the\nrelationship between case-crossover designs, the models that ensue from their\nuse, and overdispersion. In particular, we propose to relax the assumption of\nindependence between individuals traditionally made in case-crossover analyses,\nin order to explicitly introduce overdispersion in the conditional logistic\nmodel. As we show, the resulting overdispersed conditional logistic model\ncoincides with the overdispersed, conditional Poisson model, in the sense that\ntheir likelihoods are simple re-expressions of one another. We further provide\nthe technical details of a Bayesian implementation of the proposed\ncase-crossover model, which we use to demonstrate, by means of a large\nsimulation study, that standard case-crossover models can lead to dramatically\nunderestimated coverage probabilities, while the proposed models do not. We\nalso perform an illustrative analysis of the association between air pollution\nand morbidity in Toronto, Canada, which shows that the proposed models are more\nrobust than standard ones to outliers such as those associated with public\nholidays."}, "http://arxiv.org/abs/2401.14345": {"title": "Uncovering Heterogeneity of Solar Flare Mechanism With Mixture Models", "link": "http://arxiv.org/abs/2401.14345", "description": "The physics of solar flares occurring on the Sun is highly complex and far\nfrom fully understood. However, observations show that solar eruptions are\nassociated with the intense kilogauss fields of active regions, where free\nenergies are stored with field-aligned electric currents. With the advent of\nhigh-quality data sources such as the Geostationary Operational Environmental\nSatellites (GOES) and Solar Dynamics Observatory (SDO)/Helioseismic and\nMagnetic Imager (HMI), recent works on solar flare forecasting have been\nfocusing on data-driven methods. In particular, black box machine learning and\ndeep learning models are increasingly adopted in which underlying data\nstructures are not modeled explicitly. If the active regions indeed follow the\nsame laws of physics, there should be similar patterns shared among them,\nreflected by the observations. Yet, these black box models currently used in\nthe literature do not explicitly characterize the heterogeneous nature of the\nsolar flare data, within and between active regions. In this paper, we propose\ntwo finite mixture models designed to capture the heterogeneous patterns of\nactive regions and their associated solar flare events. With extensive\nnumerical studies, we demonstrate the usefulness of our proposed method for\nboth resolving the sample imbalance issue and modeling the heterogeneity for\nrare energetic solar flare events."}, "http://arxiv.org/abs/2401.14355": {"title": "Multiply Robust Estimation of Causal Effect Curves for Difference-in-Differences Designs", "link": "http://arxiv.org/abs/2401.14355", "description": "Researchers commonly use difference-in-differences (DiD) designs to evaluate\npublic policy interventions. While established methodologies exist for\nestimating effects in the context of binary interventions, policies often\nresult in varied exposures across regions implementing the policy. Yet,\nexisting approaches for incorporating continuous exposures face substantial\nlimitations in addressing confounding variables associated with intervention\nstatus, exposure levels, and outcome trends. These limitations significantly\nconstrain policymakers' ability to fully comprehend policy impacts and design\nfuture interventions. In this study, we propose innovative estimators for\ncausal effect curves within the DiD framework, accounting for multiple sources\nof confounding. Our approach accommodates misspecification of a subset of\ntreatment, exposure, and outcome models while avoiding any parametric\nassumptions on the effect curve. We present the statistical properties of the\nproposed methods and illustrate their application through simulations and a\nstudy investigating the diverse effects of a nutritional excise tax."}, "http://arxiv.org/abs/2401.14359": {"title": "Minimum Covariance Determinant: Spectral Embedding and Subset Size Determination", "link": "http://arxiv.org/abs/2401.14359", "description": "This paper introduces several ideas to the minimum covariance determinant\nproblem for outlier detection and robust estimation of means and covariances.\nWe leverage the principal component transform to achieve dimension reduction,\npaving the way for improved analyses. Our best subset selection algorithm\nstrategically combines statistical depth and concentration steps. To ascertain\nthe appropriate subset size and number of principal components, we introduce a\nnovel bootstrap procedure that estimates the instability of the best subset\nalgorithm. The parameter combination exhibiting minimal instability proves\nideal for the purposes of outlier detection and robust estimation. Rigorous\nbenchmarking against prominent MCD variants showcases our approach's superior\ncapability in outlier detection and computational speed in high dimensions.\nApplication to a fruit spectra data set and a cancer genomics data set\nillustrates our claims."}, "http://arxiv.org/abs/2401.14393": {"title": "Clustering-based spatial interpolation of parametric post-processing models", "link": "http://arxiv.org/abs/2401.14393", "description": "Since the start of the operational use of ensemble prediction systems,\nensemble-based probabilistic forecasting has become the most advanced approach\nin weather prediction. However, despite the persistent development of the last\nthree decades, ensemble forecasts still often suffer from the lack of\ncalibration and might exhibit systematic bias, which calls for some form of\nstatistical post-processing. Nowadays, one can choose from a large variety of\npost-processing approaches, where parametric methods provide full predictive\ndistributions of the investigated weather quantity. Parameter estimation in\nthese models is based on training data consisting of past forecast-observation\npairs, thus post-processed forecasts are usually available only at those\nlocations where training data are accessible. We propose a general\nclustering-based interpolation technique of extending calibrated predictive\ndistributions from observation stations to any location in the ensemble domain\nwhere there are ensemble forecasts at hand. Focusing on the ensemble model\noutput statistics (EMOS) post-processing technique, in a case study based on\nwind speed ensemble forecasts of the European Centre for Medium-Range Weather\nForecasts, we demonstrate the predictive performance of various versions of the\nsuggested method and show its superiority over the regionally estimated and\ninterpolated EMOS models and the raw ensemble forecasts as well."}, "http://arxiv.org/abs/2208.09344": {"title": "A note on incorrect inferences in non-binary qualitative probabilistic networks", "link": "http://arxiv.org/abs/2208.09344", "description": "Qualitative probabilistic networks (QPNs) combine the conditional\nindependence assumptions of Bayesian networks with the qualitative properties\nof positive and negative dependence. They formalise various intuitive\nproperties of positive dependence to allow inferences over a large network of\nvariables. However, we will demonstrate in this paper that, due to an incorrect\nsymmetry property, many inferences obtained in non-binary QPNs are not\nmathematically true. We will provide examples of such incorrect inferences and\nbriefly discuss possible resolutions."}, "http://arxiv.org/abs/2210.14080": {"title": "Learning Individual Treatment Effects under Heterogeneous Interference in Networks", "link": "http://arxiv.org/abs/2210.14080", "description": "Estimates of individual treatment effects from networked observational data\nare attracting increasing attention these days. One major challenge in network\nscenarios is the violation of the stable unit treatment value assumption\n(SUTVA), which assumes that the treatment assignment of a unit does not\ninfluence others' outcomes. In network data, due to interference, the outcome\nof a unit is influenced not only by its treatment (i.e., direct effects) but\nalso by others' treatments (i.e., spillover effects). Furthermore, the\ninfluences from other units are always heterogeneous (e.g., friends with\nsimilar interests affect a person differently than friends with different\ninterests). In this paper, we focus on the problem of estimating individual\ntreatment effects (both direct and spillover effects) under heterogeneous\ninterference. To address this issue, we propose a novel Dual Weighting\nRegression (DWR) algorithm by simultaneously learning attention weights that\ncapture the heterogeneous interference and sample weights to eliminate the\ncomplex confounding bias in networks. We formulate the entire learning process\nas a bi-level optimization problem. In theory, we present generalization error\nbounds for individual treatment effect estimation. Extensive experiments on\nfour benchmark datasets demonstrate that the proposed DWR algorithm outperforms\nstate-of-the-art methods for estimating individual treatment effects under\nheterogeneous interference."}, "http://arxiv.org/abs/2302.10836": {"title": "nlive: an R Package to facilitate the application of the sigmoidal and random changepoint mixed models", "link": "http://arxiv.org/abs/2302.10836", "description": "Background: The use of mixed effect models with a specific functional form\nsuch as the Sigmoidal Mixed Model and the Piecewise Mixed Model (or Changepoint\nMixed Model) with abrupt or smooth random change allows the interpretation of\nthe defined parameters to understand longitudinal trajectories. Currently,\nthere are no interface R packages that can easily fit the Sigmoidal Mixed Model\nallowing the inclusion of covariates or incorporating recent developments to\nfit the Piecewise Mixed Model with random change. Results: To facilitate the\nmodeling of the Sigmoidal Mixed Model, and Piecewise Mixed Model with abrupt or\nsmooth random change, we have created an R package called nlive. All needed\npieces such as functions, covariance matrices, and initials generation were\nprogrammed. The package was implemented with recent developments such as the\npolynomial smooth transition of the piecewise mixed model with improved\nproperties over Bacon-Watts, and the stochastic approximation\nexpectation-maximization (SAEM) for efficient estimation. It was designed to\nhelp interpretation of the output by providing features such as annotated\noutput, warnings, and graphs. Functionality, including time and convergence,\nwas tested using simulations. We provided a data example to illustrate the\npackage use and output features and interpretation. The package implemented in\nthe R software is available from the Comprehensive R Archive Network (CRAN) at\nhttps://CRAN.R-project.org/package=nlive. Conclusions: The nlive package for R\nfits the Sigmoidal Mixed Model and the Piecewise Mixed: abrupt and smooth. The\nnlive allows fitting these models with only five mandatory arguments that are\nintuitive enough to the less sophisticated users."}, "http://arxiv.org/abs/2304.12500": {"title": "Environmental Justice Implications of Power Plant Emissions Control Policies: Heterogeneous Causal Effect Estimation under Bipartite Network Interference", "link": "http://arxiv.org/abs/2304.12500", "description": "Emissions generators, such as coal-fired power plants, are key contributors\nto air pollution and thus environmental policies to reduce their emissions have\nbeen proposed. Furthermore, marginalized groups are exposed to\ndisproportionately high levels of this pollution and have heightened\nsusceptibility to its adverse health impacts. As a result, robust evaluations\nof the heterogeneous impacts of air pollution regulations are key to justifying\nand designing maximally protective interventions. However, such evaluations are\ncomplicated in that much of air pollution regulatory policy intervenes on large\nemissions generators while resulting impacts are measured in potentially\ndistant populations. Such a scenario can be described as that of bipartite\nnetwork interference (BNI). To our knowledge, no literature to date has\nconsidered estimation of heterogeneous causal effects with BNI. In this paper,\nwe contribute to the literature in a three-fold manner. First, we propose\nBNI-specific estimators for subgroup-specific causal effects and design an\nempirical Monte Carlo simulation approach for BNI to evaluate their\nperformance. Second, we demonstrate how these estimators can be combined with\nsubgroup discovery approaches to identify subgroups benefiting most from air\npollution policies without a priori specification. Finally, we apply the\nproposed methods to estimate the effects of coal-fired power plant emissions\ncontrol interventions on ischemic heart disease (IHD) among 27,312,190 US\nMedicare beneficiaries. Though we find no statistically significant effect of\nthe interventions in the full population, we do find significant IHD\nhospitalization decreases in communities with high poverty and smoking rates."}, "http://arxiv.org/abs/2306.00686": {"title": "A novel approach for estimating functions in the multivariate setting based on an adaptive knot selection for B-splines with an application to a chemical system used in geoscience", "link": "http://arxiv.org/abs/2306.00686", "description": "In this paper, we will outline a novel data-driven method for estimating\nfunctions in a multivariate nonparametric regression model based on an adaptive\nknot selection for B-splines. The underlying idea of our approach for selecting\nknots is to apply the generalized lasso, since the knots of the B-spline basis\ncan be seen as changes in the derivatives of the function to be estimated. This\nmethod was then extended to functions depending on several variables by\nprocessing each dimension independently, thus reducing the problem to a\nunivariate setting. The regularization parameters were chosen by means of a\ncriterion based on EBIC. The nonparametric estimator was obtained using a\nmultivariate B-spline regression with the corresponding selected knots. Our\nprocedure was validated through numerical experiments by varying the number of\nobservations and the level of noise to investigate its robustness. The\ninfluence of observation sampling was also assessed and our method was applied\nto a chemical system commonly used in geoscience. For each different framework\nconsidered in this paper, our approach performed better than state-of-the-art\nmethods. Our completely data-driven method is implemented in the glober R\npackage which is available on the Comprehensive R Archive Network (CRAN)."}, "http://arxiv.org/abs/2401.14426": {"title": "M$^3$TN: Multi-gate Mixture-of-Experts based Multi-valued Treatment Network for Uplift Modeling", "link": "http://arxiv.org/abs/2401.14426", "description": "Uplift modeling is a technique used to predict the effect of a treatment\n(e.g., discounts) on an individual's response. Although several methods have\nbeen proposed for multi-valued treatment, they are extended from binary\ntreatment methods. There are still some limitations. Firstly, existing methods\ncalculate uplift based on predicted responses, which may not guarantee a\nconsistent uplift distribution between treatment and control groups. Moreover,\nthis may cause cumulative errors for multi-valued treatment. Secondly, the\nmodel parameters become numerous with many prediction heads, leading to reduced\nefficiency. To address these issues, we propose a novel \\underline{M}ulti-gate\n\\underline{M}ixture-of-Experts based \\underline{M}ulti-valued\n\\underline{T}reatment \\underline{N}etwork (M$^3$TN). M$^3$TN consists of two\ncomponents: 1) a feature representation module with Multi-gate\nMixture-of-Experts to improve the efficiency; 2) a reparameterization module by\nmodeling uplift explicitly to improve the effectiveness. We also conduct\nextensive experiments to demonstrate the effectiveness and efficiency of our\nM$^3$TN."}, "http://arxiv.org/abs/2401.14512": {"title": "Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population", "link": "http://arxiv.org/abs/2401.14512", "description": "Randomized controlled trials (RCTs) serve as the cornerstone for\nunderstanding causal effects, yet extending inferences to target populations\npresents challenges due to effect heterogeneity and underrepresentation. Our\npaper addresses the critical issue of identifying and characterizing\nunderrepresented subgroups in RCTs, proposing a novel framework for refining\ntarget populations to improve generalizability. We introduce an\noptimization-based approach, Rashomon Set of Optimal Trees (ROOT), to\ncharacterize underrepresented groups. ROOT optimizes the target subpopulation\ndistribution by minimizing the variance of the target average treatment effect\nestimate, ensuring more precise treatment effect estimations. Notably, ROOT\ngenerates interpretable characteristics of the underrepresented population,\naiding researchers in effective communication. Our approach demonstrates\nimproved precision and interpretability compared to alternatives, as\nillustrated with synthetic data experiments. We apply our methodology to extend\ninferences from the Starting Treatment with Agonist Replacement Therapies\n(START) trial -- investigating the effectiveness of medication for opioid use\ndisorder -- to the real-world population represented by the Treatment Episode\nDataset: Admissions (TEDS-A). By refining target populations using ROOT, our\nframework offers a systematic approach to enhance decision-making accuracy and\ninform future trials in diverse populations."}, "http://arxiv.org/abs/2401.14515": {"title": "Martingale Posterior Distributions for Log-concave Density Functions", "link": "http://arxiv.org/abs/2401.14515", "description": "The family of log-concave density functions contains various kinds of common\nprobability distributions. Due to the shape restriction, it is possible to find\nthe nonparametric estimate of the density, for example, the nonparametric\nmaximum likelihood estimate (NPMLE). However, the associated uncertainty\nquantification of the NPMLE is less well developed. The current techniques for\nuncertainty quantification are Bayesian, using a Dirichlet process prior\ncombined with the use of Markov chain Monte Carlo (MCMC) to sample from the\nposterior. In this paper, we start with the NPMLE and use a version of the\nmartingale posterior distribution to establish uncertainty about the NPMLE. The\nalgorithm can be implemented in parallel and hence is fast. We prove the\nconvergence of the algorithm by constructing suitable submartingales. We also\nillustrate results with different models and settings and some real data, and\ncompare our method with that within the literature."}, "http://arxiv.org/abs/2401.14535": {"title": "CaRiNG: Learning Temporal Causal Representation under Non-Invertible Generation Process", "link": "http://arxiv.org/abs/2401.14535", "description": "Identifying the underlying time-delayed latent causal processes in sequential\ndata is vital for grasping temporal dynamics and making downstream reasoning.\nWhile some recent methods can robustly identify these latent causal variables,\nthey rely on strict assumptions about the invertible generation process from\nlatent variables to observed data. However, these assumptions are often hard to\nsatisfy in real-world applications containing information loss. For instance,\nthe visual perception process translates a 3D space into 2D images, or the\nphenomenon of persistence of vision incorporates historical data into current\nperceptions. To address this challenge, we establish an identifiability theory\nthat allows for the recovery of independent latent components even when they\ncome from a nonlinear and non-invertible mix. Using this theory as a\nfoundation, we propose a principled approach, CaRiNG, to learn the CAusal\nRepresentatIon of Non-invertible Generative temporal data with identifiability\nguarantees. Specifically, we utilize temporal context to recover lost latent\ninformation and apply the conditions in our theory to guide the training\nprocess. Through experiments conducted on synthetic datasets, we validate that\nour CaRiNG method reliably identifies the causal process, even when the\ngeneration process is non-invertible. Moreover, we demonstrate that our\napproach considerably improves temporal understanding and reasoning in\npractical applications."}, "http://arxiv.org/abs/2401.14549": {"title": "Privacy-preserving Quantile Treatment Effect Estimation for Randomized Controlled Trials", "link": "http://arxiv.org/abs/2401.14549", "description": "In accordance with the principle of \"data minimization\", many internet\ncompanies are opting to record less data. However, this is often at odds with\nA/B testing efficacy. For experiments with units with multiple observations,\none popular data minimizing technique is to aggregate data for each unit.\nHowever, exact quantile estimation requires the full observation-level data. In\nthis paper, we develop a method for approximate Quantile Treatment Effect (QTE)\nanalysis using histogram aggregation. In addition, we can also achieve formal\nprivacy guarantees using differential privacy."}, "http://arxiv.org/abs/2401.14558": {"title": "Simulation Model Calibration with Dynamic Stratification and Adaptive Sampling", "link": "http://arxiv.org/abs/2401.14558", "description": "Calibrating simulation models that take large quantities of multi-dimensional\ndata as input is a hard simulation optimization problem. Existing adaptive\nsampling strategies offer a methodological solution. However, they may not\nsufficiently reduce the computational cost for estimation and solution\nalgorithm's progress within a limited budget due to extreme noise levels and\nheteroskedasticity of system responses. We propose integrating stratification\nwith adaptive sampling for the purpose of efficiency in optimization.\nStratification can exploit local dependence in the simulation inputs and\noutputs. Yet, the state-of-the-art does not provide a full capability to\nadaptively stratify the data as different solution alternatives are evaluated.\nWe devise two procedures for data-driven calibration problems that involve a\nlarge dataset with multiple covariates to calibrate models within a fixed\noverall simulation budget. The first approach dynamically stratifies the input\ndata using binary trees, while the second approach uses closed-form solutions\nbased on linearity assumptions between the objective function and concomitant\nvariables. We find that dynamical adjustment of stratification structure\naccelerates optimization and reduces run-to-run variability in generated\nsolutions. Our case study for calibrating a wind power simulation model, widely\nused in the wind industry, using the proposed stratified adaptive sampling,\nshows better-calibrated parameters under a limited budget."}, "http://arxiv.org/abs/2401.14562": {"title": "Properties of the Mallows Model Depending on the Number of Alternatives: A Warning for an Experimentalist", "link": "http://arxiv.org/abs/2401.14562", "description": "The Mallows model is a popular distribution for ranked data. We empirically\nand theoretically analyze how the properties of rankings sampled from the\nMallows model change when increasing the number of alternatives. We find that\nreal-world data behaves differently than the Mallows model, yet is in line with\nits recent variant proposed by Boehmer et al. [2021]. As part of our study, we\nissue several warnings about using the model."}, "http://arxiv.org/abs/2401.14593": {"title": "Robust Estimation of Pareto's Scale Parameter from Grouped Data", "link": "http://arxiv.org/abs/2401.14593", "description": "Numerous robust estimators exist as alternatives to the maximum likelihood\nestimator (MLE) when a completely observed ground-up loss severity sample\ndataset is available. However, the options for robust alternatives to MLE\nbecome significantly limited when dealing with grouped loss severity data, with\nonly a handful of methods like least squares, minimum Hellinger distance, and\noptimal bounded influence function available. This paper introduces a novel\nrobust estimation technique, the Method of Truncated Moments (MTuM),\nspecifically designed to estimate the tail index of a Pareto distribution from\ngrouped data. Inferential justification of MTuM is established by employing the\ncentral limit theorem and validating them through a comprehensive simulation\nstudy."}, "http://arxiv.org/abs/2401.14655": {"title": "Distributionally Robust Optimization and Robust Statistics", "link": "http://arxiv.org/abs/2401.14655", "description": "We review distributionally robust optimization (DRO), a principled approach\nfor constructing statistical estimators that hedge against the impact of\ndeviations in the expected loss between the training and deployment\nenvironments. Many well-known estimators in statistics and machine learning\n(e.g. AdaBoost, LASSO, ridge regression, dropout training, etc.) are\ndistributionally robust in a precise sense. We hope that by discussing the DRO\ninterpretation of well-known estimators, statisticians who may not be too\nfamiliar with DRO may find a way to access the DRO literature through the\nbridge between classical results and their DRO equivalent formulation. On the\nother hand, the topic of robustness in statistics has a rich tradition\nassociated with removing the impact of contamination. Thus, another objective\nof this paper is to clarify the difference between DRO and classical\nstatistical robustness. As we will see, these are two fundamentally different\nphilosophies leading to completely different types of estimators. In DRO, the\nstatistician hedges against an environment shift that occurs after the decision\nis made; thus DRO estimators tend to be pessimistic in an adversarial setting,\nleading to a min-max type formulation. In classical robust statistics, the\nstatistician seeks to correct contamination that occurred before a decision is\nmade; thus robust statistical estimators tend to be optimistic leading to a\nmin-min type formulation."}, "http://arxiv.org/abs/2401.14684": {"title": "Inference for Cumulative Incidences and Treatment Effects in Randomized Controlled Trials with Time-to-Event Outcomes under ICH E9 (E1)", "link": "http://arxiv.org/abs/2401.14684", "description": "In randomized controlled trials (RCT) with time-to-event outcomes,\nintercurrent events occur as semi-competing/competing events, and they could\naffect the hazard of outcomes or render outcomes ill-defined. Although five\nstrategies have been proposed in ICH E9 (R1) addendum to address intercurrent\nevents in RCT, they did not readily extend to the context of time-to-event data\nfor studying causal effects with rigorously stated implications. In this study,\nwe show how to define, estimate, and infer the time-dependent cumulative\nincidence of outcome events in such contexts for obtaining causal\ninterpretations. Specifically, we derive the mathematical forms of the\nscientific objective (i.e., causal estimands) under the five strategies and\nclarify the required data structure to identify these causal estimands.\nFurthermore, we summarize estimation and inference methods for these causal\nestimands by adopting methodologies in survival analysis, including analytic\nformulas for asymptotic analysis and hypothesis testing. We illustrate our\nmethods with the LEADER Trial on investigating the effect of liraglutide on\ncardiovascular outcomes. Studies of multiple endpoints and combining strategies\nto address multiple intercurrent events can help practitioners understand\ntreatment effects more comprehensively."}, "http://arxiv.org/abs/2401.14722": {"title": "A Nonparametric Bayes Approach to Online Activity Prediction", "link": "http://arxiv.org/abs/2401.14722", "description": "Accurately predicting the onset of specific activities within defined\ntimeframes holds significant importance in several applied contexts. In\nparticular, accurate prediction of the number of future users that will be\nexposed to an intervention is an important piece of information for\nexperimenters running online experiments (A/B tests). In this work, we propose\na novel approach to predict the number of users that will be active in a given\ntime period, as well as the temporal trajectory needed to attain a desired user\nparticipation threshold. We model user activity using a Bayesian nonparametric\napproach which allows us to capture the underlying heterogeneity in user\nengagement. We derive closed-form expressions for the number of new users\nexpected in a given period, and a simple Monte Carlo algorithm targeting the\nposterior distribution of the number of days needed to attain a desired number\nof users; the latter is important for experimental planning. We illustrate the\nperformance of our approach via several experiments on synthetic and real world\ndata, in which we show that our novel method outperforms existing competitors."}, "http://arxiv.org/abs/2401.14827": {"title": "Clustering Longitudinal Ordinal Data via Finite Mixture of Matrix-Variate Distributions", "link": "http://arxiv.org/abs/2401.14827", "description": "In social sciences, studies are often based on questionnaires asking\nparticipants to express ordered responses several times over a study period. We\npresent a model-based clustering algorithm for such longitudinal ordinal data.\nAssuming that an ordinal variable is the discretization of a underlying latent\ncontinuous variable, the model relies on a mixture of matrix-variate normal\ndistributions, accounting simultaneously for within- and between-time\ndependence structures. The model is thus able to concurrently model the\nheterogeneity, the association among the responses and the temporal dependence\nstructure. An EM algorithm is developed and presented for parameters\nestimation. An evaluation of the model through synthetic data shows its\nestimation abilities and its advantages when compared to competitors. A\nreal-world application concerning changes in eating behaviours during the\nCovid-19 pandemic period in France will be presented."}, "http://arxiv.org/abs/2401.14836": {"title": "Automatic and location-adaptive estimation in functional single-index regression", "link": "http://arxiv.org/abs/2401.14836", "description": "This paper develops a new automatic and location-adaptive procedure for\nestimating regression in a Functional Single-Index Model (FSIM). This procedure\nis based on $k$-Nearest Neighbours ($k$NN) ideas. The asymptotic study includes\nresults for automatically data-driven selected number of neighbours, making the\nprocedure directly usable in practice. The local feature of the $k$NN approach\ninsures higher predictive power compared with usual kernel estimates, as\nillustrated in some finite sample analysis. As by-product we state as\npreliminary tools some new uniform asymptotic results for kernel estimates in\nthe FSIM model."}, "http://arxiv.org/abs/2401.14841": {"title": "Sparse semiparametric regression when predictors are mixture of functional and high-dimensional variables", "link": "http://arxiv.org/abs/2401.14841", "description": "This paper aims to front with dimensionality reduction in regression setting\nwhen the predictors are a mixture of functional variable and high-dimensional\nvector. A flexible model, combining both sparse linear ideas together with\nsemiparametrics, is proposed. A wide scope of asymptotic results is provided:\nthis covers as well rates of convergence of the estimators as asymptotic\nbehaviour of the variable selection procedure. Practical issues are analysed\nthrough finite sample simulated experiments while an application to Tecator's\ndata illustrates the usefulness of our methodology."}, "http://arxiv.org/abs/2401.14848": {"title": "A $k$NN procedure in semiparametric functional data analysis", "link": "http://arxiv.org/abs/2401.14848", "description": "A fast and flexible $k$NN procedure is developed for dealing with a\nsemiparametric functional regression model involving both partial-linear and\nsingle-index components. Rates of uniform consistency are presented. Simulated\nexperiments highlight the advantages of the $k$NN procedure. A real data\nanalysis is also shown."}, "http://arxiv.org/abs/2401.14864": {"title": "Fast and efficient algorithms for sparse semiparametric bi-functional regression", "link": "http://arxiv.org/abs/2401.14864", "description": "A new sparse semiparametric model is proposed, which incorporates the\ninfluence of two functional random variables in a scalar response in a flexible\nand interpretable manner. One of the functional covariates is included through\na single-index structure, while the other is included linearly through the\nhigh-dimensional vector formed by its discretised observations. For this model,\ntwo new algorithms are presented for selecting relevant variables in the linear\npart and estimating the model. Both procedures utilise the functional origin of\nlinear covariates. Finite sample experiments demonstrated the scope of\napplication of both algorithms: the first method is a fast algorithm that\nprovides a solution (without loss in predictive ability) for the significant\ncomputational time required by standard variable selection methods for\nestimating this model, and the second algorithm completes the set of relevant\nlinear covariates provided by the first, thus improving its predictive\nefficiency. Some asymptotic results theoretically support both procedures. A\nreal data application demonstrated the applicability of the presented\nmethodology from a predictive perspective in terms of the interpretability of\noutputs and low computational cost."}, "http://arxiv.org/abs/2401.14867": {"title": "Variable selection in functional regression models: a review", "link": "http://arxiv.org/abs/2401.14867", "description": "Despite of various similar features, Functional Data Analysis and\nHigh-Dimensional Data Analysis are two major fields in Statistics that grew up\nrecently almost independently one from each other. The aim of this paper is to\npropose a survey on methodological advances for variable selection in\nfunctional regression, which is typically a question for which both functional\nand multivariate ideas are crossing. More than a simple survey, this paper aims\nto promote even more new links between both areas."}, "http://arxiv.org/abs/2401.14902": {"title": "Model-assisted survey sampling with Bayesian optimization", "link": "http://arxiv.org/abs/2401.14902", "description": "Survey sampling plays an important role in the efficient allocation and\nmanagement of resources. The essence of survey sampling lies in acquiring a\nsample of data points from a population and subsequently using this sample to\nestimate the population parameters of the targeted response variable, such as\nenvironmental-related metrics or other pertinent factors. Practical limitations\nimposed on survey sampling necessitate prudent consideration of the number of\nsamples attainable from the study areas, given the constraints of a fixed\nbudget. To this end, researchers are compelled to employ sampling designs that\noptimize sample allocations to the best of their ability. Generally,\nprobability sampling serves as the preferred method, ensuring an unbiased\nestimation of population parameters. Evaluating the efficiency of estimators\ninvolves assessing their variances and benchmarking them against alternative\nbaseline approaches, such as simple random sampling. In this study, we propose\na novel model-assisted unbiased probability sampling method that leverages\nBayesian optimization for the determination of sampling designs. As a result,\nthis approach can yield in estimators with more efficient variance outcomes\ncompared to the conventional estimators such as the Horvitz-Thompson.\nFurthermore, we test the proposed method in a simulation study using an\nempirical dataset covering plot-level tree volume from central Finland. The\nresults demonstrate statistically significant improved performance for the\nproposed method when compared to the baseline."}, "http://arxiv.org/abs/2401.14910": {"title": "Modeling Extreme Events: Univariate and Multivariate Data-Driven Approaches", "link": "http://arxiv.org/abs/2401.14910", "description": "Modern inference in extreme value theory faces numerous complications, such\nas missing data, hidden covariates or design problems. Some of those\ncomplications were exemplified in the EVA 2023 data challenge. The challenge\ncomprises multiple individual problems which cover a variety of univariate and\nmultivariate settings. This note presents the contribution of team genEVA in\nsaid competition, with particular focus on a detailed presentation of\nmethodology and inference."}, "http://arxiv.org/abs/2401.15014": {"title": "A Robust Bayesian Method for Building Polygenic Risk Scores using Projected Summary Statistics and Bridge Prior", "link": "http://arxiv.org/abs/2401.15014", "description": "Polygenic Risk Scores (PRS) developed from genome-wide association studies\n(GWAS) are of increasing interest for various clinical and research\napplications. Bayesian methods have been particularly popular for building PRS\nin genome-wide scale because of their natural ability to regularize model and\nborrow information in high-dimension. In this article, we present new\ntheoretical results, methods, and extensive numerical studies to advance\nBayesian methods for PRS applications. We conduct theoretical studies to\nidentify causes of convergence issues of some Bayesian methods when required\ninput GWAS summary-statistics and linkage disequilibrium (LD) (genetic\ncorrelation) data are derived from distinct samples. We propose a remedy to the\nproblem by the projection of the summary-statistics data into the column space\nof the genetic correlation matrix. We further implement a PRS development\nalgorithm under the Bayesian Bridge prior which can allow more flexible\nspecification of effect-size distribution than those allowed under popular\nalternative methods. Finally, we conduct careful benchmarking studies of\nalternative Bayesian methods using both simulation studies and real datasets,\nwhere we carefully investigate both the effect of prior specification and\nestimation strategies for LD parameters. These studies show that the proposed\nalgorithm, equipped with the projection approach, the flexible prior\nspecification, and an efficient numerical algorithm leads to the development of\nthe most robust PRS across a wide variety of scenarios."}, "http://arxiv.org/abs/2401.15063": {"title": "Graph fission and cross-validation", "link": "http://arxiv.org/abs/2401.15063", "description": "We introduce a technique called graph fission which takes in a graph which\npotentially contains only one observation per node (whose distribution lies in\na known class) and produces two (or more) independent graphs with the same\nnode/edge set in a way that splits the original graph's information amongst\nthem in any desired proportion. Our proposal builds on data fission/thinning, a\nmethod that uses external randomization to create independent copies of an\nunstructured dataset. %under the assumption of independence between\nobservations. We extend this idea to the graph setting where there may be\nlatent structure between observations. We demonstrate the utility of this\nframework via two applications: inference after structural trend estimation on\ngraphs and a model selection procedure we term ``graph cross-validation''."}, "http://arxiv.org/abs/2401.15076": {"title": "Comparative Analysis of Practical Identifiability Methods for an SEIR Model", "link": "http://arxiv.org/abs/2401.15076", "description": "Identifiability of a mathematical model plays a crucial role in\nparameterization of the model. In this study, we establish the structural\nidentifiability of a Susceptible-Exposed-Infected-Recovered (SEIR) model given\ndifferent combinations of input data and investigate practical identifiability\nwith respect to different observable data, data frequency, and noise\ndistributions. The practical identifiability is explored by both Monte Carlo\nsimulations and a Correlation Matrix approach. Our results show that practical\nidentifiability benefits from higher data frequency and data from the peak of\nan outbreak. The incidence data gives the best practical identifiability\nresults compared to prevalence and cumulative data. In addition, we compare and\ndistinguish the practical identifiability by Monte Carlo simulations and a\nCorrelation Matrix approach, providing insights for when to use which method\nfor other applications."}, "http://arxiv.org/abs/2105.02487": {"title": "High-dimensional Functional Graphical Model Structure Learning via Neighborhood Selection Approach", "link": "http://arxiv.org/abs/2105.02487", "description": "Undirected graphical models are widely used to model the conditional\nindependence structure of vector-valued data. However, in many modern\napplications, for example those involving EEG and fMRI data, observations are\nmore appropriately modeled as multivariate random functions rather than\nvectors. Functional graphical models have been proposed to model the\nconditional independence structure of such functional data. We propose a\nneighborhood selection approach to estimate the structure of Gaussian\nfunctional graphical models, where we first estimate the neighborhood of each\nnode via a function-on-function regression and subsequently recover the entire\ngraph structure by combining the estimated neighborhoods. Our approach only\nrequires assumptions on the conditional distributions of random functions, and\nwe estimate the conditional independence structure directly. We thus circumvent\nthe need for a well-defined precision operator that may not exist when the\nfunctions are infinite dimensional. Additionally, the neighborhood selection\napproach is computationally efficient and can be easily parallelized. The\nstatistical consistency of the proposed method in the high-dimensional setting\nis supported by both theory and experimental results. In addition, we study the\neffect of the choice of the function basis used for dimensionality reduction in\nan intermediate step. We give a heuristic criterion for choosing a function\nbasis and motivate two practically useful choices, which we justify by both\ntheory and experiments."}, "http://arxiv.org/abs/2203.11469": {"title": "A new class of composite GBII regression models with varying threshold for modelling heavy-tailed data", "link": "http://arxiv.org/abs/2203.11469", "description": "The four-parameter generalized beta distribution of the second kind (GBII)\nhas been proposed for modelling insurance losses with heavy-tailed features.\nThe aim of this paper is to present a parametric composite GBII regression\nmodelling by splicing two GBII distributions using mode matching method. It is\ndesigned for simultaneous modeling of small and large claims and capturing the\npolicyholder heterogeneity by introducing the covariates into the location\nparameter. In such cases, the threshold that splits two GBII distributions\nvaries across individuals policyholders based on their risk features. The\nproposed regression modelling also contains a wide range of insurance loss\ndistributions as the head and the tail respectively and provides the\nclose-formed expressions for parameter estimation and model prediction. A\nsimulation study is conducted to show the accuracy of the proposed estimation\nmethod and the flexibility of the regressions. Some illustrations of the\napplicability of the new class of distributions and regressions are provided\nwith a Danish fire losses data set and a Chinese medical insurance claims data\nset, comparing with the results of competing models from the literature."}, "http://arxiv.org/abs/2206.14674": {"title": "Signature Methods in Machine Learning", "link": "http://arxiv.org/abs/2206.14674", "description": "Signature-based techniques give mathematical insight into the interactions\nbetween complex streams of evolving data. These insights can be quite naturally\ntranslated into numerical approaches to understanding streamed data, and\nperhaps because of their mathematical precision, have proved useful in\nanalysing streamed data in situations where the data is irregular, and not\nstationary, and the dimension of the data and the sample sizes are both\nmoderate. Understanding streamed multi-modal data is exponential: a word in $n$\nletters from an alphabet of size $d$ can be any one of $d^n$ messages.\nSignatures remove the exponential amount of noise that arises from sampling\nirregularity, but an exponential amount of information still remain. This\nsurvey aims to stay in the domain where that exponential scaling can be managed\ndirectly. Scalability issues are an important challenge in many problems but\nwould require another survey article and further ideas. This survey describes a\nrange of contexts where the data sets are small enough to remove the\npossibility of massive machine learning, and the existence of small sets of\ncontext free and principled features can be used effectively. The mathematical\nnature of the tools can make their use intimidating to non-mathematicians. The\nexamples presented in this article are intended to bridge this communication\ngap and provide tractable working examples drawn from the machine learning\ncontext. Notebooks are available online for several of these examples. This\nsurvey builds on the earlier paper of Ilya Chevryev and Andrey Kormilitzin\nwhich had broadly similar aims at an earlier point in the development of this\nmachinery. This article illustrates how the theoretical insights offered by\nsignatures are simply realised in the analysis of application data in a way\nthat is largely agnostic to the data type."}, "http://arxiv.org/abs/2208.08925": {"title": "Efficiency of nonparametric e-tests", "link": "http://arxiv.org/abs/2208.08925", "description": "The notion of an e-value has been recently proposed as a possible alternative\nto critical regions and p-values in statistical hypothesis testing. In this\npaper we consider testing the nonparametric hypothesis of symmetry, introduce\nanalogues for e-values of three popular nonparametric tests, define an analogue\nfor e-values of Pitman's asymptotic relative efficiency, and apply it to the\nthree nonparametric tests. We discuss limitations of our simple definition of\nasymptotic relative efficiency and list directions of further research."}, "http://arxiv.org/abs/2211.16468": {"title": "Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs", "link": "http://arxiv.org/abs/2211.16468", "description": "Causal effect estimation from observational data is a fundamental task in\nempirical sciences. It becomes particularly challenging when unobserved\nconfounders are involved in a system. This paper focuses on front-door\nadjustment -- a classic technique which, using observed mediators allows to\nidentify causal effects even in the presence of unobserved confounding. While\nthe statistical properties of the front-door estimation are quite well\nunderstood, its algorithmic aspects remained unexplored for a long time. In\n2022, Jeong, Tian, and Bareinboim presented the first polynomial-time algorithm\nfor finding sets satisfying the front-door criterion in a given directed\nacyclic graph (DAG), with an $O(n^3(n+m))$ run time, where $n$ denotes the\nnumber of variables and $m$ the number of edges of the causal graph. In our\nwork, we give the first linear-time, i.e., $O(n+m)$, algorithm for this task,\nwhich thus reaches the asymptotically optimal time complexity. This result\nimplies an $O(n(n+m))$ delay enumeration algorithm of all front-door adjustment\nsets, again improving previous work by a factor of $n^3$. Moreover, we provide\nthe first linear-time algorithm for finding a minimal front-door adjustment\nset. We offer implementations of our algorithms in multiple programming\nlanguages to facilitate practical usage and empirically validate their\nfeasibility, even for large graphs."}, "http://arxiv.org/abs/2305.13221": {"title": "Incorporating Subsampling into Bayesian Models for High-Dimensional Spatial Data", "link": "http://arxiv.org/abs/2305.13221", "description": "Additive spatial statistical models with weakly stationary process\nassumptions have become standard in spatial statistics. However, one\ndisadvantage of such models is the computation time, which rapidly increases\nwith the number of data points. The goal of this article is to apply an\nexisting subsampling strategy to standard spatial additive models and to derive\nthe spatial statistical properties. We call this strategy the \"spatial data\nsubset model\" (SDSM) approach, which can be applied to big datasets in a\ncomputationally feasible way. Our approach has the advantage that one does not\nrequire any additional restrictive model assumptions. That is, computational\ngains increase as model assumptions are removed when using our model framework.\nThis provides one solution to the computational bottlenecks that occur when\napplying methods such as Kriging to \"big data\". We provide several properties\nof this new spatial data subset model approach in terms of moments, sill,\nnugget, and range under several sampling designs. An advantage of our approach\nis that it subsamples without throwing away data, and can be implemented using\ndatasets of any size that can be stored. We present the results of the spatial\ndata subset model approach on simulated datasets, and on a large dataset\nconsists of 150,000 observations of daytime land surface temperatures measured\nby the MODIS instrument onboard the Terra satellite."}, "http://arxiv.org/abs/2305.13818": {"title": "A Rank-Based Sequential Test of Independence", "link": "http://arxiv.org/abs/2305.13818", "description": "We consider the problem of independence testing for two univariate random\nvariables in a sequential setting. By leveraging recent developments on safe,\nanytime-valid inference, we propose a test with time-uniform type I error\ncontrol and derive explicit bounds on the finite sample performance of the\ntest. We demonstrate the empirical performance of the procedure in comparison\nto existing sequential and non-sequential independence tests. Furthermore,\nsince the proposed test is distribution free under the null hypothesis, we\nempirically simulate the gap due to Ville's inequality, the supermartingale\nanalogue of Markov's inequality, that is commonly applied to control type I\nerror in anytime-valid inference, and apply this to construct a truncated\nsequential test."}, "http://arxiv.org/abs/2307.08594": {"title": "Tight Distribution-Free Confidence Intervals for Local Quantile Regression", "link": "http://arxiv.org/abs/2307.08594", "description": "It is well known that it is impossible to construct useful confidence\nintervals (CIs) about the mean or median of a response $Y$ conditional on\nfeatures $X = x$ without making strong assumptions about the joint distribution\nof $X$ and $Y$. This paper introduces a new framework for reasoning about\nproblems of this kind by casting the conditional problem at different levels of\nresolution, ranging from coarse to fine localization. In each of these\nproblems, we consider local quantiles defined as the marginal quantiles of $Y$\nwhen $(X,Y)$ is resampled in such a way that samples $X$ near $x$ are\nup-weighted while the conditional distribution $Y \\mid X$ does not change. We\nthen introduce the Weighted Quantile method, which asymptotically produces the\nuniformly most accurate confidence intervals for these local quantiles no\nmatter the (unknown) underlying distribution. Another method, namely, the\nQuantile Rejection method, achieves finite sample validity under no assumption\nwhatsoever. We conduct extensive numerical studies demonstrating that both of\nthese methods are valid. In particular, we show that the Weighted Quantile\nprocedure achieves nominal coverage as soon as the effective sample size is in\nthe range of 10 to 20."}, "http://arxiv.org/abs/2307.08685": {"title": "Evaluating Climate Models with Sliced Elastic Distance", "link": "http://arxiv.org/abs/2307.08685", "description": "The validation of global climate models plays a crucial role in ensuring the\naccuracy of climatological predictions. However, existing statistical methods\nfor evaluating differences between climate fields often overlook time\nmisalignment and therefore fail to distinguish between sources of variability.\nTo more comprehensively measure differences between climate fields, we\nintroduce a new vector-valued metric, the sliced elastic distance. This new\nmetric simultaneously accounts for spatial and temporal variability while\ndecomposing the total distance into shape differences (amplitude), timing\nvariability (phase), and bias (translation). We compare the sliced elastic\ndistance against a classical metric and a newly developed Wasserstein-based\napproach through a simulation study. Our results demonstrate that the sliced\nelastic distance outperforms previous methods by capturing a broader range of\nfeatures. We then apply our metric to evaluate the historical model outputs of\nthe Coupled Model Intercomparison Project (CMIP) members, focusing on monthly\naverage surface temperatures and monthly total precipitation. By comparing\nthese model outputs with quasi-observational ERA5 Reanalysis data products, we\nrank the CMIP models and assess their performance. Additionally, we investigate\nthe progression from CMIP phase 5 to phase 6 and find modest improvements in\nthe phase 6 models regarding their ability to produce realistic climate\ndynamics."}, "http://arxiv.org/abs/2401.15139": {"title": "FDR-Controlled Portfolio Optimization for Sparse Financial Index Tracking", "link": "http://arxiv.org/abs/2401.15139", "description": "In high-dimensional data analysis, such as financial index tracking or\nbiomedical applications, it is crucial to select the few relevant variables\nwhile maintaining control over the false discovery rate (FDR). In these\napplications, strong dependencies often exist among the variables (e.g., stock\nreturns), which can undermine the FDR control property of existing methods like\nthe model-X knockoff method or the T-Rex selector. To address this issue, we\nhave expanded the T-Rex framework to accommodate overlapping groups of highly\ncorrelated variables. This is achieved by integrating a nearest neighbors\npenalization mechanism into the framework, which provably controls the FDR at\nthe user-defined target level. A real-world example of sparse index tracking\ndemonstrates the proposed method's ability to accurately track the S&amp;P 500\nindex over the past 20 years based on a small number of stocks. An open-source\nimplementation is provided within the R package TRexSelector on CRAN."}, "http://arxiv.org/abs/2401.15225": {"title": "A bivariate two-state Markov modulated Poisson process for failure modelling", "link": "http://arxiv.org/abs/2401.15225", "description": "Motivated by a real failure dataset in a two-dimensional context, this paper\npresents an extension of the Markov modulated Poisson process (MMPP) to two\ndimensions. The one-dimensional MMPP has been proposed for the modeling of\ndependent and non-exponential inter-failure times (in contexts as queuing, risk\nor reliability, among others). The novel two-dimensional MMPP allows for\ndependence between the two sequences of inter-failure times, while at the same\ntime preserves the MMPP properties, marginally. The generalization is based on\nthe Marshall-Olkin exponential distribution. Inference is undertaken for the\nnew model through a method combining a matching moments approach with an\nApproximate Bayesian Computation (ABC) algorithm. The performance of the method\nis shown on simulated and real datasets representing times and distances\ncovered between consecutive failures in a public transport company. For the\nreal dataset, some quantities of importance associated with the reliability of\nthe system are estimated as the probabilities and expected number of failures\nat different times and distances covered by trains until the occurrence of a\nfailure."}, "http://arxiv.org/abs/2401.15253": {"title": "Testing the Exogeneity of Instrumental Variables and Regressors in Linear Regression Models Using Copulas", "link": "http://arxiv.org/abs/2401.15253", "description": "We provide a Copula-based approach to test the exogeneity of instrumental\nvariables in linear regression models. We show that the exogeneity of\ninstrumental variables is equivalent to the exogeneity of their standard normal\ntransformations with the same CDF value. Then, we establish a Wald test for the\nexogeneity of the instrumental variables. We demonstrate the performance of our\ntest using simulation studies. Our simulations show that if the instruments are\nactually endogenous, our test rejects the exogeneity hypothesis approximately\n93% of the time at the 5% significance level. Conversely, when instruments are\ntruly exogenous, it dismisses the exogeneity assumption less than 30% of the\ntime on average for data with 200 observations and less than 2% of the time for\ndata with 1,000 observations. Our results demonstrate our test's effectiveness,\noffering significant value to applied econometricians."}, "http://arxiv.org/abs/2401.15259": {"title": "Estimating lengths-of-stay of hospitalised COVID-19 patients using a non-parametric model: a case study in Galicia (Spain)", "link": "http://arxiv.org/abs/2401.15259", "description": "Estimating the lengths-of-stay (LoS) of hospitalised COVID-19 patients is key\nfor predicting the hospital beds' demand and planning mitigation strategies, as\noverwhelming the healthcare systems has critical consequences for disease\nmortality. However, accurately mapping the time-to-event of hospital outcomes,\nsuch as the LoS in the intensive care unit (ICU), requires understanding\npatient trajectories while adjusting for covariates and observation bias, such\nas incomplete data. Standard methods, such as the Kaplan-Meier estimator,\nrequire prior assumptions that are untenable given current knowledge. Using\nreal-time surveillance data from the first weeks of the COVID-19 epidemic in\nGalicia (Spain), we aimed to model the time-to-event and event probabilities of\npatients' hospitalised, without parametric priors and adjusting for individual\ncovariates. We applied a non-parametric mixture cure model and compared its\nperformance in estimating hospital ward (HW)/ICU LoS to the performances of\ncommonly used methods to estimate survival. We showed that the proposed model\noutperformed standard approaches, providing more accurate ICU and HW LoS\nestimates. Finally, we applied our model estimates to simulate COVID-19\nhospital demand using a Monte Carlo algorithm. We provided evidence that\nadjusting for sex, generally overlooked in prediction models, together with age\nis key for accurately forecasting HW and ICU occupancy, as well as discharge or\ndeath outcomes."}, "http://arxiv.org/abs/2401.15262": {"title": "Asymptotic Behavior of Adversarial Training Estimator under $\\ell_\\infty$-Perturbation", "link": "http://arxiv.org/abs/2401.15262", "description": "Adversarial training has been proposed to hedge against adversarial attacks\nin machine learning and statistical models. This paper focuses on adversarial\ntraining under $\\ell_\\infty$-perturbation, which has recently attracted much\nresearch attention. The asymptotic behavior of the adversarial training\nestimator is investigated in the generalized linear model. The results imply\nthat the limiting distribution of the adversarial training estimator under\n$\\ell_\\infty$-perturbation could put a positive probability mass at $0$ when\nthe true parameter is $0$, providing a theoretical guarantee of the associated\nsparsity-recovery ability. Alternatively, a two-step procedure is proposed --\nadaptive adversarial training, which could further improve the performance of\nadversarial training under $\\ell_\\infty$-perturbation. Specifically, the\nproposed procedure could achieve asymptotic unbiasedness and variable-selection\nconsistency. Numerical experiments are conducted to show the sparsity-recovery\nability of adversarial training under $\\ell_\\infty$-perturbation and to compare\nthe empirical performance between classic adversarial training and adaptive\nadversarial training."}, "http://arxiv.org/abs/2401.15281": {"title": "Improved confidence intervals for nonlinear mixed-effects and nonparametric regression models", "link": "http://arxiv.org/abs/2401.15281", "description": "Statistical inference for high dimensional parameters (HDPs) can be based on\ntheir intrinsic correlation; that is, parameters that are close spatially or\ntemporally tend to have more similar values. This is why nonlinear\nmixed-effects models (NMMs) are commonly (and appropriately) used for models\nwith HDPs. Conversely, in many practical applications of NMM, the random\neffects (REs) are actually correlated HDPs that should remain constant during\nrepeated sampling for frequentist inference. In both scenarios, the inference\nshould be conditional on REs, instead of marginal inference by integrating out\nREs. In this paper, we first summarize recent theory of conditional inference\nfor NMM, and then propose a bias-corrected RE predictor and confidence interval\n(CI). We also extend this methodology to accommodate the case where some REs\nare not associated with data. Simulation studies indicate that this new\napproach leads to substantial improvement in the conditional coverage rate of\nRE CIs, including CIs for smooth functions in generalized additive models, as\ncompared to the existing method based on marginal inference."}, "http://arxiv.org/abs/2401.15309": {"title": "Zero-inflated Smoothing Spline (ZISS) Models for Individual-level Single-cell Temporal Data", "link": "http://arxiv.org/abs/2401.15309", "description": "Recent advancements in single-cell RNA-sequencing (scRNA-seq) have enhanced\nour understanding of cell heterogeneity at a high resolution. With the ability\nto sequence over 10,000 cells per hour, researchers can collect large scRNA-seq\ndatasets for different participants, offering an opportunity to study the\ntemporal progression of individual-level single-cell data. However, the\npresence of excessive zeros, a common issue in scRNA-seq, significantly impacts\nregression/association analysis, potentially leading to biased estimates in\ndownstream analysis. Addressing these challenges, we introduce the Zero\nInflated Smoothing Spline (ZISS) method, specifically designed to model\nsingle-cell temporal data. The ZISS method encompasses two components for\nmodeling gene expression patterns over time and handling excessive zeros. Our\napproach employs the smoothing spline ANOVA model, providing robust estimates\nof mean functions and zero probabilities for irregularly observed single-cell\ntemporal data compared to existing methods in our simulation studies and real\ndata analysis."}, "http://arxiv.org/abs/2401.15382": {"title": "Inference on an heteroscedastic Gompertz tumor growth model", "link": "http://arxiv.org/abs/2401.15382", "description": "We consider a non homogeneous Gompertz diffusion process whose parameters are\nmodified by generally time-dependent exogenous factors included in the\ninfinitesimal moments. The proposed model is able to describe tumor dynamics\nunder the effect of anti-proliferative and/or cell death-induced therapies. We\nassume that such therapies can modify also the infinitesimal variance of the\ndiffusion process. An estimation procedure, based on a control group and two\ntreated groups, is proposed to infer the model by estimating the constant\nparameters and the time-dependent terms. Moreover, several concatenated\nhypothesis tests are considered in order to confirm or reject the need to\ninclude time-dependent functions in the infinitesimal moments. Simulations are\nprovided to evaluate the efficiency of the suggested procedures and to validate\nthe testing hypothesis. Finally, an application to real data is considered."}, "http://arxiv.org/abs/2401.15461": {"title": "Anytime-Valid Tests of Group Invariance through Conformal Prediction", "link": "http://arxiv.org/abs/2401.15461", "description": "The assumption that data are invariant under the action of a compact group is\nimplicit in many statistical modeling assumptions such as normality, or the\nassumption of independence and identical distributions. Hence, testing for the\npresence of such invariances offers a principled way to falsify various\nstatistical models. In this article, we develop sequential, anytime-valid tests\nof distributional symmetry under the action of general compact groups. The\ntests that are developed allow for the continuous monitoring of data as it is\ncollected while keeping type-I error guarantees, and include tests for\nexchangeability and rotational symmetry as special examples. The main tool to\nthis end is the machinery developed for conformal prediction. The resulting\ntest statistic, called a conformal martingale, can be interpreted as a\nlikelihood ratio. We use this interpretation to show that the test statistics\nare optimal -- in a specific log-optimality sense -- against certain\nalternatives. Furthermore, we draw a connection between conformal prediction,\nanytime-valid tests of distributional invariance, and current developments on\nanytime-valid testing. In particular, we extend existing anytime-valid tests of\nindependence, which leverage exchangeability, to work under general group\ninvariances. Additionally, we discuss testing for invariance under subgroups of\nthe permutation group and orthogonal group, the latter of which corresponds to\ntesting the assumptions behind linear regression models."}, "http://arxiv.org/abs/2401.15514": {"title": "Validity of Complete Case Analysis Depends on the Target Population", "link": "http://arxiv.org/abs/2401.15514", "description": "Missing data is a pernicious problem in epidemiologic research. Research on\nthe validity of complete case analysis for missing data has typically focused\non estimating the average treatment effect (ATE) in the whole population.\nHowever, other target populations like the treated (ATT) or external targets\ncan be of substantive interest. In such cases, whether missing covariate data\noccurs within or outside the target population may impact the validity of\ncomplete case analysis. We sought to assess bias in complete case analysis when\ncovariate data is missing outside the target (e.g., missing covariate data\namong the untreated when estimating the ATT). We simulated a study of the\neffect of a binary treatment X on a binary outcome Y in the presence of 3\nconfounders C1-C3 that modified the risk difference (RD). We induced\nmissingness in C1 only among the untreated under 4 scenarios: completely\nrandomly (similar to MCAR); randomly based on C2 and C3 (similar to MAR);\nrandomly based on C1 (similar to MNAR); or randomly based on Y (similar to\nMAR). We estimated the ATE and ATT using weighting and averaged results across\nthe replicates. We conducted a parallel simulation transporting trial results\nto a target population in the presence of missing covariate data in the trial.\nIn the complete case analysis, estimated ATE was unbiased only when C1 was MCAR\namong the untreated. The estimated ATT, on the other hand, was unbiased in all\nscenarios except when Y caused missingness. The parallel simulation of\ngeneralizing and transporting trial results saw similar bias patterns. If\nmissing covariate data is only present outside the target population, complete\ncase analysis is unbiased except when missingness is associated with the\noutcome."}, "http://arxiv.org/abs/2401.15519": {"title": "Large Deviation Analysis of Score-based Hypothesis Testing", "link": "http://arxiv.org/abs/2401.15519", "description": "Score-based statistical models play an important role in modern machine\nlearning, statistics, and signal processing. For hypothesis testing, a\nscore-based hypothesis test is proposed in \\cite{wu2022score}. We analyze the\nperformance of this score-based hypothesis testing procedure and derive upper\nbounds on the probabilities of its Type I and II errors. We prove that the\nexponents of our error bounds are asymptotically (in the number of samples)\ntight for the case of simple null and alternative hypotheses. We calculate\nthese error exponents explicitly in specific cases and provide numerical\nstudies for various other scenarios of interest."}, "http://arxiv.org/abs/2401.15567": {"title": "Matrix Supermartingales and Randomized Matrix Concentration Inequalities", "link": "http://arxiv.org/abs/2401.15567", "description": "We present new concentration inequalities for either martingale dependent or\nexchangeable random symmetric matrices under a variety of tail conditions,\nencompassing standard Chernoff bounds to self-normalized heavy-tailed settings.\nThese inequalities are often randomized in a way that renders them strictly\ntighter than existing deterministic results in the literature, are typically\nexpressed in the Loewner order, and are sometimes valid at arbitrary\ndata-dependent stopping times.\n\nAlong the way, we explore the theory of matrix supermartingales and maximal\ninequalities, potentially of independent interest."}, "http://arxiv.org/abs/2401.15623": {"title": "GT-PCA: Effective and Interpretable Dimensionality Reduction with General Transform-Invariant Principal Component Analysis", "link": "http://arxiv.org/abs/2401.15623", "description": "Data analysis often requires methods that are invariant with respect to\nspecific transformations, such as rotations in case of images or shifts in case\nof images and time series. While principal component analysis (PCA) is a\nwidely-used dimension reduction technique, it lacks robustness with respect to\nthese transformations. Modern alternatives, such as autoencoders, can be\ninvariant with respect to specific transformations but are generally not\ninterpretable. We introduce General Transform-Invariant Principal Component\nAnalysis (GT-PCA) as an effective and interpretable alternative to PCA and\nautoencoders. We propose a neural network that efficiently estimates the\ncomponents and show that GT-PCA significantly outperforms alternative methods\nin experiments based on synthetic and real data."}, "http://arxiv.org/abs/2401.15680": {"title": "How to achieve model-robust inference in stepped wedge trials with model-based methods?", "link": "http://arxiv.org/abs/2401.15680", "description": "A stepped wedge design is a unidirectional crossover design where clusters\nare randomized to distinct treatment sequences defined by calendar time. While\nmodel-based analysis of stepped wedge designs -- via linear mixed models or\ngeneralized estimating equations -- is standard practice to evaluate treatment\neffects accounting for clustering and adjusting for baseline covariates, formal\nresults on their model-robustness properties remain unavailable. In this\narticle, we study when a potentially misspecified multilevel model can offer\nconsistent estimators for treatment effect estimands that are functions of\ncalendar time and/or exposure time. We describe a super-population potential\noutcomes framework to define treatment effect estimands of interest in stepped\nwedge designs, and adapt linear mixed models and generalized estimating\nequations to achieve estimand-aligned inference. We prove a central result\nthat, as long as the treatment effect structure is correctly specified in each\nworking model, our treatment effect estimator is robust to arbitrary\nmisspecification of all remaining model components. The theoretical results are\nillustrated via simulation experiments and re-analysis of a cardiovascular\nstepped wedge cluster randomized trial."}, "http://arxiv.org/abs/2401.15694": {"title": "Constrained Markov decision processes for response-adaptive procedures in clinical trials with binary outcomes", "link": "http://arxiv.org/abs/2401.15694", "description": "A constrained Markov decision process (CMDP) approach is developed for\nresponse-adaptive procedures in clinical trials with binary outcomes. The\nresulting CMDP class of Bayesian response- adaptive procedures can be used to\ntarget a certain objective, e.g., patient benefit or power while using\nconstraints to keep other operating characteristics under control. In the CMDP\napproach, the constraints can be formulated under different priors, which can\ninduce a certain behaviour of the policy under a given statistical hypothesis,\nor given that the parameters lie in a specific part of the parameter space. A\nsolution method is developed to find the optimal policy, as well as a more\nefficient method, based on backward recursion, which often yields a\nnear-optimal solution with an available optimality gap. Three applications are\nconsidered, involving type I error and power constraints, constraints on the\nmean squared error, and a constraint on prior robustness. While the CMDP\napproach slightly outperforms the constrained randomized dynamic programming\n(CRDP) procedure known from literature when focussing on type I and II error\nand mean squared error, showing the general quality of CRDP, CMDP significantly\noutperforms CRDP when the focus is on type I and II error only."}, "http://arxiv.org/abs/2401.15703": {"title": "A Bayesian multivariate extreme value mixture model", "link": "http://arxiv.org/abs/2401.15703", "description": "Impact assessment of natural hazards requires the consideration of both\nextreme and non-extreme events. Extensive research has been conducted on the\njoint modeling of bulk and tail in univariate settings; however, the\ncorresponding body of research in the context of multivariate analysis is\ncomparatively scant. This study extends the univariate joint modeling of bulk\nand tail to the multivariate framework. Specifically, it pertains to cases\nwhere multivariate observations exceed a high threshold in at least one\ncomponent. We propose a multivariate mixture model that assumes a parametric\nmodel to capture the bulk of the distribution, which is in the max-domain of\nattraction (MDA) of a multivariate extreme value distribution (mGEVD). The tail\nis described by the multivariate generalized Pareto distribution, which is\nasymptotically justified to model multivariate threshold exceedances. We show\nthat if all components exceed the threshold, our mixture model is in the MDA of\nan mGEVD. Bayesian inference based on multivariate random-walk\nMetropolis-Hastings and the automated factor slice sampler allows us to\nincorporate uncertainty from the threshold selection easily. Due to\ncomputational limitations, simulations and data applications are provided for\ndimension $d=2$, but a discussion is provided with views toward scalability\nbased on pairwise likelihood."}, "http://arxiv.org/abs/2401.15730": {"title": "Statistical analysis and first-passage-time applications of a lognormal diffusion process withmulti-sigmoidal logistic mean", "link": "http://arxiv.org/abs/2401.15730", "description": "We consider a lognormal diffusion process having a multisigmoidal logistic\nmean, useful to model the evolution of a population which reaches the maximum\nlevel of the growth after many stages. Referring to the problem of statistical\ninference, two procedures to find the maximum likelihood estimates of the\nunknown parameters are described. One is based on the resolution of the system\nof the critical points of the likelihood function, and the other is on the\nmaximization of the likelihood function with the simulated annealing algorithm.\nA simulation study to validate the described strategies for finding the\nestimates is also presented, with a real application to epidemiological data.\nSpecial attention is also devoted to the first-passage-time problem of the\nconsidered diffusion process through a fixed boundary."}, "http://arxiv.org/abs/2401.15778": {"title": "On the partial autocorrelation function for locally stationary time series: characterization, estimation and inference", "link": "http://arxiv.org/abs/2401.15778", "description": "For stationary time series, it is common to use the plots of partial\nautocorrelation function (PACF) or PACF-based tests to explore the temporal\ndependence structure of such processes. To our best knowledge, such analogs for\nnon-stationary time series have not been fully established yet. In this paper,\nwe fill this gap for locally stationary time series with short-range\ndependence. {First, we characterize the PACF locally in the time domain and\nshow that the $j$th PACF, denoted as $\\rho_{j}(t),$ decays with $j$ whose rate\nis adaptive to the temporal dependence of the time series $\\{x_{i,n}\\}$.\nSecond, at time $i,$ we justify that the PACF $\\rho_j(i/n)$ can be efficiently\napproximated by the best linear prediction coefficients via the Yule-Walker's\nequations. This allows us to study the PACF via ordinary least squares (OLS)\nlocally. Third, we show that the PACF is smooth in time for locally stationary\ntime series. We use the sieve method with OLS to estimate $\\rho_j(\\cdot)$} and\nconstruct some statistics to test the PACFs and infer the structures of the\ntime series. These tests generalize and modify those used for stationary time\nseries. Finally, a multiplier bootstrap algorithm is proposed for practical\nimplementation and an $\\mathtt R$ package $\\mathtt {Sie2nts}$ is provided to\nimplement our algorithm. Numerical simulations and real data analysis also\nconfirm usefulness of our results."}, "http://arxiv.org/abs/2401.15793": {"title": "Doubly regularized generalized linear models for spatial observations with high-dimensional covariates", "link": "http://arxiv.org/abs/2401.15793", "description": "A discrete spatial lattice can be cast as a network structure over which\nspatially-correlated outcomes are observed. A second network structure may also\ncapture similarities among measured features, when such information is\navailable. Incorporating the network structures when analyzing such\ndoubly-structured data can improve predictive power, and lead to better\nidentification of important features in the data-generating process. Motivated\nby applications in spatial disease mapping, we develop a new doubly regularized\nregression framework to incorporate these network structures for analyzing\nhigh-dimensional datasets. Our estimators can easily be implemented with\nstandard convex optimization algorithms. In addition, we describe a procedure\nto obtain asymptotically valid confidence intervals and hypothesis tests for\nour model parameters. We show empirically that our framework provides improved\npredictive accuracy and inferential power compared to existing high-dimensional\nspatial methods. These advantages hold given fully accurate network\ninformation, and also with networks which are partially misspecified or\nuninformative. The application of the proposed method to modeling COVID-19\nmortality data suggests that it can improve prediction of deaths beyond\nstandard spatial models, and that it selects relevant covariates more often."}, "http://arxiv.org/abs/2401.15796": {"title": "High-Dimensional False Discovery Rate Control for Dependent Variables", "link": "http://arxiv.org/abs/2401.15796", "description": "Algorithms that ensure reproducible findings from large-scale,\nhigh-dimensional data are pivotal in numerous signal processing applications.\nIn recent years, multivariate false discovery rate (FDR) controlling methods\nhave emerged, providing guarantees even in high-dimensional settings where the\nnumber of variables surpasses the number of samples. However, these methods\noften fail to reliably control the FDR in the presence of highly dependent\nvariable groups, a common characteristic in fields such as genomics and\nfinance. To tackle this critical issue, we introduce a novel framework that\naccounts for general dependency structures. Our proposed dependency-aware T-Rex\nselector integrates hierarchical graphical models within the T-Rex framework to\neffectively harness the dependency structure among variables. Leveraging\nmartingale theory, we prove that our variable penalization mechanism ensures\nFDR control. We further generalize the FDR-controlling framework by stating and\nproving a clear condition necessary for designing both graphical and\nnon-graphical models that capture dependencies. Additionally, we formulate a\nfully integrated optimal calibration algorithm that concurrently determines the\nparameters of the graphical model and the T-Rex framework, such that the FDR is\ncontrolled while maximizing the number of selected variables. Numerical\nexperiments and a breast cancer survival analysis use-case demonstrate that the\nproposed method is the only one among the state-of-the-art benchmark methods\nthat controls the FDR and reliably detects genes that have been previously\nidentified to be related to breast cancer. An open-source implementation is\navailable within the R package TRexSelector on CRAN."}, "http://arxiv.org/abs/2401.15806": {"title": "Continuous-time structural failure time model for intermittent treatment", "link": "http://arxiv.org/abs/2401.15806", "description": "The intermittent intake of treatment is commonly seen in patients with\nchronic disease. For example, patients with atrial fibrillation may need to\ndiscontinue the oral anticoagulants when they experience a certain surgery and\nre-initiate the treatment after the surgery. As another example, patients may\nskip a few days before they refill a treatment as planned. This treatment\ndispensation information (i.e., the time at which a patient initiates and\nrefills a treatment) is recorded in the electronic healthcare records or claims\ndatabase, and each patient has a different treatment dispensation. Current\nmethods to estimate the effects of such treatments censor the patients who\nre-initiate the treatment, which results in information loss or biased\nestimation. In this work, we present methods to estimate the effect of\ntreatments on failure time outcomes by taking all the treatment dispensation\ninformation. The developed methods are based on the continuous-time structural\nfailure time model, where the dependent censoring is tackled by inverse\nprobability of censoring weighting. The estimators are doubly robust and\nlocally efficient."}, "http://arxiv.org/abs/2401.15811": {"title": "Seller-Side Experiments under Interference Induced by Feedback Loops in Two-Sided Platforms", "link": "http://arxiv.org/abs/2401.15811", "description": "Two-sided platforms are central to modern commerce and content sharing and\noften utilize A/B testing for developing new features. While user-side\nexperiments are common, seller-side experiments become crucial for specific\ninterventions and metrics. This paper investigates the effects of interference\ncaused by feedback loops on seller-side experiments in two-sided platforms,\nwith a particular focus on the counterfactual interleaving design, proposed in\n\\citet{ha2020counterfactual,nandy2021b}. These feedback loops, often generated\nby pacing algorithms, cause outcomes from earlier sessions to influence\nsubsequent ones. This paper contributes by creating a mathematical framework to\nanalyze this interference, theoretically estimating its impact, and conducting\nempirical evaluations of the counterfactual interleaving design in real-world\nscenarios. Our research shows that feedback loops can result in misleading\nconclusions about the treatment effects."}, "http://arxiv.org/abs/2401.15903": {"title": "Toward the Identifiability of Comparative Deep Generative Models", "link": "http://arxiv.org/abs/2401.15903", "description": "Deep Generative Models (DGMs) are versatile tools for learning data\nrepresentations while adequately incorporating domain knowledge such as the\nspecification of conditional probability distributions. Recently proposed DGMs\ntackle the important task of comparing data sets from different sources. One\nsuch example is the setting of contrastive analysis that focuses on describing\npatterns that are enriched in a target data set compared to a background data\nset. The practical deployment of those models often assumes that DGMs naturally\ninfer interpretable and modular latent representations, which is known to be an\nissue in practice. Consequently, existing methods often rely on ad-hoc\nregularization schemes, although without any theoretical grounding. Here, we\npropose a theory of identifiability for comparative DGMs by extending recent\nadvances in the field of non-linear independent component analysis. We show\nthat, while these models lack identifiability across a general class of mixing\nfunctions, they surprisingly become identifiable when the mixing function is\npiece-wise affine (e.g., parameterized by a ReLU neural network). We also\ninvestigate the impact of model misspecification, and empirically show that\npreviously proposed regularization techniques for fitting comparative DGMs help\nwith identifiability when the number of latent variables is not known in\nadvance. Finally, we introduce a novel methodology for fitting comparative DGMs\nthat improves the treatment of multiple data sources via multi-objective\noptimization and that helps adjust the hyperparameter for the regularization in\nan interpretable manner, using constrained optimization. We empirically\nvalidate our theory and new methodology using simulated data as well as a\nrecent data set of genetic perturbations in cells profiled via single-cell RNA\nsequencing."}, "http://arxiv.org/abs/2401.16099": {"title": "A Ridgelet Approach to Poisson Denoising", "link": "http://arxiv.org/abs/2401.16099", "description": "This paper introduces a novel ridgelet transform-based method for Poisson\nimage denoising. Our work focuses on harnessing the Poisson noise's unique\nnon-additive and signal-dependent properties, distinguishing it from Gaussian\nnoise. The core of our approach is a new thresholding scheme informed by\ntheoretical insights into the ridgelet coefficients of Poisson-distributed\nimages and adaptive thresholding guided by Stein's method.\n\nWe verify our theoretical model through numerical experiments and demonstrate\nthe potential of ridgelet thresholding across assorted scenarios. Our findings\nrepresent a significant step in enhancing the understanding of Poisson noise\nand offer an effective denoising method for images corrupted with it."}, "http://arxiv.org/abs/2401.16286": {"title": "Robust Functional Data Analysis for Stochastic Evolution Equations in Infinite Dimensions", "link": "http://arxiv.org/abs/2401.16286", "description": "This article addresses the robust measurement of covariations in the context\nof solutions to stochastic evolution equations in Hilbert spaces using\nfunctional data analysis. For such equations, standard techniques for\nfunctional data based on cross-sectional covariances are often inadequate for\nidentifying statistically relevant random drivers and detecting outliers since\nthey overlook the interplay between cross-sectional and temporal structures.\nTherefore, we develop an estimation theory for the continuous quadratic\ncovariation of the latent random driver of the equation instead of a static\ncovariance of the observable solution process. We derive identifiability\nresults under weak conditions, establish rates of convergence and a central\nlimit theorem based on infill asymptotics, and provide long-time asymptotics\nfor estimation of a static covariation of the latent driver. Applied to term\nstructure data, our approach uncovers a fundamental alignment with scaling\nlimits of covariations of specific short-term trading strategies, and an\nempirical study detects several jumps and indicates high-dimensional and\ntime-varying covariations."}, "http://arxiv.org/abs/2401.16396": {"title": "Ovarian Cancer Diagnostics using Wavelet Packet Scaling Descriptors", "link": "http://arxiv.org/abs/2401.16396", "description": "Detecting early-stage ovarian cancer accurately and efficiently is crucial\nfor timely treatment. Various methods for early diagnosis have been explored,\nincluding a focus on features derived from protein mass spectra, but these tend\nto overlook the complex interplay across protein expression levels. We propose\nan innovative method to automate the search for diagnostic features in these\nspectra by analyzing their inherent scaling characteristics. We compare two\ntechniques for estimating the self-similarity in a signal using the scaling\nbehavior of its wavelet packet decomposition. The methods are applied to the\nmass spectra using a rolling window approach, yielding a collection of\nself-similarity indexes that capture protein interactions, potentially\nindicative of ovarian cancer. Then, the most discriminatory scaling descriptors\nfrom this collection are selected for use in classification algorithms. To\nassess their effectiveness for early diagnosis of ovarian cancer, the\ntechniques are applied to two datasets from the American National Cancer\nInstitute. Comparative evaluation against an existing wavelet-based method\nshows that one wavelet packet-based technique led to improved diagnostic\nperformance for one of the analyzed datasets (95.67% vs. 96.78% test accuracy,\nrespectively). This highlights the potential of wavelet packet-based methods to\ncapture novel diagnostic information related to ovarian cancer. This innovative\napproach offers promise for better early detection and improved patient\noutcomes in ovarian cancer."}, "http://arxiv.org/abs/2010.16271": {"title": "View selection in multi-view stacking: Choosing the meta-learner", "link": "http://arxiv.org/abs/2010.16271", "description": "Multi-view stacking is a framework for combining information from different\nviews (i.e. different feature sets) describing the same set of objects. In this\nframework, a base-learner algorithm is trained on each view separately, and\ntheir predictions are then combined by a meta-learner algorithm. In a previous\nstudy, stacked penalized logistic regression, a special case of multi-view\nstacking, has been shown to be useful in identifying which views are most\nimportant for prediction. In this article we expand this research by\nconsidering seven different algorithms to use as the meta-learner, and\nevaluating their view selection and classification performance in simulations\nand two applications on real gene-expression data sets. Our results suggest\nthat if both view selection and classification accuracy are important to the\nresearch at hand, then the nonnegative lasso, nonnegative adaptive lasso and\nnonnegative elastic net are suitable meta-learners. Exactly which among these\nthree is to be preferred depends on the research context. The remaining four\nmeta-learners, namely nonnegative ridge regression, nonnegative forward\nselection, stability selection and the interpolating predictor, show little\nadvantages in order to be preferred over the other three."}, "http://arxiv.org/abs/2103.01280": {"title": "Dynamic covariate balancing: estimating treatment effects over time with potential local projections", "link": "http://arxiv.org/abs/2103.01280", "description": "This paper studies the estimation and inference of treatment histories in\npanel data settings when treatments change dynamically over time.\n\nWe propose a method that allows for (i) treatments to be assigned dynamically\nover time based on high-dimensional covariates, past outcomes and treatments;\n(ii) outcomes and time-varying covariates to depend on treatment trajectories;\n(iii) heterogeneity of treatment effects.\n\nOur approach recursively projects potential outcomes' expectations on past\nhistories. It then controls the bias by balancing dynamically observable\ncharacteristics. We study the asymptotic and numerical properties of the\nestimator and illustrate the benefits of the procedure in an empirical\napplication."}, "http://arxiv.org/abs/2201.00409": {"title": "Global convergence of optimized adaptive importance samplers", "link": "http://arxiv.org/abs/2201.00409", "description": "We analyze the optimized adaptive importance sampler (OAIS) for performing\nMonte Carlo integration with general proposals. We leverage a classical result\nwhich shows that the bias and the mean-squared error (MSE) of the importance\nsampling scales with the $\\chi^2$-divergence between the target and the\nproposal and develop a scheme which performs global optimization of\n$\\chi^2$-divergence. While it is known that this quantity is convex for\nexponential family proposals, the case of the general proposals has been an\nopen problem. We close this gap by utilizing the nonasymptotic bounds for\nstochastic gradient Langevin dynamics (SGLD) for the global optimization of\n$\\chi^2$-divergence and derive nonasymptotic bounds for the MSE by leveraging\nrecent results from non-convex optimization literature. The resulting AIS\nschemes have explicit theoretical guarantees that are uniform-in-time."}, "http://arxiv.org/abs/2205.13469": {"title": "Proximal Estimation and Inference", "link": "http://arxiv.org/abs/2205.13469", "description": "We build a unifying convex analysis framework characterizing the statistical\nproperties of a large class of penalized estimators, both under a regular and\nan irregular design. Our framework interprets penalized estimators as proximal\nestimators, defined by a proximal operator applied to a corresponding initial\nestimator. We characterize the asymptotic properties of proximal estimators,\nshowing that their asymptotic distribution follows a closed-form formula\ndepending only on (i) the asymptotic distribution of the initial estimator,\n(ii) the estimator's limit penalty subgradient and (iii) the inner product\ndefining the associated proximal operator. In parallel, we characterize the\nOracle features of proximal estimators from the properties of their penalty's\nsubgradients. We exploit our approach to systematically cover linear regression\nsettings with a regular or irregular design. For these settings, we build new\n$\\sqrt{n}-$consistent, asymptotically normal Ridgeless-type proximal\nestimators, which feature the Oracle property and are shown to perform\nsatisfactorily in practically relevant Monte Carlo settings."}, "http://arxiv.org/abs/2206.05581": {"title": "Federated Offline Reinforcement Learning", "link": "http://arxiv.org/abs/2206.05581", "description": "Evidence-based or data-driven dynamic treatment regimes are essential for\npersonalized medicine, which can benefit from offline reinforcement learning\n(RL). Although massive healthcare data are available across medical\ninstitutions, they are prohibited from sharing due to privacy constraints.\nBesides, heterogeneity exists in different sites. As a result, federated\noffline RL algorithms are necessary and promising to deal with the problems. In\nthis paper, we propose a multi-site Markov decision process model that allows\nfor both homogeneous and heterogeneous effects across sites. The proposed model\nmakes the analysis of the site-level features possible. We design the first\nfederated policy optimization algorithm for offline RL with sample complexity.\nThe proposed algorithm is communication-efficient, which requires only a single\nround of communication interaction by exchanging summary statistics. We give a\ntheoretical guarantee for the proposed algorithm, where the suboptimality for\nthe learned policies is comparable to the rate as if data is not distributed.\nExtensive simulations demonstrate the effectiveness of the proposed algorithm.\nThe method is applied to a sepsis dataset in multiple sites to illustrate its\nuse in clinical settings."}, "http://arxiv.org/abs/2210.10171": {"title": "Doubly-robust and heteroscedasticity-aware sample trimming for causal inference", "link": "http://arxiv.org/abs/2210.10171", "description": "A popular method for variance reduction in observational causal inference is\npropensity-based trimming, the practice of removing units with extreme\npropensities from the sample. This practice has theoretical grounding when the\ndata are homoscedastic and the propensity model is parametric (Yang and Ding,\n2018; Crump et al. 2009), but in modern settings where heteroscedastic data are\nanalyzed with non-parametric models, existing theory fails to support current\npractice. In this work, we address this challenge by developing new methods and\ntheory for sample trimming. Our contributions are three-fold: first, we\ndescribe novel procedures for selecting which units to trim. Our procedures\ndiffer from previous work in that we trim not only units with small\npropensities, but also units with extreme conditional variances. Second, we\ngive new theoretical guarantees for inference after trimming. In particular, we\nshow how to perform inference on the trimmed subpopulation without requiring\nthat our regressions converge at parametric rates. Instead, we make only\nfourth-root rate assumptions like those in the double machine learning\nliterature. This result applies to conventional propensity-based trimming as\nwell and thus may be of independent interest. Finally, we propose a\nbootstrap-based method for constructing simultaneously valid confidence\nintervals for multiple trimmed sub-populations, which are valuable for\nnavigating the trade-off between sample size and variance reduction inherent in\ntrimming. We validate our methods in simulation, on the 2007-2008 National\nHealth and Nutrition Examination Survey, and on a semi-synthetic Medicare\ndataset and find promising results in all settings."}, "http://arxiv.org/abs/2211.16384": {"title": "Parameter Estimation with Increased Precision for Elliptic and Hypo-elliptic Diffusions", "link": "http://arxiv.org/abs/2211.16384", "description": "This work aims at making a comprehensive contribution in the general area of\nparametric inference for discretely observed diffusion processes. Established\napproaches for likelihood-based estimation invoke a time-discretisation scheme\nfor the approximation of the intractable transition dynamics of the Stochastic\nDifferential Equation (SDE) model over finite time periods. The scheme is\napplied for a step-size that is either user-selected or determined by the data.\nRecent research has highlighted the critical ef-fect of the choice of numerical\nscheme on the behaviour of derived parameter estimates in the setting of\nhypo-elliptic SDEs. In brief, in our work, first, we develop two weak second\norder sampling schemes (to cover both hypo-elliptic and elliptic SDEs) and\nproduce a small time expansion for the density of the schemes to form a proxy\nfor the true intractable SDE transition density. Then, we establish a\ncollection of analytic results for likelihood-based parameter estimates\nobtained via the formed proxies, thus providing a theoretical framework that\nshowcases advantages from the use of the developed methodology for SDE\ncalibration. We present numerical results from carrying out classical or\nBayesian inference, for both elliptic and hypo-elliptic SDEs."}, "http://arxiv.org/abs/2212.09009": {"title": "Locally Simultaneous Inference", "link": "http://arxiv.org/abs/2212.09009", "description": "Selective inference is the problem of giving valid answers to statistical\nquestions chosen in a data-driven manner. A standard solution to selective\ninference is simultaneous inference, which delivers valid answers to the set of\nall questions that could possibly have been asked. However, simultaneous\ninference can be unnecessarily conservative if this set includes many questions\nthat were unlikely to be asked in the first place. We introduce a less\nconservative solution to selective inference that we call locally simultaneous\ninference, which only answers those questions that could plausibly have been\nasked in light of the observed data, all the while preserving rigorous type I\nerror guarantees. For example, if the objective is to construct a confidence\ninterval for the \"winning\" treatment effect in a clinical trial with multiple\ntreatments, and it is obvious in hindsight that only one treatment had a chance\nto win, then our approach will return an interval that is nearly the same as\nthe uncorrected, standard interval. Locally simultaneous inference is\nimplemented by refining any method for simultaneous inference of interest.\nUnder mild conditions satisfied by common confidence intervals, locally\nsimultaneous inference strictly dominates its underlying simultaneous inference\nmethod, meaning it can never yield less statistical power but only more.\nCompared to conditional selective inference, which demands stronger guarantees,\nlocally simultaneous inference is more easily applicable in nonparametric\nsettings and is more numerically stable."}, "http://arxiv.org/abs/2302.00230": {"title": "Revisiting the Effects of Maternal Education on Adolescents' Academic Performance: Doubly Robust Estimation in a Network-Based Observational Study", "link": "http://arxiv.org/abs/2302.00230", "description": "In many contexts, particularly when study subjects are adolescents, peer\neffects can invalidate typical statistical requirements in the data. For\ninstance, it is plausible that a student's academic performance is influenced\nboth by their own mother's educational level as well as that of their peers.\nSince the underlying social network is measured, the Add Health study provides\na unique opportunity to examine the impact of maternal college education on\nadolescent school performance, both direct and indirect. However, causal\ninference on populations embedded in social networks poses technical\nchallenges, since the typical no interference assumption no longer holds. While\ninverse probability-of-treatment weighted (IPW) estimators have been developed\nfor this setting, they are often highly unstable. Motivated by the question of\nmaternal education, we propose doubly robust (DR) estimators combining models\nfor treatment and outcome that are consistent and asymptotically normal if\neither model is correctly specified. We present empirical results that\nillustrate the DR property and the efficiency gain of DR over IPW estimators\neven when the treatment model is misspecified. Contrary to previous studies,\nour robust analysis does not provide evidence of an indirect effect of maternal\neducation on academic performance within adolescents' social circles in Add\nHealth."}, "http://arxiv.org/abs/2304.04374": {"title": "Partial Identification of Causal Effects Using Proxy Variables", "link": "http://arxiv.org/abs/2304.04374", "description": "Proximal causal inference is a recently proposed framework for evaluating\ncausal effects in the presence of unmeasured confounding. For point\nidentification of causal effects, it leverages a pair of so-called treatment\nand outcome confounding proxy variables, to identify a bridge function that\nmatches the dependence of potential outcomes or treatment variables on the\nhidden factors to corresponding functions of observed proxies. Unique\nidentification of a causal effect via a bridge function crucially requires that\nproxies are sufficiently relevant for hidden factors, a requirement that has\npreviously been formalized as a completeness condition. However, completeness\nis well-known not to be empirically testable, and although a bridge function\nmay be well-defined, lack of completeness, sometimes manifested by availability\nof a single type of proxy, may severely limit prospects for identification of a\nbridge function and thus a causal effect; therefore, potentially restricting\nthe application of the proximal causal framework. In this paper, we propose\npartial identification methods that do not require completeness and obviate the\nneed for identification of a bridge function. That is, we establish that\nproxies of unobserved confounders can be leveraged to obtain bounds on the\ncausal effect of the treatment on the outcome even if available information\ndoes not suffice to identify either a bridge function or a corresponding causal\neffect of interest. Our bounds are non-smooth functionals of the observed data\ndistribution. As a consequence, in the context of inference, we initially\nprovide a smooth approximation of our bounds. Subsequently, we leverage\nbootstrap confidence intervals on the approximated bounds. We further establish\nanalogous partial identification results in related settings where\nidentification hinges upon hidden mediators for which proxies are available."}, "http://arxiv.org/abs/2306.16858": {"title": "Methods for non-proportional hazards in clinical trials: A systematic review", "link": "http://arxiv.org/abs/2306.16858", "description": "For the analysis of time-to-event data, frequently used methods such as the\nlog-rank test or the Cox proportional hazards model are based on the\nproportional hazards assumption, which is often debatable. Although a wide\nrange of parametric and non-parametric methods for non-proportional hazards\n(NPH) has been proposed, there is no consensus on the best approaches. To close\nthis gap, we conducted a systematic literature search to identify statistical\nmethods and software appropriate under NPH. Our literature search identified\n907 abstracts, out of which we included 211 articles, mostly methodological\nones. Review articles and applications were less frequently identified. The\narticles discuss effect measures, effect estimation and regression approaches,\nhypothesis tests, and sample size calculation approaches, which are often\ntailored to specific NPH situations. Using a unified notation, we provide an\noverview of methods available. Furthermore, we derive some guidance from the\nidentified articles. We summarized the contents from the literature review in a\nconcise way in the main text and provide more detailed explanations in the\nsupplement."}, "http://arxiv.org/abs/2308.13827": {"title": "An exhaustive ADDIS principle for online FWER control", "link": "http://arxiv.org/abs/2308.13827", "description": "In this paper we consider online multiple testing with familywise error rate\n(FWER) control, where the probability of committing at least one type I error\nshall remain under control while testing a possibly infinite sequence of\nhypotheses over time. Currently, Adaptive-Discard (ADDIS) procedures seem to be\nthe most promising online procedures with FWER control in terms of power. Now,\nour main contribution is a uniform improvement of the ADDIS principle and thus\nof all ADDIS procedures. This means, the methods we propose reject as least as\nmuch hypotheses as ADDIS procedures and in some cases even more, while\nmaintaining FWER control. In addition, we show that there is no other FWER\ncontrolling procedure that enlarges the event of rejecting any hypothesis.\nFinally, we apply the new principle to derive uniform improvements of the\nADDIS-Spending and ADDIS-Graph."}, "http://arxiv.org/abs/2309.14630": {"title": "Free Discontinuity Regression: With an Application to the Economic Effects of Internet Shutdowns", "link": "http://arxiv.org/abs/2309.14630", "description": "Discontinuities in regression functions can reveal important insights. In\nmany contexts, like geographic settings, such discontinuities are multivariate\nand unknown a priori. We propose a non-parametric regression method that\nestimates the location and size of discontinuities by segmenting the regression\nsurface. This estimator is based on a convex relaxation of the Mumford-Shah\nfunctional, for which we establish identification and convergence. We use it to\nshow that an internet shutdown in India resulted in a reduction of economic\nactivity by 25--35%, greatly surpassing previous estimates and shedding new\nlight on the true cost of such shutdowns for digital economies globally."}}