{"https://arxiv.org/abs/2405.05275": {"title": "SoMeR: Multi-View User Representation Learning for Social Media", "link": "https://arxiv.org/abs/2405.05275", "description": "arXiv:2405.05275v1 Announce Type: new \nAbstract: User representation learning aims to capture user preferences, interests, and behaviors in low-dimensional vector representations. These representations have widespread applications in recommendation systems and advertising; however, existing methods typically rely on specific features like text content, activity patterns, or platform metadata, failing to holistically model user behavior across different modalities. To address this limitation, we propose SoMeR, a Social Media user Representation learning framework that incorporates temporal activities, text content, profile information, and network interactions to learn comprehensive user portraits. SoMeR encodes user post streams as sequences of timestamped textual features, uses transformers to embed this along with profile data, and jointly trains with link prediction and contrastive learning objectives to capture user similarity. We demonstrate SoMeR's versatility through two applications: 1) Identifying inauthentic accounts involved in coordinated influence operations by detecting users posting similar content simultaneously, and 2) Measuring increased polarization in online discussions after major events by quantifying how users with different beliefs moved farther apart in the embedding space. SoMeR's ability to holistically model users enables new solutions to important problems around disinformation, societal tensions, and online behavior understanding."}, "https://arxiv.org/abs/2405.05288": {"title": "Learning Social Graph for Inactive User Recommendation", "link": "https://arxiv.org/abs/2405.05288", "description": "arXiv:2405.05288v1 Announce Type: new \nAbstract: Social relations have been widely incorporated into recommender systems to alleviate data sparsity problem. However, raw social relations don't always benefit recommendation due to their inferior quality and insufficient quantity, especially for inactive users, whose interacted items are limited. In this paper, we propose a novel social recommendation method called LSIR (\\textbf{L}earning \\textbf{S}ocial Graph for \\textbf{I}nactive User \\textbf{R}ecommendation) that learns an optimal social graph structure for social recommendation, especially for inactive users. LSIR recursively aggregates user and item embeddings to collaboratively encode item and user features. Then, graph structure learning (GSL) is employed to refine the raw user-user social graph, by removing noisy edges and adding new edges based on the enhanced embeddings. Meanwhile, mimic learning is implemented to guide active users in mimicking inactive users during model training, which improves the construction of new edges for inactive users. Extensive experiments on real-world datasets demonstrate that LSIR achieves significant improvements of up to 129.58\\% on NDCG in inactive user recommendation. Our code is available at~\\url{https://github.com/liun-online/LSIR}."}, "https://arxiv.org/abs/2405.05393": {"title": "Mutual information and the encoding of contingency tables", "link": "https://arxiv.org/abs/2405.05393", "description": "arXiv:2405.05393v1 Announce Type: new \nAbstract: Mutual information is commonly used as a measure of similarity between competing labelings of a given set of objects, for example to quantify performance in classification and community detection tasks. As argued recently, however, the mutual information as conventionally defined can return biased results because it neglects the information cost of the so-called contingency table, a crucial component of the similarity calculation. In principle the bias can be rectified by subtracting the appropriate information cost, leading to the modified measure known as the reduced mutual information, but in practice one can only ever compute an upper bound on this information cost, and the value of the reduced mutual information depends crucially on how good a bound is established. In this paper we describe an improved method for encoding contingency tables that gives a substantially better bound in typical use cases, and approaches the ideal value in the common case where the labelings are closely similar, as we demonstrate with extensive numerical results."}, "https://arxiv.org/abs/2405.05400": {"title": "Comparative analysis of graph randomization: Tools,methods, pitfalls, and best practices", "link": "https://arxiv.org/abs/2405.05400", "description": "arXiv:2405.05400v1 Announce Type: new \nAbstract: Graph randomization techniques play a crucial role in network analysis, allowing researchers to assess the statistical significance of observed network properties and distinguish meaningful patterns from random fluctuations. In this survey we provide an overview of the graph randomization methods available in the most popular software tools for network analysis. We propose a comparative analysis of popular software tools to highlight their functionalities and limitations. Through case studies involving diverse graph types, we demonstrate how different randomization methods can lead to divergent conclusions, emphasizing the importance of careful method selection based on the characteristics of the observed network and the research question at hand. This survey proposes some guidelines for researchers and practitioners seeking to understand and utilize graph randomization techniques effectively in their network analysis projects."}, "https://arxiv.org/abs/2405.05576": {"title": "LayerPlexRank: Exploring Node Centrality and Layer Influence through Algebraic Connectivity in Multiplex Networks", "link": "https://arxiv.org/abs/2405.05576", "description": "arXiv:2405.05576v1 Announce Type: new \nAbstract: As the calculation of centrality in complex networks becomes increasingly vital across technological, biological, and social systems, precise and scalable ranking methods are essential for understanding these networks. This paper introduces LayerPlexRank, an algorithm that simultaneously assesses node centrality and layer influence in multiplex networks using algebraic connectivity metrics. This method enhances the robustness of the ranking algorithm by effectively assessing structural changes across layers using random walk, considering the overall connectivity of the graph. We substantiate the utility of LayerPlexRank with theoretical analyses and empirical validations on varied real-world datasets, contrasting it with established centrality measures."}, "https://arxiv.org/abs/2405.05724": {"title": "Private Online Community Detection for Censored Block Models", "link": "https://arxiv.org/abs/2405.05724", "description": "arXiv:2405.05724v1 Announce Type: new \nAbstract: We study the private online change detection problem for dynamic communities, using a censored block model (CBM). Focusing on the notion of edge differential privacy (DP), we seek to understand the fundamental tradeoffs between the privacy budget, detection delay, and exact community recovery of community labels. We establish the theoretical lower bound on the delay in detecting changes privately and propose an algorithm capable of identifying changes in the community structure, while maintaining user privacy. Further, we provide theoretical guarantees for the effectiveness of our proposed method by showing necessary and sufficient conditions on change detection and exact recovery under edge DP. Simulation and real data examples are provided to validate the proposed method."}, "https://arxiv.org/abs/2405.05903": {"title": "The Other Side of the Coin: Recipient Norms and Their Impact on Indirect Reciprocity and Cooperation", "link": "https://arxiv.org/abs/2405.05903", "description": "arXiv:2405.05903v1 Announce Type: new \nAbstract: Human cooperation depends on indirect reciprocity. In this work, we explore the concept of indirect reciprocity using a donation game in an infinitely large population. In particular, we examine how updating the reputations of recipients influences cooperation. Our work adds a time-scale parameter for updating donor and recipient reputations. We find a trade-off between the level of cooperation and evolutionary stability influenced by social norms. `Forgiving' recipient norms enhance cooperation but increase susceptibility to defectors, whereas `unforgiving' norms reduce cooperation but defend against invasion by defectors. Expanding to include gossip groups allows us to analyze the evolutionary dynamics of the time-scale parameter, identifying `generous' norms that support cooperation, and `strict' norms that discourage such generosity, ultimately showing vulnerability to defector invasions and potential cooperation collapse."}, "https://arxiv.org/abs/2405.05433": {"title": "Robust Reward Placement under Uncertainty", "link": "https://arxiv.org/abs/2405.05433", "description": "arXiv:2405.05433v1 Announce Type: cross \nAbstract: Reward placement is a common optimization problem in network diffusion processes, where a number of rewards are to be placed in a network so as to maximize the total reward obtained as agents move randomly in it. In many settings, the precise mobility network might be one of several possible, based on parameters outside our control, such as the weather conditions affecting peoples' transportation means. Solutions to the reward placement problem must thus be robust to this uncertainty, by achieving a high utility in all possible networks. To study such scenarios, we introduce the Robust Reward Placement problem (RRP). Agents move randomly on a Markovian Mobility Model that has a predetermined set of locations but its precise connectivity is unknown and chosen adversarialy from a known set $\\Pi$ of candidates. Network optimization is achieved by selecting a set of reward states, and the goal is to maximize the minimum, among all candidates, ratio of rewards obtained over the optimal solution for each candidate. We first prove that RRP is NP-hard and inapproximable in general. We then develop $\\Psi$-Saturate, a pseudo-polynomial time algorithm that achieves an $\\epsilon$-additive approximation by exceeding the budget constraint by a factor that scales as $O(ln|\\Pi|/\\epsilon)$. In addition, we present several heuristics, most prominently one inspired from a dynamic programming algorithm for the max-min 0-1 Knapsack problem. We corroborate our theoretical findings with an experimental evaluation of the methods in both synthetic and real-world datasets."}, "https://arxiv.org/abs/2405.05487": {"title": "Design of Targeted Community-Based Resource Allocation in the Presence of Vaccine Hesitancy via a Data-Driven Compartmental Stochastic Optimization Model", "link": "https://arxiv.org/abs/2405.05487", "description": "arXiv:2405.05487v1 Announce Type: cross \nAbstract: Vaccines have proven effective in mitigating the threat of severe infections and deaths during outbreaks of infectious diseases. However, vaccine hesitancy (VH) complicates disease spread prediction and healthcare resource assessment across regions and populations. We propose a modeling framework that integrates an epidemiological compartmental model that captures the spread of an infectious disease within a multi-stage stochastic program (MSP) that determines the allocation of critical resources under uncertainty. The proposed compartmental MSP model adaptively manages the allocation of resources to account for changes in population behavior toward vaccines (i.e., variability in VH), the unique patterns of disease spread, and the availability of healthcare resources over time and space. The compartmental MSP model allowed us to analyze the price of fairness in resource allocation. Using real COVID-19 vaccination and healthcare resource data from Arkansas, U.S. (January-May 2021), our findings include: (i) delaying the initial deployment of additional ventilators by one month could lead to an average increase in the expected number of deaths by 285.41/month, highlighting the importance of prompt action; (ii) each additional ventilator in the initial stockpile and in supply leads to a decrease in the expected number of deaths by 1.09/month and 0.962/month, respectively, emphasizing the importance of maintaining a large stockpile and scalable production response; (iii) the cost of ensuring equitable resource allocation varies over time and location, peaking during the peak of a disease outbreak and in densely populated areas. This study emphasizes the importance of flexible, informed public health decision-making and preparedness, providing a model for effective resource allocation in public health emergencies."}, "https://arxiv.org/abs/2211.06352": {"title": "Spectral Triadic Decompositions of Real-World Networks", "link": "https://arxiv.org/abs/2211.06352", "description": "arXiv:2211.06352v3 Announce Type: replace \nAbstract: A fundamental problem in mathematics and network analysis is to find conditions under which a graph can be partitioned into smaller pieces. The most important tool for this partitioning is the Fiedler vector or discrete Cheeger inequality. These results relate the graph spectrum (eigenvalues of the normalized adjacency matrix) to the ability to break a graph into two pieces, with few edge deletions. An entire subfield of mathematics, called spectral graph theory, has emerged from these results. Yet these results do not say anything about the rich community structure exhibited by real-world networks, which typically have a significant fraction of edges contained in numerous densely clustered blocks. Inspired by the properties of real-world networks, we discover a new spectral condition that relates eigenvalue powers to a network decomposition into densely clustered blocks. We call this the \\emph{spectral triadic decomposition}. Our relationship exactly predicts the existence of community structure, as commonly seen in real networked data. Our proof provides an efficient algorithm to produce the spectral triadic decomposition. We observe on numerous social, coauthorship, and citation network datasets that these decompositions have significant correlation with semantically meaningful communities."}, "https://arxiv.org/abs/2301.06774": {"title": "Temporal Dynamics of Coordinated Online Behavior: Stability, Archetypes, and Influence", "link": "https://arxiv.org/abs/2301.06774", "description": "arXiv:2301.06774v2 Announce Type: replace \nAbstract: Large-scale online campaigns, malicious or otherwise, require a significant degree of coordination among participants, which sparked interest in the study of coordinated online behavior. State-of-the-art methods for detecting coordinated behavior perform static analyses, disregarding the temporal dynamics of coordination. Here, we carry out the first dynamic analysis of coordinated behavior. To reach our goal we build a multiplex temporal network and we perform dynamic community detection to identify groups of users that exhibited coordinated behaviors in time. Thanks to our novel approach we find that: (i) coordinated communities feature variable degrees of temporal instability; (ii) dynamic analyses are needed to account for such instability, and results of static analyses can be unreliable and scarcely representative of unstable communities; (iii) some users exhibit distinct archetypal behaviors that have important practical implications; (iv) content and network characteristics contribute to explaining why users leave and join coordinated communities. Our results demonstrate the advantages of dynamic analyses and open up new directions of research on the unfolding of online debates, on the strategies of coordinated communities, and on the patterns of online influence."}, "https://arxiv.org/abs/2303.03774": {"title": "Network science meets history", "link": "https://arxiv.org/abs/2303.03774", "description": "arXiv:2303.03774v3 Announce Type: replace \nAbstract: Alliances and conflicts represent important features of complex systems like international relations. Such relations create a time-evolving signed network, where each node contributes in a unique manner to the global balance of the system. Therefore, a local index mathematically quantifying such a property becomes valuable. In this work, we introduce a local balance index for signed networks. We analyze its mathematical foundations and unique structural properties, differentiating it from existing local vertex invariants. We also establish a novel methodology linking changes in a nation's local balance to historical events. By scrutinizing the time series of local balance for countries between 1816 and 2014, we detect and categorize major historic events based on balance fluctuations. This approach harmonizes quantitative and qualitative analyses, and combined with the theory of \"balance of power\" is able to build up a new mixed approach to history based on network theory."}, "https://arxiv.org/abs/2312.07077": {"title": "On the Potential of an Independent Avatar to Augment Metaverse Social Networks", "link": "https://arxiv.org/abs/2312.07077", "description": "arXiv:2312.07077v2 Announce Type: replace \nAbstract: We present a computational modelling approach which targets capturing the specifics on how to virtually augment a Metaverse user's available social time capacity via using an independent and autonomous version of her digital representation in the Metaverse. We motivate why this is a fundamental building block to model large-scale social networks in the Metaverse, and emerging properties herein. We envision a Metaverse-focused extension of the traditional avatar concept: An avatar can be as well programmed to operate independently when its user is not controlling it directly, thus turning it into an agent-based digital human representation. This way, we highlight how such an independent avatar could help its user to better navigate their social relationships and optimize their socializing time in the Metaverse by (partly) offloading some interactions to the avatar. We model the setting and identify the characteristic variables by using selected concepts from social sciences: ego networks, social presence, and social cues. Then, we formulate the problem of maximizing the user's non-avatar-mediated spare time as a linear optimization. Finally, we analyze the feasible region of the problem and we present some initial insights on the spare time that can be achieved for different parameter values of the avatar-mediated interactions."}, "https://arxiv.org/abs/2402.05739": {"title": "Critical mobility in policy making for epidemic containment", "link": "https://arxiv.org/abs/2402.05739", "description": "arXiv:2402.05739v2 Announce Type: replace \nAbstract: When considering airborne epidemic spreading in social systems, a natural connection arises between mobility and epidemic contacts. As individuals travel, possibilities to encounter new people either at the final destination or during the transportation process appear. Such contacts can lead to new contagion events. In fact, mobility has been a crucial target for early non-pharmaceutical containment measures against the recent COVID-19 pandemic, with a degree of intensity ranging from public transportation line closures to regional, city or even home confinements. Nonetheless, quantitative knowledge on the relationship between mobility-contagions and, consequently, on the efficiency of containment measures remains elusive. Here we introduce an agent-based model with a simple interaction between mobility and contacts. Despite its simplicity our model shows the emergence of a critical mobility level, inducing major outbreaks when surpassed. We explore the interplay between mobility restrictions and the infection in recent intervention policies seen across many countries, and how interventions in the form of closures triggered by incidence rates can guide the epidemic into an oscillatory regime with recurrent waves. We consider how the different interventions impact societal well-being, the economy and the population. Finally, we propose a mitigation framework based on the critical nature of mobility in an epidemic, able to suppress incidence and oscillations at will, preventing extreme incidence peaks with potential to saturate health care resources."}, "https://arxiv.org/abs/2404.12178": {"title": "Designing a sector-coupled European energy system robust to 60 years of historical weather data", "link": "https://arxiv.org/abs/2404.12178", "description": "arXiv:2404.12178v2 Announce Type: replace \nAbstract: As energy systems transform to rely on renewable energy and electrification, they encounter stronger year-to-year variability in energy supply and demand. However, most infrastructure planning is based on a single weather year, resulting in a lack of robustness. In this paper, we optimize energy infrastructure for a European energy system designed for net-zero CO$_2$ emissions in 62 different weather years. Subsequently, we fix the capacity layouts and simulate their operation in every weather year, to evaluate resource adequacy and CO$_2$ emissions abatement. We show that interannual weather variability causes variation of $\\pm$10\\% in total system cost. The most expensive capacity layout obtains the lowest net CO$_2$ emissions but not the highest resource adequacy. Instead, capacity layouts designed with years including compound weather events result in a more robust and cost-effective design. Deploying CO$_2$-emitting backup generation is a cost-effective robustness measure, which only increase CO$_2$ emissions marginally as the average CO$_2$ emissions remain less than 1\\% of 1990 levels. Our findings highlight how extreme weather years drive investments in robustness measures, making them compatible with all weather conditions within six decades of historical weather data."}, "https://arxiv.org/abs/2011.08069": {"title": "Reconciling Security and Utility in Next-Generation Epidemic Risk Mitigation Systems", "link": "https://arxiv.org/abs/2011.08069", "description": "arXiv:2011.08069v3 Announce Type: replace-cross \nAbstract: Epidemics like the recent COVID-19 require proactive contact tracing and epidemiological analysis to predict and subsequently contain infection transmissions. The proactive measures require large scale data collection, which simultaneously raise concerns regarding users' privacy. Digital contact tracing systems developed in response to COVID-19 either collected extensive data for effective analytics at the cost of users' privacy or collected minimal data for the sake of user privacy but were ineffective in predicting and mitigating the epidemic risks. We present Silmarillion--in preparation for future epidemics--a system that reconciles user's privacy with rich data collection for higher utility. In Silmarillion, user devices record Bluetooth encounters with beacons installed in strategic locations. The beacons further enrich the encounters with geo-location, location type, and environment conditions at the beacon installation site. This enriched information enables detailed scientific analysis of disease parameters as well as more accurate personalized exposure risk notification. At the same time, Silmarillion provides privacy to all participants and non-participants at the same level as that guaranteed in digital and manual contact tracing. We describe the design of Silmarillion and its communication protocols that ensure user privacy and data security. We also evaluate a prototype of Silmarillion built using low-end IoT boards, showing that the power consumption and user latencies are adequately low for a practical deployment. Finally, we briefly report on a small-scale deployment within a university building as a proof-of-concept."}, "https://arxiv.org/abs/2203.07678": {"title": "Incorporating Heterophily into Graph Neural Networks for Graph Classification", "link": "https://arxiv.org/abs/2203.07678", "description": "arXiv:2203.07678v2 Announce Type: replace-cross \nAbstract: Graph Neural Networks (GNNs) often assume strong homophily for graph classification, seldom considering heterophily, which means connected nodes tend to have different class labels and dissimilar features. In real-world scenarios, graphs may have nodes that exhibit both homophily and heterophily. Failing to generalize to this setting makes many GNNs underperform in graph classification. In this paper, we address this limitation by identifying three effective designs and develop a novel GNN architecture called IHGNN (short for Incorporating Heterophily into Graph Neural Networks). These designs include the combination of integration and separation of the ego- and neighbor-embeddings of nodes, adaptive aggregation of node embeddings from different layers, and differentiation between different node embeddings for constructing the graph-level readout function. We empirically validate IHGNN on various graph datasets and demonstrate that it outperforms the state-of-the-art GNNs for graph classification."}, "https://arxiv.org/abs/2308.13604": {"title": "Network science Ising states of matter", "link": "https://arxiv.org/abs/2308.13604", "description": "arXiv:2308.13604v3 Announce Type: replace-cross \nAbstract: Network science provides very powerful tools for extracting information from interacting data. Although recently the unsupervised detection of phases of matter using machine learning has raised significant interest, the full prediction power of network science has not yet been systematically explored in this context. Here we fill this gap by providing an in-depth statistical, combinatorial, geometrical and topological characterization of 2D Ising snapshot networks (IsingNets) extracted from Monte Carlo simulations of the $2$D Ising model at different temperatures, going across the phase transition. Our analysis reveals the complex organization properties of IsingNets in both the ferromagnetic and paramagnetic phases and demonstrates the significant deviations of the IsingNets with respect to randomized null models. In particular percolation properties of the IsingNets reflect the existence of the symmetry between configurations with opposite magnetization below the critical temperature and the very compact nature of the two emerging giant clusters revealed by our persistent homology analysis of the IsingNets. Moreover, the IsingNets display a very broad degree distribution and significant degree-degree correlations and weight-degree correlations demonstrating that they encode relevant information present in the configuration space of the $2$D Ising model. The geometrical organization of the critical IsingNets is reflected in their spectral properties deviating from the one of the null model. This work reveals the important insights that network science can bring to the characterization of phases of matter. The set of tools described hereby can be applied as well to numerical and experimental data."}, "https://arxiv.org/abs/2312.11834": {"title": "Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics", "link": "https://arxiv.org/abs/2312.11834", "description": "arXiv:2312.11834v3 Announce Type: replace-cross \nAbstract: In recent years, simulations of pedestrians using the multi-agent reinforcement learning (MARL) have been studied. This study considered the roads on a grid-world environment, and implemented pedestrians as MARL agents using an echo-state network and the least squares policy iteration method. Under this environment, the ability of these agents to learn to move forward by avoiding other agents was investigated. Specifically, we considered two types of tasks: the choice between a narrow direct route and a broad detour, and the bidirectional pedestrian flow in a corridor. The simulations results indicated that the learning was successful when the density of the agents was not that high."}, "https://arxiv.org/abs/2401.09438": {"title": "Recurrence analysis of meteorological data from climate zones in India", "link": "https://arxiv.org/abs/2401.09438", "description": "arXiv:2401.09438v4 Announce Type: replace-cross \nAbstract: We present a study on the spatio-temporal pattern underlying the climate dynamics in various locations spread over India, including the Himalayan region, coastal region, central and northeastern parts of India. We try to capture the variations in the complexity of their dynamics derived from temperature and relative humidity data from 1948-2022. By estimating the recurrence-based measures from the reconstructed phase space dynamics using a sliding window analysis on the data sets, we study the climate variability in different spatial locations. The study brings out the variations in the complexity of the underlying dynamics as well as their heterogeneity across the locations in India. We find almost all locations indicate shifts to more irregular and stochastic dynamics for temperature data around 1972-79 and shifts back to more regular dynamics beyond 2000. These patterns correlate with reported shifts in the climate and Indian Summer Monsoon related to strong and moderate ENSO events and confirm their associated regional variability."}, "https://arxiv.org/abs/2405.06075": {"title": "The Building Blocks of Consciousness", "link": "https://arxiv.org/abs/2405.06075", "description": "arXiv:2405.06075v1 Announce Type: new \nAbstract: Consciousness is presented not as a unified and uniquely human characteristic, but rather as an emergent property of several building blocks, most of which are demonstrably present in other species. Each block has its own rationale under natural selection and could have arisen independently, and the jumps between blocks -- which culminate in consciousness -- are small enough to be evolutionarily plausible. One underappreciated block involves unconscious engram playback and discrimination, and plays a major role in brain storage optimization. This function is present in birds and nearly all mammals and is recognized by its side-effect: dreams."}, "https://arxiv.org/abs/2405.06282": {"title": "A cellular automata approach for modelling pedestrian-vehicle mixed traffic flow in urban city", "link": "https://arxiv.org/abs/2405.06282", "description": "arXiv:2405.06282v1 Announce Type: new \nAbstract: In urban streets, the intrusion of pedestrians presents significant safety challenges. Modelling mixed pedestrian-vehicle traffic is complex due to the distinct motion characteristics and spatial dimensions of pedestrians and vehicles, making unified modelling difficult, with few studies addressing these issues. This paper employs a multi-grid cellular automata model to bridge the gap between vehicle and pedestrian models. An Improved Kerner-Klenov-Wolf (IKKW) model and a pedestrian motion model that incorporates Time-To-Collision (TTC) are introduced. Both models update the spatial motions of vehicles and pedestrians uniformly. Empirical analysis indicates that the model achieves high simulation accuracy. This model effectively illustrates the impact of pedestrian intrusion within mixed traffic scenario. The fundamental diagram of heterogeneous traffic reveals substantial differences, highlighting the effects of pedestrian intrusion on traffic flow states and identifying six phase regions in mixed traffic. Additionally, this paper examines conflicts between pedestrians and vehicles under varying speed limits and sidewalk widths, demonstrating that lower speeds and broader sidewalks significantly reduce the frequency of pedestrian-vehicle conflicts. Notably, the frequency of peak conflicts at a vehicle speed limit of 60.48 km/h is more than three times higher than at 30.24 km/h. This model offers a potential approach to studying mixed traffic flows and exhibits substantial scalability."}, "https://arxiv.org/abs/2405.06285": {"title": "Pedestrian Crossing Discrepancy Within Static and Dynamic Crowds: An Experimental Study", "link": "https://arxiv.org/abs/2405.06285", "description": "arXiv:2405.06285v1 Announce Type: new \nAbstract: This paper aims to investigate the disparities in pedestrian crossing behaviors within static and dynamic crowds through experimental analysis. First, the crossing trajectories of pedestrians in various crowd environments were qualitatively observed. These trajectories have shown significant discrepancies and the phenomenon of cross-channel formation within static crowds was observed, a phenomenon similar to the evolution of human trails. To quantitatively assess these discrepancies, metrics of behavior patterns and swarm factor were introduced. Different behavioral patterns, including anticipation and reaction behaviors in pedestrian motion, were observed. Finally, through orthogonal velocity analysis, the variation trends of crossing motions within static and dynamic contexts were revealed."}, "https://arxiv.org/abs/2405.06395": {"title": "Fitness-Based Growth of Directed Networks with Hierarchy", "link": "https://arxiv.org/abs/2405.06395", "description": "arXiv:2405.06395v1 Announce Type: new \nAbstract: Growing attention has been brought to the fact that many real directed networks exhibit hierarchy and directionality as measured through techniques like Trophic Analysis and non-normality. We propose a simple growing network model where the probability of connecting to a node is defined by a preferential attachment mechanism based on degree and the difference in fitness between nodes. In particular, we show how mechanisms such as degree-based preferential attachment and node fitness interactions can lead to the emergence of the spectrum of hierarchy and directionality observed in real networks. In this work, we study various features of this model relating to network hierarchy, as measured by Trophic Analysis. This includes (I) how preferential attachment can lead to network hierarchy, (II) how scale-free degree distributions and network hierarchy can coexist, (III) the correlation between node fitness and trophic level, (IV) how the fitness parameters can predict trophic incoherence and how the trophic level difference distribution compares to the fitness difference distribution, (V) the relationship between trophic level and degree imbalance and the unique role of nodes at the ends of the fitness hierarchy and (VI) how fitness interactions and degree-based preferential attachment can interplay to generate networks of varying coherence and degree distribution. We also provide an example of the intuition this work enables in the analysis of a real historical network. This work provides insight into simple mechanisms which can give rise to hierarchy in directed networks and quantifies the usefulness and limitations of using Trophic Analysis as an analysis tool for real networks."}, "https://arxiv.org/abs/2405.06508": {"title": "Simple crowd dynamics to generate complex temporal contact networks", "link": "https://arxiv.org/abs/2405.06508", "description": "arXiv:2405.06508v1 Announce Type: new \nAbstract: Empirical contact networks or interaction networks demonstrate peculiar characteristics stemming from the fundamental social, psychological, physical mechanisms governing human interactions. Although these mechanisms are complex, we test whether we are able to reproduce some dynamical properties of these empirical networks from relatively simple models. In this study, we perform simulations for a range of 2D models of particle dynamics, namely the Random Walk, Active Brownian Particles, and Vicsek models, to generate artificial contact networks. We investigate temporal properties of these contact networks: the distributions of contact durations, inter-contact durations and number of contact per pair of particle. We demonstrate that the distribution of inter-contact durations can be recovered by the dynamics of these simple crowd particle models, and show that it is simply related to the well-know first-return process, which explains the -3/2 exponent that is found in both the numerical models and empirical contact networks."}, "https://arxiv.org/abs/2405.06476": {"title": "Is the panel fair? Evaluating panel compositions through network analysis", "link": "https://arxiv.org/abs/2405.06476", "description": "arXiv:2405.06476v1 Announce Type: cross \nAbstract: In research evaluation, the fair representation of panels is usually defined in terms of observable characteristics of scholars such as gender or affiliations. An an empirical strategy is proposed for exploring hidden connections between panellists such that, despite the respect of formal requirements, the panel could be considered alike as unfair with respect to the representation of diversity of research approaches and methodologies. The case study regards the three panels selected to evaluate research in economics, statistics and business during the Italian research assessment exercises. The first two panels were appointed directly by the governmental agency responsible for the evaluation, while the third was randomly selected. Hence the third panel can be considered as a control for evaluating about the fairness of the others. The fair representation is explored by comparing the networks of panellists based on their co-authorship relations, the networks based on journals in which they published and the networks based on their affiliated institutions (universities, research centres and newspapers). The results show that the members of the first two panels had connections much higher than the members of the control group. Hence the composition of the first two panels should be considered as unfair, as the results of the research assessments."}, "https://arxiv.org/abs/2405.06478": {"title": "Attention is all they need: Cognitive science and the (techno)political economy of attention in humans and machines", "link": "https://arxiv.org/abs/2405.06478", "description": "arXiv:2405.06478v1 Announce Type: cross \nAbstract: This paper critically analyses the \"attention economy\" within the framework of cognitive science and techno-political economics, as applied to both human and machine interactions. We explore how current business models, particularly in digital platform capitalism, harness user engagement by strategically shaping attentional patterns. These platforms utilize advanced AI and massive data analytics to enhance user engagement, creating a cycle of attention capture and data extraction. We review contemporary (neuro)cognitive theories of attention and platform engagement design techniques and criticize classical cognitivist and behaviourist theories for their inadequacies in addressing the potential harms of such engagement on user autonomy and wellbeing. 4E approaches to cognitive science, instead, emphasizing the embodied, extended, enactive, and ecological aspects of cognition, offer us an intrinsic normative standpoint and a more integrated understanding of how attentional patterns are actively constituted by adaptive digital environments. By examining the precarious nature of habit formation in digital contexts, we reveal the techno-economic underpinnings that threaten personal autonomy by disaggregating habits away from the individual, into an AI managed collection of behavioural patterns. Our current predicament suggests the necessity of a paradigm shift towards an ecology of attention. This shift aims to foster environments that respect and preserve human cognitive and social capacities, countering the exploitative tendencies of cognitive capitalism."}, "https://arxiv.org/abs/2405.06541": {"title": "ATSumm: Auxiliary information enhanced approach for abstractive disaster Tweet Summarization with sparse training data", "link": "https://arxiv.org/abs/2405.06541", "description": "arXiv:2405.06541v1 Announce Type: cross \nAbstract: The abundance of situational information on Twitter poses a challenge for users to manually discern vital and relevant information during disasters. A concise and human-interpretable overview of this information helps decision-makers in implementing efficient and quick disaster response. Existing abstractive summarization approaches can be categorized as sentence-based or key-phrase-based approaches. This paper focuses on sentence-based approach, which is typically implemented as a dual-phase procedure in literature. The initial phase, known as the extractive phase, involves identifying the most relevant tweets. The subsequent phase, referred to as the abstractive phase, entails generating a more human-interpretable summary. In this study, we adopt the methodology from prior research for the extractive phase. For the abstractive phase of summarization, most existing approaches employ deep learning-based frameworks, which can either be pre-trained or require training from scratch. However, to achieve the appropriate level of performance, it is imperative to have substantial training data for both methods, which is not readily available. This work presents an Abstractive Tweet Summarizer (ATSumm) that effectively addresses the issue of data sparsity by using auxiliary information. We introduced the Auxiliary Pointer Generator Network (AuxPGN) model, which utilizes a unique attention mechanism called Key-phrase attention. This attention mechanism incorporates auxiliary information in the form of key-phrases and their corresponding importance scores from the input tweets. We evaluate the proposed approach by comparing it with 10 state-of-the-art approaches across 13 disaster datasets. The evaluation results indicate that ATSumm achieves superior performance compared to state-of-the-art approaches, with improvement of 4-80% in ROUGE-N F1-score."}, "https://arxiv.org/abs/2405.06551": {"title": "ADSumm: Annotated Ground-truth Summary Datasets for Disaster Tweet Summarization", "link": "https://arxiv.org/abs/2405.06551", "description": "arXiv:2405.06551v1 Announce Type: cross \nAbstract: Online social media platforms, such as Twitter, provide valuable information during disaster events. Existing tweet disaster summarization approaches provide a summary of these events to aid government agencies, humanitarian organizations, etc., to ensure effective disaster response. In the literature, there are two types of approaches for disaster summarization, namely, supervised and unsupervised approaches. Although supervised approaches are typically more effective, they necessitate a sizable number of disaster event summaries for testing and training. However, there is a lack of good number of disaster summary datasets for training and evaluation. This motivates us to add more datasets to make supervised learning approaches more efficient. In this paper, we present ADSumm, which adds annotated ground-truth summaries for eight disaster events which consist of both natural and man-made disaster events belonging to seven different countries. Our experimental analysis shows that the newly added datasets improve the performance of the supervised summarization approaches by 8-28% in terms of ROUGE-N F1-score. Moreover, in newly annotated dataset, we have added a category label for each input tweet which helps to ensure good coverage from different categories in summary. Additionally, we have added two other features relevance label and key-phrase, which provide information about the quality of a tweet and explanation about the inclusion of the tweet into summary, respectively. For ground-truth summary creation, we provide the annotation procedure adapted in detail, which has not been described in existing literature. Experimental analysis shows the quality of ground-truth summary is very good with Coverage, Relevance and Diversity."}, "https://arxiv.org/abs/2306.16568": {"title": "Early warning signals for predicting cryptomarket vendor success using dark net forum networks", "link": "https://arxiv.org/abs/2306.16568", "description": "arXiv:2306.16568v3 Announce Type: replace \nAbstract: In this work we focus on identifying key players in dark net cryptomarkets that facilitate online trade of illegal goods. Law enforcement aims to disrupt criminal activity conducted through these markets by targeting key players vital to the market's existence and success. We particularly focus on detecting successful vendors responsible for the majority of illegal trade. Our methodology aims to uncover whether the task of key player identification should center around plainly measuring user and forum activity, or that it requires leveraging specific patterns of user communication. We focus on a large-scale dataset from the Evolution cryptomarket, which we model as an evolving communication network. Results indicate that user and forum activity, measured through topic engagement, is best able to identify successful vendors. Interestingly, considering users with higher betweenness centrality in the communication network further improves performance, also identifying successful vendors with moderate activity on the forum. But more importantly, analyzing the forum data over time, we find evidence that attaining a high betweenness score comes before vendor success. This suggests that the proposed network-driven approach of modelling user communication might prove useful as an early warning signal for key player identification."}, "https://arxiv.org/abs/2312.14040": {"title": "Balancing Specialization and Adaptation in a Transforming Scientific Landscape", "link": "https://arxiv.org/abs/2312.14040", "description": "arXiv:2312.14040v5 Announce Type: replace \nAbstract: How do scientists navigate between the need to capitalize on their prior knowledge through specialization, and the urge to adapt to evolving research opportunities? Drawing from diverse perspectives on adaptation, including cultural evolution, this paper proposes an unsupervised Bayesian approach motivated by Optimal Transport of the evolution of scientists' research portfolios in response to transformations in their field. The model relies on $186,162$ scientific abstracts and authorship data to evaluate the influence of intellectual, social, and institutional resources on scientists' trajectories within a cohort of $2\\,195$ high-energy physicists between 2000 and 2019. Using Inverse Optimal Transport, the reallocation of research efforts is shown to be shaped by learning costs, thus enhancing the utility of the scientific capital disseminated among scientists. Two dimensions of social capital, namely \"diversity\" and \"power\", have opposite associations with the magnitude of change in scientists' research interests: while \"diversity\" disrupts and expands research interests, \"power\" is associated with more stable research agendas. Social capital plays a more crucial role in shifts between cognitively distant research areas. More generally, this work suggests new approaches for understanding, measuring and modeling collective adaptation using Optimal Transport."}, "https://arxiv.org/abs/2401.03656": {"title": "CosIn: A Statistical-Based Algorithm for Computation of Speed-Space Time Delay in Pedestrian Motion", "link": "https://arxiv.org/abs/2401.03656", "description": "arXiv:2401.03656v3 Announce Type: replace \nAbstract: The precise assessment of speed-space time delay (TD) facilitates the differentiation between pedestrian anticipation behavior and reaction behavior. Importantly, the TD scale is instrumental in the evaluation of potential collision risks inherent in the crowd, thereby offering crucial quantitative metrics for crowd risk. This article introduces the CosIn algorithm for evaluate TD during pedestrian motion, comprising the CosIn-1 and CosIn-2 algorithms. The CosIn-1 algorithm specifically addresses the precise computation issue associated with the TD of individual pedestrians, while the CosIn-2 algorithm is employed for assessing TD at a crowd scale, concurrently addressing the imperative of real-time computation. Efficacy analyses of the CosIn-1 and CosIn-2 algorithms are conducted using the data from single-file pedestrian experiments and crowd cross experiments, respectively. The results obtained demonstrate commendable precision in the algorithmic solutions. This algorithm contributes to the precise assessment of behavior patterns and collision risk within crowd dynamics."}, "https://arxiv.org/abs/2403.01269": {"title": "Network analysis using Krylov subspace trajectories", "link": "https://arxiv.org/abs/2403.01269", "description": "arXiv:2403.01269v2 Announce Type: replace \nAbstract: We describe a set of network analysis methods based on the rows of the Krylov subspace matrix computed from a network adjacency matrix via power iteration using a non-random initial vector. We refer to these node-specific row vectors as Krylov subspace trajectories. While power iteration using a random initial starting vector is commonly applied to the network adjacency matrix to compute eigenvector centrality values, this application only uses the final vector generated after numerical convergence. Importantly, use of a random initial vector means that the intermediate results of power iteration are also random and lack a clear interpretation. To the best of our knowledge, use of intermediate power iteration results for network analysis has been limited to techniques that leverage just a single pre-convergence solution, e.g., Power Iteration Clustering. In this paper, we explore methods that apply power iteration with a non-random inital vector to the network adjacency matrix to generate Krylov subspace trajectories for each node. These non-random trajectories provide important information regarding network structure, node importance, and response to perturbations. We have created this short preprint in part to generate feedback from others in the network analysis community who might be aware of similar existing work."}, "https://arxiv.org/abs/2404.05334": {"title": "Modeling the Dynamic Process of Inventions for Reducing Knowledge Search Costs", "link": "https://arxiv.org/abs/2404.05334", "description": "arXiv:2404.05334v2 Announce Type: replace \nAbstract: A knowledge search is a key process for inventions. However, there is inadequate quantitative modeling of dynamic knowledge search processes and associated search costs. In this study, agent-based and complex network methodologies were proposed to quantitatively describe the dynamic process of knowledge search for actual inventions. Prior knowledge networks (PKNs), the search space of historical patents, were constructed, representative search rules were formulated for R&amp;D agents, and measures for knowledge search cost were designed to serve as search objectives. Simulation results in the field of photolithographic technology show that search costs differ significantly with different search rules. Familiarity and Degree rules significantly outperform BFS, DFS and Recency rules in terms of knowledge search costs, and are less affected by the size and density of PKNs. Interestingly, there is no significant correlation between the mean and variance of search costs and patent value, indicating that high-value patents are not particularly difficult to obtain. The implications for innovation theories and R&amp;D practices are drawn from the models and results."}, "https://arxiv.org/abs/2301.09289": {"title": "Fundamental Limits of Spectral Clustering in Stochastic Block Models", "link": "https://arxiv.org/abs/2301.09289", "description": "arXiv:2301.09289v3 Announce Type: replace-cross \nAbstract: Spectral clustering has been widely used for community detection in network sciences. While its empirical successes are well-documented, a clear theoretical understanding, particularly for sparse networks where degrees are much smaller than $\\log n$, remains unclear. In this paper, we address this significant gap by demonstrating that spectral clustering offers exponentially small error rates when applied to sparse networks under Stochastic Block Models. Our analysis provides sharp characterizations of its performance, backed by matching upper and lower bounds possessing an identical exponent with the same leading constant. The key to our results is a novel truncated $\\ell_2$ perturbation analysis for eigenvectors, coupled with a new analysis idea of eigenvectors truncation."}, "https://arxiv.org/abs/2312.15099": {"title": "Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models", "link": "https://arxiv.org/abs/2312.15099", "description": "arXiv:2312.15099v2 Announce Type: replace-cross \nAbstract: Online hate is an escalating problem that negatively impacts the lives of Internet users, and is also subject to rapid changes due to evolving events, resulting in new waves of online hate that pose a critical threat. Detecting and mitigating these new waves present two key challenges: it demands reasoning-based complex decision-making to determine the presence of hateful content, and the limited availability of training samples hinders updating the detection model. To address this critical issue, we present a novel framework called HATEGUARD for effectively moderating new waves of online hate. HATEGUARD employs a reasoning-based approach that leverages the recently introduced chain-of-thought (CoT) prompting technique, harnessing the capabilities of large language models (LLMs). HATEGUARD further achieves prompt-based zero-shot detection by automatically generating and updating detection prompts with new derogatory terms and targets in new wave samples to effectively address new waves of online hate. To demonstrate the effectiveness of our approach, we compile a new dataset consisting of tweets related to three recently witnessed new waves: the 2022 Russian invasion of Ukraine, the 2021 insurrection of the US Capitol, and the COVID-19 pandemic. Our studies reveal crucial longitudinal patterns in these new waves concerning the evolution of events and the pressing need for techniques to rapidly update existing moderation tools to counteract them. Comparative evaluations against state-of-the-art tools illustrate the superiority of our framework, showcasing a substantial 22.22% to 83.33% improvement in detecting the three new waves of online hate. Our work highlights the severe threat posed by the emergence of new waves of online hate and represents a paradigm shift in addressing this threat practically."}, "https://arxiv.org/abs/2405.06698": {"title": "Optimizing Viscous Democracy", "link": "https://arxiv.org/abs/2405.06698", "description": "arXiv:2405.06698v1 Announce Type: new \nAbstract: Viscous democracy is a generalization of liquid democracy, a social choice framework in which voters may transitively delegate their votes. In viscous democracy, a \"viscosity\" factor decreases the weight of a delegation the further it travels, reducing the chance of excessive weight flowing between ideologically misaligned voters. We demonstrate that viscous democracy often significantly improves the quality of group decision-making over liquid democracy. We first show that finding optimal delegations within a viscous setting is NP-hard. However, simulations allow us to explore the practical effects of viscosity. Across social network structures, competence distributions, and delegation mechanisms we find high viscosity reduces the chance of \"super-voters\" attaining large amounts of weight and increases the number of voters that are able to affect the outcome of elections. This, in turn, improves group accuracy as a whole. As a result, we argue that viscosity should be considered a core component of liquid democracy."}, "https://arxiv.org/abs/2405.06700": {"title": "LLM-Augmented Agent-Based Modelling for Social Simulations: Challenges and Opportunities", "link": "https://arxiv.org/abs/2405.06700", "description": "arXiv:2405.06700v1 Announce Type: new \nAbstract: As large language models (LLMs) continue to make significant strides, their better integration into agent-based simulations offers a transformational potential for understanding complex social systems. However, such integration is not trivial and poses numerous challenges. Based on this observation, in this paper, we explore architectures and methods to systematically develop LLM-augmented social simulations and discuss potential research directions in this field. We conclude that integrating LLMs with agent-based simulations offers a powerful toolset for researchers and scientists, allowing for more nuanced, realistic, and comprehensive models of complex systems and human behaviours."}, "https://arxiv.org/abs/2405.06947": {"title": "A Galton Board Approximation Method for Estimating Pedestrian Walking Preferences within Crowds", "link": "https://arxiv.org/abs/2405.06947", "description": "arXiv:2405.06947v1 Announce Type: new \nAbstract: This paper proposes a Galton board approximation method to analyze the potential walking preferences of pedestrians. We employ the binomial distribution to estimate the walking preferences of pedestrians in dynamic crowds. Estimating the probability of the right-side preference $(p)$ based on observational data poses the challenge, as statistical measures such as means and variances often lead to divergent results. This paper aims to explore this issue."}, "https://arxiv.org/abs/2405.07071": {"title": "Colocation of skill related suppliers -- Revisiting coagglomeration using firm-to-firm network data", "link": "https://arxiv.org/abs/2405.07071", "description": "arXiv:2405.07071v1 Announce Type: new \nAbstract: Strong local clusters help firms compete on global markets. One explanation for this is that firms benefit from locating close to their suppliers and customers. However, the emergence of global supply chains shows that physical proximity is not necessarily a prerequisite to successfully manage customer-supplier relations anymore. This raises the question when firms need to colocate in value chains and when they can coordinate over longer distances. We hypothesize that one important aspect is the extent to which supply chain partners exchange not just goods but also know-how. To test this, we build on an expanding literature that studies the drivers of industrial coagglomeration to analyze when supply chain connections lead firms to colocation. We exploit detailed micro-data for the Hungarian economy between 2015 and 2017, linking firm registries, employer-employee matched data and firm-to-firm transaction data from value-added tax records. This allows us to observe colocation, labor flows and value chain connections at the level of firms, as well as construct aggregated coagglomeration patterns, skill relatedness and input-output connections between pairs of industries. We show that supply chains are more likely to support coagglomeration when the industries involved are also skill related. That is, input-output and labor market channels reinforce each other, but supplier connections only matter for colocation when industries have similar labor requirements, suggesting that they employ similar types of know-how. We corroborate this finding by analyzing the interactions between firms, showing that supplier relations are more geographically constrained between companies that operate in skill related industries."}, "https://arxiv.org/abs/2405.07072": {"title": "Selecting focused digital cohorts from social media using the metric backbone of biomedical knowledge graphs", "link": "https://arxiv.org/abs/2405.07072", "description": "arXiv:2405.07072v1 Announce Type: new \nAbstract: The abundance of social media data allows researchers to construct large digital cohorts to study the interplay between human behavior and medical treatment. Identifying the users most relevant to a specific health problem is, however, a challenge in that social media sites vary in the generality of their discourse. While X (formerly Twitter), Instagram, and Facebook cater to wide ranging topics, Reddit subgroups and dedicated patient advocacy forums trade in much more specific, biomedically-relevant discourse. To hone in on relevant users anywhere, we have developed a general framework and applied it to epilepsy discourse in social media as a test case. We analyzed the text from posts by users who mention epilepsy drugs in the general-purpose social media sites X and Instagram, the epilepsy-focused Reddit subgroup (r/Epilepsy), and the Epilepsy Foundation of America (EFA) forums. We curated a medical terms dictionary and used it to generate a knowledge graph (KG) for each online community. For each KG, we computed the metric backbone--the smallest subgraph that preserves all shortest paths in the network. By comparing the subset of users who contribute to the backbone to the subset who do not, we found that epilepsy-focused social media users contribute to the KG backbone in much higher proportion than do general-purpose social media users. Furthermore, using human annotation of Instagram posts, we demonstrated that users who do not contribute to the backbone are more than twice as likely to use dictionary terms in a manner inconsistent with their biomedical meaning. For biomedical research applications, our backbone-based approach thus has several benefits over simple engagement-based approaches: It can retain low-engagement users who nonetheless contribute meaningful biomedical insights. It can filter out very vocal users who contribute no relevant content."}, "https://arxiv.org/abs/2405.07096": {"title": "Multi-Relational Structural Entropy", "link": "https://arxiv.org/abs/2405.07096", "description": "arXiv:2405.07096v1 Announce Type: new \nAbstract: Structural Entropy (SE) measures the structural information contained in a graph. Minimizing or maximizing SE helps to reveal or obscure the intrinsic structural patterns underlying graphs in an interpretable manner, finding applications in various tasks driven by networked data. However, SE ignores the heterogeneity inherent in the graph relations, which is ubiquitous in modern networks. In this work, we extend SE to consider heterogeneous relations and propose the first metric for multi-relational graph structural information, namely, Multi-relational Structural Entropy (MrSE). To this end, we first cast SE through the novel lens of the stationary distribution from random surfing, which readily extends to multi-relational networks by considering the choices of both nodes and relation types simultaneously at each step. The resulting MrSE is then optimized by a new greedy algorithm to reveal the essential structures within a multi-relational network. Experimental results highlight that the proposed MrSE offers a more insightful interpretation of the structure of multi-relational graphs compared to SE. Additionally, it enhances the performance of two tasks that involve real-world multi-relational graphs, including node clustering and social event detection."}, "https://arxiv.org/abs/2405.07277": {"title": "Mining Influential Spreaders in Complex Networks by an Effective Combination of the Degree and K-Shell", "link": "https://arxiv.org/abs/2405.07277", "description": "arXiv:2405.07277v1 Announce Type: new \nAbstract: Graph mining is an important technique that used in many applications such as predicting and understanding behaviors and information dissemination within networks. One crucial aspect of graph mining is the identification and ranking of influential nodes, which has applications in various fields including marketing, social communications, and disease control. However, existing models and methods come with high computational complexity and may not accurately distinguish and identify influential nodes. This paper develops a method based on the k-shell index and degree centrality of nodes and their neighbors. Comparisons to previous works, such as Degree and Neighborhood information Centrality (DNC) and Neighborhood and Path Information Centrality (NPIC), are conducted. The evaluations, which include the correctness with Kendall's Tau, resolution with monotonicity index, correlation plots, and time complexity, demonstrate its superior results."}, "https://arxiv.org/abs/2405.07417": {"title": "Identifying Hate Speech Peddlers in Online Platforms", "link": "https://arxiv.org/abs/2405.07417", "description": "arXiv:2405.07417v1 Announce Type: new \nAbstract: This paper studies the problem of autonomous agents performing Bayesian social learning for sequential detection when the observations of the state belong to a high-dimensional space and are expensive to analyze. Specifically, when the observations are textual, the Bayesian agent can use a large language model (LLM) as a map to get a low-dimensional private observation. The agent performs Bayesian learning and takes an action that minimizes the expected cost and is visible to subsequent agents. We prove that a sequence of such Bayesian agents herd in finite time to the public belief and take the same action disregarding the private observations. We propose a stopping time formulation for quickest time herding in social learning and optimally balance privacy and herding. Structural results are shown on the threshold nature of the optimal policy to the stopping time problem. We illustrate the application of our framework when autonomous Bayesian detectors aim to sequentially identify if a user is a hate speech peddler on an online platform by parsing text observations using an LLM. We numerically validate our results on real-world hate speech datasets. We show that autonomous Bayesian agents designed to flag hate speech peddlers in online platforms herd and misclassify the users when the public prior is strong. We also numerically show the effect of a threshold policy in delaying herding."}, "https://arxiv.org/abs/2405.07574": {"title": "Is it getting harder to make a hit? Evidence from 65 years of US music chart history", "link": "https://arxiv.org/abs/2405.07574", "description": "arXiv:2405.07574v1 Announce Type: new \nAbstract: Since the creation of the Billboard Hot 100 music chart in 1958, the chart has been a window into the music consumption of Americans. Which songs succeed on the chart is decided by consumption volumes, which can be affected by consumer music taste, and other factors such as advertisement budgets, airplay time, the specifics of ranking algorithms, and more. Since its introduction, the chart has documented music consumerism through eras of globalization, economic growth, and the emergence of new technologies for music listening. In recent years, musicians and other hitmakers have voiced their worry that the music world is changing: Many claim that it is getting harder to make a hit but until now, the claims have not been backed using chart data. Here we show that the dynamics of the Billboard Hot 100 chart have changed significantly since the chart's founding in 1958, and in particular in the past 15 years. Whereas most songs spend less time on the chart now than songs did in the past, we show that top-1 songs have tripled their chart lifetime since the 1960s, the highest-ranked songs maintain their positions for far longer than previously, and the lowest-ranked songs are replaced more frequently than ever. At the same time, who occupies the chart has also changed over the years: In recent years, fewer new artists make it into the chart and more positions are occupied by established hit makers. Finally, investigating how song chart trajectories have changed over time, we show that historical song trajectories cluster into clear trajectory archetypes characteristic of the time period they were part of. The results are interesting in the context of collective attention: Whereas recent studies have documented that other cultural products such as books, news, and movies fade in popularity quicker in recent years, music hits seem to last longer now than in the past."}, "https://arxiv.org/abs/2405.07828": {"title": "Can LLMs Help Predict Elections? (Counter)Evidence from the World's Largest Democracy", "link": "https://arxiv.org/abs/2405.07828", "description": "arXiv:2405.07828v1 Announce Type: new \nAbstract: The study of how social media affects the formation of public opinion and its influence on political results has been a popular field of inquiry. However, current approaches frequently offer a limited comprehension of the complex political phenomena, yielding inconsistent outcomes. In this work, we introduce a new method: harnessing the capabilities of Large Language Models (LLMs) to examine social media data and forecast election outcomes. Our research diverges from traditional methodologies in two crucial respects. First, we utilize the sophisticated capabilities of foundational LLMs, which can comprehend the complex linguistic subtleties and contextual details present in social media data. Second, we focus on data from X (Twitter) in India to predict state assembly election outcomes. Our method entails sentiment analysis of election-related tweets through LLMs to forecast the actual election results, and we demonstrate the superiority of our LLM-based method against more traditional exit and opinion polls. Overall, our research offers valuable insights into the unique dynamics of Indian politics and the remarkable impact of social media in molding public attitudes within this context."}, "https://arxiv.org/abs/2405.07950": {"title": "Quantum-like states on complex synchronized networks", "link": "https://arxiv.org/abs/2405.07950", "description": "arXiv:2405.07950v1 Announce Type: new \nAbstract: Recent work has exposed the idea that interesting quantum-like probability laws, including interference effects, can be manifest in classical systems. Here we propose a model for quantum-like (QL) states and QL bits. We suggest a way that huge, complex systems can host robust states that can process information in a QL fashion. Axioms that such states should satisfy are proposed. Specifically, it is shown that building blocks suited for QL states are networks, possibly very complex, that we defined based on $k$-regular random graphs. These networks can dynamically encode a lot of information that is distilled into the emergent states we can use for QL like processing. Although the emergent states are classical, they have properties analogous to quantum states. Concrete examples of how QL functions are possible are given. The possibility of a `QL advantage' for computing-type operations and the potential relevance for new kinds of function in the brain are discussed and left as open questions."}, "https://arxiv.org/abs/2405.06656": {"title": "Exploring Social Media Posts for Depression Identification: A Study on Reddit Dataset", "link": "https://arxiv.org/abs/2405.06656", "description": "arXiv:2405.06656v1 Announce Type: cross \nAbstract: Depression is one of the most common mental disorders affecting an individual's personal and professional life. In this work, we investigated the possibility of utilizing social media posts to identify depression in individuals. To achieve this goal, we conducted a preliminary study where we extracted and analyzed the top Reddit posts made in 2022 from depression-related forums. The collected data were labeled as depressive and non-depressive using UMLS Metathesaurus. Further, the pre-processed data were fed to classical machine learning models, where we achieved an accuracy of 92.28\\% in predicting the depressive and non-depressive posts."}, "https://arxiv.org/abs/2405.06668": {"title": "Exposing and Explaining Fake News On-the-Fly", "link": "https://arxiv.org/abs/2405.06668", "description": "arXiv:2405.06668v1 Announce Type: cross \nAbstract: Social media platforms enable the rapid dissemination and consumption of information. However, users instantly consume such content regardless of the reliability of the shared data. Consequently, the latter crowdsourcing model is exposed to manipulation. This work contributes with an explainable and online classification method to recognize fake news in real-time. The proposed method combines both unsupervised and supervised Machine Learning approaches with online created lexica. The profiling is built using creator-, content- and context-based features using Natural Language Processing techniques. The explainable classification mechanism displays in a dashboard the features selected for classification and the prediction confidence. The performance of the proposed solution has been validated with real data sets from Twitter and the results attain 80 % accuracy and macro F-measure. This proposal is the first to jointly provide data stream processing, profiling, classification and explainability. Ultimately, the proposed early detection, isolation and explanation of fake news contribute to increase the quality and trustworthiness of social media contents."}, "https://arxiv.org/abs/2405.06684": {"title": "QuakeBERT: Accurate Classification of Social Media Texts for Rapid Earthquake Impact Assessment", "link": "https://arxiv.org/abs/2405.06684", "description": "arXiv:2405.06684v1 Announce Type: cross \nAbstract: Social media aids disaster response but suffers from noise, hindering accurate impact assessment and decision making for resilient cities, which few studies considered. To address the problem, this study proposes the first domain-specific LLM model and an integrated method for rapid earthquake impact assessment. First, a few categories are introduced to classify and filter microblogs considering their relationship to the physical and social impacts of earthquakes, and a dataset comprising 7282 earthquake-related microblogs from twenty earthquakes in different locations is developed as well. Then, with a systematic analysis of various influential factors, QuakeBERT, a domain-specific large language model (LLM), is developed and fine-tuned for accurate classification and filtering of microblogs. Meanwhile, an integrated method integrating public opinion trend analysis, sentiment analysis, and keyword-based physical impact quantification is introduced to assess both the physical and social impacts of earthquakes based on social media texts. Experiments show that data diversity and data volume dominate the performance of QuakeBERT and increase the macro average F1 score by 27%, while the best classification model QuakeBERT outperforms the CNN- or RNN-based models by improving the macro average F1 score from 60.87% to 84.33%. Finally, the proposed approach is applied to assess two earthquakes with the same magnitude and focal depth. Results show that the proposed approach can effectively enhance the impact assessment process by accurate detection of noisy microblogs, which enables effective post-disaster emergency responses to create more resilient cities."}, "https://arxiv.org/abs/2405.07217": {"title": "Improved bounds for polylogarithmic graph distances in scale-free percolation and related models", "link": "https://arxiv.org/abs/2405.07217", "description": "arXiv:2405.07217v1 Announce Type: cross \nAbstract: In this paper, we study graph distances in the geometric random graph models scale-free percolation SFP, geometric inhomogeneous random graphs GIRG, and hyperbolic random graphs HRG. Despite the wide success of the models, the parameter regime in which graph distances are polylogarithmic is poorly understood. We provide new and improved lower bounds. In a certain portion of the parameter regime, those match the known upper bounds.\n  Compared to the best previous lower bounds by Hao and Heydenreich, our result has several advantages: it gives matching bounds for a larger range of parameters, thus settling the question for a larger portion of the parameter space. It strictly improves the lower bounds by Hao and Heydenreich for all parameters settings in which those bounds were not tight. It gives tail bounds on the probability of having short paths, which imply shape theorems for the $k$-neighbourhood of a vertex whenever our lower bounds are tight, and tight bounds for the size of this $k$-neighbourhood. And last but not least, our proof is much simpler and not much longer than two pages, and we demonstrate that it generalizes well by showing that the same technique also works for first passage percolation."}, "https://arxiv.org/abs/2405.07764": {"title": "LGDE: Local Graph-based Dictionary Expansion", "link": "https://arxiv.org/abs/2405.07764", "description": "arXiv:2405.07764v1 Announce Type: cross \nAbstract: Expanding a dictionary of pre-selected keywords is crucial for tasks in information retrieval, such as database query and online data collection. Here we propose Local Graph-based Dictionary Expansion (LGDE), a method that uses tools from manifold learning and network science for the data-driven discovery of keywords starting from a seed dictionary. At the heart of LGDE lies the creation of a word similarity graph derived from word embeddings and the application of local community detection based on graph diffusion to discover semantic neighbourhoods of pre-defined seed keywords. The diffusion in the local graph manifold allows the exploration of the complex nonlinear geometry of word embeddings and can capture word similarities based on paths of semantic association. We validate our method on a corpus of hate speech-related posts from Reddit and Gab and show that LGDE enriches the list of keywords and achieves significantly better performance than threshold methods based on direct word similarities. We further demonstrate the potential of our method through a real-world use case from communication science, where LGDE is evaluated quantitatively on data collected and analysed by domain experts by expanding a conspiracy-related dictionary."}, "https://arxiv.org/abs/2405.07877": {"title": "Optimal accuracy for linear sets of equations with the graph Laplacian", "link": "https://arxiv.org/abs/2405.07877", "description": "arXiv:2405.07877v1 Announce Type: cross \nAbstract: We show that certain Graph Laplacian linear sets of equations exhibit optimal accuracy, guaranteeing that the relative error is no larger than the norm of the relative residual and that optimality occurs for carefully chosen right-hand sides. Such sets of equations arise in PageRank and Markov chain theory. We establish new relationships among the PageRank teleportation parameter, the Markov chain discount, and approximations to linear sets of equations. The set of optimally accurate systems can be separated into two groups for an undirected graph -- those that achieve optimality asymptotically with the graph size and those that do not -- determined by the angle between the right-hand side of the linear system and the vector of all ones. We provide supporting numerical experiments."}, "https://arxiv.org/abs/2306.08426": {"title": "Patterns of Patterns II", "link": "https://arxiv.org/abs/2306.08426", "description": "arXiv:2306.08426v3 Announce Type: replace \nAbstract: Our earlier paper \"Patterns of Patterns\" combined three techniques from training, futures studies, and design in a design pattern called PLACARD that helps groups of people work together effectively. We used that pattern in five hands-on workshop case studies which took place at various locations in the US and the UK. This experience report documents what we learned, including the way our thinking about PLACARD evolved, together with additional patterns our work generated. We evaluate the reproducibility of our methods and results, and consider the broader economic implications of this way of working. We discuss implications of our prototyping work for the design of future platforms, drawing connections with recent developments in cognitive science and artificial intelligence. This positions our patterns of patterns as a toolkit for the design and governance of systems that combine social dynamics with technical components."}, "https://arxiv.org/abs/2306.12136": {"title": "Node-layer duality in networked systems", "link": "https://arxiv.org/abs/2306.12136", "description": "arXiv:2306.12136v2 Announce Type: replace \nAbstract: Real-world networks typically exhibit several aspects, or layers, of interactions among their nodes. By permuting the role of the nodes and the layers, we establish a new criterion to construct the dual of a network. This approach allows to examine information from either a node-centric or layer-centric viewpoint. Through rigorous analytical methods and extensive simulations, we demonstrate that nodewise and layerwise connectivity measure different but related aspects of the same system. Leveraging node-layer duality provides complementary insights, enabling a deeper comprehension of diverse networks across social science, technology and biology. Taken together, these findings reveal previously unappreciated features of complex systems and provide a fresh tool for delving into their structure and dynamics."}, "https://arxiv.org/abs/2310.12181": {"title": "Precise influence evaluation in complex networks", "link": "https://arxiv.org/abs/2310.12181", "description": "arXiv:2310.12181v2 Announce Type: replace \nAbstract: Evaluating node influence is fundamental for identifying key nodes in complex networks. Existing methods typically rely on generic indicators to rank node influence across diverse networks, thereby ignoring the individualized features of each network itself. Actually, node influence stems not only from general features but the multi-scale individualized information encompassing specific network structure and task. Here we design an active learning architecture to predict node influence quantitively and precisely, which samples representative nodes based on graph entropy correlation matrix integrating multi-scale individualized information. This brings two intuitive advantages: (1) discovering potential high-influence but weak-connected nodes that are usually ignored in existing methods, (2) improving the influence maximization strategy by deducing influence interference. Significantly, our architecture demonstrates exceptional transfer learning capabilities across multiple types of networks, which can identify those key nodes with large disputation across different existing methods. Additionally, our approach, combined with a simple greedy algorithm, exhibits dominant performance in solving the influence maximization problem. This architecture holds great potential for applications in graph mining and prediction tasks."}, "https://arxiv.org/abs/2312.12186": {"title": "Social Learning in Community Structured Graphs", "link": "https://arxiv.org/abs/2312.12186", "description": "arXiv:2312.12186v3 Announce Type: replace \nAbstract: Traditional social learning frameworks consider environments with a homogeneous state, where each agent receives observations conditioned on that true state of nature. In this work, we relax this assumption and study the distributed hypothesis testing problem in a heterogeneous environment, where each agent can receive observations conditioned on their own personalized state of nature (or truth). We particularly focus on community structured networks, where each community admits their own true hypothesis. This scenario is common in various contexts, such as when sensors are spatially distributed, or when individuals in a social network have differing views or opinions. We show that the adaptive social learning strategy is a preferred choice for nonstationary environments, and allows each cluster to discover its own truth."}, "https://arxiv.org/abs/2401.00651": {"title": "IRWE: Inductive Random Walk for Joint Inference of Identity and Position Network Embedding", "link": "https://arxiv.org/abs/2401.00651", "description": "arXiv:2401.00651v2 Announce Type: replace \nAbstract: Network embedding, which maps graphs to distributed representations, is a unified framework for various graph inference tasks. According to the topology properties (e.g., structural roles and community memberships of nodes) to be preserved, it can be categorized into the identity and position embedding. However, existing methods can only capture one type of property. Some approaches can support the inductive inference that generalizes the embedding model to new nodes or graphs but relies on the availability of attributes. Due to the complicated correlations between topology and attributes, it is unclear for some inductive methods which type of property they can capture. In this study, we explore a unified framework for the joint inductive inference of identity and position embeddings without attributes. An inductive random walk embedding (IRWE) method is proposed, which combines multiple attention units to handle the random walk on graph topology and simultaneously derives identity and position embeddings that are jointly optimized. In particular, we demonstrate that some random walk statistics can be informative features to characterize node identities and positions while supporting the inductive embedding inference. Experiments validate the superior performance of IRWE beyond various baselines for the transductive and inductive inference of identity and position embeddings."}, "https://arxiv.org/abs/2402.03837": {"title": "Expressivity of Geometric Inhomogeneous Random Graphs -- Metric and Non-Metric", "link": "https://arxiv.org/abs/2402.03837", "description": "arXiv:2402.03837v2 Announce Type: replace \nAbstract: Recently there has been increased interest in fitting generative graph models to real-world networks. In particular, Bl\\\"asius et al. have proposed a framework for systematic evaluation of the expressivity of random graph models. We extend this framework to Geometric Inhomogeneous Random Graphs (GIRGs). This includes a family of graphs induced by non-metric distance functions which allow capturing more complex models of partial similarity between nodes as a basis of connection - as well as homogeneous and non-homogeneous feature spaces. As part of the extension, we develop schemes for estimating the multiplicative constant and the long-range parameter in the connection probability. Moreover, we devise an algorithm for sampling Minimum-Component-Distance GIRGs whose runtime is linear both in the number of vertices and in the dimension of the underlying geometric space. Our results provide evidence that GIRGs are more realistic candidates with respect to various graph features such as closeness centrality, betweenness centrality, local clustering coefficient, and graph effective diameter, while they face difficulties to replicate higher variance and more extreme values of graph statistics observed in real-world networks."}, "https://arxiv.org/abs/2402.18850": {"title": "A simple model of global cascades on random hypergraphs", "link": "https://arxiv.org/abs/2402.18850", "description": "arXiv:2402.18850v2 Announce Type: replace \nAbstract: This study introduces a comprehensive framework that situates information cascade research within the domain of higher-order interactions, utilizing a double-threshold hypergraph model. We propose that individuals (nodes) gain awareness of information through each communication channel (hyperedge) once the number of information adopters surpasses the threshold $\\phi_m$. However, actual adoption of the information only occurs when the cumulative influence across all communication channels exceeds a second threshold, $\\phi_k$. We analytically derive the cascade condition for both the case of a single seed node using percolation methods and the case of any seed size employing mean-field approximation. Our findings underscore that when considering the fractional seed size, $r_0 \\in (0,1]$, the connectivity pattern of the random hypergraph, characterized by the hyperdegree ($k$) and cardinality ($m$) distribution, exerts an asymmetric impact on the global cascade boundary. This asymmetry manifests in the observed differences in the boundaries of the global cascade within the $(\\phi_m, \\langle m \\rangle)$ and $(\\phi_k, \\langle k \\rangle)$ planes. However, as $r_0 \\to 0$, this asymmetric effect gradually diminishes. Overall, by elucidating the mechanisms driving information cascades within a broader context of higher-order interactions, our research contributes to theoretical advancements in complex systems theory."}, "https://arxiv.org/abs/2403.13945": {"title": "$N$-player game formulation of the majority-vote model of opinion dynamics", "link": "https://arxiv.org/abs/2403.13945", "description": "arXiv:2403.13945v2 Announce Type: replace \nAbstract: From a self-centered perspective, it can be assumed that people only hold opinions that can benefit them. If opinions have no intrinsic value, and acquire their value when held by the majority of individuals in a discussion group, then we have a situation that can be modeled as an $N$-player game. Here we explore the dynamics of (binary) opinion formation using a game-theoretic framework to study an $N$-player game version of Galam's local majority-vote model. The opinion dynamics is modeled by a stochastic imitation dynamics in which the individuals copy the opinion of more successful peers. In the infinite population limit, this dynamics is described by the classical replicator equation of evolutionary game theory. The equilibrium solution shows a threshold separating the initial frequencies that lead to the fixation of one opinion or the other. A comparison with Galam's deterministic model reveals contrasting results, especially in the presence of inflexible individuals, who never change their opinions. In particular, the $N$-player game predicts a polarized equilibrium consisting only of extremists. Using finite-size scaling analysis, we evaluate the critical exponents that determine the population size dependence of the opinion's fixation probability and mean fixation times near the threshold. The results underscore the usefulness of combining evolutionary game theory with opinion dynamics and the importance of statistical physics tools to summarize the results of Monte Carlo simulations."}, "https://arxiv.org/abs/2310.16181": {"title": "Hidden Citations Obscure True Impact in Science", "link": "https://arxiv.org/abs/2310.16181", "description": "arXiv:2310.16181v2 Announce Type: replace-cross \nAbstract: References, the mechanism scientists rely on to signal previous knowledge, lately have turned into widely used and misused measures of scientific impact. Yet, when a discovery becomes common knowledge, citations suffer from obliteration by incorporation. This leads to the concept of hidden citation, representing a clear textual credit to a discovery without a reference to the publication embodying it. Here, we rely on unsupervised interpretable machine learning applied to the full text of each paper to systematically identify hidden citations. We find that for influential discoveries hidden citations outnumber citation counts, emerging regardless of publishing venue and discipline. We show that the prevalence of hidden citations is not driven by citation counts, but rather by the degree of the discourse on the topic within the text of the manuscripts, indicating that the more discussed is a discovery, the less visible it is to standard bibliometric analysis. Hidden citations indicate that bibliometric measures offer a limited perspective on quantifying the true impact of a discovery, raising the need to extract knowledge from the full text of the scientific corpus."}, "https://arxiv.org/abs/2311.08605": {"title": "Exploring the Jungle of Bias: Political Bias Attribution in Language Models via Dependency Analysis", "link": "https://arxiv.org/abs/2311.08605", "description": "arXiv:2311.08605v2 Announce Type: replace-cross \nAbstract: The rapid advancement of Large Language Models (LLMs) has sparked intense debate regarding the prevalence of bias in these models and its mitigation. Yet, as exemplified by both results on debiasing methods in the literature and reports of alignment-related defects from the wider community, bias remains a poorly understood topic despite its practical relevance. To enhance the understanding of the internal causes of bias, we analyse LLM bias through the lens of causal fairness analysis, which enables us to both comprehend the origins of bias and reason about its downstream consequences and mitigation. To operationalize this framework, we propose a prompt-based method for the extraction of confounding and mediating attributes which contribute to the LLM decision process. By applying Activity Dependency Networks (ADNs), we then analyse how these attributes influence an LLM's decision process. We apply our method to LLM ratings of argument quality in political debates. We find that the observed disparate treatment can at least in part be attributed to confounding and mitigating attributes and model misalignment, and discuss the consequences of our findings for human-AI alignment and bias mitigation. Our code and data are at https://github.com/david-jenny/LLM-Political-Study."}, "https://arxiv.org/abs/2401.13248": {"title": "\"Here's Your Evidence\": False Consensus in Public Twitter Discussions of COVID-19 Science", "link": "https://arxiv.org/abs/2401.13248", "description": "arXiv:2401.13248v2 Announce Type: replace-cross \nAbstract: The COVID-19 pandemic brought about an extraordinary rate of scientific papers on the topic that were discussed among the general public, although often in biased or misinformed ways. In this paper, we present a mixed-methods analysis aimed at examining whether public discussions were commensurate with the scientific consensus on several COVID-19 issues. We estimate scientific consensus based on samples of abstracts from preprint servers and compare against the volume of public discussions on Twitter mentioning these papers. We find that anti-consensus posts and users, though overall less numerous than pro-consensus ones, are vastly over-represented on Twitter, thus producing a false consensus effect. This transpires with favorable papers being disproportionately amplified, along with an influx of new anti-consensus user sign-ups. Finally, our content analysis highlights that anti-consensus users misrepresent scientific findings or question scientists' integrity in their efforts to substantiate their claims."}, "https://arxiv.org/abs/2402.00447": {"title": "A Survey of Data-Efficient Graph Learning", "link": "https://arxiv.org/abs/2402.00447", "description": "arXiv:2402.00447v2 Announce Type: replace-cross \nAbstract: Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including self-supervised graph learning, semi-supervised graph learning, and few-shot graph learning. Also, we state promising directions for future research, contributing to the evolution of graph machine learning."}, "https://arxiv.org/abs/2402.15119": {"title": "A multidisciplinary framework for deconstructing bots' pluripotency in dualistic antagonism", "link": "https://arxiv.org/abs/2402.15119", "description": "arXiv:2402.15119v4 Announce Type: replace-cross \nAbstract: Anthropomorphic social bots are engineered to emulate human verbal communication and generate toxic or inflammatory content across social networking services (SNSs). Bot-disseminated misinformation could subtly yet profoundly reshape societal processes by complexly interweaving factors like repeated disinformation exposure, amplified political polarization, compromised indicators of democratic health, shifted perceptions of national identity, propagation of false social norms, and manipulation of collective memory over time. However, extrapolating bots' pluripotency across hybridized, multilingual, and heterogeneous media ecologies from isolated SNS analyses remains largely unknown, underscoring the need for a comprehensive framework to characterise bots' emergent risks to civic discourse. Here we propose an interdisciplinary framework to characterise bots' pluripotency, incorporating quantification of influence, network dynamics monitoring, and interlingual feature analysis. When applied to the geopolitical discourse around the Russo-Ukrainian conflict, results from interlanguage toxicity profiling and network analysis elucidated spatiotemporal trajectories of pro-Russian and pro-Ukrainian human and bots across hybrid SNSs. Weaponized bots predominantly inhabited X, while human primarily populated Reddit in the social media warfare. This rigorous framework promises to elucidate interlingual homogeneity and heterogeneity in bots' pluripotent behaviours, revealing synergistic human-bot mechanisms underlying regimes of information manipulation, echo chamber formation, and collective memory manifestation in algorithmically structured societies."}, "https://arxiv.org/abs/2404.15986": {"title": "Seed Selection in the Heterogeneous Moran Process", "link": "https://arxiv.org/abs/2404.15986", "description": "arXiv:2404.15986v2 Announce Type: replace-cross \nAbstract: The Moran process is a classic stochastic process that models the rise and takeover of novel traits in network-structured populations. In biological terms, a set of mutants, each with fitness $m\\in(0,\\infty)$ invade a population of residents with fitness $1$. Each agent reproduces at a rate proportional to its fitness and each offspring replaces a random network neighbor. The process ends when the mutants either fixate (take over the whole population) or go extinct. The fixation probability measures the success of the invasion. To account for environmental heterogeneity, we study a generalization of the Standard process, called the Heterogeneous Moran process. Here, the fitness of each agent is determined both by its type (resident/mutant) and the node it occupies. We study the natural optimization problem of seed selection: given a budget $k$, which $k$ agents should initiate the mutant invasion to maximize the fixation probability? We show that the problem is strongly inapproximable: it is $\\mathbf{NP}$-hard to distinguish between maximum fixation probability 0 and 1. We then focus on mutant-biased networks, where each node exhibits at least as large mutant fitness as resident fitness. We show that the problem remains $\\mathbf{NP}$-hard, but the fixation probability becomes submodular, and thus the optimization problem admits a greedy $(1-1/e)$-approximation. An experimental evaluation of the greedy algorithm along with various heuristics on real-world data sets corroborates our results."}, "https://arxiv.org/abs/2404.19634": {"title": "DF Louvain: Fast Incrementally Expanding Approach for Community Detection on Dynamic Graphs", "link": "https://arxiv.org/abs/2404.19634", "description": "arXiv:2404.19634v2 Announce Type: replace-cross \nAbstract: Community detection is the problem of recognizing natural divisions in networks. A relevant challenge in this problem is to find communities on rapidly evolving graphs. In this report we present our Parallel Dynamic Frontier (DF) Louvain algorithm, which given a batch update of edge deletions and insertions, incrementally identifies and processes an approximate set of affected vertices in the graph with minimal overhead, while using a novel approach of incrementally updating weighted-degrees of vertices and total edge weights of communities. We also present our parallel implementations of Naive-dynamic (ND) and Delta-screening (DS) Louvain. On a server with a 64-core AMD EPYC-7742 processor, our experiments show that DF Louvain obtains speedups of 179x, 7.2x, and 5.3x on real-world dynamic graphs, compared to Static, ND, and DS Louvain, respectively, and is 183x, 13.8x, and 8.7x faster, respectively, on large graphs with random batch updates. Moreover, DF Louvain improves its performance by 1.6x for every doubling of threads."}, "https://arxiv.org/abs/2405.08040": {"title": "No evidence of systematic proximity ascertainment bias in early COVID-19 cases in Wuhan Reply to Weissman (2024)", "link": "https://arxiv.org/abs/2405.08040", "description": "arXiv:2405.08040v1 Announce Type: new \nAbstract: In a short text published as Letter to the Editor of the Journal of the Royal Statistical Society Series A, Weissman (2024) argues that the finding that early COVID-19 cases without an ascertained link to Wuhan's Huanan Seafood Wholesale market resided on average closer to the market than cases epidemiologically linked to it, reveals \"major proximity ascertainment bias\". Here we show that Weissman's conclusion is based on a flawed premise, and that there is no such \"internal evidence\" of major bias. The pattern can indeed be explained by places of infection not being limited to residential neighbourhoods, and by stochasticity -- i.e., without requiring any ascertainment bias."}, "https://arxiv.org/abs/2405.08203": {"title": "Community detection in bipartite signed networks is highly dependent on parameter choice", "link": "https://arxiv.org/abs/2405.08203", "description": "arXiv:2405.08203v1 Announce Type: new \nAbstract: Decision-making processes often involve voting. Human interactions with exogenous entities such as legislations or products can be effectively modeled as two-mode (bipartite) signed networks-where people can either vote positively, negatively, or abstain from voting on the entities. Detecting communities in such networks could help us understand underlying properties: for example ideological camps or consumer preferences. While community detection is an established practice separately for bipartite and signed networks, it remains largely unexplored in the case of bipartite signed networks. In this paper, we systematically evaluate the efficacy of community detection methods on bipartite signed networks using a synthetic benchmark and real-world datasets. Our findings reveal that when no communities are present in the data, these methods often recover spurious communities. When communities are present, the algorithms exhibit promising performance, although their performance is highly susceptible to parameter choice. This indicates that researchers using community detection methods in the context of bipartite signed networks should not take the communities found at face value: it is essential to assess the robustness of parameter choices or perform domain-specific external validation."}, "https://arxiv.org/abs/2405.08331": {"title": "Are Generics and Negativity about Social Groups Common on Social Media? A Comparative Analysis of Twitter (X) Data", "link": "https://arxiv.org/abs/2405.08331", "description": "arXiv:2405.08331v1 Announce Type: new \nAbstract: Generics (unquantified generalizations) are thought to be pervasive in communication and when they are about social groups, this may offend and polarize people because generics gloss over variations between individuals. Generics about social groups might be particularly common on Twitter (X). This remains unexplored, however. Using machine learning (ML) techniques, we therefore developed an automatic classifier for social generics, applied it to more than a million tweets about people, and analyzed the tweets. We found that most tweets (78%) about people contained no generics. However, tweets with social generics received more 'likes' and retweets. Furthermore, while recent psychological research may lead to the prediction that tweets with generics about political groups are more common than tweets with generics about ethnic groups, we found the opposite. However, consistent with recent claims that political animosity is less constrained by social norms than animosity against gender and ethnic groups, negative tweets with generics about political groups were significantly more prevalent and retweeted than negative tweets about ethnic groups. Our study provides the first ML-based insights into the use and impact of social generics on Twitter."}, "https://arxiv.org/abs/2405.08398": {"title": "Exploring the spatial segmentation of housing markets from online listings", "link": "https://arxiv.org/abs/2405.08398", "description": "arXiv:2405.08398v1 Announce Type: new \nAbstract: The real estate market shows an inherent connection to space. Real estate agencies unevenly operate and specialize across space, price and type of properties, thereby segmenting the market into submarkets. We introduce here a methodology based on multipartite networks to detect the spatial segmentation emerging from data on housing online listings. Considering the spatial information of the listings, we build a bipartite network that connects agencies and spatial units. This bipartite network is projected into a network of spatial units, whose connections account for similarities in the agency ecosystem. We then apply clustering methods to this network to segment markets into spatially-coherent regions, which are found to be robust across different clustering detection algorithms, discretization of space and spatial scales, and across countries with case studies in France and Spain. This methodology addresses the long-standing issue of housing market segmentation, relevant in disciplines such as urban studies and spatial economics, and with implications for policymaking."}, "https://arxiv.org/abs/2405.08746": {"title": "Decomposing geographical and universal aspects of human mobility", "link": "https://arxiv.org/abs/2405.08746", "description": "arXiv:2405.08746v1 Announce Type: new \nAbstract: Driven by access to large volumes of detailed movement data, the study of human mobility has grown rapidly over the past decade. This body of work has argued that human mobility is scale-free, has proposed models to generate scale-free moving distance distribution, and explained how the scale-free distribution arises from aggregating displacements across scales. However, the field of human mobility has not explicitly addressed how mobility is structured by geographical constraints - such as the outlines of landmasses, lakes, rivers, the placement of buildings, roadways, and cities.\n  Using unique datasets capturing millions of movements between precise locations, this paper shows how separating the effect of geography from mobility choices reveals a universal power law spanning five orders of magnitude (from 10 m to 1,000,000 m). We incorporate geography through the pair distribution function, a fundamental quantity from condensed matter physics that encapsulates the structure of locations on which mobility occurs. This distribution captures the constraints that geography places on human mobility across different length scales.\n  Our description conclusively addresses debates between distance-based and opportunity-based perspectives on human mobility. By demonstrating how the spatial distribution of human settlements shapes human mobility, we provide a novel perspective that bridges the gap between these previously opposing ideas."}, "https://arxiv.org/abs/2405.08013": {"title": "CTRL: Continuous-Time Representation Learning on Temporal Heterogeneous Information Network", "link": "https://arxiv.org/abs/2405.08013", "description": "arXiv:2405.08013v1 Announce Type: cross \nAbstract: Inductive representation learning on temporal heterogeneous graphs is crucial for scalable deep learning on heterogeneous information networks (HINs) which are time-varying, such as citation networks. However, most existing approaches are not inductive and thus cannot handle new nodes or edges. Moreover, previous temporal graph embedding methods are often trained with the temporal link prediction task to simulate the link formation process of temporal graphs, while ignoring the evolution of high-order topological structures on temporal graphs. To fill these gaps, we propose a Continuous-Time Representation Learning (CTRL) model on temporal HINs. To preserve heterogeneous node features and temporal structures, CTRL integrates three parts in a single layer, they are 1) a \\emph{heterogeneous attention} unit that measures the semantic correlation between nodes, 2) a \\emph{edge-based Hawkes process} to capture temporal influence between heterogeneous nodes, and 3) \\emph{dynamic centrality} that indicates the dynamic importance of a node. We train the CTRL model with a future event (a subgraph) prediction task to capture the evolution of the high-order network structure. Extensive experiments have been conducted on three benchmark datasets. The results demonstrate that our model significantly boosts performance and outperforms various state-of-the-art approaches. Ablation studies are conducted to demonstrate the effectiveness of the model design."}, "https://arxiv.org/abs/2405.08278": {"title": "Facilitating Feature and Topology Lightweighting: An Ethereum Transaction Graph Compression Method for Malicious Account Detection", "link": "https://arxiv.org/abs/2405.08278", "description": "arXiv:2405.08278v1 Announce Type: cross \nAbstract: Ethereum has become one of the primary global platforms for cryptocurrency, playing an important role in promoting the diversification of the financial ecosystem. However, the relative lag in regulation has led to a proliferation of malicious activities in Ethereum, posing a serious threat to fund security. Existing regulatory methods usually detect malicious accounts through feature engineering or large-scale transaction graph mining. However, due to the immense scale of transaction data and malicious attacks, these methods suffer from inefficiency and low robustness during data processing and anomaly detection. In this regard, we propose an Ethereum Transaction Graph Compression method named TGC4Eth, which assists malicious account detection by lightweighting both features and topology of the transaction graph. At the feature level, we select transaction features based on their low importance to improve the robustness of the subsequent detection models against feature evasion attacks; at the topology level, we employ focusing and coarsening processes to compress the structure of the transaction graph, thereby improving both data processing and inference efficiency of detection models. Extensive experiments demonstrate that TGC4Eth significantly improves the computational efficiency of existing detection models while preserving the connectivity of the transaction graph. Furthermore, TGC4Eth enables existing detection models to maintain stable performance and exhibit high robustness against feature evasion attacks."}, "https://arxiv.org/abs/2405.08465": {"title": "How to Surprisingly Consider Recommendations? A Knowledge-Graph-based Approach Relying on Complex Network Metrics", "link": "https://arxiv.org/abs/2405.08465", "description": "arXiv:2405.08465v1 Announce Type: cross \nAbstract: Traditional recommendation proposals, including content-based and collaborative filtering, usually focus on similarity between items or users. Existing approaches lack ways of introducing unexpectedness into recommendations, prioritizing globally popular items over exposing users to unforeseen items. This investigation aims to design and evaluate a novel layer on top of recommender systems suited to incorporate relational information and suggest items with a user-defined degree of surprise. We propose a Knowledge Graph (KG) based recommender system by encoding user interactions on item catalogs. Our study explores whether network-level metrics on KGs can influence the degree of surprise in recommendations. We hypothesize that surprisingness correlates with certain network metrics, treating user profiles as subgraphs within a larger catalog KG. The achieved solution reranks recommendations based on their impact on structural graph metrics. Our research contributes to optimizing recommendations to reflect the metrics. We experimentally evaluate our approach on two datasets of LastFM listening histories and synthetic Netflix viewing profiles. We find that reranking items based on complex network metrics leads to a more unexpected and surprising composition of recommendation lists."}, "https://arxiv.org/abs/2405.08515": {"title": "Precarious Experiences: Citizens' Frustrations, Anxieties and Burdens of an Online Welfare Benefit System", "link": "https://arxiv.org/abs/2405.08515", "description": "arXiv:2405.08515v1 Announce Type: cross \nAbstract: There is a significant overlap between people who are supported by income-related social welfare benefits, often in precarious situations, and those who experience greater digital exclusion. We report on a study of claimants using the UK's Universal Credit online welfare benefit system designed as, and still, \"digital by default\". Through data collection involving remote interviews (n=11) and online surveys (n=66), we expose claimants' own lived experiences interacting with this system. The claimants explain how digital channels can contribute to an imbalance of power and agency, at a time when their own circumstances mean they have reduced abilities, resources and capacities, and where design choices can adversely affect people's utility to leverage help from their own wider socio-technical ecosystems. We contribute eight recommendations from these accounts to inform the future design and development of digital welfare benefit systems for this population, to reduce digital barriers and harms."}, "https://arxiv.org/abs/2405.08784": {"title": "Refinement of an Epilepsy Dictionary through Human Annotation of Health-related posts on Instagram", "link": "https://arxiv.org/abs/2405.08784", "description": "arXiv:2405.08784v1 Announce Type: cross \nAbstract: We used a dictionary built from biomedical terminology extracted from various sources such as DrugBank, MedDRA, MedlinePlus, TCMGeneDIT, to tag more than 8 million Instagram posts by users who have mentioned an epilepsy-relevant drug at least once, between 2010 and early 2016. A random sample of 1,771 posts with 2,947 term matches was evaluated by human annotators to identify false-positives. OpenAI's GPT series models were compared against human annotation. Frequent terms with a high false-positive rate were removed from the dictionary. Analysis of the estimated false-positive rates of the annotated terms revealed 8 ambiguous terms (plus synonyms) used in Instagram posts, which were removed from the original dictionary. To study the effect of removing those terms, we constructed knowledge networks using the refined and the original dictionaries and performed an eigenvector-centrality analysis on both networks. We show that the refined dictionary thus produced leads to a significantly different rank of important terms, as measured by their eigenvector-centrality of the knowledge networks. Furthermore, the most important terms obtained after refinement are of greater medical relevance. In addition, we show that OpenAI's GPT series models fare worse than human annotators in this task."}, "https://arxiv.org/abs/2305.02902": {"title": "Biased versus unbiased numerical methods for stochastic simulations", "link": "https://arxiv.org/abs/2305.02902", "description": "arXiv:2305.02902v2 Announce Type: replace \nAbstract: Approximate numerical methods are one of the most used strategies to extract information from many-interacting-agents systems. In particular, numerical approximations are of extended use to deal with epidemic, ecological and biological models, since unbiased methods like the Gillespie algorithm can become unpractical due to high CPU time usage required. However, the use of approximations has been debated and there is no clear consensus about whether unbiased methods or biased approach is the best option. In this work, we derive scaling relations for the errors in approximations based on binomial extractions. This finding allows us to build rules to compute the optimal values of both the discretization time and number of realizations needed to compute averages with the biased method with a target precision and minimum CPU-time usage. Furthermore, we also present another rule to discern whether the unbiased method or biased approach is more efficient. Ultimately, we will show that the choice of the method should depend on the desired precision for the estimation of averages."}, "https://arxiv.org/abs/2312.11529": {"title": "Efficient and Scalable Graph Generation through Iterative Local Expansion", "link": "https://arxiv.org/abs/2312.11529", "description": "arXiv:2312.11529v4 Announce Type: replace \nAbstract: In the realm of generative models for graphs, extensive research has been conducted. However, most existing methods struggle with large graphs due to the complexity of representing the entire joint distribution across all node pairs and capturing both global and local graph structures simultaneously. To overcome these issues, we introduce a method that generates a graph by progressively expanding a single node to a target graph. In each step, nodes and edges are added in a localized manner through denoising diffusion, building first the global structure, and then refining the local details. The local generation avoids modeling the entire joint distribution over all node pairs, achieving substantial computational savings with subquadratic runtime relative to node count while maintaining high expressivity through multiscale generation. Our experiments show that our model achieves state-of-the-art performance on well-established benchmark datasets while successfully scaling to graphs with at least 5000 nodes. Our method is also the first to successfully extrapolate to graphs outside of the training distribution, showcasing a much better generalization capability over existing methods."}, "https://arxiv.org/abs/2404.00793": {"title": "Learning the mechanisms of network growth", "link": "https://arxiv.org/abs/2404.00793", "description": "arXiv:2404.00793v2 Announce Type: replace \nAbstract: We propose a novel model-selection method for dynamic networks. Our approach involves training a classifier on a large body of synthetic network data. The data is generated by simulating nine state-of-the-art random graph models for dynamic networks, with parameter range chosen to ensure exponential growth of the network size in time. We design a conceptually novel type of dynamic features that count new links received by a group of vertices in a particular time interval. The proposed features are easy to compute, analytically tractable, and interpretable. Our approach achieves a near-perfect classification of synthetic networks, exceeding the state-of-the-art by a large margin. Applying our classification method to real-world citation networks gives credibility to the claims in the literature that models with preferential attachment, fitness and aging fit real-world citation networks best, although sometimes, the predicted model does not involve vertex fitness."}, "https://arxiv.org/abs/2405.01514": {"title": "Valuing maintenance strategies for fusion plants as part of a future electricity grid", "link": "https://arxiv.org/abs/2405.01514", "description": "arXiv:2405.01514v2 Announce Type: replace \nAbstract: Scheduled maintenance is likely to be lengthy and therefore consequential for the economics of fusion power plants. The maintenance strategy that maximizes the economic value of a plant depends on internal factors such as the cost and durability of the replaceable components, the frequency and duration of the maintenance blocks, and the external factors of the electricity system in which the plant operates. This paper examines the value of fusion power plants with various maintenance properties in a decarbonized United States Eastern Interconnection circa 2050. Seasonal variations in electricity supply and demand mean that certain times of year, particularly spring to early summer, are best for scheduled maintenance. Seasonality has two important consequences. First, the value of a plant can be 15% higher than what one would naively expect if value were directly proportional to its availability. Second, in some cases, replacing fractions of a component in shorter maintenance blocks spread over multiple years is better than replacing it all at once during a longer outage, even through the overall availability of the plant is lower in the former scenario."}, "https://arxiv.org/abs/2401.10057": {"title": "A method for characterizing disease emergence curves from paired pathogen detection and serology data", "link": "https://arxiv.org/abs/2401.10057", "description": "arXiv:2401.10057v2 Announce Type: replace-cross \nAbstract: Wildlife disease surveillance programs and research studies track infection and identify risk factors for wild populations, humans, and agriculture. Often, several types of samples are collected from individuals to provide more complete information about an animal's infection history. Methods that jointly analyze multiple data streams to study disease emergence and drivers of infection via epidemiological process models remain underdeveloped. Joint-analysis methods can more thoroughly analyze all available data, more precisely quantifying epidemic processes, outbreak status, and risks. We contribute a paired data modeling approach that analyzes multiple samples from individuals. We use \"characterization maps\" to link paired data to epidemiological processes through a hierarchical statistical observation model. Our approach can provide both Bayesian and frequentist estimates of epidemiological parameters and state. We motivate our approach through the need to use paired pathogen and antibody detection tests to estimate parameters and infection trajectories for the widely applicable susceptible, infectious, recovered (SIR) model. We contribute general formulas to link characterization maps to arbitrary process models and datasets and an extended SIR model that better accommodates paired data. We find via simulation that paired data can more efficiently estimate SIR parameters than unpaired data, requiring samples from 5-10 times fewer individuals. We then study SARS-CoV-2 in wild White-tailed deer (Odocoileus virginianus) from three counties in the United States. Estimates for average infectious times corroborate captive animal studies. Our methods use general statistical theory to let applications extend beyond the SIR model we consider, and to more complicated examples of paired data."}, "https://arxiv.org/abs/2405.00808": {"title": "ReeSPOT: Reeb Graph Models Semantic Patterns of Normalcy in Human Trajectories", "link": "https://arxiv.org/abs/2405.00808", "description": "arXiv:2405.00808v2 Announce Type: replace-cross \nAbstract: This paper introduces ReeSPOT, a novel Reeb graph-based method to model patterns of life in human trajectories (akin to a fingerprint). Human behavior typically follows a pattern of normalcy in day-to-day activities. This is marked by recurring activities within specific time periods. In this paper, we model this behavior using Reeb graphs where any deviation from usual day-to-day activities is encoded as nodes in the Reeb graph. The complexity of the proposed algorithm is linear with respect to the number of time points in a given trajectory. We demonstrate the usage of ReeSPOT and how it captures the critically significant spatial and temporal deviations using the nodes of the Reeb graph. Our case study presented in this paper includes realistic human movement scenarios: visiting uncommon locations, taking odd routes at infrequent times, uncommon time visits, and uncommon stay durations. We analyze the Reeb graph to interpret the topological structure of the GPS trajectories. Potential applications of ReeSPOT include urban planning, security surveillance, and behavioral research."}, "https://arxiv.org/abs/2405.09185": {"title": "Influence Maximization in Hypergraphs Using A Genetic Algorithm with New Initialization and Evaluation Methods", "link": "https://arxiv.org/abs/2405.09185", "description": "arXiv:2405.09185v1 Announce Type: new \nAbstract: Influence maximization (IM) is a crucial optimization task related to analyzing complex networks in the real world, such as social networks, disease propagation networks, and marketing networks. Publications to date about the IM problem focus mainly on graphs, which fail to capture high-order interaction relationships from the real world. Therefore, the use of hypergraphs for addressing the IM problem has been receiving increasing attention. However, identifying the most influential nodes in hypergraphs remains challenging, mainly because nodes and hyperedges are often strongly coupled and correlated. In this paper, to effectively identify the most influential nodes, we first propose a novel hypergraph-independent cascade model that integrates the influences of both node and hyperedge failures. Afterward, we introduce genetic algorithms (GA) to identify the most influential nodes that leverage hypergraph collective influences. In the GA-based method, the hypergraph collective influence is effectively used to initialize the population, thereby enhancing the quality of initial candidate solutions. The designed fitness function considers the joint influences of both nodes and hyperedges. This ensures the optimal set of nodes with the best influence on both nodes and hyperedges to be evaluated accurately. Moreover, a new mutation operator is designed by introducing factors, i.e., the collective influence and overlapping effects of nodes in hypergraphs, to breed high-quality offspring. In the experiments, several simulations on both synthetic and real hypergraphs have been conducted, and the results demonstrate that the proposed method outperforms the compared methods."}, "https://arxiv.org/abs/2405.09357": {"title": "A universal optimization framework based on cycle ranking for influence maximization in complex networks", "link": "https://arxiv.org/abs/2405.09357", "description": "arXiv:2405.09357v1 Announce Type: new \nAbstract: Influence maximization aims to identify a set of influential individuals, referred to as influencers, as information sources to maximize the spread of information within networks, constituting a vital combinatorial optimization problem with extensive practical applications and sustained interdisciplinary interest. Diverse approaches have been devised to efficiently address this issue, one of which involves selecting the influencers from a given centrality ranking. In this paper, we propose a novel optimization framework based on ranking basic cycles in networks, capable of selecting the influencers from diverse centrality measures. The experimental results demonstrate that, compared to directly selecting the top-k nodes from centrality sequences and other state-of-the-art optimization approaches, the new framework can expand the dissemination range by 1.5 to 3 times. Counterintuitively, it exhibits minimal hub property, with the average distance between influencers being only one-third of alternative approaches, regardless of the centrality metrics or network types. Our study not only paves the way for novel strategies in influence maximization but also underscores the unique potential of underappreciated cycle structures."}, "https://arxiv.org/abs/2405.09488": {"title": "Nonequilibrium phase transitions and absorbing states in a model for the dynamics of religious affiliation", "link": "https://arxiv.org/abs/2405.09488", "description": "arXiv:2405.09488v1 Announce Type: new \nAbstract: We propose a simple model to describe the dynamics of religious affiliation. For such purpose, we built a compartmental model with three distinct subpopulations, namely religious committed individuals, religious noncommitted individuals and not religious affiliated individuals. The transitions among the compartments are governed by probabilities, modeling social interactions among the groups and also spontaneous transitions among the compartments. First of all, we consider the model on a fully-connected network. Thus, we write a set of ordinary differential equations to study the evolution of the subpopulations. Our analytical and numerical results show that there is an absorbing state in the model where only one of the subpopulations survive in the long-time limit. There are also regions of parameters where some of the subpopulations coexist (two or three). We also verified the occurrence of two distinct critical points. In addition, we also present Monte Carlo simulations of the model on two-dimensional square lattices, in order to analyze the impact of the presence of a lattice structure on the critical behavior of the model. Comparison of the models' results with data for religious affiliation in Northern Ireland shows a good qualitative agreement. Finally, we considered the presence of inflexible individuals in the population, i.e., individuals that never change their states. The impact of such special agents on the critical behavior of the model is also discussed."}, "https://arxiv.org/abs/2405.08830": {"title": "Evaluating Supply Chain Resilience During Pandemic Using Agent-based Simulation", "link": "https://arxiv.org/abs/2405.08830", "description": "arXiv:2405.08830v1 Announce Type: cross \nAbstract: Recent pandemics have highlighted vulnerabilities in our global economic systems, especially supply chains. Possible future pandemic raises a dilemma for businesses owners between short-term profitability and long-term supply chain resilience planning. In this study, we propose a novel agent-based simulation model integrating extended Susceptible-Infected-Recovered (SIR) epidemiological model and supply and demand economic model to evaluate supply chain resilience strategies during pandemics. Using this model, we explore a range of supply chain resilience strategies under pandemic scenarios using in silico experiments. We find that a balanced approach to supply chain resilience performs better in both pandemic and non-pandemic times compared to extreme strategies, highlighting the importance of preparedness in the form of a better supply chain resilience. However, our analysis shows that the exact supply chain resilience strategy is hard to obtain for each firm and is relatively sensitive to the exact profile of the pandemic and economic state at the beginning of the pandemic. As such, we used a machine learning model that uses the agent-based simulation to estimate a near-optimal supply chain resilience strategy for a firm. The proposed model offers insights for policymakers and businesses to enhance supply chain resilience in the face of future pandemics, contributing to understanding the trade-offs between short-term gains and long-term sustainability in supply chain management before and during pandemics."}, "https://arxiv.org/abs/2405.09529": {"title": "Artificial Intelligence for the Internal Democracy of Political Parties", "link": "https://arxiv.org/abs/2405.09529", "description": "arXiv:2405.09529v1 Announce Type: cross \nAbstract: The article argues that AI can enhance the measurement and implementation of democratic processes within political parties, known as Intra-Party Democracy (IPD). It identifies the limitations of traditional methods for measuring IPD, which often rely on formal parameters, self-reported data, and tools like surveys. Such limitations lead to the collection of partial data, rare updates, and significant demands on resources. To address these issues, the article suggests that specific data management and Machine Learning (ML) techniques, such as natural language processing and sentiment analysis, can improve the measurement (ML about) and practice (ML for) of IPD. The article concludes by considering some of the principal risks of ML for IPD, including concerns over data privacy, the potential for manipulation, and the dangers of overreliance on technology."}, "https://arxiv.org/abs/2310.16451": {"title": "The Small-World Effect for Interferometer Networks", "link": "https://arxiv.org/abs/2310.16451", "description": "arXiv:2310.16451v2 Announce Type: replace \nAbstract: Complex network theory has focused on properties of networks with real-valued edge weights. However, in signal transfer networks, such as those representing the transfer of light across an interferometer, complex-valued edge weights are needed to represent the manipulation of the signal in both magnitude and phase. These complex-valued edge weights introduce interference into the signal transfer, but it is unknown how such interference affects network properties such as small-worldness. To address this gap, we have introduced a small-world interferometer network model with complex-valued edge weights and generalized existing network measures to define the interferometric clustering coefficient, the apparent path length, and the interferometric small-world coefficient. Using high-performance computing resources, we generated a large set of small-world interferometers over a wide range of parameters in system size, nearest-neighbor count, and edge-weight phase and computed their interferometric network measures. We found that the interferometric small-world coefficient depends significantly on the amount of phase on complex-valued edge weights: for small edge-weight phases, constructive interference led to a higher interferometric small-world coefficient; while larger edge-weight phases induced destructive interference which led to a lower interferometric small-world coefficient. Thus, for the small-world interferometer model, interferometric measures are necessary to capture the effect of interference on signal transfer. This model is an example of the type of problem that necessitates interferometric measures, and applies to any wave-based network including quantum networks."}, "https://arxiv.org/abs/2403.08493": {"title": "Rumor Forwarding Prediction Model Based on Uncertain Time Series", "link": "https://arxiv.org/abs/2403.08493", "description": "arXiv:2403.08493v2 Announce Type: replace \nAbstract: The rapid spread of rumors in social media is mainly caused by individual retweets. This paper applies uncertainty time series analysis (UTSA) to analyze a rumor retweeting behavior on Weibo. First, the rumor forwarding is modeled using uncertain time series, including order selection, parameter estimation, residual analysis, uncertainty hypothesis testing and forecast, and the validity of using uncertain time series analysis is further supported by analyzing the characteristics of the residual plot. The experimental results show that the uncertain time series can better predict the next stage of rumor forwarding. The results of the study have important practical significance for rumor management and the management of social media information dissemination."}, "https://arxiv.org/abs/2306.05597": {"title": "On the implementation of zero-determinant strategies in repeated games", "link": "https://arxiv.org/abs/2306.05597", "description": "arXiv:2306.05597v2 Announce Type: replace-cross \nAbstract: Zero-determinant strategies are a class of strategies in repeated games which unilaterally control payoffs. Zero-determinant strategies have attracted much attention in studies of social dilemma, particularly in the context of evolution of cooperation. So far, not only general properties of zero-determinant strategies have been investigated, but zero-determinant strategies have been applied to control in the fields of information and communications technology and analysis of imitation. Here, we further deepen our understanding on general mathematical properties of zero-determinant strategies. We first prove that zero-determinant strategies, if exist, can be implemented by some one-dimensional transition probability. Next, we prove that, if a two-player game has a non-trivial potential function, a zero-determinant strategy exists in its repeated version. These results assist us to implement zero-determinant strategies in broader situations."}, "https://arxiv.org/abs/2405.09640": {"title": "Personalized Content Moderation and Emergent Outcomes", "link": "https://arxiv.org/abs/2405.09640", "description": "arXiv:2405.09640v1 Announce Type: new \nAbstract: Social media platforms have implemented automated content moderation tools to preserve community norms and mitigate online hate and harassment. Recently, these platforms have started to offer Personalized Content Moderation (PCM), granting users control over moderation settings or aligning algorithms with individual user preferences. While PCM addresses the limitations of the one-size-fits-all approach and enhances user experiences, it may also impact emergent outcomes on social media platforms. Our study reveals that PCM leads to asymmetric information loss (AIL), potentially impeding the development of a shared understanding among users, crucial for healthy community dynamics. We further demonstrate that PCM tools could foster the creation of echo chambers and filter bubbles, resulting in increased community polarization. Our research is the first to identify AIL as a consequence of PCM and to highlight its potential negative impacts on online communities."}, "https://arxiv.org/abs/2405.09643": {"title": "Energy Consumption of Plant Factory with Artificial Light: Challenges and Opportunities", "link": "https://arxiv.org/abs/2405.09643", "description": "arXiv:2405.09643v1 Announce Type: new \nAbstract: Plant factory with artificial light (PFAL) is a promising technology for relieving the food crisis, especially in urban areas or arid regions endowed with abundant resources. However, lighting and HVAC (heating, ventilation, and air conditioning) systems of PFAL have led to much greater energy consumption than open-field and greenhouse farming, limiting the application of PFAL to a wider extent. Recent researches pay much more attention to the optimization of energy consumption in order to develop and promote the PFAL technology with reduced energy usage. This work comprehensively summarizes the current energy-saving methods on lighting, HVAC systems, as well as their coupling methods for a more energy-efficient PFAL. Besides, we offer our perspectives on further energy-saving strategies and exploit the renewable energy resources for PFAL to respond to the urgent need for energy-efficient production."}, "https://arxiv.org/abs/2405.09978": {"title": "Pedestrian evacuations with imitation of cooperative behavior", "link": "https://arxiv.org/abs/2405.09978", "description": "arXiv:2405.09978v1 Announce Type: new \nAbstract: We analyze the dynamics of room evacuation for mixed populations that include both competitive and cooperative individuals through numerical simulations using the social force model. Cooperative agents represent well-trained individuals who know how to behave in order to reduce risks within high-density crowds. We consider that competitive agents can imitate cooperative behavior when they are in close proximity to cooperators. We study the effects of the imitation of cooperative behavior on the duration and safety of evacuations, analyzing evacuation time and other quantities of interest for varying parameters such as the proportions of mixing, the aspect ratio of the room, and the parameters characterizing individual behaviors. Our main findings reveal that the addition of a relatively small number of cooperative agents into a crowd can reduce evacuation time and the density near the exit door, making the evacuation faster and safer despite an increase in the total number of agents. In particular, for long spaces such as corridors, a small number of added cooperative agents can significantly facilitate the evacuation process. We compare our results with those of systems without imitation and also study the general role of cooperation, providing further analysis for homogeneous populations. Our main conclusions emphasize the potential relevance of training people how to behave in high-density crowds"}, "https://arxiv.org/abs/2405.10187": {"title": "Influence Maximization in Hypergraphs using Multi-Objective Evolutionary Algorithms", "link": "https://arxiv.org/abs/2405.10187", "description": "arXiv:2405.10187v1 Announce Type: new \nAbstract: The Influence Maximization (IM) problem is a well-known NP-hard combinatorial problem over graphs whose goal is to find the set of nodes in a network that spreads influence at most. Among the various methods for solving the IM problem, evolutionary algorithms (EAs) have been shown to be particularly effective. While the literature on the topic is particularly ample, only a few attempts have been made at solving the IM problem over higher-order networks, namely extensions of standard graphs that can capture interactions that involve more than two nodes. Hypergraphs are a valuable tool for modeling complex interaction networks in various domains; however, they require rethinking of several graph-based problems, including IM. In this work, we propose a multi-objective EA for the IM problem over hypergraphs that leverages smart initialization and hypergraph-aware mutation. While the existing methods rely on greedy or heuristic methods, to our best knowledge this is the first attempt at applying EAs to this problem. Our results over nine real-world datasets and three propagation models, compared with five baseline algorithms, reveal that our method achieves in most cases state-of-the-art results in terms of hypervolume and solution diversity."}, "https://arxiv.org/abs/2405.10213": {"title": "Words as Trigger Points in Social Media Discussions", "link": "https://arxiv.org/abs/2405.10213", "description": "arXiv:2405.10213v1 Announce Type: new \nAbstract: Trigger points are a concept introduced by Mau, Lux, and Westheuser (2023) to study qualitative focus group interviews and understand polarisation in Germany. When people communicate, trigger points represent moments when individuals feel that their understanding of what is fair, normal, or appropriate in society is questioned. In the original studies, individuals react affectively to such triggers and show strong and negative emotional responses. In this paper, we introduce the first systematic study of the large-scale effect of individual words as trigger points by analysing a large amount of social media posts. We examine online deliberations on Reddit between 2020 and 2022 and collect >100 million posts from subreddits related to a set of words identified as trigger points in UK politics. We find that such trigger words affect user engagement and have noticeable consequences on animosity in online discussions. We share empirical evidence of trigger words causing animosity, and how they provide incentives for hate speech, adversarial debates, and disagreements. Our work is the first to introduce trigger points to computational studies of online communication. Our findings are relevant to researchers interested in online harms and who examine how citizens debate politics and society in light of affective polarisation."}, "https://arxiv.org/abs/2405.10233": {"title": "iDRAMA-Scored-2024: A Dataset of the Scored Social Media Platform from 2020 to 2023", "link": "https://arxiv.org/abs/2405.10233", "description": "arXiv:2405.10233v1 Announce Type: new \nAbstract: Online web communities often face bans for violating platform policies, encouraging their migration to alternative platforms. This migration, however, can result in increased toxicity and unforeseen consequences on the new platform. In recent years, researchers have collected data from many alternative platforms, indicating coordinated efforts leading to offline events, conspiracy movements, hate speech propagation, and harassment. Thus, it becomes crucial to characterize and understand these alternative platforms. To advance research in this direction, we collect and release a large-scale dataset from Scored -- an alternative Reddit platform that sheltered banned fringe communities, for example, c/TheDonald (a prominent right-wing community) and c/GreatAwakening (a conspiratorial community). Over four years, we collected approximately 57M posts from Scored, with at least 58 communities identified as migrating from Reddit and over 950 communities created since the platform's inception. Furthermore, we provide sentence embeddings of all posts in our dataset, generated through a state-of-the-art model, to further advance the field in characterizing the discussions within these communities. We aim to provide these resources to facilitate their investigations without the need for extensive data collection and processing efforts."}, "https://arxiv.org/abs/2405.09982": {"title": "Dynamical behavior and optimal control of a stochastic SAIRS epidemic model with two saturated incidences", "link": "https://arxiv.org/abs/2405.09982", "description": "arXiv:2405.09982v1 Announce Type: cross \nAbstract: Stochastic models are widely used to investigate the spread of epidemics in a complex environment. This paper extends a deterministic SAIRS epidemic model to a stochastic case with limited patient capacity and exposure. We first study the dynamical properties of the model under certain conditions, including persistence, extinction, and ergodic. Then, we introduce vaccination and isolation into the model as control variables. The optimal control strategies are obtained based on the Pontryagin minimum principle. Finally, numerical simulations are given to illustrate our theoretical results."}, "https://arxiv.org/abs/2307.04612": {"title": "Emergence of Cooperation in Two-agent Repeated Games with Reinforcement Learning", "link": "https://arxiv.org/abs/2307.04612", "description": "arXiv:2307.04612v2 Announce Type: replace \nAbstract: Cooperation is the foundation of ecosystems and the human society, and the reinforcement learning provides crucial insight into the mechanism for its emergence. However, most previous work has mostly focused on the self-organization at the population level, the fundamental dynamics at the individual level remains unclear. Here, we investigate the evolution of cooperation in a two-agent system, where each agent pursues optimal policies according to the classical Q-learning algorithm in playing the strict prisoner's dilemma. We reveal that a strong memory and long-sighted expectation yield the emergence of Coordinated Optimal Policies (COPs), where both agents act like Win-Stay, Lose-Shift (WSLS) to maintain a high level of cooperation. Otherwise, players become tolerant toward their co-player's defection and the cooperation loses stability in the end where the policy all Defection (All-D) prevails. This suggests that tolerance could be a good precursor to a crisis in cooperation. Furthermore, our analysis shows that the Coordinated Optimal Modes (COMs) for different COPs gradually lose stability as memory weakens and expectation for the future decreases, where agents fail to predict co-player's action in games and defection dominates. As a result, we give the constraint to expectations of future and memory strength for maintaining cooperation. In contrast to the previous work, the impact of exploration on cooperation is found not be consistent, but depends on composition of COMs. By clarifying these fundamental issues in this two-player system, we hope that our work could be helpful for understanding the emergence and stability of cooperation in more complex scenarios in reality."}, "https://arxiv.org/abs/2401.12732": {"title": "CDRNP: Cross-Domain Recommendation to Cold-Start Users via Neural Process", "link": "https://arxiv.org/abs/2401.12732", "description": "arXiv:2401.12732v2 Announce Type: replace-cross \nAbstract: Cross-domain recommendation (CDR) has been proven as a promising way to tackle the user cold-start problem, which aims to make recommendations for users in the target domain by transferring the user preference derived from the source domain. Traditional CDR studies follow the embedding and mapping (EMCDR) paradigm, which transfers user representations from the source to target domain by learning a user-shared mapping function, neglecting the user-specific preference. Recent CDR studies attempt to learn user-specific mapping functions in meta-learning paradigm, which regards each user's CDR as an individual task, but neglects the preference correlations among users, limiting the beneficial information for user representations. Moreover, both of the paradigms neglect the explicit user-item interactions from both domains during the mapping process. To address the above issues, this paper proposes a novel CDR framework with neural process (NP), termed as CDRNP. Particularly, it develops the meta-learning paradigm to leverage user-specific preference, and further introduces a stochastic process by NP to capture the preference correlations among the overlapping and cold-start users, thus generating more powerful mapping functions by mapping the user-specific preference and common preference correlations to a predictive probability distribution. In addition, we also introduce a preference remainer to enhance the common preference from the overlapping users, and finally devises an adaptive conditional decoder with preference modulation to make prediction for cold-start users with items in the target domain. Experimental results demonstrate that CDRNP outperforms previous SOTA methods in three real-world CDR scenarios."}, "https://arxiv.org/abs/2405.10322": {"title": "Exploring the Independent Cascade Model and Its Evolution in Social Network Information Diffusion", "link": "https://arxiv.org/abs/2405.10322", "description": "arXiv:2405.10322v1 Announce Type: new \nAbstract: This paper delves into the paramount significance of information dissemination within the dynamic realm of social networks. It underscores the pivotal role of information communication models in unraveling the intricacies of data propagation in the digital age. By shedding light on the profound influence of these models, it not only lays the groundwork for exploring various hierarchies and their manifestations but also serves as a catalyst for further research in this formidable field."}, "https://arxiv.org/abs/2405.10338": {"title": "Financial Interactions and Capital Accumulation", "link": "https://arxiv.org/abs/2405.10338", "description": "arXiv:2405.10338v1 Announce Type: new \nAbstract: In a series of precedent papers, we have presented a comprehensive methodology, termed Field Economics, for translating a standard economic model into a statistical field-formalism framework. This formalism requires a large number of heterogeneous agents, possibly of different types. It reveals the emergence of collective states among these agents or type of agents while preserving the interactions and microeconomic features of the system at the individual level. In two prior papers, we applied this formalism to analyze the dynamics of capital allocation and accumulation in a simple microeconomic framework of investors and firms.Building upon our prior work, the present paper refines the initial model by expanding its scope. Instead of considering financial firms investing solely in real sectors, we now suppose that financial agents may also invest in other financial firms. We also introduce banks in the system that act as investors with a credit multiplier. Two types of interaction are now considered within the financial sector: financial agents can lend capital to, or choose to buy shares of, other financial firms. Capital now flows between financial agents and is only partly invested in real sectors, depending on their relative returns. We translate this framework into our formalism and study the diffusion of capital and possible defaults in the system, both at the macro and micro level.At the macro level, we find that several collective states may emerge, each characterized by a distinct level of average capital and investors per sector. These collective states depend on external parameters such as level of connections between investors or firms' productivity.The multiplicity of possible collective states is the consequence of the nature of the system composed of interconnected heterogeneous agents. Several equivalent patterns of returns and portfolio allocation may emerge. The multiple collective states induce the unstable nature of financial markets, and some of them include defaults may emerge. At the micro level, we study the propagation of returns and defaults within a given collective state. Our findings highlight the significant role of banks, which can either stabilize the system through lending activities or propagate instability through loans to investors."}, "https://arxiv.org/abs/2405.10355": {"title": "Assessing the Impact of Case Correction Methods on the Fairness of COVID-19 Predictive Models", "link": "https://arxiv.org/abs/2405.10355", "description": "arXiv:2405.10355v1 Announce Type: new \nAbstract: One of the central difficulties of addressing the COVID-19 pandemic has been accurately measuring and predicting the spread of infections. In particular, official COVID-19 case counts in the United States are under counts of actual caseloads due to the absence of universal testing policies. Researchers have proposed a variety of methods for recovering true caseloads, often through the estimation of statistical models on more reliable measures, such as death and hospitalization counts, positivity rates, and demographics. However, given the disproportionate impact of COVID-19 on marginalized racial, ethnic, and socioeconomic groups, it is important to consider potential unintended effects of case correction methods on these groups. Thus, we investigate two of these correction methods for their impact on a downstream COVID-19 case prediction task. For that purpose, we tailor an auditing approach and evaluation protocol to analyze the fairness of the COVID-19 prediction task by measuring the difference in model performance between majority-White counties and majority-minority counties. We find that one of the correction methods improves fairness, decreasing differences in performance between majority-White and majority-minority counties, while the other method increases differences, introducing bias. While these results are mixed, it is evident that correction methods have the potential to exacerbate existing biases in COVID-19 case data and in downstream prediction tasks. Researchers planning to develop or use case correction methods must be careful to consider negative effects on marginalized groups."}, "https://arxiv.org/abs/2405.10417": {"title": "Cosmic rays for imaging cultural heritage objects", "link": "https://arxiv.org/abs/2405.10417", "description": "arXiv:2405.10417v1 Announce Type: new \nAbstract: In cultural heritage conservation, it is increasingly common to rely on non-destructive imaging methods based on the absorption or scattering of photons ($X$ or $\\gamma$ rays) or neutrons. However, physical and practical issues limit these techniques: their penetration depth may be insufficient for large and dense objects, they require transporting the objects of interest to dedicated laboratories, artificial radiation is hazardous and may induce activation in the material under study. Muons are elementary particles abundantly and freely produced in cosmic-ray interactions in the atmosphere. Their absorption and scattering in matter are characteristically dependent on the density and elemental composition of the material that they traverse, which offers the possibility of exploiting them for sub-surface remote imaging. This novel technique, nicknamed \"muography\", has been applied in use cases ranging from geophysics to archaeology to nuclear safety, but it has been so far under-explored for a vast category of cultural heritage objects that are relatively large (from decimeters to human size) and dense (stone, metals). The development of portable muon detectors makes muography particularly competitive in cases where the items to be analysed are not transportable, or set up in a confined environment. This document reviews the relevant literature, presents some exemplary use cases, and critically assesses the strengths and weaknesses of muography in this context."}, "https://arxiv.org/abs/2405.10450": {"title": "Quantifying national space heating flexibility potential at high spatial resolution with heating consumption data", "link": "https://arxiv.org/abs/2405.10450", "description": "arXiv:2405.10450v1 Announce Type: new \nAbstract: Decarbonizing the building stock in cold countries by replacing fossil fuel boilers with heat pumps is expected to drastically increase electricity demand. While heating flexibility could reduce the impact of additional demand from heat pumps on the power system, characterizing the national spatial distribution of heating flexibility capacity to incorporate into sophisticated power system models is challenging. This paper introduces a novel method for quantifying at large scale and high spatial resolution the energy capacity and duration of heating flexibility in existing building stock based on historical heating consumption and temperature data. This method can reflect the geographic diversity of the national building stock in sophisticated power system models. The proposed heating consumption-based method was tested in Britain using national residential gas data. The results demonstrate the potential of this approach to characterize the heterogeneous distribution of heating flexibility capacity at the national scale. Assuming a 3$^\\circ$C temperature flexibility window, a total thermal energy storage capacity of 500 GWh$_{th}$ is identified in the British housing stock. For an illustrative cold weather COP value of 2.5, this thermal energy storage capacity is equivalent to 200 GWh of electricity storage. Regarding heating flexibility duration, gas-heated homes have a median of 5.9 heat-free hours for 20th percentile regional daily winter temperatures from 2010 to 2022. However, extreme cold days nearly halve flexibility duration to a median of 3.6 heat-free hours. These high spatial resolution energy capacity and self-discharge parameters can account for geographic diversity at the national scale and provide a new data-based layer of information for sophisticated power system models to support energy transition."}, "https://arxiv.org/abs/2405.10547": {"title": "GPTs Window Shopping: An analysis of the Landscape of Custom ChatGPT Models", "link": "https://arxiv.org/abs/2405.10547", "description": "arXiv:2405.10547v1 Announce Type: new \nAbstract: OpenAI's ChatGPT initiated a wave of technical iterations in the space of Large Language Models (LLMs) by demonstrating the capability and disruptive power of LLMs. OpenAI has prompted large organizations to respond with their own advancements and models to push the LLM performance envelope. OpenAI has prompted large organizations to respond with their own advancements and models to push the LLM performance envelope. OpenAI's success in spotlighting AI can be partially attributed to decreased barriers to entry, enabling any individual with an internet-enabled device to interact with LLMs. What was previously relegated to a few researchers and developers with necessary computing resources is now available to all. A desire to customize LLMs to better accommodate individual needs prompted OpenAI's creation of the GPT Store, a central platform where users can create and share custom GPT models. Customization comes in the form of prompt-tuning, analysis of reference resources, browsing, and external API interactions, alongside a promise of revenue sharing for created custom GPTs. In this work, we peer into the window of the GPT Store and measure its impact. Our analysis constitutes a large-scale overview of the store exploring community perception, GPT details, and the GPT authors, in addition to a deep-dive into a 3rd party storefront indexing user-submitted GPTs, exploring if creators seek to monetize their creations in the absence of OpenAI's revenue sharing."}, "https://arxiv.org/abs/2405.10558": {"title": "CACL: Community-Aware Heterogeneous Graph Contrastive Learning for Social Media Bot Detection", "link": "https://arxiv.org/abs/2405.10558", "description": "arXiv:2405.10558v1 Announce Type: new \nAbstract: Social media bot detection is increasingly crucial with the rise of social media platforms. Existing methods predominantly construct social networks as graph and utilize graph neural networks (GNNs) for bot detection. However, most of these methods focus on how to improve the performance of GNNs while neglecting the community structure within social networks. Moreover, GNNs based methods still face problems such as poor model generalization due to the relatively small scale of the dataset and over-smoothness caused by information propagation mechanism. To address these problems, we propose a Community-Aware Heterogeneous Graph Contrastive Learning framework (CACL), which constructs social network as heterogeneous graph with multiple node types and edge types, and then utilizes community-aware module to dynamically mine both hard positive samples and hard negative samples for supervised graph contrastive learning with adaptive graph enhancement algorithms. Extensive experiments demonstrate that our framework addresses the previously mentioned challenges and outperforms competitive baselines on three social media bot benchmarks."}, "https://arxiv.org/abs/2405.10640": {"title": "COMET: NFT Price Prediction with Wallet Profiling", "link": "https://arxiv.org/abs/2405.10640", "description": "arXiv:2405.10640v1 Announce Type: new \nAbstract: As the non-fungible token (NFT) market flourishes, price prediction emerges as a pivotal direction for investors gaining valuable insight to maximize returns. However, existing works suffer from a lack of practical definitions and standardized evaluations, limiting their practical application. Moreover, the influence of users' multi-behaviour transactions that are publicly accessible on NFT price is still not explored and exhibits challenges. In this paper, we address these gaps by presenting a practical and hierarchical problem definition. This approach unifies both collection-level and token-level task and evaluation methods, which cater to varied practical requirements of investors. To further understand the impact of user behaviours on the variation of NFT price, we propose a general wallet profiling framework and develop a COmmunity enhanced Multi-bEhavior Transaction graph model, named COMET. COMET profiles wallets with a comprehensive view and considers the impact of diverse relations and interactions within the NFT ecosystem on NFT price variations, thereby improving prediction performance. Extensive experiments conducted in our deployed system demonstrate the superiority of COMET, underscoring its potential in the insight toolkit for NFT investors."}, "https://arxiv.org/abs/2405.10665": {"title": "Leader-Follower Identification with Vehicle-Following Calibration for Non-Lane-Based Traffic", "link": "https://arxiv.org/abs/2405.10665", "description": "arXiv:2405.10665v1 Announce Type: new \nAbstract: Most car-following models were originally developed for lane-based traffic. Over the past two decades, efforts have been made to calibrate car-following models for non-lane-based traffic. However, traffic conditions with varying vehicle dimensions, intermittent following, and multiple leaders often occur and make subjective Leader-Follower (LF) pair identification challenging. In this study, we analyze Vehicle Following (VF) behavior in traffic with a lack of lane discipline using high-resolution microscopic trajectory data collected in Chennai, India. The paper's main contributions are threefold. Firstly, three criteria are used to identify LF pairs from the driver's perspective, taking into account the intermittent following, lack of lane discipline due to consideration of lateral separation, and the presence of in-between vehicles. Second, the psycho-physical concept of the regime in the Wiedemann-99 model is leveraged to determine the traffic-dependent \"influence zone\" for LF identification. Third, a joint and consistent framework is proposed for identifying LF pairs and estimating VF parameters. The proposed methodology outperforms other heuristic-based LF identification methods from the literature in terms of quantitative and qualitative performance measures. The proposed approach can enable robust and more realistic LF identification and VF parameter calibration with practical applications such as LOS analysis, capacity, and travel time estimation."}, "https://arxiv.org/abs/2405.10798": {"title": "Understanding following patterns among high-performance athletes", "link": "https://arxiv.org/abs/2405.10798", "description": "arXiv:2405.10798v1 Announce Type: new \nAbstract: Professional sports enhance interaction among athletes through training groups, sponsored events and competitions. Among these, the Olympic Games represent the largest competition with a global impact, providing the participants with a unique opportunity for interaction. We studied the following patterns among highly successful athletes to understand the structure of their interactions. We used the list of Olympic medallists in the Tokyo 2020 Games to extract their follower-followee network in Twitter, finding 7,326 connections among 964 athletes. The network displayed frequent connections to similar peers in terms of their features including sex, country and sport. We quantified the influence of these features in the followees choice through a gravity approach capturing the number of connections between homogeneous groups. Our research remarks the importance of datasets built from public exposure of professional athletes, serving as a proxy to investigate interesting aspects of many complex socio-cultural systems at different scales."}, "https://arxiv.org/abs/2405.10818": {"title": "Modeling Supply Chain Interaction and Disruption: Insights from Real-world Data and Complex Adaptive System", "link": "https://arxiv.org/abs/2405.10818", "description": "arXiv:2405.10818v1 Announce Type: new \nAbstract: In the rapidly evolving automotive industry, Systems-on-Chips (SoCs) are playing an increasingly crucial role in enhancing vehicle intelligence, connectivity, and safety features. For enterprises whose business encompasses automotive SoCs, the sustained and stable provision and receipt of SoC relevant goods or services are essential. Considering the imperative for a resilient and adaptable supply network, enterprises are concentrating their efforts on formulating strategies to address risks stemming from supply chain disruptions caused by technological obsolescence, natural disasters, and geopolitical tensions. This study presents an open supply knowledge extraction and complement approach and build a supply chain network of automotive SoC enterprises in China, which incorporates cross-domain named entity recognition under limited information, fuzzy matching of firm entities, and supply relation inferring based on knowledge graph. Subsequently, we exhibit the degree and registered capital distribution across firms, and analyze the correlations between centrality metrics in the supply chain network. Finally, based on recovery capacity and risk transfer, two interaction disruption models (IDMs) are developed to elucidate the adaptive behaviors and effect of network disruptions under various business and attack strategies. This research not only aids in exploring the complexities of Chinese automotive SoC supply chain but also enriches our understanding of the dynamics of firm behavior in this crucial industry sector."}, "https://arxiv.org/abs/2405.09843": {"title": "Organizational Selection of Innovation", "link": "https://arxiv.org/abs/2405.09843", "description": "arXiv:2405.09843v1 Announce Type: cross \nAbstract: Budgetary constraints force organizations to pursue only a subset of possible innovation projects. Identifying which subset is most promising is an error-prone exercise, and involving multiple decision makers may be prudent. This raises the question of how to most effectively aggregate their collective nous. Our model of organizational portfolio selection provides some first answers. We show that portfolio performance can vary widely. Delegating evaluation makes sense when organizations employ the relevant experts and can assign projects to them. In most other settings, aggregating the impressions of multiple agents leads to better performance than delegation. In particular, letting agents rank projects often outperforms alternative aggregation rules -- including averaging agents' project scores as well as counting their approval votes -- especially when organizations have tight budgets and can select only a few project alternatives out of many."}, "https://arxiv.org/abs/2405.10497": {"title": "SMP Challenge: An Overview and Analysis of Social Media Prediction Challenge", "link": "https://arxiv.org/abs/2405.10497", "description": "arXiv:2405.10497v1 Announce Type: cross \nAbstract: Social Media Popularity Prediction (SMPP) is a crucial task that involves automatically predicting future popularity values of online posts, leveraging vast amounts of multimodal data available on social media platforms. Studying and investigating social media popularity becomes central to various online applications and requires novel methods of comprehensive analysis, multimodal comprehension, and accurate prediction.\n  SMP Challenge is an annual research activity that has spurred academic exploration in this area. This paper summarizes the challenging task, data, and research progress. As a critical resource for evaluating and benchmarking predictive models, we have released a large-scale SMPD benchmark encompassing approximately half a million posts authored by around 70K users. The research progress analysis provides an overall analysis of the solutions and trends in recent years. The SMP Challenge website (www.smp-challenge.com) provides the latest information and news."}, "https://arxiv.org/abs/1609.00004": {"title": "On the initial value of PageRank", "link": "https://arxiv.org/abs/1609.00004", "description": "arXiv:1609.00004v5 Announce Type: replace \nAbstract: Google employs PageRank to rank web pages, determining the order in which search results are presented to users based on their queries. PageRank is primarily utilized for directed networks, although there are instances where it is also applied to undirected networks. In this paper, we have applied PageRank to undirected networks, showing that a vertex's PageRank relies on its initial value, often referred to as an intrinsic, non-network contribution. We have analytically proved that when the initial value of vertices is either proportional to their degrees or set to zero, the PageRank values of the vertices become directly proportional to their degrees. Simulated and empirical data are employed to bolster our research findings. Additionally, we have investigated the impact of initial values on PageRank localization."}, "https://arxiv.org/abs/2304.12751": {"title": "Node Feature Augmentation Vitaminizes Network Alignment", "link": "https://arxiv.org/abs/2304.12751", "description": "arXiv:2304.12751v4 Announce Type: replace \nAbstract: Network alignment (NA) is the task of discovering node correspondences across multiple networks. Although NA methods have achieved remarkable success in a myriad of scenarios, their effectiveness is not without additional information such as prior anchor links and/or node features, which may not always be available due to privacy concerns or access restrictions. To tackle this challenge, we propose Grad-Align+, a novel NA method built upon a recent state-of-the-art NA method, the so-called Grad-Align, that gradually discovers a part of node pairs until all node pairs are found. In designing Grad-Align+, we account for how to augment node features in the sense of performing the NA task and how to design our NA method by maximally exploiting the augmented node features. To achieve this goal, Grad-Align+ consists of three key components: 1) centrality-based node feature augmentation (CNFA), 2) graph neural network (GNN)-aided embedding similarity calculation alongside the augmented node features, and 3) gradual NA with similarity calculation using aligned cross-network neighbor-pairs (ACNs). Through comprehensive experiments, we demonstrate that Grad-Align+ exhibits (a) the superiority over benchmark NA methods, (b) empirical validations as well as our theoretical findings to see the effectiveness of CNFA, (c) the influence of each component, (d) the robustness to network noises, and (e) the computational efficiency."}, "https://arxiv.org/abs/2310.10155": {"title": "Analysis and implementation of nanotargeting on LinkedIn based on publicly available non-PII", "link": "https://arxiv.org/abs/2310.10155", "description": "arXiv:2310.10155v2 Announce Type: replace \nAbstract: The literature has shown that combining a few non-Personal Identifiable Information (non-PII) is enough to make a user unique in a dataset including millions of users. This work demonstrates that a combination of a few non-PII items can be activated to nanotarget users. We demonstrate that the combination of the location and {5} rare ({13} random) skills in a LinkedIn profile is enough to become unique in a user base of {$\\sim$970M} users with a probability of 75\\%. The novelty is that these attributes are publicly accessible to anyone registered on LinkedIn and can be activated through advertising campaigns. We ran an experiment configuring ad campaigns using the location and skills of three of the paper's authors, demonstrating how all the ads using $\\geq13$ skills were delivered exclusively to the targeted user. We reported this vulnerability to LinkedIn, which initially ignored the problem, but fixed it as of November 2023.%This nanotargeting may expose LinkedIn users to privacy and security risks such as malvertising or manipulation."}, "https://arxiv.org/abs/2402.03894": {"title": "Interpersonal trust: Asymptotic analysis of a stochastic coordination game with multi-agent learning", "link": "https://arxiv.org/abs/2402.03894", "description": "arXiv:2402.03894v3 Announce Type: replace \nAbstract: We study the interpersonal trust of a population of agents, asking whether chance may decide if a population ends up in a high trust or low trust state. We model this by a discrete time, random matching stochastic coordination game. Agents are endowed with an exponential smoothing learning rule about the behaviour of their neighbours. We find that, with probability one in the long run the whole population either always cooperates or always defects. By simulation we study the impact of the distributions of the payoffs in the game and of the exponential smoothing learning (memory of the agents). We find, that as the agent memory increases or as the size of the population increases, the actual dynamics start to resemble the expectation of the process. We conclude that it is indeed possible that different populations may converge upon high or low trust between its citizens simply by chance, though the game parameters (context of the society) may be quite telling."}, "https://arxiv.org/abs/2404.01319": {"title": "Information Cascade Prediction under Public Emergencies: A Survey", "link": "https://arxiv.org/abs/2404.01319", "description": "arXiv:2404.01319v2 Announce Type: replace \nAbstract: With the advent of the era of big data, massive information, expert experience, and high-accuracy models bring great opportunities to the information cascade prediction of public emergencies. However, the involvement of specialist knowledge from various disciplines has resulted in a primarily application-specific focus (e.g., earthquakes, floods, infectious diseases) for information cascade prediction of public emergencies. The lack of a unified prediction framework poses a challenge for classifying intersectional prediction methods across different application fields. This survey paper offers a systematic classification and summary of information cascade modeling, prediction, and application. We aim to help researchers identify cutting-edge research and comprehend models and methods of information cascade prediction under public emergencies. By summarizing open issues and outlining future directions in this field, this paper has the potential to be a valuable resource for researchers conducting further studies on predicting information cascades."}, "https://arxiv.org/abs/2311.03682": {"title": "Incentive Design for Eco-driving in Urban Transportation Networks", "link": "https://arxiv.org/abs/2311.03682", "description": "arXiv:2311.03682v2 Announce Type: replace-cross \nAbstract: Eco-driving emerges as a cost-effective and efficient strategy to mitigate greenhouse gas emissions in urban transportation networks. Acknowledging the persuasive influence of incentives in shaping driver behavior, this paper presents the `eco-planner,' a digital platform devised to promote eco-driving practices in urban transportation. At the outset of their trips, users provide the platform with their trip details and travel time preferences, enabling the eco-planner to formulate personalized eco-driving recommendations and corresponding incentives, while adhering to its budgetary constraints. Upon trip completion, incentives are transferred to users who comply with the recommendations and effectively reduce their emissions. By comparing our proposed incentive mechanism with a baseline scheme that offers uniform incentives to all users, we demonstrate that our approach achieves superior emission reductions and increased user compliance with a smaller budget."}, "https://arxiv.org/abs/2404.10228": {"title": "Two-Stage Stance Labeling: User-Hashtag Heuristics with Graph Neural Networks", "link": "https://arxiv.org/abs/2404.10228", "description": "arXiv:2404.10228v2 Announce Type: replace-cross \nAbstract: The high volume and rapid evolution of content on social media present major challenges for studying the stance of social media users. In this work, we develop a two stage stance labeling method that utilizes the user-hashtag bipartite graph and the user-user interaction graph. In the first stage, a simple and efficient heuristic for stance labeling uses the user-hashtag bipartite graph to iteratively update the stance association of user and hashtag nodes via a label propagation mechanism. This set of soft labels is then integrated with the user-user interaction graph to train a graph neural network (GNN) model using semi-supervised learning. We evaluate this method on two large-scale datasets containing tweets related to climate change from June 2021 to June 2022 and gun control from January 2022 to January 2023. Our experiments demonstrate that enriching text-based embeddings of users with network information from the user interaction graph using our semi-supervised GNN method outperforms both classifiers trained on user textual embeddings and zero-shot classification using LLMs such as GPT4. We discuss the need for integrating nuanced understanding from social science with the scalability of computational methods to better understand how polarization on social media occurs for divisive issues such as climate change and gun control."}, "https://arxiv.org/abs/2405.11146": {"title": "Election Polls on Social Media: Prevalence, Biases, and Voter Fraud Beliefs", "link": "https://arxiv.org/abs/2405.11146", "description": "arXiv:2405.11146v1 Announce Type: new \nAbstract: Social media platforms allow users to create polls to gather public opinion on diverse topics. However, we know little about what such polls are used for and how reliable they are, especially in significant contexts like elections. Focusing on the 2020 presidential elections in the U.S., this study shows that outcomes of election polls on Twitter deviate from election results despite their prevalence. Leveraging demographic inference and statistical analysis, we find that Twitter polls are disproportionately authored by older males and exhibit a large bias towards candidate Donald Trump relative to representative mainstream polls. We investigate potential sources of biased outcomes from the point of view of inauthentic, automated, and counter-normative behavior. Using social media experiments and interviews with poll authors, we identify inconsistencies between public vote counts and those privately visible to poll authors, with the gap potentially attributable to purchased votes. We also find that Twitter accounts participating in election polls are more likely to be bots, and election poll outcomes tend to be more biased, before the election day than after. Finally, we identify instances of polls spreading voter fraud conspiracy theories and estimate that a couple thousand of such polls were posted in 2020. The study discusses the implications of biased election polls in the context of transparency and accountability of social media platforms."}, "https://arxiv.org/abs/2405.11166": {"title": "Learning the liveability of cities from migrants: Combinatiorial-Hodge-theory approach", "link": "https://arxiv.org/abs/2405.11166", "description": "arXiv:2405.11166v1 Announce Type: new \nAbstract: Migration is a major decision to leave one place and move to another, and involves job and life changes. The migration flow of people provides relational information across places about which is better to live by ``voting with their feet'' (Tiebout, 1956; Douglas, 1997). From the people's votes, in a ``democratic'' process, we quantify a descriptive statistic of liveable cities by a potential of migration flow in Combinatorial Hodge theory. As a case study, we measure the liveability of municipalities in Japan for specific populations such as families with small children and women of reproductive age. Using these potentials as dependent variables, we perform a regression analysis to identify the factors relevant to liveability. Additionally, using the aformentioned theoretical framework, we analytically derive the expression of the utility as a function of given flow data, which was numerically estimated in previous studies (Douglas & Wall, 1993; Douglas, 1997; Douglas & Wall, 2000; Wall, 2001; Nakajima & Tabuchi, 2011). The proposed method extracts a consistent metric of interval scale from the non-transitive, pairwise comparison between locations and provides valuable statistics for urban planning by policymakers."}, "https://arxiv.org/abs/2405.11225": {"title": "SeBot: Structural Entropy Guided Multi-View Contrastive Learning for Social Bot Detection", "link": "https://arxiv.org/abs/2405.11225", "description": "arXiv:2405.11225v1 Announce Type: new \nAbstract: Recent advancements in social bot detection have been driven by the adoption of Graph Neural Networks. The social graph, constructed from social network interactions, contains benign and bot accounts that influence each other. However, previous graph-based detection methods that follow the transductive message-passing paradigm may not fully utilize hidden graph information and are vulnerable to adversarial bot behavior. The indiscriminate message passing between nodes from different categories and communities results in excessively homogeneous node representations, ultimately reducing the effectiveness of social bot detectors. In this paper, we propose SEBot, a novel multi-view graph-based contrastive learning-enabled social bot detector. In particular, we use structural entropy as an uncertainty metric to optimize the entire graph's structure and subgraph-level granularity, revealing the implicitly existing hierarchical community structure. And we design an encoder to enable message passing beyond the homophily assumption, enhancing robustness to adversarial behaviors of social bots. Finally, we employ multi-view contrastive learning to maximize mutual information between different views and enhance the detection performance through multi-task learning. Experimental results demonstrate that our approach significantly improves the performance of social bot detection compared with SOTA methods."}, "https://arxiv.org/abs/2405.11887": {"title": "Social norm dynamics in a behavioral epidemic model on multiplex networks", "link": "https://arxiv.org/abs/2405.11887", "description": "arXiv:2405.11887v1 Announce Type: new \nAbstract: Understanding the social determinants influencing preventive measures adoption during epidemics is crucial for effective disease modeling and policy making. While traditional epidemic models focused on rational decision-making and psychological biases, recent studies highlight the role of social norms. We develop a behavioral epidemic model on a multiplex network, by integrating an Experience Weighted Attractor (EWA) learning mechanism and social norm dynamics. Incorporating social norms in our decision-making mechanism significantly reduces final infected fractions, offering an alternative to altruism for boosting vaccination coverage. Furthermore, we examine the importance of the dynamics of each one of the social norms, injunctive or descriptive, in reducing the infected fraction, finding that the former has a more significant effect in agreement with some experimental evidence. We also explore the effect of external interventions on epidemic expansion, aiding in refining public communication protocols. Enhanced models of social norm dynamics, if validated and tested, can better capture the complexities of human social behavior and mitigate various societal challenges beyond pandemics."}, "https://arxiv.org/abs/2405.11922": {"title": "Effective Clustering on Large Attributed Bipartite Graphs", "link": "https://arxiv.org/abs/2405.11922", "description": "arXiv:2405.11922v1 Announce Type: new \nAbstract: Attributed bipartite graphs (ABGs) are an expressive data model for describing the interactions between two sets of heterogeneous nodes that are associated with rich attributes, such as customer-product purchase networks and author-paper authorship graphs. Partitioning the target node set in such graphs into k disjoint clusters (referred to as k-ABGC) finds widespread use in various domains, including social network analysis, recommendation systems, information retrieval, and bioinformatics. However, the majority of existing solutions towards k-ABGC either overlook attribute information or fail to capture bipartite graph structures accurately, engendering severely compromised result quality. The severity of these issues is accentuated in real ABGs, which often encompass millions of nodes and a sheer volume of attribute data, rendering effective k-ABGC over such graphs highly challenging.\n  In this paper, we propose TPO, an effective and efficient approach to k-ABGC that achieves superb clustering performance on multiple real datasets. TPO obtains high clustering quality through two major contributions: (i) a novel formulation and transformation of the k-ABGC problem based on multi-scale attribute affinity specialized for capturing attribute affinities between nodes with the consideration of their multi-hop connections in ABGs, and (ii) a highly efficient solver that includes a suite of carefully-crafted optimizations for sidestepping explicit affinity matrix construction and facilitating faster convergence. Extensive experiments, comparing TPO against 19 baselines over 5 real ABGs, showcase the superior clustering quality of TPO measured against ground-truth labels. Moreover, compared to the state of the arts, TPO is often more than 40x faster over both small and large ABGs."}, "https://arxiv.org/abs/2405.12040": {"title": "Reputation Transfer in the Twitter Diaspora", "link": "https://arxiv.org/abs/2405.12040", "description": "arXiv:2405.12040v1 Announce Type: new \nAbstract: Social media platforms have witnessed a dynamic landscape of user migration in recent years, fueled by changes in ownership, policy, and user preferences. This paper explores the phenomenon of user migration from established platforms like X/Twitter to emerging alternatives such as Threads, Mastodon, and Truth Social. Leveraging a large dataset from X/Twitter, we investigate the extent of user departure from X/Twitter and the destinations they migrate to. Additionally, we examine whether a user's reputation on one platform correlates with their reputation on another, shedding light on the transferability of digital reputation across social media ecosystems. Overall, we find that users with a large following on X/Twitter are more likely to migrate to another platform; and that their reputation on X/Twitter is highly correlated with reputations on Threads, but not Mastodon or Truth Social."}, "https://arxiv.org/abs/2405.11121": {"title": "COVID-19's Unequal Toll: An assessment of small business impact disparities with respect to ethnorace in metropolitan areas in the US using mobility data", "link": "https://arxiv.org/abs/2405.11121", "description": "arXiv:2405.11121v1 Announce Type: cross \nAbstract: Early in the pandemic, counties and states implemented a variety of non-pharmacological interventions (NPIs) focused on mobility, such as national lockdowns or work-from-home strategies, as it became clear that restricting movement was essential to containing the epidemic. Due to these restrictions, businesses were severely affected and in particular, small, urban restaurant businesses. In addition to that, COVID-19 has also amplified many of the socioeconomic disparities and systemic racial inequities that exist in our society. The overarching objective of this study was to examine the changes in small urban restaurant visitation patterns following the COVID-19 pandemic and associated mobility restrictions, as well as to uncover potential disparities across different racial/ethnic groups in order to understand inequities in the impact and recovery. Specifically, the two key objectives were: 1) to analyze the overall changes in restaurant visitation patterns in US metropolitan areas during the pandemic compared to a pre-pandemic baseline, and 2) to investigate differences in visitation pattern changes across Census Block Groups with majority Asian, Black, Hispanic, White, and American Indian populations, identifying any disproportionate effects. Using aggregated geolocated cell phone data from SafeGraph, we document the overall changes in small urban restaurant businesses' visitation patterns with respect to racial composition at a granularity of Census Block Groups. Our results show clear indications of reduced visitation patterns after the pandemic, with slow recoveries. Via visualizations and statistical analyses, we show that reductions in visitation patterns were the highest for small urban restaurant businesses in majority Asian neighborhoods."}, "https://arxiv.org/abs/2405.11192": {"title": "BrainStorm @ iREL at SMM4H 2024: Leveraging Translation and Topical Embeddings for Annotation Detection in Tweets", "link": "https://arxiv.org/abs/2405.11192", "description": "arXiv:2405.11192v1 Announce Type: cross \nAbstract: The proliferation of LLMs in various NLP tasks has sparked debates regarding their reliability, particularly in annotation tasks where biases and hallucinations may arise. In this shared task, we address the challenge of distinguishing annotations made by LLMs from those made by human domain experts in the context of COVID-19 symptom detection from tweets in Latin American Spanish. This paper presents BrainStorm @ iREL's approach to the SMM4H 2024 Shared Task, leveraging the inherent topical information in tweets, we propose a novel approach to identify and classify annotations, aiming to enhance the trustworthiness of annotated data."}, "https://arxiv.org/abs/2405.11219": {"title": "Identifying and Aligning Medical Claims Made on Social Media with Medical Evidence", "link": "https://arxiv.org/abs/2405.11219", "description": "arXiv:2405.11219v1 Announce Type: cross \nAbstract: Evidence-based medicine is the practice of making medical decisions that adhere to the latest, and best known evidence at that time. Currently, the best evidence is often found in the form of documents, such as randomized control trials, meta-analyses and systematic reviews. This research focuses on aligning medical claims made on social media platforms with this medical evidence. By doing so, individuals without medical expertise can more effectively assess the veracity of such medical claims. We study three core tasks: identifying medical claims, extracting medical vocabulary from these claims, and retrieving evidence relevant to those identified medical claims. We propose a novel system that can generate synthetic medical claims to aid each of these core tasks. We additionally introduce a novel dataset produced by our synthetic generator that, when applied to these tasks, demonstrates not only a more flexible and holistic approach, but also an improvement in all comparable metrics. We make our dataset, the Expansive Medical Claim Corpus (EMCC), available at https://zenodo.org/records/8321460"}, "https://arxiv.org/abs/2405.11414": {"title": "High-Resolution Agent-Based Modeling of Campus Population Behaviors for Pandemic Response Planning", "link": "https://arxiv.org/abs/2405.11414", "description": "arXiv:2405.11414v1 Announce Type: cross \nAbstract: This paper reports a case study of an application of high-resolution agent-based modeling and simulation to pandemic response planning on a university campus. In the summer of 2020, we were tasked with a COVID-19 pandemic response project to create a detailed behavioral simulation model of the entire campus population at Binghamton University. We conceptualized this problem as an agent migration process on a multilayer transportation network, in which each layer represented a different transportation mode. As no direct data were available about people's behaviors on campus, we collected as much indirect information as possible to inform the agents' behavioral rules. Each agent was assumed to move along the shortest path between two locations within each transportation layer and switch layers at a parking lot or a bus stop, along with several other behavioral assumptions. Using this model, we conducted simulations of the whole campus population behaviors on a typical weekday, involving more than 25,000 agents. We measured the frequency of close social contacts at each spatial location and identified several busy locations and corridors on campus that needed substantial behavioral intervention. Moreover, systematic simulations with varying population density revealed that the effect of population density reduction was nonlinear, and that reducing the population density to 40-45% would be optimal and sufficient to suppress disease spreading on campus. These results were reported to the university administration and utilized in the pandemic response planning, which led to successful outcomes."}, "https://arxiv.org/abs/2405.11658": {"title": "A Starting Point for Dynamic Community Detection with Leiden Algorithm", "link": "https://arxiv.org/abs/2405.11658", "description": "arXiv:2405.11658v1 Announce Type: cross \nAbstract: Many real-world graphs evolve with time. Identifying communities or clusters on such graphs is an important problem. In this technical report, we extend three dynamic approaches, namely, Naive-dynamic (ND), Delta-screening (DS), and Dynamic Frontier (DF), to our multicore implementation of the Leiden algorithm, an algorithm known for its high-quality community detection. Our experiments on a server with a 64-core AMD EPYC-7742 processor demonstrate that ND, DS, and DF Leiden achieve speedups of 1.25x, 1.24x, and 1.37x on large graphs with random batch updates, compared to Static, ND, and DS Leiden, respectively. However, on real-world dynamic graphs, ND Leiden performs the best, being on average 1.14x faster than Static Leiden. We hope our early results serve as a starting point for dynamic approaches to the Leiden algorithm on evolving graphs."}, "https://arxiv.org/abs/2405.11868": {"title": "Towards Graph Contrastive Learning: A Survey and Beyond", "link": "https://arxiv.org/abs/2405.11868", "description": "arXiv:2405.11868v1 Announce Type: cross \nAbstract: In recent years, deep learning on graphs has achieved remarkable success in various domains. However, the reliance on annotated graph data remains a significant bottleneck due to its prohibitive cost and time-intensive nature. To address this challenge, self-supervised learning (SSL) on graphs has gained increasing attention and has made significant progress. SSL enables machine learning models to produce informative representations from unlabeled graph data, reducing the reliance on expensive labeled data. While SSL on graphs has witnessed widespread adoption, one critical component, Graph Contrastive Learning (GCL), has not been thoroughly investigated in the existing literature. Thus, this survey aims to fill this gap by offering a dedicated survey on GCL. We provide a comprehensive overview of the fundamental principles of GCL, including data augmentation strategies, contrastive modes, and contrastive optimization objectives. Furthermore, we explore the extensions of GCL to other aspects of data-efficient graph learning, such as weakly supervised learning, transfer learning, and related scenarios. We also discuss practical applications spanning domains such as drug discovery, genomics analysis, recommender systems, and finally outline the challenges and potential future directions in this field."}, "https://arxiv.org/abs/2405.11911": {"title": "PULL: PU-Learning-based Accurate Link Prediction", "link": "https://arxiv.org/abs/2405.11911", "description": "arXiv:2405.11911v1 Announce Type: cross \nAbstract: Given an edge-incomplete graph, how can we accurately find the missing links? The link prediction in edge-incomplete graphs aims to discover the missing relations between entities when their relationships are represented as a graph. Edge-incomplete graphs are prevalent in real-world due to practical limitations, such as not checking all users when adding friends in a social network. Addressing the problem is crucial for various tasks, including recommending friends in social networks and finding references in citation networks. However, previous approaches rely heavily on the given edge-incomplete (observed) graph, making it challenging to consider the missing (unobserved) links during training. In this paper, we propose PULL (PU-Learning-based Link predictor), an accurate link prediction method based on the positive-unlabeled (PU) learning. PULL treats the observed edges in the training graph as positive examples, and the unconnected node pairs as unlabeled ones. PULL effectively prevents the link predictor from overfitting to the observed graph by proposing latent variables for every edge, and leveraging the expected graph structure with respect to the variables. Extensive experiments on five real-world datasets show that PULL consistently outperforms the baselines for predicting links in edge-incomplete graphs."}, "https://arxiv.org/abs/2405.12023": {"title": "Estimating transmission noise on networks from stationary local order", "link": "https://arxiv.org/abs/2405.12023", "description": "arXiv:2405.12023v1 Announce Type: cross \nAbstract: In this paper we study networks of nodes characterised by binary traits that change both endogenously and through nearest-neighbour interaction. Our analytical results show that those traits can be ranked according to the noisiness of their transmission using only measures of order in the stationary state. Crucially, this ranking is independent of network topology. As an example, we explain why, in line with a long-standing hypothesis, the relative stability of the structural traits of languages can be estimated from their geospatial distribution. We conjecture that similar inferences may be possible in a more general class of Markovian systems. Consequently, in many empirical domains where longitudinal information is not easily available the propensities of traits to change could be estimated from spatial data alone."}, "https://arxiv.org/abs/2405.12180": {"title": "Estimating the Impact of Social Distance Policy in Mitigating COVID-19 Spread with Factor-Based Imputation Approach", "link": "https://arxiv.org/abs/2405.12180", "description": "arXiv:2405.12180v1 Announce Type: cross \nAbstract: We identify the effectiveness of social distancing policies in reducing the transmission of the COVID-19 spread. We build a model that measures the relative frequency and geographic distribution of the virus growth rate and provides hypothetical infection distribution in the states that enacted the social distancing policies, where we control time-varying, observed and unobserved, state-level heterogeneities. Using panel data on infection and deaths in all US states from February 20 to April 20, 2020, we find that stay-at-home orders and other types of social distancing policies significantly reduced the growth rate of infection and deaths. We show that the effects are time-varying and range from the weakest at the beginning of policy intervention to the strongest by the end of our sample period. We also found that social distancing policies were more effective in states with higher income, better education, more white people, more democratic voters, and higher CNN viewership."}, "https://arxiv.org/abs/2308.05945": {"title": "Improving Ego-Cluster for Network Effect Measurement", "link": "https://arxiv.org/abs/2308.05945", "description": "arXiv:2308.05945v2 Announce Type: replace \nAbstract: The network effect, wherein one user's activity impacts another user, is common in social network platforms. Many new features in social networks are specifically designed to create a network effect, enhancing user engagement. For instance, content creators tend to produce more when their articles and posts receive positive feedback from followers. This paper discusses a new cluster-level experimentation methodology for measuring creator-side metrics in the context of A/B experiments. The methodology is designed to address cases where the experiment randomization unit and the metric measurement unit differ. It is a crucial part of LinkedIn's overall strategy to foster a robust creator community and ecosystem. The method is developed based on widely-cited research at LinkedIn but significantly improves the efficiency and flexibility of the clustering algorithm. This improvement results in a stronger capability for measuring creator-side metrics and an increased velocity for creator-related experiments."}, "https://arxiv.org/abs/2401.08680": {"title": "Proximity Ascertainment Bias in Early Covid Case Locations", "link": "https://arxiv.org/abs/2401.08680", "description": "arXiv:2401.08680v5 Announce Type: replace \nAbstract: A comparison of the distances to the Huanan Seafood Market of early Covid cases with known links to the market versus cases without known links shows results apparently incompatible with a location model lacking proximity ascertainment bias. The sign of the difference instead agrees with a model in which such ascertainment bias is large. In the presence of such bias inferences based on the clustering of case locations become unreliable."}, "https://arxiv.org/abs/2007.05637": {"title": "Multilevel Digital Contact Tracing", "link": "https://arxiv.org/abs/2007.05637", "description": "arXiv:2007.05637v4 Announce Type: replace-cross \nAbstract: Digital contact tracing plays a crucial role in alleviating an outbreak, and designing multilevel digital contact tracing for a country is an open problem due to the analysis of large volumes of temporal contact data. We develop a multilevel digital contact tracing framework that constructs dynamic contact graphs from the proximity contact data. Prominently, we introduce the edge label of the contact graph as a binary circular contact queue, which holds the temporal social interactions during the incubation period. After that, our algorithm prepares the direct and indirect (multilevel) contact list for a given set of infected persons from the contact graph. Finally, the algorithm constructs the infection pathways for the trace list. We implement the framework and validate the contact tracing process with synthetic and real-world data sets. In addition, analysis reveals that for COVID-19 close contact parameters, the framework takes reasonable space and time to create the infection pathways. Our framework can apply to any epidemic spreading by changing the algorithm's parameters."}, "https://arxiv.org/abs/2305.14375": {"title": "MGL2Rank: Learning to Rank the Importance of Nodes in Road Networks Based on Multi-Graph Fusion", "link": "https://arxiv.org/abs/2305.14375", "description": "arXiv:2305.14375v3 Announce Type: replace-cross \nAbstract: The identification of important nodes with strong propagation capabilities in road networks is a vital topic in urban planning. Existing methods for evaluating the importance of nodes in traffic networks only consider topological information and traffic volumes, the diversity of the traffic characteristics in road networks, such as the number of lanes and average speed of road segments, is ignored, thus limiting their performance. To solve this problem, we propose a graph learning-based framework (MGL2Rank) that integrates the rich characteristics of road networks to rank the importance of nodes. This framework comprises an embedding module containing a sampling algorithm (MGWalk) and an encoder network to learn the latent representations for each road segment. MGWalk utilizes multigraph fusion to capture the topology of road networks and establish associations between road segments based on their attributes. The obtained node representation is then used to learn the importance ranking of the road segments. Finally, a synthetic dataset is constructed for ranking tasks based on the regional road network of Shenyang City, and the ranking results on this dataset demonstrate the effectiveness of our method. The data and source code for MGL2Rank are available at https://github.com/iCityLab/MGL2Rank."}, "https://arxiv.org/abs/2404.14423": {"title": "A Compositional Approach to Higher-Order Structure in Complex Systems", "link": "https://arxiv.org/abs/2404.14423", "description": "arXiv:2404.14423v2 Announce Type: replace-cross \nAbstract: Relating microscopic interactions to macroscopic observables is a central challenge in the study of complex systems. Addressing this question requires understanding both pairwise and higher-order interactions, but the latter are less well understood. Here, we show that the M\\\"obius inversion theorem provides a general mathematical formalism for deriving higher-order interactions from macroscopic observables, relative to a chosen decomposition of the system into parts. Applying this framework to a diverse range of systems, we demonstrate that many existing notions of higher-order interactions, from epistasis in genetics and many-body couplings in physics, to synergy in game theory and artificial intelligence, naturally arise from an appropriate mereological decomposition. By revealing the common mathematical structure underlying seemingly disparate phenomena, our work highlights the fundamental role of decomposition choice in the definition and estimation of higher-order interactions. We discuss how this unifying perspective can facilitate the transfer of insights between domains, guide the selection of appropriate system decompositions, and motivate the search for novel interaction types through creative decomposition strategies. More broadly, our results suggest that the M\\\"obius inversion theorem provides a powerful lens for understanding the emergence of complex behaviour from the interplay of microscopic parts, with applications across a wide range of disciplines."}, "https://arxiv.org/abs/2405.12244": {"title": "Real-Time Go-Around Prediction: A case study of JFK airport", "link": "https://arxiv.org/abs/2405.12244", "description": "arXiv:2405.12244v1 Announce Type: new \nAbstract: In this paper, we employ the long-short-term memory model (LSTM) to predict the real-time go-around probability as an arrival flight is approaching JFK airport and within 10 nm of the landing runway threshold. We further develop methods to examine the causes to go-around occurrences both from a global view and an individual flight perspective. According to our results, in-trail spacing, and simultaneous runway operation appear to be the top factors that contribute to overall go-around occurrences. We then integrate these pre-trained models and analyses with real-time data streaming, and finally develop a demo web-based user interface that integrates the different components designed previously into a real-time tool that can eventually be used by flight crews and other line personnel to identify situations in which there is a high risk of a go-around."}, "https://arxiv.org/abs/2405.12253": {"title": "The statistical and dynamic modeling of the first part of the 2013-2014 Euromaidan protests in Ukraine: The Revolution of Dignity and preceding times", "link": "https://arxiv.org/abs/2405.12253", "description": "arXiv:2405.12253v1 Announce Type: new \nAbstract: Ukraine's tug-of-war between Russia and the West has had significant and lasting consequences for the country. In 2013, Viktor Yanukovych, the Ukrainian president aligned with Russia, opted against signing an association agreement with the European Union. This agreement aimed to facilitate trade and travel between the EU and Ukraine. This decision sparked widespread protests that coalesced in Kyiv's Maidan Square, eventually becoming known as the Euromaidan protests. In this study, we analyze the protest data from 2013, sourced from Ukraine's Center for Social and Labor Research. Despite the dataset's limitations and occasional inconsistencies, we demonstrate the extraction of valuable insights and the construction of a descriptive model from such data. Our investigation reveals a pre-existing state of self-excitation within the system even before the onset of the Euromaidan protests. This self-excitation intensified during the Euromaidan protests. A statistical analysis indicates that the government's utilization of force correlates with increased future protests, exacerbating rather than quelling the protest movement. Furthermore, we introduce the implementation of Hawkes process models to comprehend the spatiotemporal dynamics of the protest activity. Our findings highlight that, while protest activities spread across the entire country, the driving force behind the dynamics of these protests was the level of activity in Kyiv. Furthermore, in contrast to prior research that emphasized geographical proximity as a key predictor of event propagation, our study illustrates that the political alignment among oblasts, which are the distinct municipalities comprising Ukraine, had a more profound impact than mere geographic distance. This underscores the significance of social and cultural factors in molding the trajectory of political movements."}, "https://arxiv.org/abs/2405.12566": {"title": "Unveiling Online Conspiracy Theorists: a Text-Based Approach and Characterization", "link": "https://arxiv.org/abs/2405.12566", "description": "arXiv:2405.12566v1 Announce Type: new \nAbstract: In today's digital landscape, the proliferation of conspiracy theories within the disinformation ecosystem of online platforms represents a growing concern. This paper delves into the complexities of this phenomenon. We conducted a comprehensive analysis of two distinct X (formerly known as Twitter) datasets: one comprising users with conspiracy theorizing patterns and another made of users lacking such tendencies and thus serving as a control group. The distinguishing factors between these two groups are explored across three dimensions: emotions, idioms, and linguistic features. Our findings reveal marked differences in the lexicon and language adopted by conspiracy theorists with respect to other users. We developed a machine learning classifier capable of identifying users who propagate conspiracy theories based on a rich set of 871 features. The results demonstrate high accuracy, with an average F1 score of 0.88. Moreover, this paper unveils the most discriminating characteristics that define conspiracy theory propagators."}, "https://arxiv.org/abs/2405.12642": {"title": "Combining Twitter and Mobile Phone Data to Observe Border-Rush: The Turkish-European Border Opening", "link": "https://arxiv.org/abs/2405.12642", "description": "arXiv:2405.12642v1 Announce Type: new \nAbstract: Following Turkey's 2020 decision to revoke border controls, many individuals journeyed towards the Greek, Bulgarian, and Turkish borders. However, the lack of verifiable statistics on irregular migration and discrepancies between media reports and actual migration patterns require further exploration. The objective of this study is to bridge this knowledge gap by harnessing novel data sources, specifically mobile phone and Twitter data, to construct estimators of cross-border mobility and to cultivate a qualitative comprehension of the unfolding events. By employing a migration diplomacy framework, we analyse emergent mobility patterns at the border. Our findings demonstrate the potential of mobile phone data for quantitative metrics and Twitter data for qualitative understanding. We underscore the ethical implications of leveraging Big Data, particularly considering the vulnerability of the population under study. This underscores the imperative for exhaustive research into the socio-political facets of human mobility, with the aim of discerning the potentialities, limitations, and risks inherent in these data sources and their integration. This scholarly endeavour contributes to a more nuanced understanding of migration dynamics and paves the way for the formulation of regulations that preclude misuse and oppressive surveillance, thereby ensuring a more accurate representation of migration realities."}, "https://arxiv.org/abs/2405.12764": {"title": "Detecting and Mitigating Bias in Algorithms Used to Disseminate Information in Social Networks", "link": "https://arxiv.org/abs/2405.12764", "description": "arXiv:2405.12764v1 Announce Type: new \nAbstract: Social connections are a conduit through which individuals communicate, information propagates, and diseases spread. Identifying individuals that are more likely to adopt ideas or technologies and spread them to others is essential in order to develop effective information campaigns, fight epidemics, and to maximize the reach of limited resources. Consequently a lot of work has focused on identifying sets of influencers. Here we show that seeding information using these influence maximization methods, only benefits connected and central individuals, consistently leaving the most vulnerable behind. Our results highlights troublesome outcomes of influence maximization algorithms: they do not disseminate information in an equitable manner threatening to create an increasingly unequal society. To overcome this issue we devise a simple, multi-objective algorithm, which maximises both influence and information equity. Our work demonstrates how to find fairer influencer sets, highlighting that in our search for maximizing information, we do not need to compromise on information equality."}, "https://arxiv.org/abs/2405.12797": {"title": "Refined Graph Encoder Embedding via Self-Training and Latent Community Recovery", "link": "https://arxiv.org/abs/2405.12797", "description": "arXiv:2405.12797v1 Announce Type: new \nAbstract: This paper introduces a refined graph encoder embedding method, enhancing the original graph encoder embedding using linear transformation, self-training, and hidden community recovery within observed communities. We provide the theoretical rationale for the refinement procedure, demonstrating how and why our proposed method can effectively identify useful hidden communities via stochastic block models, and how the refinement method leads to improved vertex embedding and better decision boundaries for subsequent vertex classification. The efficacy of our approach is validated through a collection of simulated and real-world graph data."}, "https://arxiv.org/abs/2405.12340": {"title": "Cascade-based Randomization for Inferring Causal Effects under Diffusion Interference", "link": "https://arxiv.org/abs/2405.12340", "description": "arXiv:2405.12340v1 Announce Type: cross \nAbstract: The presence of interference, where the outcome of an individual may depend on the treatment assignment and behavior of neighboring nodes, can lead to biased causal effect estimation. Current approaches to network experiment design focus on limiting interference through cluster-based randomization, in which clusters are identified using graph clustering, and cluster randomization dictates the node assignment to treatment and control. However, cluster-based randomization approaches perform poorly when interference propagates in cascades, whereby the response of individuals to treatment propagates to their multi-hop neighbors. When we have knowledge of the cascade seed nodes, we can leverage this interference structure to mitigate the resulting causal effect estimation bias. With this goal, we propose a cascade-based network experiment design that initiates treatment assignment from the cascade seed node and propagates the assignment to their multi-hop neighbors to limit interference during cascade growth and thereby reduce the overall causal effect estimation error. Our extensive experiments on real-world and synthetic datasets demonstrate that our proposed framework outperforms the existing state-of-the-art approaches in estimating causal effects in network data."}, "https://arxiv.org/abs/2405.12474": {"title": "How Universal Polynomial Bases Enhance Spectral Graph Neural Networks: Heterophily, Over-smoothing, and Over-squashing", "link": "https://arxiv.org/abs/2405.12474", "description": "arXiv:2405.12474v1 Announce Type: cross \nAbstract: Spectral Graph Neural Networks (GNNs), alternatively known as graph filters, have gained increasing prevalence for heterophily graphs. Optimal graph filters rely on Laplacian eigendecomposition for Fourier transform. In an attempt to avert prohibitive computations, numerous polynomial filters have been proposed. However, polynomials in the majority of these filters are predefined and remain fixed across different graphs, failing to accommodate the varying degrees of heterophily. Addressing this gap, we demystify the intrinsic correlation between the spectral property of desired polynomial bases and the heterophily degrees via thorough theoretical analyses. Subsequently, we develop a novel adaptive heterophily basis wherein the basis vectors mutually form angles reflecting the heterophily degree of the graph. We integrate this heterophily basis with the homophily basis to construct a universal polynomial basis UniBasis, which devises a polynomial filter based graph neural network - UniFilter. It optimizes the convolution and propagation in GNN, thus effectively limiting over-smoothing and alleviating over-squashing. Our extensive experiments, conducted on a diverse range of real-world and synthetic datasets with varying degrees of heterophily, support the superiority of UniFilter. These results not only demonstrate the universality of UniBasis but also highlight its proficiency in graph explanation."}, "https://arxiv.org/abs/2311.09262": {"title": "Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values", "link": "https://arxiv.org/abs/2311.09262", "description": "arXiv:2311.09262v3 Announce Type: replace \nAbstract: The potential impact of an academic paper is determined by various factors, including its popularity and contribution. Existing models usually estimate original citation counts based on static graphs and fail to differentiate values from nuanced perspectives. In this study, we propose a novel graph neural network to Disentangle the Potential impacts of Papers into Diffusion, Conformity, and Contribution values (called DPPDCC). Given a target paper, DPPDCC encodes temporal and structural features within the constructed dynamic heterogeneous graph. Particularly, to capture the knowledge flow, we emphasize the importance of comparative and co-cited/citing information between papers and aggregate snapshots evolutionarily. To unravel popularity, we contrast augmented graphs to extract the essence of diffusion and predict the accumulated citation binning to model conformity. We further apply orthogonal constraints to encourage distinct modeling of each perspective and preserve the inherent value of contribution. To evaluate models' generalization for papers published at various times, we reformulate the problem by partitioning data based on specific time points to mirror real-world conditions. Extensive experimental results on three datasets demonstrate that DPPDCC significantly outperforms baselines for previously, freshly, and immediately published papers. Further analyses confirm its robust capabilities. We will make our datasets and codes publicly available."}, "https://arxiv.org/abs/2401.09310": {"title": "Asymmetric games on networks: mapping to Ising models and bounded rationality", "link": "https://arxiv.org/abs/2401.09310", "description": "arXiv:2401.09310v2 Announce Type: replace \nAbstract: We investigate the dynamics of coordination and consensus in an agent population. Considering agents endowed with bounded rationality, we study asymmetric coordination games using a mapping to random field Ising models. In doing so, we investigate the relationship between group coordination and agent rationality. Analytical calculations and numerical simulations of the proposed model lead to novel insight into opinion dynamics. For instance, we find that bounded rationality and preference intensity can determine a series of possible scenarios with different levels of opinion polarization. To conclude, we deem our investigation opens a new avenue for studying game dynamics through methods of statistical physics."}, "https://arxiv.org/abs/2403.02867": {"title": "Scalable Continuous-time Diffusion Framework for Network Inference and Influence Estimation", "link": "https://arxiv.org/abs/2403.02867", "description": "arXiv:2403.02867v2 Announce Type: replace \nAbstract: The study of continuous-time information diffusion has been an important area of research for many applications in recent years. When only the diffusion traces (cascades) are accessible, cascade-based network inference and influence estimation are two essential problems to explore. Alas, existing methods exhibit limited capability to infer and process networks with more than a few thousand nodes, suffering from scalability issues. In this paper, we view the diffusion process as a continuous-time dynamical system, based on which we establish a continuous-time diffusion model. Subsequently, we instantiate the model to a scalable and effective framework (FIM) to approximate the diffusion propagation from available cascades, thereby inferring the underlying network structure. Furthermore, we undertake an analysis of the approximation error of FIM for network inference. To achieve the desired scalability for influence estimation, we devise an advanced sampling technique and significantly boost the efficiency. We also quantify the effect of the approximation error on influence estimation theoretically. Experimental results showcase the effectiveness and superior scalability of FIM on network inference and influence estimation."}}