{"https://arxiv.org/abs/2405.05275": {"title": "SoMeR: Multi-View User Representation Learning for Social Media", "link": "https://arxiv.org/abs/2405.05275", "description": "arXiv:2405.05275v1 Announce Type: new \nAbstract: User representation learning aims to capture user preferences, interests, and behaviors in low-dimensional vector representations. These representations have widespread applications in recommendation systems and advertising; however, existing methods typically rely on specific features like text content, activity patterns, or platform metadata, failing to holistically model user behavior across different modalities. To address this limitation, we propose SoMeR, a Social Media user Representation learning framework that incorporates temporal activities, text content, profile information, and network interactions to learn comprehensive user portraits. SoMeR encodes user post streams as sequences of timestamped textual features, uses transformers to embed this along with profile data, and jointly trains with link prediction and contrastive learning objectives to capture user similarity. We demonstrate SoMeR's versatility through two applications: 1) Identifying inauthentic accounts involved in coordinated influence operations by detecting users posting similar content simultaneously, and 2) Measuring increased polarization in online discussions after major events by quantifying how users with different beliefs moved farther apart in the embedding space. SoMeR's ability to holistically model users enables new solutions to important problems around disinformation, societal tensions, and online behavior understanding."}, "https://arxiv.org/abs/2405.05288": {"title": "Learning Social Graph for Inactive User Recommendation", "link": "https://arxiv.org/abs/2405.05288", "description": "arXiv:2405.05288v1 Announce Type: new \nAbstract: Social relations have been widely incorporated into recommender systems to alleviate data sparsity problem. However, raw social relations don't always benefit recommendation due to their inferior quality and insufficient quantity, especially for inactive users, whose interacted items are limited. In this paper, we propose a novel social recommendation method called LSIR (\\textbf{L}earning \\textbf{S}ocial Graph for \\textbf{I}nactive User \\textbf{R}ecommendation) that learns an optimal social graph structure for social recommendation, especially for inactive users. LSIR recursively aggregates user and item embeddings to collaboratively encode item and user features. Then, graph structure learning (GSL) is employed to refine the raw user-user social graph, by removing noisy edges and adding new edges based on the enhanced embeddings. Meanwhile, mimic learning is implemented to guide active users in mimicking inactive users during model training, which improves the construction of new edges for inactive users. Extensive experiments on real-world datasets demonstrate that LSIR achieves significant improvements of up to 129.58\\% on NDCG in inactive user recommendation. Our code is available at~\\url{https://github.com/liun-online/LSIR}."}, "https://arxiv.org/abs/2405.05393": {"title": "Mutual information and the encoding of contingency tables", "link": "https://arxiv.org/abs/2405.05393", "description": "arXiv:2405.05393v1 Announce Type: new \nAbstract: Mutual information is commonly used as a measure of similarity between competing labelings of a given set of objects, for example to quantify performance in classification and community detection tasks. As argued recently, however, the mutual information as conventionally defined can return biased results because it neglects the information cost of the so-called contingency table, a crucial component of the similarity calculation. In principle the bias can be rectified by subtracting the appropriate information cost, leading to the modified measure known as the reduced mutual information, but in practice one can only ever compute an upper bound on this information cost, and the value of the reduced mutual information depends crucially on how good a bound is established. In this paper we describe an improved method for encoding contingency tables that gives a substantially better bound in typical use cases, and approaches the ideal value in the common case where the labelings are closely similar, as we demonstrate with extensive numerical results."}, "https://arxiv.org/abs/2405.05400": {"title": "Comparative analysis of graph randomization: Tools,methods, pitfalls, and best practices", "link": "https://arxiv.org/abs/2405.05400", "description": "arXiv:2405.05400v1 Announce Type: new \nAbstract: Graph randomization techniques play a crucial role in network analysis, allowing researchers to assess the statistical significance of observed network properties and distinguish meaningful patterns from random fluctuations. In this survey we provide an overview of the graph randomization methods available in the most popular software tools for network analysis. We propose a comparative analysis of popular software tools to highlight their functionalities and limitations. Through case studies involving diverse graph types, we demonstrate how different randomization methods can lead to divergent conclusions, emphasizing the importance of careful method selection based on the characteristics of the observed network and the research question at hand. This survey proposes some guidelines for researchers and practitioners seeking to understand and utilize graph randomization techniques effectively in their network analysis projects."}, "https://arxiv.org/abs/2405.05576": {"title": "LayerPlexRank: Exploring Node Centrality and Layer Influence through Algebraic Connectivity in Multiplex Networks", "link": "https://arxiv.org/abs/2405.05576", "description": "arXiv:2405.05576v1 Announce Type: new \nAbstract: As the calculation of centrality in complex networks becomes increasingly vital across technological, biological, and social systems, precise and scalable ranking methods are essential for understanding these networks. This paper introduces LayerPlexRank, an algorithm that simultaneously assesses node centrality and layer influence in multiplex networks using algebraic connectivity metrics. This method enhances the robustness of the ranking algorithm by effectively assessing structural changes across layers using random walk, considering the overall connectivity of the graph. We substantiate the utility of LayerPlexRank with theoretical analyses and empirical validations on varied real-world datasets, contrasting it with established centrality measures."}, "https://arxiv.org/abs/2405.05724": {"title": "Private Online Community Detection for Censored Block Models", "link": "https://arxiv.org/abs/2405.05724", "description": "arXiv:2405.05724v1 Announce Type: new \nAbstract: We study the private online change detection problem for dynamic communities, using a censored block model (CBM). Focusing on the notion of edge differential privacy (DP), we seek to understand the fundamental tradeoffs between the privacy budget, detection delay, and exact community recovery of community labels. We establish the theoretical lower bound on the delay in detecting changes privately and propose an algorithm capable of identifying changes in the community structure, while maintaining user privacy. Further, we provide theoretical guarantees for the effectiveness of our proposed method by showing necessary and sufficient conditions on change detection and exact recovery under edge DP. Simulation and real data examples are provided to validate the proposed method."}, "https://arxiv.org/abs/2405.05903": {"title": "The Other Side of the Coin: Recipient Norms and Their Impact on Indirect Reciprocity and Cooperation", "link": "https://arxiv.org/abs/2405.05903", "description": "arXiv:2405.05903v1 Announce Type: new \nAbstract: Human cooperation depends on indirect reciprocity. In this work, we explore the concept of indirect reciprocity using a donation game in an infinitely large population. In particular, we examine how updating the reputations of recipients influences cooperation. Our work adds a time-scale parameter for updating donor and recipient reputations. We find a trade-off between the level of cooperation and evolutionary stability influenced by social norms. `Forgiving' recipient norms enhance cooperation but increase susceptibility to defectors, whereas `unforgiving' norms reduce cooperation but defend against invasion by defectors. Expanding to include gossip groups allows us to analyze the evolutionary dynamics of the time-scale parameter, identifying `generous' norms that support cooperation, and `strict' norms that discourage such generosity, ultimately showing vulnerability to defector invasions and potential cooperation collapse."}, "https://arxiv.org/abs/2405.05433": {"title": "Robust Reward Placement under Uncertainty", "link": "https://arxiv.org/abs/2405.05433", "description": "arXiv:2405.05433v1 Announce Type: cross \nAbstract: Reward placement is a common optimization problem in network diffusion processes, where a number of rewards are to be placed in a network so as to maximize the total reward obtained as agents move randomly in it. In many settings, the precise mobility network might be one of several possible, based on parameters outside our control, such as the weather conditions affecting peoples' transportation means. Solutions to the reward placement problem must thus be robust to this uncertainty, by achieving a high utility in all possible networks. To study such scenarios, we introduce the Robust Reward Placement problem (RRP). Agents move randomly on a Markovian Mobility Model that has a predetermined set of locations but its precise connectivity is unknown and chosen adversarialy from a known set $\\Pi$ of candidates. Network optimization is achieved by selecting a set of reward states, and the goal is to maximize the minimum, among all candidates, ratio of rewards obtained over the optimal solution for each candidate. We first prove that RRP is NP-hard and inapproximable in general. We then develop $\\Psi$-Saturate, a pseudo-polynomial time algorithm that achieves an $\\epsilon$-additive approximation by exceeding the budget constraint by a factor that scales as $O(ln|\\Pi|/\\epsilon)$. In addition, we present several heuristics, most prominently one inspired from a dynamic programming algorithm for the max-min 0-1 Knapsack problem. We corroborate our theoretical findings with an experimental evaluation of the methods in both synthetic and real-world datasets."}, "https://arxiv.org/abs/2405.05487": {"title": "Design of Targeted Community-Based Resource Allocation in the Presence of Vaccine Hesitancy via a Data-Driven Compartmental Stochastic Optimization Model", "link": "https://arxiv.org/abs/2405.05487", "description": "arXiv:2405.05487v1 Announce Type: cross \nAbstract: Vaccines have proven effective in mitigating the threat of severe infections and deaths during outbreaks of infectious diseases. However, vaccine hesitancy (VH) complicates disease spread prediction and healthcare resource assessment across regions and populations. We propose a modeling framework that integrates an epidemiological compartmental model that captures the spread of an infectious disease within a multi-stage stochastic program (MSP) that determines the allocation of critical resources under uncertainty. The proposed compartmental MSP model adaptively manages the allocation of resources to account for changes in population behavior toward vaccines (i.e., variability in VH), the unique patterns of disease spread, and the availability of healthcare resources over time and space. The compartmental MSP model allowed us to analyze the price of fairness in resource allocation. Using real COVID-19 vaccination and healthcare resource data from Arkansas, U.S. (January-May 2021), our findings include: (i) delaying the initial deployment of additional ventilators by one month could lead to an average increase in the expected number of deaths by 285.41/month, highlighting the importance of prompt action; (ii) each additional ventilator in the initial stockpile and in supply leads to a decrease in the expected number of deaths by 1.09/month and 0.962/month, respectively, emphasizing the importance of maintaining a large stockpile and scalable production response; (iii) the cost of ensuring equitable resource allocation varies over time and location, peaking during the peak of a disease outbreak and in densely populated areas. This study emphasizes the importance of flexible, informed public health decision-making and preparedness, providing a model for effective resource allocation in public health emergencies."}, "https://arxiv.org/abs/2211.06352": {"title": "Spectral Triadic Decompositions of Real-World Networks", "link": "https://arxiv.org/abs/2211.06352", "description": "arXiv:2211.06352v3 Announce Type: replace \nAbstract: A fundamental problem in mathematics and network analysis is to find conditions under which a graph can be partitioned into smaller pieces. The most important tool for this partitioning is the Fiedler vector or discrete Cheeger inequality. These results relate the graph spectrum (eigenvalues of the normalized adjacency matrix) to the ability to break a graph into two pieces, with few edge deletions. An entire subfield of mathematics, called spectral graph theory, has emerged from these results. Yet these results do not say anything about the rich community structure exhibited by real-world networks, which typically have a significant fraction of edges contained in numerous densely clustered blocks. Inspired by the properties of real-world networks, we discover a new spectral condition that relates eigenvalue powers to a network decomposition into densely clustered blocks. We call this the \\emph{spectral triadic decomposition}. Our relationship exactly predicts the existence of community structure, as commonly seen in real networked data. Our proof provides an efficient algorithm to produce the spectral triadic decomposition. We observe on numerous social, coauthorship, and citation network datasets that these decompositions have significant correlation with semantically meaningful communities."}, "https://arxiv.org/abs/2301.06774": {"title": "Temporal Dynamics of Coordinated Online Behavior: Stability, Archetypes, and Influence", "link": "https://arxiv.org/abs/2301.06774", "description": "arXiv:2301.06774v2 Announce Type: replace \nAbstract: Large-scale online campaigns, malicious or otherwise, require a significant degree of coordination among participants, which sparked interest in the study of coordinated online behavior. State-of-the-art methods for detecting coordinated behavior perform static analyses, disregarding the temporal dynamics of coordination. Here, we carry out the first dynamic analysis of coordinated behavior. To reach our goal we build a multiplex temporal network and we perform dynamic community detection to identify groups of users that exhibited coordinated behaviors in time. Thanks to our novel approach we find that: (i) coordinated communities feature variable degrees of temporal instability; (ii) dynamic analyses are needed to account for such instability, and results of static analyses can be unreliable and scarcely representative of unstable communities; (iii) some users exhibit distinct archetypal behaviors that have important practical implications; (iv) content and network characteristics contribute to explaining why users leave and join coordinated communities. Our results demonstrate the advantages of dynamic analyses and open up new directions of research on the unfolding of online debates, on the strategies of coordinated communities, and on the patterns of online influence."}, "https://arxiv.org/abs/2303.03774": {"title": "Network science meets history", "link": "https://arxiv.org/abs/2303.03774", "description": "arXiv:2303.03774v3 Announce Type: replace \nAbstract: Alliances and conflicts represent important features of complex systems like international relations. Such relations create a time-evolving signed network, where each node contributes in a unique manner to the global balance of the system. Therefore, a local index mathematically quantifying such a property becomes valuable. In this work, we introduce a local balance index for signed networks. We analyze its mathematical foundations and unique structural properties, differentiating it from existing local vertex invariants. We also establish a novel methodology linking changes in a nation's local balance to historical events. By scrutinizing the time series of local balance for countries between 1816 and 2014, we detect and categorize major historic events based on balance fluctuations. This approach harmonizes quantitative and qualitative analyses, and combined with the theory of \"balance of power\" is able to build up a new mixed approach to history based on network theory."}, "https://arxiv.org/abs/2312.07077": {"title": "On the Potential of an Independent Avatar to Augment Metaverse Social Networks", "link": "https://arxiv.org/abs/2312.07077", "description": "arXiv:2312.07077v2 Announce Type: replace \nAbstract: We present a computational modelling approach which targets capturing the specifics on how to virtually augment a Metaverse user's available social time capacity via using an independent and autonomous version of her digital representation in the Metaverse. We motivate why this is a fundamental building block to model large-scale social networks in the Metaverse, and emerging properties herein. We envision a Metaverse-focused extension of the traditional avatar concept: An avatar can be as well programmed to operate independently when its user is not controlling it directly, thus turning it into an agent-based digital human representation. This way, we highlight how such an independent avatar could help its user to better navigate their social relationships and optimize their socializing time in the Metaverse by (partly) offloading some interactions to the avatar. We model the setting and identify the characteristic variables by using selected concepts from social sciences: ego networks, social presence, and social cues. Then, we formulate the problem of maximizing the user's non-avatar-mediated spare time as a linear optimization. Finally, we analyze the feasible region of the problem and we present some initial insights on the spare time that can be achieved for different parameter values of the avatar-mediated interactions."}, "https://arxiv.org/abs/2402.05739": {"title": "Critical mobility in policy making for epidemic containment", "link": "https://arxiv.org/abs/2402.05739", "description": "arXiv:2402.05739v2 Announce Type: replace \nAbstract: When considering airborne epidemic spreading in social systems, a natural connection arises between mobility and epidemic contacts. As individuals travel, possibilities to encounter new people either at the final destination or during the transportation process appear. Such contacts can lead to new contagion events. In fact, mobility has been a crucial target for early non-pharmaceutical containment measures against the recent COVID-19 pandemic, with a degree of intensity ranging from public transportation line closures to regional, city or even home confinements. Nonetheless, quantitative knowledge on the relationship between mobility-contagions and, consequently, on the efficiency of containment measures remains elusive. Here we introduce an agent-based model with a simple interaction between mobility and contacts. Despite its simplicity our model shows the emergence of a critical mobility level, inducing major outbreaks when surpassed. We explore the interplay between mobility restrictions and the infection in recent intervention policies seen across many countries, and how interventions in the form of closures triggered by incidence rates can guide the epidemic into an oscillatory regime with recurrent waves. We consider how the different interventions impact societal well-being, the economy and the population. Finally, we propose a mitigation framework based on the critical nature of mobility in an epidemic, able to suppress incidence and oscillations at will, preventing extreme incidence peaks with potential to saturate health care resources."}, "https://arxiv.org/abs/2404.12178": {"title": "Designing a sector-coupled European energy system robust to 60 years of historical weather data", "link": "https://arxiv.org/abs/2404.12178", "description": "arXiv:2404.12178v2 Announce Type: replace \nAbstract: As energy systems transform to rely on renewable energy and electrification, they encounter stronger year-to-year variability in energy supply and demand. However, most infrastructure planning is based on a single weather year, resulting in a lack of robustness. In this paper, we optimize energy infrastructure for a European energy system designed for net-zero CO$_2$ emissions in 62 different weather years. Subsequently, we fix the capacity layouts and simulate their operation in every weather year, to evaluate resource adequacy and CO$_2$ emissions abatement. We show that interannual weather variability causes variation of $\\pm$10\\% in total system cost. The most expensive capacity layout obtains the lowest net CO$_2$ emissions but not the highest resource adequacy. Instead, capacity layouts designed with years including compound weather events result in a more robust and cost-effective design. Deploying CO$_2$-emitting backup generation is a cost-effective robustness measure, which only increase CO$_2$ emissions marginally as the average CO$_2$ emissions remain less than 1\\% of 1990 levels. Our findings highlight how extreme weather years drive investments in robustness measures, making them compatible with all weather conditions within six decades of historical weather data."}, "https://arxiv.org/abs/2011.08069": {"title": "Reconciling Security and Utility in Next-Generation Epidemic Risk Mitigation Systems", "link": "https://arxiv.org/abs/2011.08069", "description": "arXiv:2011.08069v3 Announce Type: replace-cross \nAbstract: Epidemics like the recent COVID-19 require proactive contact tracing and epidemiological analysis to predict and subsequently contain infection transmissions. The proactive measures require large scale data collection, which simultaneously raise concerns regarding users' privacy. Digital contact tracing systems developed in response to COVID-19 either collected extensive data for effective analytics at the cost of users' privacy or collected minimal data for the sake of user privacy but were ineffective in predicting and mitigating the epidemic risks. We present Silmarillion--in preparation for future epidemics--a system that reconciles user's privacy with rich data collection for higher utility. In Silmarillion, user devices record Bluetooth encounters with beacons installed in strategic locations. The beacons further enrich the encounters with geo-location, location type, and environment conditions at the beacon installation site. This enriched information enables detailed scientific analysis of disease parameters as well as more accurate personalized exposure risk notification. At the same time, Silmarillion provides privacy to all participants and non-participants at the same level as that guaranteed in digital and manual contact tracing. We describe the design of Silmarillion and its communication protocols that ensure user privacy and data security. We also evaluate a prototype of Silmarillion built using low-end IoT boards, showing that the power consumption and user latencies are adequately low for a practical deployment. Finally, we briefly report on a small-scale deployment within a university building as a proof-of-concept."}, "https://arxiv.org/abs/2203.07678": {"title": "Incorporating Heterophily into Graph Neural Networks for Graph Classification", "link": "https://arxiv.org/abs/2203.07678", "description": "arXiv:2203.07678v2 Announce Type: replace-cross \nAbstract: Graph Neural Networks (GNNs) often assume strong homophily for graph classification, seldom considering heterophily, which means connected nodes tend to have different class labels and dissimilar features. In real-world scenarios, graphs may have nodes that exhibit both homophily and heterophily. Failing to generalize to this setting makes many GNNs underperform in graph classification. In this paper, we address this limitation by identifying three effective designs and develop a novel GNN architecture called IHGNN (short for Incorporating Heterophily into Graph Neural Networks). These designs include the combination of integration and separation of the ego- and neighbor-embeddings of nodes, adaptive aggregation of node embeddings from different layers, and differentiation between different node embeddings for constructing the graph-level readout function. We empirically validate IHGNN on various graph datasets and demonstrate that it outperforms the state-of-the-art GNNs for graph classification."}, "https://arxiv.org/abs/2308.13604": {"title": "Network science Ising states of matter", "link": "https://arxiv.org/abs/2308.13604", "description": "arXiv:2308.13604v3 Announce Type: replace-cross \nAbstract: Network science provides very powerful tools for extracting information from interacting data. Although recently the unsupervised detection of phases of matter using machine learning has raised significant interest, the full prediction power of network science has not yet been systematically explored in this context. Here we fill this gap by providing an in-depth statistical, combinatorial, geometrical and topological characterization of 2D Ising snapshot networks (IsingNets) extracted from Monte Carlo simulations of the $2$D Ising model at different temperatures, going across the phase transition. Our analysis reveals the complex organization properties of IsingNets in both the ferromagnetic and paramagnetic phases and demonstrates the significant deviations of the IsingNets with respect to randomized null models. In particular percolation properties of the IsingNets reflect the existence of the symmetry between configurations with opposite magnetization below the critical temperature and the very compact nature of the two emerging giant clusters revealed by our persistent homology analysis of the IsingNets. Moreover, the IsingNets display a very broad degree distribution and significant degree-degree correlations and weight-degree correlations demonstrating that they encode relevant information present in the configuration space of the $2$D Ising model. The geometrical organization of the critical IsingNets is reflected in their spectral properties deviating from the one of the null model. This work reveals the important insights that network science can bring to the characterization of phases of matter. The set of tools described hereby can be applied as well to numerical and experimental data."}, "https://arxiv.org/abs/2312.11834": {"title": "Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics", "link": "https://arxiv.org/abs/2312.11834", "description": "arXiv:2312.11834v3 Announce Type: replace-cross \nAbstract: In recent years, simulations of pedestrians using the multi-agent reinforcement learning (MARL) have been studied. This study considered the roads on a grid-world environment, and implemented pedestrians as MARL agents using an echo-state network and the least squares policy iteration method. Under this environment, the ability of these agents to learn to move forward by avoiding other agents was investigated. Specifically, we considered two types of tasks: the choice between a narrow direct route and a broad detour, and the bidirectional pedestrian flow in a corridor. The simulations results indicated that the learning was successful when the density of the agents was not that high."}, "https://arxiv.org/abs/2401.09438": {"title": "Recurrence analysis of meteorological data from climate zones in India", "link": "https://arxiv.org/abs/2401.09438", "description": "arXiv:2401.09438v4 Announce Type: replace-cross \nAbstract: We present a study on the spatio-temporal pattern underlying the climate dynamics in various locations spread over India, including the Himalayan region, coastal region, central and northeastern parts of India. We try to capture the variations in the complexity of their dynamics derived from temperature and relative humidity data from 1948-2022. By estimating the recurrence-based measures from the reconstructed phase space dynamics using a sliding window analysis on the data sets, we study the climate variability in different spatial locations. The study brings out the variations in the complexity of the underlying dynamics as well as their heterogeneity across the locations in India. We find almost all locations indicate shifts to more irregular and stochastic dynamics for temperature data around 1972-79 and shifts back to more regular dynamics beyond 2000. These patterns correlate with reported shifts in the climate and Indian Summer Monsoon related to strong and moderate ENSO events and confirm their associated regional variability."}, "https://arxiv.org/abs/2405.06075": {"title": "The Building Blocks of Consciousness", "link": "https://arxiv.org/abs/2405.06075", "description": "arXiv:2405.06075v1 Announce Type: new \nAbstract: Consciousness is presented not as a unified and uniquely human characteristic, but rather as an emergent property of several building blocks, most of which are demonstrably present in other species. Each block has its own rationale under natural selection and could have arisen independently, and the jumps between blocks -- which culminate in consciousness -- are small enough to be evolutionarily plausible. One underappreciated block involves unconscious engram playback and discrimination, and plays a major role in brain storage optimization. This function is present in birds and nearly all mammals and is recognized by its side-effect: dreams."}, "https://arxiv.org/abs/2405.06282": {"title": "A cellular automata approach for modelling pedestrian-vehicle mixed traffic flow in urban city", "link": "https://arxiv.org/abs/2405.06282", "description": "arXiv:2405.06282v1 Announce Type: new \nAbstract: In urban streets, the intrusion of pedestrians presents significant safety challenges. Modelling mixed pedestrian-vehicle traffic is complex due to the distinct motion characteristics and spatial dimensions of pedestrians and vehicles, making unified modelling difficult, with few studies addressing these issues. This paper employs a multi-grid cellular automata model to bridge the gap between vehicle and pedestrian models. An Improved Kerner-Klenov-Wolf (IKKW) model and a pedestrian motion model that incorporates Time-To-Collision (TTC) are introduced. Both models update the spatial motions of vehicles and pedestrians uniformly. Empirical analysis indicates that the model achieves high simulation accuracy. This model effectively illustrates the impact of pedestrian intrusion within mixed traffic scenario. The fundamental diagram of heterogeneous traffic reveals substantial differences, highlighting the effects of pedestrian intrusion on traffic flow states and identifying six phase regions in mixed traffic. Additionally, this paper examines conflicts between pedestrians and vehicles under varying speed limits and sidewalk widths, demonstrating that lower speeds and broader sidewalks significantly reduce the frequency of pedestrian-vehicle conflicts. Notably, the frequency of peak conflicts at a vehicle speed limit of 60.48 km/h is more than three times higher than at 30.24 km/h. This model offers a potential approach to studying mixed traffic flows and exhibits substantial scalability."}, "https://arxiv.org/abs/2405.06285": {"title": "Pedestrian Crossing Discrepancy Within Static and Dynamic Crowds: An Experimental Study", "link": "https://arxiv.org/abs/2405.06285", "description": "arXiv:2405.06285v1 Announce Type: new \nAbstract: This paper aims to investigate the disparities in pedestrian crossing behaviors within static and dynamic crowds through experimental analysis. First, the crossing trajectories of pedestrians in various crowd environments were qualitatively observed. These trajectories have shown significant discrepancies and the phenomenon of cross-channel formation within static crowds was observed, a phenomenon similar to the evolution of human trails. To quantitatively assess these discrepancies, metrics of behavior patterns and swarm factor were introduced. Different behavioral patterns, including anticipation and reaction behaviors in pedestrian motion, were observed. Finally, through orthogonal velocity analysis, the variation trends of crossing motions within static and dynamic contexts were revealed."}, "https://arxiv.org/abs/2405.06395": {"title": "Fitness-Based Growth of Directed Networks with Hierarchy", "link": "https://arxiv.org/abs/2405.06395", "description": "arXiv:2405.06395v1 Announce Type: new \nAbstract: Growing attention has been brought to the fact that many real directed networks exhibit hierarchy and directionality as measured through techniques like Trophic Analysis and non-normality. We propose a simple growing network model where the probability of connecting to a node is defined by a preferential attachment mechanism based on degree and the difference in fitness between nodes. In particular, we show how mechanisms such as degree-based preferential attachment and node fitness interactions can lead to the emergence of the spectrum of hierarchy and directionality observed in real networks. In this work, we study various features of this model relating to network hierarchy, as measured by Trophic Analysis. This includes (I) how preferential attachment can lead to network hierarchy, (II) how scale-free degree distributions and network hierarchy can coexist, (III) the correlation between node fitness and trophic level, (IV) how the fitness parameters can predict trophic incoherence and how the trophic level difference distribution compares to the fitness difference distribution, (V) the relationship between trophic level and degree imbalance and the unique role of nodes at the ends of the fitness hierarchy and (VI) how fitness interactions and degree-based preferential attachment can interplay to generate networks of varying coherence and degree distribution. We also provide an example of the intuition this work enables in the analysis of a real historical network. This work provides insight into simple mechanisms which can give rise to hierarchy in directed networks and quantifies the usefulness and limitations of using Trophic Analysis as an analysis tool for real networks."}, "https://arxiv.org/abs/2405.06508": {"title": "Simple crowd dynamics to generate complex temporal contact networks", "link": "https://arxiv.org/abs/2405.06508", "description": "arXiv:2405.06508v1 Announce Type: new \nAbstract: Empirical contact networks or interaction networks demonstrate peculiar characteristics stemming from the fundamental social, psychological, physical mechanisms governing human interactions. Although these mechanisms are complex, we test whether we are able to reproduce some dynamical properties of these empirical networks from relatively simple models. In this study, we perform simulations for a range of 2D models of particle dynamics, namely the Random Walk, Active Brownian Particles, and Vicsek models, to generate artificial contact networks. We investigate temporal properties of these contact networks: the distributions of contact durations, inter-contact durations and number of contact per pair of particle. We demonstrate that the distribution of inter-contact durations can be recovered by the dynamics of these simple crowd particle models, and show that it is simply related to the well-know first-return process, which explains the -3/2 exponent that is found in both the numerical models and empirical contact networks."}, "https://arxiv.org/abs/2405.06476": {"title": "Is the panel fair? Evaluating panel compositions through network analysis", "link": "https://arxiv.org/abs/2405.06476", "description": "arXiv:2405.06476v1 Announce Type: cross \nAbstract: In research evaluation, the fair representation of panels is usually defined in terms of observable characteristics of scholars such as gender or affiliations. An an empirical strategy is proposed for exploring hidden connections between panellists such that, despite the respect of formal requirements, the panel could be considered alike as unfair with respect to the representation of diversity of research approaches and methodologies. The case study regards the three panels selected to evaluate research in economics, statistics and business during the Italian research assessment exercises. The first two panels were appointed directly by the governmental agency responsible for the evaluation, while the third was randomly selected. Hence the third panel can be considered as a control for evaluating about the fairness of the others. The fair representation is explored by comparing the networks of panellists based on their co-authorship relations, the networks based on journals in which they published and the networks based on their affiliated institutions (universities, research centres and newspapers). The results show that the members of the first two panels had connections much higher than the members of the control group. Hence the composition of the first two panels should be considered as unfair, as the results of the research assessments."}, "https://arxiv.org/abs/2405.06478": {"title": "Attention is all they need: Cognitive science and the (techno)political economy of attention in humans and machines", "link": "https://arxiv.org/abs/2405.06478", "description": "arXiv:2405.06478v1 Announce Type: cross \nAbstract: This paper critically analyses the \"attention economy\" within the framework of cognitive science and techno-political economics, as applied to both human and machine interactions. We explore how current business models, particularly in digital platform capitalism, harness user engagement by strategically shaping attentional patterns. These platforms utilize advanced AI and massive data analytics to enhance user engagement, creating a cycle of attention capture and data extraction. We review contemporary (neuro)cognitive theories of attention and platform engagement design techniques and criticize classical cognitivist and behaviourist theories for their inadequacies in addressing the potential harms of such engagement on user autonomy and wellbeing. 4E approaches to cognitive science, instead, emphasizing the embodied, extended, enactive, and ecological aspects of cognition, offer us an intrinsic normative standpoint and a more integrated understanding of how attentional patterns are actively constituted by adaptive digital environments. By examining the precarious nature of habit formation in digital contexts, we reveal the techno-economic underpinnings that threaten personal autonomy by disaggregating habits away from the individual, into an AI managed collection of behavioural patterns. Our current predicament suggests the necessity of a paradigm shift towards an ecology of attention. This shift aims to foster environments that respect and preserve human cognitive and social capacities, countering the exploitative tendencies of cognitive capitalism."}, "https://arxiv.org/abs/2405.06541": {"title": "ATSumm: Auxiliary information enhanced approach for abstractive disaster Tweet Summarization with sparse training data", "link": "https://arxiv.org/abs/2405.06541", "description": "arXiv:2405.06541v1 Announce Type: cross \nAbstract: The abundance of situational information on Twitter poses a challenge for users to manually discern vital and relevant information during disasters. A concise and human-interpretable overview of this information helps decision-makers in implementing efficient and quick disaster response. Existing abstractive summarization approaches can be categorized as sentence-based or key-phrase-based approaches. This paper focuses on sentence-based approach, which is typically implemented as a dual-phase procedure in literature. The initial phase, known as the extractive phase, involves identifying the most relevant tweets. The subsequent phase, referred to as the abstractive phase, entails generating a more human-interpretable summary. In this study, we adopt the methodology from prior research for the extractive phase. For the abstractive phase of summarization, most existing approaches employ deep learning-based frameworks, which can either be pre-trained or require training from scratch. However, to achieve the appropriate level of performance, it is imperative to have substantial training data for both methods, which is not readily available. This work presents an Abstractive Tweet Summarizer (ATSumm) that effectively addresses the issue of data sparsity by using auxiliary information. We introduced the Auxiliary Pointer Generator Network (AuxPGN) model, which utilizes a unique attention mechanism called Key-phrase attention. This attention mechanism incorporates auxiliary information in the form of key-phrases and their corresponding importance scores from the input tweets. We evaluate the proposed approach by comparing it with 10 state-of-the-art approaches across 13 disaster datasets. The evaluation results indicate that ATSumm achieves superior performance compared to state-of-the-art approaches, with improvement of 4-80% in ROUGE-N F1-score."}, "https://arxiv.org/abs/2405.06551": {"title": "ADSumm: Annotated Ground-truth Summary Datasets for Disaster Tweet Summarization", "link": "https://arxiv.org/abs/2405.06551", "description": "arXiv:2405.06551v1 Announce Type: cross \nAbstract: Online social media platforms, such as Twitter, provide valuable information during disaster events. Existing tweet disaster summarization approaches provide a summary of these events to aid government agencies, humanitarian organizations, etc., to ensure effective disaster response. In the literature, there are two types of approaches for disaster summarization, namely, supervised and unsupervised approaches. Although supervised approaches are typically more effective, they necessitate a sizable number of disaster event summaries for testing and training. However, there is a lack of good number of disaster summary datasets for training and evaluation. This motivates us to add more datasets to make supervised learning approaches more efficient. In this paper, we present ADSumm, which adds annotated ground-truth summaries for eight disaster events which consist of both natural and man-made disaster events belonging to seven different countries. Our experimental analysis shows that the newly added datasets improve the performance of the supervised summarization approaches by 8-28% in terms of ROUGE-N F1-score. Moreover, in newly annotated dataset, we have added a category label for each input tweet which helps to ensure good coverage from different categories in summary. Additionally, we have added two other features relevance label and key-phrase, which provide information about the quality of a tweet and explanation about the inclusion of the tweet into summary, respectively. For ground-truth summary creation, we provide the annotation procedure adapted in detail, which has not been described in existing literature. Experimental analysis shows the quality of ground-truth summary is very good with Coverage, Relevance and Diversity."}, "https://arxiv.org/abs/2306.16568": {"title": "Early warning signals for predicting cryptomarket vendor success using dark net forum networks", "link": "https://arxiv.org/abs/2306.16568", "description": "arXiv:2306.16568v3 Announce Type: replace \nAbstract: In this work we focus on identifying key players in dark net cryptomarkets that facilitate online trade of illegal goods. Law enforcement aims to disrupt criminal activity conducted through these markets by targeting key players vital to the market's existence and success. We particularly focus on detecting successful vendors responsible for the majority of illegal trade. Our methodology aims to uncover whether the task of key player identification should center around plainly measuring user and forum activity, or that it requires leveraging specific patterns of user communication. We focus on a large-scale dataset from the Evolution cryptomarket, which we model as an evolving communication network. Results indicate that user and forum activity, measured through topic engagement, is best able to identify successful vendors. Interestingly, considering users with higher betweenness centrality in the communication network further improves performance, also identifying successful vendors with moderate activity on the forum. But more importantly, analyzing the forum data over time, we find evidence that attaining a high betweenness score comes before vendor success. This suggests that the proposed network-driven approach of modelling user communication might prove useful as an early warning signal for key player identification."}, "https://arxiv.org/abs/2312.14040": {"title": "Balancing Specialization and Adaptation in a Transforming Scientific Landscape", "link": "https://arxiv.org/abs/2312.14040", "description": "arXiv:2312.14040v5 Announce Type: replace \nAbstract: How do scientists navigate between the need to capitalize on their prior knowledge through specialization, and the urge to adapt to evolving research opportunities? Drawing from diverse perspectives on adaptation, including cultural evolution, this paper proposes an unsupervised Bayesian approach motivated by Optimal Transport of the evolution of scientists' research portfolios in response to transformations in their field. The model relies on $186,162$ scientific abstracts and authorship data to evaluate the influence of intellectual, social, and institutional resources on scientists' trajectories within a cohort of $2\\,195$ high-energy physicists between 2000 and 2019. Using Inverse Optimal Transport, the reallocation of research efforts is shown to be shaped by learning costs, thus enhancing the utility of the scientific capital disseminated among scientists. Two dimensions of social capital, namely \"diversity\" and \"power\", have opposite associations with the magnitude of change in scientists' research interests: while \"diversity\" disrupts and expands research interests, \"power\" is associated with more stable research agendas. Social capital plays a more crucial role in shifts between cognitively distant research areas. More generally, this work suggests new approaches for understanding, measuring and modeling collective adaptation using Optimal Transport."}, "https://arxiv.org/abs/2401.03656": {"title": "CosIn: A Statistical-Based Algorithm for Computation of Speed-Space Time Delay in Pedestrian Motion", "link": "https://arxiv.org/abs/2401.03656", "description": "arXiv:2401.03656v3 Announce Type: replace \nAbstract: The precise assessment of speed-space time delay (TD) facilitates the differentiation between pedestrian anticipation behavior and reaction behavior. Importantly, the TD scale is instrumental in the evaluation of potential collision risks inherent in the crowd, thereby offering crucial quantitative metrics for crowd risk. This article introduces the CosIn algorithm for evaluate TD during pedestrian motion, comprising the CosIn-1 and CosIn-2 algorithms. The CosIn-1 algorithm specifically addresses the precise computation issue associated with the TD of individual pedestrians, while the CosIn-2 algorithm is employed for assessing TD at a crowd scale, concurrently addressing the imperative of real-time computation. Efficacy analyses of the CosIn-1 and CosIn-2 algorithms are conducted using the data from single-file pedestrian experiments and crowd cross experiments, respectively. The results obtained demonstrate commendable precision in the algorithmic solutions. This algorithm contributes to the precise assessment of behavior patterns and collision risk within crowd dynamics."}, "https://arxiv.org/abs/2403.01269": {"title": "Network analysis using Krylov subspace trajectories", "link": "https://arxiv.org/abs/2403.01269", "description": "arXiv:2403.01269v2 Announce Type: replace \nAbstract: We describe a set of network analysis methods based on the rows of the Krylov subspace matrix computed from a network adjacency matrix via power iteration using a non-random initial vector. We refer to these node-specific row vectors as Krylov subspace trajectories. While power iteration using a random initial starting vector is commonly applied to the network adjacency matrix to compute eigenvector centrality values, this application only uses the final vector generated after numerical convergence. Importantly, use of a random initial vector means that the intermediate results of power iteration are also random and lack a clear interpretation. To the best of our knowledge, use of intermediate power iteration results for network analysis has been limited to techniques that leverage just a single pre-convergence solution, e.g., Power Iteration Clustering. In this paper, we explore methods that apply power iteration with a non-random inital vector to the network adjacency matrix to generate Krylov subspace trajectories for each node. These non-random trajectories provide important information regarding network structure, node importance, and response to perturbations. We have created this short preprint in part to generate feedback from others in the network analysis community who might be aware of similar existing work."}, "https://arxiv.org/abs/2404.05334": {"title": "Modeling the Dynamic Process of Inventions for Reducing Knowledge Search Costs", "link": "https://arxiv.org/abs/2404.05334", "description": "arXiv:2404.05334v2 Announce Type: replace \nAbstract: A knowledge search is a key process for inventions. However, there is inadequate quantitative modeling of dynamic knowledge search processes and associated search costs. In this study, agent-based and complex network methodologies were proposed to quantitatively describe the dynamic process of knowledge search for actual inventions. Prior knowledge networks (PKNs), the search space of historical patents, were constructed, representative search rules were formulated for R&amp;D agents, and measures for knowledge search cost were designed to serve as search objectives. Simulation results in the field of photolithographic technology show that search costs differ significantly with different search rules. Familiarity and Degree rules significantly outperform BFS, DFS and Recency rules in terms of knowledge search costs, and are less affected by the size and density of PKNs. Interestingly, there is no significant correlation between the mean and variance of search costs and patent value, indicating that high-value patents are not particularly difficult to obtain. The implications for innovation theories and R&amp;D practices are drawn from the models and results."}, "https://arxiv.org/abs/2301.09289": {"title": "Fundamental Limits of Spectral Clustering in Stochastic Block Models", "link": "https://arxiv.org/abs/2301.09289", "description": "arXiv:2301.09289v3 Announce Type: replace-cross \nAbstract: Spectral clustering has been widely used for community detection in network sciences. While its empirical successes are well-documented, a clear theoretical understanding, particularly for sparse networks where degrees are much smaller than $\\log n$, remains unclear. In this paper, we address this significant gap by demonstrating that spectral clustering offers exponentially small error rates when applied to sparse networks under Stochastic Block Models. Our analysis provides sharp characterizations of its performance, backed by matching upper and lower bounds possessing an identical exponent with the same leading constant. The key to our results is a novel truncated $\\ell_2$ perturbation analysis for eigenvectors, coupled with a new analysis idea of eigenvectors truncation."}, "https://arxiv.org/abs/2312.15099": {"title": "Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models", "link": "https://arxiv.org/abs/2312.15099", "description": "arXiv:2312.15099v2 Announce Type: replace-cross \nAbstract: Online hate is an escalating problem that negatively impacts the lives of Internet users, and is also subject to rapid changes due to evolving events, resulting in new waves of online hate that pose a critical threat. Detecting and mitigating these new waves present two key challenges: it demands reasoning-based complex decision-making to determine the presence of hateful content, and the limited availability of training samples hinders updating the detection model. To address this critical issue, we present a novel framework called HATEGUARD for effectively moderating new waves of online hate. HATEGUARD employs a reasoning-based approach that leverages the recently introduced chain-of-thought (CoT) prompting technique, harnessing the capabilities of large language models (LLMs). HATEGUARD further achieves prompt-based zero-shot detection by automatically generating and updating detection prompts with new derogatory terms and targets in new wave samples to effectively address new waves of online hate. To demonstrate the effectiveness of our approach, we compile a new dataset consisting of tweets related to three recently witnessed new waves: the 2022 Russian invasion of Ukraine, the 2021 insurrection of the US Capitol, and the COVID-19 pandemic. Our studies reveal crucial longitudinal patterns in these new waves concerning the evolution of events and the pressing need for techniques to rapidly update existing moderation tools to counteract them. Comparative evaluations against state-of-the-art tools illustrate the superiority of our framework, showcasing a substantial 22.22% to 83.33% improvement in detecting the three new waves of online hate. Our work highlights the severe threat posed by the emergence of new waves of online hate and represents a paradigm shift in addressing this threat practically."}, "https://arxiv.org/abs/2405.06698": {"title": "Optimizing Viscous Democracy", "link": "https://arxiv.org/abs/2405.06698", "description": "arXiv:2405.06698v1 Announce Type: new \nAbstract: Viscous democracy is a generalization of liquid democracy, a social choice framework in which voters may transitively delegate their votes. In viscous democracy, a \"viscosity\" factor decreases the weight of a delegation the further it travels, reducing the chance of excessive weight flowing between ideologically misaligned voters. We demonstrate that viscous democracy often significantly improves the quality of group decision-making over liquid democracy. We first show that finding optimal delegations within a viscous setting is NP-hard. However, simulations allow us to explore the practical effects of viscosity. Across social network structures, competence distributions, and delegation mechanisms we find high viscosity reduces the chance of \"super-voters\" attaining large amounts of weight and increases the number of voters that are able to affect the outcome of elections. This, in turn, improves group accuracy as a whole. As a result, we argue that viscosity should be considered a core component of liquid democracy."}, "https://arxiv.org/abs/2405.06700": {"title": "LLM-Augmented Agent-Based Modelling for Social Simulations: Challenges and Opportunities", "link": "https://arxiv.org/abs/2405.06700", "description": "arXiv:2405.06700v1 Announce Type: new \nAbstract: As large language models (LLMs) continue to make significant strides, their better integration into agent-based simulations offers a transformational potential for understanding complex social systems. However, such integration is not trivial and poses numerous challenges. Based on this observation, in this paper, we explore architectures and methods to systematically develop LLM-augmented social simulations and discuss potential research directions in this field. We conclude that integrating LLMs with agent-based simulations offers a powerful toolset for researchers and scientists, allowing for more nuanced, realistic, and comprehensive models of complex systems and human behaviours."}, "https://arxiv.org/abs/2405.06947": {"title": "A Galton Board Approximation Method for Estimating Pedestrian Walking Preferences within Crowds", "link": "https://arxiv.org/abs/2405.06947", "description": "arXiv:2405.06947v1 Announce Type: new \nAbstract: This paper proposes a Galton board approximation method to analyze the potential walking preferences of pedestrians. We employ the binomial distribution to estimate the walking preferences of pedestrians in dynamic crowds. Estimating the probability of the right-side preference $(p)$ based on observational data poses the challenge, as statistical measures such as means and variances often lead to divergent results. This paper aims to explore this issue."}, "https://arxiv.org/abs/2405.07071": {"title": "Colocation of skill related suppliers -- Revisiting coagglomeration using firm-to-firm network data", "link": "https://arxiv.org/abs/2405.07071", "description": "arXiv:2405.07071v1 Announce Type: new \nAbstract: Strong local clusters help firms compete on global markets. One explanation for this is that firms benefit from locating close to their suppliers and customers. However, the emergence of global supply chains shows that physical proximity is not necessarily a prerequisite to successfully manage customer-supplier relations anymore. This raises the question when firms need to colocate in value chains and when they can coordinate over longer distances. We hypothesize that one important aspect is the extent to which supply chain partners exchange not just goods but also know-how. To test this, we build on an expanding literature that studies the drivers of industrial coagglomeration to analyze when supply chain connections lead firms to colocation. We exploit detailed micro-data for the Hungarian economy between 2015 and 2017, linking firm registries, employer-employee matched data and firm-to-firm transaction data from value-added tax records. This allows us to observe colocation, labor flows and value chain connections at the level of firms, as well as construct aggregated coagglomeration patterns, skill relatedness and input-output connections between pairs of industries. We show that supply chains are more likely to support coagglomeration when the industries involved are also skill related. That is, input-output and labor market channels reinforce each other, but supplier connections only matter for colocation when industries have similar labor requirements, suggesting that they employ similar types of know-how. We corroborate this finding by analyzing the interactions between firms, showing that supplier relations are more geographically constrained between companies that operate in skill related industries."}, "https://arxiv.org/abs/2405.07072": {"title": "Selecting focused digital cohorts from social media using the metric backbone of biomedical knowledge graphs", "link": "https://arxiv.org/abs/2405.07072", "description": "arXiv:2405.07072v1 Announce Type: new \nAbstract: The abundance of social media data allows researchers to construct large digital cohorts to study the interplay between human behavior and medical treatment. Identifying the users most relevant to a specific health problem is, however, a challenge in that social media sites vary in the generality of their discourse. While X (formerly Twitter), Instagram, and Facebook cater to wide ranging topics, Reddit subgroups and dedicated patient advocacy forums trade in much more specific, biomedically-relevant discourse. To hone in on relevant users anywhere, we have developed a general framework and applied it to epilepsy discourse in social media as a test case. We analyzed the text from posts by users who mention epilepsy drugs in the general-purpose social media sites X and Instagram, the epilepsy-focused Reddit subgroup (r/Epilepsy), and the Epilepsy Foundation of America (EFA) forums. We curated a medical terms dictionary and used it to generate a knowledge graph (KG) for each online community. For each KG, we computed the metric backbone--the smallest subgraph that preserves all shortest paths in the network. By comparing the subset of users who contribute to the backbone to the subset who do not, we found that epilepsy-focused social media users contribute to the KG backbone in much higher proportion than do general-purpose social media users. Furthermore, using human annotation of Instagram posts, we demonstrated that users who do not contribute to the backbone are more than twice as likely to use dictionary terms in a manner inconsistent with their biomedical meaning. For biomedical research applications, our backbone-based approach thus has several benefits over simple engagement-based approaches: It can retain low-engagement users who nonetheless contribute meaningful biomedical insights. It can filter out very vocal users who contribute no relevant content."}, "https://arxiv.org/abs/2405.07096": {"title": "Multi-Relational Structural Entropy", "link": "https://arxiv.org/abs/2405.07096", "description": "arXiv:2405.07096v1 Announce Type: new \nAbstract: Structural Entropy (SE) measures the structural information contained in a graph. Minimizing or maximizing SE helps to reveal or obscure the intrinsic structural patterns underlying graphs in an interpretable manner, finding applications in various tasks driven by networked data. However, SE ignores the heterogeneity inherent in the graph relations, which is ubiquitous in modern networks. In this work, we extend SE to consider heterogeneous relations and propose the first metric for multi-relational graph structural information, namely, Multi-relational Structural Entropy (MrSE). To this end, we first cast SE through the novel lens of the stationary distribution from random surfing, which readily extends to multi-relational networks by considering the choices of both nodes and relation types simultaneously at each step. The resulting MrSE is then optimized by a new greedy algorithm to reveal the essential structures within a multi-relational network. Experimental results highlight that the proposed MrSE offers a more insightful interpretation of the structure of multi-relational graphs compared to SE. Additionally, it enhances the performance of two tasks that involve real-world multi-relational graphs, including node clustering and social event detection."}, "https://arxiv.org/abs/2405.07277": {"title": "Mining Influential Spreaders in Complex Networks by an Effective Combination of the Degree and K-Shell", "link": "https://arxiv.org/abs/2405.07277", "description": "arXiv:2405.07277v1 Announce Type: new \nAbstract: Graph mining is an important technique that used in many applications such as predicting and understanding behaviors and information dissemination within networks. One crucial aspect of graph mining is the identification and ranking of influential nodes, which has applications in various fields including marketing, social communications, and disease control. However, existing models and methods come with high computational complexity and may not accurately distinguish and identify influential nodes. This paper develops a method based on the k-shell index and degree centrality of nodes and their neighbors. Comparisons to previous works, such as Degree and Neighborhood information Centrality (DNC) and Neighborhood and Path Information Centrality (NPIC), are conducted. The evaluations, which include the correctness with Kendall's Tau, resolution with monotonicity index, correlation plots, and time complexity, demonstrate its superior results."}, "https://arxiv.org/abs/2405.07417": {"title": "Identifying Hate Speech Peddlers in Online Platforms", "link": "https://arxiv.org/abs/2405.07417", "description": "arXiv:2405.07417v1 Announce Type: new \nAbstract: This paper studies the problem of autonomous agents performing Bayesian social learning for sequential detection when the observations of the state belong to a high-dimensional space and are expensive to analyze. Specifically, when the observations are textual, the Bayesian agent can use a large language model (LLM) as a map to get a low-dimensional private observation. The agent performs Bayesian learning and takes an action that minimizes the expected cost and is visible to subsequent agents. We prove that a sequence of such Bayesian agents herd in finite time to the public belief and take the same action disregarding the private observations. We propose a stopping time formulation for quickest time herding in social learning and optimally balance privacy and herding. Structural results are shown on the threshold nature of the optimal policy to the stopping time problem. We illustrate the application of our framework when autonomous Bayesian detectors aim to sequentially identify if a user is a hate speech peddler on an online platform by parsing text observations using an LLM. We numerically validate our results on real-world hate speech datasets. We show that autonomous Bayesian agents designed to flag hate speech peddlers in online platforms herd and misclassify the users when the public prior is strong. We also numerically show the effect of a threshold policy in delaying herding."}, "https://arxiv.org/abs/2405.07574": {"title": "Is it getting harder to make a hit? Evidence from 65 years of US music chart history", "link": "https://arxiv.org/abs/2405.07574", "description": "arXiv:2405.07574v1 Announce Type: new \nAbstract: Since the creation of the Billboard Hot 100 music chart in 1958, the chart has been a window into the music consumption of Americans. Which songs succeed on the chart is decided by consumption volumes, which can be affected by consumer music taste, and other factors such as advertisement budgets, airplay time, the specifics of ranking algorithms, and more. Since its introduction, the chart has documented music consumerism through eras of globalization, economic growth, and the emergence of new technologies for music listening. In recent years, musicians and other hitmakers have voiced their worry that the music world is changing: Many claim that it is getting harder to make a hit but until now, the claims have not been backed using chart data. Here we show that the dynamics of the Billboard Hot 100 chart have changed significantly since the chart's founding in 1958, and in particular in the past 15 years. Whereas most songs spend less time on the chart now than songs did in the past, we show that top-1 songs have tripled their chart lifetime since the 1960s, the highest-ranked songs maintain their positions for far longer than previously, and the lowest-ranked songs are replaced more frequently than ever. At the same time, who occupies the chart has also changed over the years: In recent years, fewer new artists make it into the chart and more positions are occupied by established hit makers. Finally, investigating how song chart trajectories have changed over time, we show that historical song trajectories cluster into clear trajectory archetypes characteristic of the time period they were part of. The results are interesting in the context of collective attention: Whereas recent studies have documented that other cultural products such as books, news, and movies fade in popularity quicker in recent years, music hits seem to last longer now than in the past."}, "https://arxiv.org/abs/2405.07828": {"title": "Can LLMs Help Predict Elections? (Counter)Evidence from the World's Largest Democracy", "link": "https://arxiv.org/abs/2405.07828", "description": "arXiv:2405.07828v1 Announce Type: new \nAbstract: The study of how social media affects the formation of public opinion and its influence on political results has been a popular field of inquiry. However, current approaches frequently offer a limited comprehension of the complex political phenomena, yielding inconsistent outcomes. In this work, we introduce a new method: harnessing the capabilities of Large Language Models (LLMs) to examine social media data and forecast election outcomes. Our research diverges from traditional methodologies in two crucial respects. First, we utilize the sophisticated capabilities of foundational LLMs, which can comprehend the complex linguistic subtleties and contextual details present in social media data. Second, we focus on data from X (Twitter) in India to predict state assembly election outcomes. Our method entails sentiment analysis of election-related tweets through LLMs to forecast the actual election results, and we demonstrate the superiority of our LLM-based method against more traditional exit and opinion polls. Overall, our research offers valuable insights into the unique dynamics of Indian politics and the remarkable impact of social media in molding public attitudes within this context."}, "https://arxiv.org/abs/2405.07950": {"title": "Quantum-like states on complex synchronized networks", "link": "https://arxiv.org/abs/2405.07950", "description": "arXiv:2405.07950v1 Announce Type: new \nAbstract: Recent work has exposed the idea that interesting quantum-like probability laws, including interference effects, can be manifest in classical systems. Here we propose a model for quantum-like (QL) states and QL bits. We suggest a way that huge, complex systems can host robust states that can process information in a QL fashion. Axioms that such states should satisfy are proposed. Specifically, it is shown that building blocks suited for QL states are networks, possibly very complex, that we defined based on $k$-regular random graphs. These networks can dynamically encode a lot of information that is distilled into the emergent states we can use for QL like processing. Although the emergent states are classical, they have properties analogous to quantum states. Concrete examples of how QL functions are possible are given. The possibility of a `QL advantage' for computing-type operations and the potential relevance for new kinds of function in the brain are discussed and left as open questions."}, "https://arxiv.org/abs/2405.06656": {"title": "Exploring Social Media Posts for Depression Identification: A Study on Reddit Dataset", "link": "https://arxiv.org/abs/2405.06656", "description": "arXiv:2405.06656v1 Announce Type: cross \nAbstract: Depression is one of the most common mental disorders affecting an individual's personal and professional life. In this work, we investigated the possibility of utilizing social media posts to identify depression in individuals. To achieve this goal, we conducted a preliminary study where we extracted and analyzed the top Reddit posts made in 2022 from depression-related forums. The collected data were labeled as depressive and non-depressive using UMLS Metathesaurus. Further, the pre-processed data were fed to classical machine learning models, where we achieved an accuracy of 92.28\\% in predicting the depressive and non-depressive posts."}, "https://arxiv.org/abs/2405.06668": {"title": "Exposing and Explaining Fake News On-the-Fly", "link": "https://arxiv.org/abs/2405.06668", "description": "arXiv:2405.06668v1 Announce Type: cross \nAbstract: Social media platforms enable the rapid dissemination and consumption of information. However, users instantly consume such content regardless of the reliability of the shared data. Consequently, the latter crowdsourcing model is exposed to manipulation. This work contributes with an explainable and online classification method to recognize fake news in real-time. The proposed method combines both unsupervised and supervised Machine Learning approaches with online created lexica. The profiling is built using creator-, content- and context-based features using Natural Language Processing techniques. The explainable classification mechanism displays in a dashboard the features selected for classification and the prediction confidence. The performance of the proposed solution has been validated with real data sets from Twitter and the results attain 80 % accuracy and macro F-measure. This proposal is the first to jointly provide data stream processing, profiling, classification and explainability. Ultimately, the proposed early detection, isolation and explanation of fake news contribute to increase the quality and trustworthiness of social media contents."}, "https://arxiv.org/abs/2405.06684": {"title": "QuakeBERT: Accurate Classification of Social Media Texts for Rapid Earthquake Impact Assessment", "link": "https://arxiv.org/abs/2405.06684", "description": "arXiv:2405.06684v1 Announce Type: cross \nAbstract: Social media aids disaster response but suffers from noise, hindering accurate impact assessment and decision making for resilient cities, which few studies considered. To address the problem, this study proposes the first domain-specific LLM model and an integrated method for rapid earthquake impact assessment. First, a few categories are introduced to classify and filter microblogs considering their relationship to the physical and social impacts of earthquakes, and a dataset comprising 7282 earthquake-related microblogs from twenty earthquakes in different locations is developed as well. Then, with a systematic analysis of various influential factors, QuakeBERT, a domain-specific large language model (LLM), is developed and fine-tuned for accurate classification and filtering of microblogs. Meanwhile, an integrated method integrating public opinion trend analysis, sentiment analysis, and keyword-based physical impact quantification is introduced to assess both the physical and social impacts of earthquakes based on social media texts. Experiments show that data diversity and data volume dominate the performance of QuakeBERT and increase the macro average F1 score by 27%, while the best classification model QuakeBERT outperforms the CNN- or RNN-based models by improving the macro average F1 score from 60.87% to 84.33%. Finally, the proposed approach is applied to assess two earthquakes with the same magnitude and focal depth. Results show that the proposed approach can effectively enhance the impact assessment process by accurate detection of noisy microblogs, which enables effective post-disaster emergency responses to create more resilient cities."}, "https://arxiv.org/abs/2405.07217": {"title": "Improved bounds for polylogarithmic graph distances in scale-free percolation and related models", "link": "https://arxiv.org/abs/2405.07217", "description": "arXiv:2405.07217v1 Announce Type: cross \nAbstract: In this paper, we study graph distances in the geometric random graph models scale-free percolation SFP, geometric inhomogeneous random graphs GIRG, and hyperbolic random graphs HRG. Despite the wide success of the models, the parameter regime in which graph distances are polylogarithmic is poorly understood. We provide new and improved lower bounds. In a certain portion of the parameter regime, those match the known upper bounds.\n  Compared to the best previous lower bounds by Hao and Heydenreich, our result has several advantages: it gives matching bounds for a larger range of parameters, thus settling the question for a larger portion of the parameter space. It strictly improves the lower bounds by Hao and Heydenreich for all parameters settings in which those bounds were not tight. It gives tail bounds on the probability of having short paths, which imply shape theorems for the $k$-neighbourhood of a vertex whenever our lower bounds are tight, and tight bounds for the size of this $k$-neighbourhood. And last but not least, our proof is much simpler and not much longer than two pages, and we demonstrate that it generalizes well by showing that the same technique also works for first passage percolation."}, "https://arxiv.org/abs/2405.07764": {"title": "LGDE: Local Graph-based Dictionary Expansion", "link": "https://arxiv.org/abs/2405.07764", "description": "arXiv:2405.07764v1 Announce Type: cross \nAbstract: Expanding a dictionary of pre-selected keywords is crucial for tasks in information retrieval, such as database query and online data collection. Here we propose Local Graph-based Dictionary Expansion (LGDE), a method that uses tools from manifold learning and network science for the data-driven discovery of keywords starting from a seed dictionary. At the heart of LGDE lies the creation of a word similarity graph derived from word embeddings and the application of local community detection based on graph diffusion to discover semantic neighbourhoods of pre-defined seed keywords. The diffusion in the local graph manifold allows the exploration of the complex nonlinear geometry of word embeddings and can capture word similarities based on paths of semantic association. We validate our method on a corpus of hate speech-related posts from Reddit and Gab and show that LGDE enriches the list of keywords and achieves significantly better performance than threshold methods based on direct word similarities. We further demonstrate the potential of our method through a real-world use case from communication science, where LGDE is evaluated quantitatively on data collected and analysed by domain experts by expanding a conspiracy-related dictionary."}, "https://arxiv.org/abs/2405.07877": {"title": "Optimal accuracy for linear sets of equations with the graph Laplacian", "link": "https://arxiv.org/abs/2405.07877", "description": "arXiv:2405.07877v1 Announce Type: cross \nAbstract: We show that certain Graph Laplacian linear sets of equations exhibit optimal accuracy, guaranteeing that the relative error is no larger than the norm of the relative residual and that optimality occurs for carefully chosen right-hand sides. Such sets of equations arise in PageRank and Markov chain theory. We establish new relationships among the PageRank teleportation parameter, the Markov chain discount, and approximations to linear sets of equations. The set of optimally accurate systems can be separated into two groups for an undirected graph -- those that achieve optimality asymptotically with the graph size and those that do not -- determined by the angle between the right-hand side of the linear system and the vector of all ones. We provide supporting numerical experiments."}, "https://arxiv.org/abs/2306.08426": {"title": "Patterns of Patterns II", "link": "https://arxiv.org/abs/2306.08426", "description": "arXiv:2306.08426v3 Announce Type: replace \nAbstract: Our earlier paper \"Patterns of Patterns\" combined three techniques from training, futures studies, and design in a design pattern called PLACARD that helps groups of people work together effectively. We used that pattern in five hands-on workshop case studies which took place at various locations in the US and the UK. This experience report documents what we learned, including the way our thinking about PLACARD evolved, together with additional patterns our work generated. We evaluate the reproducibility of our methods and results, and consider the broader economic implications of this way of working. We discuss implications of our prototyping work for the design of future platforms, drawing connections with recent developments in cognitive science and artificial intelligence. This positions our patterns of patterns as a toolkit for the design and governance of systems that combine social dynamics with technical components."}, "https://arxiv.org/abs/2306.12136": {"title": "Node-layer duality in networked systems", "link": "https://arxiv.org/abs/2306.12136", "description": "arXiv:2306.12136v2 Announce Type: replace \nAbstract: Real-world networks typically exhibit several aspects, or layers, of interactions among their nodes. By permuting the role of the nodes and the layers, we establish a new criterion to construct the dual of a network. This approach allows to examine information from either a node-centric or layer-centric viewpoint. Through rigorous analytical methods and extensive simulations, we demonstrate that nodewise and layerwise connectivity measure different but related aspects of the same system. Leveraging node-layer duality provides complementary insights, enabling a deeper comprehension of diverse networks across social science, technology and biology. Taken together, these findings reveal previously unappreciated features of complex systems and provide a fresh tool for delving into their structure and dynamics."}, "https://arxiv.org/abs/2310.12181": {"title": "Precise influence evaluation in complex networks", "link": "https://arxiv.org/abs/2310.12181", "description": "arXiv:2310.12181v2 Announce Type: replace \nAbstract: Evaluating node influence is fundamental for identifying key nodes in complex networks. Existing methods typically rely on generic indicators to rank node influence across diverse networks, thereby ignoring the individualized features of each network itself. Actually, node influence stems not only from general features but the multi-scale individualized information encompassing specific network structure and task. Here we design an active learning architecture to predict node influence quantitively and precisely, which samples representative nodes based on graph entropy correlation matrix integrating multi-scale individualized information. This brings two intuitive advantages: (1) discovering potential high-influence but weak-connected nodes that are usually ignored in existing methods, (2) improving the influence maximization strategy by deducing influence interference. Significantly, our architecture demonstrates exceptional transfer learning capabilities across multiple types of networks, which can identify those key nodes with large disputation across different existing methods. Additionally, our approach, combined with a simple greedy algorithm, exhibits dominant performance in solving the influence maximization problem. This architecture holds great potential for applications in graph mining and prediction tasks."}, "https://arxiv.org/abs/2312.12186": {"title": "Social Learning in Community Structured Graphs", "link": "https://arxiv.org/abs/2312.12186", "description": "arXiv:2312.12186v3 Announce Type: replace \nAbstract: Traditional social learning frameworks consider environments with a homogeneous state, where each agent receives observations conditioned on that true state of nature. In this work, we relax this assumption and study the distributed hypothesis testing problem in a heterogeneous environment, where each agent can receive observations conditioned on their own personalized state of nature (or truth). We particularly focus on community structured networks, where each community admits their own true hypothesis. This scenario is common in various contexts, such as when sensors are spatially distributed, or when individuals in a social network have differing views or opinions. We show that the adaptive social learning strategy is a preferred choice for nonstationary environments, and allows each cluster to discover its own truth."}, "https://arxiv.org/abs/2401.00651": {"title": "IRWE: Inductive Random Walk for Joint Inference of Identity and Position Network Embedding", "link": "https://arxiv.org/abs/2401.00651", "description": "arXiv:2401.00651v2 Announce Type: replace \nAbstract: Network embedding, which maps graphs to distributed representations, is a unified framework for various graph inference tasks. According to the topology properties (e.g., structural roles and community memberships of nodes) to be preserved, it can be categorized into the identity and position embedding. However, existing methods can only capture one type of property. Some approaches can support the inductive inference that generalizes the embedding model to new nodes or graphs but relies on the availability of attributes. Due to the complicated correlations between topology and attributes, it is unclear for some inductive methods which type of property they can capture. In this study, we explore a unified framework for the joint inductive inference of identity and position embeddings without attributes. An inductive random walk embedding (IRWE) method is proposed, which combines multiple attention units to handle the random walk on graph topology and simultaneously derives identity and position embeddings that are jointly optimized. In particular, we demonstrate that some random walk statistics can be informative features to characterize node identities and positions while supporting the inductive embedding inference. Experiments validate the superior performance of IRWE beyond various baselines for the transductive and inductive inference of identity and position embeddings."}, "https://arxiv.org/abs/2402.03837": {"title": "Expressivity of Geometric Inhomogeneous Random Graphs -- Metric and Non-Metric", "link": "https://arxiv.org/abs/2402.03837", "description": "arXiv:2402.03837v2 Announce Type: replace \nAbstract: Recently there has been increased interest in fitting generative graph models to real-world networks. In particular, Bl\\\"asius et al. have proposed a framework for systematic evaluation of the expressivity of random graph models. We extend this framework to Geometric Inhomogeneous Random Graphs (GIRGs). This includes a family of graphs induced by non-metric distance functions which allow capturing more complex models of partial similarity between nodes as a basis of connection - as well as homogeneous and non-homogeneous feature spaces. As part of the extension, we develop schemes for estimating the multiplicative constant and the long-range parameter in the connection probability. Moreover, we devise an algorithm for sampling Minimum-Component-Distance GIRGs whose runtime is linear both in the number of vertices and in the dimension of the underlying geometric space. Our results provide evidence that GIRGs are more realistic candidates with respect to various graph features such as closeness centrality, betweenness centrality, local clustering coefficient, and graph effective diameter, while they face difficulties to replicate higher variance and more extreme values of graph statistics observed in real-world networks."}, "https://arxiv.org/abs/2402.18850": {"title": "A simple model of global cascades on random hypergraphs", "link": "https://arxiv.org/abs/2402.18850", "description": "arXiv:2402.18850v2 Announce Type: replace \nAbstract: This study introduces a comprehensive framework that situates information cascade research within the domain of higher-order interactions, utilizing a double-threshold hypergraph model. We propose that individuals (nodes) gain awareness of information through each communication channel (hyperedge) once the number of information adopters surpasses the threshold $\\phi_m$. However, actual adoption of the information only occurs when the cumulative influence across all communication channels exceeds a second threshold, $\\phi_k$. We analytically derive the cascade condition for both the case of a single seed node using percolation methods and the case of any seed size employing mean-field approximation. Our findings underscore that when considering the fractional seed size, $r_0 \\in (0,1]$, the connectivity pattern of the random hypergraph, characterized by the hyperdegree ($k$) and cardinality ($m$) distribution, exerts an asymmetric impact on the global cascade boundary. This asymmetry manifests in the observed differences in the boundaries of the global cascade within the $(\\phi_m, \\langle m \\rangle)$ and $(\\phi_k, \\langle k \\rangle)$ planes. However, as $r_0 \\to 0$, this asymmetric effect gradually diminishes. Overall, by elucidating the mechanisms driving information cascades within a broader context of higher-order interactions, our research contributes to theoretical advancements in complex systems theory."}, "https://arxiv.org/abs/2403.13945": {"title": "$N$-player game formulation of the majority-vote model of opinion dynamics", "link": "https://arxiv.org/abs/2403.13945", "description": "arXiv:2403.13945v2 Announce Type: replace \nAbstract: From a self-centered perspective, it can be assumed that people only hold opinions that can benefit them. If opinions have no intrinsic value, and acquire their value when held by the majority of individuals in a discussion group, then we have a situation that can be modeled as an $N$-player game. Here we explore the dynamics of (binary) opinion formation using a game-theoretic framework to study an $N$-player game version of Galam's local majority-vote model. The opinion dynamics is modeled by a stochastic imitation dynamics in which the individuals copy the opinion of more successful peers. In the infinite population limit, this dynamics is described by the classical replicator equation of evolutionary game theory. The equilibrium solution shows a threshold separating the initial frequencies that lead to the fixation of one opinion or the other. A comparison with Galam's deterministic model reveals contrasting results, especially in the presence of inflexible individuals, who never change their opinions. In particular, the $N$-player game predicts a polarized equilibrium consisting only of extremists. Using finite-size scaling analysis, we evaluate the critical exponents that determine the population size dependence of the opinion's fixation probability and mean fixation times near the threshold. The results underscore the usefulness of combining evolutionary game theory with opinion dynamics and the importance of statistical physics tools to summarize the results of Monte Carlo simulations."}, "https://arxiv.org/abs/2310.16181": {"title": "Hidden Citations Obscure True Impact in Science", "link": "https://arxiv.org/abs/2310.16181", "description": "arXiv:2310.16181v2 Announce Type: replace-cross \nAbstract: References, the mechanism scientists rely on to signal previous knowledge, lately have turned into widely used and misused measures of scientific impact. Yet, when a discovery becomes common knowledge, citations suffer from obliteration by incorporation. This leads to the concept of hidden citation, representing a clear textual credit to a discovery without a reference to the publication embodying it. Here, we rely on unsupervised interpretable machine learning applied to the full text of each paper to systematically identify hidden citations. We find that for influential discoveries hidden citations outnumber citation counts, emerging regardless of publishing venue and discipline. We show that the prevalence of hidden citations is not driven by citation counts, but rather by the degree of the discourse on the topic within the text of the manuscripts, indicating that the more discussed is a discovery, the less visible it is to standard bibliometric analysis. Hidden citations indicate that bibliometric measures offer a limited perspective on quantifying the true impact of a discovery, raising the need to extract knowledge from the full text of the scientific corpus."}, "https://arxiv.org/abs/2311.08605": {"title": "Exploring the Jungle of Bias: Political Bias Attribution in Language Models via Dependency Analysis", "link": "https://arxiv.org/abs/2311.08605", "description": "arXiv:2311.08605v2 Announce Type: replace-cross \nAbstract: The rapid advancement of Large Language Models (LLMs) has sparked intense debate regarding the prevalence of bias in these models and its mitigation. Yet, as exemplified by both results on debiasing methods in the literature and reports of alignment-related defects from the wider community, bias remains a poorly understood topic despite its practical relevance. To enhance the understanding of the internal causes of bias, we analyse LLM bias through the lens of causal fairness analysis, which enables us to both comprehend the origins of bias and reason about its downstream consequences and mitigation. To operationalize this framework, we propose a prompt-based method for the extraction of confounding and mediating attributes which contribute to the LLM decision process. By applying Activity Dependency Networks (ADNs), we then analyse how these attributes influence an LLM's decision process. We apply our method to LLM ratings of argument quality in political debates. We find that the observed disparate treatment can at least in part be attributed to confounding and mitigating attributes and model misalignment, and discuss the consequences of our findings for human-AI alignment and bias mitigation. Our code and data are at https://github.com/david-jenny/LLM-Political-Study."}, "https://arxiv.org/abs/2401.13248": {"title": "\"Here's Your Evidence\": False Consensus in Public Twitter Discussions of COVID-19 Science", "link": "https://arxiv.org/abs/2401.13248", "description": "arXiv:2401.13248v2 Announce Type: replace-cross \nAbstract: The COVID-19 pandemic brought about an extraordinary rate of scientific papers on the topic that were discussed among the general public, although often in biased or misinformed ways. In this paper, we present a mixed-methods analysis aimed at examining whether public discussions were commensurate with the scientific consensus on several COVID-19 issues. We estimate scientific consensus based on samples of abstracts from preprint servers and compare against the volume of public discussions on Twitter mentioning these papers. We find that anti-consensus posts and users, though overall less numerous than pro-consensus ones, are vastly over-represented on Twitter, thus producing a false consensus effect. This transpires with favorable papers being disproportionately amplified, along with an influx of new anti-consensus user sign-ups. Finally, our content analysis highlights that anti-consensus users misrepresent scientific findings or question scientists' integrity in their efforts to substantiate their claims."}, "https://arxiv.org/abs/2402.00447": {"title": "A Survey of Data-Efficient Graph Learning", "link": "https://arxiv.org/abs/2402.00447", "description": "arXiv:2402.00447v2 Announce Type: replace-cross \nAbstract: Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including self-supervised graph learning, semi-supervised graph learning, and few-shot graph learning. Also, we state promising directions for future research, contributing to the evolution of graph machine learning."}, "https://arxiv.org/abs/2402.15119": {"title": "A multidisciplinary framework for deconstructing bots' pluripotency in dualistic antagonism", "link": "https://arxiv.org/abs/2402.15119", "description": "arXiv:2402.15119v4 Announce Type: replace-cross \nAbstract: Anthropomorphic social bots are engineered to emulate human verbal communication and generate toxic or inflammatory content across social networking services (SNSs). Bot-disseminated misinformation could subtly yet profoundly reshape societal processes by complexly interweaving factors like repeated disinformation exposure, amplified political polarization, compromised indicators of democratic health, shifted perceptions of national identity, propagation of false social norms, and manipulation of collective memory over time. However, extrapolating bots' pluripotency across hybridized, multilingual, and heterogeneous media ecologies from isolated SNS analyses remains largely unknown, underscoring the need for a comprehensive framework to characterise bots' emergent risks to civic discourse. Here we propose an interdisciplinary framework to characterise bots' pluripotency, incorporating quantification of influence, network dynamics monitoring, and interlingual feature analysis. When applied to the geopolitical discourse around the Russo-Ukrainian conflict, results from interlanguage toxicity profiling and network analysis elucidated spatiotemporal trajectories of pro-Russian and pro-Ukrainian human and bots across hybrid SNSs. Weaponized bots predominantly inhabited X, while human primarily populated Reddit in the social media warfare. This rigorous framework promises to elucidate interlingual homogeneity and heterogeneity in bots' pluripotent behaviours, revealing synergistic human-bot mechanisms underlying regimes of information manipulation, echo chamber formation, and collective memory manifestation in algorithmically structured societies."}, "https://arxiv.org/abs/2404.15986": {"title": "Seed Selection in the Heterogeneous Moran Process", "link": "https://arxiv.org/abs/2404.15986", "description": "arXiv:2404.15986v2 Announce Type: replace-cross \nAbstract: The Moran process is a classic stochastic process that models the rise and takeover of novel traits in network-structured populations. In biological terms, a set of mutants, each with fitness $m\\in(0,\\infty)$ invade a population of residents with fitness $1$. Each agent reproduces at a rate proportional to its fitness and each offspring replaces a random network neighbor. The process ends when the mutants either fixate (take over the whole population) or go extinct. The fixation probability measures the success of the invasion. To account for environmental heterogeneity, we study a generalization of the Standard process, called the Heterogeneous Moran process. Here, the fitness of each agent is determined both by its type (resident/mutant) and the node it occupies. We study the natural optimization problem of seed selection: given a budget $k$, which $k$ agents should initiate the mutant invasion to maximize the fixation probability? We show that the problem is strongly inapproximable: it is $\\mathbf{NP}$-hard to distinguish between maximum fixation probability 0 and 1. We then focus on mutant-biased networks, where each node exhibits at least as large mutant fitness as resident fitness. We show that the problem remains $\\mathbf{NP}$-hard, but the fixation probability becomes submodular, and thus the optimization problem admits a greedy $(1-1/e)$-approximation. An experimental evaluation of the greedy algorithm along with various heuristics on real-world data sets corroborates our results."}, "https://arxiv.org/abs/2404.19634": {"title": "DF Louvain: Fast Incrementally Expanding Approach for Community Detection on Dynamic Graphs", "link": "https://arxiv.org/abs/2404.19634", "description": "arXiv:2404.19634v2 Announce Type: replace-cross \nAbstract: Community detection is the problem of recognizing natural divisions in networks. A relevant challenge in this problem is to find communities on rapidly evolving graphs. In this report we present our Parallel Dynamic Frontier (DF) Louvain algorithm, which given a batch update of edge deletions and insertions, incrementally identifies and processes an approximate set of affected vertices in the graph with minimal overhead, while using a novel approach of incrementally updating weighted-degrees of vertices and total edge weights of communities. We also present our parallel implementations of Naive-dynamic (ND) and Delta-screening (DS) Louvain. On a server with a 64-core AMD EPYC-7742 processor, our experiments show that DF Louvain obtains speedups of 179x, 7.2x, and 5.3x on real-world dynamic graphs, compared to Static, ND, and DS Louvain, respectively, and is 183x, 13.8x, and 8.7x faster, respectively, on large graphs with random batch updates. Moreover, DF Louvain improves its performance by 1.6x for every doubling of threads."}, "https://arxiv.org/abs/2405.08040": {"title": "No evidence of systematic proximity ascertainment bias in early COVID-19 cases in Wuhan Reply to Weissman (2024)", "link": "https://arxiv.org/abs/2405.08040", "description": "arXiv:2405.08040v1 Announce Type: new \nAbstract: In a short text published as Letter to the Editor of the Journal of the Royal Statistical Society Series A, Weissman (2024) argues that the finding that early COVID-19 cases without an ascertained link to Wuhan's Huanan Seafood Wholesale market resided on average closer to the market than cases epidemiologically linked to it, reveals \"major proximity ascertainment bias\". Here we show that Weissman's conclusion is based on a flawed premise, and that there is no such \"internal evidence\" of major bias. The pattern can indeed be explained by places of infection not being limited to residential neighbourhoods, and by stochasticity -- i.e., without requiring any ascertainment bias."}, "https://arxiv.org/abs/2405.08203": {"title": "Community detection in bipartite signed networks is highly dependent on parameter choice", "link": "https://arxiv.org/abs/2405.08203", "description": "arXiv:2405.08203v1 Announce Type: new \nAbstract: Decision-making processes often involve voting. Human interactions with exogenous entities such as legislations or products can be effectively modeled as two-mode (bipartite) signed networks-where people can either vote positively, negatively, or abstain from voting on the entities. Detecting communities in such networks could help us understand underlying properties: for example ideological camps or consumer preferences. While community detection is an established practice separately for bipartite and signed networks, it remains largely unexplored in the case of bipartite signed networks. In this paper, we systematically evaluate the efficacy of community detection methods on bipartite signed networks using a synthetic benchmark and real-world datasets. Our findings reveal that when no communities are present in the data, these methods often recover spurious communities. When communities are present, the algorithms exhibit promising performance, although their performance is highly susceptible to parameter choice. This indicates that researchers using community detection methods in the context of bipartite signed networks should not take the communities found at face value: it is essential to assess the robustness of parameter choices or perform domain-specific external validation."}, "https://arxiv.org/abs/2405.08331": {"title": "Are Generics and Negativity about Social Groups Common on Social Media? A Comparative Analysis of Twitter (X) Data", "link": "https://arxiv.org/abs/2405.08331", "description": "arXiv:2405.08331v1 Announce Type: new \nAbstract: Generics (unquantified generalizations) are thought to be pervasive in communication and when they are about social groups, this may offend and polarize people because generics gloss over variations between individuals. Generics about social groups might be particularly common on Twitter (X). This remains unexplored, however. Using machine learning (ML) techniques, we therefore developed an automatic classifier for social generics, applied it to more than a million tweets about people, and analyzed the tweets. We found that most tweets (78%) about people contained no generics. However, tweets with social generics received more 'likes' and retweets. Furthermore, while recent psychological research may lead to the prediction that tweets with generics about political groups are more common than tweets with generics about ethnic groups, we found the opposite. However, consistent with recent claims that political animosity is less constrained by social norms than animosity against gender and ethnic groups, negative tweets with generics about political groups were significantly more prevalent and retweeted than negative tweets about ethnic groups. Our study provides the first ML-based insights into the use and impact of social generics on Twitter."}, "https://arxiv.org/abs/2405.08398": {"title": "Exploring the spatial segmentation of housing markets from online listings", "link": "https://arxiv.org/abs/2405.08398", "description": "arXiv:2405.08398v1 Announce Type: new \nAbstract: The real estate market shows an inherent connection to space. Real estate agencies unevenly operate and specialize across space, price and type of properties, thereby segmenting the market into submarkets. We introduce here a methodology based on multipartite networks to detect the spatial segmentation emerging from data on housing online listings. Considering the spatial information of the listings, we build a bipartite network that connects agencies and spatial units. This bipartite network is projected into a network of spatial units, whose connections account for similarities in the agency ecosystem. We then apply clustering methods to this network to segment markets into spatially-coherent regions, which are found to be robust across different clustering detection algorithms, discretization of space and spatial scales, and across countries with case studies in France and Spain. This methodology addresses the long-standing issue of housing market segmentation, relevant in disciplines such as urban studies and spatial economics, and with implications for policymaking."}, "https://arxiv.org/abs/2405.08746": {"title": "Decomposing geographical and universal aspects of human mobility", "link": "https://arxiv.org/abs/2405.08746", "description": "arXiv:2405.08746v1 Announce Type: new \nAbstract: Driven by access to large volumes of detailed movement data, the study of human mobility has grown rapidly over the past decade. This body of work has argued that human mobility is scale-free, has proposed models to generate scale-free moving distance distribution, and explained how the scale-free distribution arises from aggregating displacements across scales. However, the field of human mobility has not explicitly addressed how mobility is structured by geographical constraints - such as the outlines of landmasses, lakes, rivers, the placement of buildings, roadways, and cities.\n  Using unique datasets capturing millions of movements between precise locations, this paper shows how separating the effect of geography from mobility choices reveals a universal power law spanning five orders of magnitude (from 10 m to 1,000,000 m). We incorporate geography through the pair distribution function, a fundamental quantity from condensed matter physics that encapsulates the structure of locations on which mobility occurs. This distribution captures the constraints that geography places on human mobility across different length scales.\n  Our description conclusively addresses debates between distance-based and opportunity-based perspectives on human mobility. By demonstrating how the spatial distribution of human settlements shapes human mobility, we provide a novel perspective that bridges the gap between these previously opposing ideas."}, "https://arxiv.org/abs/2405.08013": {"title": "CTRL: Continuous-Time Representation Learning on Temporal Heterogeneous Information Network", "link": "https://arxiv.org/abs/2405.08013", "description": "arXiv:2405.08013v1 Announce Type: cross \nAbstract: Inductive representation learning on temporal heterogeneous graphs is crucial for scalable deep learning on heterogeneous information networks (HINs) which are time-varying, such as citation networks. However, most existing approaches are not inductive and thus cannot handle new nodes or edges. Moreover, previous temporal graph embedding methods are often trained with the temporal link prediction task to simulate the link formation process of temporal graphs, while ignoring the evolution of high-order topological structures on temporal graphs. To fill these gaps, we propose a Continuous-Time Representation Learning (CTRL) model on temporal HINs. To preserve heterogeneous node features and temporal structures, CTRL integrates three parts in a single layer, they are 1) a \\emph{heterogeneous attention} unit that measures the semantic correlation between nodes, 2) a \\emph{edge-based Hawkes process} to capture temporal influence between heterogeneous nodes, and 3) \\emph{dynamic centrality} that indicates the dynamic importance of a node. We train the CTRL model with a future event (a subgraph) prediction task to capture the evolution of the high-order network structure. Extensive experiments have been conducted on three benchmark datasets. The results demonstrate that our model significantly boosts performance and outperforms various state-of-the-art approaches. Ablation studies are conducted to demonstrate the effectiveness of the model design."}, "https://arxiv.org/abs/2405.08278": {"title": "Facilitating Feature and Topology Lightweighting: An Ethereum Transaction Graph Compression Method for Malicious Account Detection", "link": "https://arxiv.org/abs/2405.08278", "description": "arXiv:2405.08278v1 Announce Type: cross \nAbstract: Ethereum has become one of the primary global platforms for cryptocurrency, playing an important role in promoting the diversification of the financial ecosystem. However, the relative lag in regulation has led to a proliferation of malicious activities in Ethereum, posing a serious threat to fund security. Existing regulatory methods usually detect malicious accounts through feature engineering or large-scale transaction graph mining. However, due to the immense scale of transaction data and malicious attacks, these methods suffer from inefficiency and low robustness during data processing and anomaly detection. In this regard, we propose an Ethereum Transaction Graph Compression method named TGC4Eth, which assists malicious account detection by lightweighting both features and topology of the transaction graph. At the feature level, we select transaction features based on their low importance to improve the robustness of the subsequent detection models against feature evasion attacks; at the topology level, we employ focusing and coarsening processes to compress the structure of the transaction graph, thereby improving both data processing and inference efficiency of detection models. Extensive experiments demonstrate that TGC4Eth significantly improves the computational efficiency of existing detection models while preserving the connectivity of the transaction graph. Furthermore, TGC4Eth enables existing detection models to maintain stable performance and exhibit high robustness against feature evasion attacks."}, "https://arxiv.org/abs/2405.08465": {"title": "How to Surprisingly Consider Recommendations? A Knowledge-Graph-based Approach Relying on Complex Network Metrics", "link": "https://arxiv.org/abs/2405.08465", "description": "arXiv:2405.08465v1 Announce Type: cross \nAbstract: Traditional recommendation proposals, including content-based and collaborative filtering, usually focus on similarity between items or users. Existing approaches lack ways of introducing unexpectedness into recommendations, prioritizing globally popular items over exposing users to unforeseen items. This investigation aims to design and evaluate a novel layer on top of recommender systems suited to incorporate relational information and suggest items with a user-defined degree of surprise. We propose a Knowledge Graph (KG) based recommender system by encoding user interactions on item catalogs. Our study explores whether network-level metrics on KGs can influence the degree of surprise in recommendations. We hypothesize that surprisingness correlates with certain network metrics, treating user profiles as subgraphs within a larger catalog KG. The achieved solution reranks recommendations based on their impact on structural graph metrics. Our research contributes to optimizing recommendations to reflect the metrics. We experimentally evaluate our approach on two datasets of LastFM listening histories and synthetic Netflix viewing profiles. We find that reranking items based on complex network metrics leads to a more unexpected and surprising composition of recommendation lists."}, "https://arxiv.org/abs/2405.08515": {"title": "Precarious Experiences: Citizens' Frustrations, Anxieties and Burdens of an Online Welfare Benefit System", "link": "https://arxiv.org/abs/2405.08515", "description": "arXiv:2405.08515v1 Announce Type: cross \nAbstract: There is a significant overlap between people who are supported by income-related social welfare benefits, often in precarious situations, and those who experience greater digital exclusion. We report on a study of claimants using the UK's Universal Credit online welfare benefit system designed as, and still, \"digital by default\". Through data collection involving remote interviews (n=11) and online surveys (n=66), we expose claimants' own lived experiences interacting with this system. The claimants explain how digital channels can contribute to an imbalance of power and agency, at a time when their own circumstances mean they have reduced abilities, resources and capacities, and where design choices can adversely affect people's utility to leverage help from their own wider socio-technical ecosystems. We contribute eight recommendations from these accounts to inform the future design and development of digital welfare benefit systems for this population, to reduce digital barriers and harms."}, "https://arxiv.org/abs/2405.08784": {"title": "Refinement of an Epilepsy Dictionary through Human Annotation of Health-related posts on Instagram", "link": "https://arxiv.org/abs/2405.08784", "description": "arXiv:2405.08784v1 Announce Type: cross \nAbstract: We used a dictionary built from biomedical terminology extracted from various sources such as DrugBank, MedDRA, MedlinePlus, TCMGeneDIT, to tag more than 8 million Instagram posts by users who have mentioned an epilepsy-relevant drug at least once, between 2010 and early 2016. A random sample of 1,771 posts with 2,947 term matches was evaluated by human annotators to identify false-positives. OpenAI's GPT series models were compared against human annotation. Frequent terms with a high false-positive rate were removed from the dictionary. Analysis of the estimated false-positive rates of the annotated terms revealed 8 ambiguous terms (plus synonyms) used in Instagram posts, which were removed from the original dictionary. To study the effect of removing those terms, we constructed knowledge networks using the refined and the original dictionaries and performed an eigenvector-centrality analysis on both networks. We show that the refined dictionary thus produced leads to a significantly different rank of important terms, as measured by their eigenvector-centrality of the knowledge networks. Furthermore, the most important terms obtained after refinement are of greater medical relevance. In addition, we show that OpenAI's GPT series models fare worse than human annotators in this task."}, "https://arxiv.org/abs/2305.02902": {"title": "Biased versus unbiased numerical methods for stochastic simulations", "link": "https://arxiv.org/abs/2305.02902", "description": "arXiv:2305.02902v2 Announce Type: replace \nAbstract: Approximate numerical methods are one of the most used strategies to extract information from many-interacting-agents systems. In particular, numerical approximations are of extended use to deal with epidemic, ecological and biological models, since unbiased methods like the Gillespie algorithm can become unpractical due to high CPU time usage required. However, the use of approximations has been debated and there is no clear consensus about whether unbiased methods or biased approach is the best option. In this work, we derive scaling relations for the errors in approximations based on binomial extractions. This finding allows us to build rules to compute the optimal values of both the discretization time and number of realizations needed to compute averages with the biased method with a target precision and minimum CPU-time usage. Furthermore, we also present another rule to discern whether the unbiased method or biased approach is more efficient. Ultimately, we will show that the choice of the method should depend on the desired precision for the estimation of averages."}, "https://arxiv.org/abs/2312.11529": {"title": "Efficient and Scalable Graph Generation through Iterative Local Expansion", "link": "https://arxiv.org/abs/2312.11529", "description": "arXiv:2312.11529v4 Announce Type: replace \nAbstract: In the realm of generative models for graphs, extensive research has been conducted. However, most existing methods struggle with large graphs due to the complexity of representing the entire joint distribution across all node pairs and capturing both global and local graph structures simultaneously. To overcome these issues, we introduce a method that generates a graph by progressively expanding a single node to a target graph. In each step, nodes and edges are added in a localized manner through denoising diffusion, building first the global structure, and then refining the local details. The local generation avoids modeling the entire joint distribution over all node pairs, achieving substantial computational savings with subquadratic runtime relative to node count while maintaining high expressivity through multiscale generation. Our experiments show that our model achieves state-of-the-art performance on well-established benchmark datasets while successfully scaling to graphs with at least 5000 nodes. Our method is also the first to successfully extrapolate to graphs outside of the training distribution, showcasing a much better generalization capability over existing methods."}, "https://arxiv.org/abs/2404.00793": {"title": "Learning the mechanisms of network growth", "link": "https://arxiv.org/abs/2404.00793", "description": "arXiv:2404.00793v2 Announce Type: replace \nAbstract: We propose a novel model-selection method for dynamic networks. Our approach involves training a classifier on a large body of synthetic network data. The data is generated by simulating nine state-of-the-art random graph models for dynamic networks, with parameter range chosen to ensure exponential growth of the network size in time. We design a conceptually novel type of dynamic features that count new links received by a group of vertices in a particular time interval. The proposed features are easy to compute, analytically tractable, and interpretable. Our approach achieves a near-perfect classification of synthetic networks, exceeding the state-of-the-art by a large margin. Applying our classification method to real-world citation networks gives credibility to the claims in the literature that models with preferential attachment, fitness and aging fit real-world citation networks best, although sometimes, the predicted model does not involve vertex fitness."}, "https://arxiv.org/abs/2405.01514": {"title": "Valuing maintenance strategies for fusion plants as part of a future electricity grid", "link": "https://arxiv.org/abs/2405.01514", "description": "arXiv:2405.01514v2 Announce Type: replace \nAbstract: Scheduled maintenance is likely to be lengthy and therefore consequential for the economics of fusion power plants. The maintenance strategy that maximizes the economic value of a plant depends on internal factors such as the cost and durability of the replaceable components, the frequency and duration of the maintenance blocks, and the external factors of the electricity system in which the plant operates. This paper examines the value of fusion power plants with various maintenance properties in a decarbonized United States Eastern Interconnection circa 2050. Seasonal variations in electricity supply and demand mean that certain times of year, particularly spring to early summer, are best for scheduled maintenance. Seasonality has two important consequences. First, the value of a plant can be 15% higher than what one would naively expect if value were directly proportional to its availability. Second, in some cases, replacing fractions of a component in shorter maintenance blocks spread over multiple years is better than replacing it all at once during a longer outage, even through the overall availability of the plant is lower in the former scenario."}, "https://arxiv.org/abs/2401.10057": {"title": "A method for characterizing disease emergence curves from paired pathogen detection and serology data", "link": "https://arxiv.org/abs/2401.10057", "description": "arXiv:2401.10057v2 Announce Type: replace-cross \nAbstract: Wildlife disease surveillance programs and research studies track infection and identify risk factors for wild populations, humans, and agriculture. Often, several types of samples are collected from individuals to provide more complete information about an animal's infection history. Methods that jointly analyze multiple data streams to study disease emergence and drivers of infection via epidemiological process models remain underdeveloped. Joint-analysis methods can more thoroughly analyze all available data, more precisely quantifying epidemic processes, outbreak status, and risks. We contribute a paired data modeling approach that analyzes multiple samples from individuals. We use \"characterization maps\" to link paired data to epidemiological processes through a hierarchical statistical observation model. Our approach can provide both Bayesian and frequentist estimates of epidemiological parameters and state. We motivate our approach through the need to use paired pathogen and antibody detection tests to estimate parameters and infection trajectories for the widely applicable susceptible, infectious, recovered (SIR) model. We contribute general formulas to link characterization maps to arbitrary process models and datasets and an extended SIR model that better accommodates paired data. We find via simulation that paired data can more efficiently estimate SIR parameters than unpaired data, requiring samples from 5-10 times fewer individuals. We then study SARS-CoV-2 in wild White-tailed deer (Odocoileus virginianus) from three counties in the United States. Estimates for average infectious times corroborate captive animal studies. Our methods use general statistical theory to let applications extend beyond the SIR model we consider, and to more complicated examples of paired data."}, "https://arxiv.org/abs/2405.00808": {"title": "ReeSPOT: Reeb Graph Models Semantic Patterns of Normalcy in Human Trajectories", "link": "https://arxiv.org/abs/2405.00808", "description": "arXiv:2405.00808v2 Announce Type: replace-cross \nAbstract: This paper introduces ReeSPOT, a novel Reeb graph-based method to model patterns of life in human trajectories (akin to a fingerprint). Human behavior typically follows a pattern of normalcy in day-to-day activities. This is marked by recurring activities within specific time periods. In this paper, we model this behavior using Reeb graphs where any deviation from usual day-to-day activities is encoded as nodes in the Reeb graph. The complexity of the proposed algorithm is linear with respect to the number of time points in a given trajectory. We demonstrate the usage of ReeSPOT and how it captures the critically significant spatial and temporal deviations using the nodes of the Reeb graph. Our case study presented in this paper includes realistic human movement scenarios: visiting uncommon locations, taking odd routes at infrequent times, uncommon time visits, and uncommon stay durations. We analyze the Reeb graph to interpret the topological structure of the GPS trajectories. Potential applications of ReeSPOT include urban planning, security surveillance, and behavioral research."}, "https://arxiv.org/abs/2405.09185": {"title": "Influence Maximization in Hypergraphs Using A Genetic Algorithm with New Initialization and Evaluation Methods", "link": "https://arxiv.org/abs/2405.09185", "description": "arXiv:2405.09185v1 Announce Type: new \nAbstract: Influence maximization (IM) is a crucial optimization task related to analyzing complex networks in the real world, such as social networks, disease propagation networks, and marketing networks. Publications to date about the IM problem focus mainly on graphs, which fail to capture high-order interaction relationships from the real world. Therefore, the use of hypergraphs for addressing the IM problem has been receiving increasing attention. However, identifying the most influential nodes in hypergraphs remains challenging, mainly because nodes and hyperedges are often strongly coupled and correlated. In this paper, to effectively identify the most influential nodes, we first propose a novel hypergraph-independent cascade model that integrates the influences of both node and hyperedge failures. Afterward, we introduce genetic algorithms (GA) to identify the most influential nodes that leverage hypergraph collective influences. In the GA-based method, the hypergraph collective influence is effectively used to initialize the population, thereby enhancing the quality of initial candidate solutions. The designed fitness function considers the joint influences of both nodes and hyperedges. This ensures the optimal set of nodes with the best influence on both nodes and hyperedges to be evaluated accurately. Moreover, a new mutation operator is designed by introducing factors, i.e., the collective influence and overlapping effects of nodes in hypergraphs, to breed high-quality offspring. In the experiments, several simulations on both synthetic and real hypergraphs have been conducted, and the results demonstrate that the proposed method outperforms the compared methods."}, "https://arxiv.org/abs/2405.09357": {"title": "A universal optimization framework based on cycle ranking for influence maximization in complex networks", "link": "https://arxiv.org/abs/2405.09357", "description": "arXiv:2405.09357v1 Announce Type: new \nAbstract: Influence maximization aims to identify a set of influential individuals, referred to as influencers, as information sources to maximize the spread of information within networks, constituting a vital combinatorial optimization problem with extensive practical applications and sustained interdisciplinary interest. Diverse approaches have been devised to efficiently address this issue, one of which involves selecting the influencers from a given centrality ranking. In this paper, we propose a novel optimization framework based on ranking basic cycles in networks, capable of selecting the influencers from diverse centrality measures. The experimental results demonstrate that, compared to directly selecting the top-k nodes from centrality sequences and other state-of-the-art optimization approaches, the new framework can expand the dissemination range by 1.5 to 3 times. Counterintuitively, it exhibits minimal hub property, with the average distance between influencers being only one-third of alternative approaches, regardless of the centrality metrics or network types. Our study not only paves the way for novel strategies in influence maximization but also underscores the unique potential of underappreciated cycle structures."}, "https://arxiv.org/abs/2405.09488": {"title": "Nonequilibrium phase transitions and absorbing states in a model for the dynamics of religious affiliation", "link": "https://arxiv.org/abs/2405.09488", "description": "arXiv:2405.09488v1 Announce Type: new \nAbstract: We propose a simple model to describe the dynamics of religious affiliation. For such purpose, we built a compartmental model with three distinct subpopulations, namely religious committed individuals, religious noncommitted individuals and not religious affiliated individuals. The transitions among the compartments are governed by probabilities, modeling social interactions among the groups and also spontaneous transitions among the compartments. First of all, we consider the model on a fully-connected network. Thus, we write a set of ordinary differential equations to study the evolution of the subpopulations. Our analytical and numerical results show that there is an absorbing state in the model where only one of the subpopulations survive in the long-time limit. There are also regions of parameters where some of the subpopulations coexist (two or three). We also verified the occurrence of two distinct critical points. In addition, we also present Monte Carlo simulations of the model on two-dimensional square lattices, in order to analyze the impact of the presence of a lattice structure on the critical behavior of the model. Comparison of the models' results with data for religious affiliation in Northern Ireland shows a good qualitative agreement. Finally, we considered the presence of inflexible individuals in the population, i.e., individuals that never change their states. The impact of such special agents on the critical behavior of the model is also discussed."}, "https://arxiv.org/abs/2405.08830": {"title": "Evaluating Supply Chain Resilience During Pandemic Using Agent-based Simulation", "link": "https://arxiv.org/abs/2405.08830", "description": "arXiv:2405.08830v1 Announce Type: cross \nAbstract: Recent pandemics have highlighted vulnerabilities in our global economic systems, especially supply chains. Possible future pandemic raises a dilemma for businesses owners between short-term profitability and long-term supply chain resilience planning. In this study, we propose a novel agent-based simulation model integrating extended Susceptible-Infected-Recovered (SIR) epidemiological model and supply and demand economic model to evaluate supply chain resilience strategies during pandemics. Using this model, we explore a range of supply chain resilience strategies under pandemic scenarios using in silico experiments. We find that a balanced approach to supply chain resilience performs better in both pandemic and non-pandemic times compared to extreme strategies, highlighting the importance of preparedness in the form of a better supply chain resilience. However, our analysis shows that the exact supply chain resilience strategy is hard to obtain for each firm and is relatively sensitive to the exact profile of the pandemic and economic state at the beginning of the pandemic. As such, we used a machine learning model that uses the agent-based simulation to estimate a near-optimal supply chain resilience strategy for a firm. The proposed model offers insights for policymakers and businesses to enhance supply chain resilience in the face of future pandemics, contributing to understanding the trade-offs between short-term gains and long-term sustainability in supply chain management before and during pandemics."}, "https://arxiv.org/abs/2405.09529": {"title": "Artificial Intelligence for the Internal Democracy of Political Parties", "link": "https://arxiv.org/abs/2405.09529", "description": "arXiv:2405.09529v1 Announce Type: cross \nAbstract: The article argues that AI can enhance the measurement and implementation of democratic processes within political parties, known as Intra-Party Democracy (IPD). It identifies the limitations of traditional methods for measuring IPD, which often rely on formal parameters, self-reported data, and tools like surveys. Such limitations lead to the collection of partial data, rare updates, and significant demands on resources. To address these issues, the article suggests that specific data management and Machine Learning (ML) techniques, such as natural language processing and sentiment analysis, can improve the measurement (ML about) and practice (ML for) of IPD. The article concludes by considering some of the principal risks of ML for IPD, including concerns over data privacy, the potential for manipulation, and the dangers of overreliance on technology."}, "https://arxiv.org/abs/2310.16451": {"title": "The Small-World Effect for Interferometer Networks", "link": "https://arxiv.org/abs/2310.16451", "description": "arXiv:2310.16451v2 Announce Type: replace \nAbstract: Complex network theory has focused on properties of networks with real-valued edge weights. However, in signal transfer networks, such as those representing the transfer of light across an interferometer, complex-valued edge weights are needed to represent the manipulation of the signal in both magnitude and phase. These complex-valued edge weights introduce interference into the signal transfer, but it is unknown how such interference affects network properties such as small-worldness. To address this gap, we have introduced a small-world interferometer network model with complex-valued edge weights and generalized existing network measures to define the interferometric clustering coefficient, the apparent path length, and the interferometric small-world coefficient. Using high-performance computing resources, we generated a large set of small-world interferometers over a wide range of parameters in system size, nearest-neighbor count, and edge-weight phase and computed their interferometric network measures. We found that the interferometric small-world coefficient depends significantly on the amount of phase on complex-valued edge weights: for small edge-weight phases, constructive interference led to a higher interferometric small-world coefficient; while larger edge-weight phases induced destructive interference which led to a lower interferometric small-world coefficient. Thus, for the small-world interferometer model, interferometric measures are necessary to capture the effect of interference on signal transfer. This model is an example of the type of problem that necessitates interferometric measures, and applies to any wave-based network including quantum networks."}, "https://arxiv.org/abs/2403.08493": {"title": "Rumor Forwarding Prediction Model Based on Uncertain Time Series", "link": "https://arxiv.org/abs/2403.08493", "description": "arXiv:2403.08493v2 Announce Type: replace \nAbstract: The rapid spread of rumors in social media is mainly caused by individual retweets. This paper applies uncertainty time series analysis (UTSA) to analyze a rumor retweeting behavior on Weibo. First, the rumor forwarding is modeled using uncertain time series, including order selection, parameter estimation, residual analysis, uncertainty hypothesis testing and forecast, and the validity of using uncertain time series analysis is further supported by analyzing the characteristics of the residual plot. The experimental results show that the uncertain time series can better predict the next stage of rumor forwarding. The results of the study have important practical significance for rumor management and the management of social media information dissemination."}, "https://arxiv.org/abs/2306.05597": {"title": "On the implementation of zero-determinant strategies in repeated games", "link": "https://arxiv.org/abs/2306.05597", "description": "arXiv:2306.05597v2 Announce Type: replace-cross \nAbstract: Zero-determinant strategies are a class of strategies in repeated games which unilaterally control payoffs. Zero-determinant strategies have attracted much attention in studies of social dilemma, particularly in the context of evolution of cooperation. So far, not only general properties of zero-determinant strategies have been investigated, but zero-determinant strategies have been applied to control in the fields of information and communications technology and analysis of imitation. Here, we further deepen our understanding on general mathematical properties of zero-determinant strategies. We first prove that zero-determinant strategies, if exist, can be implemented by some one-dimensional transition probability. Next, we prove that, if a two-player game has a non-trivial potential function, a zero-determinant strategy exists in its repeated version. These results assist us to implement zero-determinant strategies in broader situations."}, "https://arxiv.org/abs/2405.09640": {"title": "Personalized Content Moderation and Emergent Outcomes", "link": "https://arxiv.org/abs/2405.09640", "description": "arXiv:2405.09640v1 Announce Type: new \nAbstract: Social media platforms have implemented automated content moderation tools to preserve community norms and mitigate online hate and harassment. Recently, these platforms have started to offer Personalized Content Moderation (PCM), granting users control over moderation settings or aligning algorithms with individual user preferences. While PCM addresses the limitations of the one-size-fits-all approach and enhances user experiences, it may also impact emergent outcomes on social media platforms. Our study reveals that PCM leads to asymmetric information loss (AIL), potentially impeding the development of a shared understanding among users, crucial for healthy community dynamics. We further demonstrate that PCM tools could foster the creation of echo chambers and filter bubbles, resulting in increased community polarization. Our research is the first to identify AIL as a consequence of PCM and to highlight its potential negative impacts on online communities."}, "https://arxiv.org/abs/2405.09643": {"title": "Energy Consumption of Plant Factory with Artificial Light: Challenges and Opportunities", "link": "https://arxiv.org/abs/2405.09643", "description": "arXiv:2405.09643v1 Announce Type: new \nAbstract: Plant factory with artificial light (PFAL) is a promising technology for relieving the food crisis, especially in urban areas or arid regions endowed with abundant resources. However, lighting and HVAC (heating, ventilation, and air conditioning) systems of PFAL have led to much greater energy consumption than open-field and greenhouse farming, limiting the application of PFAL to a wider extent. Recent researches pay much more attention to the optimization of energy consumption in order to develop and promote the PFAL technology with reduced energy usage. This work comprehensively summarizes the current energy-saving methods on lighting, HVAC systems, as well as their coupling methods for a more energy-efficient PFAL. Besides, we offer our perspectives on further energy-saving strategies and exploit the renewable energy resources for PFAL to respond to the urgent need for energy-efficient production."}, "https://arxiv.org/abs/2405.09978": {"title": "Pedestrian evacuations with imitation of cooperative behavior", "link": "https://arxiv.org/abs/2405.09978", "description": "arXiv:2405.09978v1 Announce Type: new \nAbstract: We analyze the dynamics of room evacuation for mixed populations that include both competitive and cooperative individuals through numerical simulations using the social force model. Cooperative agents represent well-trained individuals who know how to behave in order to reduce risks within high-density crowds. We consider that competitive agents can imitate cooperative behavior when they are in close proximity to cooperators. We study the effects of the imitation of cooperative behavior on the duration and safety of evacuations, analyzing evacuation time and other quantities of interest for varying parameters such as the proportions of mixing, the aspect ratio of the room, and the parameters characterizing individual behaviors. Our main findings reveal that the addition of a relatively small number of cooperative agents into a crowd can reduce evacuation time and the density near the exit door, making the evacuation faster and safer despite an increase in the total number of agents. In particular, for long spaces such as corridors, a small number of added cooperative agents can significantly facilitate the evacuation process. We compare our results with those of systems without imitation and also study the general role of cooperation, providing further analysis for homogeneous populations. Our main conclusions emphasize the potential relevance of training people how to behave in high-density crowds"}, "https://arxiv.org/abs/2405.10187": {"title": "Influence Maximization in Hypergraphs using Multi-Objective Evolutionary Algorithms", "link": "https://arxiv.org/abs/2405.10187", "description": "arXiv:2405.10187v1 Announce Type: new \nAbstract: The Influence Maximization (IM) problem is a well-known NP-hard combinatorial problem over graphs whose goal is to find the set of nodes in a network that spreads influence at most. Among the various methods for solving the IM problem, evolutionary algorithms (EAs) have been shown to be particularly effective. While the literature on the topic is particularly ample, only a few attempts have been made at solving the IM problem over higher-order networks, namely extensions of standard graphs that can capture interactions that involve more than two nodes. Hypergraphs are a valuable tool for modeling complex interaction networks in various domains; however, they require rethinking of several graph-based problems, including IM. In this work, we propose a multi-objective EA for the IM problem over hypergraphs that leverages smart initialization and hypergraph-aware mutation. While the existing methods rely on greedy or heuristic methods, to our best knowledge this is the first attempt at applying EAs to this problem. Our results over nine real-world datasets and three propagation models, compared with five baseline algorithms, reveal that our method achieves in most cases state-of-the-art results in terms of hypervolume and solution diversity."}, "https://arxiv.org/abs/2405.10213": {"title": "Words as Trigger Points in Social Media Discussions", "link": "https://arxiv.org/abs/2405.10213", "description": "arXiv:2405.10213v1 Announce Type: new \nAbstract: Trigger points are a concept introduced by Mau, Lux, and Westheuser (2023) to study qualitative focus group interviews and understand polarisation in Germany. When people communicate, trigger points represent moments when individuals feel that their understanding of what is fair, normal, or appropriate in society is questioned. In the original studies, individuals react affectively to such triggers and show strong and negative emotional responses. In this paper, we introduce the first systematic study of the large-scale effect of individual words as trigger points by analysing a large amount of social media posts. We examine online deliberations on Reddit between 2020 and 2022 and collect >100 million posts from subreddits related to a set of words identified as trigger points in UK politics. We find that such trigger words affect user engagement and have noticeable consequences on animosity in online discussions. We share empirical evidence of trigger words causing animosity, and how they provide incentives for hate speech, adversarial debates, and disagreements. Our work is the first to introduce trigger points to computational studies of online communication. Our findings are relevant to researchers interested in online harms and who examine how citizens debate politics and society in light of affective polarisation."}, "https://arxiv.org/abs/2405.10233": {"title": "iDRAMA-Scored-2024: A Dataset of the Scored Social Media Platform from 2020 to 2023", "link": "https://arxiv.org/abs/2405.10233", "description": "arXiv:2405.10233v1 Announce Type: new \nAbstract: Online web communities often face bans for violating platform policies, encouraging their migration to alternative platforms. This migration, however, can result in increased toxicity and unforeseen consequences on the new platform. In recent years, researchers have collected data from many alternative platforms, indicating coordinated efforts leading to offline events, conspiracy movements, hate speech propagation, and harassment. Thus, it becomes crucial to characterize and understand these alternative platforms. To advance research in this direction, we collect and release a large-scale dataset from Scored -- an alternative Reddit platform that sheltered banned fringe communities, for example, c/TheDonald (a prominent right-wing community) and c/GreatAwakening (a conspiratorial community). Over four years, we collected approximately 57M posts from Scored, with at least 58 communities identified as migrating from Reddit and over 950 communities created since the platform's inception. Furthermore, we provide sentence embeddings of all posts in our dataset, generated through a state-of-the-art model, to further advance the field in characterizing the discussions within these communities. We aim to provide these resources to facilitate their investigations without the need for extensive data collection and processing efforts."}, "https://arxiv.org/abs/2405.09982": {"title": "Dynamical behavior and optimal control of a stochastic SAIRS epidemic model with two saturated incidences", "link": "https://arxiv.org/abs/2405.09982", "description": "arXiv:2405.09982v1 Announce Type: cross \nAbstract: Stochastic models are widely used to investigate the spread of epidemics in a complex environment. This paper extends a deterministic SAIRS epidemic model to a stochastic case with limited patient capacity and exposure. We first study the dynamical properties of the model under certain conditions, including persistence, extinction, and ergodic. Then, we introduce vaccination and isolation into the model as control variables. The optimal control strategies are obtained based on the Pontryagin minimum principle. Finally, numerical simulations are given to illustrate our theoretical results."}, "https://arxiv.org/abs/2307.04612": {"title": "Emergence of Cooperation in Two-agent Repeated Games with Reinforcement Learning", "link": "https://arxiv.org/abs/2307.04612", "description": "arXiv:2307.04612v2 Announce Type: replace \nAbstract: Cooperation is the foundation of ecosystems and the human society, and the reinforcement learning provides crucial insight into the mechanism for its emergence. However, most previous work has mostly focused on the self-organization at the population level, the fundamental dynamics at the individual level remains unclear. Here, we investigate the evolution of cooperation in a two-agent system, where each agent pursues optimal policies according to the classical Q-learning algorithm in playing the strict prisoner's dilemma. We reveal that a strong memory and long-sighted expectation yield the emergence of Coordinated Optimal Policies (COPs), where both agents act like Win-Stay, Lose-Shift (WSLS) to maintain a high level of cooperation. Otherwise, players become tolerant toward their co-player's defection and the cooperation loses stability in the end where the policy all Defection (All-D) prevails. This suggests that tolerance could be a good precursor to a crisis in cooperation. Furthermore, our analysis shows that the Coordinated Optimal Modes (COMs) for different COPs gradually lose stability as memory weakens and expectation for the future decreases, where agents fail to predict co-player's action in games and defection dominates. As a result, we give the constraint to expectations of future and memory strength for maintaining cooperation. In contrast to the previous work, the impact of exploration on cooperation is found not be consistent, but depends on composition of COMs. By clarifying these fundamental issues in this two-player system, we hope that our work could be helpful for understanding the emergence and stability of cooperation in more complex scenarios in reality."}, "https://arxiv.org/abs/2401.12732": {"title": "CDRNP: Cross-Domain Recommendation to Cold-Start Users via Neural Process", "link": "https://arxiv.org/abs/2401.12732", "description": "arXiv:2401.12732v2 Announce Type: replace-cross \nAbstract: Cross-domain recommendation (CDR) has been proven as a promising way to tackle the user cold-start problem, which aims to make recommendations for users in the target domain by transferring the user preference derived from the source domain. Traditional CDR studies follow the embedding and mapping (EMCDR) paradigm, which transfers user representations from the source to target domain by learning a user-shared mapping function, neglecting the user-specific preference. Recent CDR studies attempt to learn user-specific mapping functions in meta-learning paradigm, which regards each user's CDR as an individual task, but neglects the preference correlations among users, limiting the beneficial information for user representations. Moreover, both of the paradigms neglect the explicit user-item interactions from both domains during the mapping process. To address the above issues, this paper proposes a novel CDR framework with neural process (NP), termed as CDRNP. Particularly, it develops the meta-learning paradigm to leverage user-specific preference, and further introduces a stochastic process by NP to capture the preference correlations among the overlapping and cold-start users, thus generating more powerful mapping functions by mapping the user-specific preference and common preference correlations to a predictive probability distribution. In addition, we also introduce a preference remainer to enhance the common preference from the overlapping users, and finally devises an adaptive conditional decoder with preference modulation to make prediction for cold-start users with items in the target domain. Experimental results demonstrate that CDRNP outperforms previous SOTA methods in three real-world CDR scenarios."}, "https://arxiv.org/abs/2405.10322": {"title": "Exploring the Independent Cascade Model and Its Evolution in Social Network Information Diffusion", "link": "https://arxiv.org/abs/2405.10322", "description": "arXiv:2405.10322v1 Announce Type: new \nAbstract: This paper delves into the paramount significance of information dissemination within the dynamic realm of social networks. It underscores the pivotal role of information communication models in unraveling the intricacies of data propagation in the digital age. By shedding light on the profound influence of these models, it not only lays the groundwork for exploring various hierarchies and their manifestations but also serves as a catalyst for further research in this formidable field."}, "https://arxiv.org/abs/2405.10338": {"title": "Financial Interactions and Capital Accumulation", "link": "https://arxiv.org/abs/2405.10338", "description": "arXiv:2405.10338v1 Announce Type: new \nAbstract: In a series of precedent papers, we have presented a comprehensive methodology, termed Field Economics, for translating a standard economic model into a statistical field-formalism framework. This formalism requires a large number of heterogeneous agents, possibly of different types. It reveals the emergence of collective states among these agents or type of agents while preserving the interactions and microeconomic features of the system at the individual level. In two prior papers, we applied this formalism to analyze the dynamics of capital allocation and accumulation in a simple microeconomic framework of investors and firms.Building upon our prior work, the present paper refines the initial model by expanding its scope. Instead of considering financial firms investing solely in real sectors, we now suppose that financial agents may also invest in other financial firms. We also introduce banks in the system that act as investors with a credit multiplier. Two types of interaction are now considered within the financial sector: financial agents can lend capital to, or choose to buy shares of, other financial firms. Capital now flows between financial agents and is only partly invested in real sectors, depending on their relative returns. We translate this framework into our formalism and study the diffusion of capital and possible defaults in the system, both at the macro and micro level.At the macro level, we find that several collective states may emerge, each characterized by a distinct level of average capital and investors per sector. These collective states depend on external parameters such as level of connections between investors or firms' productivity.The multiplicity of possible collective states is the consequence of the nature of the system composed of interconnected heterogeneous agents. Several equivalent patterns of returns and portfolio allocation may emerge. The multiple collective states induce the unstable nature of financial markets, and some of them include defaults may emerge. At the micro level, we study the propagation of returns and defaults within a given collective state. Our findings highlight the significant role of banks, which can either stabilize the system through lending activities or propagate instability through loans to investors."}, "https://arxiv.org/abs/2405.10355": {"title": "Assessing the Impact of Case Correction Methods on the Fairness of COVID-19 Predictive Models", "link": "https://arxiv.org/abs/2405.10355", "description": "arXiv:2405.10355v1 Announce Type: new \nAbstract: One of the central difficulties of addressing the COVID-19 pandemic has been accurately measuring and predicting the spread of infections. In particular, official COVID-19 case counts in the United States are under counts of actual caseloads due to the absence of universal testing policies. Researchers have proposed a variety of methods for recovering true caseloads, often through the estimation of statistical models on more reliable measures, such as death and hospitalization counts, positivity rates, and demographics. However, given the disproportionate impact of COVID-19 on marginalized racial, ethnic, and socioeconomic groups, it is important to consider potential unintended effects of case correction methods on these groups. Thus, we investigate two of these correction methods for their impact on a downstream COVID-19 case prediction task. For that purpose, we tailor an auditing approach and evaluation protocol to analyze the fairness of the COVID-19 prediction task by measuring the difference in model performance between majority-White counties and majority-minority counties. We find that one of the correction methods improves fairness, decreasing differences in performance between majority-White and majority-minority counties, while the other method increases differences, introducing bias. While these results are mixed, it is evident that correction methods have the potential to exacerbate existing biases in COVID-19 case data and in downstream prediction tasks. Researchers planning to develop or use case correction methods must be careful to consider negative effects on marginalized groups."}, "https://arxiv.org/abs/2405.10417": {"title": "Cosmic rays for imaging cultural heritage objects", "link": "https://arxiv.org/abs/2405.10417", "description": "arXiv:2405.10417v1 Announce Type: new \nAbstract: In cultural heritage conservation, it is increasingly common to rely on non-destructive imaging methods based on the absorption or scattering of photons ($X$ or $\\gamma$ rays) or neutrons. However, physical and practical issues limit these techniques: their penetration depth may be insufficient for large and dense objects, they require transporting the objects of interest to dedicated laboratories, artificial radiation is hazardous and may induce activation in the material under study. Muons are elementary particles abundantly and freely produced in cosmic-ray interactions in the atmosphere. Their absorption and scattering in matter are characteristically dependent on the density and elemental composition of the material that they traverse, which offers the possibility of exploiting them for sub-surface remote imaging. This novel technique, nicknamed \"muography\", has been applied in use cases ranging from geophysics to archaeology to nuclear safety, but it has been so far under-explored for a vast category of cultural heritage objects that are relatively large (from decimeters to human size) and dense (stone, metals). The development of portable muon detectors makes muography particularly competitive in cases where the items to be analysed are not transportable, or set up in a confined environment. This document reviews the relevant literature, presents some exemplary use cases, and critically assesses the strengths and weaknesses of muography in this context."}, "https://arxiv.org/abs/2405.10450": {"title": "Quantifying national space heating flexibility potential at high spatial resolution with heating consumption data", "link": "https://arxiv.org/abs/2405.10450", "description": "arXiv:2405.10450v1 Announce Type: new \nAbstract: Decarbonizing the building stock in cold countries by replacing fossil fuel boilers with heat pumps is expected to drastically increase electricity demand. While heating flexibility could reduce the impact of additional demand from heat pumps on the power system, characterizing the national spatial distribution of heating flexibility capacity to incorporate into sophisticated power system models is challenging. This paper introduces a novel method for quantifying at large scale and high spatial resolution the energy capacity and duration of heating flexibility in existing building stock based on historical heating consumption and temperature data. This method can reflect the geographic diversity of the national building stock in sophisticated power system models. The proposed heating consumption-based method was tested in Britain using national residential gas data. The results demonstrate the potential of this approach to characterize the heterogeneous distribution of heating flexibility capacity at the national scale. Assuming a 3$^\\circ$C temperature flexibility window, a total thermal energy storage capacity of 500 GWh$_{th}$ is identified in the British housing stock. For an illustrative cold weather COP value of 2.5, this thermal energy storage capacity is equivalent to 200 GWh of electricity storage. Regarding heating flexibility duration, gas-heated homes have a median of 5.9 heat-free hours for 20th percentile regional daily winter temperatures from 2010 to 2022. However, extreme cold days nearly halve flexibility duration to a median of 3.6 heat-free hours. These high spatial resolution energy capacity and self-discharge parameters can account for geographic diversity at the national scale and provide a new data-based layer of information for sophisticated power system models to support energy transition."}, "https://arxiv.org/abs/2405.10547": {"title": "GPTs Window Shopping: An analysis of the Landscape of Custom ChatGPT Models", "link": "https://arxiv.org/abs/2405.10547", "description": "arXiv:2405.10547v1 Announce Type: new \nAbstract: OpenAI's ChatGPT initiated a wave of technical iterations in the space of Large Language Models (LLMs) by demonstrating the capability and disruptive power of LLMs. OpenAI has prompted large organizations to respond with their own advancements and models to push the LLM performance envelope. OpenAI has prompted large organizations to respond with their own advancements and models to push the LLM performance envelope. OpenAI's success in spotlighting AI can be partially attributed to decreased barriers to entry, enabling any individual with an internet-enabled device to interact with LLMs. What was previously relegated to a few researchers and developers with necessary computing resources is now available to all. A desire to customize LLMs to better accommodate individual needs prompted OpenAI's creation of the GPT Store, a central platform where users can create and share custom GPT models. Customization comes in the form of prompt-tuning, analysis of reference resources, browsing, and external API interactions, alongside a promise of revenue sharing for created custom GPTs. In this work, we peer into the window of the GPT Store and measure its impact. Our analysis constitutes a large-scale overview of the store exploring community perception, GPT details, and the GPT authors, in addition to a deep-dive into a 3rd party storefront indexing user-submitted GPTs, exploring if creators seek to monetize their creations in the absence of OpenAI's revenue sharing."}, "https://arxiv.org/abs/2405.10558": {"title": "CACL: Community-Aware Heterogeneous Graph Contrastive Learning for Social Media Bot Detection", "link": "https://arxiv.org/abs/2405.10558", "description": "arXiv:2405.10558v1 Announce Type: new \nAbstract: Social media bot detection is increasingly crucial with the rise of social media platforms. Existing methods predominantly construct social networks as graph and utilize graph neural networks (GNNs) for bot detection. However, most of these methods focus on how to improve the performance of GNNs while neglecting the community structure within social networks. Moreover, GNNs based methods still face problems such as poor model generalization due to the relatively small scale of the dataset and over-smoothness caused by information propagation mechanism. To address these problems, we propose a Community-Aware Heterogeneous Graph Contrastive Learning framework (CACL), which constructs social network as heterogeneous graph with multiple node types and edge types, and then utilizes community-aware module to dynamically mine both hard positive samples and hard negative samples for supervised graph contrastive learning with adaptive graph enhancement algorithms. Extensive experiments demonstrate that our framework addresses the previously mentioned challenges and outperforms competitive baselines on three social media bot benchmarks."}, "https://arxiv.org/abs/2405.10640": {"title": "COMET: NFT Price Prediction with Wallet Profiling", "link": "https://arxiv.org/abs/2405.10640", "description": "arXiv:2405.10640v1 Announce Type: new \nAbstract: As the non-fungible token (NFT) market flourishes, price prediction emerges as a pivotal direction for investors gaining valuable insight to maximize returns. However, existing works suffer from a lack of practical definitions and standardized evaluations, limiting their practical application. Moreover, the influence of users' multi-behaviour transactions that are publicly accessible on NFT price is still not explored and exhibits challenges. In this paper, we address these gaps by presenting a practical and hierarchical problem definition. This approach unifies both collection-level and token-level task and evaluation methods, which cater to varied practical requirements of investors. To further understand the impact of user behaviours on the variation of NFT price, we propose a general wallet profiling framework and develop a COmmunity enhanced Multi-bEhavior Transaction graph model, named COMET. COMET profiles wallets with a comprehensive view and considers the impact of diverse relations and interactions within the NFT ecosystem on NFT price variations, thereby improving prediction performance. Extensive experiments conducted in our deployed system demonstrate the superiority of COMET, underscoring its potential in the insight toolkit for NFT investors."}, "https://arxiv.org/abs/2405.10665": {"title": "Leader-Follower Identification with Vehicle-Following Calibration for Non-Lane-Based Traffic", "link": "https://arxiv.org/abs/2405.10665", "description": "arXiv:2405.10665v1 Announce Type: new \nAbstract: Most car-following models were originally developed for lane-based traffic. Over the past two decades, efforts have been made to calibrate car-following models for non-lane-based traffic. However, traffic conditions with varying vehicle dimensions, intermittent following, and multiple leaders often occur and make subjective Leader-Follower (LF) pair identification challenging. In this study, we analyze Vehicle Following (VF) behavior in traffic with a lack of lane discipline using high-resolution microscopic trajectory data collected in Chennai, India. The paper's main contributions are threefold. Firstly, three criteria are used to identify LF pairs from the driver's perspective, taking into account the intermittent following, lack of lane discipline due to consideration of lateral separation, and the presence of in-between vehicles. Second, the psycho-physical concept of the regime in the Wiedemann-99 model is leveraged to determine the traffic-dependent \"influence zone\" for LF identification. Third, a joint and consistent framework is proposed for identifying LF pairs and estimating VF parameters. The proposed methodology outperforms other heuristic-based LF identification methods from the literature in terms of quantitative and qualitative performance measures. The proposed approach can enable robust and more realistic LF identification and VF parameter calibration with practical applications such as LOS analysis, capacity, and travel time estimation."}, "https://arxiv.org/abs/2405.10798": {"title": "Understanding following patterns among high-performance athletes", "link": "https://arxiv.org/abs/2405.10798", "description": "arXiv:2405.10798v1 Announce Type: new \nAbstract: Professional sports enhance interaction among athletes through training groups, sponsored events and competitions. Among these, the Olympic Games represent the largest competition with a global impact, providing the participants with a unique opportunity for interaction. We studied the following patterns among highly successful athletes to understand the structure of their interactions. We used the list of Olympic medallists in the Tokyo 2020 Games to extract their follower-followee network in Twitter, finding 7,326 connections among 964 athletes. The network displayed frequent connections to similar peers in terms of their features including sex, country and sport. We quantified the influence of these features in the followees choice through a gravity approach capturing the number of connections between homogeneous groups. Our research remarks the importance of datasets built from public exposure of professional athletes, serving as a proxy to investigate interesting aspects of many complex socio-cultural systems at different scales."}, "https://arxiv.org/abs/2405.10818": {"title": "Modeling Supply Chain Interaction and Disruption: Insights from Real-world Data and Complex Adaptive System", "link": "https://arxiv.org/abs/2405.10818", "description": "arXiv:2405.10818v1 Announce Type: new \nAbstract: In the rapidly evolving automotive industry, Systems-on-Chips (SoCs) are playing an increasingly crucial role in enhancing vehicle intelligence, connectivity, and safety features. For enterprises whose business encompasses automotive SoCs, the sustained and stable provision and receipt of SoC relevant goods or services are essential. Considering the imperative for a resilient and adaptable supply network, enterprises are concentrating their efforts on formulating strategies to address risks stemming from supply chain disruptions caused by technological obsolescence, natural disasters, and geopolitical tensions. This study presents an open supply knowledge extraction and complement approach and build a supply chain network of automotive SoC enterprises in China, which incorporates cross-domain named entity recognition under limited information, fuzzy matching of firm entities, and supply relation inferring based on knowledge graph. Subsequently, we exhibit the degree and registered capital distribution across firms, and analyze the correlations between centrality metrics in the supply chain network. Finally, based on recovery capacity and risk transfer, two interaction disruption models (IDMs) are developed to elucidate the adaptive behaviors and effect of network disruptions under various business and attack strategies. This research not only aids in exploring the complexities of Chinese automotive SoC supply chain but also enriches our understanding of the dynamics of firm behavior in this crucial industry sector."}, "https://arxiv.org/abs/2405.09843": {"title": "Organizational Selection of Innovation", "link": "https://arxiv.org/abs/2405.09843", "description": "arXiv:2405.09843v1 Announce Type: cross \nAbstract: Budgetary constraints force organizations to pursue only a subset of possible innovation projects. Identifying which subset is most promising is an error-prone exercise, and involving multiple decision makers may be prudent. This raises the question of how to most effectively aggregate their collective nous. Our model of organizational portfolio selection provides some first answers. We show that portfolio performance can vary widely. Delegating evaluation makes sense when organizations employ the relevant experts and can assign projects to them. In most other settings, aggregating the impressions of multiple agents leads to better performance than delegation. In particular, letting agents rank projects often outperforms alternative aggregation rules -- including averaging agents' project scores as well as counting their approval votes -- especially when organizations have tight budgets and can select only a few project alternatives out of many."}, "https://arxiv.org/abs/2405.10497": {"title": "SMP Challenge: An Overview and Analysis of Social Media Prediction Challenge", "link": "https://arxiv.org/abs/2405.10497", "description": "arXiv:2405.10497v1 Announce Type: cross \nAbstract: Social Media Popularity Prediction (SMPP) is a crucial task that involves automatically predicting future popularity values of online posts, leveraging vast amounts of multimodal data available on social media platforms. Studying and investigating social media popularity becomes central to various online applications and requires novel methods of comprehensive analysis, multimodal comprehension, and accurate prediction.\n  SMP Challenge is an annual research activity that has spurred academic exploration in this area. This paper summarizes the challenging task, data, and research progress. As a critical resource for evaluating and benchmarking predictive models, we have released a large-scale SMPD benchmark encompassing approximately half a million posts authored by around 70K users. The research progress analysis provides an overall analysis of the solutions and trends in recent years. The SMP Challenge website (www.smp-challenge.com) provides the latest information and news."}, "https://arxiv.org/abs/1609.00004": {"title": "On the initial value of PageRank", "link": "https://arxiv.org/abs/1609.00004", "description": "arXiv:1609.00004v5 Announce Type: replace \nAbstract: Google employs PageRank to rank web pages, determining the order in which search results are presented to users based on their queries. PageRank is primarily utilized for directed networks, although there are instances where it is also applied to undirected networks. In this paper, we have applied PageRank to undirected networks, showing that a vertex's PageRank relies on its initial value, often referred to as an intrinsic, non-network contribution. We have analytically proved that when the initial value of vertices is either proportional to their degrees or set to zero, the PageRank values of the vertices become directly proportional to their degrees. Simulated and empirical data are employed to bolster our research findings. Additionally, we have investigated the impact of initial values on PageRank localization."}, "https://arxiv.org/abs/2304.12751": {"title": "Node Feature Augmentation Vitaminizes Network Alignment", "link": "https://arxiv.org/abs/2304.12751", "description": "arXiv:2304.12751v4 Announce Type: replace \nAbstract: Network alignment (NA) is the task of discovering node correspondences across multiple networks. Although NA methods have achieved remarkable success in a myriad of scenarios, their effectiveness is not without additional information such as prior anchor links and/or node features, which may not always be available due to privacy concerns or access restrictions. To tackle this challenge, we propose Grad-Align+, a novel NA method built upon a recent state-of-the-art NA method, the so-called Grad-Align, that gradually discovers a part of node pairs until all node pairs are found. In designing Grad-Align+, we account for how to augment node features in the sense of performing the NA task and how to design our NA method by maximally exploiting the augmented node features. To achieve this goal, Grad-Align+ consists of three key components: 1) centrality-based node feature augmentation (CNFA), 2) graph neural network (GNN)-aided embedding similarity calculation alongside the augmented node features, and 3) gradual NA with similarity calculation using aligned cross-network neighbor-pairs (ACNs). Through comprehensive experiments, we demonstrate that Grad-Align+ exhibits (a) the superiority over benchmark NA methods, (b) empirical validations as well as our theoretical findings to see the effectiveness of CNFA, (c) the influence of each component, (d) the robustness to network noises, and (e) the computational efficiency."}, "https://arxiv.org/abs/2310.10155": {"title": "Analysis and implementation of nanotargeting on LinkedIn based on publicly available non-PII", "link": "https://arxiv.org/abs/2310.10155", "description": "arXiv:2310.10155v2 Announce Type: replace \nAbstract: The literature has shown that combining a few non-Personal Identifiable Information (non-PII) is enough to make a user unique in a dataset including millions of users. This work demonstrates that a combination of a few non-PII items can be activated to nanotarget users. We demonstrate that the combination of the location and {5} rare ({13} random) skills in a LinkedIn profile is enough to become unique in a user base of {$\\sim$970M} users with a probability of 75\\%. The novelty is that these attributes are publicly accessible to anyone registered on LinkedIn and can be activated through advertising campaigns. We ran an experiment configuring ad campaigns using the location and skills of three of the paper's authors, demonstrating how all the ads using $\\geq13$ skills were delivered exclusively to the targeted user. We reported this vulnerability to LinkedIn, which initially ignored the problem, but fixed it as of November 2023.%This nanotargeting may expose LinkedIn users to privacy and security risks such as malvertising or manipulation."}, "https://arxiv.org/abs/2402.03894": {"title": "Interpersonal trust: Asymptotic analysis of a stochastic coordination game with multi-agent learning", "link": "https://arxiv.org/abs/2402.03894", "description": "arXiv:2402.03894v3 Announce Type: replace \nAbstract: We study the interpersonal trust of a population of agents, asking whether chance may decide if a population ends up in a high trust or low trust state. We model this by a discrete time, random matching stochastic coordination game. Agents are endowed with an exponential smoothing learning rule about the behaviour of their neighbours. We find that, with probability one in the long run the whole population either always cooperates or always defects. By simulation we study the impact of the distributions of the payoffs in the game and of the exponential smoothing learning (memory of the agents). We find, that as the agent memory increases or as the size of the population increases, the actual dynamics start to resemble the expectation of the process. We conclude that it is indeed possible that different populations may converge upon high or low trust between its citizens simply by chance, though the game parameters (context of the society) may be quite telling."}, "https://arxiv.org/abs/2404.01319": {"title": "Information Cascade Prediction under Public Emergencies: A Survey", "link": "https://arxiv.org/abs/2404.01319", "description": "arXiv:2404.01319v2 Announce Type: replace \nAbstract: With the advent of the era of big data, massive information, expert experience, and high-accuracy models bring great opportunities to the information cascade prediction of public emergencies. However, the involvement of specialist knowledge from various disciplines has resulted in a primarily application-specific focus (e.g., earthquakes, floods, infectious diseases) for information cascade prediction of public emergencies. The lack of a unified prediction framework poses a challenge for classifying intersectional prediction methods across different application fields. This survey paper offers a systematic classification and summary of information cascade modeling, prediction, and application. We aim to help researchers identify cutting-edge research and comprehend models and methods of information cascade prediction under public emergencies. By summarizing open issues and outlining future directions in this field, this paper has the potential to be a valuable resource for researchers conducting further studies on predicting information cascades."}, "https://arxiv.org/abs/2311.03682": {"title": "Incentive Design for Eco-driving in Urban Transportation Networks", "link": "https://arxiv.org/abs/2311.03682", "description": "arXiv:2311.03682v2 Announce Type: replace-cross \nAbstract: Eco-driving emerges as a cost-effective and efficient strategy to mitigate greenhouse gas emissions in urban transportation networks. Acknowledging the persuasive influence of incentives in shaping driver behavior, this paper presents the `eco-planner,' a digital platform devised to promote eco-driving practices in urban transportation. At the outset of their trips, users provide the platform with their trip details and travel time preferences, enabling the eco-planner to formulate personalized eco-driving recommendations and corresponding incentives, while adhering to its budgetary constraints. Upon trip completion, incentives are transferred to users who comply with the recommendations and effectively reduce their emissions. By comparing our proposed incentive mechanism with a baseline scheme that offers uniform incentives to all users, we demonstrate that our approach achieves superior emission reductions and increased user compliance with a smaller budget."}, "https://arxiv.org/abs/2404.10228": {"title": "Two-Stage Stance Labeling: User-Hashtag Heuristics with Graph Neural Networks", "link": "https://arxiv.org/abs/2404.10228", "description": "arXiv:2404.10228v2 Announce Type: replace-cross \nAbstract: The high volume and rapid evolution of content on social media present major challenges for studying the stance of social media users. In this work, we develop a two stage stance labeling method that utilizes the user-hashtag bipartite graph and the user-user interaction graph. In the first stage, a simple and efficient heuristic for stance labeling uses the user-hashtag bipartite graph to iteratively update the stance association of user and hashtag nodes via a label propagation mechanism. This set of soft labels is then integrated with the user-user interaction graph to train a graph neural network (GNN) model using semi-supervised learning. We evaluate this method on two large-scale datasets containing tweets related to climate change from June 2021 to June 2022 and gun control from January 2022 to January 2023. Our experiments demonstrate that enriching text-based embeddings of users with network information from the user interaction graph using our semi-supervised GNN method outperforms both classifiers trained on user textual embeddings and zero-shot classification using LLMs such as GPT4. We discuss the need for integrating nuanced understanding from social science with the scalability of computational methods to better understand how polarization on social media occurs for divisive issues such as climate change and gun control."}, "https://arxiv.org/abs/2405.11146": {"title": "Election Polls on Social Media: Prevalence, Biases, and Voter Fraud Beliefs", "link": "https://arxiv.org/abs/2405.11146", "description": "arXiv:2405.11146v1 Announce Type: new \nAbstract: Social media platforms allow users to create polls to gather public opinion on diverse topics. However, we know little about what such polls are used for and how reliable they are, especially in significant contexts like elections. Focusing on the 2020 presidential elections in the U.S., this study shows that outcomes of election polls on Twitter deviate from election results despite their prevalence. Leveraging demographic inference and statistical analysis, we find that Twitter polls are disproportionately authored by older males and exhibit a large bias towards candidate Donald Trump relative to representative mainstream polls. We investigate potential sources of biased outcomes from the point of view of inauthentic, automated, and counter-normative behavior. Using social media experiments and interviews with poll authors, we identify inconsistencies between public vote counts and those privately visible to poll authors, with the gap potentially attributable to purchased votes. We also find that Twitter accounts participating in election polls are more likely to be bots, and election poll outcomes tend to be more biased, before the election day than after. Finally, we identify instances of polls spreading voter fraud conspiracy theories and estimate that a couple thousand of such polls were posted in 2020. The study discusses the implications of biased election polls in the context of transparency and accountability of social media platforms."}, "https://arxiv.org/abs/2405.11166": {"title": "Learning the liveability of cities from migrants: Combinatiorial-Hodge-theory approach", "link": "https://arxiv.org/abs/2405.11166", "description": "arXiv:2405.11166v1 Announce Type: new \nAbstract: Migration is a major decision to leave one place and move to another, and involves job and life changes. The migration flow of people provides relational information across places about which is better to live by ``voting with their feet'' (Tiebout, 1956; Douglas, 1997). From the people's votes, in a ``democratic'' process, we quantify a descriptive statistic of liveable cities by a potential of migration flow in Combinatorial Hodge theory. As a case study, we measure the liveability of municipalities in Japan for specific populations such as families with small children and women of reproductive age. Using these potentials as dependent variables, we perform a regression analysis to identify the factors relevant to liveability. Additionally, using the aformentioned theoretical framework, we analytically derive the expression of the utility as a function of given flow data, which was numerically estimated in previous studies (Douglas & Wall, 1993; Douglas, 1997; Douglas & Wall, 2000; Wall, 2001; Nakajima & Tabuchi, 2011). The proposed method extracts a consistent metric of interval scale from the non-transitive, pairwise comparison between locations and provides valuable statistics for urban planning by policymakers."}, "https://arxiv.org/abs/2405.11225": {"title": "SeBot: Structural Entropy Guided Multi-View Contrastive Learning for Social Bot Detection", "link": "https://arxiv.org/abs/2405.11225", "description": "arXiv:2405.11225v1 Announce Type: new \nAbstract: Recent advancements in social bot detection have been driven by the adoption of Graph Neural Networks. The social graph, constructed from social network interactions, contains benign and bot accounts that influence each other. However, previous graph-based detection methods that follow the transductive message-passing paradigm may not fully utilize hidden graph information and are vulnerable to adversarial bot behavior. The indiscriminate message passing between nodes from different categories and communities results in excessively homogeneous node representations, ultimately reducing the effectiveness of social bot detectors. In this paper, we propose SEBot, a novel multi-view graph-based contrastive learning-enabled social bot detector. In particular, we use structural entropy as an uncertainty metric to optimize the entire graph's structure and subgraph-level granularity, revealing the implicitly existing hierarchical community structure. And we design an encoder to enable message passing beyond the homophily assumption, enhancing robustness to adversarial behaviors of social bots. Finally, we employ multi-view contrastive learning to maximize mutual information between different views and enhance the detection performance through multi-task learning. Experimental results demonstrate that our approach significantly improves the performance of social bot detection compared with SOTA methods."}, "https://arxiv.org/abs/2405.11887": {"title": "Social norm dynamics in a behavioral epidemic model on multiplex networks", "link": "https://arxiv.org/abs/2405.11887", "description": "arXiv:2405.11887v1 Announce Type: new \nAbstract: Understanding the social determinants influencing preventive measures adoption during epidemics is crucial for effective disease modeling and policy making. While traditional epidemic models focused on rational decision-making and psychological biases, recent studies highlight the role of social norms. We develop a behavioral epidemic model on a multiplex network, by integrating an Experience Weighted Attractor (EWA) learning mechanism and social norm dynamics. Incorporating social norms in our decision-making mechanism significantly reduces final infected fractions, offering an alternative to altruism for boosting vaccination coverage. Furthermore, we examine the importance of the dynamics of each one of the social norms, injunctive or descriptive, in reducing the infected fraction, finding that the former has a more significant effect in agreement with some experimental evidence. We also explore the effect of external interventions on epidemic expansion, aiding in refining public communication protocols. Enhanced models of social norm dynamics, if validated and tested, can better capture the complexities of human social behavior and mitigate various societal challenges beyond pandemics."}, "https://arxiv.org/abs/2405.11922": {"title": "Effective Clustering on Large Attributed Bipartite Graphs", "link": "https://arxiv.org/abs/2405.11922", "description": "arXiv:2405.11922v1 Announce Type: new \nAbstract: Attributed bipartite graphs (ABGs) are an expressive data model for describing the interactions between two sets of heterogeneous nodes that are associated with rich attributes, such as customer-product purchase networks and author-paper authorship graphs. Partitioning the target node set in such graphs into k disjoint clusters (referred to as k-ABGC) finds widespread use in various domains, including social network analysis, recommendation systems, information retrieval, and bioinformatics. However, the majority of existing solutions towards k-ABGC either overlook attribute information or fail to capture bipartite graph structures accurately, engendering severely compromised result quality. The severity of these issues is accentuated in real ABGs, which often encompass millions of nodes and a sheer volume of attribute data, rendering effective k-ABGC over such graphs highly challenging.\n  In this paper, we propose TPO, an effective and efficient approach to k-ABGC that achieves superb clustering performance on multiple real datasets. TPO obtains high clustering quality through two major contributions: (i) a novel formulation and transformation of the k-ABGC problem based on multi-scale attribute affinity specialized for capturing attribute affinities between nodes with the consideration of their multi-hop connections in ABGs, and (ii) a highly efficient solver that includes a suite of carefully-crafted optimizations for sidestepping explicit affinity matrix construction and facilitating faster convergence. Extensive experiments, comparing TPO against 19 baselines over 5 real ABGs, showcase the superior clustering quality of TPO measured against ground-truth labels. Moreover, compared to the state of the arts, TPO is often more than 40x faster over both small and large ABGs."}, "https://arxiv.org/abs/2405.12040": {"title": "Reputation Transfer in the Twitter Diaspora", "link": "https://arxiv.org/abs/2405.12040", "description": "arXiv:2405.12040v1 Announce Type: new \nAbstract: Social media platforms have witnessed a dynamic landscape of user migration in recent years, fueled by changes in ownership, policy, and user preferences. This paper explores the phenomenon of user migration from established platforms like X/Twitter to emerging alternatives such as Threads, Mastodon, and Truth Social. Leveraging a large dataset from X/Twitter, we investigate the extent of user departure from X/Twitter and the destinations they migrate to. Additionally, we examine whether a user's reputation on one platform correlates with their reputation on another, shedding light on the transferability of digital reputation across social media ecosystems. Overall, we find that users with a large following on X/Twitter are more likely to migrate to another platform; and that their reputation on X/Twitter is highly correlated with reputations on Threads, but not Mastodon or Truth Social."}, "https://arxiv.org/abs/2405.11121": {"title": "COVID-19's Unequal Toll: An assessment of small business impact disparities with respect to ethnorace in metropolitan areas in the US using mobility data", "link": "https://arxiv.org/abs/2405.11121", "description": "arXiv:2405.11121v1 Announce Type: cross \nAbstract: Early in the pandemic, counties and states implemented a variety of non-pharmacological interventions (NPIs) focused on mobility, such as national lockdowns or work-from-home strategies, as it became clear that restricting movement was essential to containing the epidemic. Due to these restrictions, businesses were severely affected and in particular, small, urban restaurant businesses. In addition to that, COVID-19 has also amplified many of the socioeconomic disparities and systemic racial inequities that exist in our society. The overarching objective of this study was to examine the changes in small urban restaurant visitation patterns following the COVID-19 pandemic and associated mobility restrictions, as well as to uncover potential disparities across different racial/ethnic groups in order to understand inequities in the impact and recovery. Specifically, the two key objectives were: 1) to analyze the overall changes in restaurant visitation patterns in US metropolitan areas during the pandemic compared to a pre-pandemic baseline, and 2) to investigate differences in visitation pattern changes across Census Block Groups with majority Asian, Black, Hispanic, White, and American Indian populations, identifying any disproportionate effects. Using aggregated geolocated cell phone data from SafeGraph, we document the overall changes in small urban restaurant businesses' visitation patterns with respect to racial composition at a granularity of Census Block Groups. Our results show clear indications of reduced visitation patterns after the pandemic, with slow recoveries. Via visualizations and statistical analyses, we show that reductions in visitation patterns were the highest for small urban restaurant businesses in majority Asian neighborhoods."}, "https://arxiv.org/abs/2405.11192": {"title": "BrainStorm @ iREL at SMM4H 2024: Leveraging Translation and Topical Embeddings for Annotation Detection in Tweets", "link": "https://arxiv.org/abs/2405.11192", "description": "arXiv:2405.11192v1 Announce Type: cross \nAbstract: The proliferation of LLMs in various NLP tasks has sparked debates regarding their reliability, particularly in annotation tasks where biases and hallucinations may arise. In this shared task, we address the challenge of distinguishing annotations made by LLMs from those made by human domain experts in the context of COVID-19 symptom detection from tweets in Latin American Spanish. This paper presents BrainStorm @ iREL's approach to the SMM4H 2024 Shared Task, leveraging the inherent topical information in tweets, we propose a novel approach to identify and classify annotations, aiming to enhance the trustworthiness of annotated data."}, "https://arxiv.org/abs/2405.11219": {"title": "Identifying and Aligning Medical Claims Made on Social Media with Medical Evidence", "link": "https://arxiv.org/abs/2405.11219", "description": "arXiv:2405.11219v1 Announce Type: cross \nAbstract: Evidence-based medicine is the practice of making medical decisions that adhere to the latest, and best known evidence at that time. Currently, the best evidence is often found in the form of documents, such as randomized control trials, meta-analyses and systematic reviews. This research focuses on aligning medical claims made on social media platforms with this medical evidence. By doing so, individuals without medical expertise can more effectively assess the veracity of such medical claims. We study three core tasks: identifying medical claims, extracting medical vocabulary from these claims, and retrieving evidence relevant to those identified medical claims. We propose a novel system that can generate synthetic medical claims to aid each of these core tasks. We additionally introduce a novel dataset produced by our synthetic generator that, when applied to these tasks, demonstrates not only a more flexible and holistic approach, but also an improvement in all comparable metrics. We make our dataset, the Expansive Medical Claim Corpus (EMCC), available at https://zenodo.org/records/8321460"}, "https://arxiv.org/abs/2405.11414": {"title": "High-Resolution Agent-Based Modeling of Campus Population Behaviors for Pandemic Response Planning", "link": "https://arxiv.org/abs/2405.11414", "description": "arXiv:2405.11414v1 Announce Type: cross \nAbstract: This paper reports a case study of an application of high-resolution agent-based modeling and simulation to pandemic response planning on a university campus. In the summer of 2020, we were tasked with a COVID-19 pandemic response project to create a detailed behavioral simulation model of the entire campus population at Binghamton University. We conceptualized this problem as an agent migration process on a multilayer transportation network, in which each layer represented a different transportation mode. As no direct data were available about people's behaviors on campus, we collected as much indirect information as possible to inform the agents' behavioral rules. Each agent was assumed to move along the shortest path between two locations within each transportation layer and switch layers at a parking lot or a bus stop, along with several other behavioral assumptions. Using this model, we conducted simulations of the whole campus population behaviors on a typical weekday, involving more than 25,000 agents. We measured the frequency of close social contacts at each spatial location and identified several busy locations and corridors on campus that needed substantial behavioral intervention. Moreover, systematic simulations with varying population density revealed that the effect of population density reduction was nonlinear, and that reducing the population density to 40-45% would be optimal and sufficient to suppress disease spreading on campus. These results were reported to the university administration and utilized in the pandemic response planning, which led to successful outcomes."}, "https://arxiv.org/abs/2405.11658": {"title": "A Starting Point for Dynamic Community Detection with Leiden Algorithm", "link": "https://arxiv.org/abs/2405.11658", "description": "arXiv:2405.11658v1 Announce Type: cross \nAbstract: Many real-world graphs evolve with time. Identifying communities or clusters on such graphs is an important problem. In this technical report, we extend three dynamic approaches, namely, Naive-dynamic (ND), Delta-screening (DS), and Dynamic Frontier (DF), to our multicore implementation of the Leiden algorithm, an algorithm known for its high-quality community detection. Our experiments on a server with a 64-core AMD EPYC-7742 processor demonstrate that ND, DS, and DF Leiden achieve speedups of 1.25x, 1.24x, and 1.37x on large graphs with random batch updates, compared to Static, ND, and DS Leiden, respectively. However, on real-world dynamic graphs, ND Leiden performs the best, being on average 1.14x faster than Static Leiden. We hope our early results serve as a starting point for dynamic approaches to the Leiden algorithm on evolving graphs."}, "https://arxiv.org/abs/2405.11868": {"title": "Towards Graph Contrastive Learning: A Survey and Beyond", "link": "https://arxiv.org/abs/2405.11868", "description": "arXiv:2405.11868v1 Announce Type: cross \nAbstract: In recent years, deep learning on graphs has achieved remarkable success in various domains. However, the reliance on annotated graph data remains a significant bottleneck due to its prohibitive cost and time-intensive nature. To address this challenge, self-supervised learning (SSL) on graphs has gained increasing attention and has made significant progress. SSL enables machine learning models to produce informative representations from unlabeled graph data, reducing the reliance on expensive labeled data. While SSL on graphs has witnessed widespread adoption, one critical component, Graph Contrastive Learning (GCL), has not been thoroughly investigated in the existing literature. Thus, this survey aims to fill this gap by offering a dedicated survey on GCL. We provide a comprehensive overview of the fundamental principles of GCL, including data augmentation strategies, contrastive modes, and contrastive optimization objectives. Furthermore, we explore the extensions of GCL to other aspects of data-efficient graph learning, such as weakly supervised learning, transfer learning, and related scenarios. We also discuss practical applications spanning domains such as drug discovery, genomics analysis, recommender systems, and finally outline the challenges and potential future directions in this field."}, "https://arxiv.org/abs/2405.11911": {"title": "PULL: PU-Learning-based Accurate Link Prediction", "link": "https://arxiv.org/abs/2405.11911", "description": "arXiv:2405.11911v1 Announce Type: cross \nAbstract: Given an edge-incomplete graph, how can we accurately find the missing links? The link prediction in edge-incomplete graphs aims to discover the missing relations between entities when their relationships are represented as a graph. Edge-incomplete graphs are prevalent in real-world due to practical limitations, such as not checking all users when adding friends in a social network. Addressing the problem is crucial for various tasks, including recommending friends in social networks and finding references in citation networks. However, previous approaches rely heavily on the given edge-incomplete (observed) graph, making it challenging to consider the missing (unobserved) links during training. In this paper, we propose PULL (PU-Learning-based Link predictor), an accurate link prediction method based on the positive-unlabeled (PU) learning. PULL treats the observed edges in the training graph as positive examples, and the unconnected node pairs as unlabeled ones. PULL effectively prevents the link predictor from overfitting to the observed graph by proposing latent variables for every edge, and leveraging the expected graph structure with respect to the variables. Extensive experiments on five real-world datasets show that PULL consistently outperforms the baselines for predicting links in edge-incomplete graphs."}, "https://arxiv.org/abs/2405.12023": {"title": "Estimating transmission noise on networks from stationary local order", "link": "https://arxiv.org/abs/2405.12023", "description": "arXiv:2405.12023v1 Announce Type: cross \nAbstract: In this paper we study networks of nodes characterised by binary traits that change both endogenously and through nearest-neighbour interaction. Our analytical results show that those traits can be ranked according to the noisiness of their transmission using only measures of order in the stationary state. Crucially, this ranking is independent of network topology. As an example, we explain why, in line with a long-standing hypothesis, the relative stability of the structural traits of languages can be estimated from their geospatial distribution. We conjecture that similar inferences may be possible in a more general class of Markovian systems. Consequently, in many empirical domains where longitudinal information is not easily available the propensities of traits to change could be estimated from spatial data alone."}, "https://arxiv.org/abs/2405.12180": {"title": "Estimating the Impact of Social Distance Policy in Mitigating COVID-19 Spread with Factor-Based Imputation Approach", "link": "https://arxiv.org/abs/2405.12180", "description": "arXiv:2405.12180v1 Announce Type: cross \nAbstract: We identify the effectiveness of social distancing policies in reducing the transmission of the COVID-19 spread. We build a model that measures the relative frequency and geographic distribution of the virus growth rate and provides hypothetical infection distribution in the states that enacted the social distancing policies, where we control time-varying, observed and unobserved, state-level heterogeneities. Using panel data on infection and deaths in all US states from February 20 to April 20, 2020, we find that stay-at-home orders and other types of social distancing policies significantly reduced the growth rate of infection and deaths. We show that the effects are time-varying and range from the weakest at the beginning of policy intervention to the strongest by the end of our sample period. We also found that social distancing policies were more effective in states with higher income, better education, more white people, more democratic voters, and higher CNN viewership."}, "https://arxiv.org/abs/2308.05945": {"title": "Improving Ego-Cluster for Network Effect Measurement", "link": "https://arxiv.org/abs/2308.05945", "description": "arXiv:2308.05945v2 Announce Type: replace \nAbstract: The network effect, wherein one user's activity impacts another user, is common in social network platforms. Many new features in social networks are specifically designed to create a network effect, enhancing user engagement. For instance, content creators tend to produce more when their articles and posts receive positive feedback from followers. This paper discusses a new cluster-level experimentation methodology for measuring creator-side metrics in the context of A/B experiments. The methodology is designed to address cases where the experiment randomization unit and the metric measurement unit differ. It is a crucial part of LinkedIn's overall strategy to foster a robust creator community and ecosystem. The method is developed based on widely-cited research at LinkedIn but significantly improves the efficiency and flexibility of the clustering algorithm. This improvement results in a stronger capability for measuring creator-side metrics and an increased velocity for creator-related experiments."}, "https://arxiv.org/abs/2401.08680": {"title": "Proximity Ascertainment Bias in Early Covid Case Locations", "link": "https://arxiv.org/abs/2401.08680", "description": "arXiv:2401.08680v5 Announce Type: replace \nAbstract: A comparison of the distances to the Huanan Seafood Market of early Covid cases with known links to the market versus cases without known links shows results apparently incompatible with a location model lacking proximity ascertainment bias. The sign of the difference instead agrees with a model in which such ascertainment bias is large. In the presence of such bias inferences based on the clustering of case locations become unreliable."}, "https://arxiv.org/abs/2007.05637": {"title": "Multilevel Digital Contact Tracing", "link": "https://arxiv.org/abs/2007.05637", "description": "arXiv:2007.05637v4 Announce Type: replace-cross \nAbstract: Digital contact tracing plays a crucial role in alleviating an outbreak, and designing multilevel digital contact tracing for a country is an open problem due to the analysis of large volumes of temporal contact data. We develop a multilevel digital contact tracing framework that constructs dynamic contact graphs from the proximity contact data. Prominently, we introduce the edge label of the contact graph as a binary circular contact queue, which holds the temporal social interactions during the incubation period. After that, our algorithm prepares the direct and indirect (multilevel) contact list for a given set of infected persons from the contact graph. Finally, the algorithm constructs the infection pathways for the trace list. We implement the framework and validate the contact tracing process with synthetic and real-world data sets. In addition, analysis reveals that for COVID-19 close contact parameters, the framework takes reasonable space and time to create the infection pathways. Our framework can apply to any epidemic spreading by changing the algorithm's parameters."}, "https://arxiv.org/abs/2305.14375": {"title": "MGL2Rank: Learning to Rank the Importance of Nodes in Road Networks Based on Multi-Graph Fusion", "link": "https://arxiv.org/abs/2305.14375", "description": "arXiv:2305.14375v3 Announce Type: replace-cross \nAbstract: The identification of important nodes with strong propagation capabilities in road networks is a vital topic in urban planning. Existing methods for evaluating the importance of nodes in traffic networks only consider topological information and traffic volumes, the diversity of the traffic characteristics in road networks, such as the number of lanes and average speed of road segments, is ignored, thus limiting their performance. To solve this problem, we propose a graph learning-based framework (MGL2Rank) that integrates the rich characteristics of road networks to rank the importance of nodes. This framework comprises an embedding module containing a sampling algorithm (MGWalk) and an encoder network to learn the latent representations for each road segment. MGWalk utilizes multigraph fusion to capture the topology of road networks and establish associations between road segments based on their attributes. The obtained node representation is then used to learn the importance ranking of the road segments. Finally, a synthetic dataset is constructed for ranking tasks based on the regional road network of Shenyang City, and the ranking results on this dataset demonstrate the effectiveness of our method. The data and source code for MGL2Rank are available at https://github.com/iCityLab/MGL2Rank."}, "https://arxiv.org/abs/2404.14423": {"title": "A Compositional Approach to Higher-Order Structure in Complex Systems", "link": "https://arxiv.org/abs/2404.14423", "description": "arXiv:2404.14423v2 Announce Type: replace-cross \nAbstract: Relating microscopic interactions to macroscopic observables is a central challenge in the study of complex systems. Addressing this question requires understanding both pairwise and higher-order interactions, but the latter are less well understood. Here, we show that the M\\\"obius inversion theorem provides a general mathematical formalism for deriving higher-order interactions from macroscopic observables, relative to a chosen decomposition of the system into parts. Applying this framework to a diverse range of systems, we demonstrate that many existing notions of higher-order interactions, from epistasis in genetics and many-body couplings in physics, to synergy in game theory and artificial intelligence, naturally arise from an appropriate mereological decomposition. By revealing the common mathematical structure underlying seemingly disparate phenomena, our work highlights the fundamental role of decomposition choice in the definition and estimation of higher-order interactions. We discuss how this unifying perspective can facilitate the transfer of insights between domains, guide the selection of appropriate system decompositions, and motivate the search for novel interaction types through creative decomposition strategies. More broadly, our results suggest that the M\\\"obius inversion theorem provides a powerful lens for understanding the emergence of complex behaviour from the interplay of microscopic parts, with applications across a wide range of disciplines."}, "https://arxiv.org/abs/2405.12244": {"title": "Real-Time Go-Around Prediction: A case study of JFK airport", "link": "https://arxiv.org/abs/2405.12244", "description": "arXiv:2405.12244v1 Announce Type: new \nAbstract: In this paper, we employ the long-short-term memory model (LSTM) to predict the real-time go-around probability as an arrival flight is approaching JFK airport and within 10 nm of the landing runway threshold. We further develop methods to examine the causes to go-around occurrences both from a global view and an individual flight perspective. According to our results, in-trail spacing, and simultaneous runway operation appear to be the top factors that contribute to overall go-around occurrences. We then integrate these pre-trained models and analyses with real-time data streaming, and finally develop a demo web-based user interface that integrates the different components designed previously into a real-time tool that can eventually be used by flight crews and other line personnel to identify situations in which there is a high risk of a go-around."}, "https://arxiv.org/abs/2405.12253": {"title": "The statistical and dynamic modeling of the first part of the 2013-2014 Euromaidan protests in Ukraine: The Revolution of Dignity and preceding times", "link": "https://arxiv.org/abs/2405.12253", "description": "arXiv:2405.12253v1 Announce Type: new \nAbstract: Ukraine's tug-of-war between Russia and the West has had significant and lasting consequences for the country. In 2013, Viktor Yanukovych, the Ukrainian president aligned with Russia, opted against signing an association agreement with the European Union. This agreement aimed to facilitate trade and travel between the EU and Ukraine. This decision sparked widespread protests that coalesced in Kyiv's Maidan Square, eventually becoming known as the Euromaidan protests. In this study, we analyze the protest data from 2013, sourced from Ukraine's Center for Social and Labor Research. Despite the dataset's limitations and occasional inconsistencies, we demonstrate the extraction of valuable insights and the construction of a descriptive model from such data. Our investigation reveals a pre-existing state of self-excitation within the system even before the onset of the Euromaidan protests. This self-excitation intensified during the Euromaidan protests. A statistical analysis indicates that the government's utilization of force correlates with increased future protests, exacerbating rather than quelling the protest movement. Furthermore, we introduce the implementation of Hawkes process models to comprehend the spatiotemporal dynamics of the protest activity. Our findings highlight that, while protest activities spread across the entire country, the driving force behind the dynamics of these protests was the level of activity in Kyiv. Furthermore, in contrast to prior research that emphasized geographical proximity as a key predictor of event propagation, our study illustrates that the political alignment among oblasts, which are the distinct municipalities comprising Ukraine, had a more profound impact than mere geographic distance. This underscores the significance of social and cultural factors in molding the trajectory of political movements."}, "https://arxiv.org/abs/2405.12566": {"title": "Unveiling Online Conspiracy Theorists: a Text-Based Approach and Characterization", "link": "https://arxiv.org/abs/2405.12566", "description": "arXiv:2405.12566v1 Announce Type: new \nAbstract: In today's digital landscape, the proliferation of conspiracy theories within the disinformation ecosystem of online platforms represents a growing concern. This paper delves into the complexities of this phenomenon. We conducted a comprehensive analysis of two distinct X (formerly known as Twitter) datasets: one comprising users with conspiracy theorizing patterns and another made of users lacking such tendencies and thus serving as a control group. The distinguishing factors between these two groups are explored across three dimensions: emotions, idioms, and linguistic features. Our findings reveal marked differences in the lexicon and language adopted by conspiracy theorists with respect to other users. We developed a machine learning classifier capable of identifying users who propagate conspiracy theories based on a rich set of 871 features. The results demonstrate high accuracy, with an average F1 score of 0.88. Moreover, this paper unveils the most discriminating characteristics that define conspiracy theory propagators."}, "https://arxiv.org/abs/2405.12642": {"title": "Combining Twitter and Mobile Phone Data to Observe Border-Rush: The Turkish-European Border Opening", "link": "https://arxiv.org/abs/2405.12642", "description": "arXiv:2405.12642v1 Announce Type: new \nAbstract: Following Turkey's 2020 decision to revoke border controls, many individuals journeyed towards the Greek, Bulgarian, and Turkish borders. However, the lack of verifiable statistics on irregular migration and discrepancies between media reports and actual migration patterns require further exploration. The objective of this study is to bridge this knowledge gap by harnessing novel data sources, specifically mobile phone and Twitter data, to construct estimators of cross-border mobility and to cultivate a qualitative comprehension of the unfolding events. By employing a migration diplomacy framework, we analyse emergent mobility patterns at the border. Our findings demonstrate the potential of mobile phone data for quantitative metrics and Twitter data for qualitative understanding. We underscore the ethical implications of leveraging Big Data, particularly considering the vulnerability of the population under study. This underscores the imperative for exhaustive research into the socio-political facets of human mobility, with the aim of discerning the potentialities, limitations, and risks inherent in these data sources and their integration. This scholarly endeavour contributes to a more nuanced understanding of migration dynamics and paves the way for the formulation of regulations that preclude misuse and oppressive surveillance, thereby ensuring a more accurate representation of migration realities."}, "https://arxiv.org/abs/2405.12764": {"title": "Detecting and Mitigating Bias in Algorithms Used to Disseminate Information in Social Networks", "link": "https://arxiv.org/abs/2405.12764", "description": "arXiv:2405.12764v1 Announce Type: new \nAbstract: Social connections are a conduit through which individuals communicate, information propagates, and diseases spread. Identifying individuals that are more likely to adopt ideas or technologies and spread them to others is essential in order to develop effective information campaigns, fight epidemics, and to maximize the reach of limited resources. Consequently a lot of work has focused on identifying sets of influencers. Here we show that seeding information using these influence maximization methods, only benefits connected and central individuals, consistently leaving the most vulnerable behind. Our results highlights troublesome outcomes of influence maximization algorithms: they do not disseminate information in an equitable manner threatening to create an increasingly unequal society. To overcome this issue we devise a simple, multi-objective algorithm, which maximises both influence and information equity. Our work demonstrates how to find fairer influencer sets, highlighting that in our search for maximizing information, we do not need to compromise on information equality."}, "https://arxiv.org/abs/2405.12797": {"title": "Refined Graph Encoder Embedding via Self-Training and Latent Community Recovery", "link": "https://arxiv.org/abs/2405.12797", "description": "arXiv:2405.12797v1 Announce Type: new \nAbstract: This paper introduces a refined graph encoder embedding method, enhancing the original graph encoder embedding using linear transformation, self-training, and hidden community recovery within observed communities. We provide the theoretical rationale for the refinement procedure, demonstrating how and why our proposed method can effectively identify useful hidden communities via stochastic block models, and how the refinement method leads to improved vertex embedding and better decision boundaries for subsequent vertex classification. The efficacy of our approach is validated through a collection of simulated and real-world graph data."}, "https://arxiv.org/abs/2405.12340": {"title": "Cascade-based Randomization for Inferring Causal Effects under Diffusion Interference", "link": "https://arxiv.org/abs/2405.12340", "description": "arXiv:2405.12340v1 Announce Type: cross \nAbstract: The presence of interference, where the outcome of an individual may depend on the treatment assignment and behavior of neighboring nodes, can lead to biased causal effect estimation. Current approaches to network experiment design focus on limiting interference through cluster-based randomization, in which clusters are identified using graph clustering, and cluster randomization dictates the node assignment to treatment and control. However, cluster-based randomization approaches perform poorly when interference propagates in cascades, whereby the response of individuals to treatment propagates to their multi-hop neighbors. When we have knowledge of the cascade seed nodes, we can leverage this interference structure to mitigate the resulting causal effect estimation bias. With this goal, we propose a cascade-based network experiment design that initiates treatment assignment from the cascade seed node and propagates the assignment to their multi-hop neighbors to limit interference during cascade growth and thereby reduce the overall causal effect estimation error. Our extensive experiments on real-world and synthetic datasets demonstrate that our proposed framework outperforms the existing state-of-the-art approaches in estimating causal effects in network data."}, "https://arxiv.org/abs/2405.12474": {"title": "How Universal Polynomial Bases Enhance Spectral Graph Neural Networks: Heterophily, Over-smoothing, and Over-squashing", "link": "https://arxiv.org/abs/2405.12474", "description": "arXiv:2405.12474v1 Announce Type: cross \nAbstract: Spectral Graph Neural Networks (GNNs), alternatively known as graph filters, have gained increasing prevalence for heterophily graphs. Optimal graph filters rely on Laplacian eigendecomposition for Fourier transform. In an attempt to avert prohibitive computations, numerous polynomial filters have been proposed. However, polynomials in the majority of these filters are predefined and remain fixed across different graphs, failing to accommodate the varying degrees of heterophily. Addressing this gap, we demystify the intrinsic correlation between the spectral property of desired polynomial bases and the heterophily degrees via thorough theoretical analyses. Subsequently, we develop a novel adaptive heterophily basis wherein the basis vectors mutually form angles reflecting the heterophily degree of the graph. We integrate this heterophily basis with the homophily basis to construct a universal polynomial basis UniBasis, which devises a polynomial filter based graph neural network - UniFilter. It optimizes the convolution and propagation in GNN, thus effectively limiting over-smoothing and alleviating over-squashing. Our extensive experiments, conducted on a diverse range of real-world and synthetic datasets with varying degrees of heterophily, support the superiority of UniFilter. These results not only demonstrate the universality of UniBasis but also highlight its proficiency in graph explanation."}, "https://arxiv.org/abs/2311.09262": {"title": "Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values", "link": "https://arxiv.org/abs/2311.09262", "description": "arXiv:2311.09262v3 Announce Type: replace \nAbstract: The potential impact of an academic paper is determined by various factors, including its popularity and contribution. Existing models usually estimate original citation counts based on static graphs and fail to differentiate values from nuanced perspectives. In this study, we propose a novel graph neural network to Disentangle the Potential impacts of Papers into Diffusion, Conformity, and Contribution values (called DPPDCC). Given a target paper, DPPDCC encodes temporal and structural features within the constructed dynamic heterogeneous graph. Particularly, to capture the knowledge flow, we emphasize the importance of comparative and co-cited/citing information between papers and aggregate snapshots evolutionarily. To unravel popularity, we contrast augmented graphs to extract the essence of diffusion and predict the accumulated citation binning to model conformity. We further apply orthogonal constraints to encourage distinct modeling of each perspective and preserve the inherent value of contribution. To evaluate models' generalization for papers published at various times, we reformulate the problem by partitioning data based on specific time points to mirror real-world conditions. Extensive experimental results on three datasets demonstrate that DPPDCC significantly outperforms baselines for previously, freshly, and immediately published papers. Further analyses confirm its robust capabilities. We will make our datasets and codes publicly available."}, "https://arxiv.org/abs/2401.09310": {"title": "Asymmetric games on networks: mapping to Ising models and bounded rationality", "link": "https://arxiv.org/abs/2401.09310", "description": "arXiv:2401.09310v2 Announce Type: replace \nAbstract: We investigate the dynamics of coordination and consensus in an agent population. Considering agents endowed with bounded rationality, we study asymmetric coordination games using a mapping to random field Ising models. In doing so, we investigate the relationship between group coordination and agent rationality. Analytical calculations and numerical simulations of the proposed model lead to novel insight into opinion dynamics. For instance, we find that bounded rationality and preference intensity can determine a series of possible scenarios with different levels of opinion polarization. To conclude, we deem our investigation opens a new avenue for studying game dynamics through methods of statistical physics."}, "https://arxiv.org/abs/2403.02867": {"title": "Scalable Continuous-time Diffusion Framework for Network Inference and Influence Estimation", "link": "https://arxiv.org/abs/2403.02867", "description": "arXiv:2403.02867v2 Announce Type: replace \nAbstract: The study of continuous-time information diffusion has been an important area of research for many applications in recent years. When only the diffusion traces (cascades) are accessible, cascade-based network inference and influence estimation are two essential problems to explore. Alas, existing methods exhibit limited capability to infer and process networks with more than a few thousand nodes, suffering from scalability issues. In this paper, we view the diffusion process as a continuous-time dynamical system, based on which we establish a continuous-time diffusion model. Subsequently, we instantiate the model to a scalable and effective framework (FIM) to approximate the diffusion propagation from available cascades, thereby inferring the underlying network structure. Furthermore, we undertake an analysis of the approximation error of FIM for network inference. To achieve the desired scalability for influence estimation, we devise an advanced sampling technique and significantly boost the efficiency. We also quantify the effect of the approximation error on influence estimation theoretically. Experimental results showcase the effectiveness and superior scalability of FIM on network inference and influence estimation."}, "https://arxiv.org/abs/2405.13094": {"title": "KPG: Key Propagation Graph Generator for Rumor Detection based on Reinforcement Learning", "link": "https://arxiv.org/abs/2405.13094", "description": "arXiv:2405.13094v1 Announce Type: new \nAbstract: The proliferation of rumors on social media platforms during significant events, such as the US elections and the COVID-19 pandemic, has a profound impact on social stability and public health. Existing approaches for rumor detection primarily rely on propagation graphs to enhance model effectiveness. However, the presence of noisy and irrelevant structures during the propagation process limits the efficacy of these approaches. To tackle this issue, techniques such as weight adjustment and data augmentation have been proposed. However, these techniques heavily depend on rich original propagation structures, thus hindering performance when dealing with rumors that lack sufficient propagation information in the early propagation stages. In this paper, we propose Key Propagation Graph Generator (KPG), a novel reinforcement learning-based rumor detection framework that generates contextually coherent and informative propagation patterns for events with insufficient topology information, while also identifies indicative substructures for events with redundant and noisy propagation structures. KPG consists of two key components: the Candidate Response Generator (CRG) and the Ending Node Selector (ENS). CRG learns the latent distribution from refined propagation patterns, filtering out noise and generating new candidates for ENS. Simultaneously, ENS identifies the most influential substructures within propagation graphs and generates training data for CRG. Moreover, we introduce an end-to-end framework that utilizes rewards to guide the entire training process via a pre-trained graph neural network. Extensive experiments conducted on four datasets demonstrate the superiority of our KPG compared to the state-of-the-art approaches."}, "https://arxiv.org/abs/2405.13224": {"title": "Integrating behavioral experimental findings into dynamical models to inform social change interventions", "link": "https://arxiv.org/abs/2405.13224", "description": "arXiv:2405.13224v1 Announce Type: new \nAbstract: Addressing global challenges -- from public health to climate change -- often involves stimulating the large-scale adoption of new products or behaviors. Research traditions that focus on individual decision making suggest that achieving this objective requires better identifying the drivers of individual adoption choices. On the other hand, computational approaches rooted in complexity science focus on maximizing the propagation of a given product or behavior throughout social networks of interconnected adopters. The integration of these two perspectives -- although advocated by several research communities -- has remained elusive so far. Here we show how achieving this integration could inform seeding policies to facilitate the large-scale adoption of a given behavior or product. Drawing on complex contagion and discrete choice theories, we propose a method to estimate individual-level thresholds to adoption, and validate its predictive power in two choice experiments. By integrating the estimated thresholds into computational simulations, we show that state-of-the-art seeding methods for social influence maximization might be suboptimal if they neglect individual-level behavioral drivers, which can be corrected through the proposed experimental method."}, "https://arxiv.org/abs/2405.13480": {"title": "What is a typical signalized intersection in a city? A pipeline for intersection data imputation from OpenStreetMap", "link": "https://arxiv.org/abs/2405.13480", "description": "arXiv:2405.13480v1 Announce Type: new \nAbstract: Signalized intersections, arguably the most complicated type of traffic scenario, are essential to urban mobility systems. With recent advancements in intelligent transportation technologies, signalized intersections have great prospects for making transportation greener, safer, and faster. Several studies have been conducted focusing on intersection-level control and optimization. However, arbitrarily structured signalized intersections that are often used do not represent the ground-truth distribution, and there is no standardized way that exists to extract information about real-world signalized intersections. As the largest open-source map in the world, OpenStreetMap (OSM) has been used by many transportation researchers for a variety of studies, including intersection-level research such as adaptive traffic signal control and eco-driving. However, the quality of OSM data has been a serious concern.\n  In this paper, we propose a pipeline for effectively extracting information about signalized intersections from OSM and constructing a comprehensive dataset. We thoroughly discuss challenges related to this task and we propose our solution for each challenge. We also use Salt Lake City as an example to demonstrate the performance of our methods. The pipeline has been published as an open-source Python library so everyone can freely download and use it to facilitate their research. Hopefully, this paper can serve as a starting point that inspires more efforts to build a standardized and systematic data pipeline for various types of transportation problems."}, "https://arxiv.org/abs/2405.13530": {"title": "Through energy droughts: hydropower's ability to sustain a high output", "link": "https://arxiv.org/abs/2405.13530", "description": "arXiv:2405.13530v1 Announce Type: new \nAbstract: Previous research has raised concerns about energy droughts in renewables-based energy systems. This study explores the ability of reservoir hydropower to sustain a high output and, thereby, mitigate such energy droughts. Using detailed modelling, we estimate that Swedish hydropower can sustain 67-92% of its installed capacity for 3 weeks, with higher values possible in springtime. The variation of the sustained output, equivalent to the capacity of 3-4 Swedish nuclear reactors, under-scores the importance of understanding the potential output levels when devising strategies to counteract energy droughts. Moreover, we find that regulations imposed on the flows in river bottlenecks hinder higher sustained output levels. With the upcoming renewal of environmental permits for hydropower plants in Sweden, these findings provide valuable insights for policymakers. Furthermore, the sustained output capabilities demonstrated in this study challenge the prevalent simplified representations of hydropower in energy models, suggesting a need for more-sophisticated modelling approaches."}, "https://arxiv.org/abs/2405.13670": {"title": "GNN-based Anomaly Detection for Encoded Network Traffic", "link": "https://arxiv.org/abs/2405.13670", "description": "arXiv:2405.13670v1 Announce Type: new \nAbstract: The early research report explores the possibility of using Graph Neural Networks (GNNs) for anomaly detection in internet traffic data enriched with information. While recent studies have made significant progress in using GNNs for anomaly detection in finance, multivariate time-series, and biochemistry domains, there is limited research in the context of network flow data. In this report, we explore the idea that leverages information-enriched features extracted from network flow packet data to improve the performance of GNN in anomaly detection. The idea is to utilize feature encoding (binary, numerical, and string) to capture the relationships between the network components, allowing the GNN to learn latent relationships and better identify anomalies."}, "https://arxiv.org/abs/2405.14100": {"title": "Water Management Considerations for a Self-Sustaining Moonbase", "link": "https://arxiv.org/abs/2405.14100", "description": "arXiv:2405.14100v1 Announce Type: new \nAbstract: The most pragmatic first step in the all-but-inevitable 3rd-millennium V\\\"olkerwanderung of humanity throughout the Solar System is the establishment of a permanent human presence on the Moon. This research examines: 1. the human, agricultural, and technical water needs of a 100-person, 500 m x 100 m x 6 m self-sustaining lunar colony; 2. choosing a strategic location for the moonbase; 3. a heat drill model by which the needed lunar water ice could be sublimated; and 4. the robust water treatment and recovery infrastructure and water management personnel that would be needed for a self-sustaining moonbase."}, "https://arxiv.org/abs/2405.14168": {"title": "A generative model for community types in directed networks", "link": "https://arxiv.org/abs/2405.14168", "description": "arXiv:2405.14168v1 Announce Type: new \nAbstract: Large complex networks are often organized into groups or communities. In this paper, we introduce and investigate a generative model of network evolution that reproduces all four pairwise community types that exist in directed networks: assortative, core-periphery, disassortative, and the newly introduced source-basin type. We fix the number of nodes and the community membership of each node, allowing node connectivity to change through rewiring mechanisms that depend on the community membership of the involved nodes. We determine the dependence of the community relationship on the model parameters using a mean-field solution. It reveals that a difference in the swap probabilities of the two communities is a necessary condition to obtain a core-periphery relationship and that a difference in the average in-degree of the communities is a necessary condition for a source-basin relationship. More generally, our analysis reveals multiple possible scenarios for the transition between the different structure types, and sheds light on the mechanisms underlying the observation of the different types of communities in network data."}, "https://arxiv.org/abs/2405.14194": {"title": "Graphlets correct for the topological information missed by random walks", "link": "https://arxiv.org/abs/2405.14194", "description": "arXiv:2405.14194v1 Announce Type: new \nAbstract: Random walks are widely used for mining networks due to the computational efficiency of computing them. For instance, graph representation learning learns a d-dimensional embedding space, so that the nodes that tend to co-occur on random walks (a proxy of being in the same network neighborhood) are close in the embedding space. Specific local network topology (i.e., structure) influences the co-occurrence of nodes on random walks, so random walks of limited length capture only partial topological information, hence diminishing the performance of downstream methods. We explicitly capture all topological neighborhood information and improve performance by introducing orbit adjacencies that quantify the adjacencies of two nodes as co-occurring on a given pair of graphlet orbits, which are symmetric positions on graphlets (small, connected, non-isomorphic, induced subgraphs of a large network). Importantly, we mathematically prove that random walks on up to k nodes capture only a subset of all the possible orbit adjacencies for up to k-node graphlets. Furthermore, we enable orbit adjacency-based analysis of networks by developing an efficient GRaphlet-orbit ADjacency COunter (GRADCO), which exhaustively computes all 28 orbit adjacency matrices for up to four-node graphlets. Note that four-node graphlets suffice, because real networks are usually small-world. In large networks on around 20,000 nodes, GRADCOcomputesthe28matricesinminutes. Onsixrealnetworksfromvarious domains, we compare the performance of node-label predictors obtained by using the network embeddings based on our orbit adjacencies to those based on random walks. We find that orbit adjacencies, which include those unseen by random walks, outperform random walk-based adjacencies, demonstrating the importance of the inclusion of the topological neighborhood information that is unseen by random walks."}, "https://arxiv.org/abs/2405.14503": {"title": "Radial analysis and scaling of housing prices in French urban areas", "link": "https://arxiv.org/abs/2405.14503", "description": "arXiv:2405.14503v1 Announce Type: new \nAbstract: Urban scaling laws summarize how urban attributes evolve with city size. Recent criticism questions notably the aggregate view of this approach, which leads to neglecting the internal structure of cities. This is all the more relevant for housing prices due to their important variations across space. Based on a dataset compiling millions of real estate transactions over the period 2017-2021, we investigate the regularities of the radial (center-periphery) profiles of housing prices across cities, with respect to their size. Results are threefold. First, they corroborate prior findings in the urban scaling literature stating that largest cities agglomerate higher housing prices. Second, we find that housing price radial profiles scale in three dimensions with the power 1/5 of city population. After rescaling, great regularities between radial profiles can be observed, although some locational amenities have a significant impact on prices. Third, it appears that our rescaled profiles approach fails to explain housing price variations in the city center across cities. In fact, prices near the city center rise much faster with city size than those in the periphery. This has strong implications for low-income households seeking homeownership, because prohibitive prices in the center may contribute to pushing them out into peripheral locations."}, "https://arxiv.org/abs/2405.14543": {"title": "Initial Burst of Disruptive Efforts over Individual Scientific Careers", "link": "https://arxiv.org/abs/2405.14543", "description": "arXiv:2405.14543v1 Announce Type: new \nAbstract: Despite persistent efforts to understand the dynamics of creativity of scientists over careers in terms of productivity, impact, and prize, little is known about the dynamics of scientists' disruptive efforts that affect individual academic careers and drive scientific advance. Drawing on millions of data over six decades and across nineteen disciplines, associating the publication records of individual scientists with the disruption index, we systematically quantify the temporal pattern of disruptive ideas over individual scientific careers, providing a detailed understanding of the macro phenomenon of scientific stagnation from the individual perspective. We start by checking the relationship between disruption-based and citation-based publication profiles. Next, we observe the finite inequality in the disruptive productivity of scientists, diminishing gradually as the level of disruption increases. We then identify the initial burst phenomenon in disruption dynamics. It is further revealed that while early engagement in high disruption frictions away initial productivity, compared to initial advantage in productivity or impact, initial high disruption ensures more subsequent academic viability evidenced by a longer career span and relatively final higher productivity, but does not necessarily guarantee academic success throughout careers. Further analysis shows that increasing disruptive work is uncorrelated to overall productivity but negatively correlated with the overall impact. However, increasing disruptive work in the early career is associated with higher overall productivity, yet lower overall productivity in the later career. Our research underscores the urgent need for a policy shift that encourages a balance between the pursuit of disruptive efforts and the achievement of impactful outcomes."}, "https://arxiv.org/abs/2405.14717": {"title": "The impact of temporal hydrogen regulation on hydrogen exporters and their domestic energy transition", "link": "https://arxiv.org/abs/2405.14717", "description": "arXiv:2405.14717v1 Announce Type: new \nAbstract: As global demand for green hydrogen rises, potential hydrogen exporters move into the spotlight. However, the large-scale installation of on-grid hydrogen electrolysis for export can have profound impacts on domestic energy prices and energy-related emissions. Our investigation explores the interplay of hydrogen exports, domestic energy transition and temporal hydrogen regulation, employing a sector-coupled energy model in Morocco. We find substantial co-benets of domestic climate change mitigation and hydrogen exports, whereby exports can reduce domestic electricity prices while mitigation reduces hydrogen export prices. However, increasing hydrogen exports quickly in a system that is still dominated by fossil fuels can substantially raise domestic electricity prices, if green hydrogen production is not regulated. Surprisingly, temporal matching of hydrogen production lowers domestic electricity cost by up to 31% while the effect on exporters is minimal. This policy instrument can steer the welfare (re-)distribution between hydrogen exporting firms, hydrogen importers, and domestic electricity consumers and hereby increases acceptance among actors."}, "https://arxiv.org/abs/2310.08029": {"title": "Increasing the Earth's Albedo: The K\\\"ohler Equation at Sea", "link": "https://arxiv.org/abs/2310.08029", "description": "arXiv:2310.08029v2 Announce Type: cross \nAbstract: Increasing marine haze and clouds has been considered as a possible means of increasing the Earth's albedo. This would reduce Solar heating and global warming, counteracting the effects of the anthropogenic increase in greenhouse gases. One proposed method of doing so would inject small droplets of seawater or condensation nuclei into the marine boundary layer, creating artificial haze and cloud. The equilibrium size of such droplets is described by the K\\\"{o}hler equation that includes the vapor pressure reduction attributable to the solute according to Raoult's law and the vapor pressure increase of a small droplet as a result of surface tension according to Kelvin. Here we apply this classic result to small droplets in the marine boundary layer, where the partial pressure of water vapor is less than the equilibrium vapor pressure because it is in equilibrium with the saline ocean. We calculate the equilibrium size of a droplet containing dissolved ions and find that the radius of a droplet of seawater shrinks greatly before it achieves equilibrium."}, "https://arxiv.org/abs/2405.13005": {"title": "Understanding the Rare Inflammatory Disease Using Large Language Models and Social Media Data", "link": "https://arxiv.org/abs/2405.13005", "description": "arXiv:2405.13005v1 Announce Type: cross \nAbstract: Sarcoidosis is a rare inflammatory disease characterized by the formation of granulomas in various organs. The disease presents diagnostic and treatment challenges due to its diverse manifestations and unpredictable nature. In this study, we employed a Large Language Model (LLM) to analyze sarcoidosis-related discussions on the social media platform Reddit. Our findings underscore the efficacy of LLMs in accurately identifying sarcoidosis-related content. We discovered a wide array of symptoms reported by patients, with fatigue, swollen lymph nodes, and shortness of breath as the most prevalent. Prednisone was the most prescribed medication, while infliximab showed the highest effectiveness in improving prognoses. Notably, our analysis revealed disparities in prognosis based on age and gender, with women and younger patients experiencing good and polarized outcomes, respectively. Furthermore, unsupervised clustering identified three distinct patient subgroups (phenotypes) with unique symptom profiles, prognostic outcomes, and demographic distributions. Finally, sentiment analysis revealed a moderate negative impact on patients' mental health post-diagnosis, particularly among women and younger individuals. Our study represents the first application of LLMs to understand sarcoidosis through social media data. It contributes to understanding the disease by providing data-driven insights into its manifestations, treatments, prognoses, and impact on patients' lives. Our findings have direct implications for improving personalized treatment strategies and enhancing the quality of care for individuals living with sarcoidosis."}, "https://arxiv.org/abs/2405.13071": {"title": "A Novel Method for News Article Event-Based Embedding", "link": "https://arxiv.org/abs/2405.13071", "description": "arXiv:2405.13071v1 Announce Type: cross \nAbstract: Embedding news articles is a crucial tool for multiple fields, such as media bias detection, identifying fake news, and news recommendations. However, existing news embedding methods are not optimized for capturing the latent context of news events. In many cases, news embedding methods rely on full-textual information and neglect the importance of time-relevant embedding generation. Here, we aim to address these shortcomings by presenting a novel lightweight method that optimizes news embedding generation by focusing on the entities and themes mentioned in the articles and their historical connections to specific events. We suggest a method composed of three stages. First, we process and extract the events, entities, and themes for the given news articles. Second, we generate periodic time embeddings for themes and entities by training timely separated GloVe models on current and historical data. Lastly, we concatenate the news embeddings generated by two distinct approaches: Smooth Inverse Frequency (SIF) for article-level vectors and Siamese Neural Networks for embeddings with nuanced event-related information. To test and evaluate our method, we leveraged over 850,000 news articles and 1,000,000 events from the GDELT project. For validation purposes, we conducted a comparative analysis of different news embedding generation methods, applying them twice to a shared event detection task - first on articles published within the same day and subsequently on those published within the same month. Our experiments show that our method significantly improves the Precision-Recall (PR) AUC across all tasks and datasets. Specifically, we observed an average PR AUC improvement of 2.15% and 2.57% compared to SIF, as well as 2.57% and 2.43% compared to the semi-supervised approach for daily and monthly shared event detection tasks, respectively."}, "https://arxiv.org/abs/2405.13099": {"title": "The Role of Emotions in Informational Support Question-Response Pairs in Online Health Communities: A Multimodal Deep Learning Approach", "link": "https://arxiv.org/abs/2405.13099", "description": "arXiv:2405.13099v1 Announce Type: cross \nAbstract: This study explores the relationship between informational support seeking questions, responses, and helpfulness ratings in online health communities. We created a labeled data set of question-response pairs and developed multimodal machine learning and deep learning models to reliably predict informational support questions and responses. We employed explainable AI to reveal the emotions embedded in informational support exchanges, demonstrating the importance of emotion in providing informational support. This complex interplay between emotional and informational support has not been previously researched. The study refines social support theory and lays the groundwork for the development of user decision aids. Further implications are discussed."}, "https://arxiv.org/abs/2405.13341": {"title": "Wealth inequality and utility: Effect evaluation of redistribution and consumption morals using macro-econophysical coupled approach", "link": "https://arxiv.org/abs/2405.13341", "description": "arXiv:2405.13341v1 Announce Type: cross \nAbstract: Reducing wealth inequality and increasing utility are critical issues. This study reveals the effects of redistribution and consumption morals on wealth inequality and utility. To this end, we present a novel approach that couples the dynamic model of capital, consumption, and utility in macroeconomics with the interaction model of joint business and redistribution in econophysics. With this approach, we calculate the capital (wealth), the utility based on consumption, and the Gini index of these inequality using redistribution and consumption thresholds as moral parameters. The results show that: under-redistribution and waste exacerbate inequality; conversely, over-redistribution and stinginess reduce utility; and a balanced moderate moral leads to achieve both reduced inequality and increased utility. These findings provide renewed economic and numerical support for the moral importance known from philosophy, anthropology, and religion. The revival of redistribution and consumption morals should promote the transformation to a human mutual-aid economy, as indicated by philosopher and anthropologist, instead of the capitalist economy that has produced the current inequality. The practical challenge is to implement bottom-up social business, on a foothold of worker coops and platform cooperatives as a community against the state and the market, with moral consensus and its operation."}, "https://arxiv.org/abs/2405.13744": {"title": "A Privacy Measure Turned Upside Down? Investigating the Use of HTTP Client Hints on the Web", "link": "https://arxiv.org/abs/2405.13744", "description": "arXiv:2405.13744v1 Announce Type: cross \nAbstract: HTTP client hints are a set of standardized HTTP request headers designed to modernize and potentially replace the traditional user agent string. While the user agent string exposes a wide range of information about the client's browser and device, client hints provide a controlled and structured approach for clients to selectively disclose their capabilities and preferences to servers. Essentially, client hints aim at more effective and privacy-friendly disclosure of browser or client properties than the user agent string.\n  We present a first long-term study of the use of HTTP client hints in the wild. We found that despite being implemented in almost all web browsers, server-side usage of client hints remains generally low. However, in the context of third-party websites, which are often linked to trackers, the adoption rate is significantly higher. This is concerning because client hints allow the retrieval of more data from the client than the user agent string provides, and there are currently no mechanisms for users to detect or control this potential data leakage. Our work provides valuable insights for web users, browser vendors, and researchers by exposing potential privacy violations via client hints and providing help in developing remediation strategies as well as further research."}, "https://arxiv.org/abs/2405.14043": {"title": "Attitudes Towards Migration in a COVID-19 Context: Testing a Behavioral Immune System Hypothesis with Twitter Data", "link": "https://arxiv.org/abs/2405.14043", "description": "arXiv:2405.14043v1 Announce Type: cross \nAbstract: The COVID-19 outbreak implied many changes in the daily life of most of the world's population for a long time, prompting severe restrictions on sociality. The Behavioral Immune System (BIS) suggests that when facing pathogens, a psychological mechanism would be activated that, among other things, would generate an increase in prejudice and discrimination towards marginalized groups, including immigrants. This study aimed to test if people tend to enhance their rejection of minorities and foreign groups under the threat of contagious diseases, using the users' attitudes towards migrants in Twitter data from Chile, for pre-pandemic and pandemic contexts. Our results only partially support the BIS hypothesis, since threatened users increased their tweet production in the pandemic period, compared to empathetic users, but the latter grew in number and also increased the reach of their tweets between the two periods. We also found differences in the use of language between these types of users. Alternative explanations for these results may be context-dependent."}, "https://arxiv.org/abs/2405.14403": {"title": "Representative electricity price profiles for European day-ahead and intraday spot markets", "link": "https://arxiv.org/abs/2405.14403", "description": "arXiv:2405.14403v1 Announce Type: cross \nAbstract: We propose a method to construct representative price profiles of the day-ahead (DA) and the intraday (ID) electricity spot markets and use this method to provide examples of ready-to-use price data sets. In contrast to common scenario generation approaches, the method is deterministic and relies on a small number of degrees of freedom, with the aim to be well defined and easy to use. We thereby target an enhanced comparability of future research studies on demand-side management and energy cost optimization. We construct the price profiles based on historical time series from the spot markets of interest, e.g., European Power Exchange (EPEX) spot. To this end, we extract key price components from the data while also accounting for known dominant mechanisms in the price variation. Further, the method is able to preserve key statistical features of the historical data (e.g., mean and standard deviation) when constructing the benchmark profile. Finally, our approach ensures comparability of ID and DA price profiles by design, as their cumulative (integral) price can be made identical if needed."}, "https://arxiv.org/abs/2405.14761": {"title": "Effective & Ethical Mentorship in Physics and Astronomy through Grassroots Organizations", "link": "https://arxiv.org/abs/2405.14761", "description": "arXiv:2405.14761v1 Announce Type: cross \nAbstract: Effective and ethical mentorship practices are crucial to improving recruitment and retention especially for historically minoritized groups (HMGs). Spectrum is a diversity, inclusion, equity, and accessibility (DEIA) grassroots organization committed to empowering equitable excellence through sustainable change. By improving transparency and DEIA within the fields of physics and astronomy, we can empower the next generation of diverse scientists and increase field retention. Starting within our home department at George Mason University and moving outwards, we ensure our students leave as advocates for DEIA and AJEDI (access, justice, equity, diversity, and inclusion) through education and mentorship. Spectrum is providing professionally trained peer mentors to aid students in all facets of their academic and personal lives. Although the peer mentoring program existed since the creation of Spectrum in Spring 2020, we have recently developed and implemented a formal mentorship training for both student and faculty mentors thus increasing the quality, trustworthiness, and confidence of our mentors. Using the latest mentorship research available, this training is developed by Spectrum for George Mason University, with the ability to implement the training at any institution."}, "https://arxiv.org/abs/2108.01727": {"title": "Scalable Community Detection in Massive Networks Using Aggregated Relational Data", "link": "https://arxiv.org/abs/2108.01727", "description": "arXiv:2108.01727v3 Announce Type: replace \nAbstract: The mixed membership stochastic blockmodel (MMSB) is a popular Bayesian network model for community detection. Fitting such large Bayesian network models quickly becomes computationally infeasible when the number of nodes grows into hundreds of thousands and millions. In this paper we propose a novel mini-batch strategy based on aggregated relational data that leverages nodal information to fit MMSB to massive networks. We describe a scalable inference method that can utilize nodal information that often accompanies real-world networks. Conditioning on this extra information leads to a model that admits a parallel stochastic variational inference algorithm, utilizing stochastic gradients of bipartite graph formed from aggregated network ties between node subpopulations. We apply our method to a citation network with over two million nodes and 25 million edges, capturing explainable structure in this network. Our method recovers parameters and achieves better convergence on simulated networks generated according to the MMSB."}, "https://arxiv.org/abs/2401.08539": {"title": "Mapping low-resolution edges to high-resolution paths: the case of traffic measurements in cities", "link": "https://arxiv.org/abs/2401.08539", "description": "arXiv:2401.08539v2 Announce Type: replace \nAbstract: We consider the following problem : we have a high-resolution street network of a given city, and low-resolution measurements of traffic within this city. We want to associate to each measurement the set of streets corresponding to the observed traffic. To do so, we take benefit of specific properties of these data to match measured links to links in the street network. We propose several success criteria for the obtained matching. They show that the matching algorithm generally performs very well, and they give complementary ways to detect data discrepancies that makes any matching highly dubious."}, "https://arxiv.org/abs/2401.09647": {"title": "Large Language Models Help Reveal Unhealthy Diet and Body Concerns in Online Eating Disorders Communities", "link": "https://arxiv.org/abs/2401.09647", "description": "arXiv:2401.09647v2 Announce Type: replace \nAbstract: Eating disorders (ED), a severe mental health condition with high rates of mortality and morbidity, affect millions of people globally, especially adolescents. The proliferation of online communities that promote and normalize ED has been linked to this public health crisis. However, identifying harmful communities is challenging due to the use of coded language and other obfuscations. To address this challenge, we propose a novel framework to surface implicit attitudes of online communities by adapting large language models (LLMs) to the language of the community. We describe an alignment method and evaluate results along multiple dimensions of semantics and affect. We then use the community-aligned LLM to respond to psychometric questionnaires designed to identify ED in individuals. We demonstrate that LLMs can effectively adopt community-specific perspectives and reveal significant variations in eating disorder risks in different online communities. These findings highlight the utility of LLMs to reveal implicit attitudes and collective mindsets of communities, offering new tools for mitigating harmful content on social media."}, "https://arxiv.org/abs/2401.16504": {"title": "Effect of recommending users and opinions on the network connectivity and idea generation process", "link": "https://arxiv.org/abs/2401.16504", "description": "arXiv:2401.16504v2 Announce Type: replace \nAbstract: The growing reliance on online services underscores the crucial role of recommendation systems, especially on social media platforms seeking increased user engagement. This study investigates how recommendation systems influence the impact of personal behavioral traits on social network dynamics. It explores the interplay between homophily, users' openness to novel ideas, and recommendation-driven exposure to new opinions. Additionally, the research examines the impact of recommendation systems on the diversity of newly generated ideas, shedding light on the challenges and opportunities in designing effective systems that balance the exploration of new ideas with the risk of reinforcing biases or filtering valuable, unconventional concepts."}, "https://arxiv.org/abs/2402.19157": {"title": "Broken detailed balance and entropy production in directed networks", "link": "https://arxiv.org/abs/2402.19157", "description": "arXiv:2402.19157v3 Announce Type: replace \nAbstract: The structure of a complex network plays a crucial role in determining its dynamical properties. In this work, we show that the the degree to which a network is directed and hierarchically organised is closely associated with the degree to which its dynamics break detailed balance and produce entropy. We consider a range of dynamical processes and show how different directed network features affect their entropy production rate. We begin with an analytical treatment of a 2-node network followed by numerical simulations of synthetic networks using the preferential attachment and Erd\\\"os-Renyi algorithms. Next, we analyse a collection of 97 empirical networks to determine the effect of complex real-world topologies. Finally, we present a simple method for inferring broken detailed balance and directed network structure from multivariate time-series and apply our method to identify non-equilibrium dynamics and hierarchical organisation in both human neuroimaging and financial time-series. Overall, our results shed light on the consequences of directed network structure on non-equilibrium dynamics and highlight the importance and ubiquity of hierarchical organisation and non-equilibrium dynamics in real-world systems."}, "https://arxiv.org/abs/2404.00754": {"title": "Imitation dynamics and the replicator equation", "link": "https://arxiv.org/abs/2404.00754", "description": "arXiv:2404.00754v2 Announce Type: replace \nAbstract: Evolutionary game theory has impacted many fields of research by providing a mathematical framework for studying the evolution and maintenance of social and moral behaviors. This success is owed in large part to the demonstration that the central equation of this theory - the replicator equation - is the deterministic limit of a stochastic imitation (social learning) dynamics. Here we offer an alternative elementary proof of this result, which holds for the scenario where players compare their instantaneous (not average) payoffs to decide whether to maintain or change their strategies, and only more successful individuals can be imitated."}, "https://arxiv.org/abs/2404.04307": {"title": "PREDIS-MHI Thermal Data", "link": "https://arxiv.org/abs/2404.04307", "description": "arXiv:2404.04307v2 Announce Type: replace \nAbstract: Tertiary buildings could be an important lever to meet the goals necessitated by the energy transition. The availability of high-quality datasets from this sector will be a crucial enabler in meeting these goals by developing and testing new energy management approaches in the buildings. In this paper, we present the thermal energy datasets available and published online for the PREDIS-MHI zone of the GreEn-ER building, a tertiary building with more than a thousand sensors used for research, teaching, and administrative activities in Grenoble. PREDIS-MHI platform is a net-zero sub-section that is energetically isolated from the rest of the building. Its data has been used in a wide range of applications from indoor temperature forecasting, thermal simulation calibration, and even occupant comfort experiments"}, "https://arxiv.org/abs/2404.11465": {"title": "X-posing Free Speech: Examining the Impact of Moderation Relaxation on Online Social Networks", "link": "https://arxiv.org/abs/2404.11465", "description": "arXiv:2404.11465v2 Announce Type: replace \nAbstract: We investigate the impact of free speech and the relaxation of moderation on online social media platforms using Elon Musk's takeover of Twitter as a case study. By curating a dataset of over 10 million tweets, our study employs a novel framework combining content and network analysis. Our findings reveal a significant increase in the distribution of certain forms of hate content, particularly targeting the LGBTQ+ community and liberals. Network analysis reveals the formation of cohesive hate communities facilitated by influential bridge users, with substantial growth in interactions hinting at increased hate production and diffusion. By tracking the temporal evolution of PageRank, we identify key influencers, primarily self-identified far-right supporters disseminating hate against liberals and woke culture. Ironically, embracing free speech principles appears to have enabled hate speech against the very concept of freedom of expression and free speech itself. Our findings underscore the delicate balance platforms must strike between open expression and robust moderation to curb the proliferation of hate online."}, "https://arxiv.org/abs/2405.02856": {"title": "A tale of two emergent games: opinion dynamics in dynamical directed networks", "link": "https://arxiv.org/abs/2405.02856", "description": "arXiv:2405.02856v2 Announce Type: replace \nAbstract: Uni-directional social interactions are ubiquitous in real social networks whereas undirected interactions are intensively studied. We establish a voter model in a dynamical directed network. We analytically obtain the degree distribution of the evolving network at any given time. Furthermore, we find that the average degree is captured by an emergent game. On the other hand, we find that the fate of opinions is captured by another emergent game. Beyond expectation, the two emergent games are typically different due to the unidirectionality of the evolving networks. The Nash equilibrium analysis of the two games facilitates us to give the criterion under which the minority opinion with few disciples initially takes over the population eventually for in-group bias. Our work fosters the understanding of opinion dynamics ranging from methodology to research content."}, "https://arxiv.org/abs/2204.04510": {"title": "Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning", "link": "https://arxiv.org/abs/2204.04510", "description": "arXiv:2204.04510v4 Announce Type: replace-cross \nAbstract: Subgraph representation learning has emerged as an important problem, but it is by default approached with specialized graph neural networks on a large global graph. These models demand extensive memory and computational resources but challenge modeling hierarchical structures of subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation for learning representations of subgraphs. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. Demonstrating both theoretical and empirical evidence, S2N not only significantly reduces memory and computational costs compared to state-of-the-art models but also outperforms them by capturing both local and global structures of the subgraph. By leveraging graph coarsening methods, our method outperforms baselines even in a data-scarce setting with insufficient subgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned models with S2N translation can process 183 -- 711 times more subgraph samples than state-of-the-art models at a better or similar performance level."}, "https://arxiv.org/abs/2301.10960": {"title": "Visiting Distant Neighbors in Graph Convolutional Networks", "link": "https://arxiv.org/abs/2301.10960", "description": "arXiv:2301.10960v3 Announce Type: replace-cross \nAbstract: We extend the graph convolutional network method for deep learning on graph data to higher order in terms of neighboring nodes. In order to construct representations for a node in a graph, in addition to the features of the node and its immediate neighboring nodes, we also include more distant nodes in the calculations. In experimenting with a number of publicly available citation graph datasets, we show that this higher order neighbor visiting pays off by outperforming the original model especially when we have a limited number of available labeled data points for the training of the model."}, "https://arxiv.org/abs/2311.06840": {"title": "Omitted Labels in Causality: A Study of Paradoxes", "link": "https://arxiv.org/abs/2311.06840", "description": "arXiv:2311.06840v3 Announce Type: replace-cross \nAbstract: We explore what we call ``omitted label contexts,'' in which training data is limited to a subset of the possible labels. This setting is common among specialized human experts or specific focused studies. We lean on well-studied paradoxes (Simpson's and Condorcet) to illustrate the more general difficulties of causal inference in omitted label contexts. Contrary to the fundamental principles on which much of causal inference is built, we show that ``correct'' adjustments sometimes require non-exchangeable treatment and control groups. These pitfalls lead us to the study networks of conclusions drawn from different contexts and the structures the form, proving an interesting connection between these networks and social choice theory."}, "https://arxiv.org/abs/2312.09041": {"title": "Graph Neural Networks with Diverse Spectral Filtering", "link": "https://arxiv.org/abs/2312.09041", "description": "arXiv:2312.09041v3 Announce Type: replace-cross \nAbstract: Spectral Graph Neural Networks (GNNs) have achieved tremendous success in graph machine learning, with polynomial filters applied for graph convolutions, where all nodes share the identical filter weights to mine their local contexts. Despite the success, existing spectral GNNs usually fail to deal with complex networks (e.g., WWW) due to such homogeneous spectral filtering setting that ignores the regional heterogeneity as typically seen in real-world networks. To tackle this issue, we propose a novel diverse spectral filtering (DSF) framework, which automatically learns node-specific filter weights to exploit the varying local structure properly. Particularly, the diverse filter weights consist of two components -- A global one shared among all nodes, and a local one that varies along network edges to reflect node difference arising from distinct graph parts -- to balance between local and global information. As such, not only can the global graph characteristics be captured, but also the diverse local patterns can be mined with awareness of different node positions. Interestingly, we formulate a novel optimization problem to assist in learning diverse filters, which also enables us to enhance any spectral GNNs with our DSF framework. We showcase the proposed framework on three state-of-the-arts including GPR-GNN, BernNet, and JacobiConv. Extensive experiments over 10 benchmark datasets demonstrate that our framework can consistently boost model performance by up to 4.92% in node classification tasks, producing diverse filters with enhanced interpretability. Code is available at \\url{https://github.com/jingweio/DSF}."}, "https://arxiv.org/abs/2403.15855": {"title": "Initialisation and Topology Effects in Decentralised Federated Learning", "link": "https://arxiv.org/abs/2403.15855", "description": "arXiv:2403.15855v2 Announce Type: replace-cross \nAbstract: Fully decentralised federated learning enables collaborative training of individual machine learning models on distributed devices on a communication network while keeping the training data localised. This approach enhances data privacy and eliminates both the single point of failure and the necessity for central coordination. Our research highlights that the effectiveness of decentralised federated learning is significantly influenced by the network topology of connected devices. We propose a strategy for uncoordinated initialisation of the artificial neural networks, which leverages the distribution of eigenvector centralities of the nodes of the underlying communication network, leading to a radically improved training efficiency. Additionally, our study explores the scaling behaviour and choice of environmental parameters under our proposed initialisation strategy. This work paves the way for more efficient and scalable artificial neural network training in a distributed and uncoordinated environment, offering a deeper understanding of the intertwining roles of network structure and learning dynamics."}, "https://arxiv.org/abs/2405.14884": {"title": "The story around the first 4n signal", "link": "https://arxiv.org/abs/2405.14884", "description": "arXiv:2405.14884v1 Announce Type: new \nAbstract: The GANIL campaign around the first 4n signal was very peculiar. The beginning and end were both dictated by unexpected events that, unfortunately, do not fit within the streamlined format of standard scientific publications. However, they illustrate many aspects of how basic research should work, or at least does work. Therefore, I take this opportunity to share them with those not involved in the campaign, hoping that they will offer a better perspective of that research in particular and of basic research in general. As a disclaimer, this is only a personal recollection of those events."}, "https://arxiv.org/abs/2405.14902": {"title": "Global urban activity changes from COVID-19 physical distancing restrictions", "link": "https://arxiv.org/abs/2405.14902", "description": "arXiv:2405.14902v1 Announce Type: new \nAbstract: During the COVID-19 pandemic changes in human activity became widespread through official policies and organically in response to the virus's transmission, which in turn, impacted the environment and the economy. The pandemic has been described as a natural experiment that tested how social and economic disruptions impacted different components of the global Earth System. To move this beyond hypotheses, locally-resolved, globally-available measures of how, where, and when human activity changed are critically needed. Here we use satellite-derived nighttime lights to quantify and map daily changes in human activity that are atypical for each urban area globally for two years after the onset of the pandemic using machine learning anomaly detectors. Metrics characterizing changes in lights from pre-COVID baseline in human settlements and quality assurance measures are reported. This dataset, TRacking Anomalous COVID-19 induced changEs in NTL (TRACE-NTL), is the first to resolve COVID-19 disruptions for all metropolitan regions globally, daily. It is suitable to support a variety of post-pandemic studies that assess how changes in human activity impact environmental systems."}, "https://arxiv.org/abs/2405.14985": {"title": "Implicit degree bias in the link prediction task", "link": "https://arxiv.org/abs/2405.14985", "description": "arXiv:2405.14985v1 Announce Type: new \nAbstract: Link prediction -- a task of distinguishing actual hidden edges from random unconnected node pairs -- is one of the quintessential tasks in graph machine learning. Despite being widely accepted as a universal benchmark and a downstream task for representation learning, the validity of the link prediction benchmark itself has been rarely questioned. Here, we show that the common edge sampling procedure in the link prediction task has an implicit bias toward high-degree nodes and produces a highly skewed evaluation that favors methods overly dependent on node degree, to the extent that a ``null'' link prediction method based solely on node degree can yield nearly optimal performance. We propose a degree-corrected link prediction task that offers a more reasonable assessment that aligns better with the performance in the recommendation task. Finally, we demonstrate that the degree-corrected benchmark can more effectively train graph machine-learning models by reducing overfitting to node degrees and facilitating the learning of relevant structures in graphs."}, "https://arxiv.org/abs/2405.15498": {"title": "Node Accessibility Characterization of Radially-Grown Structures", "link": "https://arxiv.org/abs/2405.15498", "description": "arXiv:2405.15498v1 Announce Type: new \nAbstract: Complex systems have motivated continuing interest from the scientific community, leading to new concepts and methods. Growing systems represent a case of particular interest, as their topological, geometrical, and also dynamical properties change along time, as new elements are incorporated into the existing structure. In the present work, an approach is the case in which systems grown radially around some straight axis of reference, such as particle deposition on electrodes, or urban expansion along avenues, roads, coastline, or rivers, among several other possibilities. More specifically, we aim at characterizing the topological properties of simulated growing structures, which are represented as graphs, in terms of a measurement corresponding to the accessibility of each involved node. The incorporation of new elements (nodes and links) is performed preferentially to the angular orientation respectively to the reference axis. Several interesting results are reported, including the tendency of structures grown preferentially to the orientation normal to the axis to have smaller accessibility."}, "https://arxiv.org/abs/2405.15473": {"title": "Encoder Embedding for General Graph and Node Classification", "link": "https://arxiv.org/abs/2405.15473", "description": "arXiv:2405.15473v1 Announce Type: cross \nAbstract: Graph encoder embedding, a recent technique for graph data, offers speed and scalability in producing vertex-level representations from binary graphs. In this paper, we extend the applicability of this method to a general graph model, which includes weighted graphs, distance matrices, and kernel matrices. We prove that the encoder embedding satisfies the law of large numbers and the central limit theorem on a per-observation basis. Under certain condition, it achieves asymptotic normality on a per-class basis, enabling optimal classification through discriminant analysis. These theoretical findings are validated through a series of experiments involving weighted graphs, as well as text and image data transformed into general graph representations using appropriate distance metrics."}, "https://arxiv.org/abs/2405.15739": {"title": "Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias", "link": "https://arxiv.org/abs/2405.15739", "description": "arXiv:2405.15739v1 Announce Type: cross \nAbstract: Citation practices are crucial in shaping the structure of scientific knowledge, yet they are often influenced by contemporary norms and biases. The emergence of Large Language Models (LLMs) like GPT-4 introduces a new dynamic to these practices. Interestingly, the characteristics and potential biases of references recommended by LLMs that entirely rely on their parametric knowledge, and not on search or retrieval-augmented generation, remain unexplored. Here, we analyze these characteristics in an experiment using a dataset of 166 papers from AAAI, NeurIPS, ICML, and ICLR, published after GPT-4's knowledge cut-off date, encompassing 3,066 references in total. In our experiment, GPT-4 was tasked with suggesting scholarly references for the anonymized in-text citations within these papers. Our findings reveal a remarkable similarity between human and LLM citation patterns, but with a more pronounced high citation bias in GPT-4, which persists even after controlling for publication year, title length, number of authors, and venue. Additionally, we observe a large consistency between the characteristics of GPT-4's existing and non-existent generated references, indicating the model's internalization of citation patterns. By analyzing citation graphs, we show that the references recommended by GPT-4 are embedded in the relevant citation context, suggesting an even deeper conceptual internalization of the citation networks. While LLMs can aid in citation generation, they may also amplify existing biases and introduce new ones, potentially skewing scientific knowledge dissemination. Our results underscore the need for identifying the model's biases and for developing balanced methods to interact with LLMs in general."}, "https://arxiv.org/abs/2311.10837": {"title": "Evaluating the Relationship Between News Source Sharing and Political Beliefs", "link": "https://arxiv.org/abs/2311.10837", "description": "arXiv:2311.10837v2 Announce Type: replace \nAbstract: In an era marked by an abundance of news sources, access to information significantly influences public opinion. Notably, the bias of news sources often serves as an indicator of individuals' political leanings. This study explores this hypothesis by examining the news sharing behavior of politically active social media users, whose political ideologies were identified in a previous study. Using correspondence analysis, we estimate the Media Sharing Index (MSI), a measure that captures bias in media outlets and user preferences within a hidden space. During Argentina's 2019 election on Twitter, we observed a predictable pattern: center-right individuals predominantly shared media from center-right biased outlets. However, it is noteworthy that those with center-left inclinations displayed a more diverse media consumption, which is a significant finding. Despite a noticeable polarization based on political affiliation observed in a retweet network analysis, center-left users showed more diverse media sharing preferences, particularly concerning the MSI. Although these findings are specific to Argentina, the developed methodology can be applied in other countries to assess the correlation between users' political leanings and the media they share."}, "https://arxiv.org/abs/2302.00360": {"title": "Faster maximal clique enumeration in large real-world link streams", "link": "https://arxiv.org/abs/2302.00360", "description": "arXiv:2302.00360v3 Announce Type: replace-cross \nAbstract: Link streams offer a good model for representing interactions over time. They consist of links $(b,e,u,v)$, where $u$ and $v$ are vertices interacting during the whole time interval $[b,e]$. In this paper, we deal with the problem of enumerating maximal cliques in link streams. A clique is a pair $(C,[t_0,t_1])$, where $C$ is a set of vertices that all interact pairwise during the full interval $[t_0,t_1]$. It is maximal when neither its set of vertices nor its time interval can be increased. Some of the main works solving this problem are based on the famous Bron-Kerbosch algorithm for enumerating maximal cliques in graphs. We take this idea as a starting point to propose a new algorithm which matches the cliques of the instantaneous graphs formed by links existing at a given time $t$ to the maximal cliques of the link stream. We prove its validity and compute its complexity, which is better than the state-of-the art ones in many cases of interest. We also study the output-sensitive complexity, which is close to the output size, thereby showing that our algorithm is efficient. To confirm this, we perform experiments on link streams used in the state of the art, and on massive link streams, up to 100 million links. In all cases our algorithm is faster, mostly by a factor of at least 10 and up to a factor of $10^4$. Moreover, it scales to massive link streams for which the existing algorithms are not able to provide the solution."}, "https://arxiv.org/abs/2403.04679": {"title": "Canadian Physics Counts: An exploration of the diverse identities of physics students and professionals in Canada", "link": "https://arxiv.org/abs/2403.04679", "description": "arXiv:2403.04679v2 Announce Type: replace-cross \nAbstract: The lack of diversity in physics remains a persistent worldwide problem. Despite being a quantitative discipline which relies on measurements to construct and validate hypotheses, there remains a paucity of data on both demographics and experiences of marginalized groups. In Canada, there has never been a nationwide assessment of those studying or working in physics. Here, we present findings from Canadian Physics Counts: the first national survey of equity, diversity, and inclusion (EDI) in the Canadian physics community. Our intersectional approach allowed us to gather a wealth of information on gender identity, sexual orientation, race, disability, and more. Analyses revealed key findings, including the first data on physicists who identify as non-binary or gender diverse, as well as the first data on Black and Indigenous scholars. Black physicists (1.2%) and Indigenous physicists (.3%) were found to be the most underrepresented, while White men were overrepresented across all sectors. Among respondents with a disability, 5% reported receiving full accommodations for their required needs at their place of work or study. One in four respondents from BIPOC gender diverse backgrounds identified as being disabled, and the proportion of sexually diverse students who reported having a disability was more than three times higher than the proportion of heterosexual students with a disability. The data also revealed that students represented more demographic diversity than working professionals, highlighting the importance of acting today in order to retain the diverse physicists of tomorrow. Our analysis identifies areas for intervention and offers recommendations for building a diverse and inclusive physics community in Canada that can be a global exemplar."}, "https://arxiv.org/abs/2404.01679": {"title": "Event Detection from Social Media for Epidemic Prediction", "link": "https://arxiv.org/abs/2404.01679", "description": "arXiv:2404.01679v2 Announce Type: replace-cross \nAbstract: Social media is an easy-to-access platform providing timely updates about societal trends and events. Discussions regarding epidemic-related events such as infections, symptoms, and social interactions can be crucial for informing policymaking during epidemic outbreaks. In our work, we pioneer exploiting Event Detection (ED) for better preparedness and early warnings of any upcoming epidemic by developing a framework to extract and analyze epidemic-related events from social media posts. To this end, we curate an epidemic event ontology comprising seven disease-agnostic event types and construct a Twitter dataset SPEED with human-annotated events focused on the COVID-19 pandemic. Experimentation reveals how ED models trained on COVID-based SPEED can effectively detect epidemic events for three unseen epidemics of Monkeypox, Zika, and Dengue; while models trained on existing ED datasets fail miserably. Furthermore, we show that reporting sharp increases in the extracted events by our framework can provide warnings 4-9 weeks earlier than the WHO epidemic declaration for Monkeypox. This utility of our framework lays the foundations for better preparedness against emerging epidemics."}, "https://arxiv.org/abs/2405.04428": {"title": "BBK: a simpler, faster algorithm for enumerating maximal bicliques in large sparse bipartite graphs", "link": "https://arxiv.org/abs/2405.04428", "description": "arXiv:2405.04428v2 Announce Type: replace-cross \nAbstract: Bipartite graphs are a prevalent modeling tool for real-world networks, capturing interactions between vertices of two different types. Within this framework, bicliques emerge as crucial structures when studying dense subgraphs: they are sets of vertices such that all vertices of the first type interact with all vertices of the second type. Therefore, they allow identifying groups of closely related vertices of the network, such as individuals with similar interests or webpages with similar contents. This article introduces a new algorithm designed for the exhaustive enumeration of maximal bicliques within a bipartite graph. This algorithm, called BBK for Bipartite Bron-Kerbosch, is a new extension to the bipartite case of the Bron-Kerbosch algorithm, which enumerates the maximal cliques in standard (non-bipartite) graphs. It is faster than the state-of-the-art algorithms and allows the enumeration on massive bipartite graphs that are not manageable with existing implementations. We analyze it theoretically to establish two complexity formulas: one as a function of the input and one as a function of the output characteristics of the algorithm. We also provide an open-access implementation of BBK in C++, which we use to experiment and validate its efficiency on massive real-world datasets and show that its execution time is shorter in practice than state-of-the art algorithms. These experiments also show that the order in which the vertices are processed, as well as the choice of one of the two types of vertices on which to initiate the enumeration have an impact on the computation time."}, "https://arxiv.org/abs/2405.15838": {"title": "What You Shouldn't Know About Quantum Computers", "link": "https://arxiv.org/abs/2405.15838", "description": "arXiv:2405.15838v1 Announce Type: new \nAbstract: Whether you're a CEO strategizing the future of your company, a tech enthusiast debating your next career move, a high school teacher eager to enlighten your students, or simply tired of the relentless quantum hype, this is crafted just for you. Cutting through the complex jargon to deliver the straight facts on quantum computing, peeling away the layers of mystique to reveal the true potential and limitations of this groundbreaking technology. Prepare to have your misconceptions challenged, and your understanding deepened in this clear-eyed view of the quantum future, written to inform and inspire readers across the spectrum of curiosity and need."}, "https://arxiv.org/abs/2405.15893": {"title": "Quantifying Influencer Effects on Affective Polarization", "link": "https://arxiv.org/abs/2405.15893", "description": "arXiv:2405.15893v1 Announce Type: new \nAbstract: In an era where digital platforms increasingly mediate public discourse, grasping the complexities and nuances in affective polarization--especially as influenced by key figures on social media--has never been more vital. This study delves into the intricate web of interactions on Twitter, now rebranded as 'X', to unravel how influencer-led conversations catalyze shifts in public sentiment, laying bare the complex dynamics that underpin online polarization. Employing a novel methodological framework that includes counterfactual analysis, we analyze scenarios with and without specific influencer-led conversations. Our findings illuminate the significant role influencers play in shaping public discourse, offering insights into the mechanisms of online polarization and suggesting pathways for future research to mitigate divisiveness and explore new methods for quantifying affective polarization. This research contributes to the broader understanding of digital communication's impact on societal polarization, underscoring the importance of detailed analysis in developing strategies to foster a more cohesive digital public sphere."}, "https://arxiv.org/abs/2405.15930": {"title": "ArguSense: Argument-Centric Analysis of Online Discourse", "link": "https://arxiv.org/abs/2405.15930", "description": "arXiv:2405.15930v1 Announce Type: new \nAbstract: How can we model arguments and their dynamics in online forum discussions? The meteoric rise of online forums presents researchers across different disciplines with an unprecedented opportunity: we have access to texts containing discourse between groups of users generated in a voluntary and organic fashion. Most prior work so far has focused on classifying individual monological comments as either argumentative or not argumentative. However, few efforts quantify and describe the dialogical processes between users found in online forum discourse: the structure and content of interpersonal argumentation. Modeling dialogical discourse requires the ability to identify the presence of arguments, group them into clusters, and summarize the content and nature of clusters of arguments within a discussion thread in the forum. In this work, we develop ArguSense, a comprehensive and systematic framework for understanding arguments and debate in online forums. Our framework consists of methods for, among other things: (a) detecting argument topics in an unsupervised manner; (b) describing the structure of arguments within threads with powerful visualizations; and (c) quantifying the content and diversity of threads using argument similarity and clustering algorithms. We showcase our approach by analyzing the discussions of four communities on the Reddit platform over a span of 21 months. Specifically, we analyze the structure and content of threads related to GMOs in forums related to agriculture or farming to demonstrate the value of our framework."}, "https://arxiv.org/abs/2405.16059": {"title": "Interpretable Transformer Hawkes Processes: Unveiling Complex Interactions in Social Networks", "link": "https://arxiv.org/abs/2405.16059", "description": "arXiv:2405.16059v1 Announce Type: new \nAbstract: Social networks represent complex ecosystems where the interactions between users or groups play a pivotal role in information dissemination, opinion formation, and social interactions. Effectively harnessing event sequence data within social networks to unearth interactions among users or groups has persistently posed a challenging frontier within the realm of point processes. Current deep point process models face inherent limitations within the context of social networks, constraining both their interpretability and expressive power. These models encounter challenges in capturing interactions among users or groups and often rely on parameterized extrapolation methods when modelling intensity over non-event intervals, limiting their capacity to capture intricate intensity patterns, particularly beyond observed events. To address these challenges, this study proposes modifications to Transformer Hawkes processes (THP), leading to the development of interpretable Transformer Hawkes processes (ITHP). ITHP inherits the strengths of THP while aligning with statistical nonlinear Hawkes processes, thereby enhancing its interpretability and providing valuable insights into interactions between users or groups. Additionally, ITHP enhances the flexibility of the intensity function over non-event intervals, making it better suited to capture complex event propagation patterns in social networks. Experimental results, both on synthetic and real data, demonstrate the effectiveness of ITHP in overcoming the identified limitations. Moreover, they highlight ITHP's applicability in the context of exploring the complex impact of users or groups within social networks."}, "https://arxiv.org/abs/2405.16100": {"title": "Congestion transition on random walks on graphs", "link": "https://arxiv.org/abs/2405.16100", "description": "arXiv:2405.16100v1 Announce Type: new \nAbstract: The congestion formation on a urban road network is one of the key issue for the development of a sustainable mobility in the future smart cities. In this work we propose a reductionist approach studying the stationary states of a simple transport model using of a random process on a graph, where each node represents a location and the weight links give the transition rates to move from one node to another that represent the mobility demand. Each node has a finite transport capacity and a maximum load capacity and we assume that the average. In the approximation of the single step process we are able to analytically characterize the traffic load distribution on the single nodes, using a local Maximum Entropy Principle. Our results explain how the congested nodes emerge when the total traffic load increases in analogous way to a percolation transition where the appearance of a congested node is a independent random event, However, using numerical simulations, we show that in the more realistic case of the synchronous dynamics for the nodes, there are entropic forces that introduce correlation among the node state and favor the clustering of the empty and congested nodes. Our aim is to highlight universal properties of the congestion formation and, in particular, to understand the role traffic load fluctuations as a possible precursor of congestion in a transport network."}, "https://arxiv.org/abs/2405.16352": {"title": "Quantifying Multipolar Polarization", "link": "https://arxiv.org/abs/2405.16352", "description": "arXiv:2405.16352v1 Announce Type: new \nAbstract: Studying and understanding social networks is crucial for accurately defining ideological polarization, since they enable precise modeling of social structures. One of the limitations of many methods for quantifying polarization on networks is the assumption of a two-dimensional opinion space. This prevents accurate study of multipolar systems like multi-party political systems, where modeling more than two opinion poles is beneficial. Here, I experimentally compare methods for quantifying multipolar polarization on a network, and find that the average pairwise distance extension of generalized Euclidean distance conforms to several desired properties, showing its advantages over other methods. This allows study of multipolar polarized systems based on an empirically and intuitively good metric."}, "https://arxiv.org/abs/2405.16606": {"title": "Link Prediction on Textual Edge Graphs", "link": "https://arxiv.org/abs/2405.16606", "description": "arXiv:2405.16606v1 Announce Type: new \nAbstract: Textual-edge Graphs (TEGs), characterized by rich text annotations on edges, are increasingly significant in network science due to their ability to capture rich contextual information among entities. Existing works have proposed various edge-aware graph neural networks (GNNs) or let language models directly make predictions. However, they often fall short of fully capturing the contextualized semantics on edges and graph topology, respectively. This inadequacy is particularly evident in link prediction tasks that require a comprehensive understanding of graph topology and semantics between nodes. In this paper, we present a novel framework - Link2Doc, designed especially for link prediction on textual-edge graphs. Specifically, we propose to summarize neighborhood information between node pairs as a human-written document to preserve both semantic and topology information. A self-supervised learning model is then utilized to enhance GNN's text-understanding ability from language models. Empirical evaluations, including link prediction, edge classification, parameter analysis, runtime comparison, and ablation studies, on four real-world datasets demonstrate that Link2Doc achieves generally better performance against existing edge-aware GNNs and pre-trained language models in predicting links on TEGs."}, "https://arxiv.org/abs/2405.16772": {"title": "Balancing User Preferences by Social Networks: A Condition-Guided Social Recommendation Model for Mitigating Popularity Bias", "link": "https://arxiv.org/abs/2405.16772", "description": "arXiv:2405.16772v1 Announce Type: new \nAbstract: Social recommendation models weave social interactions into their design to provide uniquely personalized recommendation results for users. However, social networks not only amplify the popularity bias in recommendation models, resulting in more frequent recommendation of hot items and fewer long-tail items, but also include a substantial amount of redundant information that is essentially meaningless for the model's performance. Existing social recommendation models fail to address the issues of popularity bias and the redundancy of social information, as they directly characterize social influence across the entire social network without making targeted adjustments. In this paper, we propose a Condition-Guided Social Recommendation Model (named CGSoRec) to mitigate the model's popularity bias by denoising the social network and adjusting the weights of user's social preferences. More specifically, CGSoRec first includes a Condition-Guided Social Denoising Model (CSD) to remove redundant social relations in the social network for capturing users' social preferences with items more precisely. Then, CGSoRec calculates users' social preferences based on denoised social network and adjusts the weights in users' social preferences to make them can counteract the popularity bias present in the recommendation model. At last, CGSoRec includes a Condition-Guided Diffusion Recommendation Model (CGD) to introduce the adjusted social preferences as conditions to control the recommendation results for a debiased direction. Comprehensive experiments on three real-world datasets demonstrate the effectiveness of our proposed method. The code is in: https://github.com/hexin5515/CGSoRec."}, "https://arxiv.org/abs/2405.16913": {"title": "Chasing the eternal sun: Does a global super grid favor the deployment of solar power?", "link": "https://arxiv.org/abs/2405.16913", "description": "arXiv:2405.16913v1 Announce Type: new \nAbstract: The One Sun One World One Grid (OSOWOG) initiative advocates the development of a global Super grid for sharing renewable energy, especially solar energy. This study evaluates the economic benefits of such a Super grid, which connects six large regions spanning from Australia to the US, utilizing a detailed energy system optimization model and considering heterogeneous discount rates among countries. Integrating the six regions into a Super grid reduces the electricity system cost by 3.8% compared to isolating them. In contrast, grid expansion within each region reduces the electricity system cost by 12% on average. The economic benefits of the OSOWOG initiative's global Super grid expansion seem to be rather limited. Moreover, the allowance for a Super grid consistently results in decreased investments in solar power, indicating that it is not an effective strategy for enhancing the deployment of solar power, even when transmission grids covering 18 time zones are available."}, "https://arxiv.org/abs/2405.16928": {"title": "TopoLa: a novel embedding framework for understanding complex networks", "link": "https://arxiv.org/abs/2405.16928", "description": "arXiv:2405.16928v1 Announce Type: new \nAbstract: Complex networks, which are the abstractions of many real-world systems, present a persistent challenge across disciplines for people to decipher their underlying information. Recently, hyperbolic geometry of latent spaces has gained traction in network analysis, due to its ability to preserve certain local intrinsic properties of the nodes. In this study, we explore the problem from a much broader perspective: understanding the impact of nodes' global topological structures on latent space placements. Our investigations reveal a direct correlation between the topological structure of nodes and their positioning within the latent space. Building on this deep and strong connection between node distance and network topology, we propose a novel embedding framework called Topology-encoded Latent Hyperbolic Geometry (TopoLa) for analyzing complex networks. With the encoded topological information in the latent space, TopoLa is capable of enhancing both conventional and low-rank networks, using the singular value gap to clarify the mathematical principles behind this enhancement. Meanwhile, we show that the equipped TopoLa distance can also help augment pivotal deep learning models encompassing knowledge distillation and contrastive learning."}, "https://arxiv.org/abs/2405.17182": {"title": "Exploring the Performance of Continuous-Time Dynamic Link Prediction Algorithms", "link": "https://arxiv.org/abs/2405.17182", "description": "arXiv:2405.17182v1 Announce Type: new \nAbstract: Dynamic Link Prediction (DLP) addresses the prediction of future links in evolving networks. However, accurately portraying the performance of DLP algorithms poses challenges that might impede progress in the field. Importantly, common evaluation pipelines usually calculate ranking or binary classification metrics, where the scores of observed interactions (positives) are compared with those of randomly generated ones (negatives). However, a single metric is not sufficient to fully capture the differences between DLP algorithms, and is prone to overly optimistic performance evaluation. Instead, an in-depth evaluation should reflect performance variations across different nodes, edges, and time segments. In this work, we contribute tools to perform such a comprehensive evaluation. (1) We propose Birth-Death diagrams, a simple but powerful visualization technique that illustrates the effect of time-based train-test splitting on the difficulty of DLP on a given dataset. (2) We describe an exhaustive taxonomy of negative sampling methods that can be used at evaluation time. (3) We carry out an empirical study of the effect of the different negative sampling strategies. Our comparison between heuristics and state-of-the-art memory-based methods on various real-world datasets confirms a strong effect of using different negative sampling strategies on the test Area Under the Curve (AUC). Moreover, we conduct a visual exploration of the prediction, with additional insights on which different types of errors are prominent over time."}, "https://arxiv.org/abs/2405.17189": {"title": "Rebound in epidemic control: How misaligned vaccination timing amplifies infection peaks", "link": "https://arxiv.org/abs/2405.17189", "description": "arXiv:2405.17189v1 Announce Type: new \nAbstract: In this study, we explore the dynamic interplay between the timing of vaccination campaigns and the trajectory of disease spread in a population. Through comprehensive data analysis and modeling, we have uncovered a counter-intuitive phenomenon: initiating a vaccination process at an inopportune moment can paradoxically result in a more pronounced second peak of infections. This \"rebound\" phenomenon challenges the conventional understanding of vaccination impacts on epidemic dynamics. We provide a detailed examination of how improperly timed vaccination efforts can inadvertently reduce the overall immunity level in a population, considering both natural and vaccine-induced immunity. Our findings reveal that such a decrease in population-wide immunity can lead to a delayed, yet more severe, resurgence of cases. This study not only adds a critical dimension to our understanding of vaccination strategies in controlling pandemics but also underscores the necessity for strategically timed interventions to optimize public health outcomes. Furthermore, we compute which vaccination strategies are optimal for a COVID-19 tailored mathematical model, and find that there are two types of optimal strategies. The first type prioritizes vaccinating early and rapidly to reduce the number of deaths, while the second type acts later and more slowly to reduce the number of cases; both of them target primarily the elderly population. Our results hold significant implications for the formulation of vaccination policies, particularly in the context of rapidly evolving infectious diseases."}, "https://arxiv.org/abs/2405.17268": {"title": "Suppressing defection by increasing temptation: the impact of smart cooperators on a social dilemma situation", "link": "https://arxiv.org/abs/2405.17268", "description": "arXiv:2405.17268v1 Announce Type: new \nAbstract: In a social dilemma situation, where individual and collective interests are in conflict, it sounds a reasonable assumption that the presence of super or smart players, who simultaneously punish defection and reward cooperation without allowing exploitation, could solve the basic problem. The behavior of such a multi-strategy system, however, is more subtle than it is firstly anticipated. When exploring the complete parameter space, we find that the emergence of cyclic dominance among strategies is rather common, which results in several counter-intuitive phenomena. For example, the defection level can be lowered at higher temptation, or weaker punishment provides better conditions for smart players. Our study indicates that smart cooperators can unexpectedly thrive under high temptation, emphasizing the complexity of strategic interactions. This study suggests that the principles governing these interactions can be applied to other moral behaviors, such as truth-telling and honesty, providing valuable insights for future research in multi-agent systems."}, "https://arxiv.org/abs/2405.17282": {"title": "R-ODE: Ricci Curvature Tells When You Will be Informed", "link": "https://arxiv.org/abs/2405.17282", "description": "arXiv:2405.17282v1 Announce Type: new \nAbstract: Information diffusion prediction is fundamental to understand the structure and organization of the online social networks, and plays a crucial role to blocking rumor spread, influence maximization, political propaganda, etc. So far, most existing solutions primarily predict the next user who will be informed with historical cascades, but ignore an important factor in the diffusion process - the time. Such limitation motivates us to pose the problem of the time-aware personalized information diffusion prediction for the first time, telling the time when the target user will be informed. In this paper, we address this problem from a fresh geometric perspective of Ricci curvature, and propose a novel Ricci-curvature regulated Ordinary Differential Equation (R-ODE). In the diffusion process, R-ODE considers that the inter-correlated users are organized in a dynamic system in the representation space, and the cascades give the observations sampled from the continuous realm. At each infection time, the message diffuses along the largest Ricci curvature, signifying less transportation effort. In the continuous realm, the message triggers users' movement, whose trajectory in the space is parameterized by an ODE with graph neural network. Consequently, R-ODE predicts the infection time of a target user by the movement trajectory learnt from the observations. Extensive experiments evaluate the personalized time prediction ability of R-ODE, and show R-ODE outperforms the state-of-the-art baselines."}, "https://arxiv.org/abs/2405.17410": {"title": "The Peripatetic Hater: Predicting Movement Among Hate Subreddits", "link": "https://arxiv.org/abs/2405.17410", "description": "arXiv:2405.17410v1 Announce Type: new \nAbstract: Many online hate groups exist to disparage others based on race, gender identity, sex, or other characteristics. The accessibility of these communities allows users to join multiple types of hate groups (e.g., a racist community and misogynistic community), which calls into question whether these peripatetic users could be further radicalized compared to users that stay in one type of hate group. However, little is known about the dynamics of joining multiple types of hate groups, nor the effect of these groups on peripatetic users. In this paper, we develop a new method to classify hate subreddits, and the identities they disparage, which we use to better understand how users become peripatetic (join different types of hate subreddits). The hate classification technique utilizes human-validated LLMs to extract the protected identities attacked, if any, across 168 subreddits. We then cluster identity-attacking subreddits to discover three broad categories of hate: racist, anti-LGBTQ, and misogynistic. We show that becoming active in a user's first hate subreddit can cause them to become active in additional hate subreddits of a different category. We also find that users who join additional hate subreddits, especially of a different category, become more active in hate subreddits as a whole and develop a wider hate group lexicon. We are therefore motivated to train an AI model that we find usefully predicts the hate categories users will become active in based on post text read and written. The accuracy of this model may be partly driven by peripatetic users often using the language of hate subreddits they eventually join. Overall, these results highlight the unique risks associated with hate communities on a social media platform, as discussion of alternative targets of hate may lead users to target more protected identities."}, "https://arxiv.org/abs/2405.16346": {"title": "A modular and scalable web platform for computational phylogenetics", "link": "https://arxiv.org/abs/2405.16346", "description": "arXiv:2405.16346v1 Announce Type: cross \nAbstract: Phylogenetic analysis, which allow to understand the evolution of bacterial and viral epidemics, requires large quantities of data to be analysed and processed for knowledge extraction. One of the major challenges consists on the integration of the results from typing and phylogenetic inference methods with epidemiological data, namely in what concerns their integrated and simultaneous analysis and visualization. Numerous approaches to support phylogenetic analysis have been proposed, varying from standalone tools to integrative web applications that include tools and/or algorithms for executing the common analysis tasks for this kind of data. However, most of them lack the capacity to integrate epidemiological data. Others provide the ability for visualizing and analyzing such data, allowing the integration of epidemiological data but they do not scale for large data analysis and visualization. Namely, most of them run inference and/or visualization optimization tasks on the client side, which becomes often unfeasible for large amounts of data, usually implying transferring data from existing databases in order to be analysed. Moreover, the results and optimizations are not stored for reuse. We propose the PHYLOViZ Web Platform, a cloud based tool for phylogenetic analysis, that not only unifies the features of both existing versions of PHYLOViZ, but also supports structured and customized workflows for executing data processing and analyses tasks, and promotes the reproducibility of previous phylogenetic analyses. This platform supports large scale analyses by relying on a workflow system that enables the distribution of parallel computations on cloud and HPC environments. Moreover, it has a modular architecture, allowing easy integration of new methods and tools, as well as customized workflows, making it flexible and extensible."}, "https://arxiv.org/abs/2405.16616": {"title": "DPHGNN: A Dual Perspective Hypergraph Neural Networks", "link": "https://arxiv.org/abs/2405.16616", "description": "arXiv:2405.16616v1 Announce Type: cross \nAbstract: Message passing on hypergraphs has been a standard framework for learning higher-order correlations between hypernodes. Recently-proposed hypergraph neural networks (HGNNs) can be categorized into spatial and spectral methods based on their design choices. In this work, we analyze the impact of change in hypergraph topology on the suboptimal performance of HGNNs and propose DPHGNN, a novel dual-perspective HGNN that introduces equivariant operator learning to capture lower-order semantics by inducing topology-aware spatial and spectral inductive biases. DPHGNN employs a unified framework to dynamically fuse lower-order explicit feature representations from the underlying graph into the super-imposed hypergraph structure. We benchmark DPHGNN over eight benchmark hypergraph datasets for the semi-supervised hypernode classification task and obtain superior performance compared to seven state-of-the-art baselines. We also provide a theoretical framework and a synthetic hypergraph isomorphism test to express the power of spatial HGNNs and quantify the expressivity of DPHGNN beyond the Generalized Weisfeiler Leman (1-GWL) test. Finally, DPHGNN was deployed by our partner e-commerce company for the Return-to-Origin (RTO) prediction task, which shows ~7% higher macro F1-Score than the best baseline."}, "https://arxiv.org/abs/2405.16631": {"title": "Let Silence Speak: Enhancing Fake News Detection with Generated Comments from Large Language Models", "link": "https://arxiv.org/abs/2405.16631", "description": "arXiv:2405.16631v1 Announce Type: cross \nAbstract: Fake news detection plays a crucial role in protecting social media users and maintaining a healthy news ecosystem. Among existing works, comment-based fake news detection methods are empirically shown as promising because comments could reflect users' opinions, stances, and emotions and deepen models' understanding of fake news. Unfortunately, due to exposure bias and users' different willingness to comment, it is not easy to obtain diverse comments in reality, especially for early detection scenarios. Without obtaining the comments from the ``silent'' users, the perceived opinions may be incomplete, subsequently affecting news veracity judgment. In this paper, we explore the possibility of finding an alternative source of comments to guarantee the availability of diverse comments, especially those from silent users. Specifically, we propose to adopt large language models (LLMs) as a user simulator and comment generator, and design GenFEND, a generated feedback-enhanced detection framework, which generates comments by prompting LLMs with diverse user profiles and aggregating generated comments from multiple subpopulation groups. Experiments demonstrate the effectiveness of GenFEND and further analysis shows that the generated comments cover more diverse users and could even be more effective than actual comments."}, "https://arxiv.org/abs/2302.01397": {"title": "Waiting for Q: An Exploration of QAnon Users' Online Migration to Poal in the Wake of Voat's Demise", "link": "https://arxiv.org/abs/2302.01397", "description": "arXiv:2302.01397v4 Announce Type: replace \nAbstract: Online communities are groups of people who interact primarily via the Internet, often sharing common interests. Some of these groups, particularly supporters of Q who created the far-right conspiracy theory known as QAnon, are highly toxic and controversial. These communities are often banned from various mainstream online social networks due to their controversy. This study examines the deplatforming and subsequent migrations of QAnon adherents, following a two-step process. We analyze Reddit data, finding that users opt for Voat as an alternative following the Reddit bans, particularly influenced by Q's postings on 4chan. Subsequently, upon Voat's shutdown announcement, we observe users recommending Poal. Among several insights, we compare the effects of abrupt permanent bans and announced shutdowns on the migration patterns of these conspiracists. Specifically, we find that almost half of Poal's active users are Voat migrants who registered after the shutdown was announced. This contradicts the patterns observed after the Reddit bans, suggesting that advance warning can facilitate more coordinated migrations. Lastly, our research uncovers evidence of discussions and planning related to the January 6th, 2021, attack on the US Capitol, which emerged shortly after Voat's shutdown, predominantly on Poal. This underscores the continued activity of the conspiracy, albeit at a diminished scale due to various bans and a shutdown, while also exposing Poal as a platform that hosts dangerous individuals."}, "https://arxiv.org/abs/2305.15413": {"title": "Proper Interpretation of Heaps' and Zipf's Laws", "link": "https://arxiv.org/abs/2305.15413", "description": "arXiv:2305.15413v2 Announce Type: replace \nAbstract: We checked that the distribution of words in text should uniform, which gives Heaps' law as natural result, that is, the number of types of words can be expressed as a power law of the number of tokens within text. We developed a ``superposition'' model, which leads to an asymptotic power-law distribution of the number of occurrences (or frequency) of words, that is, Zipf's law. The model is well consistent with observations."}, "https://arxiv.org/abs/2401.09425": {"title": "Quantifying Attrition in Science: A Cohort-Based, Longitudinal Study of Scientists in 38 OECD Countries", "link": "https://arxiv.org/abs/2401.09425", "description": "arXiv:2401.09425v3 Announce Type: replace \nAbstract: In this paper, we explore how members of the scientific community leave academic science and how attrition (defined as ceasing to publish) differs across genders, academic disciplines, and over time. Our approach is cohort based and longitudinal: We track individual male and female scientists over time and quantify the phenomenon traditionally referred to as 'leaving science.' Using publication metadata from Scopus - a global bibliometric database of publications and citations - we follow the details of the publishing careers of scientists from 38 OECD countries who started publishing in 2000 (N = 142,776) and 2010 (N = 232,843). Our study is restricted to 16 STEMM disciplines (science, technology, engineering, mathematics, and medicine), and we track the individual scholarly output of the two cohorts until 2022. Survival analyses show that attrition becomes less gendered. In addition to the combined aggregated changes at the level of all STEMM disciplines, widely nuanced changes were found to occur at the discipline level and over time. Attrition in science means different things for men versus women depending on the discipline; moreover, it means different things for scientists from different cohorts entering the scientific workforce. Finally, global bibliometric datasets were tested in the current study, opening new opportunities to explore gender and disciplinary differences in attrition."}, "https://arxiv.org/abs/2301.01926": {"title": "Auditing citation polarization during the early COVID-19 pandemic", "link": "https://arxiv.org/abs/2301.01926", "description": "arXiv:2301.01926v2 Announce Type: replace-cross \nAbstract: The recent pandemic stimulated scientists to publish a significant amount of research that created a surge of citations of COVID-19-related publications in a short time, leading to an abrupt inflation of the journal impact factor (IF). By auditing the complete set of COVID-19-related publications in the Web of Science, we reveal here that COVID-19-related research worsened the polarization of academic journals: the IF before the pandemic was proportional to the increment of IF, which had the effect of increasing inequality while retaining the journal rankings. We also found that the most highly cited studies related to COVID-19 were published in prestigious journals at the onset of the epidemic. Through the present quantitative investigation, our findings caution against the belief that quantitative metrics, particularly IF, can indicate the significance of individual papers. Rather, such metrics reflect the social attention given to a particular study."}, "https://arxiv.org/abs/2308.11129": {"title": "Enhancing Graph Transformers with Hierarchical Distance Structural Encoding", "link": "https://arxiv.org/abs/2308.11129", "description": "arXiv:2308.11129v4 Announce Type: replace-cross \nAbstract: Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current methods often fall short in capturing longer ranges, hierarchical structures, or community structures, which are common in various graphs such as molecules, social networks, and citation networks. This paper presents a Hierarchical Distance Structural Encoding (HDSE) method to model node distances in a graph, focusing on its multi-level, hierarchical nature. We introduce a novel framework to seamlessly integrate HDSE into the attention mechanism of existing graph transformers, allowing for simultaneous application with other positional encodings. To apply graph transformers with HDSE to large-scale graphs, we further propose a high-level HDSE that effectively biases the linear transformers towards graph hierarchies. We theoretically prove the superiority of HDSE over shortest path distances in terms of expressivity and generalization. Empirically, we demonstrate that graph transformers with HDSE excel in graph classification, regression on 7 graph-level datasets, and node classification on 11 large-scale graphs, including those with up to a billion nodes."}, "https://arxiv.org/abs/2403.16049": {"title": "Improving Demand Forecasting in Open Systems with Cartogram-Enhanced Deep Learning", "link": "https://arxiv.org/abs/2403.16049", "description": "arXiv:2403.16049v2 Announce Type: replace-cross \nAbstract: Predicting temporal patterns across various domains poses significant challenges due to their nuanced and often nonlinear trajectories. To address this challenge, prediction frameworks have been continuously refined, employing data-driven statistical methods, mathematical models, and machine learning. Recently, as one of the challenging systems, shared transport systems such as public bicycles have gained prominence due to urban constraints and environmental concerns. Predicting rental and return patterns at bicycle stations remains a formidable task due to the system's openness and imbalanced usage patterns across stations. In this study, we propose a deep learning framework to predict rental and return patterns by leveraging cartogram approaches. The cartogram approach facilitates the prediction of demand for newly installed stations with no training data as well as long-period prediction, which has not been achieved before. We apply this method to public bicycle rental-and-return data in Seoul, South Korea, employing a spatial-temporal convolutional graph attention network. Our improved architecture incorporates batch attention and modified node feature updates for better prediction accuracy across different time scales. We demonstrate the effectiveness of our framework in predicting temporal patterns and its potential applications."}, "https://arxiv.org/abs/2404.14192": {"title": "Swap distance minimization beyond entropy minimization in word order variation", "link": "https://arxiv.org/abs/2404.14192", "description": "arXiv:2404.14192v3 Announce Type: replace-cross \nAbstract: Here we consider the problem of all the possible orders of a linguistic structure formed by $n$ elements, for instance, subject, direct object and verb ($n=3$) or subject, direct object, indirect object and verb ($n=4$). We investigate if the frequency of the $n!$ possible orders is constrained by two principles. First, entropy minimization, a principle that has been suggested to shape natural communication systems at distinct levels of organization. Second, swap distance minimization, namely a preference for word orders that require fewer swaps of adjacent elements to be produced from a source order. Here we present average swap distance, a novel score for research on swap distance minimization, and investigate the theoretical distribution of that score for any $n$: its minimum and maximum values and its expected value in die rolling experiments or when the word order frequencies are shuffled. We investigate whether entropy and average swap distance are significantly small in distinct linguistic structures with $n=3$ or $n=4$ in agreement with the corresponding minimization principles. We find strong evidence of entropy minimization and swap distance minimization with respect to a die rolling experiment. The evidence of these two forces with respect to a Polya urn process is strong for $n=4$ but weaker for $n=3$. We still find evidence of swap distance minimization when word order frequencies are shuffled, indicating that swap distance minimization effects are beyond pressure to minimize word order entropy."}, "https://arxiv.org/abs/2405.17511": {"title": "On the Analogy of Gauge Theory of Plasticity and Economics", "link": "https://arxiv.org/abs/2405.17511", "description": "arXiv:2405.17511v1 Announce Type: new \nAbstract: We demonstrated the analogy between Economics and Gauge Theory of Plasticity and used it to describe the relationship between money supply and inflation at the economic market. The received equations of economical dynamics in phase space are similar to the plasticity equations and economic variables - choice, competition and profit correspond to the state of the market with inflation. We described the meaning of equations and the role of its variables in the stabilization mechanism of the market with inflation. The equation of market equilibrium including the Profit turnover, time changes of competition, capital and choice was discussed in detail."}, "https://arxiv.org/abs/2405.17571": {"title": "Bluesky: Network Topology, Polarisation, and Algorithmic Curation", "link": "https://arxiv.org/abs/2405.17571", "description": "arXiv:2405.17571v1 Announce Type: new \nAbstract: Bluesky is a nascent ``Twitter-like'' and decentralized social media network with novel features and unprecedented data access. This paper provides a characterization of the network, studying the political leaning, polarization, network structure, and algorithmic curation mechanisms of five million users. The dataset spans from the website's first release in February of 2023. Users of the new social media site are predominantly left-center leaning and share little to no links associated with questionable sources. In contrast to the homogeneous political stance, we find significant issues-based divergence by studying opinions related to the Israel-Palestine conflict. Two clear homophilic clusters emerge: Pro-Palestinian voices make up the plurality of messages related to the conflict and the proportion has increased with a lessening of interest. We investigate multiple layers of the multi-scale Bluesky network based on replies, likes, reposts, and follows, highlighting differences and similarities between the layers. We differentiate between persistent and non-persistent interactions and measure metrics of network topology over time. All networks are heavy-tailed, clustered, and connected by short paths. We showcase all feeds - algorithmic content recommenders - created for and by users. A large number of custom feeds have been created but their uptake by users is limited. Multiple popular feeds aim to provide similar feeds that are neither topical nor chronological. We conclude by claiming that Bluesky - for all its novel features - is very similar in terms of its network structure to existing and larger social media sites and provides unprecedented research opportunities for social scientists, network scientists, and political scientists alike."}, "https://arxiv.org/abs/2405.17710": {"title": "Does Geo-co-location Matter? A Case Study of Public Health Conversations during COVID-19", "link": "https://arxiv.org/abs/2405.17710", "description": "arXiv:2405.17710v1 Announce Type: new \nAbstract: Social media platforms like Twitter (now X) have been pivotal in information dissemination and public engagement, especially during COVID-19. A key goal for public health experts was to encourage prosocial behavior that could impact local outcomes such as masking and social distancing. Given the importance of local news and guidance during COVID-19, the objective of our research is to analyze the effect of localized engagement, on social media conversations. This study examines the impact of geographic co-location, as a proxy for localized engagement between public health experts (PHEs) and the public, on social media. We analyze a Twitter conversation dataset from January 2020 to November 2021, comprising over 19 K tweets from nearly five hundred PHEs, along with approximately 800 K replies from 350 K participants. Our findings reveal that geo-co-location is associated with higher engagement rates, especially in conversations on topics including masking, lockdowns, and education, and in conversations with academic and medical professionals. Lexical features associated with emotion and personal experiences were more common in geo-co-located contexts. This research provides insights into how geographic co-location influences social media engagement and can inform strategies to improve public health messaging."}, "https://arxiv.org/abs/2405.18059": {"title": "Rank-Refining Seed Selection Methods for Budget Constrained Influence Maximisation in Multilayer Networks under Linear Threshold Model", "link": "https://arxiv.org/abs/2405.18059", "description": "arXiv:2405.18059v1 Announce Type: new \nAbstract: The problem of selecting an optimal seed set to maximise influence in networks has been a subject of intense research in recent years. However, despite numerous works addressing this area, it remains a topic that requires further elaboration. Most often, it is considered within the scope of classically defined graphs with a spreading model in the form of Independent Cascades. In this work, we focus on the problem of budget-constrained influence maximisation in multilayer networks using a Linear Threshold Model. Both the graph model and the spreading process we employ are less prevalent in the literature, even though their application allows for a more precise representation of the opinion dynamics in social networks. This paper aims to answer which of the sixteen evaluated seed selection methods is the most effective and how similar they are. Additionally, we focus our analysis on the impact of spreading model parameters, network characteristics, a budget, and the seed selection methods on the diffusion effectiveness in multilayer networks. Our contribution also includes extending several centrality measures and heuristics to the case of such graphs. The results indicate that all the factors mentioned above collectively contribute to the effectiveness of influence maximisation. Moreover, there is no seed selection method which always provides the best results. However, the seeds chosen with VoteRank-based methods (especially with the $v-rnk-m$ variant we propose) usually provide the most extensive diffusion."}, "https://arxiv.org/abs/2405.18085": {"title": "Network Diffusion -- Framework to Simulate Spreading Processes in Complex Networks", "link": "https://arxiv.org/abs/2405.18085", "description": "arXiv:2405.18085v1 Announce Type: new \nAbstract: With the advancement of computational network science, its research scope has significantly expanded beyond static graphs to encompass more complex structures. The introduction of streaming, temporal, multilayer, and hypernetwork approaches has brought new possibilities and imposed additional requirements. For instance, by utilising these advancements, one can model structures such as social networks in a much more refined manner, which is particularly relevant in simulations of the spreading processes. Unfortunately, the pace of advancement is often too rapid for existing computational packages to keep up with the functionality updates. This results in a significant proliferation of tools used by researchers and, consequently, a lack of a universally accepted technological stack that would standardise experimental methods (as seen, e.g. in machine learning). This article addresses that issue by presenting an extended version of the Network Diffusion library. First, a survey of the existing approaches and toolkits for simulating spreading phenomena is shown and then, an overview of the framework functionalities. Finally, we report four case studies conducted with the package to demonstrate its usefulness: the impact of sanitary measures on the spread of COVID-19, the comparison of information diffusion on two temporal network models, and the effectiveness of seed selection methods in the task of influence maximisation in multilayer networks. We conclude the paper with a critical assessment of the library and the outline of still awaiting challenges to standardise research environments in computational network science."}, "https://arxiv.org/abs/2405.17473": {"title": "Repeat-Aware Neighbor Sampling for Dynamic Graph Learning", "link": "https://arxiv.org/abs/2405.17473", "description": "arXiv:2405.17473v1 Announce Type: cross \nAbstract: Dynamic graph learning equips the edges with time attributes and allows multiple links between two nodes, which is a crucial technology for understanding evolving data scenarios like traffic prediction and recommendation systems. Existing works obtain the evolving patterns mainly depending on the most recent neighbor sequences. However, we argue that whether two nodes will have interaction with each other in the future is highly correlated with the same interaction that happened in the past. Only considering the recent neighbors overlooks the phenomenon of repeat behavior and fails to accurately capture the temporal evolution of interactions. To fill this gap, this paper presents RepeatMixer, which considers evolving patterns of first and high-order repeat behavior in the neighbor sampling strategy and temporal information learning. Firstly, we define the first-order repeat-aware nodes of the source node as the destination nodes that have interacted historically and extend this concept to high orders as nodes in the destination node's high-order neighbors. Then, we extract neighbors of the source node that interacted before the appearance of repeat-aware nodes with a slide window strategy as its neighbor sequence. Next, we leverage both the first and high-order neighbor sequences of source and destination nodes to learn temporal patterns of interactions via an MLP-based encoder. Furthermore, considering the varying temporal patterns on different orders, we introduce a time-aware aggregation mechanism that adaptively aggregates the temporal representations from different orders based on the significance of their interaction time sequences. Experimental results demonstrate the superiority of RepeatMixer over state-of-the-art models in link prediction tasks, underscoring the effectiveness of the proposed repeat-aware neighbor sampling strategy."}, "https://arxiv.org/abs/2405.17530": {"title": "Universal deterministic patterns in stochastic count data", "link": "https://arxiv.org/abs/2405.17530", "description": "arXiv:2405.17530v1 Announce Type: cross \nAbstract: We report the existence of deterministic patterns in plots showing the relationship between the mean and the Fano factor (ratio of variance and mean) of stochastic count data. These patterns are found in a wide variety of datasets, including those from genomics, paper citations, commerce, ecology, disease outbreaks, and employment statistics. We develop a theory showing that the patterns naturally emerge when data sampled from discrete probability distributions is organised in matrix form. The theory precisely predicts the patterns and shows that they are a function of only one variable - the sample size."}, "https://arxiv.org/abs/2405.17735": {"title": "State Feedback as a Strategy for Control and Analysis of COVID-19", "link": "https://arxiv.org/abs/2405.17735", "description": "arXiv:2405.17735v1 Announce Type: cross \nAbstract: This paper presents a study on a compartmental epidemic model for COVID-19, examining the stability of its equilibrium points upon the introduction of vaccination as a strategy to mitigate the spread of the disease. Initially, the SIQR (Susceptible-Infectious-Quarantine-Recovered) mathematical model and its technical aspects are introduced. Subsequently, vaccination is incorporated as a control measure within the model scope. Equilibrium points and the basic reproductive number are determined, followed by an analysis of their stability. Furthermore, controllability characteristics and Optimal Control strategies for the system are investigated, supplemented by numerical simulations."}, "https://arxiv.org/abs/2405.17768": {"title": "Revisiting the Message Passing in Heterophilous Graph Neural Networks", "link": "https://arxiv.org/abs/2405.17768", "description": "arXiv:2405.17768v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs) have demonstrated strong performance in graph mining tasks due to their message-passing mechanism, which is aligned with the homophily assumption that adjacent nodes exhibit similar behaviors. However, in many real-world graphs, connected nodes may display contrasting behaviors, termed as heterophilous patterns, which has attracted increased interest in heterophilous GNNs (HTGNNs). Although the message-passing mechanism seems unsuitable for heterophilous graphs due to the propagation of class-irrelevant information, it is still widely used in many existing HTGNNs and consistently achieves notable success. This raises the question: why does message passing remain effective on heterophilous graphs? To answer this question, in this paper, we revisit the message-passing mechanisms in heterophilous graph neural networks and reformulate them into a unified heterophilious message-passing (HTMP) mechanism. Based on HTMP and empirical analysis, we reveal that the success of message passing in existing HTGNNs is attributed to implicitly enhancing the compatibility matrix among classes. Moreover, we argue that the full potential of the compatibility matrix is not completely achieved due to the existence of incomplete and noisy semantic neighborhoods in real-world heterophilous graphs. To bridge this gap, we introduce a new approach named CMGNN, which operates within the HTMP mechanism to explicitly leverage and improve the compatibility matrix. A thorough evaluation involving 10 benchmark datasets and comparative analysis against 13 well-established baselines highlights the superior performance of the HTMP mechanism and CMGNN method."}, "https://arxiv.org/abs/2405.18116": {"title": "Emergent Inequalities in a Primitive Agent-Based Good-Exchange Model", "link": "https://arxiv.org/abs/2405.18116", "description": "arXiv:2405.18116v1 Announce Type: cross \nAbstract: Rising inequalities around the globe bring into question our economic systems and the origin of such inequalities. Here we propose a toy agent-based model where each entity is simultaneously producing and consuming indivisible goods. We find that the system exhibits a non-trivial phase transition beyond which a market clearing equilibrium exists but becomes dynamically unreachable. When production capacity exceeds a threshold and adapts too slowly, some agents cannot sell all their goods. This leads to global price deflation and induces strong wealth inequalities, with the spontaneous separation of the population into a rich class and a poor class. We explore ways to alleviate poverty in this model and whether they have real life significance."}, "https://arxiv.org/abs/2405.18255": {"title": "Channel Reciprocity Based Attack Detection for Securing UWB Ranging by Autoencoder", "link": "https://arxiv.org/abs/2405.18255", "description": "arXiv:2405.18255v1 Announce Type: cross \nAbstract: A variety of ranging threats represented by Ghost Peak attack have raised concerns regarding the security performance of Ultra-Wide Band (UWB) systems with the finalization of the IEEE 802.15.4z standard. Based on channel reciprocity, this paper proposes a low complexity attack detection scheme that compares Channel Impulse Response (CIR) features of both ranging sides utilizing an autoencoder with the capability of data compression and feature extraction. Taking Ghost Peak attack as an example, this paper demonstrates the effectiveness, feasibility and generalizability of the proposed attack detection scheme through simulation and experimental validation. The proposed scheme achieves an attack detection success rate of over 99% and can be implemented in current systems at low cost."}, "https://arxiv.org/abs/2405.18414": {"title": "Don't Forget to Connect! Improving RAG with Graph-based Reranking", "link": "https://arxiv.org/abs/2405.18414", "description": "arXiv:2405.18414v1 Announce Type: cross \nAbstract: Retrieval Augmented Generation (RAG) has greatly improved the performance of Large Language Model (LLM) responses by grounding generation with context from existing documents. These systems work well when documents are clearly relevant to a question context. But what about when a document has partial information, or less obvious connections to the context? And how should we reason about connections between documents? In this work, we seek to answer these two core questions about RAG generation. We introduce G-RAG, a reranker based on graph neural networks (GNNs) between the retriever and reader in RAG. Our method combines both connections between documents and semantic information (via Abstract Meaning Representation graphs) to provide a context-informed ranker for RAG. G-RAG outperforms state-of-the-art approaches while having smaller computational footprint. Additionally, we assess the performance of PaLM 2 as a reranker and find it to significantly underperform G-RAG. This result emphasizes the importance of reranking for RAG even when using Large Language Models."}, "https://arxiv.org/abs/2405.18419": {"title": "Exploring the Evolution of Altruistic Punishment with a PDE Model of Cultural Multilevel Selection", "link": "https://arxiv.org/abs/2405.18419", "description": "arXiv:2405.18419v1 Announce Type: cross \nAbstract: Two mechanisms that have been used to study the evolution of cooperative behavior are altruistic punishment, in which cooperative individuals pay additional costs to punish defection, and multilevel selection, in which competition between groups can help to counteract individual-level incentives to cheat. Boyd, Gintis, Bowles, and Richerson have used simulation models of cultural evolution to suggest that altruistic punishment and pairwise group-level competition can work in concert to promote cooperation, even when neither mechanism can do so on its own. In this paper, we formulate a PDE model for multilevel selection motivated by the approach of Boyd and coauthors, modeling individual-level birth-death competition with a replicator equation based on individual payoffs and describing group-level competition with pairwise conflicts based on differences in the average payoffs of the competing groups. Building off of existing PDE models for multilevel selection with frequency-independent group-level competition, we use analytical and numerical techniques to understand how the forms of individual and average payoffs can impact the long-time ability to sustain altruistic punishment in group-structured populations. We find several interesting differences between the behavior of our new PDE model with pairwise group-level competition and existing multilevel PDE models, including the observation that our new model can feature a non-monotonic dependence of the long-time collective payoff on the strength of altruistic punishment. Going forward, our PDE framework can serve as a way to connect and compare disparate approaches for understanding multilevel selection across the literature in evolutionary biology and anthropology."}, "https://arxiv.org/abs/2401.06872": {"title": "Disease Transmission on Random Graphs Using Edge-Based Percolation", "link": "https://arxiv.org/abs/2401.06872", "description": "arXiv:2401.06872v2 Announce Type: replace \nAbstract: Edge-based percolation methods can be used to analyze disease transmission on complex social networks. This allows us to include complex social heterogeneity in our models while maintaining tractability. Here we review the seminal works on this field by Newman et al (2001); Newman (2002, 2003), and Miller et al (2012). We present a systematic discussion of the theoretical background behind these models, including an extensive derivation of the major results. We also connect these results relate back to the classical literature in random graph theory Molloy and Reed (1995, 1998). Finally, we also present an accompanying R package that takes epidemic and network parameters as input and generates estimates of the epidemic trajectory and final size. This manuscript and the R package was developed to help researchers easily understand and use network models to investigate the interaction between different community structures and disease transmission."}, "https://arxiv.org/abs/2401.11254": {"title": "The Great Ban: Efficacy and Unintended Consequences of a Massive Deplatforming Operation on Reddit", "link": "https://arxiv.org/abs/2401.11254", "description": "arXiv:2401.11254v5 Announce Type: replace \nAbstract: In the current landscape of online abuses and harms, effective content moderation is necessary to cultivate safe and inclusive online spaces. Yet, the effectiveness of many moderation interventions is still unclear. Here, we assess the effectiveness of The Great Ban, a massive deplatforming operation that affected nearly 2,000 communities on Reddit. By analyzing 16M comments posted by 17K users during 14 months, we provide nuanced results on the effects, both desired and otherwise, of the ban. Among our main findings is that 15.6% of the affected users left Reddit and that those who remained reduced their toxicity by 6.6% on average. The ban also caused 5% users to increase their toxicity by more than 70% of their pre-ban level. Overall, our multifaceted results provide new insights into the efficacy of deplatforming. As such, our findings can inform the development of future moderation interventions and the policing of online platforms."}, "https://arxiv.org/abs/2403.00603": {"title": "Modeling of obstacle avoidance by a dense crowd as a Mean-Field Game", "link": "https://arxiv.org/abs/2403.00603", "description": "arXiv:2403.00603v2 Announce Type: replace \nAbstract: In this paper we use a minimal model based on Mean-Field Games (a mathematical framework apt to describe situations where a large number of agents compete strategically) to simulate the scenario where a static dense human crowd is crossed by a cylindrical intruder. After a brief explanation of the mathematics behind it, we compare our model directly against the empirical data collected during a controlled experiment replicating the aforementioned situation. We then summarize the features that make the model adhere so well to the experiment and clarify the anticipation time in this framework."}, "https://arxiv.org/abs/2403.01168": {"title": "Mean-Field Games Modeling of Anticipation in Dense Crowds", "link": "https://arxiv.org/abs/2403.01168", "description": "arXiv:2403.01168v2 Announce Type: replace \nAbstract: Understanding and modeling pedestrian dynamics in dense crowds is a complex yet essential aspect of crowd management and urban planning. In this work, we investigate the dynamics of a dense crowd crossed by a cylindrical intruder using a Mean-Field Game (MFG) model. By incorporating a discount factor to account for pedestrians' limited anticipation and information processing, we examine the model's ability to simulate two distinct experimental configurations: pedestrians facing the obstacle and pedestrians giving their back to the intruder. Through a comprehensive comparison with experimental data, we demonstrate that the MFG model effectively captures essential crowd behaviors, including anticipatory motion and collision avoidance."}, "https://arxiv.org/abs/2301.10856": {"title": "Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram", "link": "https://arxiv.org/abs/2301.10856", "description": "arXiv:2301.10856v5 Announce Type: replace-cross \nAbstract: In response to disinformation and propaganda from Russian online media following the invasion of Ukraine, Russian media outlets such as Russia Today and Sputnik News were banned throughout Europe. To maintain viewership, many of these Russian outlets began to heavily promote their content on messaging services like Telegram. In this work, we study how 16 Russian media outlets interacted with and utilized 732 Telegram channels throughout 2022. Leveraging the foundational model MPNet, DP-means clustering, and Hawkes processes, we trace how narratives spread between news sites and Telegram channels. We show that news outlets not only propagate existing narratives through Telegram but that they source material from the messaging platform. For example, across the websites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of articles discussed content that originated/resulted from activity on Telegram. Finally, tracking the spread of individual topics, we measure the rate at which news outlets and Telegram channels disseminate content within the Russian media ecosystem, finding that websites like ura.news and Telegram channels such as @genshab are the most effective at disseminating their content."}, "https://arxiv.org/abs/2308.08012": {"title": "Comprehensive Analysis of Network Robustness Evaluation Based on Convolutional Neural Networks with Spatial Pyramid Pooling", "link": "https://arxiv.org/abs/2308.08012", "description": "arXiv:2308.08012v2 Announce Type: replace-cross \nAbstract: Connectivity robustness, a crucial aspect for understanding, optimizing, and repairing complex networks, has traditionally been evaluated through time-consuming and often impractical simulations. Fortunately, machine learning provides a new avenue for addressing this challenge. However, several key issues remain unresolved, including the performance in more general edge removal scenarios, capturing robustness through attack curves instead of directly training for robustness, scalability of predictive tasks, and transferability of predictive capabilities. In this paper, we address these challenges by designing a convolutional neural networks (CNN) model with spatial pyramid pooling networks (SPP-net), adapting existing evaluation metrics, redesigning the attack modes, introducing appropriate filtering rules, and incorporating the value of robustness as training data. The results demonstrate the thoroughness of the proposed CNN framework in addressing the challenges of high computational time across various network types, failure component types and failure scenarios. However, the performance of the proposed CNN model varies: for evaluation tasks that are consistent with the trained network type, the proposed CNN model consistently achieves accurate evaluations of both attack curves and robustness values across all removal scenarios. When the predicted network type differs from the trained network, the CNN model still demonstrates favorable performance in the scenario of random node failure, showcasing its scalability and performance transferability. Nevertheless, the performance falls short of expectations in other removal scenarios. This observed scenario-sensitivity in the evaluation of network features has been overlooked in previous studies and necessitates further attention and optimization. Lastly, we discuss important unresolved questions and further investigation."}, "https://arxiv.org/abs/2310.19697": {"title": "A nonlinear spectral core-periphery detection method for multiplex networks", "link": "https://arxiv.org/abs/2310.19697", "description": "arXiv:2310.19697v2 Announce Type: replace-cross \nAbstract: Core-periphery detection aims to separate the nodes of a complex network into two subsets: a core that is densely connected to the entire network and a periphery that is densely connected to the core but sparsely connected internally. The definition of core-periphery structure in multiplex networks that record different types of interactions between the same set of nodes on different layers is nontrivial since a node may belong to the core in some layers and to the periphery in others. We propose a nonlinear spectral method for multiplex networks that simultaneously optimises a node and a layer coreness vector by maximising a suitable nonconvex homogeneous objective function by a provably convergent alternating fixed point iteration. We derive a quantitative measure for the quality of a given multiplex core-periphery structure that allows the determination of the optimal core size. Numerical experiments on synthetic and real-world networks illustrate that our approach is robust against noisy layers and significantly outperforms baseline methods while improving the latter with our novel optimised layer coreness weights. As the runtime of our method depends linearly on the number of edges of the network it is scalable to large-scale multiplex networks."}, "https://arxiv.org/abs/2405.04773": {"title": "Hypergraph-enhanced Dual Semi-supervised Graph Classification", "link": "https://arxiv.org/abs/2405.04773", "description": "arXiv:2405.04773v2 Announce Type: replace-cross \nAbstract: In this paper, we study semi-supervised graph classification, which aims at accurately predicting the categories of graphs in scenarios with limited labeled graphs and abundant unlabeled graphs. Despite the promising capability of graph neural networks (GNNs), they typically require a large number of costly labeled graphs, while a wealth of unlabeled graphs fail to be effectively utilized. Moreover, GNNs are inherently limited to encoding local neighborhood information using message-passing mechanisms, thus lacking the ability to model higher-order dependencies among nodes. To tackle these challenges, we propose a Hypergraph-Enhanced DuAL framework named HEAL for semi-supervised graph classification, which captures graph semantics from the perspective of the hypergraph and the line graph, respectively. Specifically, to better explore the higher-order relationships among nodes, we design a hypergraph structure learning to adaptively learn complex node dependencies beyond pairwise relations. Meanwhile, based on the learned hypergraph, we introduce a line graph to capture the interaction between hyperedges, thereby better mining the underlying semantic structures. Finally, we develop a relational consistency learning to facilitate knowledge transfer between the two branches and provide better mutual guidance. Extensive experiments on real-world graph datasets verify the effectiveness of the proposed method against existing state-of-the-art methods."}, "https://arxiv.org/abs/2405.18555": {"title": "Multigraph reconstruction via nonlinear random walk", "link": "https://arxiv.org/abs/2405.18555", "description": "arXiv:2405.18555v1 Announce Type: new \nAbstract: Over the last few years, network science has proved to be useful in modeling a variety of complex systems, composed of a large number of interconnected units. The intricate pattern of interactions often allows the system to achieve complex tasks, such as synchronization or collective motions. In this regard, the interplay between network structure and dynamics has long been recognized as a cornerstone of network science. Among dynamical processes, random walks are undoubtedly among the most studied stochastic processes. While traditionally, the random walkers are assumed to be independent, this assumption breaks down if nodes are endowed with a finite carrying capacity, a feature shared by many real-life systems. Recently, a class of nonlinear diffusion processes accounting for the finite carrying capacities of the nodes was introduced. The stationary nodes densities were shown to be nonlinearly correlated with the nodes degrees, allowing to uncover the network structure by performing a few measurements of the stationary density at the level of a single arbitrary node and by solving an inverse problem. In this work, we extend this class of nonlinear diffusion processes to the case of multigraphs, in which links between nodes carry distinct attributes. Assuming the knowledge of the pattern of interactions associated with one type of links, we show how the degree distribution of the whole multigraph can be reconstructed. The effectiveness of the reconstruction algorithm is demonstrated through simulations on various multigraph topologies."}, "https://arxiv.org/abs/2405.18748": {"title": "Equity Implications of Net-Zero Emissions: A Multi-Model Analysis of Energy Expenditures Across Income Classes Under Economy-Wide Deep Decarbonization Policies", "link": "https://arxiv.org/abs/2405.18748", "description": "arXiv:2405.18748v1 Announce Type: new \nAbstract: With companies, states, and countries targeting net-zero emissions around midcentury, there are questions about how these targets alter household welfare and finances, including distributional effects across income groups. This paper examines the distributional dimensions of technology transitions and net-zero policies with a focus on welfare impacts across household incomes. The analysis uses a model intercomparison with a range of energy-economy models using harmonized policy scenarios reaching economy-wide, net-zero CO2 emissions across the United States in 2050. We employ a novel linking approach that connects output from detailed energy system models with survey microdata on energy expenditures across income classes to provide distributional analysis of net-zero policies. Although there are differences in model structure and input assumptions, we find broad agreement in qualitative trends in policy incidence and energy burdens across income groups. Models generally agree that direct energy expenditures for many households will likely decline over time with reference and net-zero policies. However, there is variation in the extent of changes relative to current levels, energy burdens relative to reference levels, and electricity expenditures. Policy design, primarily how climate policy revenues are used, has first-order impacts on distributional outcomes. Net-zero policy costs, in both absolute and relative terms, are unevenly distributed across households, and relative increases in energy expenditures are higher for lowest-income households. However, we also find that recycled revenues from climate policies have countervailing effects when rebated on a per-capita basis, offsetting higher energy burdens and potentially even leading to net progressive outcomes."}, "https://arxiv.org/abs/2405.18803": {"title": "Information Dynamics in Evolving Networks Based on the Birth-Death Process: Random Drift and Natural Selection Perspective", "link": "https://arxiv.org/abs/2405.18803", "description": "arXiv:2405.18803v1 Announce Type: new \nAbstract: Dynamic processes in complex networks are crucial for better understanding collective behavior in human societies, biological systems, and the internet. In this paper, we first focus on the continuous Markov-based modeling of evolving networks with the birth-death of individuals. A new individual arrives at the group by the Poisson process, while new links are established in the network through either uniform connection or preferential attachment. Moreover, an existing individual has a limited lifespan before leaving the network. We determine stationary topological properties of these networks, including their size and mean degree. To address the effect of the birth-death evolution, we further study the information dynamics in the proposed network model from the random drift and natural selection perspective, based on assumptions of total-stochastic and fitness-driven evolution, respectively. In simulations, we analyze the fixation probability of individual information and find that means of new connections affect the random drift process but do not affect the natural selection process."}, "https://arxiv.org/abs/2405.19141": {"title": "Resilience of mobility network to dynamic population response across COVID-19 interventions: evidences from Chile", "link": "https://arxiv.org/abs/2405.19141", "description": "arXiv:2405.19141v1 Announce Type: new \nAbstract: The COVID19 pandemic highlighted the importance of non-traditional data sources, such as mobile phone data, to inform effective public health interventions and monitor adherence to such measures. Previous studies showed how socioeconomic characteristics shaped population response during restrictions and how repeated interventions eroded adherence over time. Less is known about how different population strata changed their response to repeated interventions and how this impacted the resulting mobility network. We study population response during the first and second infection waves of the COVID-19 pandemic in Chile and Spain. Via spatial lag and regression models, we investigate the adherence to mobility interventions at the municipality level in Chile, highlighting the significant role of wealth, labor structure, COVID-19 incidence, and network metrics characterizing business-as-usual municipality connectivity in shaping mobility changes during the two waves. We assess network structural similarities in the two periods by defining mobility hotspots and traveling probabilities in the two countries. As a proof of concept, we simulate and compare outcomes of an epidemic diffusion occurring in the two waves. Our analysis reveals the resilience of the mobility network across waves. We test the robustness of our findings recovering similar results for Spain. Finally, epidemic modeling suggests that historical mobility data from past waves can be leveraged to inform future disease spatial invasion models in repeated interventions. This study highlights the value of historical mobile phone data for building pandemic preparedness and lessens the need for real-time data streams for risk assessment and outbreak response. Our work provides valuable insights into the complex interplay of factors driving mobility across repeated interventions, aiding in developing targeted mitigation strategies."}, "https://arxiv.org/abs/2405.19199": {"title": "A statistical analysis of drug seizures and opioid overdose deaths in Ohio from 2014 to 2018", "link": "https://arxiv.org/abs/2405.19199", "description": "arXiv:2405.19199v1 Announce Type: new \nAbstract: This paper examines the association between police drug seizures and drug overdose deaths in Ohio from 2014 to 2018. We use linear regression, ARIMA models, and categorical data analysis to quantify the effect of drug seizure composition and weight on drug overdose deaths, to quantify the lag between drug seizures and overdose deaths, and to compare the weight distributions of drug seizures conducted by different types of law enforcement (national, local, and drug task forces). We find that drug seizure composition and weight have strong predictive value for drug overdose deaths (F = 27.14, p < 0.0001, R^2 = .7799). A time series analysis demonstrates no statistically significant lag between drug seizures and overdose deaths or weight. Histograms and Kolmogorov-Smirnov tests demonstrate stark differences between seizure weight distributions of different types of law enforcement (p < 0.0001 for each pairwise comparison). We include a discussion of what our conclusions mean for law enforcement and harm reduction efforts."}, "https://arxiv.org/abs/2405.18526": {"title": "Unlocking the Potential of Renewable Energy Through Curtailment Prediction", "link": "https://arxiv.org/abs/2405.18526", "description": "arXiv:2405.18526v1 Announce Type: cross \nAbstract: A significant fraction (5-15%) of renewable energy generated goes into waste in the grids around the world today due to oversupply issues and transmission constraints. Being able to predict when and where renewable curtailment occurs would improve renewable utilization. The core of this work is to enable the machine learning community to help decarbonize electricity grids by unlocking the potential of renewable energy through curtailment prediction."}, "https://arxiv.org/abs/2405.18873": {"title": "A Return to Biased Nets: New Specifications and Approximate Bayesian Inference", "link": "https://arxiv.org/abs/2405.18873", "description": "arXiv:2405.18873v1 Announce Type: cross \nAbstract: The biased net paradigm was the first general and empirically tractable scheme for parameterizing complex patterns of dependence in networks, expressing deviations from uniform random graph structure in terms of latent ``bias events,'' whose realizations enhance reciprocity, transitivity, or other structural features. Subsequent developments have introduced local specifications of biased nets, which reduce the need for approximations required in early specifications based on tracing processes. Here, we show that while one such specification leads to inconsistencies, a closely related Markovian specification both evades these difficulties and can be extended to incorporate new types of effects. We introduce the notion of inhibitory bias events, with satiation as an example, which are useful for avoiding degeneracies that can arise from closure bias terms. Although our approach does not lead to a computable likelihood, we provide a strategy for approximate Bayesian inference using random forest prevision. We demonstrate our approach on a network of friendship ties among college students, recapitulating a relationship between the sibling bias and tie strength posited in earlier work by Fararo."}, "https://arxiv.org/abs/2405.19125": {"title": "Early Detection of Critical Urban Events using Mobile Phone Network Data", "link": "https://arxiv.org/abs/2405.19125", "description": "arXiv:2405.19125v1 Announce Type: cross \nAbstract: Network Signalling Data (NSD) have the potential to provide continuous spatio-temporal information about the presence, mobility, and usage patterns of cell phone services by individuals. Such information is invaluable for monitoring large urban areas and supporting the implementation of decision-making services. When analyzed in real time, NSD can enable the early detection of critical urban events, including fires, large accidents, stampedes, terrorist attacks, and sports and leisure gatherings, especially if these events significantly impact mobile phone network activity in the affected areas. This paper presents empirical evidence that advanced NSD can detect anomalies in mobile traffic service consumption, attributable to critical urban events, with fine spatial and temporal resolutions. We introduce two methodologies for real-time anomaly detection from multivariate time series extracted from large-scale NSD, utilizing a range of algorithms adapted from the state-of-the-art in unsupervised machine learning techniques for anomaly detection. Our research includes a comprehensive quantitative evaluation of these algorithms on a large-scale dataset of NSD service consumption for the Paris region. The evaluation uses an original dataset of documented critical or unusual urban events. This dataset has been built as a ground truth basis for assessing the algorithms performance. The obtained results demonstrate that our framework can detect unusual events almost instantaneously and locate the affected areas with high precision, largely outperforming random classifiers. This efficiency and effectiveness underline the potential of NSD-based anomaly detection in significantly enhancing emergency response strategies and urban planning."}, "https://arxiv.org/abs/2405.19242": {"title": "Modeling public opinion control by a charismatic leader", "link": "https://arxiv.org/abs/2405.19242", "description": "arXiv:2405.19242v1 Announce Type: cross \nAbstract: We study the average long-time behavior of the binary opinions of a social group with peer-to-peer interactions under the influence of an external bias and a persuadable leader, a strongly-biased agent with a dynamic opinion with the intention of spreading it across the system. We use a generalized, fully-connected Ising model, with each spin representing the binary opinion of an agent at a given time and a single, super spin representing the opinion of the leader. External fields and interaction constants model the opinion bias and peer-to-peer interactions, respectively, while the temperature $T$ models an idealized social climate, representing an authoritarian regime if $T$ is low or a liberal one if $T$ is high. We derive a mean-field solution for the average magnetization $m$, the \"social mood\", and investigate how $m$ and the super spin magnetization vary as a function of $T$. We find that, depending on the initial conditions, due to the presence of metastable states, the sign of the average magnetization depends on the temperature. Finally, we verify that this effect is also present even if we consider only nearest-neighbor interactions within the social group."}, "https://arxiv.org/abs/2207.14016": {"title": "Cascades towards noise-induced transitions on networks revealed using information flows", "link": "https://arxiv.org/abs/2207.14016", "description": "arXiv:2207.14016v4 Announce Type: replace \nAbstract: Abrupt, system-wide transitions can be endogenously generated by seemingly stable networks of interacting dynamical units, such as mode switching in neuronal networks or public opinion changes in social systems. However, it remains poorly understood how such `noise-induced transitions' emerge from the interplay of network structure and dynamics on the network. Here, we report on two key roles that nodes can play in the progression towards noise-induced tipping points. The models used are dynamical networks where the nodes are governed by the Boltzmann-Gibbs distribution, but the concept is easily generalized. First, so-called `initiator nodes' absorb and then transmit short-lived fluctuations to neighboring nodes, making them temporarily more dynamic. These neighbor nodes can then in turn transmit fluctuations to their neighbors, and so on, leading to a domino-effect where the more stable a node is (i.e., high average free energy barrier), the more neighbors are needed that have become temporarily dynamic. Interestingly, towards the tipping point we identify so-called `stabilizer nodes' whose state information becomes part of the long-term memory of the system, after which the domino-effect is reversed and settles the node in their new stable attractor. We validate these roles by targeted interventions that make tipping points more (or less) likely to begin or lead to systemic change. This opens up possibilities for understanding and controlling endogenously generated metastable behavior."}, "https://arxiv.org/abs/2311.03275": {"title": "HetCAN: A Heterogeneous Graph Cascade Attention Network with Dual-Level Awareness", "link": "https://arxiv.org/abs/2311.03275", "description": "arXiv:2311.03275v2 Announce Type: replace-cross \nAbstract: Heterogeneous graph neural networks(HGNNs) have recently shown impressive capability in modeling heterogeneous graphs that are ubiquitous in real-world applications. Most existing methods for heterogeneous graphs mainly learn node embeddings by stacking multiple convolutional or attentional layers, which can be considered as capturing the high-order information from node-level aspect. However, different types of nodes in heterogeneous graphs have diverse features, it is also necessary to capture interactions among node features, namely the high-order information from feature-level aspect. In addition, most methods first align node features by mapping them into one same low-dimensional space, while they may lose some type information of nodes in this way. To address these problems, in this paper, we propose a novel Heterogeneous graph Cascade Attention Network (HetCAN) composed of multiple cascade blocks. Each cascade block includes two components, the type-aware encoder and the dimension-aware encoder. Specifically, the type-aware encoder compensates for the loss of node type information and aims to make full use of graph heterogeneity. The dimension-aware encoder is able to learn the feature-level high-order information by capturing the interactions among node features. With the assistance of these components, HetCAN can comprehensively encode information of node features, graph heterogeneity and graph structure in node embeddings. Extensive experiments demonstrate the superiority of HetCAN over advanced competitors and also exhibit its efficiency and robustness."}, "https://arxiv.org/abs/2401.03390": {"title": "Dynamics-based Feature Augmentation of Graph Neural Networks for Variant Emergence Prediction", "link": "https://arxiv.org/abs/2401.03390", "description": "arXiv:2401.03390v2 Announce Type: replace-cross \nAbstract: During the COVID-19 pandemic, a major driver of new surges has been the emergence of new variants. When a new variant emerges in one or more countries, other nations monitor its spread in preparation for its potential arrival. The impact of the new variant and the timings of epidemic peaks in a country highly depend on when the variant arrives. The current methods for predicting the spread of new variants rely on statistical modeling, however, these methods work only when the new variant has already arrived in the region of interest and has a significant prevalence. Can we predict when a variant existing elsewhere will arrive in a given region? To address this question, we propose a variant-dynamics-informed Graph Neural Network (GNN) approach. First, we derive the dynamics of variant prevalence across pairs of regions (countries) that apply to a large class of epidemic models. The dynamics motivate the introduction of certain features in the GNN. We demonstrate that our proposed dynamics-informed GNN outperforms all the baselines, including the currently pervasive framework of Physics-Informed Neural Networks (PINNs). To advance research in this area, we introduce a benchmarking tool to assess a user-defined model's prediction performance across 87 countries and 36 variants."}, "https://arxiv.org/abs/2401.04133": {"title": "SynHING: Synthetic Heterogeneous Information Network Generation for Graph Learning and Explanation", "link": "https://arxiv.org/abs/2401.04133", "description": "arXiv:2401.04133v2 Announce Type: replace-cross \nAbstract: Graph Neural Networks (GNNs) excel in delineating graph structures in diverse domains, including community analysis and recommendation systems. As the interpretation of GNNs becomes increasingly important, the demand for robust baselines and expansive graph datasets is accentuated, particularly in the context of Heterogeneous Information Networks (HIN). Addressing this, we introduce SynHING, a novel framework for Synthetic Heterogeneous Information Network Generation aimed at enhancing graph learning and explanation. SynHING systematically identifies major motifs in a target HIN and employs a bottom-up generation process with intra-cluster and inter-cluster merge modules. This process, supplemented by post-pruning techniques, ensures the synthetic HIN closely mirrors the original graph's structural and statistical properties. Crucially, SynHING provides ground-truth motifs for evaluating GNN explainer models, setting a new standard for explainable, synthetic HIN generation and contributing to the advancement of interpretable machine learning in complex networks."}, "https://arxiv.org/abs/2402.02464": {"title": "A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer", "link": "https://arxiv.org/abs/2402.02464", "description": "arXiv:2402.02464v3 Announce Type: replace-cross \nAbstract: Can we model Non-Euclidean graphs as pure language or even Euclidean vectors while retaining their inherent information? The Non-Euclidean property have posed a long term challenge in graph modeling. Despite recent graph neural networks and graph transformers efforts encoding graphs as Euclidean vectors, recovering the original graph from vectors remains a challenge. In this paper, we introduce GraphsGPT, featuring an Graph2Seq encoder that transforms Non-Euclidean graphs into learnable Graph Words in the Euclidean space, along with a GraphGPT decoder that reconstructs the original graph from Graph Words to ensure information equivalence. We pretrain GraphsGPT on $100$M molecules and yield some interesting findings: (1) The pretrained Graph2Seq excels in graph representation learning, achieving state-of-the-art results on $8/9$ graph classification and regression tasks. (2) The pretrained GraphGPT serves as a strong graph generator, demonstrated by its strong ability to perform both few-shot and conditional graph generation. (3) Graph2Seq+GraphGPT enables effective graph mixup in the Euclidean space, overcoming previously known Non-Euclidean challenges. (4) The edge-centric pretraining framework GraphsGPT demonstrates its efficacy in graph domain tasks, excelling in both representation and generation. Code is available at \\href{https://github.com/A4Bio/GraphsGPT}{GitHub}."}, "https://arxiv.org/abs/2405.19369": {"title": "Sublinear Cuts are the Exception in BDF-GIRGs", "link": "https://arxiv.org/abs/2405.19369", "description": "arXiv:2405.19369v1 Announce Type: new \nAbstract: The introduction of geometry has proven instrumental in the efforts towards more realistic models for real-world networks. In Geometric Inhomogeneous Random Graphs (GIRGs), Euclidean Geometry induces clustering of the vertices, which is widely observed in networks in the wild. Euclidean Geometry in multiple dimensions however restricts proximity of vertices to those cases where vertices are close in each coordinate. We introduce a large class of GIRG extensions, called BDF-GIRGs, which capture arbitrary hierarchies of the coordinates within the distance function of the vertex feature space. These distance functions have the potential to allow more realistic modeling of the complex formation of social ties in real-world networks, where similarities between people lead to connections. Here, similarity with respect to certain features, such as familial kinship or a shared workplace, suffices for the formation of ties. It is known that - while many key properties of GIRGs, such as log-log average distance and sparsity, are independent of the distance function - the Euclidean metric induces small separators, i.e. sublinear cuts of the unique giant component in GIRGs, whereas no such sublinear separators exist under the component-wise minimum distance. Building on work of Lengler and Todorovi\\'{c}, we give a complete classification for the existence of small separators in BDF-GIRGs. We further show that BDF-GIRGs all fulfill a stochastic triangle inequality and thus also exhibit clustering."}, "https://arxiv.org/abs/2405.19375": {"title": "Improving global awareness of linkset predictions using Cross-Attentive Modulation tokens", "link": "https://arxiv.org/abs/2405.19375", "description": "arXiv:2405.19375v1 Announce Type: new \nAbstract: Most of multiple link prediction or graph generation techniques rely on the attention mechanism or on Graph Neural Networks (GNNs), which consist in leveraging node-level information exchanges in order to form proper link predictions. Such node-level interactions do not process nodes as an ordered sequence, which would imply some kind of natural ordering of the nodes: they are said to be permutation invariant mechanisms. They are well suited for graph problems, but struggle at providing a global orchestration of the predicted links, which can result in a loss of performance. Some typical issues can be the difficulty to ensure high-level properties such as global connectedness, fixed diameter or to avoid information bottleneck effects such as oversmoothing and oversquashing, which respectively consist in abundant smoothing in dense areas leading to a loss of information and a tendency to exclude isolated nodes from the message passing scheme, and often result in irrelevant, unbalanced link predictions. To tackle this problem, we hereby present Cross-Attentive Modulation (CAM) tokens, which introduce cross-attentive units used to condition node and edge-level modulations in order to enable context-aware computations that improve the global consistency of the prediction links. We will implement it on a few permutation invariant architectures, and showcase benchmarks that prove the merits of our work."}, "https://arxiv.org/abs/2405.19383": {"title": "Network Analytics for Anti-Money Laundering -- A Systematic Literature Review and Experimental Evaluation", "link": "https://arxiv.org/abs/2405.19383", "description": "arXiv:2405.19383v1 Announce Type: new \nAbstract: Money laundering presents a pervasive challenge, burdening society by financing illegal activities. To more effectively combat and detect money laundering, the use of network information is increasingly being explored, exploiting that money laundering necessarily involves interconnected parties. This has lead to a surge in literature on network analytics (NA) for anti-money laundering (AML). The literature, however, is fragmented and a comprehensive overview of existing work is missing. This results in limited understanding of the methods that may be applied and their comparative detection power. Therefore, this paper presents an extensive and systematic review of the literature. We identify and analyse 97 papers in the Web of Science and Scopus databases, resulting in a taxonomy of approaches following the fraud analytics framework of Bockel-Rickermann et al.. Moreover, this paper presents a comprehensive experimental framework to evaluate and compare the performance of prominent NA methods in a uniform setup. The framework is applied on the publicly available Elliptic data set and implements manual feature engineering, random walk-based methods, and deep learning GNNs. We conclude from the results that network analytics increases the predictive power of the AML model with graph neural networks giving the best results. An open source implementation of the experimental framework is provided to facilitate researchers and practitioners to extend upon these results and experiment on proprietary data. As such, we aim to promote a standardised approach towards the analysis and evaluation of network analytics for AML."}, "https://arxiv.org/abs/2405.19459": {"title": "Math behind everyday life: on distribution of \"black days\" and beyond", "link": "https://arxiv.org/abs/2405.19459", "description": "arXiv:2405.19459v1 Announce Type: new \nAbstract: In our daily lives, we encounter numerous independent events, each occurring with varying probabilities over time. This letter delves into the scientific background behind the inhomogeneous distribution of these events over time, often resulting in what we refer to as ``black days'', where multiple events seem to converge at once. In the first part of the work we have performed an analysis involving $D$ independent periodic and random sequences of events. Employing the Uniform Manifold Approximation and Projection (UMAP) technique, we observed a clustering of sequences of events on a 2D manifold ${\\cal M}$ at some large $D_{cr}$, which we interpret as the manifestation of ``black days'' and which occurs in a narrow interval around $D_{cr}$. We found that increasing the number of sequences within rather wide interval below $D_{cr}$ leads to a plateau in the clustering on ${\\cal M}$. In the second part of the work we examined in detail clustering patterns of independently distributed $N$ points within the corners of a $D$-dimensional cube when $1\\ll N<D$. Our findings revealed that a transition to a single-component cluster occurs at a critical dimensionality, $D_{cr}$, via a nearly third-order phase transition. Finally, we have addressed the question ``How the infinite-dimensional space could look like?'' Considering the eigenvalue problem for the hyperspherical Laplacian $\\nabla^2_D$ in the limit $D\\to\\infty$, we conjectured about the topology of a target space representing the hyperspherical layer."}, "https://arxiv.org/abs/2405.19552": {"title": "Point process analysis of geographical diffusion of news in Argentina", "link": "https://arxiv.org/abs/2405.19552", "description": "arXiv:2405.19552v1 Announce Type: new \nAbstract: The diffusion of information plays a crucial role in a society, characterizing the diffusion process is challenging because it is highly non-stationary and varies with the media type. To understand the spreading of newspaper news in Argentina, we collected data from more than 27000 articles published in six main provinces during four months. We classified the articles into 20 thematic axes and obtained a set of 120 time series that capture daily newspaper attention on different topics in different provinces. To analyze the data we use a point process approach. For each topic, $n$, and for all pairs of provinces, $i$ and $j$, we use two measures to quantify the synchronicity of the events, $Q_s(i,j)$, which quantifies the number of events that occur almost simultaneously in $i$ and $j$, and $Q_a(i,j)$, which quantifies the direction of news spreading. We also analyze the dataset using well-known measures to detect correlations and dependencies, computed from the raw time series: undirected measures (linear cross-correlation, $CC$, and nonlinear mutual information, $MI$) and directed measures (linear Granger causality, $GC$, and nonlinear Transfer entropy, $TE$). Our analysis unveils how fast the information diffusion process is, as high values of $Q_{s}$, $CC$, and $MI$ reveal pairs of provinces with very similar and almost simultaneous temporal variations of media attention. On the other hand, $GC$ and $TE$ do not perform well in this context because they often return opposite directions of information transfer. We interpret this as due to three main factors: the characteristics of the data, which is highly non-stationary, the characteristics of the information diffusion process, which is very fast and probably acts at a sub-resolution time scale, and the action of large media companies that act as global, external drivers of information dissemination."}, "https://arxiv.org/abs/2405.19565": {"title": "Unbending strategies shepherd cooperation and suppress extortion in spatial populations", "link": "https://arxiv.org/abs/2405.19565", "description": "arXiv:2405.19565v1 Announce Type: new \nAbstract: Evolutionary game dynamics on networks typically consider the competition among simple strategies such as cooperation and defection in the Prisoner's Dilemma and summarize the effect of population structure as network reciprocity. However, it remains largely unknown regarding the evolutionary dynamics involving multiple powerful strategies typically considered in repeated games, such as the zero-determinant (ZD) strategies that are able to enforce a linear payoff relationship between them and their co-players. Here, we consider the evolutionary dynamics of always cooperate (AllC), extortionate ZD (extortioners), and unbending players in lattice populations based on the commonly used death-birth updating. Out of the class of unbending strategies, we consider a particular candidate, PSO Gambler, a machine-learning-optimized memory-one strategy, which can foster reciprocal cooperation and fairness among extortionate players. We derive analytical results under weak selection and rare mutations, including pairwise fixation probabilities and long-term frequencies of strategies. In the absence of the third unbending type, extortioners can achieve a half-half split in equilibrium with unconditional cooperators for sufficiently large extortion factors. However, the presence of unbending players fundamentally changes the dynamics and tilts the system to favor unbending cooperation. Most surprisingly, extortioners cannot dominate at all regardless of how large their extortion factor is, and the long-term frequency of unbending players is maintained almost as a constant. Our analytical method is applicable to studying the evolutionary dynamics of multiple strategies in structured populations. Our work provides insights into the interplay between network reciprocity and direct reciprocity, revealing the role of unbending strategies in enforcing fairness and suppressing extortion."}, "https://arxiv.org/abs/2405.20166": {"title": "An approximation for return time distributions of random walks on sparse networks", "link": "https://arxiv.org/abs/2405.20166", "description": "arXiv:2405.20166v1 Announce Type: new \nAbstract: We propose an approximation for the first return time distribution of random walks on undirected networks. We combine a message-passing solution with a mean-field approximation, to account for the short- and long-term behaviours respectively. We test this approximation on several classes of large graphs and find excellent agreement between our approximations and the true distributions. While the statistical properties of a random walk will depend on the structure of the network, the observed agreement between our approximations and numerical calculations implies that while local structure is clearly very influential, global structure is only important in a relatively superficial way, namely through the total number of edges."}, "https://arxiv.org/abs/2405.20277": {"title": "Pre-train and Refine: Towards Higher Efficiency in K-Agnostic Community Detection without Quality Degradation", "link": "https://arxiv.org/abs/2405.20277", "description": "arXiv:2405.20277v1 Announce Type: new \nAbstract: Community detection (CD) is a classic graph inference task that partitions nodes of a graph into densely connected groups. While many CD methods have been proposed with either impressive quality or efficiency, balancing the two aspects remains a challenge. This study explores the potential of deep graph learning to achieve a better trade-off between the quality and efficiency of K-agnostic CD, where the number of communities K is unknown. We propose PRoCD (Pre-training & Refinement fOr Community Detection), a simple yet effective method that reformulates K-agnostic CD as the binary node pair classification. PRoCD follows a pre-training & refinement paradigm inspired by recent advances in pre-training techniques. We first conduct the offline pre-training of PRoCD on small synthetic graphs covering various topology properties. Based on the inductive inference across graphs, we then generalize the pre-trained model (with frozen parameters) to large real graphs and use the derived CD results as the initialization of an existing efficient CD method (e.g., InfoMap) to further refine the quality of CD results. In addition to benefiting from the transfer ability regarding quality, the online generalization and refinement can also help achieve high inference efficiency, since there is no time-consuming model optimization. Experiments on public datasets with various scales demonstrate that PRoCD can ensure higher efficiency in K-agnostic CD without significant quality degradation."}, "https://arxiv.org/abs/2405.19436": {"title": "Traffic Modeling and Forecast based on Stochastic Cell-Automata and Distributed Fiber-Optic Sensing -- A Numerical Experiment", "link": "https://arxiv.org/abs/2405.19436", "description": "arXiv:2405.19436v1 Announce Type: cross \nAbstract: This paper demonstrates accurate traffic modeling and forecast using stochastic cell-automata (CA) and distributed fiber-optic sensing (DFOS). Traffic congestion is a dominant issue in highways. To reduce congestion, real-time traffic control by short-term forecast is necessary. For achieving this, data assimilation using a stochastic CA model and DFOS is promising. Data assimilation with a CA enables us to model real-time traffic flow with simple processes even when rare or sudden events occur, which is challenging for usual machine learning-based methods. DFOS overcomes issues of conventional point sensors that have dead zones of observation. By estimating optimal model parameters that reproduce observed traffic flow in the simulation, future traffic flow is forecasted from the simulation. We propose an optimal model parameter estimation method using mean velocity as an extracted feature and the particle filter. In addition, an estimation methodology for the microscopic traffic situation is developed to set the initial condition of simulation for forecast in accordance with observation. The proposed methods are verified by simulation-based traffic flow. The simulation adopts the stochastic Nishinari-Fukui-Schadschneider model. The optimal model parameters are successfully derived from posterior probability distributions (PPDs) estimated from DFOS data. In contrast, those estimated from point sensors fail. The PPDs of model parameters also indicate that each parameter has different sensitivities to traffic flow. A traffic forecast up to 60 minutes later is carried out. Using optimal model parameters estimated from DFOS, the forecast error of mean velocity is approximately $\\pm$10 km/h (percentage error is 18%). The error attains half of it when conventional point sensors are used. We conclude that DFOS is a powerful technique for traffic modeling and short-term forecast."}, "https://arxiv.org/abs/2402.07656": {"title": "Low Cost Carriers induce specific and identifiable delay propagation patterns: an analysis of the EU and US systems", "link": "https://arxiv.org/abs/2402.07656", "description": "arXiv:2402.07656v2 Announce Type: replace \nAbstract: The impact of air transport delays and their propagation has long been studied, mainly from environmental and mobility viewpoints, using a wide range of data analysis tools and simulations. Less attention has nevertheless been devoted to how delays create meso-scale structures around each airport. In this work we tackle this issue by reconstructing functional networks of delay propagation centred at each airport, and studying their identifiability (i.e. how unique they are) using Deep Learning models. We find that such delay propagation neighbourhoods are highly unique when they correspond to airports with a high share of Low Cost Carriers operations; and demonstrate the robustness of these findings for the EU and US systems, and to different methodological choices. We further discuss some operational implications of this uniqueness."}, "https://arxiv.org/abs/2405.20457": {"title": "Online network topology shapes personal narratives and hashtag generation", "link": "https://arxiv.org/abs/2405.20457", "description": "arXiv:2405.20457v1 Announce Type: new \nAbstract: While narratives have shaped cognition and cultures for centuries, digital media and online social networks have introduced new narrative phenomena. With increased narrative agency, networked groups of individuals can directly contribute and steer narratives that center our collective discussions of politics, science, and morality. We report the results of an online network experiment on narrative and hashtag generation, in which networked groups of participants interpreted a text-based narrative of a disaster event, and were incentivized to produce matching hashtags with their network neighbors. We found that network structure not only influences the emergence of dominant beliefs through coordination with network neighbors, but also impacts participants' use of causal language in their personal narratives."}, "https://arxiv.org/abs/2405.20740": {"title": "Discrete Lanchester attrition models: the case of precautionary surrender", "link": "https://arxiv.org/abs/2405.20740", "description": "arXiv:2405.20740v1 Announce Type: new \nAbstract: Discrete Lanchester-type attrition models describe many types of antagonistic situations; the preferred interpretation is two fleets of battleships, each trying to sink the other. Such models may be characterised by a bivariate recurrence relation. Here I consider a restricted case in which a fleet that finds itself two or three units behind its opponent immediately surrenders. I present some theoretical and numerical results and suggest lines for further work."}, "https://arxiv.org/abs/2405.20918": {"title": "Flexible inference in heterogeneous and attributed multilayer networks", "link": "https://arxiv.org/abs/2405.20918", "description": "arXiv:2405.20918v1 Announce Type: new \nAbstract: Networked datasets are often enriched by different types of information about individual nodes or edges. However, most existing methods for analyzing such datasets struggle to handle the complexity of heterogeneous data, often requiring substantial model-specific analysis. In this paper, we develop a probabilistic generative model to perform inference in multilayer networks with arbitrary types of information. Our approach employs a Bayesian framework combined with the Laplace matching technique to ease interpretation of inferred parameters. Furthermore, the algorithmic implementation relies on automatic differentiation, avoiding the need for explicit derivations. This makes our model scalable and flexible to adapt to any combination of input data. We demonstrate the effectiveness of our method in detecting overlapping community structures and performing various prediction tasks on heterogeneous multilayer data, where nodes and edges have different types of attributes. Additionally, we showcase its ability to unveil a variety of patterns in a social support network among villagers in rural India by effectively utilizing all input information in a meaningful way."}, "https://arxiv.org/abs/2405.20445": {"title": "GraphAny: A Foundation Model for Node Classification on Any Graph", "link": "https://arxiv.org/abs/2405.20445", "description": "arXiv:2405.20445v1 Announce Type: cross \nAbstract: Foundation models that can perform inference on any new task without requiring specific training have revolutionized machine learning in vision and language applications. However, applications involving graph-structured data remain a tough nut for foundation models, due to challenges in the unique feature- and label spaces associated with each graph. Traditional graph ML models such as graph neural networks (GNNs) trained on graphs cannot perform inference on a new graph with feature and label spaces different from the training ones. Furthermore, existing models learn functions specific to the training graph and cannot generalize to new graphs. In this work, we tackle these two challenges with a new foundational architecture for inductive node classification named GraphAny. GraphAny models inference on a new graph as an analytical solution to a LinearGNN, thereby solving the first challenge. To solve the second challenge, we learn attention scores for each node to fuse the predictions of multiple LinearGNNs. Specifically, the attention module is carefully parameterized as a function of the entropy-normalized distance-features between multiple LinearGNNs predictions to ensure generalization to new graphs. Empirically, GraphAny trained on the Wisconsin dataset with only 120 labeled nodes can effectively generalize to 30 new graphs with an average accuracy of 67.26\\% in an inductive manner, surpassing GCN and GAT trained in the supervised regime, as well as other inductive baselines."}, "https://arxiv.org/abs/2405.20640": {"title": "Heterophilous Distribution Propagation for Graph Neural Networks", "link": "https://arxiv.org/abs/2405.20640", "description": "arXiv:2405.20640v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs) have achieved remarkable success in various graph mining tasks by aggregating information from neighborhoods for representation learning. The success relies on the homophily assumption that nearby nodes exhibit similar behaviors, while it may be violated in many real-world graphs. Recently, heterophilous graph neural networks (HeterGNNs) have attracted increasing attention by modifying the neural message passing schema for heterophilous neighborhoods. However, they suffer from insufficient neighborhood partition and heterophily modeling, both of which are critical but challenging to break through. To tackle these challenges, in this paper, we propose heterophilous distribution propagation (HDP) for graph neural networks. Instead of aggregating information from all neighborhoods, HDP adaptively separates the neighbors into homophilous and heterphilous parts based on the pseudo assignments during training. The heterophilous neighborhood distribution is learned with orthogonality-oriented constraint via a trusted prototype contrastive learning paradigm. Both the homophilous and heterophilous patterns are propagated with a novel semantic-aware message passing mechanism. We conduct extensive experiments on 9 benchmark datasets with different levels of homophily. Experimental results show that our method outperforms representative baselines on heterophilous datasets."}, "https://arxiv.org/abs/2405.20702": {"title": "Effect of antibody levels on the spread of disease in multiple infections", "link": "https://arxiv.org/abs/2405.20702", "description": "arXiv:2405.20702v1 Announce Type: cross \nAbstract: There are complex interactions between antibody levels and epidemic propagation, the antibody level of an individual influences the probability of infection, and the spread of the virus influences the antibody level of each individual. There exist some viruses that, in their natural state, cause antibody levels in an infected individual to gradually decay. When these antibody levels decay to a certain point, the individual can be reinfected, such as with COVID 19. To describe their interaction, we introduce a novel mathematical model that incorporates the presence of an antibody retention rate to investigate the infection patterns of individuals who survive multiple infections. The model is composed of a system of stochastic differential equations to derive the equilibrium point and threshold of the model and presents rich experimental results of numerical simulations to further elucidate the propagation properties of the model. We find that the antibody decay rate strongly affects the propagation process, and also that different network structures have different sensitivities to the antibody decay rate, and that changes in the antibody decay rate cause stronger changes in the propagation process in Barabasi Albert networks. Furthermore, we investigate the stationary distribution of the number of infection states and the final antibody levels, and find that they both satisfy the normal distribution, but the standard deviation is small in the Barabasi Albert network. Finally, we explore the effect of individual antibody differences and decay rates on the final population antibody levels, and uncover that individual antibody differences do not affect the final mean antibody levels. The study offers valuable insights for epidemic prevention and control in practical applications."}, "https://arxiv.org/abs/2405.20724": {"title": "Learning on Large Graphs using Intersecting Communities", "link": "https://arxiv.org/abs/2405.20724", "description": "arXiv:2405.20724v1 Announce Type: cross \nAbstract: Message Passing Neural Networks (MPNNs) are a staple of graph machine learning. MPNNs iteratively update each node's representation in an input graph by aggregating messages from the node's neighbors, which necessitates a memory complexity of the order of the number of graph edges. This complexity might quickly become prohibitive for large graphs provided they are not very sparse. In this paper, we propose a novel approach to alleviate this problem by approximating the input graph as an intersecting community graph (ICG) -- a combination of intersecting cliques. The key insight is that the number of communities required to approximate a graph does not depend on the graph size. We develop a new constructive version of the Weak Graph Regularity Lemma to efficiently construct an approximating ICG for any input graph. We then devise an efficient graph learning algorithm operating directly on ICG in linear memory and time with respect to the number of nodes (rather than edges). This offers a new and fundamentally different pipeline for learning on very large non-sparse graphs, whose applicability is demonstrated empirically on node classification tasks and spatio-temporal data processing."}, "https://arxiv.org/abs/2404.01489": {"title": "Perceived Social Influence on Vaccination Decisions: A COVID-19 Case Study", "link": "https://arxiv.org/abs/2404.01489", "description": "arXiv:2404.01489v2 Announce Type: replace \nAbstract: In this study, we examine the perceived influence of others, across both strong and weak social ties, on COVID-19 vaccination decisions in the United States. We add context to social influence by measuring related concepts, such as perceived agreement of others and perceived danger of COVID-19 to others. We find that vaccinated populations perceived more influence from their social circles than unvaccinated populations. This finding holds true across various social groups, including family, close friends, and neighbors. Vaccinated participants perceived that others agreed with their decision to get vaccinated more than unvaccinated participants perceived others to agree with their decision to not get vaccinated. Despite the clear differences in perceived social influence and agreement across the groups, the majority of participants across both vaccinated and unvaccinated populations perceived no social influence from all social group in their decisions. Aligning with this result, we find through open-ended responses that both vaccinated and unvaccinated participants frequently cited fear as a motivating factor in their decision, rather than social influence: vaccinated participants feared COVID-19, while unvaccinated participants feared the vaccine itself."}, "https://arxiv.org/abs/2211.04634": {"title": "Learning Optimal Graph Filters for Clustering of Attributed Graphs", "link": "https://arxiv.org/abs/2211.04634", "description": "arXiv:2211.04634v2 Announce Type: replace-cross \nAbstract: Many real-world systems can be represented as graphs where the different entities in the system are presented by nodes and their interactions by edges. An important task in studying large datasets with graphical structure is graph clustering. While there has been a lot of work on graph clustering using the connectivity between the nodes, many real-world networks also have node attributes. Clustering attributed graphs requires joint modeling of graph structure and node attributes. Recent work has focused on combining these two complementary sources of information through graph convolutional networks and graph filtering. However, these methods are mostly limited to lowpass filtering and do not explicitly learn the filter parameters for the clustering task. In this paper, we introduce a graph signal processing based approach, where we learn the parameters of Finite Impulse Response (FIR) and Autoregressive Moving Average (ARMA) graph filters optimized for clustering. The proposed approach is formulated as a two-step iterative optimization problem, focusing on learning interpretable graph filters that are optimal for the given data and that maximize the separation between different clusters. The proposed approach is evaluated on attributed networks and compared to the state-of-the-art methods."}, "https://arxiv.org/abs/2305.09938": {"title": "Mastering Long-Tail Complexity on Graphs: Characterization, Learning, and Generalization", "link": "https://arxiv.org/abs/2305.09938", "description": "arXiv:2305.09938v4 Announce Type: replace-cross \nAbstract: In the context of long-tail classification on graphs, the vast majority of existing work primarily revolves around the development of model debiasing strategies, intending to mitigate class imbalances and enhance the overall performance. Despite the notable success, there is very limited literature that provides a theoretical tool for characterizing the behaviors of long-tail classes in graphs and gaining insight into generalization performance in real-world scenarios. To bridge this gap, we propose a generalization bound for long-tail classification on graphs by formulating the problem in the fashion of multi-task learning, i.e., each task corresponds to the prediction of one particular class. Our theoretical results show that the generalization performance of long-tail classification is dominated by the overall loss range and the task complexity. Building upon the theoretical findings, we propose a novel generic framework HierTail for long-tail classification on graphs. In particular, we start with a hierarchical task grouping module that allows us to assign related tasks into hypertasks and thus control the complexity of the task space; then, we further design a balanced contrastive learning module to adaptively balance the gradients of both head and tail classes to control the loss range across all tasks in a unified fashion. Extensive experiments demonstrate the effectiveness of HierTail in characterizing long-tail classes on real graphs, which achieves up to 12.9% improvement over the leading baseline method in accuracy."}, "https://arxiv.org/abs/2406.00344": {"title": "Efficient Historical Butterfly Counting in Large Temporal Bipartite Networks via Graph Structure-aware Index", "link": "https://arxiv.org/abs/2406.00344", "description": "arXiv:2406.00344v1 Announce Type: new \nAbstract: Bipartite graphs are ubiquitous in many domains, e.g., e-commerce platforms, social networks, and academia, by modeling interactions between distinct entity sets. Within these graphs, the butterfly motif, a complete 2*2 biclique, represents the simplest yet significant subgraph structure, crucial for analyzing complex network patterns. Counting the butterflies offers significant benefits across various applications, including community analysis and recommender systems. Additionally, the temporal dimension of bipartite graphs, where edges activate within specific time frames, introduces the concept of historical butterfly counting, i.e., counting butterflies within a given time interval. This temporal analysis sheds light on the dynamics and evolution of network interactions, offering new insights into their mechanisms. Despite its importance, no existing algorithm can efficiently solve the historical butterfly counting task. To address this, we design two novel indices whose memory footprints are dependent on #butterflies and #wedges, respectively. Combining these indices, we propose a graph structure-aware indexing approach that significantly reduces memory usage while preserving exceptional query speed. We theoretically prove that our approach is particularly advantageous on power-law graphs, a common characteristic of real-world bipartite graphs, by surpassing traditional complexity barriers for general graphs. Extensive experiments reveal that our query algorithms outperform existing methods by up to five magnitudes, effectively balancing speed with manageable memory requirements."}, "https://arxiv.org/abs/2406.01113": {"title": "Bridging the Digital Divide: Mapping Internet Connectivity Evolution, Inequalities, and Resilience in six Brazilian Cities", "link": "https://arxiv.org/abs/2406.01113", "description": "arXiv:2406.01113v1 Announce Type: new \nAbstract: We investigate the evolution of Internet speed and its implications for access to key digital services, as well as the resilience of the network during crises, focusing on six major Brazilian cities: Belo Horizonte, Bras\\'ilia, Fortaleza, Manaus, Rio de Janeiro, and S\\~ao Paulo. Leveraging a unique dataset of Internet Speedtest results provided by Ookla, we analyze Internet speed trends from 2017 to 2023. Our findings reveal significant improvements in Internet speed across all cities. However, we find that prosperous areas generally exhibit better Internet access, and that the dependence of Internet quality on wealth have increased over time. Additionally, we investigate the impact of Internet quality on access to critical online services, focusing on e-learning. Our analysis shows that nearly 13% of catchment areas around educational facilities have Internet speeds below the threshold required for e-learning, with less rich areas experiencing more significant challenges. Moreover, we investigate the network's resilience during the COVID-19 pandemic, finding a sharp decline in network quality following the declaration of national emergency. We also find that less wealthy areas experience larger drops in network quality during crises. Overall, this study underscores the importance of addressing disparities in Internet access to ensure equitable access to digital services and enhance network resilience during crises"}, "https://arxiv.org/abs/2406.01341": {"title": "Important node identification for complex networks based on improved Electre Multi-Attribute fusion", "link": "https://arxiv.org/abs/2406.01341", "description": "arXiv:2406.01341v1 Announce Type: new \nAbstract: Influence maximization problem involves selecting a subset of seed nodes within a social network to maximize information spread under a given diffusion model, so how to identify the important nodes is the problem to be considered in this paper. Due to the great differences in the reality of the network, a class of multi-attribute decision fusion methods is often used to solve this problem. Electre is mostly used to solve the problems of investment order, benefit, and risk assessment of projects in economics, which supports the decision maker to make choices by comparing the differences between a set of alternatives. In this paper, we propose a multi-attribute decision fusion method named SK-E, which construct local and global metrics for different networks, use the improved Electre to make decision fusion between local and global metrics of nodes, to get the optimal weight between local and global metrics, and then identify the important nodes. The proposed method demonstrates superior accuracy compared to other methods, as evaluated through three experiments: the SIR epidemic model, the independent cascade model, and constraint efficiency. These experiments were conducted across six different real networks selected as the experimental dataset."}, "https://arxiv.org/abs/2406.01367": {"title": "Structural prediction of super-diffusion in multiplex networks", "link": "https://arxiv.org/abs/2406.01367", "description": "arXiv:2406.01367v1 Announce Type: new \nAbstract: Diffusion dynamics in multiplex networks can model a diverse number of real-world processes. In some specific configurations of these systems, the super-diffusion phenomenon arises, in which the diffusion is faster in the multiplex network than in any of its layers. Many studies attempt to characterize this phenomenon by examining its dependency on structural properties of the network, such as overlap, average degree, network dissimilarity, and others. While certain properties show a correlation with super-diffusion in specific networks, a broader characterization is still missing. Here, we introduce a structural parameter based on the minimum node strength that effectively predicts the occurrence of super-diffusion in multiplex networks. Additionally, we propose a novel framework for deriving analytical bounds for several multiplex networks structures. Finally, we analyze and justify why certain arrangements of the inter-layer connections induce super-diffusion. These findings provide novel insights into the super-diffusion phenomenon and the interplay between network structure and dynamics."}, "https://arxiv.org/abs/2406.01517": {"title": "Beyond symmetrization: effective adjacency matrices and renormalization for (un)singed directed graphs", "link": "https://arxiv.org/abs/2406.01517", "description": "arXiv:2406.01517v1 Announce Type: new \nAbstract: To address the peculiarities of directed and/or signed graphs, new Laplacian operators have emerged. For instance, in the case of directionality, we encounter the magnetic operator, dilation (which is underexplored), operators based on random walks, and so forth. The definition of these new operators leads to the need for new studies and concepts, and consequently, the development of new computational tools. But is this really necessary? In this work, we define the concept of effective adjacency matrices that arise from the definition of deformed Laplacian operators such as magnetic, dilation, and signal. These effective matrices allow mapping generic graphs to a family of unsigned, undirected graphs, enabling the application of the well-explored toolkit of measures, machine learning methods, and renormalization groups of undirected graphs. To explore the interplay between deformed operators and effective matrices, we show how the Hodge-Helmholtz decomposition can assist us in navigating this complexity."}, "https://arxiv.org/abs/2406.00617": {"title": "Maximum $k$-Plex Search: An Alternated Reduction-and-Bound Method", "link": "https://arxiv.org/abs/2406.00617", "description": "arXiv:2406.00617v1 Announce Type: cross \nAbstract: $k$-plexes relax cliques by allowing each vertex to disconnect to at most $k$ vertices. Finding a maximum $k$-plex in a graph is a fundamental operator in graph mining and has been receiving significant attention from various domains. The state-of-the-art algorithms all adopt the branch-reduction-and-bound (BRB) framework where a key step, called reduction-and-bound (RB), is used for narrowing down the search space. A common practice of RB in existing works is SeqRB, which sequentially conducts the reduction process followed by the bounding process once at a branch. However, these algorithms suffer from the efficiency issues. In this paper, we propose a new alternated reduction-and-bound method AltRB for conducting RB. AltRB first partitions a branch into two parts and then alternatively and iteratively conducts the reduction process and the bounding process at each part of a branch. With newly-designed reduction rules and bounding methods, AltRB is superior to SeqRB in effectively narrowing down the search space in both theory and practice. Further, to boost the performance of BRB algorithms, we develop efficient and effective pre-processing methods which reduce the size of the input graph and heuristically compute a large $k$-plex as the lower bound. We conduct extensive experiments on 664 real and synthetic graphs. The experimental results show that our proposed algorithm kPEX with AltRB and novel pre-processing techniques runs up to two orders of magnitude faster and solves more instances than state-of-the-art algorithms."}, "https://arxiv.org/abs/2406.00864": {"title": "Optimal Control of General Impulsive VS-EIAR Epidemic Models with Application to Covid-19", "link": "https://arxiv.org/abs/2406.00864", "description": "arXiv:2406.00864v1 Announce Type: cross \nAbstract: In this work, we are interested in a VS-EIAR epidemiological model considering vaccinated individuals ${V_i: i=1,\\ldots,n}$, where $n\\in \\mathbb{N}^{*}$. The dynamic of the VS-EIAR model involves several ordinary differential equations that describe the changes in the vaccinated, susceptible, infected, exposed, asymptomatic, and deceased population groups. Our aim is to reduce the number of susceptible, exposed, infected, and asymptomatic individuals by administering vaccination doses to susceptible individuals and treatment to infected population. To achieve this, we utilize optimal control theory to regulate the dynamic of our considered epidemic model within a terminal optimal time $\\tau^{*}$. Pontryagin's maximum principle (PMP) will be employed to establish the existence of an optimal control time $(v^{*}(t), u^{*}(t))$. We also incorporate an impulsive VS-EIAR epidemic model, with special attention given to immigration or the travel of certain population groups. Finally, we provide a numerical simulation to demonstrate the practical implementation of the theoretical findings."}, "https://arxiv.org/abs/2406.00987": {"title": "Enhancing Fairness in Unsupervised Graph Anomaly Detection through Disentanglement", "link": "https://arxiv.org/abs/2406.00987", "description": "arXiv:2406.00987v1 Announce Type: cross \nAbstract: Graph anomaly detection (GAD) is increasingly crucial in various applications, ranging from financial fraud detection to fake news detection. However, current GAD methods largely overlook the fairness problem, which might result in discriminatory decisions skewed toward certain demographic groups defined on sensitive attributes (e.g., gender, religion, ethnicity, etc.). This greatly limits the applicability of these methods in real-world scenarios in light of societal and ethical restrictions. To address this critical gap, we make the first attempt to integrate fairness with utility in GAD decision-making. Specifically, we devise a novel DisEntangle-based FairnEss-aware aNomaly Detection framework on the attributed graph, named DEFEND. DEFEND first introduces disentanglement in GNNs to capture informative yet sensitive-irrelevant node representations, effectively reducing societal bias inherent in graph representation learning. Besides, to alleviate discriminatory bias in evaluating anomalous nodes, DEFEND adopts a reconstruction-based anomaly detection, which concentrates solely on node attributes without incorporating any graph structure. Additionally, given the inherent association between input and sensitive attributes, DEFEND constrains the correlation between the reconstruction error and the predicted sensitive attributes. Our empirical evaluations on real-world datasets reveal that DEFEND performs effectively in GAD and significantly enhances fairness compared to state-of-the-art baselines. To foster reproducibility, our code is available at https://github.com/AhaChang/DEFEND."}, "https://arxiv.org/abs/2406.01101": {"title": "Fast and Robust Flocking of Protesters on Street Networks", "link": "https://arxiv.org/abs/2406.01101", "description": "arXiv:2406.01101v1 Announce Type: cross \nAbstract: We propose a simple model of protesters scattered throughout a city who want to gather into large and mobile groups. This model relies on random walkers on a street network that follow tactics built from a set of basic rules. Our goal is to identify the most important rules for fast and robust flocking of walkers. We explore a wide set of tactics and show the central importance of a specific rule based on alignment. Other rules alone perform poorly, but our experiments show that combining alignment with them enhances flocking, and that obtained groups are then remarkably robust."}, "https://arxiv.org/abs/2010.12303": {"title": "Random hyperbolic graphs in $d+1$ dimensions", "link": "https://arxiv.org/abs/2010.12303", "description": "arXiv:2010.12303v4 Announce Type: replace \nAbstract: We consider random hyperbolic graphs in hyperbolic spaces of any dimension $d+1\\geq 2$. We present a rescaling of model parameters that casts the random hyperbolic graph model of any dimension to a unified mathematical framework, leaving the degree distribution invariant with respect to the dimension. Unlike the degree distribution, clustering does depend on the dimension, decreasing to 0 at $d \\rightarrow \\infty$. We analyze all of the other limiting regimes of the model, and we release a software package that generates random hyperbolic graphs and their limits in hyperbolic spaces of any dimension."}, "https://arxiv.org/abs/2206.01393": {"title": "Simulation of Crowd Egress with Environmental Stressors", "link": "https://arxiv.org/abs/2206.01393", "description": "arXiv:2206.01393v5 Announce Type: replace \nAbstract: This article introduces a modeling framework to characterize evacuee response to environmental stimuli during emergency egress. The model is developed in consistency with stress theory, which explains how an organism reacts to environmental stressors. We integrate the theory into the well-known social-force model, and develop a framework to simulate crowd evacuation behavior in multi-compartment buildings. Our method serves as a theoretical basis to study crowd movement at bottlenecks, and simulate their herding and way-finding behavior in normal and hazardous conditions. The pre-movement behavior is also briefly investigated by using opinion dynamics with social group model. The algorithms have been partly tested in FDS+EVAC as well as our simulation platform crowdEgress."}, "https://arxiv.org/abs/2212.11051": {"title": "Correlation distances in social networks", "link": "https://arxiv.org/abs/2212.11051", "description": "arXiv:2212.11051v2 Announce Type: replace \nAbstract: In this work we explore degree assortativity in complex networks, and extend its usual definition beyond that of nearest neighbours. We apply this definition to model networks, and describe a rewiring algorithm that induces assortativity. We compare these results to real networks. Social networks in particular tend to be assortatively mixed by degree in contrast to many other types of complex networks. However, we show here that these positive correlations diminish after one step and in most of the empirical networks analysed. Properties besides degree support this, such as the number of papers in scientific coauthorship networks, with no correlations beyond nearest neighbours. Beyond next-nearest neighbours we also observe a diasassortative tendency for nodes three steps away indicating that nodes at that distance are more likely different than similar."}, "https://arxiv.org/abs/2305.09601": {"title": "Operationalizing content moderation \"accuracy\" in the Digital Services Act", "link": "https://arxiv.org/abs/2305.09601", "description": "arXiv:2305.09601v4 Announce Type: replace \nAbstract: The Digital Services Act, recently adopted by the EU, requires social media platforms to report the \"accuracy\" of their automated content moderation systems. The colloquial term is vague, or open-textured -- the literal accuracy (number of correct predictions divided by the total) is not suitable for problems with large class imbalance, and the ground truth and dataset to measure accuracy against is unspecified. Without further specification, the regulatory requirement allows for deficient reporting. In this interdisciplinary work, we operationalize \"accuracy\" reporting by refining legal concepts and relating them to technical implementation. We start by elucidating the legislative purpose of the Act to legally justify an interpretation of \"accuracy\" as precision and recall. These metrics remain informative in class imbalanced settings, and reflect the proportional balancing of Fundamental Rights of the EU Charter. We then focus on the estimation of recall, as its naive estimation can incur extremely high annotation costs and disproportionately interfere with the platform's right to conduct business. Through a simulation study, we show that recall can be efficiently estimated using stratified sampling with trained classifiers, and provide concrete recommendations for its application. Finally, we present a case study of recall reporting for a subset of Reddit under the Act. Based on the language in the Act, we identify a number of ways recall could be reported due to underspecification. We report on one possibility using our improved estimator, and discuss the implications and areas for further legal clarification."}, "https://arxiv.org/abs/2402.10659": {"title": "Network Formation and Dynamics Among Multi-LLMs", "link": "https://arxiv.org/abs/2402.10659", "description": "arXiv:2402.10659v3 Announce Type: replace \nAbstract: Social networks shape opinions, behaviors, and information dissemination in human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social interactions and networks becomes essential. Our study analyzes LLMs' network formation behavior to examine whether the dynamics of multiple LLMs are similar to or different from human social dynamics. We observe that LLMs exhibit key social network principles, including preferential attachment, triadic closure, homophily, community structure, and the small-world phenomenon, when asked about their preferences in network formation. We also investigate LLMs' decision-making based on real-world networks, revealing that triadic closure and homophily have a stronger influence than preferential attachment and that LLMs perform well in network formation predictions. Overall, our study opens up new possibilities for using LLMs in network science research and helps develop socially aware LLMs by shedding light on their social interaction behaviors and exploring their impacts on social dynamics."}, "https://arxiv.org/abs/2403.08372": {"title": "Negative Impact of Online Political Incivility on Willingness to See Political Comments", "link": "https://arxiv.org/abs/2403.08372", "description": "arXiv:2403.08372v2 Announce Type: replace \nAbstract: Recently, there has been significant attention on online political incivility. While previous research suggests that uncivil political comments lead people to be less willing to see more comments on the same issue, two critical questions have received limited exploration: (1) Are people exposed to uncivil political comments less willing to see other comments from the person who posted the uncivil comment?; (2) Are people exposed to uncivil political comments less willing to see comments from people who have different thoughts than them? To address these questions, the present study conducted a preregistered online survey experiment targeting Japanese citizens, focusing on the pro- vs anti-Kishida cabinet conflict in Japan. The results show that the participants were less willing to see other comments by the person who posted the comment when the comment was uncivil than when it was civil. In addition, the anti-Kishida participants were less willing to see political opinions posted online by people who have different thoughts than them when the comment was uncivil than when it was civil, while the participants in the other subgroups did not show a similar tendency. These findings suggest that uncivil expressions in online political communication might prompt people to avoid reading opinions from those who have different thoughts than them, which might promote political echo chambers."}, "https://arxiv.org/abs/2004.03925": {"title": "Word frequency and sentiment analysis of twitter messages during Coronavirus pandemic", "link": "https://arxiv.org/abs/2004.03925", "description": "arXiv:2004.03925v2 Announce Type: replace-cross \nAbstract: The COVID-19 epidemic has had a great impact on social media conversation, especially on sites like Twitter, which has emerged as a hub for public reaction and information sharing. This paper deals by analyzing a vast dataset of Twitter messages related to this disease, starting from January 2020. Two approaches were used: a statistical analysis of word frequencies and a sentiment analysis to gauge user attitudes. Word frequencies are modeled using unigrams, bigrams, and trigrams, with power law distribution as the fitting model. The validity of the model is confirmed through metrics like Sum of Squared Errors (SSE), R-squared ($R^2$), and Root Mean Squared Error (RMSE). High $R^2$ and low SSE/RMSE values indicate a good fit for the model. Sentiment analysis is conducted to understand the general emotional tone of Twitter users messages. The results reveal that a majority of tweets exhibit neutral sentiment polarity, with only 2.57\\% expressing negative polarity."}, "https://arxiv.org/abs/2204.12095": {"title": "PyGOD: A Python Library for Graph Outlier Detection", "link": "https://arxiv.org/abs/2204.12095", "description": "arXiv:2204.12095v3 Announce Type: replace-cross \nAbstract: PyGOD is an open-source Python library for detecting outliers in graph data. As the first comprehensive library of its kind, PyGOD supports a wide array of leading graph-based methods for outlier detection under an easy-to-use, well-documented API designed for use by both researchers and practitioners. PyGOD provides modularized components of the different detectors implemented so that users can easily customize each detector for their purposes. To ease the construction of detection workflows, PyGOD offers numerous commonly used utility functions. To scale computation to large graphs, PyGOD supports functionalities for deep models such as sampling and mini-batch processing. PyGOD uses best practices in fostering code reliability and maintainability, including unit testing, continuous integration, and code coverage. To facilitate accessibility, PyGOD is released under a BSD 2-Clause license at https://pygod.org and at the Python Package Index (PyPI)."}, "https://arxiv.org/abs/2304.10578": {"title": "Quantifying the Benefit of Artificial Intelligence for Scientific Research", "link": "https://arxiv.org/abs/2304.10578", "description": "arXiv:2304.10578v2 Announce Type: replace-cross \nAbstract: The ongoing artificial intelligence (AI) revolution has the potential to change almost every line of work. As AI capabilities continue to improve in accuracy, robustness, and reach, AI may outperform and even replace human experts across many valuable tasks. Despite enormous effort devoted to understanding the impact of AI on labor and the economy and AI's recent successes in accelerating scientific discovery and progress, we lack a systematic understanding of how AI advances may benefit scientific research across disciplines and fields. Here, drawing from the literature on the future of work and the science of science, we develop a measurement framework to estimate both the direct use of AI and the potential benefit of AI in scientific research, applying natural language processing techniques to 74.6 million publications and 7.1 million patents. We find that the use of AI in research is widespread throughout the sciences, growing especially rapidly since 2015, and papers that use AI exhibit a citation premium, more likely to be highly cited both within and outside their disciplines. Moreover, our analysis reveals considerable potential for AI to benefit numerous scientific fields, yet a notable disconnect exists between AI education and its research applications, highlighting a mismatch between the supply of AI expertise and its demand in research. Lastly, we examine demographic disparities in AI's benefits across scientific disciplines and find that disciplines with a higher proportion of women or Black scientists tend to be associated with less benefit, suggesting that AI's growing impact on research may further exacerbate existing inequalities in science. As the connection between AI and scientific research deepens, our findings may become increasingly important, with implications for the equity and sustainability of the research enterprise."}, "https://arxiv.org/abs/2305.15927": {"title": "Parameter Estimation in DAGs from Incomplete Data via Optimal Transport", "link": "https://arxiv.org/abs/2305.15927", "description": "arXiv:2305.15927v4 Announce Type: replace-cross \nAbstract: Estimating the parameters of a probabilistic directed graphical model from incomplete data is a long-standing challenge. This is because, in the presence of latent variables, both the likelihood function and posterior distribution are intractable without assumptions about structural dependencies or model classes. While existing learning methods are fundamentally based on likelihood maximization, here we offer a new view of the parameter learning problem through the lens of optimal transport. This perspective licenses a general framework that operates on any directed graphs without making unrealistic assumptions on the posterior over the latent variables or resorting to variational approximations. We develop a theoretical framework and support it with extensive empirical evidence demonstrating the versatility and robustness of our approach. Across experiments, we show that not only can our method effectively recover the ground-truth parameters but it also performs comparably or better than competing baselines on downstream applications."}, "https://arxiv.org/abs/2306.00488": {"title": "Reconstructing Graph Diffusion History from a Single Snapshot", "link": "https://arxiv.org/abs/2306.00488", "description": "arXiv:2306.00488v4 Announce Type: replace-cross \nAbstract: Diffusion on graphs is ubiquitous with numerous high-impact applications. In these applications, complete diffusion histories play an essential role in terms of identifying dynamical patterns, reflecting on precaution actions, and forecasting intervention effects. Despite their importance, complete diffusion histories are rarely available and are highly challenging to reconstruct due to ill-posedness, explosive search space, and scarcity of training data. To date, few methods exist for diffusion history reconstruction. They are exclusively based on the maximum likelihood estimation (MLE) formulation and require to know true diffusion parameters. In this paper, we study an even harder problem, namely reconstructing Diffusion history from A single SnapsHot} (DASH), where we seek to reconstruct the history from only the final snapshot without knowing true diffusion parameters. We start with theoretical analyses that reveal a fundamental limitation of the MLE formulation. We prove: (a) estimation error of diffusion parameters is unavoidable due to NP-hardness of diffusion parameter estimation, and (b) the MLE formulation is sensitive to estimation error of diffusion parameters. To overcome the inherent limitation of the MLE formulation, we propose a novel barycenter formulation: finding the barycenter of the posterior distribution of histories, which is provably stable against the estimation error of diffusion parameters. We further develop an effective solver named DIffusion hiTting Times with Optimal proposal (DITTO) by reducing the problem to estimating posterior expected hitting times via the Metropolis--Hastings Markov chain Monte Carlo method (M--H MCMC) and employing an unsupervised graph neural network to learn an optimal proposal to accelerate the convergence of M--H MCMC. We conduct extensive experiments to demonstrate the efficacy of the proposed method."}, "https://arxiv.org/abs/2311.06835": {"title": "Open-Set Graph Anomaly Detection via Normal Structure Regularisation", "link": "https://arxiv.org/abs/2311.06835", "description": "arXiv:2311.06835v3 Announce Type: replace-cross \nAbstract: This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to train a detection model using a small number of normal and anomaly nodes (referred to as seen anomalies) to detect both seen anomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the training anomalies). The availability of those labelled training data provides crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current methods tend to over-emphasise fitting the seen anomalies, leading to a weak generalisation ability to detect the unseen anomalies. Further, they were introduced to handle Euclidean data, failing to effectively capture important information on graph structure and node attributes for GAD. In this work, we propose a novel open-set GAD approach, namely Normal Structure Regularisation (NSReg) to achieve generalised detection ability to unseen anomalies, while maintaining its effectiveness on detecting seen anomalies. The key idea in NSReg is to introduce a regularisation term that enforces the learning of compact, semantically-rich representations of normal nodes based on their structural relations to other nodes. When being optimised with supervised anomaly detection losses, the regularisation term helps incorporate strong normality into the modelling, and thus, it effectively avoids the overfitting the seen anomalies solely. In doing so, it helps learn better normality decision boundary, reducing the errors of detecting unseen anomalies as normal. Extensive empirical results on seven real-world datasets show the superiority of NSReg for open-set GAD."}, "https://arxiv.org/abs/2403.02630": {"title": "FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal Decoupling", "link": "https://arxiv.org/abs/2403.02630", "description": "arXiv:2403.02630v3 Announce Type: replace-cross \nAbstract: In recent years, Cross-Domain Recommendation (CDR) has drawn significant attention, which utilizes user data from multiple domains to enhance the recommendation performance. However, current CDR methods require sharing user data across domains, thereby violating the General Data Protection Regulation (GDPR). Consequently, numerous approaches have been proposed for Federated Cross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity across different domains inevitably influences the overall performance of federated learning. In this study, we propose FedHCDR, a novel Federated Cross-Domain Recommendation framework with Hypergraph signal decoupling. Specifically, to address the data heterogeneity across domains, we introduce an approach called hypergraph signal decoupling (HSD) to decouple the user features into domain-exclusive and domain-shared features. The approach employs high-pass and low-pass hypergraph filters to decouple domain-exclusive and domain-shared user representations, which are trained by the local-global bi-directional transfer algorithm. In addition, a hypergraph contrastive learning (HCL) module is devised to enhance the learning of domain-shared user relationship information by perturbing the user hypergraph. Extensive experiments conducted on three real-world scenarios demonstrate that FedHCDR outperforms existing baselines significantly."}, "https://arxiv.org/abs/2403.11332": {"title": "Graph Machine Learning based Doubly Robust Estimator for Network Causal Effects", "link": "https://arxiv.org/abs/2403.11332", "description": "arXiv:2403.11332v2 Announce Type: replace-cross \nAbstract: We address the challenge of inferring causal effects in social network data. This results in challenges due to interference -- where a unit's outcome is affected by neighbors' treatments -- and network-induced confounding factors. While there is extensive literature focusing on estimating causal effects in social network setups, a majority of them make prior assumptions about the form of network-induced confounding mechanisms. Such strong assumptions are rarely likely to hold especially in high-dimensional networks. We propose a novel methodology that combines graph machine learning approaches with the double machine learning framework to enable accurate and efficient estimation of direct and peer effects using a single observational social network. We demonstrate the semiparametric efficiency of our proposed estimator under mild regularity conditions, allowing for consistent uncertainty quantification. We demonstrate that our method is accurate, robust, and scalable via an extensive simulation study. We use our method to investigate the impact of Self-Help Group participation on financial risk tolerance."}, "https://arxiv.org/abs/2404.07797": {"title": "Illicit Promotion on Twitter", "link": "https://arxiv.org/abs/2404.07797", "description": "arXiv:2404.07797v2 Announce Type: replace-cross \nAbstract: In this paper, we present an extensive study of the promotion of illicit goods and services on Twitter, a popular online social network(OSN). This study is made possible through the design and implementation of multiple novel tools for detecting and analyzing illicit promotion activities as well as their underlying campaigns. As the results, we observe that illicit promotion is prevalent on Twitter, along with noticeable existence on other three popular OSNs including Youtube, Facebook, and TikTok. Particularly, 12 million distinct posts of illicit promotion (PIPs) have been observed on the Twitter platform, which are widely distributed in 5 major natural languages and 10 categories of illicit goods and services, e.g., drugs, data leakage, gambling, and weapon sales. What are also observed are 580K Twitter accounts publishing PIPs as well as 37K distinct instant messaging (IM) accounts that are embedded in PIPs and serve as next hops of communication, which strongly indicates that the campaigns underpinning PIPs are also of a large scale. Also, an arms race between Twitter and illicit promotion operators is also observed. On one hand, Twitter is observed to conduct content moderation in a continuous manner and almost 80% PIPs will get gradually unpublished within six months since posted. However, in the meantime, miscreants adopt various evasion tactics to masquerade their PIPs, which renders more than 90% PIPs keeping hidden from the detection radar for two months or longer."}, "https://arxiv.org/abs/2405.19919": {"title": "Unraveling the Impact of Heterophilic Structures on Graph Positive-Unlabeled Learning", "link": "https://arxiv.org/abs/2405.19919", "description": "arXiv:2405.19919v2 Announce Type: replace-cross \nAbstract: While Positive-Unlabeled (PU) learning is vital in many real-world scenarios, its application to graph data still remains under-explored. We unveil that a critical challenge for PU learning on graph lies on the edge heterophily, which directly violates the irreducibility assumption for Class-Prior Estimation (class prior is essential for building PU learning algorithms) and degenerates the latent label inference on unlabeled nodes during classifier training. In response to this challenge, we introduce a new method, named Graph PU Learning with Label Propagation Loss (GPL). Specifically, GPL considers learning from PU nodes along with an intermediate heterophily reduction, which helps mitigate the negative impact of the heterophilic structure. We formulate this procedure as a bilevel optimization that reduces heterophily in the inner loop and efficiently learns a classifier in the outer loop. Extensive experiments across a variety of datasets have shown that GPL significantly outperforms baseline methods, confirming its effectiveness and superiority."}, "https://arxiv.org/abs/2406.01610": {"title": "Graph structural complexity", "link": "https://arxiv.org/abs/2406.01610", "description": "arXiv:2406.01610v1 Announce Type: new \nAbstract: Introduced the quantitative measure of the structural complexity of the graph (complex network, etc.) based on a procedure similar to the renormalization process, considering the difference between actual and averaged graph structures on different scales. The proposed concept of the graph structural complexity corresponds to qualitative comprehension of the complexity. The proposed measure can be obtained for the weighted graphs also. The structural complexities for various graph types were found - the deterministic infinite and finite size graphs, artificial graphs of different natures including percolation structures, and the time series of cardiac rhythms mapped to complex networks using the parametric visibility graph algorithm. The latter reaches a maximum near the formation of a giant component in the graph or at the percolation threshold for 2D and 3D square lattices when a giant cluster having a fractal structure has emerged. Therefore, the graph structural complexity allows us to detect and study the processes similar to a second-order phase transition in complex networks. A new node centrality index, characterizing the structural complexity of a certain node within the graph structure is introduced also, it can serve as a good auxiliary or generalization to the local clustering coefficient. Such an index provides another new ranking manner for the graph nodes. Being an easily computable measure, the graph structural complexity might help to reveal different features of complex systems and processes of the real world."}, "https://arxiv.org/abs/2406.01612": {"title": "Universal behavior of the Covid-19 tails: Inverse power-law distribution", "link": "https://arxiv.org/abs/2406.01612", "description": "arXiv:2406.01612v1 Announce Type: new \nAbstract: Power-law distribution is one of the most important laws known in nature. Such a special universal behavior is known to occur in very few physical systems. In this work, we analyzed the mortality distribution of the Covid-19 pandemic tails for different countries and continents to discuss the possible universal behavior of the pandemic. Surprisingly, we found that the mortality distribution of Covid-19 follows inverse power-law decays. These universal behaviors for the pandemic are reported in the present work for the first time. Additionally, we showed that mortality tails also decay with time obeying to the inverse power law."}, "https://arxiv.org/abs/2406.01621": {"title": "Susceptibility to Misinformation about COVID-19 Vaccines: A Signal Detection Analysis", "link": "https://arxiv.org/abs/2406.01621", "description": "arXiv:2406.01621v1 Announce Type: new \nAbstract: An analysis drawing on Signal Detection Theory suggests that people may fall for misinformation because they are unable to discern true from false information (truth insensitivity) or because they tend to accept information with a particular slant regardless of whether it is true or false (belief bias). Three preregistered experiments with participants from the United States and the United Kingdom (N = 961) revealed that (i) truth insensitivity in responses to (mis)information about COVID-19 vaccines differed as a function of prior attitudes toward COVID-19 vaccines; (ii) participants exhibited a strong belief bias favoring attitude-congruent information; (iii) truth insensitivity and belief bias jointly predicted acceptance of false information about COVID-19 vaccines, but belief bias was a much stronger predictor; (iv) cognitive elaboration increased truth sensitivity without reducing belief bias; and (v) higher levels of confidence in one's beliefs were associated with greater belief bias. The findings provide insights into why people fall for misinformation, which is essential for individual-level interventions to reduce susceptibility to misinformation."}, "https://arxiv.org/abs/2406.01639": {"title": "Apparent structural changes in contact patterns during COVID-19 were driven by survey design and long-term demographic trends", "link": "https://arxiv.org/abs/2406.01639", "description": "arXiv:2406.01639v1 Announce Type: new \nAbstract: Social contact patterns are key drivers of infectious disease transmission. During the COVID-19 pandemic, differences between pre-COVID and COVID-era contact rates were widely attributed to non-pharmaceutical interventions such as lockdowns. However, the factors that drive changes in the distribution of contacts between different subpopulations remain poorly understood. Here, we present a clustering analysis of 33 contact matrices generated from surveys conducted before and during the COVID-19 pandemic, and analyse key features distinguishing their topological structures. While we expected to identify aspects of pandemic scenarios responsible for these features, our analysis demonstrates that they can be explained by differences in study design and long-term demographic trends. Our results caution against using survey data from different studies in counterfactual analysis of epidemic mitigation strategies. Doing so risks attributing differences stemming from methodological choices or long-term changes to the short-term effects of interventions."}, "https://arxiv.org/abs/2406.01642": {"title": "The Entropy of Knowledge (EoN): Complexity, Uncertainty, and the Quest for Scientific Knowledge", "link": "https://arxiv.org/abs/2406.01642", "description": "arXiv:2406.01642v1 Announce Type: new \nAbstract: This paper explores the concept of \"entropy of knowledge\" (EoN) as a framework for understanding the challenges and complexities of scientific discovery. Drawing from principles in thermodynamics and information theory, I propose that the pursuit of knowledge is characterized by a natural tendency towards disorder, uncertainty, and false conclusions. My central argument is that this entropy of knowledge is not merely an obstacle to be overcome but a fundamental feature of the scientific process, necessary for the exploration of new ideas and the ultimate attainment of truth. The implications of this perspective for the management of scientific inquiry is considered and the same suggests a cyclical approach that balances periods of openness and disorder with phases of consolidation and consensus-building. An attempt is made to situate the EoM model within the broader context of theories of scientific progress, drawing connections to Thomas Kuhn's concept of paradigm shifts and the notion of punctuated equilibrium in the history of science."}, "https://arxiv.org/abs/2406.01911": {"title": "Influence Maximization in Hypergraphs by Stratified Sampling for Efficient Generation of Reverse Reachable Sets", "link": "https://arxiv.org/abs/2406.01911", "description": "arXiv:2406.01911v1 Announce Type: new \nAbstract: Given a hypergraph, influence maximization (IM) is to discover a seed set containing $k$ vertices that have the maximal influence. Although the existing vertex-based IM algorithms perform better than the hyperedge-based algorithms by generating random reverse researchable (RR) sets, they are inefficient because (i) they ignore important structural information associated with hyperedges and thus obtain inferior results, (ii) the frequently-used sampling methods for generating RR sets have low efficiency because of a large number of required samplings along with high sampling variances, and (iii) the vertex-based IM algorithms have large overheads in terms of running time and memory costs. To overcome these shortcomings, this paper proposes a novel approach, called \\emph{HyperIM}. The key idea behind \\emph{HyperIM} is to differentiate structural information of vertices for developing stratified sampling combined with highly-efficient strategies to generate the RR sets. With theoretical guarantees, \\emph{HyperIM} is able to accelerate the influence spread, improve the sampling efficiency, and cut down the expected running time. To further reduce the running time and memory costs, we optimize \\emph{HyperIM} by inferring the bound of the required number of RR sets in conjunction with stratified sampling. Experimental results on real-world hypergraphs show that \\emph{HyperIM} is able to reduce the number of required RR sets and running time by orders of magnitude while increasing the influence spread by up to $2.73X$ on average, compared to the state-of-the-art IM algorithms."}, "https://arxiv.org/abs/2406.02307": {"title": "Traffic Response Functions: Patterns, Propagation and Congestion", "link": "https://arxiv.org/abs/2406.02307", "description": "arXiv:2406.02307v1 Announce Type: new \nAbstract: Using empirical data gathered on motorways in Germany, we follow a new approach by further exploring response functions as a possible tool to study traffic dynamics in motorway networks. We uncover the basic characteristics of responses of flow and density to given signals and the capability of responses to capture the correlation between these fundamental observables. Furthermore, we uncover the potential use of responses to characterize traffic patterns. We are able to demonstrate the differentiation of congestion patterns and the determination of the propagation velocity of moving congestion."}, "https://arxiv.org/abs/2406.01629": {"title": "RecDiff: Diffusion Model for Social Recommendation", "link": "https://arxiv.org/abs/2406.01629", "description": "arXiv:2406.01629v1 Announce Type: cross \nAbstract: Social recommendation has emerged as a powerful approach to enhance personalized recommendations by leveraging the social connections among users, such as following and friend relations observed in online social platforms. The fundamental assumption of social recommendation is that socially-connected users exhibit homophily in their preference patterns. This means that users connected by social ties tend to have similar tastes in user-item activities, such as rating and purchasing. However, this assumption is not always valid due to the presence of irrelevant and false social ties, which can contaminate user embeddings and adversely affect recommendation accuracy. To address this challenge, we propose a novel diffusion-based social denoising framework for recommendation (RecDiff). Our approach utilizes a simple yet effective hidden-space diffusion paradigm to alleivate the noisy effect in the compressed and dense representation space. By performing multi-step noise diffusion and removal, RecDiff possesses a robust ability to identify and eliminate noise from the encoded user representations, even when the noise levels vary. The diffusion module is optimized in a downstream task-aware manner, thereby maximizing its ability to enhance the recommendation process. We conducted extensive experiments to evaluate the efficacy of our framework, and the results demonstrate its superiority in terms of recommendation accuracy, training efficiency, and denoising effectiveness. The source code for the model implementation is publicly available at: https://github.com/HKUDS/RecDiff."}, "https://arxiv.org/abs/2406.01842": {"title": "GraphWeaver: Billion-Scale Cybersecurity Incident Correlation", "link": "https://arxiv.org/abs/2406.01842", "description": "arXiv:2406.01842v1 Announce Type: cross \nAbstract: In the dynamic landscape of large enterprise cybersecurity, accurately and efficiently correlating billions of security alerts into comprehensive incidents is a substantial challenge. Traditional correlation techniques often struggle with maintenance, scaling, and adapting to emerging threats and novel sources of telemetry. We introduce GraphWeaver, an industry-scale framework that shifts the traditional incident correlation process to a data-optimized, geo-distributed graph based approach. GraphWeaver introduces a suite of innovations tailored to handle the complexities of correlating billions of shared evidence alerts across hundreds of thousands of enterprises. Key among these innovations are a geo-distributed database and PySpark analytics engine for large-scale data processing, a minimum spanning tree algorithm to optimize correlation storage, integration of security domain knowledge and threat intelligence, and a human-in-the-loop feedback system to continuously refine key correlation processes and parameters. GraphWeaver is integrated into the Microsoft Defender XDR product and deployed worldwide, handling billions of correlations with a 99% accuracy rate, as confirmed by customer feedback and extensive investigations by security experts. This integration has not only maintained high correlation accuracy but reduces traditional correlation storage requirements by 7.4x. We provide an in-depth overview of the key design and operational features of GraphWeaver, setting a precedent as the first cybersecurity company to openly discuss these critical capabilities at this level of depth."}, "https://arxiv.org/abs/2406.01865": {"title": "The influence of active agent motility on SIRS epidemiological dynamics", "link": "https://arxiv.org/abs/2406.01865", "description": "arXiv:2406.01865v1 Announce Type: cross \nAbstract: Active Brownian disks moving in two dimensions that exchange information about their internal state stochastically are chosen to model epidemic spread in a self-propelled population of agents under the susceptible-infected-recovered-susceptible (SIRS) framework. The state of infection of an agent, or disk, governs its self-propulsion speed; consequently, the activity of the agents in the system varies in time. Two different protocols (one-to-one and one-to-many) are considered for the transmission of disease from the infected to susceptible populations. The effectiveness of the two protocols are practically identical at high values of the infection transmission rate. The one-to-many protocol, however, outperforms the one-to-one protocol at lower values of the infection transmission rate. Salient features of the macroscopic SIRS model are revisited, and compared to predictions from the agent-based model. Lastly, the motility induced phase separation in a population of such agents with a fluctuating fraction of active disks is found to be well-described by theories governing phase separation in a mixture of active and passive particles with a constant fraction of passive disks."}, "https://arxiv.org/abs/2406.01866": {"title": "#EpiTwitter: Public Health Messaging During the COVID-19 Pandemic", "link": "https://arxiv.org/abs/2406.01866", "description": "arXiv:2406.01866v1 Announce Type: cross \nAbstract: Effective communication during health crises is critical, with social media serving as a key platform for public health experts (PHEs) to engage with the public. However, it also amplifies pseudo-experts promoting contrarian views. Despite its importance, the role of emotional and moral language in PHEs' communication during COVID-19 remains under explored. This study examines how PHEs and pseudo-experts communicated on Twitter during the pandemic, focusing on emotional and moral language and their engagement with political elites. Analyzing tweets from 489 PHEs and 356 pseudo-experts from January 2020 to January 2021, alongside public responses, we identified key priorities and differences in messaging strategy. PHEs prioritize masking, healthcare, education, and vaccines, using positive emotional language like optimism. In contrast, pseudo-experts discuss therapeutics and lockdowns more frequently, employing negative emotions like pessimism and disgust. Negative emotional and moral language tends to drive engagement, but positive language from PHEs fosters positivity in public responses. PHEs exhibit liberal partisanship, expressing more positivity towards liberals and negativity towards conservative elites, while pseudo-experts show conservative partisanship. These findings shed light on the polarization of COVID-19 discourse and underscore the importance of strategic use of emotional and moral language by experts to mitigate polarization and enhance public trust."}, "https://arxiv.org/abs/2406.01957": {"title": "Backward bifurcation arising from decline of immunity against emerging infectious diseases", "link": "https://arxiv.org/abs/2406.01957", "description": "arXiv:2406.01957v1 Announce Type: cross \nAbstract: Decline of immunity is a phenomenon characterized by immunocompromised host and plays a crucial role in the epidemiology of emerging infectious diseases (EIDs) such as COVID-19. In this paper, we propose an age-structured model with vaccination and reinfection of immune individuals. We prove that the disease-free equilibrium of the model undergoes backward and forward transcritical bifurcations at the critical value of the basic reproduction number for different values of parameters. We illustrate the results by numerical computations, and also find that the endemic equilibrium exhibits a saddle-node bifurcation on the extended branch of the forward transcritical bifurcation. These results allow us to understand the interplay between the decline of immunity and EIDs, and are able to provide strategies for mitigating the impact of EIDs on global health."}, "https://arxiv.org/abs/2406.01999": {"title": "Random Abstract Cell Complexes", "link": "https://arxiv.org/abs/2406.01999", "description": "arXiv:2406.01999v1 Announce Type: cross \nAbstract: We define a model for random (abstract) cell complexes (CCs), similiar to the well-known Erd\\H{o}s-R\\'enyi model for graphs and its extensions for simplicial complexes. To build a random cell complex, we first draw from an Erd\\H{o}s-R\\'enyi graph, and consecutively augment the graph with cells for each dimension with a specified probability. As the number of possible cells increases combinatorially -- e.g., 2-cells can be represented as cycles, or permutations -- we derive an approximate sampling algorithm for this model limited to two-dimensional abstract cell complexes. Since there is a large variance in the number of simple cycles on graphs drawn from the same configuration of ER, we also provide an efficient method to approximate that number, which is of independent interest. Moreover, it enables us to specify the expected number of 2-cells of each boundary length we want to sample. We provide some initial analysis into the properties of random CCs drawn from this model. We further showcase practical applications for our random CCs as null models, and in the context of (random) liftings of graphs to cell complexes. Both the sampling and cycle count estimation algorithms are available in the package `py-raccoon` on the Python Packaging Index."}, "https://arxiv.org/abs/2406.02362": {"title": "Temporal Graph Rewiring with Expander Graphs", "link": "https://arxiv.org/abs/2406.02362", "description": "arXiv:2406.02362v1 Announce Type: cross \nAbstract: Evolving relations in real-world networks are often modelled by temporal graphs. Graph rewiring techniques have been utilised on Graph Neural Networks (GNNs) to improve expressiveness and increase model performance. In this work, we propose Temporal Graph Rewiring (TGR), the first approach for graph rewiring on temporal graphs. TGR enables communication between temporally distant nodes in a continuous time dynamic graph by utilising expander graph propagation to construct a message passing highway for message passing between distant nodes. Expander graphs are suitable candidates for rewiring as they help overcome the oversquashing problem often observed in GNNs. On the public tgbl-wiki benchmark, we show that TGR improves the performance of a widely used TGN model by a significant margin. Our code repository is accessible at https://anonymous.4open.science/r/TGR-254C."}, "https://arxiv.org/abs/2208.06251": {"title": "Identifying User Profiles Via User Footprints", "link": "https://arxiv.org/abs/2208.06251", "description": "arXiv:2208.06251v2 Announce Type: replace \nAbstract: User identification has been a major field of research in privacy and security topics. Users might utilize multiple Online Social Networks (OSNs) to access a variety of text, videos, and links, and connect to their friends. Identifying user profiles corresponding to multiple virtual activities of users across social networks is significant for the development of related fields, such as network security, user behavior patterns analysis, and user recommendation systems. In addition, predicting personal attributes based on public content is a challenging topic. In this work, we perform an empirical study and proposed a scheme with considerable performance. In this work, we investigate Reddit, a famous social network for questioning and answering. By considering available personal and non-personal attributes, we discuss our main findings based on mapping the different features such as user activities to a special user profile. we collected a dataset with wide distribution consisting of 5000 samples. To map non-personal attributes to personal attributes, a classification approach based on support vector machines (SVM), Random Forests (RF), and deep belief network has been used. Experimental results demonstrate the effectiveness of the proposed methodology and achieved classification accuracy higher than 89%."}, "https://arxiv.org/abs/2303.18051": {"title": "Synergistic Graph Fusion via Encoder Embedding", "link": "https://arxiv.org/abs/2303.18051", "description": "arXiv:2303.18051v3 Announce Type: replace \nAbstract: In this paper, we introduce a method called graph fusion embedding, designed for multi-graph embedding with shared vertex sets. Under the framework of supervised learning, our method exhibits a remarkable and highly desirable synergistic effect: for sufficiently large vertex size, the accuracy of vertex classification consistently benefits from the incorporation of additional graphs. We establish the mathematical foundation for the method, including the asymptotic convergence of the embedding, a sufficient condition for asymptotic optimal classification, and the proof of the synergistic effect for vertex classification. Our comprehensive simulations and real data experiments provide compelling evidence supporting the effectiveness of our proposed method, showcasing the pronounced synergistic effect for multiple graphs from disparate sources."}, "https://arxiv.org/abs/2309.10486": {"title": "Infection patterns in simple and complex contagion processes on networks", "link": "https://arxiv.org/abs/2309.10486", "description": "arXiv:2309.10486v2 Announce Type: replace \nAbstract: Contagion processes, representing the spread of infectious diseases, information, or social behaviors, are often schematized as taking place on networks, which encode for instance the interactions between individuals. The impact of the network structure on spreading process has been widely investigated, but not the reverse question: do different processes unfolding on a given network lead to different infection patterns? How do the infection patterns depend on a model's parameters or on the nature of the contagion processes? Here we address this issue by investigating the infection patterns for a variety of models. In simple contagion processes, where contagion events involve one connection at a time, we find that the infection patterns are extremely robust across models and parameters. In complex contagion models instead, in which multiple interactions are needed for a contagion event, non-trivial dependencies on models parameters emerge, as the infection pattern depends on the interplay between pairwise and group contagions. In models involving threshold mechanisms moreover, slight parameter changes can significantly impact the spreading paths. Our results show that it is possible to study crucial features of a spread from schematized models, and inform us on the variations between spreading patterns in processes of different nature."}, "https://arxiv.org/abs/2309.16717": {"title": "A mobile observer method for the estimation of road traffic using communicating vehicles", "link": "https://arxiv.org/abs/2309.16717", "description": "arXiv:2309.16717v2 Announce Type: replace \nAbstract: Estimation of road traffic is a fundamental problem which has been addressed with a variety of methods. In the present paper, a variant of the mobile observer method is proposed. It is assumed that some vehicles composing the road traffic are communicating vehicles. These communicating vehicles broadcast periodically beacon messages. The proposed method uses only these beacon messages as input data, and needs no additional equipment such as radar or GPS device in order to estimate the road traffic. The model is tested with the bi-directional simulation framework VEINS, which combines a microscopic road traffic simulator and a communication simulator. The preliminary results show the potential of the method and confirm the validity of the approach."}, "https://arxiv.org/abs/2312.16878": {"title": "Voting power in the Council of the European Union: A comprehensive sensitivity analysis", "link": "https://arxiv.org/abs/2312.16878", "description": "arXiv:2312.16878v2 Announce Type: replace \nAbstract: The Council of the European Union (EU) is one of the main decision-making bodies of the EU. Many decisions require a qualified majority: the support of 55% of the member states (currently 15) that represent at least 65% of the total population. We investigate how the power distribution, based on the Shapley-Shubik index, and the proportion of winning coalitions change if these criteria are modified within reasonable bounds. The power of the two countries with about 4% of the total population each is found to be almost flat. The level of decisiveness decreases if the population criterion is above 68\\% or the states criterion is at least 17. The proportion of winning coalitions can be increased from 13.2% to 20.8% (30.1%) such that the maximal relative change in the Shapley-Shubik indices remains below 3.5% (5.5%). Our results are indispensable to evaluate any proposal for reforming the qualified majority voting system."}, "https://arxiv.org/abs/2401.11415": {"title": "A Fast Parallel Approach for Neighborhood-based Link Prediction by Disregarding Large Hubs", "link": "https://arxiv.org/abs/2401.11415", "description": "arXiv:2401.11415v3 Announce Type: replace \nAbstract: Link prediction can help rectify inaccuracies in various graph algorithms, stemming from unaccounted-for or overlooked links within networks. However, many existing works use a baseline approach, which incurs unnecessary computational costs due to its high time complexity. Further, many studies focus on smaller graphs, which can lead to misleading conclusions. Here, we study the prediction of links using neighborhood-based similarity measures on large graphs. In particular, we improve upon the baseline approach (IBase), and propose a heuristic approach that additionally disregards large hubs (DLH), based on the idea that high-degree nodes contribute little similarity among their neighbors. On a server equipped with dual 16-core Intel Xeon Gold 6226R processors, DLH is on average 1019x faster than IBase, especially on web graphs and social networks, while maintaining similar prediction accuracy. Notably, DLH achieves a link prediction rate of 38.1M edges/s and improves performance by 1.6x for every doubling of threads."}, "https://arxiv.org/abs/2302.00890": {"title": "Neural Common Neighbor with Completion for Link Prediction", "link": "https://arxiv.org/abs/2302.00890", "description": "arXiv:2302.00890v4 Announce Type: replace-cross \nAbstract: In this work, we propose a novel link prediction model and further boost it by studying graph incompleteness. First, we introduce MPNN-then-SF, an innovative architecture leveraging structural feature (SF) to guide MPNN's representation pooling, with its implementation, namely Neural Common Neighbor (NCN). NCN exhibits superior expressiveness and scalability compared with existing models, which can be classified into two categories: SF-then-MPNN, augmenting MPNN's input with SF, and SF-and-MPNN, decoupling SF and MPNN. Second, we investigate the impact of graph incompleteness -- the phenomenon that some links are unobserved in the input graph -- on SF, like the common neighbor. Through dataset visualization, we observe that incompleteness reduces common neighbors and induces distribution shifts, significantly affecting model performance. To address this issue, we propose to use a link prediction model to complete the common neighbor structure. Combining this method with NCN, we propose Neural Common Neighbor with Completion (NCNC). NCN and NCNC outperform recent strong baselines by large margins, and NCNC further surpasses state-of-the-art models in standard link prediction benchmarks. Our code is available at https://github.com/GraphPKU/NeuralCommonNeighbor."}, "https://arxiv.org/abs/2304.05223": {"title": "Inhomogeneous graph trend filtering via a l2,0 cardinality penalty", "link": "https://arxiv.org/abs/2304.05223", "description": "arXiv:2304.05223v3 Announce Type: replace-cross \nAbstract: We study estimation of piecewise smooth signals over a graph. We propose a $\\ell_{2,0}$-norm penalized Graph Trend Filtering (GTF) model to estimate piecewise smooth graph signals that exhibit inhomogeneous levels of smoothness across the nodes. We prove that the proposed GTF model is simultaneously a k-means clustering on the signal over the nodes and a minimum graph cut on the edges of the graph, where the clustering and the cut share the same assignment matrix. We propose two methods to solve the proposed GTF model: a spectral decomposition method and a method based on simulated annealing. In the experiment on synthetic and real-world datasets, we show that the proposed GTF model has a better performances compared with existing approaches on the tasks of denoising, support recovery and semi-supervised classification. We also show that the proposed GTF model can be solved more efficiently than existing models for the dataset with a large edge set."}, "https://arxiv.org/abs/2305.16102": {"title": "Demystifying Oversmoothing in Attention-Based Graph Neural Networks", "link": "https://arxiv.org/abs/2305.16102", "description": "arXiv:2305.16102v4 Announce Type: replace-cross \nAbstract: Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models, including random walk GCNs, Graph Attention Networks (GATs) and (graph) transformers. In particular, our analysis accounts for asymmetric, state-dependent and time-varying aggregation operators and a wide range of common nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU."}, "https://arxiv.org/abs/2404.14928": {"title": "Graph Machine Learning in the Era of Large Language Models (LLMs)", "link": "https://arxiv.org/abs/2404.14928", "description": "arXiv:2404.14928v2 Announce Type: replace-cross \nAbstract: Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graph structures. Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications such as computer vision and recommender systems. This remarkable success has also attracted interest in applying LLMs to the graph domain. Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML's generalization, transferability, and few-shot learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations such as hallucinations and the lack of explainability. Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners. Therefore, in this survey, we first review the recent developments in Graph ML. We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph heterogeneity and out-of-distribution (OOD) generalization. Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre-training and inference. Furthermore, we investigate various applications and discuss the potential future directions in this promising field."}, "https://arxiv.org/abs/2406.02801": {"title": "SenTopX: Benchmark for User Sentiment on Various Topics", "link": "https://arxiv.org/abs/2406.02801", "description": "arXiv:2406.02801v1 Announce Type: new \nAbstract: Toxic sentiment analysis on Twitter (X) often focuses on specific topics and events such as politics and elections. Datasets of toxic users in such research are typically gathered through lexicon-based techniques, providing only a cross-sectional view. his approach has a tight confine for studying toxic user behavior and effective platform moderation. To identify users consistently spreading toxicity, a longitudinal analysis of their tweets is essential. However, such datasets currently do not exist.\n  This study addresses this gap by collecting a longitudinal dataset from 143K Twitter users, covering the period from 2007 to 2021, amounting to a total of 293 million tweets. Using topic modeling, we extract all topics discussed by each user and categorize users into eight groups based on the predominant topic in their timelines. We then analyze the sentiments of each group using 16 toxic scores. Our research demonstrates that examining users longitudinally reveals a distinct perspective on their comprehensive personality traits and their overall impact on the platform. Our comprehensive dataset is accessible to researchers for additional analysis."}, "https://arxiv.org/abs/2406.03139": {"title": "Patterns of co-occurrent skills in UK job adverts", "link": "https://arxiv.org/abs/2406.03139", "description": "arXiv:2406.03139v1 Announce Type: new \nAbstract: A job usually involves the application of several complementary or synergistic skills to perform its required tasks. Such relationships are implicitly recognised by employers in the skills they demand when recruiting new employees. Here we construct a skills network based on their co-occurrence in a national level data set of 65 million job postings from the UK spanning 2016 to 2022. We then apply multiscale graph-based community detection to obtain data-driven skill clusters at different levels of resolution that reveal a modular structure across scales. Skill clusters display diverse levels of demand and occupy varying roles within the skills network: some have broad reach across the network (high closeness centrality) while others have higher levels of within-cluster containment, yet with high interconnection across clusters and no skill silos. The skill clusters also display varying levels of semantic similarity, highlighting the difference between co-occurrence in adverts and intrinsic thematic consistency. Clear geographic variation is evident in the demand for each skill cluster across the UK, broadly reflecting the industrial characteristics of each region, e.g., London appears as an outlier as an international hub for finance, education and business. Comparison of data from 2016 and 2022 reveals employers are demanding a broader range of skills over time, with more adverts featuring skills spanning different clusters. We also show that our data-driven clusters differ from expert-authored categorisations of skills, indicating that important relationships between skills are not captured by expert assessment alone."}, "https://arxiv.org/abs/2406.03340": {"title": "Analyzing and Estimating Support for U", "link": "https://arxiv.org/abs/2406.03340", "description": "arXiv:2406.03340v1 Announce Type: new \nAbstract: Polls posted on social media have emerged in recent years as an important tool for estimating public opinion, e.g., to gauge public support for business decisions and political candidates in national elections. Here, we examine nearly two thousand Twitter polls gauging support for U.S. presidential candidates during the 2016 and 2020 election campaigns. First, we describe the rapidly emerging prevalence of social polls. Second, we characterize social polls in terms of their heterogeneity and response options. Third, leveraging machine learning models for user attribute inference, we describe the demographics, political leanings, and other characteristics of the users who author and interact with social polls. Finally, we study the relationship between social poll results, their attributes, and the characteristics of users interacting with them. Our findings reveal that Twitter polls are biased in various ways, starting from the position of the presidential candidates among the poll options to biases in demographic attributes and poll results. The 2016 and 2020 polls were predominantly crafted by older males and manifested a pronounced bias favoring candidate Donald Trump, in contrast to traditional surveys, which favored Democratic candidates. We further identify and explore the potential reasons for such biases in social polling and discuss their potential repercussions. Finally, we show that biases in social media polls can be corrected via regression and poststratification. The errors of the resulting election estimates can be as low as 1%-2%, suggesting that social media polls can become a promising source of information about public opinion."}, "https://arxiv.org/abs/2406.03354": {"title": "Can Social Media Platforms Transcend Political Labels? An Analysis of Neutral Conservations on Truth Social", "link": "https://arxiv.org/abs/2406.03354", "description": "arXiv:2406.03354v1 Announce Type: new \nAbstract: There is a prevailing perception that content on a social media platform generally have the same political leaning. These platforms are often viewed as ideologically congruent entities, reflecting the majority opinion of their users; a prime example of this is Truth Social. While this perception may exist, it is essential to verify the platform's credibility, acknowledging that such platforms contain meaningful insights with neutral stances. To this end, we examine the dissemination of Wikipedia links on the alt-right platform, Truth Social. Wikipedia is recognized for enforcing content neutrality and serves as a unique lens to analyze the objectivity of user-generated content on Truth Social. By scrutinizing Truths with and without Wikipedia links, identifying toxicity trends & recognizing coordinated networks, we observe a lower level of engagement and a tendency for Truths shared on Truth Social to cover more neutral topics when it includes Wikipedia links (Wiki Truths). Given the significantly different engagement and nature of content shared of Wiki Truths against Non-Wiki Truths, we emphasize that we should not generalize the techno-political affiliation of a social media platform, but rather should investigate the content closely."}, "https://arxiv.org/abs/2406.03443": {"title": "Investigating the Relationship Between User Specialization and Toxicity on Reddit: A Sentiment Analysis Approach", "link": "https://arxiv.org/abs/2406.03443", "description": "arXiv:2406.03443v1 Announce Type: new \nAbstract: Online platforms host a diverse user base, which can be broadly categorized into \"specialist users\" with focused interests and \"generalist users\" who engage in a wide range of topics. This study explores the behavioral differences between these two user types on the popular platform Reddit, focusing on the level of toxicity in their posts and the associated sentiment scores across 24 emotional categories and a neutral state. By employing community embeddings to represent users in a high-dimensional space, we measure activity diversity using the GS score. We analyze a dataset of 16,291,992 posts from 4,926,237 users spanning the period from 2019 to 2021, assessing the degree of toxicity and sentiment scores for each post. Our findings indicate that specialist users exhibit higher levels of toxic behavior compared to generalist users. Furthermore, specialist users demonstrate elevated scores for annoyance, sadness, and fear, while generalist users show higher scores for curiosity, admiration, and love. These insights contribute to a better understanding of user behavior on online platforms and can inform strategies for fostering healthier online communities."}, "https://arxiv.org/abs/2406.02794": {"title": "PriME: Privacy-aware Membership profile Estimation in networks", "link": "https://arxiv.org/abs/2406.02794", "description": "arXiv:2406.02794v1 Announce Type: cross \nAbstract: This paper presents a novel approach to estimating community membership probabilities for network vertices generated by the Degree Corrected Mixed Membership Stochastic Block Model while preserving individual edge privacy. Operating within the $\\varepsilon$-edge local differential privacy framework, we introduce an optimal private algorithm based on a symmetric edge flip mechanism and spectral clustering for accurate estimation of vertex community memberships. We conduct a comprehensive analysis of the estimation risk and establish the optimality of our procedure by providing matching lower bounds to the minimax risk under privacy constraints. To validate our approach, we demonstrate its performance through numerical simulations and its practical application to real-world data. This work represents a significant step forward in balancing accurate community membership estimation with stringent privacy preservation in network data analysis."}, "https://arxiv.org/abs/2406.03245": {"title": "Reconfiguring Participatory Design to Resist AI Realism", "link": "https://arxiv.org/abs/2406.03245", "description": "arXiv:2406.03245v1 Announce Type: cross \nAbstract: The growing trend of artificial intelligence (AI) as a solution to social and technical problems reinforces AI Realism -- the belief that AI is an inevitable and natural order. In response, this paper argues that participatory design (PD), with its focus on democratic values and processes, can play a role in questioning and resisting AI Realism. I examine three concerning aspects of AI Realism: the facade of democratization that lacks true empowerment, demands for human adaptability in contrast to AI systems' inflexibility, and the obfuscation of essential human labor enabling the AI system. I propose resisting AI Realism by reconfiguring PD to continue engaging with value-centered visions, increasing its exploration of non-AI alternatives, and making the essential human labor underpinning AI systems visible. I position PD as a means to generate friction against AI Realism and open space for alternative futures centered on human needs and values."}, "https://arxiv.org/abs/2406.03390": {"title": "Author, Content or Sharers? Estimating Spread Dynamics with Bayesian Mixture Hawkes", "link": "https://arxiv.org/abs/2406.03390", "description": "arXiv:2406.03390v1 Announce Type: cross \nAbstract: The spread of content on social media is shaped by intertwining factors on three levels: the source, the content itself, and the pathways of content spread. At the lowest level, the popularity of the sharing user determines its eventual reach. However, higher-level factors such as the nature of the online item and the credibility of its source also play crucial roles in determining how widely and rapidly the online item spreads. In this work, we propose the Bayesian Mixture Hawkes (BMH) model to jointly learn the influence of source, content and spread. We formulate the BMH model as a hierarchical mixture model of separable Hawkes processes, accommodating different classes of Hawkes dynamics and the influence of feature sets on these classes. We test the BMH model on two learning tasks, cold-start popularity prediction and temporal profile generalization performance, applying to two real-world retweet cascade datasets referencing articles from controversial and traditional media publishers. The BMH model outperforms the state-of-the-art models and predictive baselines on both datasets and utilizes cascade- and item-level information better than the alternatives. Lastly, we perform a counter-factual analysis where we apply the trained publisher-level BMH models to a set of article headlines and show that effectiveness of headline writing style (neutral, clickbait, inflammatory) varies across publishers. The BMH model unveils differences in style effectiveness between controversial and reputable publishers, where we find clickbait to be notably more effective for reputable publishers as opposed to controversial ones, which links to the latter's overuse of clickbait."}, "https://arxiv.org/abs/2208.06370": {"title": "Mitigating an epidemic on a geographic network using vaccination", "link": "https://arxiv.org/abs/2208.06370", "description": "arXiv:2208.06370v3 Announce Type: replace \nAbstract: We consider a mathematical model describing the propagation of an epidemic on a geographical network. The size of the outbreak is governed by the initial growth rate of the disease given by the maximal eigenvalue of the epidemic matrix formed by the susceptibles and the graph Laplacian representing the mobility. We use matrix perturbation theory to analyze the epidemic matrix and define a vaccination strategy, assuming the vaccination reduces the susceptibles. When mobility and local disease dynamics have similar time scales, it is most efficient to vaccinate the whole network because the disease grows uniformly. However, if only a few vertices can be vaccinated then which ones do we choose? We answer this question, and show that it is most efficient to vaccinate along an eigenvector corresponding to the largest eigenvalue of the Laplacian. We illustrate these general results on a 7 vertex graph and a realistic example of the french rail network. When mobility is slower than local disease dynamics, the epidemic grows on the vertex with largest susceptibles. The epidemic growth rate is more reduced when vaccinating a larger degree vertex; it also depends on the neighboring vertices. This study and its conclusions provides guidelines for the planning of vaccination on a network at the onset of an epidemic."}, "https://arxiv.org/abs/2312.01565": {"title": "Finding mixed memberships in categorical data", "link": "https://arxiv.org/abs/2312.01565", "description": "arXiv:2312.01565v2 Announce Type: replace \nAbstract: Latent class analysis, a fundamental problem in categorical data analysis, often encounters overlapping latent classes that introduce further challenges. This paper presents a solution to this problem by focusing on finding latent mixed memberships of subjects in categorical data with polytomous responses. We employ the Grade of Membership (GoM) model, which assigns each subject a membership score in each latent class. To address this, we propose two efficient spectral algorithms for estimating these mixed memberships and other GoM parameters. Our algorithms are based on the singular value decomposition of a regularized Laplacian matrix. We establish their convergence rates under a mild condition on data sparsity. Additionally, we introduce a metric to evaluate the quality of estimated mixed memberships for real-world categorical data and determine the optimal number of latent classes based on this metric. Finally, we demonstrate the practicality of our methods through experiments on both computer-generated and real-world categorical datasets."}, "https://arxiv.org/abs/2402.13392": {"title": "An SEIR network epidemic model with manual and digital contact tracing allowing delays", "link": "https://arxiv.org/abs/2402.13392", "description": "arXiv:2402.13392v4 Announce Type: replace \nAbstract: We consider an SEIR epidemic model on a network also allowing random contacts, where recovered individuals could either recover naturally or be diagnosed. Upon diagnosis, manual contact tracing is triggered such that each infected network contact is reported, tested and isolated with some probability and after a random delay. Additionally, digital tracing (based on a tracing app) is triggered if the diagnosed individual is an app-user, and then all of its app-using infectees are immediately notified and isolated. The early phase of the epidemic with manual and/or digital tracing is approximated by different multi-type branching processes, and three respective reproduction numbers are derived. The effectiveness of both contact tracing mechanisms is numerically quantified through the reduction of the reproduction number. This shows that app-using fraction plays an essential role in the overall effectiveness of contact tracing. The relative effectiveness of manual tracing compared to digital tracing increases if: more of the transmission occurs on the network, when the tracing delay is shortened, and when the network degree distribution is heavy-tailed. For realistic values, the combined tracing case can reduce $R_0$ by $20-30\\%$, so other preventive measures are needed to reduce the reproduction number down to $1.2-1.4$ for contact tracing to make it successful in avoiding big outbreaks."}, "https://arxiv.org/abs/2404.12451": {"title": "Assessing the Risk of Proliferation via Fissile Material Breeding in ARC-class Fusion Power Plants", "link": "https://arxiv.org/abs/2404.12451", "description": "arXiv:2404.12451v2 Announce Type: replace \nAbstract: Construction of a nuclear weapon requires access to kilogram-scale quantities of fissile material, which can be bred from fertile material like U-238 and Th-232 via neutron capture. Future fusion power plants, with total neutron source rates in excess of $10^{20}$ n/s, could breed weapons-relevant quantities of fissile material on short timescales, posing a breakout proliferation risk. The ARC-class fusion reactor design is characterized by demountable high temperature superconducting magnets, a FLiBe liquid immersion blanket, and a relatively small size ($\\sim$ 4 m major radius, $\\sim$ 1 m minor radius). We use the open-source Monte Carlo neutronics code OpenMC to perform self-consistent time-dependent simulations of a representative ARC-class blanket to assess the feasibility of a fissile breeding breakout scenario. We find that a significant quantity of fissile material can be bred in less than six months of full power operation for initial fertile inventories ranging from 5 to 50 metric tons, representing a non-negligible proliferation risk. We further study the feasibility of this scenario by examining other consequences of fissile breeding such as reduced tritium breeding ratio, extra heat from fission and decay heat, isotopic purity of bred material, and self-protection time of irradiated blanket material. We also examine the impact of Li-6 enrichment on fissile breeding and find that it substantially reduces breeding rate, motivating its use as a proliferation resistance tool."}, "https://arxiv.org/abs/2305.15745": {"title": "Robust Ante-hoc Graph Explainer using Bilevel Optimization", "link": "https://arxiv.org/abs/2305.15745", "description": "arXiv:2305.15745v2 Announce Type: replace-cross \nAbstract: Explaining the decisions made by machine learning models for high-stakes applications is critical for increasing transparency and guiding improvements to these decisions. This is particularly true in the case of models for graphs, where decisions often depend on complex patterns combining rich structural and attribute data. While recent work has focused on designing so-called post-hoc explainers, the broader question of what constitutes a good explanation remains open. One intuitive property is that explanations should be sufficiently informative to reproduce the predictions given the data. In other words, a good explainer can be repurposed as a predictor. Post-hoc explainers do not achieve this goal as their explanations are highly dependent on fixed model parameters (e.g., learned GNN weights). To address this challenge, we propose RAGE (Robust Ante-hoc Graph Explainer), a novel and flexible ante-hoc explainer designed to discover explanations for graph neural networks using bilevel optimization, with a focus on the chemical domain. RAGE can effectively identify molecular substructures that contain the full information needed for prediction while enabling users to rank these explanations in terms of relevance. Our experiments on various molecular classification tasks show that RAGE explanations are better than existing post-hoc and ante-hoc approaches."}, "https://arxiv.org/abs/2309.08631": {"title": "Large Language Models Can Infer Psychological Dispositions of Social Media Users", "link": "https://arxiv.org/abs/2309.08631", "description": "arXiv:2309.08631v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) demonstrate increasingly human-like abilities across a wide variety of tasks. In this paper, we investigate whether LLMs like ChatGPT can accurately infer the psychological dispositions of social media users and whether their ability to do so varies across socio-demographic groups. Specifically, we test whether GPT-3.5 and GPT-4 can derive the Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario. Our results show an average correlation of r = .29 (range = [.22, .33]) between LLM-inferred and self-reported trait scores - a level of accuracy that is similar to that of supervised machine learning models specifically trained to infer personality. Our findings also highlight heterogeneity in the accuracy of personality inferences across different age groups and gender categories: predictions were found to be more accurate for women and younger individuals on several traits, suggesting a potential bias stemming from the underlying training data or differences in online self-expression. The ability of LLMs to infer psychological dispositions from user-generated text has the potential to democratize access to cheap and scalable psychometric assessments for both researchers and practitioners. On the one hand, this democratization might facilitate large-scale research of high ecological validity and spark innovation in personalized services. On the other hand, it also raises ethical concerns regarding user privacy and self-determination, highlighting the need for stringent ethical frameworks and regulation."}, "https://arxiv.org/abs/2309.17417": {"title": "Networked Inequality: Preferential Attachment Bias in Graph Neural Network Link Prediction", "link": "https://arxiv.org/abs/2309.17417", "description": "arXiv:2309.17417v2 Announce Type: replace-cross \nAbstract: Graph neural network (GNN) link prediction is increasingly deployed in citation, collaboration, and online social networks to recommend academic literature, collaborators, and friends. While prior research has investigated the dyadic fairness of GNN link prediction, the within-group (e.g., queer women) fairness and \"rich get richer\" dynamics of link prediction remain underexplored. However, these aspects have significant consequences for degree and power imbalances in networks. In this paper, we shed light on how degree bias in networks affects Graph Convolutional Network (GCN) link prediction. In particular, we theoretically uncover that GCNs with a symmetric normalized graph filter have a within-group preferential attachment bias. We validate our theoretical analysis on real-world citation, collaboration, and online social networks. We further bridge GCN's preferential attachment bias with unfairness in link prediction and propose a new within-group fairness metric. This metric quantifies disparities in link prediction scores within social groups, towards combating the amplification of degree and power disparities. Finally, we propose a simple training-time strategy to alleviate within-group unfairness, and we show that it is effective on citation, social, and credit networks."}, "https://arxiv.org/abs/2404.03528": {"title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic Neural Graph Filtering", "link": "https://arxiv.org/abs/2404.03528", "description": "arXiv:2404.03528v3 Announce Type: replace-cross \nAbstract: Knowledge Graphs (KGs) have proven essential in information processing and reasoning applications because they link related entities and give context-rich information, supporting efficient information retrieval and knowledge discovery; presenting information flow in a very effective manner. Despite being widely used globally, Bangla is relatively underrepresented in KGs due to a lack of comprehensive datasets, encoders, NER (named entity recognition) models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient information processing and reasoning applications in the language. Addressing the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework that is able to automatically construct Bengali KGs from any Bangla text. We utilize multilingual LLMs to understand various languages and correlate entities and relations universally. By employing a translation dictionary to identify English equivalents and extracting word features from pre-trained BERT models, we construct the foundational KG. To reduce noise and align word embeddings with our goal, we employ graph-based polynomial filters. Lastly, we implement a GNN-based semantic filter, which elevates contextual understanding and trims unnecessary edges, culminating in the formation of the definitive KG. Empirical findings and case studies demonstrate the universal effectiveness of our model, capable of autonomously constructing semantically enriched KGs from any text."}, "https://arxiv.org/abs/2406.03587": {"title": "Subsuming Complex Networks by Node Walks", "link": "https://arxiv.org/abs/2406.03587", "description": "arXiv:2406.03587v1 Announce Type: new \nAbstract: The concept of node walk in graphs and complex networks has been addressed, consisting of one or more nodes that move into adjacent nodes, henceforth incorporating the respective connections. This type of dynamics is then applied to subsume complex networks. Three types of networks (Erd\\'os- R\\'eny, Barab\\'asi-Albert, as well as a geometric model) are considered, while three node walks heuristics (uniformly random, largest degree, and smallest degree) are taken into account. Several interesting results are obtained and described, including the identification that the subsuming dynamics depend strongly on both the specific topology of the networks as well as the criteria controlling the node walks. The use of node walks as a model for studying the relationship between network topology and dynamics is motivated by this result. In addition, relatively high correlations between the initial node degree and the accumulated strength of the walking node were observed for some combinations of network types and dynamic rules, allowing some of the properties of the subsumption to be roughly predicted from the initial topology around the waking node which has been found, however, not to be enough for full determination of the subsumption dynamics. Another interesting result regards the quite distinct signatures (along the iterations) of walking node strengths obtained for the several considered combinations of network type and subsumption rules."}, "https://arxiv.org/abs/2406.03763": {"title": "The impact of nodes of information dissemination on epidemic spreading in dynamic multiplex networks", "link": "https://arxiv.org/abs/2406.03763", "description": "arXiv:2406.03763v1 Announce Type: new \nAbstract: Epidemic spreading processes on dynamic multiplex networks provide a more accurate description of natural spreading processes than those on single layered networks. To describe the influence of different individuals in the awareness layer on epidemic spreading, we propose a two-layer network-based epidemic spreading model, including some individuals who neglect the epidemic, and we explore how individuals with different properties in the awareness layer will affect the spread of epidemics. The two-layer network model is divided into an information transmission layer and a disease spreading layer. Each node in the layer represents an individual with different connections in different layers. Individuals with awareness will be infected with a lower probability compared to unaware individuals, which corresponds to the various epidemic prevention measures in real life. We adopt the micro-Markov chain approach to analytically derive the threshold for the proposed epidemic model, which demonstrates that the awareness layer affects the threshold of disease spreading. We then explore how individuals with different properties would affect the disease spreading process through extensive Monte Carlo numerical simulations. We find that individuals with high centrality in the awareness layer would significantly inhibit the transmission of infectious diseases. Additionally, we propose conjectures and explanations for the approximately linear effect of individuals with low centrality in the awareness layer on the number of infected individuals."}, "https://arxiv.org/abs/2406.03796": {"title": "Beyond a binary theorizing of prosociality", "link": "https://arxiv.org/abs/2406.03796", "description": "arXiv:2406.03796v1 Announce Type: new \nAbstract: A stylized experiment, the public goods game, has taught us the peculiar reproducible fact that humans tend to contribute more to shared resources than expected from economically rational assumptions. There have been two competing explanations for this phenomenon: either contributing to the public good is an innate human trait (the prosocial preference hypothesis) or a transitory effect while learning the game (the confused learner hypothesis). We use large-scale experimental data from a novel experimental design to distinguish between these two hypotheses. By monitoring the effects of zealots (persistently cooperating bots) and varying the participants' awareness of them, we find a considerably more complex scenario than previously reported. People indeed have a prosocial bias, but not to the degree that they always forego taking action to increase their profit. While our findings end the simplistic theorizing of prosociality in the public goods game, an observed positive, cooperative response to zealots has actionable policy implications."}, "https://arxiv.org/abs/2406.03852": {"title": "Why the Metric Backbone Preserves Community Structure", "link": "https://arxiv.org/abs/2406.03852", "description": "arXiv:2406.03852v1 Announce Type: new \nAbstract: The metric backbone of a weighted graph is the union of all-pairs shortest paths. It is obtained by removing all edges $(u,v)$ that are not the shortest path between $u$ and $v$. In networks with well-separated communities, the metric backbone tends to preserve many inter-community edges, because these edges serve as bridges connecting two communities, but tends to delete many intra-community edges because the communities are dense. This suggests that the metric backbone would dilute or destroy the community structure of the network. However, this is not borne out by prior empirical work, which instead showed that the metric backbone of real networks preserves the community structure of the original network well. In this work, we analyze the metric backbone of a broad class of weighted random graphs with communities, and we formally prove the robustness of the community structure with respect to the deletion of all the edges that are not in the metric backbone. An empirical comparison of several graph sparsification techniques confirms our theoretical finding and shows that the metric backbone is an efficient sparsifier in the presence of communities."}, "https://arxiv.org/abs/2406.03921": {"title": "Knowledge Transfer, Knowledge Gaps, and Knowledge Silos in Citation Networks", "link": "https://arxiv.org/abs/2406.03921", "description": "arXiv:2406.03921v1 Announce Type: new \nAbstract: The advancement of science relies on the exchange of ideas across disciplines and the integration of diverse knowledge domains. However, tracking knowledge flows and interdisciplinary integration in rapidly evolving, multidisciplinary fields remains a significant challenge. This work introduces a novel network analysis framework to study the dynamics of knowledge transfer directly from citation data. By applying dynamic community detection to cumulative, time-evolving citation networks, we can identify research areas as groups of papers sharing knowledge sources and outputs. Our analysis characterises the life-cycles and knowledge transfer patterns of these dynamic communities over time. We demonstrate our approach through a case study of eXplainable Artificial Intelligence (XAI) research, an emerging interdisciplinary field at the intersection of machine learning, statistics, and psychology. Key findings include: (i) knowledge transfer between these important foundational topics and the contemporary topics in XAI research is limited, and the extent of knowledge transfer varies across different contemporary research topics; (ii) certain application domains exist as isolated \"knowledge silos\"; (iii) significant \"knowledge gaps\" are identified between related XAI research areas, suggesting opportunities for cross-pollination and improved knowledge integration. By mapping interdisciplinary integration and bridging knowledge gaps, this work can inform strategies to synthesise ideas from disparate sources and drive innovation. More broadly, our proposed framework enables new insights into the evolution of knowledge ecosystems directly from citation data, with applications spanning literature review, research planning, and science policy."}, "https://arxiv.org/abs/2406.04005": {"title": "The Failed Migration of Academic Twitter", "link": "https://arxiv.org/abs/2406.04005", "description": "arXiv:2406.04005v1 Announce Type: new \nAbstract: Following change in Twitter's ownership and subsequent changes to content moderation policies, many in academia looked to move their discourse elsewhere and migration to Mastodon was pursued by some. Our study looks at the dynamics of this migration. Utilizing publicly available user account data, we track the posting activity of academics on Mastodon over a one year period. Our analyses reveal significant challenges sustaining user engagement on Mastodon due to its decentralized structure as well as competition from other platforms such as Bluesky and Threads. The movement lost momentum after an initial surge of enthusiasm as most users did not maintain their activity levels, and those who did faced lower levels of engagement compared to Twitter. Our findings highlight the challenges involved in transitioning professional communities to decentralized platforms, emphasizing the need for focusing on migrating social connections for long-term user engagement."}, "https://arxiv.org/abs/2406.04039": {"title": "Shaping History: Advanced Machine Learning Techniques for the Analysis and Dating of Cuneiform Tablets over Three Millennia", "link": "https://arxiv.org/abs/2406.04039", "description": "arXiv:2406.04039v1 Announce Type: cross \nAbstract: Cuneiform tablets, emerging in ancient Mesopotamia around the late fourth millennium BCE, represent one of humanity's earliest writing systems. Characterized by wedge-shaped marks on clay tablets, these artifacts provided insight into Mesopotamian civilization across various domains. Traditionally, the analysis and dating of these tablets rely on subjective assessment of shape and writing style, leading to uncertainties in pinpointing their exact temporal origins. Recent advances in digitization have revolutionized the study of cuneiform by enhancing accessibility and analytical capabilities. Our research uniquely focuses on the silhouette of tablets as significant indicators of their historical periods, diverging from most studies that concentrate on textual content. Utilizing an unprecedented dataset of over 94,000 images from the Cuneiform Digital Library Initiative collection, we apply deep learning methods to classify cuneiform tablets, covering over 3,000 years of history. By leveraging statistical, computational techniques, and generative modeling through Variational Auto-Encoders (VAEs), we achieve substantial advancements in the automatic classification of these ancient documents, focusing on the tablets' silhouettes as key predictors. Our classification approach begins with a Decision Tree using height-to-width ratios and culminates with a ResNet50 model, achieving a 61% macro F1-score for tablet silhouettes. Moreover, we introduce novel VAE-powered tools to enhance explainability and enable researchers to explore changes in tablet shapes across different eras and genres. This research contributes to document analysis and diplomatics by demonstrating the value of large-scale data analysis combined with statistical methods. These insights offer valuable tools for historians and epigraphists, enriching our understanding of cuneiform tablets and the cultures that produced them."}, "https://arxiv.org/abs/2406.04299": {"title": "NoisyGL: A Comprehensive Benchmark for Graph Neural Networks under Label Noise", "link": "https://arxiv.org/abs/2406.04299", "description": "arXiv:2406.04299v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs) exhibit strong potential in node classification task through a message-passing mechanism. However, their performance often hinges on high-quality node labels, which are challenging to obtain in real-world scenarios due to unreliable sources or adversarial attacks. Consequently, label noise is common in real-world graph data, negatively impacting GNNs by propagating incorrect information during training. To address this issue, the study of Graph Neural Networks under Label Noise (GLN) has recently gained traction. However, due to variations in dataset selection, data splitting, and preprocessing techniques, the community currently lacks a comprehensive benchmark, which impedes deeper understanding and further development of GLN. To fill this gap, we introduce NoisyGL in this paper, the first comprehensive benchmark for graph neural networks under label noise. NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy labeled graph data across various datasets, with unified experimental settings and interface. Our benchmark has uncovered several important insights that were missed in previous research, and we believe these findings will be highly beneficial for future studies. We hope our open-source benchmark library will foster further advancements in this field. The code of the benchmark can be found in https://github.com/eaglelab-zju/NoisyGL."}, "https://arxiv.org/abs/2207.12264": {"title": "Dynamics and triggers of misinformation on vaccines", "link": "https://arxiv.org/abs/2207.12264", "description": "arXiv:2207.12264v3 Announce Type: replace \nAbstract: The Covid-19 pandemic has sparked renewed attention on the prevalence of misinformation online, whether intentional or not, underscoring the potential risks posed to individuals' quality of life associated with the dissemination of misconceptions and enduring myths on health-related subjects. In this study, we analyze 6 years (2016-2021) of Italian vaccine debate across diverse social media platforms (Facebook, Instagram, Twitter, YouTube), encompassing all major news sources - both questionable and reliable. We first use the symbolic transfer entropy analysis of news production time-series to dynamically determine which category of sources, questionable or reliable, causally drives the agenda on vaccines. Then, leveraging deep learning models capable to accurately classify vaccine-related content based on the conveyed stance and discussed topic, respectively, we evaluate the focus on various topics by news sources promoting opposing views and compare the resulting user engagement. Aside from providing valuable resources for further investigation of vaccine-related misinformation, particularly in a language (Italian) that receives less attention in scientific research compared to languages like English, our study uncovers misinformation not as a parasite of the news ecosystem that merely opposes the perspectives offered by mainstream media, but as an autonomous force capable of even overwhelming the production of vaccine-related content from the latter. While the pervasiveness of misinformation is evident in the significantly higher engagement of questionable sources compared to reliable ones, our findings underscore the importance of consistent and thorough pro-vax coverage. This is especially crucial in addressing the most sensitive topics where the risk of misinformation spreading and potentially exacerbating negative attitudes toward vaccines among the users involved is higher."}, "https://arxiv.org/abs/2307.02818": {"title": "Degree Heterogeneity in Higher-Order Networks: Inference in the Hypergraph $\\boldsymbol{\\beta}$-Model", "link": "https://arxiv.org/abs/2307.02818", "description": "arXiv:2307.02818v4 Announce Type: replace-cross \nAbstract: The $\\boldsymbol{\\beta}$-model for random graphs is commonly used for representing pairwise interactions in a network with degree heterogeneity. Going beyond pairwise interactions, Stasi et al. (2014) introduced the hypergraph $\\boldsymbol{\\beta}$-model for capturing degree heterogeneity in networks with higher-order (multi-way) interactions. In this paper we initiate the rigorous study of the hypergraph $\\boldsymbol{\\beta}$-model with multiple layers, which allows for hyperedges of different sizes across the layers. To begin with, we derive the rates of convergence of the maximum likelihood (ML) estimate and establish their minimax rate optimality. We also derive the limiting distribution of the ML estimate and construct asymptotically valid confidence intervals for the model parameters. Next, we consider the goodness-of-fit problem in the hypergraph $\\boldsymbol{\\beta}$-model. Specifically, we establish the asymptotic normality of the likelihood ratio (LR) test under the null hypothesis, derive its detection threshold, and also its limiting power at the threshold. Interestingly, the detection threshold of the LR test turns out to be minimax optimal, that is, all tests are asymptotically powerless below this threshold. The theoretical results are further validated in numerical experiments. In addition to developing the theoretical framework for estimation and inference for hypergraph $\\boldsymbol{\\beta}$-models, the above results fill a number of gaps in the graph $\\boldsymbol{\\beta}$-model literature, such as the minimax optimality of the ML estimates and the non-null properties of the LR test, which, to the best of our knowledge, have not been studied before."}, "https://arxiv.org/abs/2403.13872": {"title": "Spatial-Temporal Graph Representation Learning for Tactical Networks Future State Prediction", "link": "https://arxiv.org/abs/2403.13872", "description": "arXiv:2403.13872v2 Announce Type: replace-cross \nAbstract: Resource allocation in tactical ad-hoc networks presents unique challenges due to their dynamic and multi-hop nature. Accurate prediction of future network connectivity is essential for effective resource allocation in such environments. In this paper, we introduce the Spatial-Temporal Graph Encoder-Decoder (STGED) framework for Tactical Communication Networks that leverages both spatial and temporal features of network states to learn latent tactical behaviors effectively. STGED hierarchically utilizes graph-based attention mechanism to spatially encode a series of communication network states, leverages a recurrent neural network to temporally encode the evolution of states, and a fully-connected feed-forward network to decode the connectivity in the future state. Through extensive experiments, we demonstrate that STGED consistently outperforms baseline models by large margins across different time-steps input, achieving an accuracy of up to 99.2\\% for the future state prediction task of tactical communication networks."}, "https://arxiv.org/abs/2406.04462": {"title": "Adaptive Algorithmic Interventions for Escaping Pessimism Traps in Dynamic Sequential Decisions", "link": "https://arxiv.org/abs/2406.04462", "description": "arXiv:2406.04462v1 Announce Type: new \nAbstract: In this paper, we relate the philosophical literature on pessimism traps to information cascades, a formal model derived from the economics and mathematics literature. A pessimism trap is a social pattern in which individuals in a community, in situations of uncertainty, begin to copy the sub-optimal actions of others, despite their individual beliefs. This maps nicely onto the concept of an information cascade, which involves a sequence of agents making a decision between two alternatives, with a private signal of the superior alternative and a public history of others' actions. Key results from the economics literature show that information cascades occur with probability one in many contexts, and depending on the strength of the signal, populations can fall into the incorrect cascade very easily and quickly. Once formed, in the absence of external perturbation, a cascade cannot be broken -- therefore, we derive an intervention that can be used to nudge a population from an incorrect to a correct cascade and, importantly, maintain the cascade once the subsidy is discontinued. We study this both theoretically and empirically."}, "https://arxiv.org/abs/2406.04543": {"title": "Function and form of U", "link": "https://arxiv.org/abs/2406.04543", "description": "arXiv:2406.04543v1 Announce Type: new \nAbstract: The relationship between urban form and function is a complex challenge that can be examined from multiple perspectives. In this study, we propose a method to characterize the urban function of U.S. metropolitan areas by analyzing trip patterns extracted from the 2017 National Household Travel Survey (NHTS). To characterize urban form, we employ measures that capture road network topology. We cluster cities based on both form and function and subsequently compare these clusters. Our analysis of 52 U.S. metropolitan areas identifies 7 distinct clusters of cities that exhibit similar travel behavior, suggesting that diverse mobility patterns can be effectively grouped into a few universal classes. The observed disparity between the urban-function clustering and the urban-form clustering suggests that travel behavior in the U.S. is not strongly influenced by the physical infrastructure of the city."}, "https://arxiv.org/abs/2406.04962": {"title": "Mapping the Global Election Landscape on Social Media in 2024", "link": "https://arxiv.org/abs/2406.04962", "description": "arXiv:2406.04962v1 Announce Type: new \nAbstract: In 2024, half of the global population is expected to participate in elections, offering researchers a unique opportunity to study online information diffusion and user behavior. This study investigates the media landscape on social media by analyzing Facebook posts from national political parties and major news agencies across Europe, Mexico, and India. Our methodology identifies key topics and evaluates public interaction, reflecting broader trends in political engagement. Using Principal Component Analysis, we distil these topics to uncover patterns of correlation and differentiation. This approach reveals dominant themes that engage global audiences, providing critical insights into the interplay between public opinion and digital narratives during a major electoral cycle. Our findings highlight how different topics resonate across political spectrums, shaping political debate and offering a comprehensive view of the interaction between media content, political ideology, and audience engagement."}, "https://arxiv.org/abs/2406.05021": {"title": "From cryptomarkets to the surface web: Scouting eBay for counterfeits", "link": "https://arxiv.org/abs/2406.05021", "description": "arXiv:2406.05021v1 Announce Type: new \nAbstract: Detecting counterfeits on online marketplaces is challenging, and current methods struggle with the volume of sales on platforms like eBay, while cryptomarkets openly sell counterfeits. Leveraging information from 453 cryptomarket counterfeits, we automated a search for corresponding products on eBay, utilizing image and text similarity metrics. We collected data twice over 4-months to analyze changes with an average of 159 eBay products per cryptomarket item, totaling 134k products. We found identical products, which would warrant further investigation as to whether they are counterfeits. Results indicate increasing difficulty finding similar products over time, moderated by product type and origin. Future improved versions of the current system could be used to examine possible connections between cryptomarket and surface web listings more closely and could hold practical value in supporting the detection of counterfeits on the surface web."}, "https://arxiv.org/abs/2406.05026": {"title": "Higher-order modeling of face-to-face interactions", "link": "https://arxiv.org/abs/2406.05026", "description": "arXiv:2406.05026v1 Announce Type: new \nAbstract: The most fundamental social interactions among humans occur face to face. Their features have been extensively studied in recent years, owing to the availability of high-resolution data on individuals' proximity. Mathematical models based on mobile agents have been crucial to understand the spatio-temporal organization of face-to-face interactions. However, these models focus on dyadic relationships only, failing to characterize interactions in larger groups of individuals. Here, we propose a model in which agents interact with each other by forming groups of different sizes. Each group has a degree of social attractiveness, based on which neighboring agents decide whether to join. Our framework reproduces different properties of groups in face-to-face interactions, including their distribution, the correlation in their number, and their persistence in time, which cannot be replicated by dyadic models. Furthermore, it captures homophilic patterns at the level of higher-order interactions, going beyond standard pairwise approaches. Our work sheds light on the higher-order mechanisms at the heart of human face-to-face interactions, paving the way for further investigation of how group dynamics at a microscopic scale affects social phenomena at a macroscopic scale."}, "https://arxiv.org/abs/2406.04423": {"title": "Determining the Number of Communities in Sparse and Imbalanced Settings", "link": "https://arxiv.org/abs/2406.04423", "description": "arXiv:2406.04423v1 Announce Type: cross \nAbstract: Community structures represent a crucial aspect of network analysis, and various methods have been developed to identify these communities. However, a common hurdle lies in determining the number of communities K, a parameter that often requires estimation in practice. Existing approaches for estimating K face two notable challenges: the weak community signal present in sparse networks and the imbalance in community sizes or edge densities that result in unequal per-community expected degree. We propose a spectral method based on a novel network operator whose spectral properties effectively overcome both challenges. This operator is a refined version of the non-backtracking operator, adapted from a \"centered\" adjacency matrix. Its leading eigenvalues are more concentrated than those of the adjacency matrix for sparse networks, while they also demonstrate enhanced signal under imbalance scenarios, a benefit attributed to the centering step. This is justified, either theoretically or numerically, under the null model K = 1, in both dense and ultra-sparse settings. A goodness-of-fit test based on the leading eigenvalue can be applied to determine the number of communities K."}, "https://arxiv.org/abs/2406.04548": {"title": "GNNAnatomy: Systematic Generation and Evaluation of Multi-Level Explanations for Graph Neural Networks", "link": "https://arxiv.org/abs/2406.04548", "description": "arXiv:2406.04548v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs) have proven highly effective in various machine learning (ML) tasks involving graphs, such as node/graph classification and link prediction. However, explaining the decisions made by GNNs poses challenges because of the aggregated relational information based on graph structure, leading to complex data transformations. Existing methods for explaining GNNs often face limitations in systematically exploring diverse substructures and evaluating results in the absence of ground truths. To address this gap, we introduce GNNAnatomy, a model- and dataset-agnostic visual analytics system designed to facilitate the generation and evaluation of multi-level explanations for GNNs. In GNNAnatomy, we employ graphlets to elucidate GNN behavior in graph-level classification tasks. By analyzing the associations between GNN classifications and graphlet frequencies, we formulate hypothesized factual and counterfactual explanations. To validate a hypothesized graphlet explanation, we introduce two metrics: (1) the correlation between its frequency and the classification confidence, and (2) the change in classification confidence after removing this substructure from the original graph. To demonstrate the effectiveness of GNNAnatomy, we conduct case studies on both real-world and synthetic graph datasets from various domains. Additionally, we qualitatively compare GNNAnatomy with a state-of-the-art GNN explainer, demonstrating the utility and versatility of our design."}, "https://arxiv.org/abs/2406.04612": {"title": "Revisiting Attention Weights as Interpretations of Message-Passing Neural Networks", "link": "https://arxiv.org/abs/2406.04612", "description": "arXiv:2406.04612v1 Announce Type: cross \nAbstract: The self-attention mechanism has been adopted in several widely-used message-passing neural networks (MPNNs) (e.g., GATs), which adaptively controls the amount of information that flows along the edges of the underlying graph. This usage of attention has made such models a baseline for studies on explainable AI (XAI) since interpretations via attention have been popularized in various domains (e.g., natural language processing and computer vision). However, existing studies often use naive calculations to derive attribution scores from attention, and do not take the precise and careful calculation of edge attribution into consideration. In our study, we aim to fill the gap between the widespread usage of attention-enabled MPNNs and their potential in largely under-explored explainability, a topic that has been actively investigated in other areas. To this end, as the first attempt, we formalize the problem of edge attribution from attention weights in GNNs. Then, we propose GATT, an edge attribution calculation method built upon the computation tree. Through comprehensive experiments, we demonstrate the effectiveness of our proposed method when evaluating attributions from GATs. Conversely, we empirically validate that simply averaging attention weights over graph attention layers is insufficient to interpret the GAT model's behavior. Code is publicly available at https://github.com/jordan7186/GAtt/tree/main."}, "https://arxiv.org/abs/2406.04701": {"title": "Transition to synchronization in adaptive Sakaguchi-Kuramoto model with higher-order interactions", "link": "https://arxiv.org/abs/2406.04701", "description": "arXiv:2406.04701v1 Announce Type: cross \nAbstract: We investigate the phenomenon of transition to synchronization in Sakaguchi-Kuramoto model in the presence of higher-order interactions and global order parameter adaptation. The investigation is done by performing extensive numerical simulations and low dimensional modeling of the system. Numerical simulations of the full system show both continuous (second order) as well as discontinuous transitions. The discontinuous transitions can either be associated with explosive (first order) or with tiered synchronization states depending on the choice of parameters. To develop an in depth understanding of the transition scenario in the parameter space we derive a reduced order model (ROM) using the Ott-Antonsen ansatz, the results of which closely matches with that of the numerical simulations of the full system. The simplicity and analytical accessibility of the ROM helps to conveniently unfold the transition scenario in the system having complex dependence on the parameters. Simultaneous analysis of the full system and the ROM clearly identifies the regions of the parameter space exhibiting different types of transitions. It is observed that the second order continuous transition is connected with a supercritical pitchfork bifurcation (PB) of the ROM. On the other hand, the discontinuous teired transition is associated with multiple saddle-node (SN) bifurcations along with a supercritical PB and the first order explosive transition involves a subcritical PB alongside a SN bifurcation."}, "https://arxiv.org/abs/2406.04916": {"title": "Combinatorial Complex Score-based Diffusion Modelling through Stochastic Differential Equations", "link": "https://arxiv.org/abs/2406.04916", "description": "arXiv:2406.04916v1 Announce Type: cross \nAbstract: Graph structures offer a versatile framework for representing diverse patterns in nature and complex systems, applicable across domains like molecular chemistry, social networks, and transportation systems. While diffusion models have excelled in generating various objects, generating graphs remains challenging. This thesis explores the potential of score-based generative models in generating such objects through a modelization as combinatorial complexes, which are powerful topological structures that encompass higher-order relationships.\n  In this thesis, we propose a unified framework by employing stochastic differential equations. We not only generalize the generation of complex objects such as graphs and hypergraphs, but we also unify existing generative modelling approaches such as Score Matching with Langevin dynamics and Denoising Diffusion Probabilistic Models. This innovation overcomes limitations in existing frameworks that focus solely on graph generation, opening up new possibilities in generative AI.\n  The experiment results showed that our framework could generate these complex objects, and could also compete against state-of-the-art approaches for mere graph and molecule generation tasks."}, "https://arxiv.org/abs/2305.16488": {"title": "Assessing inequities in electrification via heat pumps across the U", "link": "https://arxiv.org/abs/2305.16488", "description": "arXiv:2305.16488v3 Announce Type: replace \nAbstract: Heat pumps are an energy-efficient and increasingly cost-effective solution for reducing greenhouse gas emissions in the building sector. However, other clean energy technologies such as rooftop solar are less likely to be adopted in underserved communities, and thus policies incentivizing their adoption may funnel tax dollars to well-resourced communities. Unlike previously-studied technologies, the effects of heat pumps on household energy bills may be positive or negative depending on local climate, fuel availability and costs, and other factors. Here we propose a framework for assessing heat pump inequities across the U.S. We find that households in communities of color and with higher percentages of renters are less likely to use heat pumps across the board. Moreover, communities of color are least likely to use heat pumps in regions where they are most likely to reduce energy bills. Public policies must address these inequities to advance beneficial electrification and energy justice."}, "https://arxiv.org/abs/2404.17082": {"title": "Evolutionary game dynamics with environmental feedback in a network with two communities", "link": "https://arxiv.org/abs/2404.17082", "description": "arXiv:2404.17082v2 Announce Type: replace \nAbstract: Recent developments of eco-evolutionary models have shown that evolving feedbacks between behavioral strategies and the environment of game interactions, leading to changes in the underlying payoff matrix, can impact the underlying population dynamics in various manners. We propose and analyze an eco-evolutionary game dynamics model on a network with two communities such that players interact with other players in the same community and those in the opposite community at different rates. In our model, we consider two-person matrix games with pairwise interactions occurring on individual edges and assume that the environmental state depends on edges rather than on nodes or being globally shared in the population. We analytically determine the equilibria and their stability under a symmetric population structure assumption, and we also numerically study the replicator dynamics of the general model. The model shows rich dynamical behavior, such as multiple transcritical bifurcations, multistability, and anti-synchronous oscillations. Our work offers insights into understanding how the presence of community structure impacts the eco-evolutionary dynamics within and between niches."}, "https://arxiv.org/abs/2403.07294": {"title": "Graph Data Condensation via Self-expressive Graph Structure Reconstruction", "link": "https://arxiv.org/abs/2403.07294", "description": "arXiv:2403.07294v2 Announce Type: replace-cross \nAbstract: With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN. However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator. They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset. To address these issues, we introduce a novel framework named \\textbf{G}raph Data \\textbf{C}ondensation via \\textbf{S}elf-expressive Graph Structure \\textbf{R}econstruction (\\textbf{GCSR}). Our method stands out by (1) explicitly incorporating the original graph structure into the condensing process and (2) capturing the nuanced interdependencies between the condensed nodes by reconstructing an interpretable self-expressive graph structure. Extensive experiments and comprehensive analysis validate the efficacy of the proposed method across diverse GNN models and datasets. Our code is available at \\url{https://github.com/zclzcl0223/GCSR}."}, "https://arxiv.org/abs/2404.13468": {"title": "A Grassroots Architecture to Supplant Global Digital Platforms by a Global Digital Democracy", "link": "https://arxiv.org/abs/2404.13468", "description": "arXiv:2404.13468v4 Announce Type: replace-cross \nAbstract: We present an architectural alternative to global digital platforms termed grassroots, designed to serve the social, economic, civic, and political needs of local digital communities, as well as their federation. Grassroots platforms may offer local communities an alternative to global digital platforms while operating solely on the smartphones of their members, forsaking any global resources other than the network itself. Such communities may form digital economies without initial capital or external credit, exercise sovereign democratic governance, and federate, ultimately resulting in the grassroots formation of a global digital democracy."}, "https://arxiv.org/abs/2406.05809": {"title": "The Paradox of Collective Certainty in Science", "link": "https://arxiv.org/abs/2406.05809", "description": "arXiv:2406.05809v1 Announce Type: new \nAbstract: We explore a paradox of collective action and certainty in science wherein the more scientists research together, the less that work contributes to the value of their collective certainty. When scientists address similar problems and share data, methods, and collaborators, their understanding of and trust in their colleagues' research rises, a quality required for scientific advance. This increases the positive reinforcement scientists receive for shared beliefs as they become more dependent on their colleagues' knowledge, interests, and findings. This collective action increases the potential for scientists to reside in epistemic ''bubbles'' that limit their capacity to make new discoveries or have their discoveries generalize. In short, as scientists grow closer, their experience of scientific validity rises as the likelihood of genuine replication falls, creating a trade-off between certainty and truth."}, "https://arxiv.org/abs/2406.05884": {"title": "Revisiting institutional punishment in the $N$-person prisoner's dilemma", "link": "https://arxiv.org/abs/2406.05884", "description": "arXiv:2406.05884v1 Announce Type: new \nAbstract: The conflict between individual and collective interests makes fostering cooperation in human societies a challenging task, requiring drastic measures such as the establishment of sanctioning institutions. These institutions are costly because they have to be maintained regardless of the presence or absence of offenders. Here, we propose realistic improvements to the standard $N$-person prisoner's dilemma formulation with institutional punishment by eliminating overpunishment, requiring a minimum number of contributors to establish the sanctioning institution, and sharing the cost among them once this minimum number is reached. In addition, we focus on large groups or communities for which sanctioning institutions are ubiquitous. Using the replicator equation framework for an infinite population, we find that by sufficiently fining players who fail to contribute either to the public good or to the sanctioning institution, a population of contributors immune to invasion by these free riders can be established, provided that the contributors are sufficiently numerous. In a finite population, we use finite-size scaling to show that, for some parameter settings, demographic noise helps to fixate the strategy that contributes to the public good but not to the sanctioning institution even for infinitely large populations when, somewhat counterintuitively, its proportion in the initial population vanishes with a small power of the population size."}, "https://arxiv.org/abs/2406.06084": {"title": "Ecological Data Reveal Imbalances in Collision Avoidance Due to Groups' Social Interaction", "link": "https://arxiv.org/abs/2406.06084", "description": "arXiv:2406.06084v1 Announce Type: new \nAbstract: The relative dynamics in collision avoidance between individual pedestrians and dyads has been recently studied, and it was shown that individuals may intrude dyads that are not socially interacting. Building on this, our current study examines how much each party contributes to collision avoidance by measuring deviations from their intended paths. Our findings suggest that individuals prioritise trajectory efficiency in undisturbed situations, but prioritise safety when encountering dyads, deviating more from their intended path. Non-interacting dyads present a similar behavior, although their trajectories appear to be even more efficient than those of individuals in undisturbed situations, and their deviations during encounters less pronounced. On the other hand, socially interacting dyads are not very efficient in undisturbed situations, and their behavior is mostly unaffected by encounters. These results strongly suggest that group dynamics affects in two ways the behavior of pedestrians, namely it has a dynamical and a social effect. The dynamical effect stabilises their trajectory, while the social one decreases the ability to focus on the external environment, leading to reduced efficiency and safety. Another finding concerns the tendency of individuals to avoid in a more prominent way the interacting dyads as compared to non-interacting ones. This suggests that individuals may assess others' contribution to collision avoidance. An impact parameter analysis reveals that collision risk influences path deviations in pedestrian encounters. For individuals, larger behavioral differences between low and high interaction levels of the dyad occur both when the collision risk is high and during less critical encounters. For dyads, the deviation differences between low and high interaction levels are most pronounced when the individual is on course to pass close to the dyad."}, "https://arxiv.org/abs/2406.06200": {"title": "Inequalities of energy release rates in compression of nano-porous materials predict its imminent breakdown", "link": "https://arxiv.org/abs/2406.06200", "description": "arXiv:2406.06200v1 Announce Type: new \nAbstract: We show that the divergent acoustic energy release rate in a quasi-statically compressed nano-porous material can be used as a precursor to failure in such materials. A quantification of the inequality of the energy release rate using social inequality measure indices help constructing a warning signal for large bursts of energy release. We also verify similar behavior for simulations of viscoelastic fiber bundle models that mimic the strain-hardening dynamics of the samples. The results demonstrate experimental applicability of the precursory signal formulation for any diverging response function near a transition point using social inequality indices."}, "https://arxiv.org/abs/2406.06440": {"title": "Messengers: Breaking Echo Chambers in Collective Opinion Dynamics with Homophily", "link": "https://arxiv.org/abs/2406.06440", "description": "arXiv:2406.06440v1 Announce Type: new \nAbstract: Collective estimation manifests computational intelligence emerging from inter-individual local interactions, e.g., by aggregating opinions from neighbors to estimate a quantity. Use cases of collective estimation may include directed motion in physical space, such that agents, for example, have to collectively explore a distributed feature, and collectively agree on a numerical value. In doing so, collectives face several challenges in achieving precise estimations. These challenges exhibit complex behaviors, particularly when the interaction network and opinion of agents evolve simultaneously. We take homophilic networks as an example, where disproportionate interaction with like-minded neighbors leads to the emergence of echo chambers, preventing collective consensus. Our simulation results confirm that, besides a lack of exposure to attitude-challenging opinions, seeking reaffirming information entraps agents in echo chambers. We propose a generic novel approach based on a Dichotomous Markov Process (DMP) where stubborn agents (called Messengers) connect the disconnected clusters by physically transporting their opinions to other clusters to inform and direct the other agents. We show that diverse collective behaviors arise from the DMP and study a continuum between task specialization with no switching (full-time Messengers), generalization with slow task switching (part-time Messengers), and rapid task switching (short-time Messengers) and its impact on system performance. Our results show that stubborn agents can, in various ways, break the echo chambers and promote consensus in collective opinion."}, "https://arxiv.org/abs/2406.06490": {"title": "How much longer do you have to drive than the crow has to fly?", "link": "https://arxiv.org/abs/2406.06490", "description": "arXiv:2406.06490v1 Announce Type: new \nAbstract: When we travel by car from one location to another, our route is constrained by the road network. The resulting network distance is generally longer than the geodetic distance, i.e. the distance as the crow flies, between the two locations. We report a systematic relation between the statistical properties of these two distances. In empirical analyses for large motorway networks in various countries and areas, we work out distributions of network and geodetic distances and identify a surprisingly robust scaling property between them. A simple consequence is that we typically have to drive $1.3\\pm0.1$ times longer than the crow flies. Moreover, we show that this scaling is not present in standard random networks; rather, it requires a certain non-randomness, namely adjacency. We develop a set of rules to build a realistic motorway network, also consistent with the scaling properties found empirically. We hypothesize that the scaling reflects, in a rather universal fashion, a compromise between two societal needs: high efficiency and accessibility on the one hand, and limitation of costs and other burdens on the other."}, "https://arxiv.org/abs/2406.05246": {"title": "Blended Bots: Infiltration through Identity Deception on Social Media", "link": "https://arxiv.org/abs/2406.05246", "description": "arXiv:2406.05246v1 Announce Type: cross \nAbstract: Bots are automated social media users that can be used to amplify (mis)information and sow harmful discourse. In order to effectively influence users, bots can be generated to reproduce human user behavior. Indeed, people tend to trust information coming from users with profiles that fit roles they expect to exist, such as users with gender role stereotypes. In this work, we examine differences in the types of identities in profiles of human and bot accounts with a focus on combinations of identities that represent gender role stereotypes. We find that some types of identities differentiate between human and bot profiles, confirming this approach can be a useful in distinguishing between human and bot accounts on social media. However, contrary to our expectations, we reveal that gender bias is expressed more in human accounts than bots overall. Despite having less gender bias overall, we provide examples of identities with strong associations with gender identities in bot profiles, such as those related to technology, finance, sports, and horoscopes. Finally, we discuss implications for designing constructive social media bot detection training materials."}, "https://arxiv.org/abs/2406.05560": {"title": "A Shape Change Enhancing Hierarchical Layout for the Pairwise Comparison of Directed Acyclic Graphs", "link": "https://arxiv.org/abs/2406.05560", "description": "arXiv:2406.05560v1 Announce Type: cross \nAbstract: Comparing directed acyclic graphs is essential in various fields such as healthcare, social media, finance, biology, and marketing. DAGs often result from contagion processes over networks, including information spreading, retweet activity, disease transmission, financial crisis propagation, malware spread, and gene mutations. For instance, in disease spreading, an infected patient can transmit the disease to contacts, making it crucial to analyze and predict scenarios. Similarly, in finance, understanding the effects of saving or not saving specific banks during a crisis is vital. Experts often need to identify small differences between DAGs, such as changes in a few nodes or edges. Even the presence or absence of a single edge can be significant. Visualization plays a crucial role in facilitating these comparisons. However, standard hierarchical layout algorithms struggle to visualize subtle changes effectively. The typical hierarchical layout, with the root on top, is preferred due to its performance in comparison to other layouts. Nevertheless, these standard algorithms prioritize single-graph aesthetics over comparison suitability, making it challenging for users to spot changes. To address this issue, we propose a layout that enhances shape changes in DAGs while minimizing the impact on aesthetics. Our approach involves outwardly swapping changes, altering the DAG's shape. We introduce new drawing criteria. Our layout builds upon a Sugiyama-like hierarchical layout and implements these criteria through two extensions. We designed it this way to maintain interchangeability and accommodate future optimizations, such as pseudo-nodes for edge crossing minimization. In our evaluations, our layout achieves excellent results, with edge crossing aesthetics averaging around 0.8 (on a scale of 0 to 1). Additionally, our layout outperforms the base implementation by an average of 60-75\\%."}, "https://arxiv.org/abs/2406.05582": {"title": "On the Role of Communications for Space Domain Awareness", "link": "https://arxiv.org/abs/2406.05582", "description": "arXiv:2406.05582v1 Announce Type: cross \nAbstract: Space Domain Awareness (SDA) has become increasingly vital with the rapid growth of commercial space activities and the expansion of New Space. This paper stresses the necessity of transitioning from centralized to distributed SDA architectures. The current architecture predominantly relies on individual downhaul, which we propose to transition to on-orbit distribution. Our results demonstrate that the individual downhaul architecture does not scale efficiently with the increasing number of nodes, while on-orbit distribution offers significant improvements. By comparing the centralized architecture with the proposed distributed architecture, we highlight the advantages of enhanced coverage and resilience. Our findings show that on-orbit distribution greatly outperforms individual downhaul in terms of latency and scalability. Specifically, the latency results for on-orbit distribution are substantially lower and more consistent, even as the number of satellites increases. In addition, we address the inherent challenges associated with on-orbit distribution architecture, particularly cybersecurity concerns. We focus on link security to ensure the availability and integrity of data transmission in these advanced SDA systems. Future expectations include further refinement of on-orbit distribution strategies and the development of robust cybersecurity measures to support the scalability and resilience of SDA systems."}, "https://arxiv.org/abs/2406.06014": {"title": "Network two-sample test for block models", "link": "https://arxiv.org/abs/2406.06014", "description": "arXiv:2406.06014v1 Announce Type: cross \nAbstract: We consider the two-sample testing problem for networks, where the goal is to determine whether two sets of networks originated from the same stochastic model. Assuming no vertex correspondence and allowing for different numbers of nodes, we address a fundamental network testing problem that goes beyond simple adjacency matrix comparisons. We adopt the stochastic block model (SBM) for network distributions, due to their interpretability and the potential to approximate more general models. The lack of meaningful node labels and vertex correspondence translate to a graph matching challenge when developing a test for SBMs. We introduce an efficient algorithm to match estimated network parameters, allowing us to properly combine and contrast information within and across samples, leading to a powerful test. We show that the matching algorithm, and the overall test are consistent, under mild conditions on the sparsity of the networks and the sample sizes, and derive a chi-squared asymptotic null distribution for the test. Through a mixture of theoretical insights and empirical validations, including experiments with both synthetic and real-world data, this study advances robust statistical inference for complex network data."}, "https://arxiv.org/abs/2406.06346": {"title": "Dynamical Mean-Field Theory of Complex Systems on Sparse Directed Networks", "link": "https://arxiv.org/abs/2406.06346", "description": "arXiv:2406.06346v1 Announce Type: cross \nAbstract: Although real-world complex systems typically interact through sparse and heterogeneous networks, analytic solutions of their dynamics are limited to models with all-to-all interactions. Here, we solve the dynamics of a broad range of nonlinear models of complex systems on sparse directed networks with a random structure. By generalizing dynamical mean-field theory to sparse systems, we derive an exact equation for the path-probability describing the effective dynamics of a single degree of freedom. Our general solution applies to key models in the study of neural networks, ecosystems, epidemic spreading, and synchronization. Using the population dynamics algorithm, we solve the path-probability equation to determine the phase diagram of a seminal neural network model in the sparse regime, showing that this model undergoes a transition from a fixed-point phase to chaos as a function of the network topology."}, "https://arxiv.org/abs/2103.01093": {"title": "Quantifying Indirect Gender Discrimination on Collaborative Platforms", "link": "https://arxiv.org/abs/2103.01093", "description": "arXiv:2103.01093v3 Announce Type: replace \nAbstract: Digital collaborative platforms have become crucial venues of career advancement and individual success in many creative fields, from engineering to the arts. Indirect gender discrimination is a key component to gendered disadvantage on platforms. Such platforms carried the promise of opening avenues of advancement to previously discriminated groups, such as women, as platforms lack managerial gatekeepers with conventional prejudice. We analyzed the extent of indirect gender discriminatory on two diverse platforms, GitHub and Behance, focused on software development and fine arts and design. We found that the main cause of women's disadvantage in attention, success, and survival is largely due to indirect discrimination that varies between 60-90\\% of total female disadvantage. Men and women are penalized if they follow highly female-like behavior, while categorical gender's impact varies by outcome and field. As platforms employ algorithmic tools and AI systems to manage users' activity, visibility and recommend new projects to collaborate, stereotypes rooted in behavior can have long-lasting consequences."}, "https://arxiv.org/abs/2303.00927": {"title": "QuickCent: a fast and frugal heuristic for harmonic centrality estimation on scale-free networks", "link": "https://arxiv.org/abs/2303.00927", "description": "arXiv:2303.00927v2 Announce Type: replace \nAbstract: We present a simple and quick method to approximate network centrality indexes. Our approach, called QuickCent, is inspired by so-called fast and frugal heuristics, which are heuristics initially proposed to model some human decision and inference processes. The centrality index that we estimate is the harmonic centrality, which is a measure based on shortest-path distances, so infeasible to compute on large networks. We compare QuickCent with known machine learning algorithms on synthetic data generated with preferential attachment, and some empirical networks. Our experiments show that QuickCent is able to make estimates that are competitive in accuracy with the best alternative methods tested, either on synthetic scale-free networks or empirical networks. QuickCent has the feature of achieving low error variance estimates, even with a small training set. Moreover, QuickCent is comparable in efficiency -- accuracy and time cost -- to those produced by more complex methods. We discuss and provide some insight into how QuickCent exploits the fact that in some networks, such as those generated by preferential attachment, local density measures such as the in-degree, can be a proxy for the size of the network region to which a node has access, opening up the possibility of approximating centrality indices based on size such as the harmonic centrality. Our initial results show that simple heuristics and biologically inspired computational methods are a promising line of research in the context of network measure estimations."}, "https://arxiv.org/abs/2308.12743": {"title": "Video Recommendation Using Social Network Analysis and User Viewing Patterns", "link": "https://arxiv.org/abs/2308.12743", "description": "arXiv:2308.12743v2 Announce Type: replace \nAbstract: This study proposes a novel video recommendation approach that leverages implicit user feedback in the form of viewing percentages and social network analysis techniques. By constructing a video similarity network based on user viewing patterns and computing centrality measures, the methodology identifies important and well-connected videos. Modularity analysis is then used to cluster closely related videos, forming the basis for personalized recommendations. For each user, candidate videos are selected from the cluster containing their preferred items and ranked using an ego-centric index that measures proximity to the user's likes and dislikes. The proposed approach was evaluated on real user data from an Asian video-on-demand platform. Offline experiments demonstrated improved accuracy compared to conventional methods such as Naive Bayes, SVM, decision trees, and nearest neighbor algorithms. An online user study further validated the effectiveness of the recommendations, with significant increases observed in click-through rate, view completion rate, and user satisfaction scores relative to the platform's existing system. These results underscore the value of incorporating implicit feedback and social network analysis for video recommendations. The key contributions of this research include a novel video recommendation framework that integrates implicit user data and social network analysis, the use of centrality measures and modularity-based clustering, an ego-centric ranking approach, and rigorous offline and online evaluation demonstrating superior performance compared to existing techniques. This study opens new avenues for enhancing video recommendations and user engagement in VOD platforms."}, "https://arxiv.org/abs/2310.08909": {"title": "Evading Community Detection via Counterfactual Neighborhood Search", "link": "https://arxiv.org/abs/2310.08909", "description": "arXiv:2310.08909v2 Announce Type: replace \nAbstract: Community detection techniques are useful for social media platforms to discover tightly connected groups of users who share common interests. However, this functionality often comes at the expense of potentially exposing individuals to privacy breaches by inadvertently revealing their tastes or preferences. Therefore, some users may wish to preserve their anonymity and opt out of community detection for various reasons, such as affiliation with political or religious organizations, without leaving the platform. In this study, we address the challenge of community membership hiding, which involves strategically altering the structural properties of a network graph to prevent one or more nodes from being identified by a given community detection algorithm. We tackle this problem by formulating it as a constrained counterfactual graph objective, and we solve it via deep reinforcement learning. Extensive experiments demonstrate that our method outperforms existing baselines, striking the best balance between accuracy and cost."}, "https://arxiv.org/abs/2401.09368": {"title": "Feature-aware ultra-low dimensional reduction of real networks", "link": "https://arxiv.org/abs/2401.09368", "description": "arXiv:2401.09368v2 Announce Type: replace \nAbstract: In existing models and embedding methods of networked systems, node features describing their qualities are usually overlooked in favor of focusing solely on node connectivity. This study introduces $FiD$-Mercator, a model-based ultra-low dimensional reduction technique that integrates node features with network structure to create $D$-dimensional maps of complex networks in a hyperbolic space. This embedding method efficiently uses features as an initial condition, guiding the search of nodes' coordinates towards an optimal solution. The research reveals that downstream task performance improves with the correlation between network connectivity and features, emphasizing the importance of such correlation for enhancing the description and predictability of real networks. Simultaneously, hyperbolic embedding's ability to reproduce local network properties remains unaffected by the inclusion of features. The findings highlight the necessity for developing network embedding techniques capable of exploiting such correlations to optimize both network structure and feature association jointly in the future."}, "https://arxiv.org/abs/2311.11282": {"title": "Individual misinformation tagging reinforces echo chambers; Collective tagging does not", "link": "https://arxiv.org/abs/2311.11282", "description": "arXiv:2311.11282v2 Announce Type: replace-cross \nAbstract: Fears about the destabilizing impact of misinformation online have motivated individuals and platforms to respond. Individuals have become empowered to challenge others' online claims with fact-checks in pursuit of a healthier information ecosystem and to break down echo chambers of self-reinforcing opinion. Using Twitter data, here we show the consequences of individual misinformation tagging: tagged posters had explored novel political information and expanded topical interests immediately prior, but being tagged caused posters to retreat into information bubbles. These unintended consequences were softened by a collective verification system for misinformation moderation. In Twitter's new platform, Community Notes, misinformation tagging was peer-reviewed by other fact-checkers before exposure to the poster. With collective misinformation tagging, posters were less likely to retreat from diverse information engagement. Detailed comparison suggests differences in toxicity, sentiment, readability, and delay in individual versus collective misinformation tagging messages. These findings provide evidence for differential impacts from individual versus collective moderation strategies on the diversity of information engagement and mobility across the information ecosystem."}, "https://arxiv.org/abs/2311.11344": {"title": "Unveiling Deception: Establishing a Taxonomic Framework for Disinformation within Scientific Discourse", "link": "https://arxiv.org/abs/2311.11344", "description": "arXiv:2311.11344v2 Announce Type: replace-cross \nAbstract: Disinformation spreads among the public and in scientific discourse through the actions of individuals, organizations, and governments that distort scholarly communications, media narratives, and institutional trust. This taxonomy introduces a structured framework and specialized set of definitions to elucidate the key participants, platforms, and strategies employed in the propagation of disinformation. Enhanced comprehension of the mechanisms and pathways of scientific disinformation equips journalists and policymakers with the tools necessary to more effectively recognize and address these issues. The authors developed this taxonomy of disinformation through a multi-faceted approach, encompassing a literature review, expert review, and case study analysis. The literature review revealed a scarcity of taxonomical models amidst prevalent algorithmic detection studies. Subsequently, an expert review process refined our taxonomy through collaborative analysis of twenty-two cases of identified disinformation, categorized by their methods, motives, and impacts. Finally, we validated and fine-tuned our taxonomy through detailed case studies of twelve diverse disinformation instances, assessing the taxonomy's effectiveness in capturing the essential characteristics of each case and making necessary adjustments to ensure its relevance and accuracy in real-world applications."}, "https://arxiv.org/abs/2311.11456": {"title": "The Arrow of Time is Alive and Well but Forbidden Under the Received View of Physics", "link": "https://arxiv.org/abs/2311.11456", "description": "arXiv:2311.11456v2 Announce Type: replace-cross \nAbstract: This essay offers a meta-level analysis in the sociology and history of physics in the context of the so-called \"Arrow of Time Problem\" or \"Two Times Problem,\" which asserts that the empirically observed directionality of time is in conflict with physical theory. I argue that there is actually no necessary conflict between physics and the arrow of time, and that the observed directionality of time is perfectly consistent with physics unconstrained by certain optional metaphysical, epistemological and methodological beliefs and practices characterizing the conventional or Received View."}, "https://arxiv.org/abs/2402.11804": {"title": "LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs", "link": "https://arxiv.org/abs/2402.11804", "description": "arXiv:2402.11804v2 Announce Type: replace-cross \nAbstract: Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-resource KG datasets and find that ProLINK outperforms previous methods in three-shot, one-shot, and zero-shot reasoning tasks, exhibiting average performance improvements by 20%, 45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong robustness for various LLM promptings as well as full-shot scenarios."}, "https://arxiv.org/abs/2404.17128": {"title": "Simple Network Mechanism Leads to Quasi-Real Brain Activation Patterns with Drosophila Connectome", "link": "https://arxiv.org/abs/2404.17128", "description": "arXiv:2404.17128v2 Announce Type: replace-cross \nAbstract: Considering the high computational demands of most methods, using network communication models to simulate the brain is a more economical way. However, there is still insufficient evidence that they can effectively replicate the brains' real activation patterns. Moreover, it remains unclear whether actual network structures are crucial in simulating intelligence. Addressing these issues, we propose a large scale network communication model based on simple rules and design criteria to assess the differences between network models and real situations. To enhance the connection with the real world, we also incorporate an improved neuron dynamic model. We conduct research on the biggest adult Drosophila connectome data set. Experimental results show significant activation in neurons that should respond to stimulus and slight activation in irrelevant ones, which we call quasi-real activation pattern. Besides, when changing the network structure, the quasi-activation patterns disappear. Interestingly, activation regions have shorter network distances to their input neurons, implying that the network structure (not spatial distance) is the core to form brain functionality. In addition, giving input neurons a unilateral stimulus, we observe a bilateral response, which is consistent with reality. Then we find that both hemispheres have extremely similar statistical indicators. We also develop real-time 3D large spatial network visualization software to observe experimental phenomena, filling the software gap. This research reveals network models' power: it can reach the quasi-activation pattern with simple rules. Besides, it proves network structure matters in brain activity pattern generation. Future research could fully simulate brain behavior through network models, paving the way for artificial intelligence by developing new propagation rules and optimizing link weights."}, "https://arxiv.org/abs/2406.06617": {"title": "Collaborative Team Recognition: A Core Plus Extension Structure", "link": "https://arxiv.org/abs/2406.06617", "description": "arXiv:2406.06617v1 Announce Type: new \nAbstract: Scientific collaboration is a significant behavior in knowledge creation and idea exchange. To tackle large and complex research questions, a trend of team formation has been observed in recent decades. In this study, we focus on recognizing collaborative teams and exploring inner patterns using scholarly big graph data. We propose a collaborative team recognition (CORE) model with a \"core + extension\" team structure to recognize collaborative teams in large academic networks. In CORE, we combine an effective evaluation index called the collaboration intensity index with a series of structural features to recognize collaborative teams in which members are in close collaboration relationships. Then, CORE is used to guide the core team members to their extension members. CORE can also serve as the foundation for team-based research. The simulation results indicate that CORE reveals inner patterns of scientific collaboration: senior scholars have broad collaborative relationships and fixed collaboration patterns, which are the underlying mechanisms of team assembly. The experimental results demonstrate that CORE is promising compared with state-of-the-art methods."}, "https://arxiv.org/abs/2406.06618": {"title": "PANDORA: Deep graph learning based COVID-19 infection risk level forecasting", "link": "https://arxiv.org/abs/2406.06618", "description": "arXiv:2406.06618v1 Announce Type: new \nAbstract: COVID-19 as a global pandemic causes a massive disruption to social stability that threatens human life and the economy. Policymakers and all elements of society must deliver measurable actions based on the pandemic's severity to minimize the detrimental impact of COVID-19. A proper forecasting system is arguably important to provide an early signal of the risk of COVID-19 infection so that the authorities are ready to protect the people from the worst. However, making a good forecasting model for infection risks in different cities or regions is not an easy task, because it has a lot of influential factors that are difficult to be identified manually. To address the current limitations, we propose a deep graph learning model, called PANDORA, to predict the infection risks of COVID-19, by considering all essential factors and integrating them into a geographical network. The framework uses geographical position relations and transportation frequency as higher-order structural properties formulated by higher-order network structures (i.e., network motifs). Moreover, four significant node attributes (i.e., multiple features of a particular area, including climate, medical condition, economy, and human mobility) are also considered. We propose three different aggregators to better aggregate node attributes and structural features, namely, Hadamard, Summation, and Connection. Experimental results over real data show that PANDORA outperforms the baseline method with higher accuracy and faster convergence speed, no matter which aggregator is chosen. We believe that PANDORA using deep graph learning provides a promising approach to get superior performance in infection risk level forecasting and help humans battle the COVID-19 crisis."}, "https://arxiv.org/abs/2406.06658": {"title": "Link Prediction in Bipartite Networks", "link": "https://arxiv.org/abs/2406.06658", "description": "arXiv:2406.06658v1 Announce Type: new \nAbstract: Bipartite networks serve as highly suitable models to represent systems involving interactions between two distinct types of entities, such as online dating platforms, job search services, or ecommerce websites. These models can be leveraged to tackle a number of tasks, including link prediction among the most useful ones, especially to design recommendation systems. However, if this task has garnered much interest when conducted on unipartite (i.e. standard) networks, it is far from being the case for bipartite ones. In this study, we address this gap by performing an experimental comparison of 19 link prediction methods able to handle bipartite graphs. Some come directly from the literature, and some are adapted by us from techniques originally designed for unipartite networks. We also propose to repurpose recommendation systems based on graph convolutional networks (GCN) as a novel link prediction solution for bipartite networks. To conduct our experiments, we constitute a benchmark of 3 real-world bipartite network datasets with various topologies. Our results indicate that GCN-based personalized recommendation systems, which have received significant attention in recent years, can produce successful results for link prediction in bipartite networks. Furthermore, purely heuristic metrics that do not rely on any learning process, like the Structural Perturbation Method (SPM), can also achieve success."}, "https://arxiv.org/abs/2406.06662": {"title": "Proximity Matters: Analyzing the Role of Geographical Proximity in Shaping AI Research Collaborations", "link": "https://arxiv.org/abs/2406.06662", "description": "arXiv:2406.06662v1 Announce Type: new \nAbstract: The role of geographical proximity in facilitating inter-regional or inter-organizational collaborations has been studied thoroughly in recent years. However, the effect of geographical proximity on forming scientific collaborations at the individual level still needs to be addressed. Using publication data in the field of artificial intelligence from 2001 to 2019, in this work, the effect of geographical proximity on the likelihood of forming future scientific collaborations among researchers is studied. In addition, the interaction between geographical and network proximities is examined to see whether network proximity can substitute geographical proximity in encouraging long-distance scientific collaborations. Employing conventional and machine learning techniques, our results suggest that geographical distance impedes scientific collaboration at the individual level despite the tremendous improvements in transportation and communication technologies during recent decades. Moreover, our findings show that the effect of network proximity on the likelihood of scientific collaboration increases with geographical distance, implying that network proximity can act as a substitute for geographical proximity."}, "https://arxiv.org/abs/2406.06717": {"title": "Analyzing user archetypes in Singapore's Telegram groups on COVID-19 and climate change", "link": "https://arxiv.org/abs/2406.06717", "description": "arXiv:2406.06717v1 Announce Type: new \nAbstract: Social media platforms, particularly Telegram, play a pivotal role in shaping public perceptions and opinions on global and national issues. Unlike traditional news media, Telegram allows for the proliferation of user-generated content with minimal oversight, making it a significant venue for the spread of controversial and misinformative content. During the COVID-19 pandemic, Telegram's popularity surged in Singapore, a country with one of the highest rates of social media use globally. We leverage Singapore-based Telegram data to analyze information flows within groups focused on COVID-19 and climate change. Using k-means clustering, we identified distinct user archetypes, including Skeptic, Engaged Advocate, Observer, and Analyst, each contributing uniquely to the discourse. We developed a model to classify users into these clusters (Precision: Climate change: 0.99; COVID-19: 0.95). By identifying these user archetypes and examining their contributions to information dissemination, we sought to uncover patterns to inform effective strategies for combating misinformation and enhancing public discourse on pressing global issues."}, "https://arxiv.org/abs/2406.06814": {"title": "Temporal Link Prediction in Social Networks Based on Agent Behavior Synchrony and a Cognitive Mechanism", "link": "https://arxiv.org/abs/2406.06814", "description": "arXiv:2406.06814v1 Announce Type: new \nAbstract: Temporality, a crucial characteristic in the formation of social relationships, was used to quantify the long-term time effects of networks for link prediction models, ignoring the heterogeneity of time effects on different time scales. In this work, we propose a novel approach to link prediction in temporal networks, extending existing methods with a cognitive mechanism that captures the dynamics of the interactions. Our approach computes the weight of the edges and their change over time, similar to memory traces in the human brain, by simulating the process of forgetting and strengthening connections depending on the intensity of interactions. We utilized five ground-truth datasets, which were used to predict social ties, missing events, and potential links. We found: (a) the cognitive mechanism enables more accurate capture of the heterogeneity of the temporal effect, leading to an average precision improvement of 9\\% compared to baselines with competitive AUC. (b) the local structure and synchronous agent behavior contribute differently to different types of datasets. (c) appropriately increasing the time intervals, which may reduce the negative impact from noise when dividing time windows to calculate the behavioral synchrony of agents, is effective for link prediction tasks."}, "https://arxiv.org/abs/2406.06889": {"title": "Universal spatial inflation of human mobility", "link": "https://arxiv.org/abs/2406.06889", "description": "arXiv:2406.06889v1 Announce Type: new \nAbstract: Understanding the interplay between egocentric preference and urban structure in shaping human mobility has profound implications for improving epidemic intervention, social equity, and urban resilience. However, numerous existing studies either solely identify the egocentric preferences -- the anchoring effects from home -- or the impact of hierarchical urban structures. Here, we propose a network-based approach to present human mobility in both spatial and topological aspects within the urban system, using cell phone trajectory data from millions of users across three countries. By segmenting mobility trajectories into modules and examining their overlap with urban scales, we have observed the inflation law that the geospatial extent of these modules increases sub-linearly with their distance from home. Moreover, the egocentric preference for higher urban levels leads to this increase. This universal finding indicates that home-based preferences distort the hierarchical scales of human mobility in the urban environment, regardless of demographics or geography."}, "https://arxiv.org/abs/2406.06912": {"title": "Controlling noisy herds", "link": "https://arxiv.org/abs/2406.06912", "description": "arXiv:2406.06912v1 Announce Type: new \nAbstract: The wisdom of the crowd breaks down in small groups. While large flocks exhibit swarm intelligence to evade predators, small groups display erratic behavior, oscillating between unity and discord. We investigate these dynamics using small groups of sheep controlled by shepherd dogs in century-old sheepdog trials, proposing a two-parameter stochastic dynamic framework. Our model employs pressure (stimulus intensity) and lightness (response isotropy) to simulate herding and shedding behaviors. Light sheep rapidly achieve a stable herding state, while heavy sheep exhibit intermittent herding and orthogonal alignment to the dog. High response isotropy enhances group cohesion but complicates group splitting. We construct a unified phase diagram for sheep behavior, identifying three regimes (fleeing, flocking, and grazing) based on group size and stimulus specificity. Increasing stimulus specificity shifts small group behavior from grazing to fleeing, while larger groups exhibit flocking. This transition underscores the challenge of controlling small indecisive collectives. Introducing the Indecisive Collective Algorithm (ICA), we show that deliberate indecisiveness and stochasticity improve control efficiency. ICA outperforms traditional averaging-based algorithms in high-noise settings and excels in tasks requiring group splitting. Our study offers a foundational framework for controlling small, indecisive groups, applicable to biochemical reactions, cell populations, and opinion dynamics."}, "https://arxiv.org/abs/2406.07293": {"title": "Exploring Cognitive Bias Triggers in COVID-19 Misinformation Tweets: A Bot vs", "link": "https://arxiv.org/abs/2406.07293", "description": "arXiv:2406.07293v1 Announce Type: new \nAbstract: During the COVID-19 pandemic, the proliferation of misinformation on social media has been rapidly increasing. Automated Bot authors are believed to be significant contributors of this surge. It is hypothesized that Bot authors deliberately craft online misinformation aimed at triggering and exploiting human cognitive biases, thereby enhancing tweet engagement and persuasive influence. This study investigates this hypothesis by studying triggers of biases embedded in Bot-authored misinformation and comparing them with their counterparts, Human-authored misinformation. We complied a Misinfo Dataset that contains COVID-19 vaccine-related misinformation tweets annotated by author identities, Bots vs Humans, from Twitter during the vaccination period from July 2020 to July 2021. We developed an algorithm to computationally automate the extraction of triggers for eight cognitive biase. Our analysis revealed that the Availability Bias, Cognitive Dissonance, and Confirmation Bias were most commonly present in misinformation, with Bot-authored tweets exhibiting a greater prevalence, with distinct patterns in utilizing bias triggers between Humans and Bots. We further linked these bias triggers with engagement metrics, inferring their potential influence on tweet engagement and persuasiveness. Overall, our findings indicate that bias-triggering tactics have been more influential on Bot-authored tweets than Human-authored tweets. While certain bias triggers boosted engagement for Bot-authored tweets, some other bias triggers unexpectedly decreased it. Conversely, triggers of most biases appeared to be unrelated to the engagement of Human-authored tweets. Our work sheds light on the differential utilization and effect of persuasion strategies between Bot-authored and Human-authored misinformation from the lens of human biases, offering insights for the development of effective counter-measures."}, "https://arxiv.org/abs/2406.06770": {"title": "Optimal control for a SIR model with limited hospitalised patients", "link": "https://arxiv.org/abs/2406.06770", "description": "arXiv:2406.06770v1 Announce Type: cross \nAbstract: This paper analyses the optimal control of infectious disease propagation using a classic susceptible-infected-recovered (SIR) model characterised by permanent immunity and the absence of available vaccines. The control is performed over a time-dependent mean reproduction number, in order to minimise the cumulative number of ever-infected individuals (recovered), under different constraints. We consider constraints on isolation measures ranging from partial lockdown to non-intervention, as well as the social and economic costs associated with such isolation, and the capacity limitations of intensive care units that limits the number of infected individuals to a maximum allowed value. We rigorously derive an optimal quarantine strategy based on necessary optimality conditions. The obtained optimal strategy is of a boundary-bang type, comprising three phases: an initial phase with no intervention, a second phase maintaining the infected population at its maximum possible value, and a final phase of partial lockdown applied over a single interval. The optimal policy is further refined by optimising the transition times between these phases. We show that these results are in excellent agreement with the numerical solution of the problem."}, "https://arxiv.org/abs/2406.06934": {"title": "Decentralized Social Networks and the Future of Free Speech Online", "link": "https://arxiv.org/abs/2406.06934", "description": "arXiv:2406.06934v1 Announce Type: cross \nAbstract: Decentralized social networks like Mastodon and BlueSky are trending topics that have drawn much attention and discussion in recent years. By devolving powers from the central node to the end users, decentralized social networks aim to cure existing pathologies on the centralized platforms and have been viewed by many as the future of the Internet. This article critically and systematically assesses the decentralization project's prospect for communications online. It uses normative theories of free speech to examine whether and how the decentralization design could facilitate users' freedom of expression online. The analysis shows that both promises and pitfalls exist, highlighting the importance of value-based design in this area. Two most salient issues for the design of the decentralized networks are: how to balance the decentralization ideal with constant needs of centralization on the network, and how to empower users to make them truly capable of exercising their control. The article then uses some design examples, such as the shared blocklist and the opt-in search function, to illustrate the value considerations underlying the design choices. Some tentative proposals for law and policy interventions are offered to better facilitate the design of the new network. Rather than providing clear answers, the article seeks to map the value implications of the design choices, highlight the stakes, and point directions for future research."}, "https://arxiv.org/abs/2406.06958": {"title": "Turning the Tide on Dark Pools? Towards Multi-Stakeholder Vulnerability Notifications in the Ad-Tech Supply Chain", "link": "https://arxiv.org/abs/2406.06958", "description": "arXiv:2406.06958v1 Announce Type: cross \nAbstract: Online advertising relies on a complex and opaque supply chain that involves multiple stakeholders, including advertisers, publishers, and ad-networks, each with distinct and sometimes conflicting incentives. Recent research has demonstrated the existence of ad-tech supply chain vulnerabilities such as dark pooling, where low-quality publishers bundle their ad inventory with higher-quality ones to mislead advertisers. We investigate the effectiveness of vulnerability notification campaigns aimed at mitigating dark pooling. Prior research on vulnerability notifications has primarily focused on single-stakeholder scenarios, and it is unclear whether vulnerability notifications can be effective in the multi-stakeholder ad-tech supply chain. We implement an automated vulnerability notification pipeline to systematically evaluate the responsiveness of various stakeholders, including publishers, ad-networks, and advertisers to vulnerability notifications by academics and activists. Our nine-month long multi-stakeholder notification study shows that notifications are an effective method for reducing dark pooling vulnerabilities in the online advertising ecosystem, especially when targeted towards ad-networks. Further, the sender reputation does not impact responses to notifications from activists and academics in a statistically different way. In addition to being the first notification study targeting the online advertising ecosystem, we are also the first to study multi-stakeholder context in vulnerability notifications."}, "https://arxiv.org/abs/2406.07016": {"title": "Delving into ChatGPT usage in academic writing through excess vocabulary", "link": "https://arxiv.org/abs/2406.07016", "description": "arXiv:2406.07016v1 Announce Type: cross \nAbstract: Recent large language models (LLMs) can generate and revise text with human-level performance, and have been widely commercialized in systems like ChatGPT. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists have been using them to assist their scholarly writing. How wide-spread is LLM usage in the academic literature currently? To answer this question, we use an unbiased, large-scale approach, free from any assumptions on academic LLM usage. We study vocabulary changes in 14 million PubMed abstracts from 2010-2024, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. Our analysis based on excess words usage suggests that at least 10% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, and was as high as 30% for some PubMed sub-corpora. We show that the appearance of LLM-based writing assistants has had an unprecedented impact in the scientific literature, surpassing the effect of major world events such as the Covid pandemic."}, "https://arxiv.org/abs/2406.07155": {"title": "Scaling Large-Language-Model-based Multi-Agent Collaboration", "link": "https://arxiv.org/abs/2406.07155", "description": "arXiv:2406.07155v1 Announce Type: cross \nAbstract: Pioneering advancements in large language model-powered agents have underscored the design pattern of multi-agent collaboration, demonstrating that collective intelligence can surpass the capabilities of each individual. Inspired by the neural scaling law, which posits that increasing neurons leads to emergent abilities, this study investigates whether a similar principle applies to increasing agents in multi-agent collaboration. Technically, we propose multi-agent collaboration networks (MacNet), which utilize directed acyclic graphs to organize agents and streamline their interactive reasoning via topological ordering, with solutions derived from their dialogues. Extensive experiments show that MacNet consistently outperforms baseline models, enabling effective agent collaboration across various network topologies and supporting cooperation among more than a thousand agents. Notably, we observed a small-world collaboration phenomenon, where topologies resembling small-world properties achieved superior performance. Additionally, we identified a collaborative scaling law, indicating that normalized solution quality follows a logistic growth pattern as scaling agents, with collaborative emergence occurring much earlier than previously observed instances of neural emergence. The code and data will be available at https://github.com/OpenBMB/ChatDev."}, "https://arxiv.org/abs/2406.07210": {"title": "The green hydrogen ambition and implementation gap", "link": "https://arxiv.org/abs/2406.07210", "description": "arXiv:2406.07210v1 Announce Type: cross \nAbstract: Green hydrogen is critical for decarbonising hard-to-electrify sectors, but faces high costs and investment risks. Here we define and quantify the green hydrogen ambition and implementation gap, showing that meeting hydrogen expectations will remain challenging despite surging announcements of projects and subsidies. Tracking 137 projects over three years, we identify a wide 2022 implementation gap with only 2% of global capacity announcements finished on schedule. In contrast, the 2030 ambition gap towards 1.5{\\deg}C scenarios is gradually closing as the announced project pipeline has nearly tripled to 441 GW within three years. However, we estimate that, without carbon pricing, realising all these projects would require global subsidies of \\$1.6 trillion (\\$1.2 - 2.6 trillion range), far exceeding announced subsidies. Given past and future implementation gaps, policymakers must prepare for prolonged green hydrogen scarcity. Policy support needs to secure hydrogen investments, but should focus on applications where hydrogen is indispensable."}, "https://arxiv.org/abs/2406.07353": {"title": "Toxic Memes: A Survey of Computational Perspectives on the Detection and Explanation of Meme Toxicities", "link": "https://arxiv.org/abs/2406.07353", "description": "arXiv:2406.07353v1 Announce Type: cross \nAbstract: Internet memes, channels for humor, social commentary, and cultural expression, are increasingly used to spread toxic messages. Studies on the computational analyses of toxic memes have significantly grown over the past five years, and the only three surveys on computational toxic meme analysis cover only work published until 2022, leading to inconsistent terminology and unexplored trends. Our work fills this gap by surveying content-based computational perspectives on toxic memes, and reviewing key developments until early 2024. Employing the PRISMA methodology, we systematically extend the previously considered papers, achieving a threefold result. First, we survey 119 new papers, analyzing 158 computational works focused on content-based toxic meme analysis. We identify over 30 datasets used in toxic meme analysis and examine their labeling systems. Second, after observing the existence of unclear definitions of meme toxicity in computational works, we introduce a new taxonomy for categorizing meme toxicity types. We also note an expansion in computational tasks beyond the simple binary classification of memes as toxic or non-toxic, indicating a shift towards achieving a nuanced comprehension of toxicity. Third, we identify three content-based dimensions of meme toxicity under automatic study: target, intent, and conveyance tactics. We develop a framework illustrating the relationships between these dimensions and meme toxicities. The survey analyzes key challenges and recent trends, such as enhanced cross-modal reasoning, integrating expert and cultural knowledge, the demand for automatic toxicity explanations, and handling meme toxicity in low-resource languages. Also, it notes the rising use of Large Language Models (LLMs) and generative AI for detecting and generating toxic memes. Finally, it proposes pathways for advancing toxic meme detection and interpretation."}, "https://arxiv.org/abs/2401.08428": {"title": "Herd Behaviour in Public Goods Games", "link": "https://arxiv.org/abs/2401.08428", "description": "arXiv:2401.08428v3 Announce Type: replace \nAbstract: The problem of free-riding arises when individuals benefit from a shared resource, service, or public good without contributing proportionately to its provision. This conduct often leads to a collective action problem, as individuals pursue personal gains while relying on the contributions of others. In this study, we present a Bayesian inference model to elucidate the behaviour of participants in a Public Goods Game, a conceptual framework that captures the essence of the free-riding problem. Here, individuals possess information on the distribution of group donations to the public good. Our model is grounded in the premise that individuals strive to harmonise their actions with the group's donation patterns. Our model is able to replicate behavioural patterns that resemble those observed in experiments with midsized groups (100 people), but fails to replicate those for larger scales (1000 people). Our results suggest that, in these scenarios, humans prefer imitation and convergence behaviours over profit optimisation. These insights contribute to understanding how cooperation is achieved through alignment with group behaviour."}, "https://arxiv.org/abs/2403.00311": {"title": "Enhancing social cohesion with cooperative bots in societies of greedy, mobile individuals", "link": "https://arxiv.org/abs/2403.00311", "description": "arXiv:2403.00311v3 Announce Type: replace \nAbstract: Addressing collective issues in social development requires a high level of social cohesion, characterized by cooperation and close social connections. However, social cohesion is challenged by selfish, greedy individuals. With the advancement of artificial intelligence (AI), the dynamics of human-machine hybrid interactions introduce new complexities in fostering social cohesion. This study explores the impact of simple bots on social cohesion from the perspective of human-machine hybrid populations within network. By investigating collective self-organizing movement during migration, results indicate that cooperative bots can promote cooperation, facilitate individual aggregation, and thereby enhance social cohesion. The random exploration movement of bots can break the frozen state of greedy population, help to separate defectors in cooperative clusters, and promote the establishment of cooperative clusters. However, the presence of defective bots can weaken social cohesion, underscoring the importance of carefully designing bot behavior. Our research reveals the potential of bots in guiding social self-organization and provides insights for enhancing social cohesion in the era of human-machine interaction within social networks."}, "https://arxiv.org/abs/2403.10543": {"title": "Mitigating Oversmoothing Through Reverse Process of GNNs for Heterophilic Graphs", "link": "https://arxiv.org/abs/2403.10543", "description": "arXiv:2403.10543v2 Announce Type: replace \nAbstract: Graph Neural Network (GNN) resembles the diffusion process, leading to the over-smoothing of learned representations when stacking many layers. Hence, the reverse process of message passing can produce the distinguishable node representations by inverting the forward message propagation. The distinguishable representations can help us to better classify neighboring nodes with different labels, such as in heterophilic graphs. In this work, we apply the design principle of the reverse process to the three variants of the GNNs. Through the experiments on heterophilic graph data, where adjacent nodes need to have different representations for successful classification, we show that the reverse process significantly improves the prediction performance in many cases. Additional analysis reveals that the reverse mechanism can mitigate the over-smoothing over hundreds of layers. Our code is available at https://github.com/ml-postech/reverse-gnn."}, "https://arxiv.org/abs/2309.05638": {"title": "Errors are Robustly Tamed in Cumulative Knowledge Processes", "link": "https://arxiv.org/abs/2309.05638", "description": "arXiv:2309.05638v3 Announce Type: replace-cross \nAbstract: We study processes of societal knowledge accumulation, where the validity of a new unit of knowledge depends both on the correctness of its derivation and on the validity of the units it depends on. A fundamental question in this setting is: If a constant fraction of the new derivations is wrong, can investing a constant fraction, bounded away from one, of effort ensure that a constant fraction of knowledge in society is valid? Ben-Eliezer, Mikulincer, Mossel, and Sudan (ITCS 2023) introduced a concrete probabilistic model to analyze such questions and showed an affirmative answer to this question. Their study, however, focuses on the simple case where each new unit depends on just one existing unit, and units attach according to a $\\textit{preferential attachment rule}$.\n  In this work, we consider much more general families of cumulative knowledge processes, where new units may attach according to varied attachment mechanisms and depend on multiple existing units. We also allow a (random) fraction of insertions of adversarial nodes.\n  We give a robust affirmative answer to the above question by showing that for $\\textit{all}$ of these models, as long as many of the units follow simple heuristics for checking a bounded number of units they depend on, all errors will be eventually eliminated. Our results indicate that preserving the quality of large interdependent collections of units of knowledge is feasible, as long as careful but not too costly checks are performed when new units are derived/deposited."}, "https://arxiv.org/abs/2406.07574": {"title": "Biharmonic Distance of Graphs and its Higher-Order Variants: Theoretical Properties with Applications to Centrality and Clustering", "link": "https://arxiv.org/abs/2406.07574", "description": "arXiv:2406.07574v1 Announce Type: new \nAbstract: Effective resistance is a distance between vertices of a graph that is both theoretically interesting and useful in applications. We study a variant of effective resistance called the biharmonic distance. While the effective resistance measures how well-connected two vertices are, we prove several theoretical results supporting the idea that the biharmonic distance measures how important an edge is to the global topology of the graph. Our theoretical results connect the biharmonic distance to well-known measures of connectivity of a graph like its total resistance and sparsity. Based on these results, we introduce two clustering algorithms using the biharmonic distance. Finally, we introduce a further generalization of the biharmonic distance that we call the $k$-harmonic distance. We empirically study the utility of biharmonic and $k$-harmonic distance for edge centrality and graph clustering."}, "https://arxiv.org/abs/2406.07805": {"title": "Wiser than the Wisest of Crowds: The Asch Effect Revisited under Friedkin-Johnsen Opinion Dynamics", "link": "https://arxiv.org/abs/2406.07805", "description": "arXiv:2406.07805v1 Announce Type: new \nAbstract: In 1907, Sir Francis Galton independently asked 787 villagers to estimate the weight of an ox. Although none of them guessed the exact weight, the average estimate was remarkably accurate. This phenomenon is known as wisdom of crowds. In a clever experiment, Asch employed actors to demonstrate the human tendency to conform to others' opinions. The question we ask is: what would Sir Francis Galton have observed if Asch had interfered by employing actors? Would the wisdom of crowds become even wiser or not? The problem becomes intriguing when considering the inter-connectedness of the villagers, which is the central theme of this work. We examine a scenario where $n$ agents are interconnected and influence each other. The average of their opinions provides an estimator of a certain quality for some unknown quantity. How can one improve or reduce the quality of the original estimator in terms of the MSE by utilizing Asch's strategy of hiring a few stooges?\n  We present a new formulation of this problem, assuming that nodes adjust their opinions according to the Friedkin-Johnsen opinion dynamics. We demonstrate that selecting $k$ stooges for maximizing and minimizing the MSE is NP-hard. We also demonstrate that our formulation is closely related to maximizing or minimizing polarization and show NP-hardness. We propose an efficient greedy heuristic that scales to large networks and test our algorithm on synthetic and real-world datasets. Although MSE and polarization objectives differ, we find in practice that maximizing polarization often yields solutions that are nearly optimal for minimizing the wisdom of crowds in terms of MSE. Our analysis of real-world data reveals that even a small number of stooges can significantly influence the conversation on the war in Ukraine, resulting in a relative increase of the MSE of 207.80% (maximization) or a decrease of 50.62% (minimization)."}, "https://arxiv.org/abs/2406.07964": {"title": "Political Leaning Inference through Plurinational Scenarios", "link": "https://arxiv.org/abs/2406.07964", "description": "arXiv:2406.07964v1 Announce Type: new \nAbstract: Social media users express their political preferences via interaction with other users, by spontaneous declarations or by participation in communities within the network. This makes a social network such as Twitter a valuable data source to study computational science approaches to political learning inference. In this work we focus on three diverse regions in Spain (Basque Country, Catalonia and Galicia) to explore various methods for multi-party categorization, required to analyze evolving and complex political landscapes, and compare it with binary left-right approaches. We use a two-step method involving unsupervised user representations obtained from the retweets and their subsequent use for political leaning detection. Comprehensive experimentation on a newly collected and curated dataset comprising labeled users and their interactions demonstrate the effectiveness of using Relational Embeddings as representation method for political ideology detection in both binary and multi-party frameworks, even with limited training data. Finally, data visualization illustrates the ability of the Relational Embeddings to capture intricate intra-group and inter-group political affinities."}, "https://arxiv.org/abs/2406.07993": {"title": "How social reinforcement learning can lead to metastable polarisation and the voter model", "link": "https://arxiv.org/abs/2406.07993", "description": "arXiv:2406.07993v1 Announce Type: new \nAbstract: Previous explanations for the persistence of polarization of opinions have typically included modelling assumptions that predispose the possibility of polarization (e.g.\\ repulsive interactions). An exception is recent research showing that polarization is stable when agents form their opinions using reinforcement learning.\n  We show that the polarization observed in this model is not stable, but exhibits consensus asymptotically with probability one. By constructing a link between the reinforcement learning model and the voter model, we argue that the observed polarization is metastable. Finally, we show that a slight modification in the learning process of the agents changes the model from being non-ergodic to being ergodic.\n  Our results show that reinforcement learning may be a powerful method for modelling polarization in opinion dynamics, but that the tools appropriate for analysing such models crucially depend on the properties of the resulting systems. Properties which are determined by the details of the learning process."}, "https://arxiv.org/abs/2406.08034": {"title": "Strong and Weak Random Walks on Signed Networks", "link": "https://arxiv.org/abs/2406.08034", "description": "arXiv:2406.08034v1 Announce Type: new \nAbstract: Random walks play an important role in probing the structure of complex networks. On traditional networks, they can be used to extract community structure, understand node centrality, perform link prediction, or capture the similarity between nodes. On signed networks, where the edge weights can be either positive or negative, it is non-trivial to design a random walk which can be used to extract information about the signed structure of the network, in particular the ability to partition the graph into communities with positive edges inside and negative edges in between. Prior works on signed network random walks focus on the case where there are only two such communities (strong balance), which is rarely the case in empirical networks. In this paper, we propose a signed network random walk which can capture the structure of a network with more than two such communities (weak balance). The walk results in a similarity matrix which can be used to cluster the nodes into antagonistic communities. We compare the characteristics of the so-called strong and weak random walks, in terms of walk length and stationarity. We show through a series of experiments on synthetic and empirical networks that the similarity matrix based on weak walks can be used for both unsupervised and semi-supervised clustering, outperforming the same similarity matrix based on strong walks when the graph has more than two communities, or exhibits asymmetry in the density of links. These results suggest that other random-walk based algorithms for signed networks could be improved simply by running them with weak walks instead of strong walks."}, "https://arxiv.org/abs/2406.08084": {"title": "Characterizing and Detecting Propaganda-Spreading Accounts on Telegram", "link": "https://arxiv.org/abs/2406.08084", "description": "arXiv:2406.08084v1 Announce Type: new \nAbstract: Information-based attacks on social media, such as disinformation campaigns and propaganda, are emerging cybersecurity threats. The security community has focused on countering these threats on social media platforms like X and Reddit. However, they also appear in instant-messaging social media platforms such as WhatsApp, Telegram, and Signal. In these platforms information-based attacks primarily happen in groups and channels, requiring manual moderation efforts by channel administrators. We collect, label, and analyze a large dataset of more than 17 million Telegram comments and messages. Our analysis uncovers two independent, coordinated networks that spread pro-Russian and pro-Ukrainian propaganda, garnering replies from real users. We propose a novel mechanism for detecting propaganda that capitalizes on the relationship between legitimate user messages and propaganda replies and is tailored to the information that Telegram makes available to moderators. Our method is faster, cheaper, and has a detection rate (97.6%) 11.6 percentage points higher than human moderators after seeing only one message from an account. It remains effective despite evolving propaganda."}, "https://arxiv.org/abs/2406.08190": {"title": "CrowdEgress: A Multi-Agent Simulation Platform for Pedestrian Crowd", "link": "https://arxiv.org/abs/2406.08190", "description": "arXiv:2406.08190v1 Announce Type: new \nAbstract: This article introduces a simulation platform to study complex crowd behavior in social context. The agent-based model is extended based on the well-known social force model, and it mainly describes how agents interact with each other, and also with surrounding facilities such as walls, doors and exits. The simulation platform is compatible to FDS+Evac, and the input data in FDS+Evac could be imported into our simulation platform to create single-floor compartment geometry, and a flow solver is used to generate the roadmap towards exits. Most importantly, we plan to integrate advanced social and psychological theory into our simulation platform, especially investigating human behavior in emergency evacuation,such as pre-evacuation behavior, exit-selection activities, social group and herding effect and so forth."}, "https://arxiv.org/abs/2406.08201": {"title": "HTIM: Hybrid Text-Interaction Modeling for Broadening Political Leaning Inference in Social Media", "link": "https://arxiv.org/abs/2406.08201", "description": "arXiv:2406.08201v1 Announce Type: new \nAbstract: Political leaning can be defined as the inclination of an individual towards certain political orientations that align with their personal beliefs. Political leaning inference has traditionally been framed as a binary classification problem, namely, to distinguish between left vs. right or conservative vs liberal. Furthermore, although some recent work considers political leaning inference in a multi-party multi-region framework, their study is limited to the application of social interaction data. In order to address these shortcomings, in this study we propose Hybrid Text-Interaction Modeling (HTIM), a framework that enables hybrid modeling fusioning text and interactions from Social Media to accurately identify the political leaning of users in a multi-party multi-region framework. Access to textual and interaction-based data not only allows us to compare these data sources but also avoids reliance on specific data types. We show that, while state-of-the-art text-based representations on their own are not able to improve over interaction-based representations, a combination of text-based and interaction-based modeling using HTIM considerably improves the performance across the three regions, an improvement that is more prominent when we focus on the most challenging cases involving users who are less engaged in politics."}, "https://arxiv.org/abs/2406.08299": {"title": "Dynamical evolution of social network polarization and its impact on the propagation of a virus", "link": "https://arxiv.org/abs/2406.08299", "description": "arXiv:2406.08299v1 Announce Type: new \nAbstract: The COVID-19 pandemic that emerged in 2020 has highlighted the complex interplay between vaccine hesitancy and societal polarization. In this study, we analyse the dynamical polarization within a social network as well as the network properties before and after a vaccine was made available. Our results show that as the network evolves from a less structured state to one with more clustered communities. Then using an agent-based modeling approach, we simulate the propagation of a virus in a polarized society by assigning vaccines to pro-vaccine individuals and none to the anti-vaccine individuals. We compare this propagation to the case where the same number of vaccines is distributed homogeneously across the population. In polarized networks, we observe a significantly more widespread diffusion of the virus, highlighting the importance of considering polarization for epidemic forecasting."}, "https://arxiv.org/abs/2406.08429": {"title": "A Sticker is Worth a Thousand Words: Characterizing the Use of Stickers in WhatsApp Political Groups in Brazil", "link": "https://arxiv.org/abs/2406.08429", "description": "arXiv:2406.08429v1 Announce Type: new \nAbstract: With the increasing use of smartphones, instant messaging platforms turned into important communication tools. According to WhatsApp, more than 100 billion messages are sent each day on the app. Communication on these platforms has allowed individuals to express themselves in other types of media, rather than simple text, including audio, videos, images, and stickers. Particularly, stickers are a new multimedia format that emerged with messaging apps, promoting new forms of interactions among users, especially in the Brazilian context, transcending their role as a mere form of humor to become a key element in political strategy. In this regard, we investigate how stickers are being used, unveiling unique characteristics that these media bring to WhatsApp chats and the political use of this new media format. To achieve that, we collected a large sample of messages from WhatsApp public political discussion groups in Brazil and analyzed the sticker messages shared in this context"}, "https://arxiv.org/abs/2406.07642": {"title": "Generating Human Understandable Explanations for Node Embeddings", "link": "https://arxiv.org/abs/2406.07642", "description": "arXiv:2406.07642v1 Announce Type: cross \nAbstract: Node embedding algorithms produce low-dimensional latent representations of nodes in a graph. These embeddings are often used for downstream tasks, such as node classification and link prediction. In this paper, we investigate the following two questions: (Q1) Can we explain each embedding dimension with human-understandable graph features (e.g. degree, clustering coefficient and PageRank). (Q2) How can we modify existing node embedding algorithms to produce embeddings that can be easily explained by human-understandable graph features? We find that the answer to Q1 is yes and introduce a new framework called XM (short for eXplain eMbedding) to answer Q2. A key aspect of XM involves minimizing the nuclear norm of the generated explanations. We show that by minimizing the nuclear norm, we minimize the lower bound on the entropy of the generated explanations. We test XM on a variety of real-world graphs and show that XM not only preserves the performance of existing node embedding methods, but also enhances their explainability."}, "https://arxiv.org/abs/2406.07668": {"title": "Python4Physics: A physics outreach program", "link": "https://arxiv.org/abs/2406.07668", "description": "arXiv:2406.07668v1 Announce Type: cross \nAbstract: We describe a summer outreach program developed to cultivate interest in physics in particular and physical sciences more broadly among high school and early college students using small projects in the Python programming language. We discuss the lessons we learned in the hopes that they will be valuable to other physicists in planning their own outreach efforts. We also provide links to resources and materials from the Python4Physics program, which we hope might be useful in other outreach programs."}, "https://arxiv.org/abs/2406.07693": {"title": "A Labelled Dataset for Sentiment Analysis of Videos on YouTube, TikTok, and Other Sources about the 2024 Outbreak of Measles", "link": "https://arxiv.org/abs/2406.07693", "description": "arXiv:2406.07693v1 Announce Type: cross \nAbstract: The work of this paper presents a dataset that contains the data of 4011 videos about the ongoing outbreak of measles published on 264 websites on the internet between January 1, 2024, and May 31, 2024. The dataset is available at https://dx.doi.org/10.21227/40s8-xf63. These websites primarily include YouTube and TikTok, which account for 48.6% and 15.2% of the videos, respectively. The remainder of the websites include Instagram and Facebook as well as the websites of various global and local news organizations. For each of these videos, the URL of the video, title of the post, description of the post, and the date of publication of the video are presented as separate attributes in the dataset. After developing this dataset, sentiment analysis (using VADER), subjectivity analysis (using TextBlob), and fine-grain sentiment analysis (using DistilRoBERTa-base) of the video titles and video descriptions were performed. This included classifying each video title and video description into (i) one of the sentiment classes i.e. positive, negative, or neutral, (ii) one of the subjectivity classes i.e. highly opinionated, neutral opinionated, or least opinionated, and (iii) one of the fine-grain sentiment classes i.e. fear, surprise, joy, sadness, anger, disgust, or neutral. These results are presented as separate attributes in the dataset for the training and testing of machine learning algorithms for performing sentiment analysis or subjectivity analysis in this field as well as for other applications. Finally, this paper also presents a list of open research questions that may be investigated using this dataset."}, "https://arxiv.org/abs/2406.07926": {"title": "Efficient Neural Common Neighbor for Temporal Graph Link Prediction", "link": "https://arxiv.org/abs/2406.07926", "description": "arXiv:2406.07926v1 Announce Type: cross \nAbstract: Temporal graphs are ubiquitous in real-world scenarios, such as social network, trade and transportation. Predicting dynamic links between nodes in a temporal graph is of vital importance. Traditional methods usually leverage the temporal neighborhood of interaction history to generate node embeddings first and then aggregate the source and target node embeddings to predict the link. However, such methods focus on learning individual node representations, but overlook the pairwise representation learning nature of link prediction and fail to capture the important pairwise features of links such as common neighbors (CN). Motivated by the success of Neural Common Neighbor (NCN) for static graph link prediction, we propose TNCN, a temporal version of NCN for link prediction in temporal graphs. TNCN dynamically updates a temporal neighbor dictionary for each node, and utilizes multi-hop common neighbors between the source and target node to learn a more effective pairwise representation. We validate our model on five large-scale real-world datasets from the Temporal Graph Benchmark (TGB), and find that it achieves new state-of-the-art performance on three of them. Additionally, TNCN demonstrates excellent scalability on large datasets, outperforming popular GNN baselines by up to 6.4 times in speed. Our code is available at https: //github.com/GraphPKU/TNCN."}, "https://arxiv.org/abs/2406.08420": {"title": "Designing Child-Centered Content Exposure and Moderation", "link": "https://arxiv.org/abs/2406.08420", "description": "arXiv:2406.08420v1 Announce Type: cross \nAbstract: Research on children's online experience and computer interaction often overlooks the relationship children have with hidden algorithms that control the content they encounter. Furthermore, it is not only about how children interact with targeted content but also how their development and agency are largely affected by these. By engaging with the body of literature at the intersection of i) human-centered design approaches, ii) exclusion and discrimination in A.I., iii) privacy, transparency, and accountability, and iv) children's online citizenship, this article dives into the question of \"How can we approach the design of a child-centered moderation process to (1) include aspects that families value for their children and (2) provide explanations for content appropriateness and removal so that we can scale (according to systems and human needs) the moderation process assisted by A.I.?\".\n  This article contributes a sociotechnical highlight of core challenges and opportunities of designing child-centered content control tools. The article concludes by grounding and characterizing design considerations for a child-centered, family-guided moderation system. We hope this work serves as a stepping stone for designers and researchers pursuing children's safety online with an eye on hidden agents controlling children's online experiences and, by extension, the values and opportunities children are exposed to."}, "https://arxiv.org/abs/2402.08765": {"title": "Who is driving the conversation? Analysing the nodality of British MPs and journalists on Twitter", "link": "https://arxiv.org/abs/2402.08765", "description": "arXiv:2402.08765v2 Announce Type: replace \nAbstract: Who sets the policy agenda? In this paper, we explore the roles of policy actors in agenda setting by studying their relative influence in policy-related discussions. Our approach builds on ``nodality'' \\textemdash a concept in political science that determines the capacity of an actor to share information and to be at the centre of information networks. We propose a novel methodology that quantifies the nodality of all individual actors in any conversation by analysing a comprehensive set of their centrality measures in the related information network. We combine this with the analysis of the activity time-series, of the related conversation (or topic), to demonstrate how nodality scores relate to the capacity to drive topic-related activity. Here we analyse policy-related discussions on X (previously Twitter) and quantify the nodality of two sets of actors in the UK political system \\textemdash Members of Parliament (MPs) and accredited journalists - on four policy topics: The Russia-Ukraine War, the Cost-of-Living Crisis, Brexit and COVID-19. Our results show that the capacity to influence the activity related to a topic is significantly and positively associated with nodality. In particular, we identify two dimensions of nodality that drive the capacity to influence topic-related activity. The first is ``active nodality\", which reflects the level of topic-related engagement an individual actor has on the platform. The second dimension is ``inherent nodality\" which is entirely independent of the platform and reflects the actor's institutional position (such as an MP in a front-bench role, or a journalist's position at a prominent media outlet)."}, "https://arxiv.org/abs/2403.12619": {"title": "Detection of Malicious Agents in Social Learning", "link": "https://arxiv.org/abs/2403.12619", "description": "arXiv:2403.12619v3 Announce Type: replace \nAbstract: Non-Bayesian social learning is a framework for distributed hypothesis testing aimed at learning the true state of the environment. Traditionally, the agents are assumed to receive observations conditioned on the same true state, although it is also possible to examine the case of heterogeneous models across the graph. One important special case is when heterogeneity is caused by the presence of malicious agents whose goal is to move the agents toward a wrong hypothesis. In this work, we propose an algorithm that allows to discover the true state of every individual agent based on the sequence of their beliefs. In so doing, the methodology is also able to locate malicious behavior."}, "https://arxiv.org/abs/2403.13215": {"title": "Leveraging advances in machine learning for the robust classification and interpretation of networks", "link": "https://arxiv.org/abs/2403.13215", "description": "arXiv:2403.13215v2 Announce Type: replace \nAbstract: The ability to simulate realistic networks based on empirical data is an important task across scientific disciplines, from epidemiology to computer science. Often simulation approaches involve selecting a suitable network generative model such as Erd\\\"os-R\\'enyi or small-world. However, few tools are available to quantify if a particular generative model is suitable for capturing a given network structure or organization. We utilize advances in interpretable machine learning to classify simulated networks by our generative models based on various network attributes, using both primary features and their interactions. Our study underscores the significance of specific network features and their interactions in distinguishing generative models, comprehending complex network structures, and the formation of real-world networks."}, "https://arxiv.org/abs/2403.18856": {"title": "Charge de travail du personnel infirmier dans les h{\\^o}pitaux -{\\'e}tude bibliographique", "link": "https://arxiv.org/abs/2403.18856", "description": "arXiv:2403.18856v2 Announce Type: replace \nAbstract: For decades, hospital services have been faced with the challenge of ensuring quality care for patients despite the pressures on staff due to workload overload. Nursing staff are particularly affected by this reality, with a patient/nursing staff ratio so imbalanced that it directly affects the quality of care and the well-being of nurses. This article examines in detail the workload of nursing staff, shedding light on the various factors influencing this workload, including the type of care provided, the characteristics of patients and nursing staff, as well as the organizational context. Furthermore, different methods of calculating and estimating workload are analyzed, ranging from workload calculation systems to advanced predictive models. Finally, reflection is made on future research avenues, particularly concerning the identification of factors influencing workload, data collection and processing, and model validation."}, "https://arxiv.org/abs/2403.05704": {"title": "Non-robustness of diffusion estimates on networks with measurement error", "link": "https://arxiv.org/abs/2403.05704", "description": "arXiv:2403.05704v4 Announce Type: replace-cross \nAbstract: Network diffusion models are used to study things like disease transmission, information spread, and technology adoption. However, small amounts of mismeasurement are extremely likely in the networks constructed to operationalize these models. We show that estimates of diffusions are highly non-robust to this measurement error. First, we show that even when measurement error is vanishingly small, such that the share of missed links is close to zero, forecasts about the extent of diffusion will greatly underestimate the truth. Second, a small mismeasurement in the identity of the initial seed generates a large shift in the locations of expected diffusion path. We show that both of these results still hold when the vanishing measurement error is only local in nature. Such non-robustness in forecasting exists even under conditions where the basic reproductive number is consistently estimable. Possible solutions, such as estimating the measurement error or implementing widespread detection efforts, still face difficulties because the number of missed links are so small. Finally, we conduct Monte Carlo simulations on simulated networks, and real networks from three settings: travel data from the COVID-19 pandemic in the western US, a mobile phone marketing campaign in rural India, and in an insurance experiment in China."}, "https://arxiv.org/abs/2406.08522": {"title": "Predicting Cascading Failures with a Hyperparametric Diffusion Model", "link": "https://arxiv.org/abs/2406.08522", "description": "arXiv:2406.08522v1 Announce Type: new \nAbstract: In this paper, we study cascading failures in power grids through the lens of information diffusion models. Similar to the spread of rumors or influence in an online social network, it has been observed that failures (outages) in a power grid can spread contagiously, driven by viral spread mechanisms. We employ a stochastic diffusion model that is Markovian (memoryless) and local (the activation of one node, i.e., transmission line, can only be caused by its neighbors). Our model integrates viral diffusion principles with physics-based concepts, by correlating the diffusion weights (contagion probabilities between transmission lines) with the hyperparametric Information Cascades (IC) model. We show that this diffusion model can be learned from traces of cascading failures, enabling accurate modeling and prediction of failure propagation. This approach facilitates actionable information through well-understood and efficient graph analysis methods and graph diffusion simulations. Furthermore, by leveraging the hyperparametric model, we can predict diffusion and mitigate the risks of cascading failures even in unseen grid configurations, whereas existing methods falter due to a lack of training data. Extensive experiments based on a benchmark power grid and simulations therein show that our approach effectively captures the failure diffusion phenomena and guides decisions to strengthen the grid, reducing the risk of large-scale cascading failures. Additionally, we characterize our model's sample complexity, improving upon the existing bound."}, "https://arxiv.org/abs/2406.08762": {"title": "LGB: Language Model and Graph Neural Network-Driven Social Bot Detection", "link": "https://arxiv.org/abs/2406.08762", "description": "arXiv:2406.08762v1 Announce Type: new \nAbstract: Malicious social bots achieve their malicious purposes by spreading misinformation and inciting social public opinion, seriously endangering social security, making their detection a critical concern. Recently, graph-based bot detection methods have achieved state-of-the-art (SOTA) performance. However, our research finds many isolated and poorly linked nodes in social networks, as shown in Fig.1, which graph-based methods cannot effectively detect. To address this problem, our research focuses on effectively utilizing node semantics and network structure to jointly detect sparsely linked nodes. Given the excellent performance of language models (LMs) in natural language understanding (NLU), we propose a novel social bot detection framework LGB, which consists of two main components: language model (LM) and graph neural network (GNN). Specifically, the social account information is first extracted into unified user textual sequences, which is then used to perform supervised fine-tuning (SFT) of the language model to improve its ability to understand social account semantics. Next, the semantically enriched node representation is fed into the pre-trained GNN to further enhance the node representation by aggregating information from neighbors. Finally, LGB fuses the information from both modalities to improve the detection performance of sparsely linked nodes. Extensive experiments on two real-world datasets demonstrate that LGB consistently outperforms state-of-the-art baseline models by up to 10.95%. LGB is already online: https://botdetection.aminer.cn/robotmain."}, "https://arxiv.org/abs/2406.08876": {"title": "Heuristics for Influence Maximization with Tiered Influence and Activation thresholds", "link": "https://arxiv.org/abs/2406.08876", "description": "arXiv:2406.08876v1 Announce Type: new \nAbstract: The information flows among the people while they communicate through social media websites. Due to the dependency on digital media, a person shares important information or regular updates with friends and family. The set of persons on social media forms a social network. Influence Maximization (IM) is a known problem in social networks. In social networks, information flows from one person to another using an underlying diffusion model. There are two fundamental diffusion models: the Independent Cascade Model (ICM) and the Linear Threshold Model (LTM). In this paper, we study a variant of the IM problem called Minimum Influential Seeds (MINFS) problem proposed by Qiang et al.[16]. It generalizes the classical IM problem with LTM as the diffusion model. Compared to IM, this variant has additional parameters: the influence threshold for each node and the propagation range. The propagation range is a positive integer that specifies how far the information can propagate from a node. A node on the network is not immediately influenced until it receives the same information from enough number of neighbors (influence threshold). Similarly, any node does not forward information until it receives the same information from a sufficient number of neighbors (activation threshold). Once a node becomes activated, it tries to activate or influence its neighbors. The MINFS problem aims to select the minimum number of initial spreader nodes such that all nodes of the graph are influenced. In this paper, we extend the study of the MINFS problem. We propose heuristics that construct seed sets based on the average degree of non-activated nodes, closest first, and backbone-based heaviest path."}, "https://arxiv.org/abs/2406.08899": {"title": "ESND: An Embedding-based Framework for Signed Network Dismantling", "link": "https://arxiv.org/abs/2406.08899", "description": "arXiv:2406.08899v1 Announce Type: new \nAbstract: Network dismantling aims to maximize the disintegration of a network by removing a specific set of nodes or edges and is applied to various tasks in diverse domains, such as cracking down on crime organizations, delaying the propagation of rumors, and blocking the transmission of viruses. Most of the current network dismantling methods are tailored for unsigned networks, which only consider the connection between nodes without evaluating the nature of the relationships, such as friendship/hostility, enhancing/repressing, and trust/distrust. We here propose an embedding-based algorithm, namely ESND, to solve the signed network dismantling problem. The algorithm generally iterates the following four steps, i.e., giant component detection, network embedding, node clustering, and removal node selection. To illustrate the efficacy and stability of ESND, we conduct extensive experiments on six signed network datasets as well as null models, and compare the performance of our method with baselines. Experimental results consistently show that the proposed ESND is superior to the baselines and displays stable performance with the change in the network structure. Additionally, we examine the impact of sign proportions on network robustness via ESND, observing that networks with a high ratio of negative edges are generally easier to dismantle than networks with high positive edges."}, "https://arxiv.org/abs/2406.09123": {"title": "PSN: Persian Social Norms Dataset for Cross-Cultural AI", "link": "https://arxiv.org/abs/2406.09123", "description": "arXiv:2406.09123v1 Announce Type: new \nAbstract: Datasets capturing cultural norms are essential for developing globally aware AI systems. We present Persian Social Norms (PSN) a novel dataset of over 1.7k Persian social norms, including environments, contexts, and cultural labels, alongside English translations. Leveraging large language models and prompt-engineering techniques, we generated potential norms that were reviewed by native speakers for quality and ethical compliance. As the first Persian dataset of its kind, this resource enables computational modeling of norm adaptation, a crucial challenge for cross-cultural AI informed by diverse cultural perspectives."}, "https://arxiv.org/abs/2406.09142": {"title": "Effects of Antivaccine Tweets on COVID-19 Vaccinations, Cases, and Deaths", "link": "https://arxiv.org/abs/2406.09142", "description": "arXiv:2406.09142v1 Announce Type: new \nAbstract: Vaccines were critical in reducing hospitalizations and mortality during the COVID-19 pandemic. Despite their wide availability in the United States, 62% of Americans chose not to be vaccinated during 2021. While online misinformation about COVID-19 is correlated to vaccine hesitancy, little prior work has explored a causal link between real-world exposure to antivaccine content and vaccine uptake. Here we present a compartmental epidemic model that includes vaccination, vaccine hesitancy, and exposure to antivaccine content. We fit the model to observational data to determine that a geographical pattern of exposure to online antivaccine content across US counties is responsible for a pattern of reduced vaccine uptake in the same counties. We find that exposure to antivaccine content on Twitter caused about 750,000 people to refuse vaccination between February and August 2021 in the US, resulting in at least 29,000 additional cases and 430 additional deaths. This work provides a methodology for linking online speech to offline epidemic outcomes. Our findings should inform social media moderation policy as well as public health interventions."}, "https://arxiv.org/abs/2406.09169": {"title": "Empirical Networks are Sparse: Enhancing Multi-Edge Models with Zero-Inflation", "link": "https://arxiv.org/abs/2406.09169", "description": "arXiv:2406.09169v1 Announce Type: new \nAbstract: Real-world networks are sparse. As we show in this article, even when a large number of interactions is observed most node pairs remain disconnected. We demonstrate that classical multi-edge network models, such as the $G(N,p)$, configuration models, and stochastic block models, fail to accurately capture this phenomenon. To mitigate this issue, zero-inflation must be integrated into these traditional models. Through zero-inflation, we incorporate a mechanism that accounts for the excess number of zeroes (disconnected pairs) observed in empirical data. By performing an analysis on all the datasets from the Sociopatterns repository, we illustrate how zero-inflated models more accurately reflect the sparsity and heavy-tailed edge count distributions observed in empirical data. Our findings underscore that failing to account for these ubiquitous properties in real-world networks inadvertently leads to biased models which do not accurately represent complex systems and their dynamics."}, "https://arxiv.org/abs/2406.09343": {"title": "Frameworks, Modeling and Simulations of Misinformation and Disinformation: A Systematic Literature Review", "link": "https://arxiv.org/abs/2406.09343", "description": "arXiv:2406.09343v1 Announce Type: new \nAbstract: The prevalence of misinformation and disinformation poses a significant challenge in today's digital landscape. That is why several methods and tools are proposed to analyze and understand these phenomena from a scientific perspective. To assess how the mis/disinformation is being conceptualized and evaluated in the literature, this paper surveys the existing frameworks, models and simulations of mis/disinformation dynamics by performing a systematic literature review up to 2023. After applying the PRISMA methodology, 57 research papers are inspected to determine (1) the terminology and definitions of mis/disinformation, (2) the methods used to represent mis/disinformation, (3) the primary purpose beyond modeling and simulating mis/disinformation, (4) the context where the mis/disinformation is studied, and (5) the validation of the proposed methods for understanding mis/disinformation.\n  The main findings reveal a consistent essence definition of misinformation and disinformation across studies, with intent as the key distinguishing factor. Research predominantly uses social frameworks, epidemiological models, and belief updating simulations. These studies aim to estimate the effectiveness of mis/disinformation, primarily in health and politics. The preferred validation strategy is to compare methods with real-world data and statistics. Finally, this paper identifies current trends and open challenges in the mis/disinformation research field, providing recommendations for future work agenda."}, "https://arxiv.org/abs/2406.09348": {"title": "Emergence of Fluctuation Relations in UNO", "link": "https://arxiv.org/abs/2406.09348", "description": "arXiv:2406.09348v1 Announce Type: new \nAbstract: Fluctuation theorems are generalisations of the second law that describe the relations between work, temperature, and free energy in thermodynamic processes and are used extensively in studies of irreversibility and entropy. Many experiments have verified these relations for different physical systems in the setting of thermodynamics. In this study, we observe the same behavior away from physical thermodynamics, namely for the card game UNO, by performing numerical simulations of the game. As the analog of work, we choose the number of steps one player needs to effect a transition in her deck; the other players and the remaining cards play the role of a finite, non-Markovian bath. We also compare our observation with is expected for a Markovian random walk."}, "https://arxiv.org/abs/2406.08594": {"title": "Limiting behaviour of Branching Processes and Online Social Networks", "link": "https://arxiv.org/abs/2406.08594", "description": "arXiv:2406.08594v1 Announce Type: cross \nAbstract: The literature considers multi-type Markov branching processes (BPs), where the offspring distribution depends only on the living (current) population. We analyse the total-current population-dependent BPs where the offspring distribution can also depend on the total (dead and living) population. Such a generalization is inspired by the need to accurately model content propagation over online social networks (OSNs). The key question investigated is the time-asymptotic proportion of the populations, which translates to the proportional visibility of the posts on the OSN. We provide the answer using a stochastic approximation (SA) technique, which has not been used in the existing BP literature. The analysis is derived using a non-trivial autonomous measurable ODE. Interestingly, we prove the possibility of a new limiting behaviour for the stochastic trajectory, named as hovering around. Such a result is not just new to the theory of BPs but also to the SA based literature.\n  Later, we explore three new variants of BPs: (i) any living individual of a population can attack and acquire the living individuals of the other population, in addition to producing its offspring; (ii) the individuals can die due to abnormal circumstances, and not just at the completion of their lifetimes; (iii) the expected number of offspring decreases as the total-population increases, leading to the saturation of the total-population.\n  Such variants aid in analysing unexplored aspects of content propagation over OSNs: (i) competition in advertisement posts for similar products; (ii) controlling fake-post propagation, while not affecting the sharing of real-post; (iii) impact of re-forwarding the posts. We also designed and analysed a participation (mean-field) game where the OSN lures the users with a reward-based scheme to provide their opinion about the actuality of the post (fake or real)."}, "https://arxiv.org/abs/2406.08624": {"title": "A Sublinear Algorithm for Approximate Shortest Paths in Large Networks", "link": "https://arxiv.org/abs/2406.08624", "description": "arXiv:2406.08624v1 Announce Type: cross \nAbstract: Computing distances and finding shortest paths in massive real-world networks is a fundamental algorithmic task in network analysis. There are two main approaches to solving this task. On one hand are traversal-based algorithms like bidirectional breadth-first search (BiBFS) with no preprocessing step and slow individual distance inquiries. On the other hand are indexing-based approaches, which maintain a large index. This allows for answering individual inquiries very fast; however, index creation is prohibitively expensive. We seek to bridge these two extremes: quickly answer distance inquiries without the need for costly preprocessing.\n  In this work, we propose a new algorithm and data structure, WormHole, for approximate shortest path computations. WormHole leverages structural properties of social networks to build a sublinearly sized index, drawing upon the explicit core-periphery decomposition of Ben-Eliezer et al. Empirically, the preprocessing time of WormHole improves upon index-based solutions by orders of magnitude, and individual inquiries are consistently much faster than in BiBFS. The acceleration comes at the cost of a minor accuracy trade-off. Nonetheless, our empirical evidence demonstrates that WormHole accurately answers essentially all inquiries within a maximum additive error of 2. We complement these empirical results with provable theoretical guarantees, showing that WormHole requires $n^{o(1)}$ node queries per distance inquiry in random power-law networks. In contrast, any approach without a preprocessing step requires $n^{\\Omega(1)}$ queries for the same task.\n  WormHole does not require reading the whole graph. Unlike the vast majority of index-based algorithms, it returns paths, not just distances. For faster inquiry times, it can be combined effectively with other index-based solutions, by running them only on the sublinear core."}, "https://arxiv.org/abs/2312.11326": {"title": "Topic Shifts as a Proxy for Assessing Politicization in Social Media", "link": "https://arxiv.org/abs/2312.11326", "description": "arXiv:2312.11326v2 Announce Type: replace \nAbstract: Politicization is a social phenomenon studied by political science characterized by the extent to which ideas and facts are given a political tone. A range of topics, such as climate change, religion and vaccines has been subject to increasing politicization in the media and social media platforms. In this work, we propose a computational method for assessing politicization in online conversations based on topic shifts, i.e., the degree to which people switch topics in online conversations. The intuition is that topic shifts from a non-political topic to politics are a direct measure of politicization -- making something political, and that the more people switch conversations to politics, the more they perceive politics as playing a vital role in their daily lives. A fundamental challenge that must be addressed when one studies politicization in social media is that, a priori, any topic may be politicized. Hence, any keyword-based method or even machine learning approaches that rely on topic labels to classify topics are expensive to run and potentially ineffective. Instead, we learn from a seed of political keywords and use Positive-Unlabeled (PU) Learning to detect political comments in reaction to non-political news articles posted on Twitter, YouTube, and TikTok during the 2022 Brazilian presidential elections. Our findings indicate that all platforms show evidence of politicization as discussion around topics adjacent to politics such as economy, crime and drugs tend to shift to politics. Even the least politicized topics had the rate in which their topics shift to politics increased in the lead up to the elections and after other political events in Brazil -- an evidence of politicization."}, "https://arxiv.org/abs/2312.16708": {"title": "Capital Inequality Induced Business Cycles", "link": "https://arxiv.org/abs/2312.16708", "description": "arXiv:2312.16708v2 Announce Type: replace \nAbstract: In this letter we present a stochastic dynamic model which can explain economic cycles. We show that the macroscopic description yields a complex dynamical landscape consisting of multiple stable fixed points, each corresponding to a split of the population into a large low and a small high income group. The stochastic fluctuations induce switching between the resulting metastable states, and excitation oscillations just below a deterministic bifurcation. The shocks are caused by the decisions of a few agents who have a disproportionate influence over the macroscopic state of the economy due to the unequal distribution of wealth among the population. The fluctuations have a long-term effect on the growth of economic output and lead to business cycle oscillations exhibiting coherence resonance, where the correlation time is controlled by the population size which is inversely proportional to the noise intensity."}, "https://arxiv.org/abs/2403.06689": {"title": "Dynamics of matrix coupled Kuramoto oscillators on modular networks: excitable behavior and global decoherence", "link": "https://arxiv.org/abs/2403.06689", "description": "arXiv:2403.06689v2 Announce Type: replace \nAbstract: Synchronization is observed in many natural systems, with examples ranging from neuronal activation to walking pedestrians. The models proposed by Winfree and Kuramoto stand as the classic frameworks for investigating these phenomena. The Kuramoto model, in particular, has been extended in different ways since its original formulation to account for more general scenarios. One such extension replaces the coupling parameter with a coupling matrix, describing a form of generalized frustration with broken rotational symmetry. A key feature of this model is the existence of {\\it phase tuned states}, characterized by having the phase of the order parameter pointing in the direction of the dominant eigenvector of the coupling matrix. Here we investigate the matrix coupled Kuramoto model on networks with two modules, such that one module is in the phase tuned state and the other in a state where the order parameter rotates. We identified different regimes in which one or the other module dominates the dynamics. We found, in particular, that the phase tuned module can create a bottleneck for the oscillation of the rotating module, leading to a behavior similar to the charge and fire regimes of excitable systems. We also found an extended region in the parameter space where motion is globally disordered, even though one of the modules presented high levels of synchronization when uncoupled."}, "https://arxiv.org/abs/2404.03685": {"title": "Cooperative Evolutionary Pressure and Diminishing Returns Might Explain the Fermi Paradox: On What Super-AIs Are Like", "link": "https://arxiv.org/abs/2404.03685", "description": "arXiv:2404.03685v3 Announce Type: replace \nAbstract: With an evolutionary approach, the basis of morality can be explained as adaptations to problems of cooperation. With 'evolution' taken in a broad sense, evolving AIs that satisfy the conditions for evolution to apply will be subject to the same cooperative evolutionary pressure as biological entities. Here the adaptiveness of increased cooperation as material safety and wealth increase is discussed -- for humans, for other societies, and for AIs. Diminishing beneficial returns from increased access to material resources also suggests the possibility that, on the whole, there will be no incentive to for instance colonize entire galaxies, thus providing a possible explanation of the Fermi paradox, wondering where everybody is. It is further argued that old societies could engender, give way to, super-AIs, since it is likely that super-AIs are feasible, and fitter. Closing is an aside on effective ways for morals and goals to affect life and society, emphasizing environments, cultures, and laws, and exemplified by how to eat.\n  Appended are an algorithm for colonizing for example a galaxy quickly, models of the evolution of cooperation and fairness under diminishing returns, and software for simulating signaling development. It is also noted that there can be no exponential colonization or reproduction, for mathematical reasons, as each entity takes up a certain amount of space."}, "https://arxiv.org/abs/2402.02520": {"title": "A minimal model of cognition based on oscillatory and reinforcement processes", "link": "https://arxiv.org/abs/2402.02520", "description": "arXiv:2402.02520v2 Announce Type: replace-cross \nAbstract: Building mathematical models of brains is difficult because of the sheer complexity of the problem. One potential starting point is through basal cognition, which give abstract representation of a range of organisms without central nervous systems, including fungi, slime moulds and bacteria. We propose one such model, demonstrating how a combination of oscillatory and current-based reinforcement processes can be used to couple resources in an efficient manner, mimicking the way these organisms function. A key ingredient in our model, not found in previous basal cognition models, is that we explicitly model oscillations in the number of particles (i.e. the nutrients, chemical signals or similar, which make up the biological system) and the flow of these particles within the modelled organisms. Using this approach, we find that our model builds efficient solutions, provided the environmental oscillations are sufficiently out of phase. We further demonstrate that amplitude differences can promote efficient solutions and that the system is robust to frequency differences. In the context of these findings, we discuss connections between our model and basal cognition in biological systems and slime moulds, in particular, how oscillations might contribute to self-organised problem-solving by these organisms."}, "https://arxiv.org/abs/2406.09907": {"title": "Balance with Memory in Signed Networks via Mittag-Leffler Matrix Functions", "link": "https://arxiv.org/abs/2406.09907", "description": "arXiv:2406.09907v1 Announce Type: new \nAbstract: Structural balance is an important characteristic of graphs/networks where edges can be positive or negative, with direct impact on the study of real-world complex systems. When a network is not structurally balanced, it is important to know how much balance still exists in it. Although several measures have been proposed to characterize the degree of balance, the use of matrix functions of the signed adjacency matrix emerges as a very promising area of research. Here, we take a step forward to using Mittag-Leffler (ML) matrix functions to quantify the notion of balance of signed networks. We show that the ML balance index can be obtained from first principles on the basis of a nonconservative diffusion dynamic, and that it accounts for the memory of the system about the past, by diminishing the penalization that long cycles typically receive in other matrix functions. Finally, we demonstrate the important information in the ML balance index with both artificial signed networks and real-world networks in various contexts, ranging from biological and ecological to social ones."}, "https://arxiv.org/abs/2406.09983": {"title": "Epidemic-induced local awareness behavior inferred from surveys and genetic sequence data", "link": "https://arxiv.org/abs/2406.09983", "description": "arXiv:2406.09983v1 Announce Type: new \nAbstract: Behavior-disease models suggest that if individuals are aware and take preventive actions when the prevalence of the disease increases among their close contacts, then the pandemic can be contained in a cost-effective way. To measure the true impact of local awareness behavior on epidemic spreading, we propose an efficient approach to identify superspreading events and assign corresponding Event Containment Scores (ECSs) in clinical genetic sequence data. We validate ECS as a measure of local awareness in simulation experiments, and we find that ECS was correlated positively with policy stringency during the COVID-19 pandemic. Finally, we observe a temporary drop in ECS during the Omicron wave in most European countries, matching a survey experiment we carried out at the same time. Our findings bring important insight into the field of awareness modeling through the analysis of large-scale genetic sequence data, one of the most promising data sources in epidemics research."}, "https://arxiv.org/abs/2406.09639": {"title": "TGB 2", "link": "https://arxiv.org/abs/2406.09639", "description": "arXiv:2406.09639v1 Announce Type: cross \nAbstract: Multi-relational temporal graphs are powerful tools for modeling real-world data, capturing the evolving and interconnected nature of entities over time. Recently, many novel models are proposed for ML on such graphs intensifying the need for robust evaluation and standardized benchmark datasets. However, the availability of such resources remains scarce and evaluation faces added complexity due to reproducibility issues in experimental protocols. To address these challenges, we introduce Temporal Graph Benchmark 2.0 (TGB 2.0), a novel benchmarking framework tailored for evaluating methods for predicting future links on Temporal Knowledge Graphs and Temporal Heterogeneous Graphs with a focus on large-scale datasets, extending the Temporal Graph Benchmark. TGB 2.0 facilitates comprehensive evaluations by presenting eight novel datasets spanning five domains with up to 53 million edges. TGB 2.0 datasets are significantly larger than existing datasets in terms of number of nodes, edges, or timestamps. In addition, TGB 2.0 provides a reproducible and realistic evaluation pipeline for multi-relational temporal graphs. Through extensive experimentation, we observe that 1) leveraging edge-type information is crucial to obtain high performance, 2) simple heuristic baselines are often competitive with more complex methods, 3) most methods fail to run on our largest datasets, highlighting the need for research on more scalable methods."}, "https://arxiv.org/abs/1907.06130": {"title": "Quantifying the Vulnerabilities of the Online Public Square to Adversarial Manipulation Tactics", "link": "https://arxiv.org/abs/1907.06130", "description": "arXiv:1907.06130v5 Announce Type: replace-cross \nAbstract: Social media, seen by some as the modern public square, is vulnerable to manipulation. By controlling inauthentic accounts impersonating humans, malicious actors can amplify disinformation within target communities. The consequences of such operations are difficult to evaluate due to the challenges posed by collecting data and carrying out ethical experiments that would influence online communities. Here we use a social media model that simulates information diffusion in an empirical network to quantify the impacts of several adversarial manipulation tactics on the quality of content. We find that the presence of influential accounts, a hallmark of social media, exacerbates the vulnerabilities of online communities to manipulation. Among the explored tactics that bad actors can employ, infiltrating a community is the most likely to make low-quality content go viral. Such harm can be further compounded by inauthentic agents flooding the network with low-quality, yet appealing content, but is mitigated when bad actors focus on specific targets, such as influential or vulnerable individuals. These insights suggest countermeasures that platforms could employ to increase the resilience of social media users to manipulation."}, "https://arxiv.org/abs/2310.14533": {"title": "Context-Aware Prediction of User Engagement on Online Social Platforms", "link": "https://arxiv.org/abs/2310.14533", "description": "arXiv:2310.14533v2 Announce Type: replace-cross \nAbstract: The success of online social platforms hinges on their ability to predict and understand user behavior at scale. Here, we present data suggesting that context-aware modeling approaches may offer a holistic yet lightweight and potentially privacy-preserving representation of user engagement on online social platforms. Leveraging deep LSTM neural networks to analyze more than 100 million Snapchat sessions from almost 80.000 users, we demonstrate that patterns of active and passive use are predictable from past behavior (R2=0.345) and that the integration of context features substantially improves predictive performance compared to the behavioral baseline model (R2=0.522). Features related to smartphone connectivity status, location, temporal context, and weather were found to capture non-redundant variance in user engagement relative to features derived from histories of in-app behaviors. Further, we show that a large proportion of variance can be accounted for with minimal behavioral histories if momentary context is considered (R2=0.442). These results indicate the potential of context-aware approaches for making models more efficient and privacy-preserving by reducing the need for long data histories. Finally, we employ model explainability techniques to glean preliminary insights into the underlying behavioral mechanisms. Our findings are consistent with the notion of context-contingent, habit-driven patterns of active and passive use, underscoring the value of contextualized representations of user behavior for predicting user engagement on social platforms."}, "https://arxiv.org/abs/2311.18526": {"title": "HOT: Higher-Order Dynamic Graph Representation Learning with Efficient Transformers", "link": "https://arxiv.org/abs/2311.18526", "description": "arXiv:2311.18526v2 Announce Type: replace-cross \nAbstract: Many graph representation learning (GRL) problems are dynamic, with millions of edges added or removed per second. A fundamental workload in this setting is dynamic link prediction: using a history of graph updates to predict whether a given pair of vertices will become connected. Recent schemes for link prediction in such dynamic settings employ Transformers, modeling individual graph updates as single tokens. In this work, we propose HOT: a model that enhances this line of works by harnessing higher-order (HO) graph structures; specifically, k-hop neighbors and more general subgraphs containing a given pair of vertices. Harnessing such HO structures by encoding them into the attention matrix of the underlying Transformer results in higher accuracy of link prediction outcomes, but at the expense of increased memory pressure. To alleviate this, we resort to a recent class of schemes that impose hierarchy on the attention matrix, significantly reducing memory footprint. The final design offers a sweetspot between high accuracy and low memory utilization. HOT outperforms other dynamic GRL schemes, for example achieving 9%, 7%, and 15% higher accuracy than - respectively - DyGFormer, TGN, and GraphMixer, for the MOOC dataset. Our design can be seamlessly extended towards other dynamic GRL workloads."}, "https://arxiv.org/abs/2312.15489": {"title": "Browsing behavior exposes identities on the Web", "link": "https://arxiv.org/abs/2312.15489", "description": "arXiv:2312.15489v2 Announce Type: replace-cross \nAbstract: How easy is it to uniquely identify a person based solely on their web browsing behavior? Here we show that when people navigate the Web, their online traces produce fingerprints that identify them. Merely the four most visited web domains are enough to identify 95% of the individuals. These digital fingerprints are stable and render high re-identifiability. We demonstrate that we can re-identify 80% of the individuals in separate time slices of data. Such a privacy threat persists even with limited information about individuals' browsing behavior, reinforcing existing concerns around online privacy."}, "https://arxiv.org/abs/2406.10240": {"title": "Indicators of the human origin of numbers", "link": "https://arxiv.org/abs/2406.10240", "description": "arXiv:2406.10240v1 Announce Type: new \nAbstract: Researchers have demonstrated that humans are unable to generate a sequence of random numbers that corresponds in a statistical sense to a simple distribution such as the uniform distribution. The purpose of this article is to present the results of research on the generation of random number sequences by humans. The article describes 10 effects found in such studies, mechanisms explaining these effects, and 14 measures (not including modifications) used to detect deviations from randomness in the sequences. The analysis of numerical sequences is not only of academic interest; it can also be used for the purpose of data validation (auditing)."}, "https://arxiv.org/abs/2406.10241": {"title": "Shape patterns in popularity series of video games", "link": "https://arxiv.org/abs/2406.10241", "description": "arXiv:2406.10241v1 Announce Type: new \nAbstract: In recent years, digital games have become increasingly present in people's lives both as a leisure activity or in gamified activities of everyday life. Despite this growing presence, large-scale, data-driven analyses of video games remain a small fraction of the related literature. In this sense, the present work constitutes an investigation of patterns in popularity series of video games based on monthly popularity series, spanning eleven years, for close to six thousand games listed on the online platform Steam. Utilizing these series, after a preprocessing stage, we perform a clustering task in order to group the series solely based on their shape. Our results indicate the existence of five clusters of shape patterns named decreasing, hilly, increasing, valley, and bursty, with approximately half of the games showing a decreasing popularity pattern, 20.7% being hilly, 11.8% increasing, 11.0% bursty, and 9.1% valley. Finally, we have probed the prevalence and persistence of shape patterns by comparing the shapes of longer popularity series during their early stages and after completion. We have found the majority of games tend to maintain their pattern over time, except for a constant pattern that appears early in popularity series only to later originate hilly and bursty popularity series."}, "https://arxiv.org/abs/2406.10369": {"title": "On the Preservation of Input/Output Directed Graph Informativeness under Crossover", "link": "https://arxiv.org/abs/2406.10369", "description": "arXiv:2406.10369v1 Announce Type: new \nAbstract: There is a broad class of networks which connect inputs to outputs. While evolutionary operators have been applied to a wide array of complex problems, methods to apply such operators to these networks remain ill-defined. We aim to remedy this. We define Input/Output Directed Graphs (or IOD Graphs) as graphs with nodes $N$ and directed edges $E$, where $N$ contains (a) a set of ``input nodes'' $I \\subset N$, where each $i \\in I$ has no incoming edges and any number of outgoing edges, and (b) a set of ``output nodes'' $O \\subset N$, where each $o \\in O$ has no outgoing edges and any number of incoming edges, and $I\\cap O = \\emptyset$. We define informativeness, which involves the connections via directed paths from the input nodes to the output nodes: A partially informative IOD Graph has at least one path from an input to an output, a very informative IOD Graph has a path from every input to some output, and a fully informative IOD Graph has a path from every input to every output.\n  A perceptron is an example of an IOD Graph. If it has non-zero weights and any number of layers, it is fully informative. As links are removed (assigned zero weight), the perceptron might become very, partially, or not informative.\n  We define a crossover operation on IOD Graphs in which we find subgraphs with matching sets of forward and backward directed links to ``swap.'' With this operation, IOD Graphs can be subject to evolutionary computation methods. We show that fully informative parents may yield a non-informative child. We also show that under conditions of contiguousness and the no dangling nodes condition, crossover compatible, partially informative parents yield partially informative children, and very informative input parents with partially informative output parents yield very informative children. However, even under these conditions, full informativeness may not be retained."}, "https://arxiv.org/abs/2406.10423": {"title": "A comprehensive generalization of the Friendship Paradox to weights and attributes", "link": "https://arxiv.org/abs/2406.10423", "description": "arXiv:2406.10423v1 Announce Type: new \nAbstract: The Friendship Paradox is a simple and powerful statement about node degrees in a graph (Feld 1991). However, it only applies to undirected graphs with no edge weights, and the only node characteristic it concerns is degree. Since many social networks are more complex than that, it is useful to generalize this phenomenon, if possible, and a number of papers have proposed different generalizations. Here, we unify these generalizations in a common framework, retaining the focus on undirected graphs and allowing for weighted edges and for numeric node attributes other than degree to be considered, since this extension allows for a clean characterization and links to the original concepts most naturally. While the original Friendship Paradox and the Weighted Friendship Paradox hold for all graphs, considering non-degree attributes actually makes the extensions fail around 50% of the time, given random attribute assignment. We provide simple correlation-based rules to see whether an attribute-based version of the paradox holds. In addition to theory, our simulation and data results show how all the concepts can be applied to synthetic and real networks. Where applicable, we draw connections to prior work to make this an accessible and comprehensive paper that lets one understand the math behind the Friendship Paradox and its basic extensions."}, "https://arxiv.org/abs/2406.10451": {"title": "Climate Change Task Force Report for the American Astronomical Society", "link": "https://arxiv.org/abs/2406.10451", "description": "arXiv:2406.10451v1 Announce Type: new \nAbstract: The AAS Strategic Plan for 2021-26 called for the creation of a task force to identify how the AAS can meet the goals of the Paris Agreement. The AAS and its membership recognize the danger climate change represents to humanity and our world, and to astronomy -- as a profession, a hobby, and a cultural good. Our profession in general -- and the AAS in particular -- should work to make it possible for all astronomers to have an equal opportunity to be successful without needing to incur high carbon emissions, and to preserve astronomy for future generations.\n  A study was completed of the carbon emissions associated with the AAS, finding that 84% of total AAS-related emissions are from in-person conferences. We also conducted a survey of AAS members to determine their attitudes about climate change. Respondents overwhelmingly (97%) think that the AAS should reduce its carbon footprint. Our task force created a list of fourteen recommendations, with two ranked as top priorities: The AAS should not schedule additional in-person meetings before 2030 and it should work to innovate the AAS conference model. Based upon our analysis it is clear that online interaction is the only way to increase participation while meaningfully decreasing emissions.\n  Our recommendations are aligned with the Astro2020 Decadal Survey as well as AAS values to disseminate our scientific understanding of the universe, and to do our work in an ethically responsible way. Because of their other benefits -- particularly in making our society more welcoming to those who traditionally have been excluded -- we feel that these are sound decisions, worthy of implementation even if the AAS wasn't trying to reduce its carbon footprint. They simply make sense as steps towards a professional society that better serves a broader membership, as our profession evolves to be greener, more inclusive, and more productive."}, "https://arxiv.org/abs/2406.10572": {"title": "Collaborative Framework with Shared Responsibility for Relief Management in Disaster Scenarios", "link": "https://arxiv.org/abs/2406.10572", "description": "arXiv:2406.10572v1 Announce Type: new \nAbstract: Disasters instances have been increasing both in frequency and intensity causing the tragic loss of life and making life harder for the survivors. Disaster relief management plays a crucial role in enhancing the lifestyle of disaster victims by managing the disaster impacts. Disaster relief management is a process with many collaborative sectors where different stakeholders should operate in all major phases of the disaster management progression. In the different phases of the disaster management process, many collaborative government organisations along with nongovernment organisations, leadership, community, and media at different levels need to share the responsibility with disaster victims to achieve effective disaster relief management. Shared responsibility enhances disaster relief management effectiveness and reduces the disaster's impact on the victims. Considering the diverse roles of different stakeholders, there has been a need for a framework that can bind different stakeholders together during disaster management. this paper shows a framework with major stakeholders of disaster relief management and how different stakeholders can take part in an effective disaster relief management process. The framework also highlights how each stakeholder can contribute to relief management at different phases after a disaster. The paper also explores some of the shared responsibility collaborative practices that have been implemented around the world in response to the disaster as a disaster relief management process. In addition, the paper highlights the knowledge obtained from those disaster instances and how this knowledge can be transferred and can be helpful in disaster mitigation and preparedness for future disaster scenarios."}, "https://arxiv.org/abs/2406.10589": {"title": "Resilience patterns in higher-order meta-population networks", "link": "https://arxiv.org/abs/2406.10589", "description": "arXiv:2406.10589v1 Announce Type: new \nAbstract: Meta-population networks are effective tools for capturing population movement across distinct regions, but the assumption of well-mixed regions fails to capture the reality of population higher-order interactions. As a multidimensional system capturing mobility characteristics, meta-population networks are inherently complex and difficult to interpret when subjected to resilience analysis based on N-dimensional equations. We propose a higher-order meta-population model that captures large-scale global cross-regional mobility and small-scale higher-order interactions within regions. Remarkably, we extend the dimension-reduction approach, simplifying the N-dimensional higher-order meta-population system into a one-dimensional equation by decomposing different network behaviours into a single universal resilience function, thereby allowing for convenient and accurate prediction of the system resilience. The network structure and human mobility parameters can clearly and simply express the epidemic threshold. Numerical experimental results on both real networks and star networks confirm the accuracy of the proposed dimension-reduction framework in predicting the evolution of epidemic dynamics on higher-order meta-population networks. Additionally, higher-order interactions among populations are shown to lead to explosive growth in the epidemic infection size potentially. Population mobility causes changes in the spatial distribution of infectious diseases across different regions."}, "https://arxiv.org/abs/2406.10717": {"title": "Economical representation of spatial networks", "link": "https://arxiv.org/abs/2406.10717", "description": "arXiv:2406.10717v1 Announce Type: new \nAbstract: Network visualization is essential for many scientific, societal, technological and artistic domains. The primary goal is to highlight patterns out of nodes interconnected by edges that are easy to understand, facilitate communication and support decision-making. This is typically achieved by rearranging the nodes to minimize the edge crossings responsible of unintelligible and often unaesthetic trends. But when the nodes cannot be moved, as in spatial and physical networks, this procedure is not viable. Here, we overcome this situation by turning the edge crossing problem into a graph filtering optimization. We demonstrate that the presence of longer connections prompt the optimal solution to yield sparser networks, thereby limiting the number of intersections and getting more readable layouts. This theoretical result matches human behavior and provides an ecologically-inspired criterion to visualize and model real-world interconnected systems."}, "https://arxiv.org/abs/2406.11405": {"title": "Network growth under opportunistic attachment", "link": "https://arxiv.org/abs/2406.11405", "description": "arXiv:2406.11405v1 Announce Type: new \nAbstract: Growing network models can potentially be a useful tool in the development of economic theory. This work introduces an \"opportunistic attachment\" mechanism where incoming nodes, in deciding where to join a network, consider features of the entry points available to them. For example, an entrepreneur looking to start a thriving business might consider the expected revenue of many hypothetical businesses. This mechanism is explored, in isolation, via a minimal model where PageRank serves to score the available opportunities. Despite its simplicity, this model gives rise to rich node dynamics, path-dependence, and an unexpected degenerate structure. We go on to argue that this model might be useful to theoretical development as a maximally stylised model of entrepreneurial growth. Central to the argument is an alternative set of microfoundations introduced in Leontief & Brody (1993) whereby the steady state of a random walk is a notion of economic equilibrium. To the extent this argument holds, our findings suggest that entrepreneurs face a shifting \"opportunity space\" where the number of potential business opportunities is effectively unbounded. Opportunistic attachment is thus a candidate mechanism for relating the structure of an economic system to its future growth."}, "https://arxiv.org/abs/2406.11423": {"title": "Dredge Word, Social Media, and Webgraph Networks for Unreliable Website Classification and Identification", "link": "https://arxiv.org/abs/2406.11423", "description": "arXiv:2406.11423v1 Announce Type: new \nAbstract: In an attempt to mimic the complex paths through which unreliable content spreads between search engines and social media, we explore the impact of incorporating both webgraph and large-scale social media contexts into website credibility classification and discovery systems. We further explore the usage of what we define as \\textit{dredge words} on social media -- terms or phrases for which unreliable domains rank highly. Through comprehensive graph neural network ablations, we demonstrate that curriculum-based heterogeneous graph models that leverage context from both webgraphs and social media data outperform homogeneous and single-mode approaches. We further demonstrate that the incorporation of dredge words into our model strongly associates unreliable websites with social media and online commerce platforms. Finally, we show our heterogeneous model greatly outperforms competing systems in the top-k identification of unlabeled unreliable websites. We demonstrate the strong unreliability signals present in the diverse paths that users follow to uncover unreliable content, and we release a novel dataset of dredge words."}, "https://arxiv.org/abs/2406.11450": {"title": "The Evolution of Language in Social Media Comments", "link": "https://arxiv.org/abs/2406.11450", "description": "arXiv:2406.11450v1 Announce Type: new \nAbstract: Understanding the impact of digital platforms on user behavior presents foundational challenges, including issues related to polarization, misinformation dynamics, and variation in news consumption. Comparative analyses across platforms and over different years can provide critical insights into these phenomena. This study investigates the linguistic characteristics of user comments over 34 years, focusing on their complexity and temporal shifts. Utilizing a dataset of approximately 300 million English comments from eight diverse platforms and topics, we examine the vocabulary size and linguistic richness of user communications and their evolution over time. Our findings reveal consistent patterns of complexity across social media platforms and topics, characterized by a nearly universal reduction in text length, diminished lexical richness, but decreased repetitiveness. Despite these trends, users consistently introduce new words into their comments at a nearly constant rate. This analysis underscores that platforms only partially influence the complexity of user comments. Instead, it reflects a broader, universal pattern of human behaviour, suggesting intrinsic linguistic tendencies of users when interacting online."}, "https://arxiv.org/abs/2406.11553": {"title": "The Susceptibility Paradox in Online Social Influence", "link": "https://arxiv.org/abs/2406.11553", "description": "arXiv:2406.11553v1 Announce Type: new \nAbstract: Understanding susceptibility to online influence is crucial for mitigating the spread of misinformation and protecting vulnerable audiences. This paper investigates susceptibility to influence within social networks, focusing on the differential effects of influence-driven versus spontaneous behaviors on user content adoption. Our analysis reveals that influence-driven adoption exhibits high homophily, indicating that individuals prone to influence often connect with similarly susceptible peers, thereby reinforcing peer influence dynamics. Conversely, spontaneous adoption shows significant but lower homophily. Additionally, we extend the Generalized Friendship Paradox to influence-driven behaviors, demonstrating that users' friends are generally more susceptible to influence than the users themselves, de facto establishing the notion of Susceptibility Paradox in online social influence. This pattern does not hold for spontaneous behaviors, where friends exhibit fewer spontaneous adoptions. We find that susceptibility to influence can be accurately predicted using friends' susceptibility alone, while predicting spontaneous adoption requires additional features, such as user metadata. These findings highlight the complex interplay between user engagement and preferences in spontaneous content adoption. Our results provide new insights into social influence mechanisms and offer implications for designing more effective moderation strategies to protect vulnerable audiences."}, "https://arxiv.org/abs/2406.10238": {"title": "Early Detection of Misinformation for Infodemic Management: A Domain Adaptation Approach", "link": "https://arxiv.org/abs/2406.10238", "description": "arXiv:2406.10238v1 Announce Type: cross \nAbstract: An infodemic refers to an enormous amount of true information and misinformation disseminated during a disease outbreak. Detecting misinformation at the early stage of an infodemic is key to manage it and reduce its harm to public health. An early stage infodemic is characterized by a large volume of unlabeled information concerning a disease. As a result, conventional misinformation detection methods are not suitable for this misinformation detection task because they rely on labeled information in the infodemic domain to train their models. To address the limitation of conventional methods, state-of-the-art methods learn their models using labeled information in other domains to detect misinformation in the infodemic domain. The efficacy of these methods depends on their ability to mitigate both covariate shift and concept shift between the infodemic domain and the domains from which they leverage labeled information. These methods focus on mitigating covariate shift but overlook concept shift, rendering them less effective for the task. In response, we theoretically show the necessity of tackling both covariate shift and concept shift as well as how to operationalize each of them. Built on the theoretical analysis, we develop a novel misinformation detection method that addresses both covariate shift and concept shift. Using two real-world datasets, we conduct extensive empirical evaluations to demonstrate the superior performance of our method over state-of-the-art misinformation detection methods as well as prevalent domain adaptation methods that can be tailored to solve the misinformation detection task."}, "https://arxiv.org/abs/2406.10266": {"title": "COVID-19 Twitter Sentiment Classification Using Hybrid Deep Learning Model Based on Grid Search Methodology", "link": "https://arxiv.org/abs/2406.10266", "description": "arXiv:2406.10266v1 Announce Type: cross \nAbstract: In the contemporary era, social media platforms amass an extensive volume of social data contributed by their users. In order to promptly grasp the opinions and emotional inclinations of individuals regarding a product or event, it becomes imperative to perform sentiment analysis on the user-generated content. Microblog comments often encompass both lengthy and concise text entries, presenting a complex scenario. This complexity is particularly pronounced in extensive textual content due to its rich content and intricate word interrelations compared to shorter text entries. Sentiment analysis of public opinion shared on social networking websites such as Facebook or Twitter has evolved and found diverse applications. However, several challenges remain to be tackled in this field. The hybrid methodologies have emerged as promising models for mitigating sentiment analysis errors, particularly when dealing with progressively intricate training data. In this article, to investigate the hesitancy of COVID-19 vaccination, we propose eight different hybrid deep learning models for sentiment classification with an aim of improving overall accuracy of the model. The sentiment prediction is achieved using embedding, deep learning model and grid search algorithm on Twitter COVID-19 dataset. According to the study, public sentiment towards COVID-19 immunization appears to be improving with time, as evidenced by the gradual decline in vaccine reluctance. Through extensive evaluation, proposed model reported an increased accuracy of 98.86%, outperforming other models. Specifically, the combination of BERT, CNN and GS yield the highest accuracy, while the combination of GloVe, BiLSTM, CNN and GS follows closely behind with an accuracy of 98.17%. In addition, increase in accuracy in the range of 2.11% to 14.46% is reported by the proposed model in comparisons with existing works."}, "https://arxiv.org/abs/2406.10380": {"title": "A Model for Economic Freedom on Mars", "link": "https://arxiv.org/abs/2406.10380", "description": "arXiv:2406.10380v1 Announce Type: cross \nAbstract: The momentum of human spaceflight initiatives continues to build toward Mars, and technological advances may eventually enable the potential for permanent space settlement. Aspirations for sustaining human life in space must be predicated on human factors, rather than technological constraints alone, and advances in models of governance and ethics are necessary as human civilization becomes a spacefaring species. This paper presents an idealistic but feasible model for economic freedom on Mars, which is situated within a framework in which Mars has been designated as a sovereign juridical peer to Earth. Under such conditions, Mars could maintain monetary stability through full reserve banking and a restriction on exchange with any fractional reserve Earth currencies, with a volume of circulating currency that changes based on the total population within fixed capacity infrastructure. Mars could maintain long-term political stability by diffusing the ownership of capital on Mars, which would allow all citizens of Mars to draw sufficient wealth from a combination of capital ownership and labor to live a good life. This model could also support limited tourism on Mars, in which real goods are exchanged for services but currency transactions between planets are prohibited. This model demonstrates the potential for a viable and sustainable economy on Mars that could conceivably be implemented, including on a sovereign Mars but also in other scenarios of space settlement. More broadly, this model illustrates that ideas such as diffuse capital ownership and limited government can enable freedom in space, and numerous models beyond a centralized world space agency should be explored to ensure the optimal governance of the emerging space economy."}, "https://arxiv.org/abs/2406.10498": {"title": "A Unified Graph Selective Prompt Learning for Graph Neural Networks", "link": "https://arxiv.org/abs/2406.10498", "description": "arXiv:2406.10498v1 Announce Type: cross \nAbstract: In recent years, graph prompt learning/tuning has garnered increasing attention in adapting pre-trained models for graph representation learning. As a kind of universal graph prompt learning method, Graph Prompt Feature (GPF) has achieved remarkable success in adapting pre-trained models for Graph Neural Networks (GNNs). By fixing the parameters of a pre-trained GNN model, the aim of GPF is to modify the input graph data by adding some (learnable) prompt vectors into graph node features to better align with the downstream tasks on the smaller dataset. However, existing GPFs generally suffer from two main limitations. First, GPFs generally focus on node prompt learning which ignore the prompting for graph edges. Second, existing GPFs generally conduct the prompt learning on all nodes equally which fails to capture the importances of different nodes and may perform sensitively w.r.t noisy nodes in aligning with the downstream tasks. To address these issues, in this paper, we propose a new unified Graph Selective Prompt Feature learning (GSPF) for GNN fine-tuning. The proposed GSPF integrates the prompt learning on both graph node and edge together, which thus provides a unified prompt model for the graph data. Moreover, it conducts prompt learning selectively on nodes and edges by concentrating on the important nodes and edges for prompting which thus make our model be more reliable and compact. Experimental results on many benchmark datasets demonstrate the effectiveness and advantages of the proposed GSPF method."}, "https://arxiv.org/abs/2406.10500": {"title": "Geodesic Distance Between Graphs: A Spectral Metric for Assessing the Stability of Graph Neural Networks", "link": "https://arxiv.org/abs/2406.10500", "description": "arXiv:2406.10500v1 Announce Type: cross \nAbstract: This paper presents a spectral framework for assessing the generalization and stability of Graph Neural Networks (GNNs) by introducing a Graph Geodesic Distance (GGD) metric. For two different graphs with the same number of nodes, our framework leverages a spectral graph matching procedure to find node correspondence so that the geodesic distance between them can be subsequently computed by solving a generalized eigenvalue problem associated with their Laplacian matrices. For graphs with different sizes, a resistance-based spectral graph coarsening scheme is introduced to reduce the size of the bigger graph while preserving the original spectral properties. We show that the proposed GGD metric can effectively quantify dissimilarities between two graphs by encapsulating their differences in key structural (spectral) properties, such as effective resistances between nodes, cuts, the mixing time of random walks, etc. Through extensive experiments comparing with the state-of-the-art metrics, such as the latest Tree-Mover's Distance (TMD) metric, the proposed GGD metric shows significantly improved performance for stability evaluation of GNNs especially when only partial node features are available."}, "https://arxiv.org/abs/2406.10608": {"title": "Scalable Temporal Motif Densest Subnetwork Discovery", "link": "https://arxiv.org/abs/2406.10608", "description": "arXiv:2406.10608v1 Announce Type: cross \nAbstract: Finding dense subnetworks, with density based on edges or more complex structures, such as subgraphs or $k$-cliques, is a fundamental algorithmic problem with many applications. While the problem has been studied extensively in static networks, much remains to be explored for temporal networks.\n  In this work we introduce the novel problem of identifying the temporal motif densest subnetwork, i.e., the densest subnetwork with respect to temporal motifs, which are high-order patterns characterizing temporal networks. This problem significantly differs from analogous formulations for dense temporal (or static) subnetworks as these do not account for temporal motifs. Identifying temporal motifs is an extremely challenging task, and thus, efficient methods are required. To this end, we design two novel randomized approximation algorithms with rigorous probabilistic guarantees that provide high-quality solutions. We perform extensive experiments showing that our methods outperform baselines. Furthermore, our algorithms scale on networks with up to billions of temporal edges, while baselines cannot handle such large networks. We use our techniques to analyze a financial network and show that our formulation reveals important network structures, such as bursty temporal events and communities of users with similar interests."}, "https://arxiv.org/abs/2406.10711": {"title": "Symmetry-driven embedding of networks in hyperbolic space", "link": "https://arxiv.org/abs/2406.10711", "description": "arXiv:2406.10711v1 Announce Type: cross \nAbstract: Hyperbolic models can reproduce the heavy-tailed degree distribution, high clustering, and hierarchical structure of empirical networks. Current algorithms for finding the hyperbolic coordinates of networks, however, do not quantify uncertainty in the inferred coordinates. We present BIGUE, a Markov chain Monte Carlo (MCMC) algorithm that samples the posterior distribution of a Bayesian hyperbolic random graph model. We show that combining random walk and random cluster transformations significantly improves mixing compared to the commonly used and state-of-the-art dynamic Hamiltonian Monte Carlo algorithm. Using this algorithm, we also provide evidence that the posterior distribution cannot be approximated by a multivariate normal distribution, thereby justifying the use of MCMC to quantify the uncertainty of the inferred parameters."}, "https://arxiv.org/abs/2406.10965": {"title": "DocNet: Semantic Structure in Inductive Bias Detection Models", "link": "https://arxiv.org/abs/2406.10965", "description": "arXiv:2406.10965v1 Announce Type: cross \nAbstract: News will have biases so long as people have opinions. However, as social media becomes the primary entry point for news and partisan gaps increase, it is increasingly important for informed citizens to be able to identify bias. People will be able to take action to avoid polarizing echo chambers if they know how the news they are consuming is biased. In this paper, we explore an often overlooked aspect of bias detection in documents: the semantic structure of news articles. We present DocNet, a novel, inductive, and low-resource document embedding and bias detection model that outperforms large language models. We also demonstrate that the semantic structure of news articles from opposing partisan sides, as represented in document-level graph embeddings, have significant similarities. These results can be used to advance bias detection in low-resource environments. Our code and data are made available at https://github.com/nlpresearchanon."}, "https://arxiv.org/abs/2406.10978": {"title": "Local wealth condensation for yard-sale models with wealth-dependent biases", "link": "https://arxiv.org/abs/2406.10978", "description": "arXiv:2406.10978v1 Announce Type: cross \nAbstract: In Chakraborti's yard-sale model of an economy, identical agents engage in pairwise trades, resulting in wealth exchanges that conserve each agent's expected wealth. Doob's martingale convergence theorem immediately implies almost sure wealth condensation, i.e., convergence to a state in which a single agent owns the entire economy. If some pairs of agents are not allowed to trade with each other, the martingale convergence theorem still implies local wealth condensation, i.e., convergence to a state in which some agents are wealthy, while all their trading partners are impoverished. In this note, we propose a new, more elementary proof of this result. Unlike the proof based on the martingale convergence theorem, our argument applies to models with a wealth-acquired advantage, and even to certain models with a poverty-acquired advantage."}, "https://arxiv.org/abs/2406.11046": {"title": "Impact of the Availability of ChatGPT on Software Development: A Synthetic Difference in Differences Estimation using GitHub Data", "link": "https://arxiv.org/abs/2406.11046", "description": "arXiv:2406.11046v1 Announce Type: cross \nAbstract: Advancements in Artificial Intelligence, particularly with ChatGPT, have significantly impacted software development. Utilizing novel data from GitHub Innovation Graph, we hypothesize that ChatGPT enhances software production efficiency. Utilizing natural experiments where some governments banned ChatGPT, we employ Difference-in-Differences (DID), Synthetic Control (SC), and Synthetic Difference-in-Differences (SDID) methods to estimate its effects. Our findings indicate a significant positive impact on the number of git pushes, repositories, and unique developers per 100,000 people, particularly for high-level, general purpose, and shell scripting languages. These results suggest that AI tools like ChatGPT can substantially boost developer productivity, though further analysis is needed to address potential downsides such as low quality code and privacy concerns."}, "https://arxiv.org/abs/2406.11504": {"title": "On the Feasibility of Fidelity$^-$ for Graph Pruning", "link": "https://arxiv.org/abs/2406.11504", "description": "arXiv:2406.11504v1 Announce Type: cross \nAbstract: As one of popular quantitative metrics to assess the quality of explanation of graph neural networks (GNNs), fidelity measures the output difference after removing unimportant parts of the input graph. Fidelity has been widely used due to its straightforward interpretation that the underlying model should produce similar predictions when features deemed unimportant from the explanation are removed. This raises a natural question: \"Does fidelity induce a global (soft) mask for graph pruning?\" To solve this, we aim to explore the potential of the fidelity measure to be used for graph pruning, eventually enhancing the GNN models for better efficiency. To this end, we propose Fidelity$^-$-inspired Pruning (FiP), an effective framework to construct global edge masks from local explanations. Our empirical observations using 7 edge attribution methods demonstrate that, surprisingly, general eXplainable AI methods outperform methods tailored to GNNs in terms of graph pruning performance."}, "https://arxiv.org/abs/2406.11685": {"title": "Edge Classification on Graphs: New Directions in Topological Imbalance", "link": "https://arxiv.org/abs/2406.11685", "description": "arXiv:2406.11685v1 Announce Type: cross \nAbstract: Recent years have witnessed the remarkable success of applying Graph machine learning (GML) to node/graph classification and link prediction. However, edge classification task that enjoys numerous real-world applications such as social network analysis and cybersecurity, has not seen significant advancement. To address this gap, our study pioneers a comprehensive approach to edge classification. We identify a novel `Topological Imbalance Issue', which arises from the skewed distribution of edges across different classes, affecting the local subgraph of each edge and harming the performance of edge classifications. Inspired by the recent studies in node classification that the performance discrepancy exists with varying local structural patterns, we aim to investigate if the performance discrepancy in topological imbalanced edge classification can also be mitigated by characterizing the local class distribution variance. To overcome this challenge, we introduce Topological Entropy (TE), a novel topological-based metric that measures the topological imbalance for each edge. Our empirical studies confirm that TE effectively measures local class distribution variance, and indicate that prioritizing edges with high TE values can help address the issue of topological imbalance. Based on this, we develop two strategies - Topological Reweighting and TE Wedge-based Mixup - to focus training on (synthetic) edges based on their TEs. While topological reweighting directly manipulates training edge weights according to TE, our wedge-based mixup interpolates synthetic edges between high TE wedges. Ultimately, we integrate these strategies into a novel topological imbalance strategy for edge classification: TopoEdge. Through extensive experiments, we demonstrate the efficacy of our proposed strategies on newly curated datasets and thus establish a new benchmark for (imbalanced) edge classification."}, "https://arxiv.org/abs/2406.11729": {"title": "Secure Cross-Chain Provenance for Digital Forensics Collaboration", "link": "https://arxiv.org/abs/2406.11729", "description": "arXiv:2406.11729v1 Announce Type: cross \nAbstract: In digital forensics and various sectors like medicine and supply chain, blockchains play a crucial role in providing a secure and tamper-resistant system that meticulously records every detail, ensuring accountability. However, collaboration among different agencies, each with its own blockchains, creates challenges due to diverse protocols and a lack of interoperability, hindering seamless information sharing. Cross-chain technology has been introduced to address these challenges. Current research about blockchains in digital forensics, tends to focus on individual agencies, lacking a comprehensive approach to collaboration and the essential aspect of cross-chain functionality. This emphasizes the necessity for a framework capable of effectively addressing challenges in securely sharing case information, implementing access controls, and capturing provenance data across interconnected blockchains. Our solution, ForensiCross, is the first cross-chain solution specifically designed for digital forensics and provenance. It includes BridgeChain and features a unique communication protocol for cross-chain and multi-chain solutions. ForensiCross offers meticulous provenance capture and extraction methods, mathematical analysis to ensure reliability, scalability considerations for a distributed intermediary in collaborative blockchain contexts, and robust security measures against potential vulnerabilities and attacks. Analysis and evaluation results indicate that ForensiCross is secure and, despite a slight increase in communication time, outperforms in node count efficiency and has secure provenance extraction. As an all-encompassing solution, ForensiCross aims to simplify collaborative investigations by ensuring data integrity and traceability."}, "https://arxiv.org/abs/2207.12123": {"title": "Entropy-based random models for hypergraphs", "link": "https://arxiv.org/abs/2207.12123", "description": "arXiv:2207.12123v2 Announce Type: replace \nAbstract: Network theory has primarily focused on pairwise relationships, disregarding many-body interactions: neglecting them, however, can lead to misleading representations of complex systems. Hypergraphs represent an increasingly popular alternative for describing polyadic interactions: our innovation lies in leveraging the representation of hypergraphs based on the incidence matrix for extending the entropy-based framework to higher-order structures. In analogy with the Exponential Random Graphs, we name the members of this novel class of models Exponential Random Hypergraphs. Here, we focus on two explicit examples, i.e. the generalisations of the Erd\\\"os-R\\'enyi Model and of the Configuration Model. After discussing their asymptotic properties, we employ them to analyse real-world configurations: more specifically, i) we extend the definition of several network quantities to hypergraphs, ii) compute their expected value under each null model and iii) compare it with the empirical one, in order to detect deviations from random behaviours. Differently from currently available techniques, ours is analytically tractable, scalable and effective in singling out the structural patterns of real-world hypergraphs differing significantly from those emerging as a consequence of simpler, structural constraints."}, "https://arxiv.org/abs/2211.12301": {"title": "Is this correct? Let's check!", "link": "https://arxiv.org/abs/2211.12301", "description": "arXiv:2211.12301v2 Announce Type: replace \nAbstract: Societal accumulation of knowledge is a complex process. The correctness of new units of knowledge depends not only on the correctness of new reasoning, but also on the correctness of old units that the new one builds on. The errors in such accumulation processes are often remedied by error correction and detection heuristics.\n  Motivating examples include the scientific process based on scientific publications, and software development based on libraries of code.\n  Natural processes that aim to keep errors under control, such as peer review in scientific publications, and testing and debugging in software development, would typically check existing pieces of knowledge -- both for the reasoning that generated them and the previous facts they rely on. In this work, we present a simple process that models such accumulation of knowledge and study the persistence (or lack thereof) of errors.\n  We consider a simple probabilistic model for the generation of new units of knowledge based on the preferential attachment growth model, which additionally allows for errors. Furthermore, the process includes checks aimed at catching these errors. We investigate when effects of errors persist forever in the system (with positive probability) and when they get rooted out completely by the checking process.\n  The two basic parameters associated with the checking process are the {\\em probability} of conducting a check and the depth of the check. We show that errors are rooted out if checks are sufficiently frequent and sufficiently deep. In contrast, shallow or infrequent checks are insufficient to root out errors."}, "https://arxiv.org/abs/2304.06970": {"title": "$\\text{H}^2\\text{TNE}$: Temporal Heterogeneous Information Network Embedding in Hyperbolic Spaces", "link": "https://arxiv.org/abs/2304.06970", "description": "arXiv:2304.06970v3 Announce Type: replace \nAbstract: Temporal heterogeneous information network (temporal HIN) embedding, aiming to represent various types of nodes of different timestamps into low dimensional spaces while preserving structural and semantic information, is of vital importance in diverse real-life tasks. Researchers have made great efforts on temporal HIN embedding in Euclidean spaces and got some considerable achievements. However, there is always a fundamental conflict that many real-world networks show hierarchical property and power-law distribution, and are not isometric of Euclidean spaces. Recently, representation learning in hyperbolic spaces has been proved to be valid for data with hierarchical and power-law structure. Inspired by this character, we propose a hyperbolic heterogeneous temporal network embedding ($\\text{H}^2\\text{TNE}$) model for temporal HINs. Specifically, we leverage a temporally and heterogeneously double-constrained random walk strategy to capture the structural and semantic information, and then calculate the embedding by exploiting hyperbolic distance in proximity measurement. Experimental results show that our method has superior performance on temporal link prediction and node classification compared with SOTA models."}, "https://arxiv.org/abs/2305.01552": {"title": "The Topology of a Family Tree Graph and Its Members' Satisfaction with One Another: A Machine Learning Approach", "link": "https://arxiv.org/abs/2305.01552", "description": "arXiv:2305.01552v2 Announce Type: replace \nAbstract: Family members' satisfaction with one another is central to creating healthy and supportive family environments. In this work, we propose and implement a novel computational technique aimed at exploring the possible relationship between the topology of a given family tree graph and its members' satisfaction with one another. Through an extensive empirical evaluation ($N=486$ families), we show that the proposed technique brings about highly accurate results in predicting family members' satisfaction with one another based solely on the family graph's topology. Furthermore, the results indicate that our technique favorably compares to baseline regression models which rely on established features associated with family members' satisfaction with one another in prior literature."}, "https://arxiv.org/abs/2309.01675": {"title": "Fundamental dynamics of popularity-similarity trajectories in real networks", "link": "https://arxiv.org/abs/2309.01675", "description": "arXiv:2309.01675v2 Announce Type: replace \nAbstract: Real networks are complex dynamical systems, evolving over time with the addition and deletion of nodes and links. Currently, there exists no principled mathematical theory for their dynamics -- a grand-challenge open problem. Here, we show that the popularity and similarity trajectories of nodes in hyperbolic embeddings of different real networks manifest universal self-similar properties with typical Hurst exponents $H \\ll 0.5$. This means that the trajectories are predictable, displaying anti-persistent or 'mean-reverting' behavior, and they can be adequately captured by a fractional Brownian motion process. The observed behavior can be qualitatively reproduced in synthetic networks that possess a latent geometric space, but not in networks that lack such space, suggesting that the observed subdiffusive dynamics are inherently linked to the hidden geometry of real networks. These results set the foundations for rigorous mathematical machinery for describing and predicting real network dynamics."}, "https://arxiv.org/abs/2310.10001": {"title": "Systematic discrepancies in the delivery of political ads on Facebook and Instagram", "link": "https://arxiv.org/abs/2310.10001", "description": "arXiv:2310.10001v2 Announce Type: replace \nAbstract: Political advertising on social media has become a central element in election campaigns. However, granular information about political advertising on social media was previously unavailable, thus raising concerns regarding fairness, accountability, and transparency in the electoral process. In this paper, we analyze targeted political advertising on social media via a unique, large-scale dataset of over 80000 political ads from Meta during the 2021 German federal election, with more than 1.1 billion impressions. For each political ad, our dataset records granular information about targeting strategies, spending, and actual impressions. We then study (i) the prevalence of targeted ads across the political spectrum; (ii) the discrepancies between targeted and actual audiences due to algorithmic ad delivery; and (iii) which targeting strategies on social media attain a wide reach at low cost. We find that targeted ads are prevalent across the entire political spectrum. Moreover, there are considerable discrepancies between targeted and actual audiences, and systematic differences in the reach of political ads (in impressions-per-EUR) among parties, where the algorithm favors ads from populists over others."}, "https://arxiv.org/abs/2402.05006": {"title": "Scalable Algorithm for Finding Balanced Subgraphs with Tolerance in Signed Networks", "link": "https://arxiv.org/abs/2402.05006", "description": "arXiv:2402.05006v2 Announce Type: replace \nAbstract: Signed networks, characterized by edges labeled as either positive or negative, offer nuanced insights into interaction dynamics beyond the capabilities of unsigned graphs. Central to this is the task of identifying the maximum balanced subgraph, crucial for applications like polarized community detection in social networks and portfolio analysis in finance. Traditional models, however, are limited by an assumption of perfect partitioning, which fails to mirror the complexities of real-world data. Addressing this gap, we introduce an innovative generalized balanced subgraph model that incorporates tolerance for irregularities. Our proposed region-based heuristic algorithm, tailored for this NP-hard problem, strikes a balance between low time complexity and high-quality outcomes. Comparative experiments validate its superior performance against leading solutions, delivering enhanced effectiveness (notably larger subgraph sizes) and efficiency (achieving up to 100x speedup) in both traditional and generalized contexts."}, "https://arxiv.org/abs/2312.08672": {"title": "CAT: A Causally Graph Attention Network for Trimming Heterophilic Graph", "link": "https://arxiv.org/abs/2312.08672", "description": "arXiv:2312.08672v3 Announce Type: replace-cross \nAbstract: Local Attention-guided Message Passing Mechanism (LAMP) adopted in Graph Attention Networks (GATs) is designed to adaptively learn the importance of neighboring nodes for better local aggregation on the graph, which can bring the representations of similar neighbors closer effectively, thus showing stronger discrimination ability. However, existing GATs suffer from a significant discrimination ability decline in heterophilic graphs because the high proportion of dissimilar neighbors can weaken the self-attention of the central node, jointly resulting in the deviation of the central node from similar nodes in the representation space. This kind of effect generated by neighboring nodes is called the Distraction Effect (DE) in this paper. To estimate and weaken the DE of neighboring nodes, we propose a Causally graph Attention network for Trimming heterophilic graph (CAT). To estimate the DE, since the DE are generated through two paths (grab the attention assigned to neighbors and reduce the self-attention of the central node), we use Total Effect to model DE, which is a kind of causal estimand and can be estimated from intervened data; To weaken the DE, we identify the neighbors with the highest DE (we call them Distraction Neighbors) and remove them. We adopt three representative GATs as the base model within the proposed CAT framework and conduct experiments on seven heterophilic datasets in three different sizes. Comparative experiments show that CAT can improve the node classification accuracy of all base GAT models. Ablation experiments and visualization further validate the enhancement of discrimination ability brought by CAT. The source code is available at https://github.com/GeoX-Lab/CAT."}, "https://arxiv.org/abs/2403.07183": {"title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews", "link": "https://arxiv.org/abs/2403.07183", "description": "arXiv:2403.07183v2 Announce Type: replace-cross \nAbstract: We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices."}, "https://arxiv.org/abs/2403.11456": {"title": "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models", "link": "https://arxiv.org/abs/2403.11456", "description": "arXiv:2403.11456v3 Announce Type: replace-cross \nAbstract: The widespread use of social media necessitates reliable and efficient detection of offensive content to mitigate harmful effects. Although sophisticated models perform well on individual datasets, they often fail to generalize due to varying definitions and labeling of \"offensive content.\" In this paper, we introduce HateCOT, an English dataset with over 52,000 samples from diverse sources, featuring explanations generated by GPT-3.5Turbo and curated by humans. We demonstrate that pretraining on HateCOT significantly enhances the performance of open-source Large Language Models on three benchmark datasets for offensive content detection in both zero-shot and few-shot settings, despite differences in domain and task. Additionally, HateCOT facilitates effective K-shot fine-tuning of LLMs with limited data and improves the quality of their explanations, as confirmed by our human evaluation."}, "https://arxiv.org/abs/2406.11884": {"title": "Hierarchical Compression of Text-Rich Graphs via Large Language Models", "link": "https://arxiv.org/abs/2406.11884", "description": "arXiv:2406.11884v1 Announce Type: new \nAbstract: Text-rich graphs, prevalent in data mining contexts like e-commerce and academic graphs, consist of nodes with textual features linked by various relations. Traditional graph machine learning models, such as Graph Neural Networks (GNNs), excel in encoding the graph structural information, but have limited capability in handling rich text on graph nodes. Large Language Models (LLMs), noted for their superior text understanding abilities, offer a solution for processing the text in graphs but face integration challenges due to their limitation for encoding graph structures and their computational complexities when dealing with extensive text in large neighborhoods of interconnected nodes. This paper introduces ``Hierarchical Compression'' (HiCom), a novel method to align the capabilities of LLMs with the structure of text-rich graphs. HiCom processes text in a node's neighborhood in a structured manner by organizing the extensive textual information into a more manageable hierarchy and compressing node text step by step. Therefore, HiCom not only preserves the contextual richness of the text but also addresses the computational challenges of LLMs, which presents an advancement in integrating the text processing power of LLMs with the structural complexities of text-rich graphs. Empirical results show that HiCom can outperform both GNNs and LLM backbones for node classification on e-commerce and citation graphs. HiCom is especially effective for nodes from a dense region in a graph, where it achieves a 3.48% average performance improvement on five datasets while being more efficient than LLM backbones."}, "https://arxiv.org/abs/2406.11887": {"title": "Understanding the Dynamics of the Stack Overflow Community through Social Network Analysis and Graph Algorithms", "link": "https://arxiv.org/abs/2406.11887", "description": "arXiv:2406.11887v1 Announce Type: new \nAbstract: This thesis conducts a focused literature review on online communities, centering on Stack Overflow, employing social network analysis and graph algorithms. It examines the evolving landscape of health information quality within the digital ecosystem, emphasizing the challenges posed and the multifaceted nature of quality. The significance of online communities, notably Stack Overflow, as hubs for social interaction and knowledge sharing is underscored. Proposing advanced approaches, the thesis introduces an ensemble deep learning model for traffic flow forecasting, an efficient multi-objective optimization method for influence maximization, and a graph convolutional neural network-based approach for link prediction."}, "https://arxiv.org/abs/2406.11891": {"title": "Towards Adaptive Neighborhood for Advancing Temporal Interaction Graph Modeling", "link": "https://arxiv.org/abs/2406.11891", "description": "arXiv:2406.11891v1 Announce Type: new \nAbstract: Temporal Graph Networks (TGNs) have demonstrated their remarkable performance in modeling temporal interaction graphs. These works can generate temporal node representations by encoding the surrounding neighborhoods for the target node. However, an inherent limitation of existing TGNs is their reliance on fixed, hand-crafted rules for neighborhood encoding, overlooking the necessity for an adaptive and learnable neighborhood that can accommodate both personalization and temporal evolution across different timestamps. In this paper, we aim to enhance existing TGNs by introducing an adaptive neighborhood encoding mechanism. We present SEAN, a flexible plug-and-play model that can be seamlessly integrated with existing TGNs, effectively boosting their performance. To achieve this, we decompose the adaptive neighborhood encoding process into two phases: (i) representative neighbor selection, and (ii) temporal-aware neighborhood information aggregation. Specifically, we propose the Representative Neighbor Selector component, which automatically pinpoints the most important neighbors for the target node. It offers a tailored understanding of each node's unique surrounding context, facilitating personalization. Subsequently, we propose a Temporal-aware Aggregator, which synthesizes neighborhood aggregation by selectively determining the utilization of aggregation routes and decaying the outdated information, allowing our model to adaptively leverage both the contextually significant and current information during aggregation. We conduct extensive experiments by integrating SEAN into three representative TGNs, evaluating their performance on four public datasets and one financial benchmark dataset introduced in this paper. The results demonstrate that SEAN consistently leads to performance improvements across all models, achieving SOTA performance and exceptional robustness."}, "https://arxiv.org/abs/2406.11901": {"title": "Model Evaluation and Anomaly Detection in Temporal Complex Networks using Deep Learning Methods", "link": "https://arxiv.org/abs/2406.11901", "description": "arXiv:2406.11901v1 Announce Type: new \nAbstract: Modeling complex networks allows us to analyze the characteristics and discover the basic mechanisms governing phenomena such as disease outbreaks, information diffusion, transportation efficiency, social influence, and even human brain function. Consequently, various network generative models (called temporal network models) have been presented to model how the network topologies evolve dynamically over time. Temporal network models face the challenge of results evaluation because common evaluation methods are appropriate only for static networks. This paper proposes an automatic approach based on deep learning to handle this issue. In addition to an evaluation method, the proposed method can also be used for anomaly detection in evolving networks. The proposed method has been evaluated on five different datasets, and the evaluations show that it outperforms the alternative methods based on the error rate measure in different datasets."}, "https://arxiv.org/abs/2406.11904": {"title": "Pay Attention to Weak Ties: A Heterogeneous Multiplex Representation Learning Framework for Link Prediction", "link": "https://arxiv.org/abs/2406.11904", "description": "arXiv:2406.11904v1 Announce Type: new \nAbstract: Graph neural networks (GNNs) can learn effective node representations that significantly improve link prediction accuracy. However, most GNN-based link prediction algorithms are incompetent to predict weak ties connecting different communities. Most link prediction algorithms are designed for networks with only one type of relation between nodes but neglect the fact that many complex systems, including transportation and social networks, consisting of multi-modalities of interactions that correspond to different nature of interactions and dynamics that can be modeled as multiplex network, where different types of relation are represented in different layers. This paper proposes a Multi-Relations-aware Graph Neural Network (MRGNN) framework to learn effective node representations for multiplex networks and make more accurate link predictions, especially for weak ties. Specifically, our model utilizes an intra-layer node-level feature propagation process and an inter-layer representation merge process, which applies a simple yet effective logistic or semantic attention voting mechanism to adaptively aggregate information from different layers. Extensive experiments on four diversified multiplex networks show that MRGNN outperforms the state-of-the-art multiplex link prediction algorithms on overall prediction accuracy, and works pretty well on forecasting weak ties"}, "https://arxiv.org/abs/2406.11907": {"title": "Mapping Literary Space: A Social Network from the Timeline of Cultural Events", "link": "https://arxiv.org/abs/2406.11907", "description": "arXiv:2406.11907v1 Announce Type: new \nAbstract: This paper applies social network analysis (SNA) to explore the literary networks of St. Petersburg from 1999 to 2019. By analyzing data from the \"SPbLitGuide\" newsletter, which documents literary events and their participants, we map the connections and collaborations within the literary community. Our approach uses text processing, knowledge extraction, and geographic data to construct a detailed network graph. Using SNA algorithms, we identify key communities and influential figures. The analysis reveals a robust small-world network with strong local clustering and widespread collaboration. These findings provide insights into the structure and dynamics of literary groups in St. Petersburg and provide a foundation for further research in the digital humanities."}, "https://arxiv.org/abs/2406.11924": {"title": "Explainable assessment of financial experts' credibility by classifying social media forecasts and checking the predictions with actual market data", "link": "https://arxiv.org/abs/2406.11924", "description": "arXiv:2406.11924v1 Announce Type: new \nAbstract: Social media include diverse interaction metrics related to user popularity, the most evident example being the number of user followers. The latter has raised concerns about the credibility of the posts by the most popular creators. However, most existing approaches to assess credibility in social media strictly consider this problem a binary classification, often based on a priori information, without checking if actual real-world facts back the users' comments. In addition, they do not provide automatic explanations of their predictions to foster their trustworthiness. In this work, we propose a credibility assessment solution for financial creators in social media that combines Natural Language Processing and Machine Learning. The reputation of the contributors is assessed by automatically classifying their forecasts on asset values by type and verifying these predictions with actual market data to approximate their probability of success. The outcome of this verification is a continuous credibility score instead of a binary result, an entirely novel contribution by this work. Moreover, social media metrics (i.e., user context) are exploited by calculating their correlation with the credibility rankings, providing insights on the interest of the end-users in financial posts and their forecasts (i.e., drop or rise). Finally, the system provides natural language explanations of its decisions based on a model-agnostic analysis of relevant features."}, "https://arxiv.org/abs/2406.12469": {"title": "Tracing the Unseen: Uncovering Human Trafficking Patterns in Job Listings", "link": "https://arxiv.org/abs/2406.12469", "description": "arXiv:2406.12469v1 Announce Type: new \nAbstract: In the shadow of the digital revolution, the insidious issue of human trafficking has found new breeding grounds within the realms of social media and online job boards. Previous research efforts have predominantly centered on identifying victims via the analysis of escort advertisements. However, our work shifts the focus towards enabling a proactive approach: pinpointing potential traffickers before they lure their preys through false job opportunities. In this study, we collect and analyze a vast dataset comprising over a quarter million job postings collected from eight relevant regions across the United States, spanning nearly two decades (2006-2024). The job boards we considered are specifically catered towards Chinese-speaking immigrants in the US. We classify the job posts into distinct groups based on the self-reported information of the posting user. Our investigation into the types of advertised opportunities, the modes of preferred contact, and the frequency of postings uncovers the patterns characterizing suspicious ads. Additionally, we highlight how external events such as health emergencies and conflicts appear to strongly correlate with increased volume of suspicious job posts: traffickers are more likely to prey upon vulnerable populations in times of crises. This research underscores the imperative for a deeper dive into how online job boards and communication platforms could be unwitting facilitators of human trafficking. More importantly, it calls for the urgent formulation of targeted strategies to dismantle these digital conduits of exploitation."}, "https://arxiv.org/abs/2406.12525": {"title": "Anatomy of Elite and Mass Polarization in Social Networks", "link": "https://arxiv.org/abs/2406.12525", "description": "arXiv:2406.12525v1 Announce Type: new \nAbstract: Existing methods for quantifying polarization in social networks typically report a single value describing the amount of polarization in a social system. While this approach can be used to confirm the observation that many societies have witnessed an increase in political polarization in recent years, it misses the complexities that could be used to understand the reasons behind this phenomenon. Notably, opposing groups can have unequal impact on polarization, and the elites are often understood to be more divided than the masses, making it critical to differentiate their roles in polarized systems. We propose a method to characterize these distinct hierarchies in polarized networks, enabling separate polarization measurements for these groups within a single social system. Applied to polarized topics in the Finnish Twittersphere surrounding the 2019 and 2023 parliamentary elections, our analysis reveals valuable insights: 1) The impact of opposing groups on observed polarization is rarely balanced, and 2) while the elite strongly contributes to structural polarization and consistently display greater alignment across various topics, the masses have also recently experienced a surge in issue alignment, a special form of polarization. Our findings suggest that the masses may not be as immune to an increasingly polarized environment as previously thought."}, "https://arxiv.org/abs/2406.12636": {"title": "UEFA Champions League entry is incentive incompatible again from the 2024/25 season", "link": "https://arxiv.org/abs/2406.12636", "description": "arXiv:2406.12636v1 Announce Type: new \nAbstract: A tournament is called incentive incompatible if it allows for a situation where a team could be strictly better off by losing. This paper uncovers that the entry rules of the UEFA Champions League, the most prestigious club football competition in Europe, suffer from this shortcoming again from the 2024/25 season. Consequently, a match with misaligned incentives has been barely avoided in the 2023/24 German Bundesliga. Two straightforward solutions are recommended. Fairness can be easily guaranteed by reversing the order of two paragraphs in the UEFA Champions League regulation."}, "https://arxiv.org/abs/2406.12647": {"title": "Evolution of cooperation with the diversity of cooperation tendencies", "link": "https://arxiv.org/abs/2406.12647", "description": "arXiv:2406.12647v1 Announce Type: new \nAbstract: The complete cooperation and the complete defection are two typical strategies considered in evolutionary games in many previous works. However, in real life, strategies of individuals are full of variety rather than only two complete ones. In this work, the diversity of strategies is introduced into the weak prisoners' dilemma game, which is measured by the diversity of the cooperation tendency. A higher diversity means more cooperation tendencies are provided. The complete cooperation strategy is the full cooperation tendency and the complete defection strategy is without any cooperation tendency. Agents with other cooperation tendencies behave as partial cooperators and as partial defectors simultaneously. The numerical simulation shows that increasing the diversity of the cooperation tendency promotes the cooperation level, not only the number of cooperators but also the average tendency over the whole population, until the diversity reaches its saturated value. Furthermore, our work points out maintaining cooperation is based on the cooperation efficiency approximating to the reward of cooperators and that the cooperation efficiency oscillates and quickly decreases to zero when cooperator clusters cannot resist the invasion of defectors. When the effect of the noise for the Femi update mechanism is considered, a higher diversity of strategies not only improves the cooperation level of the whole population but also supports the survival of more rational agents."}, "https://arxiv.org/abs/2406.11940": {"title": "Model-Based Inference and Experimental Design for Interference Using Partial Network Data", "link": "https://arxiv.org/abs/2406.11940", "description": "arXiv:2406.11940v1 Announce Type: cross \nAbstract: The stable unit treatment value assumption states that the outcome of an individual is not affected by the treatment statuses of others, however in many real world applications, treatments can have an effect on many others beyond the immediately treated. Interference can generically be thought of as mediated through some network structure. In many empirically relevant situations however, complete network data (required to adjust for these spillover effects) are too costly or logistically infeasible to collect. Partially or indirectly observed network data (e.g., subsamples, aggregated relational data (ARD), egocentric sampling, or respondent-driven sampling) reduce the logistical and financial burden of collecting network data, but the statistical properties of treatment effect adjustments from these design strategies are only beginning to be explored. In this paper, we present a framework for the estimation and inference of treatment effect adjustments using partial network data through the lens of structural causal models. We also illustrate procedures to assign treatments using only partial network data, with the goal of either minimizing estimator variance or optimally seeding. We derive single network asymptotic results applicable to a variety of choices for an underlying graph model. We validate our approach using simulated experiments on observed graphs with applications to information diffusion in India and Malawi."}, "https://arxiv.org/abs/2406.12002": {"title": "Modeling, Inference, and Prediction in Mobility-Based Compartmental Models for Epidemiology", "link": "https://arxiv.org/abs/2406.12002", "description": "arXiv:2406.12002v1 Announce Type: cross \nAbstract: Classical compartmental models in epidemiology often struggle to accurately capture real-world dynamics due to their inability to address the inherent heterogeneity of populations. In this paper, we introduce a novel approach that incorporates heterogeneity through a mobility variable, transforming the traditional ODE system into a system of integro-differential equations that describe the dynamics of population densities across different compartments. Our results show that, for the same basic reproduction number, our mobility-based model predicts a smaller final pandemic size compared to classic compartmental models, whose population densities are represented as Dirac delta functions in our density-based framework. This addresses the overestimation issue common in many classical models. Additionally, we demonstrate that the time series of the infected population is sufficient to uniquely identify the mobility distribution. We reconstruct this distribution using a machine-learning-based framework, providing both theoretical and algorithmic support to effectively constrain the mobility-based model with real-world data."}, "https://arxiv.org/abs/2406.12028": {"title": "Mixed-resolution hybrid modeling in an element-based framework", "link": "https://arxiv.org/abs/2406.12028", "description": "arXiv:2406.12028v1 Announce Type: cross \nAbstract: Computational modeling of a complex system is limited by the parts of the system with the least information. While detailed models and high-resolution data may be available for parts of a system, abstract relationships are often necessary to connect the parts and model the full system. For example, modeling food security necessitates the interaction of climate and socioeconomic factors, with models of system components existing at different levels of information in terms of granularity and resolution. Connecting these models is an ongoing challenge. In this work, we demonstrate methodology to quantize and integrate information from data and detailed component models alongside abstract relationships in a hybrid element-based modeling and simulation framework. In a case study of modeling food security, we apply quantization methods to generate (1) time-series model input from climate data and (2) a discrete representation of a component model (a statistical emulator of crop yield), which we then incorporate as an update rule in the hybrid element-based model, bridging differences in model granularity and resolution. Simulation of the hybrid element-based model recapitulated the trends of the original emulator, supporting the use of this methodology to integrate data and information from component models to simulate complex systems."}, "https://arxiv.org/abs/2406.12059": {"title": "A Scalable and Effective Alternative to Graph Transformers", "link": "https://arxiv.org/abs/2406.12059", "description": "arXiv:2406.12059v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs) have shown impressive performance in graph representation learning, but they face challenges in capturing long-range dependencies due to their limited expressive power. To address this, Graph Transformers (GTs) were introduced, utilizing self-attention mechanism to effectively model pairwise node relationships. Despite their advantages, GTs suffer from quadratic complexity w.r.t. the number of nodes in the graph, hindering their applicability to large graphs. In this work, we present Graph-Enhanced Contextual Operator (GECO), a scalable and effective alternative to GTs that leverages neighborhood propagation and global convolutions to effectively capture local and global dependencies in quasilinear time. Our study on synthetic datasets reveals that GECO reaches 169x speedup on a graph with 2M nodes w.r.t. optimized attention. Further evaluations on diverse range of benchmarks showcase that GECO scales to large graphs where traditional GTs often face memory and time limitations. Notably, GECO consistently achieves comparable or superior quality compared to baselines, improving the SOTA up to 4.5%, and offering a scalable and effective solution for large-scale graph learning."}, "https://arxiv.org/abs/2406.12098": {"title": "Circular transformation of the European steel industry renders scrap metal a strategic resource", "link": "https://arxiv.org/abs/2406.12098", "description": "arXiv:2406.12098v1 Announce Type: cross \nAbstract: The steel industry is a major contributor to CO2 emissions, accounting for 7% of global emissions. The European steel industry is seeking to reduce its emissions by increasing the use of electric arc furnaces (EAFs), which can produce steel from scrap, marking a major shift towards a circular steel economy. Here, we show by combining trade with business intelligence data that this shift requires a deep restructuring of the global and European scrap trade, as well as a substantial scaling of the underlying business ecosystem. We find that the scrap imports of European countries with major EAF installations have steadily decreased since 2007 while globally scrap trade started to increase recently. Our statistical modelling shows that every 1,000 tonnes of EAF capacity installed is associated with an increase in annual imports of 550 tonnes and a decrease in annual exports of 1,000 tonnes of scrap, suggesting increased competition for scrap metal as countries ramp up their EAF capacity. Furthermore, each scrap company enables an increase of around 79,000 tonnes of EAF-based steel production per year in the EU. Taking these relations as causal and extrapolating to the currently planned EAF capacity, we find that an additional 730 (SD 140) companies might be required, employing about 35,000 people (IQR 29,000-50,000) and generating an additional estimated turnover of USD 35 billion (IQR 27-48). Our results thus suggest that scrap metal is likely to become a strategic resource. They highlight the need for a massive restructuring of the industry's supply networks and identify the resulting growth opportunities for companies."}, "https://arxiv.org/abs/2406.12119": {"title": "Deploying scalable traffic prediction models for efficient management in real-world large transportation networks during hurricane evacuations", "link": "https://arxiv.org/abs/2406.12119", "description": "arXiv:2406.12119v1 Announce Type: cross \nAbstract: Accurate traffic prediction is vital for effective traffic management during hurricane evacuation. This paper proposes a predictive modeling system that integrates Multilayer Perceptron (MLP) and Long-Short Term Memory (LSTM) models to capture both long-term congestion patterns and short-term speed patterns. Leveraging various input variables, including archived traffic data, spatial-temporal road network information, and hurricane forecast data, the framework is designed to address challenges posed by heterogeneous human behaviors, limited evacuation data, and hurricane event uncertainties. Deployed in a real-world traffic prediction system in Louisiana, the model achieved an 82% accuracy in predicting long-term congestion states over a 6-hour period during a 7-day hurricane-impacted duration. The short-term speed prediction model exhibited Mean Absolute Percentage Errors (MAPEs) ranging from 7% to 13% across evacuation horizons from 1 to 6 hours. Evaluation results underscore the model's potential to enhance traffic management during hurricane evacuations, and real-world deployment highlights its adaptability and scalability in diverse hurricane scenarios within extensive transportation networks."}, "https://arxiv.org/abs/2406.12161": {"title": "Understanding Help-Seeking and Help-Giving on Social Media for Image-Based Sexual Abuse", "link": "https://arxiv.org/abs/2406.12161", "description": "arXiv:2406.12161v1 Announce Type: cross \nAbstract: Image-based sexual abuse (IBSA), like other forms of technology-facilitated abuse, is a growing threat to people's digital safety. Attacks include unwanted solicitations for sexually explicit images, extorting people under threat of leaking their images, or purposefully leaking images to enact revenge or exert control. In this paper, we explore how people seek and receive help for IBSA on social media. Specifically, we identify over 100,000 Reddit posts that engage relationship and advice communities for help related to IBSA. We draw on a stratified sample of 261 posts to qualitatively examine how various types of IBSA unfold, including the mapping of gender, relationship dynamics, and technology involvement to different types of IBSA. We also explore the support needs of victim-survivors experiencing IBSA and how communities help victim-survivors navigate their abuse through technical, emotional, and relationship advice. Finally, we highlight sociotechnical gaps in connecting victim-survivors with important care, regardless of whom they turn to for help."}, "https://arxiv.org/abs/2406.12167": {"title": "Bounds and Bugs: The Limits of Symmetry Metrics to Detect Partisan Gerrymandering", "link": "https://arxiv.org/abs/2406.12167", "description": "arXiv:2406.12167v1 Announce Type: cross \nAbstract: We provide both a theoretical and empirical analysis of the Mean-Median Difference (MM) and Partisan Bias (PB), which are both symmetry metrics intended to detect gerrymandering. We consider vote-share, seat-share pairs $(V, S)$ for which one can construct election data having vote share $V$ and seat share $S$, and turnout is equal in each district. We calculate the range of values that MM and PB can achieve on that constructed election data. In the process, we find the range of vote-share, seat share pairs $(V, S)$ for which there is constructed election data with vote share $V$, seat share $S$, and $MM=0$, and see that the corresponding range for PB is the same set of $(V,S)$ pairs. We show how the set of such $(V,S)$ pairs allowing for $MM=0$ (and $PB=0$) changes when turnout in each district is allowed to be different.\n  Although the set of $(V,S)$ pairs for which there is election data with $MM=0$ is the same as the set of $(V,S)$ pairs for which there is election data with $PB=0$, the range of possible values for MM and PB on a fixed $(V, S)$ is different. Additionally, for a fixed constructed election outcome, the values of the Mean-Median Difference and Partisan Bias can theoretically be as large as 0.5. We show empirically that these two metric values can differ by as much as 0.33 in US congressional map data. We use both neutral ensemble analysis and the short-burst method to show that neither the Mean-Median Difference nor the Partisan Bias can reliably detect when a districting map has an extreme number of districts won by a particular party. Finally, we give additional empirical and logical arguments in an attempt to explain why other metrics are better at detecting when a districting map has an extreme number of districts won by a particular party."}, "https://arxiv.org/abs/2406.12374": {"title": "Problem-Solving in Language Model Networks", "link": "https://arxiv.org/abs/2406.12374", "description": "arXiv:2406.12374v1 Announce Type: cross \nAbstract: To improve the reasoning and question-answering capabilities of Large Language Models (LLMs), several multi-agent approaches have been introduced. While these methods enhance performance, the application of collective intelligence-based approaches to complex network structures and the dynamics of agent interactions remain underexplored. This work extends the concept of multi-agent debate to more general network topologies, measuring the question-answering accuracy, influence, consensus, and the effects of bias on the collective. The results show that random networks perform similarly to fully connected networks despite using significantly fewer tokens. Furthermore, a strong consensus among agents in correlates with correct answers, whereas divided responses typically indicate incorrect answers. Analysing the influence of the agents reveals a balance between self-reflection and interconnectedness; self-reflection aids when local interactions are incorrect, and local interactions aid when the agent itself is incorrect. Additionally, bias plays a strong role in system performance with correctly biased hub nodes boosting performance. These insights suggest that using random networks or scale-free networks with knowledgeable agents placed in central positions can enhance the overall performance of multi-agent systems."}, "https://arxiv.org/abs/2406.12412": {"title": "A Novel Algorithm for Community Detection in Networks using Rough Sets and Consensus Clustering", "link": "https://arxiv.org/abs/2406.12412", "description": "arXiv:2406.12412v1 Announce Type: cross \nAbstract: Complex networks, such as those in social, biological, and technological systems, often present challenges to the task of community detection. Our research introduces a novel rough clustering based consensus community framework (RC-CCD) for effective structure identification of network communities. The RC-CCD method employs rough set theory to handle uncertainties within data and utilizes a consensus clustering approach to aggregate multiple clustering results, enhancing the reliability and accuracy of community detection. This integration allows the RC-CCD to effectively manage overlapping communities, which are often present in complex networks.\n  This approach excels at detecting overlapping communities, offering a detailed and accurate representation of network structures. Comprehensive testing on benchmark networks generated by the Lancichinetti-Fortunato-Radicchi method showcased the strength and adaptability of the new proposal to varying node degrees and community sizes. Cross-comparisons of RC-CCD versus other well known detection algorithms outcomes highlighted its stability and adaptability."}, "https://arxiv.org/abs/2406.12444": {"title": "Who Checks the Checkers? Exploring Source Credibility in Twitter's Community Notes", "link": "https://arxiv.org/abs/2406.12444", "description": "arXiv:2406.12444v1 Announce Type: cross \nAbstract: In recent years, the proliferation of misinformation on social media platforms has become a significant concern. Initially designed for sharing information and fostering social connections, platforms like Twitter (now rebranded as X) have also unfortunately become conduits for spreading misinformation. To mitigate this, these platforms have implemented various mechanisms, including the recent suggestion to use crowd-sourced non-expert fact-checkers to enhance the scalability and efficiency of content vetting. An example of this is the introduction of Community Notes on Twitter.\n  While previous research has extensively explored various aspects of Twitter tweets, such as information diffusion, sentiment analytics and opinion summarization, there has been a limited focus on the specific feature of Twitter Community Notes, despite its potential role in crowd-sourced fact-checking. Prior research on Twitter Community Notes has involved empirical analysis of the feature's dataset and comparative studies that also include other methods like expert fact-checking. Distinguishing itself from prior works, our study covers a multi-faceted analysis of sources and audience perception within Community Notes. We find that the majority of cited sources are news outlets that are left-leaning and are of high factuality, pointing to a potential bias in the platform's community fact-checking. Left biased and low factuality sources validate tweets more, while Center sources are used more often to refute tweet content. Additionally, source factuality significantly influences public agreement and helpfulness of the notes, highlighting the effectiveness of the Community Notes Ranking algorithm. These findings showcase the impact and biases inherent in community-based fact-checking initiatives."}, "https://arxiv.org/abs/2406.12818": {"title": "Optimal Bailouts in Diversified Financial Networks", "link": "https://arxiv.org/abs/2406.12818", "description": "arXiv:2406.12818v1 Announce Type: cross \nAbstract: Widespread default involves substantial deadweight costs which could be countered by injecting capital into failing firms. Injections have positive spillovers that can trigger a repayment cascade. But which firms should a regulator bailout so as to minimize the total injection of capital while ensuring solvency of all firms? While the problem is, in general, NP-hard, for a wide range of networks that arise from a stochastic block model, we show that the optimal bailout can be implemented by a simple policy that targets firms based on their characteristics and position in the network. Specific examples of the setting include core-periphery networks."}, "https://arxiv.org/abs/2406.12835": {"title": "Influence Maximization via Graph Neural Bandits", "link": "https://arxiv.org/abs/2406.12835", "description": "arXiv:2406.12835v1 Announce Type: cross \nAbstract: We consider a ubiquitous scenario in the study of Influence Maximization (IM), in which there is limited knowledge about the topology of the diffusion network. We set the IM problem in a multi-round diffusion campaign, aiming to maximize the number of distinct users that are influenced. Leveraging the capability of bandit algorithms to effectively balance the objectives of exploration and exploitation, as well as the expressivity of neural networks, our study explores the application of neural bandit algorithms to the IM problem. We propose the framework IM-GNB (Influence Maximization with Graph Neural Bandits), where we provide an estimate of the users' probabilities of being influenced by influencers (also known as diffusion seeds). This initial estimate forms the basis for constructing both an exploitation graph and an exploration one. Subsequently, IM-GNB handles the exploration-exploitation tradeoff, by selecting seed nodes in real-time using Graph Convolutional Networks (GCN), in which the pre-estimated graphs are employed to refine the influencers' estimated rewards in each contextual setting. Through extensive experiments on two large real-world datasets, we demonstrate the effectiveness of IM-GNB compared with other baseline methods, significantly improving the spread outcome of such diffusion campaigns, when the underlying network is unknown."}, "https://arxiv.org/abs/2406.12841": {"title": "Demystifying Higher-Order Graph Neural Networks", "link": "https://arxiv.org/abs/2406.12841", "description": "arXiv:2406.12841v1 Announce Type: cross \nAbstract: Higher-order graph neural networks (HOGNNs) are an important class of GNN models that harness polyadic relations between vertices beyond plain edges. They have been used to eliminate issues such as over-smoothing or over-squashing, to significantly enhance the accuracy of GNN predictions, to improve the expressiveness of GNN architectures, and for numerous other goals. A plethora of HOGNN models have been introduced, and they come with diverse neural architectures, and even with different notions of what the \"higher-order\" means. This richness makes it very challenging to appropriately analyze and compare HOGNN models, and to decide in what scenario to use specific ones. To alleviate this, we first design an in-depth taxonomy and a blueprint for HOGNNs. This facilitates designing models that maximize performance. Then, we use our taxonomy to analyze and compare the available HOGNN models. The outcomes of our analysis are synthesized in a set of insights that help to select the most beneficial GNN model in a given scenario, and a comprehensive list of challenges and opportunities for further research into more powerful HOGNNs."}, "https://arxiv.org/abs/2309.15070": {"title": "Timeliness criticality in complex systems", "link": "https://arxiv.org/abs/2309.15070", "description": "arXiv:2309.15070v3 Announce Type: replace \nAbstract: In complex systems, external parameters often determine the phase in which the system operates, i.e., its macroscopic behavior. For nearly a century, statistical physics has extensively studied systems' transitions across phases, (universal) critical exponents, and related dynamical properties. Here we consider the functionality of systems, notably operations in socio-technical ones, production in economic ones and, more generally, any schedule-based system, where timing is of crucial importance. We introduce a stylized model of delay propagation on temporal networks, where the magnitude of delay-mitigating buffer acts as a control parameter. The model exhibits {\\it timeliness criticality}, a novel form of critical behavior. We characterize fluctuations near criticality, commonly referred to as ``avalanches'', and identify the corresponding critical exponents. The model exhibits timeliness criticality also when run on real-world temporal systems such as production networks. Additionally, we explore potential connections with the Mode-Coupling Theory of glasses, the depinning transition and the directed polymer problem."}, "https://arxiv.org/abs/2402.14177": {"title": "Investigating Human Values in Online Communities", "link": "https://arxiv.org/abs/2402.14177", "description": "arXiv:2402.14177v2 Announce Type: replace \nAbstract: Human values play a vital role as an analytical tool in social sciences, enabling the study of diverse dimensions within society as a whole and among individual communities. This paper addresses the limitations of traditional survey-based studies of human values by proposing a computational application of Schwartz's values framework to Reddit, a platform organized into distinct online communities. After ensuring the reliability of automated value extraction tools for Reddit content, we automatically annotate six million posts across 10,000 subreddits with Schwartz values. Our analysis unveils both previously recorded and novel insights into the values prevalent within various online communities. For instance, when examining subreddits with differing opinions on controversial topics, we discover higher universalism values in the Vegan subreddit compared to Carnivores. Additionally, our study of geographically specific subreddits highlights the correlation between traditional values and conservative U.S. states."}, "https://arxiv.org/abs/2402.11114": {"title": "Whose Emotions and Moral Sentiments Do Language Models Reflect?", "link": "https://arxiv.org/abs/2402.11114", "description": "arXiv:2402.11114v2 Announce Type: replace-cross \nAbstract: Language models (LMs) are known to represent the perspectives of some social groups better than others, which may impact their performance, especially on subjective tasks such as content moderation and hate speech detection. To explore how LMs represent different perspectives, existing research focused on positional alignment, i.e., how closely the models mimic the opinions and stances of different groups, e.g., liberals or conservatives. However, human communication also encompasses emotional and moral dimensions. We define the problem of affective alignment, which measures how LMs' emotional and moral tone represents those of different groups. By comparing the affect of responses generated by 36 LMs to the affect of Twitter messages, we observe significant misalignment of LMs with both ideological groups. This misalignment is larger than the partisan divide in the U.S. Even after steering the LMs towards specific ideological perspectives, the misalignment and liberal tendencies of the model persist, suggesting a systemic bias within LMs."}, "https://arxiv.org/abs/2406.12926": {"title": "Diffusion on assortative networks: from mean-field to agent-based, via Newman rewiring", "link": "https://arxiv.org/abs/2406.12926", "description": "arXiv:2406.12926v1 Announce Type: new \nAbstract: In mathematical models of epidemic diffusion on networks based upon systems of differential equations, it is convenient to use the Heterogeneous Mean Field approximation (HMF) because it allows to write one single equation for all nodes of a certain degree $k$, each one virtually present with a probability given by the degree distribution $P(k)$. The two-point correlations between nodes are defined by the matrix $P(h|k)$, which can typically be uncorrelated, assortative or disassortative. After a brief review of this approach and of the results obtained within this approximation for the Bass diffusion model, in this work we look at the transition from the HMF approximation to the description of diffusion through the dynamics of single nodes, first still with differential equations, and then with agent-based models. For this purpose, one needs a method for the explicit construction of ensembles of random networks or scale-free networks having a pre-defined degree distribution (Configuration Model) and a method for rewiring these networks towards some desired or \"target\" degree correlations (Newman Rewiring). We describe Python-NetworkX codes implemented for the two methods in our recent work and compare some of the results obtained in the HMF approximation with the new results obtained with statistical ensembles of real networks, including the case of signed networks."}, "https://arxiv.org/abs/2406.12951": {"title": "Reviewing climate change attribution in UK natural hazards and their impacts", "link": "https://arxiv.org/abs/2406.12951", "description": "arXiv:2406.12951v1 Announce Type: new \nAbstract: The field of Detection and Attribution is rapidly moving beyond weather and climate, and towards incorporating hazards and their impacts on natural and human systems. Here, we review the comprehensive literature base relevant for the UK ahead of the next Climate Change Risk Assessment. The current literature highlights a detectable and non-trivial influence of climate change in many UK impact sectors already - notably health, agriculture, and infrastructure. We found that heatwaves were the most studied hazard overall, with a unanimous consensus on a strong attributable signal of human-induced climate change in their increased frequency and intensity over the last century. The most notable gap identified overall was in attributing climate-related impacts to human influence, with a few impact studies for only a handful of the hazards assessed. Furthermore, just under half of the 29 hazards were not found to have any UK-relevant attribution studies, with most of the remainder having three or fewer. This review highlights requirements for and opportunities to develop attribution scicnce to meet the needs of the UK. Diversifying hazards and impacts studied, in conjunction with the techniques and approaches used, will undoubtedly benefit the community."}, "https://arxiv.org/abs/2406.13016": {"title": "Verhulst Equation and the Universal Pattern for the Global Population Growth", "link": "https://arxiv.org/abs/2406.13016", "description": "arXiv:2406.13016v1 Announce Type: new \nAbstract: The global population growth from 10,000 BC to 2023 is discussed within the Verhulst scaling equation and its extensions framework. The analysis focuses on per the capita global population rate coefficient Gp(P)=[dP(t)/P(t)]/dt=dlnP(t)/d, which reveals two linear domains: from 700CE till 1966 and from 1966 till 2023. Such a pattern can be considered a universal reference for reliable scaling relations describing P(t) changes. It is also the distortions-sensitive test indicating domains of their applicability and yielding optimal values of parameters. For models recalling the Verhulst equation, a single pair of growth rate and system capacity coefficients (r,s) should describe global population rise in the mentioned periods. However, the Verhulst equation with such effective parameters does not describe P(t) changes. Notable is the new way of data preparation, based on collecting data from various sources and their numerical filtering to obtain a smooth set of optimal values enabling the derivative-based analysis. The analysis reveals links between P(t) changes and some historical and pre-historical references influencing the global scale."}, "https://arxiv.org/abs/2406.13075": {"title": "Exact Community Recovery (under Side Information): Optimality of Spectral Algorithms", "link": "https://arxiv.org/abs/2406.13075", "description": "arXiv:2406.13075v1 Announce Type: new \nAbstract: In this paper, we study the problem of exact community recovery in general, two-community block models considering both Bernoulli and Gaussian matrix models, capturing the Stochastic Block Model, submatrix localization, and $\\mathbb{Z}_2$-synchronization as special cases. We also study the settings where $side$ $information$ about community assignment labels is available, modeled as passing the true labels through a noisy channel: either the binary erasure channel (where some community labels are known while others are erased) or the binary symmetric channel (where some labels are flipped). We provide a unified analysis of the effect of side information on the information-theoretic limits of exact recovery, generalizing prior works and extending to new settings. Additionally, we design a simple but optimal spectral algorithm that incorporates side information (when present) along with the eigenvectors of the matrix observation. Using the powerful tool of entrywise eigenvector analysis [Abbe, Fan, Wang, Zhong 2020], we show that our spectral algorithm can mimic the so called $genie$-$aided$ $estimators$, where the $i^{\\mathrm{th}}$ genie-aided estimator optimally computes the estimate of the $i^{\\mathrm{th}}$ label, when all remaining labels are revealed by a genie. This perspective provides a unified understanding of the optimality of spectral algorithms for various exact recovery problems in a recent line of work."}, "https://arxiv.org/abs/2406.13299": {"title": "Empirical Evaluation of Integrated Trust Mechanism to Improve Trust in E-commerce Services", "link": "https://arxiv.org/abs/2406.13299", "description": "arXiv:2406.13299v1 Announce Type: new \nAbstract: There are mostly two approaches to tackle trust management worldwide Strong and crisp and Soft and Social. We analyze the impact of integrated trust mechanism in three different e-commerce services. The trust aspect is a dormant element between potential users and being developed expert or internet systems. We support our integration by preside over an experiment in controlled laboratory environment. The model selected for the experiment is a composite of policy and reputation based trust mechanisms and widely acknowledged in e-commerce industry. The integration between policy and trust mechanism was accomplished through mapping process, weakness of one brought to a close with the strength of other. Furthermore, experiment has been supervised to validate the effectiveness of implementation by segregating both integrated and traditional trust mechanisms in learning system"}, "https://arxiv.org/abs/2406.13499": {"title": "GraphMU: Repairing Robustness of Graph Neural Networks via Machine Unlearning", "link": "https://arxiv.org/abs/2406.13499", "description": "arXiv:2406.13499v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs) have demonstrated significant application potential in various fields. However, GNNs are still vulnerable to adversarial attacks. Numerous adversarial defense methods on GNNs are proposed to address the problem of adversarial attacks. However, these methods can only serve as a defense before poisoning, but cannot repair poisoned GNN. Therefore, there is an urgent need for a method to repair poisoned GNN. In this paper, we address this gap by introducing the novel concept of model repair for GNNs. We propose a repair framework, Repairing Robustness of Graph Neural Networks via Machine Unlearning (GraphMU), which aims to fine-tune poisoned GNN to forget adversarial samples without the need for complete retraining. We also introduce a unlearning validation method to ensure that our approach effectively forget specified poisoned data. To evaluate the effectiveness of GraphMU, we explore three fine-tuned subgraph construction scenarios based on the available perturbation information: (i) Known Perturbation Ratios, (ii) Known Complete Knowledge of Perturbations, and (iii) Unknown any Knowledge of Perturbations. Our extensive experiments, conducted across four citation datasets and four adversarial attack scenarios, demonstrate that GraphMU can effectively restore the performance of poisoned GNN."}, "https://arxiv.org/abs/2406.13734": {"title": "A Unified Core Structure in Multiplex Networks: From Finding the Densest Subgraph to Modeling User Engagement", "link": "https://arxiv.org/abs/2406.13734", "description": "arXiv:2406.13734v1 Announce Type: new \nAbstract: In many complex systems, the interactions between objects span multiple aspects. Multiplex networks are accurate paradigms to model such systems, where each edge is associated with a type. A key graph mining primitive is extracting dense subgraphs, and this has led to interesting notions such as K-cores, known as building blocks of complex networks. Despite recent attempts to extend the notion of core to multiplex networks, existing studies suffer from a subset of the following limitations: They 1) force all nodes to exhibit their high degree in the same set of relation types while in multiplex networks some connection types can be noisy for some nodes, 2) either require high computational cost or miss the complex information of multiplex networks, and 3) assume the same importance for all relation types. We introduce S-core, a novel and unifying family of dense structures in multiplex networks that uses a function S(.) to summarize the degree vector of each node. We then discuss how one can choose a proper S(.) from the data. To demonstrate the usefulness of S-cores, we focus on finding the densest subgraph as well as modeling user engagement in multiplex networks. We present a new density measure in multiplex networks and discuss its advantages over existing density measures. We show that the problem of finding the densest subgraph in multiplex networks is NP-hard and design an efficient approximation algorithm based on S-cores. Finally, we present a new mathematical model of user engagement in the presence of different relation types. Our experiments shows the efficiency and effectiveness of our algorithms and supports the proposed mathematical model of user engagement."}, "https://arxiv.org/abs/2406.13789": {"title": "Death, Taxes, and Inequality", "link": "https://arxiv.org/abs/2406.13789", "description": "arXiv:2406.13789v1 Announce Type: new \nAbstract: Income inequalities and redistribution policies are modeled with a minimal, endogenous model of a simple foraging economy. The model is scaled to match human lifespans and overall death rates. Stochastic income distributions from the model are compared to empirical data from actual economies. Empirical data are fit to implied distributions providing necessary resolution for comparison. The impacts of redistribution policies on total wealth, income distributions, and inequality are shown to be similar for the empirical data and the model. These comparisons enable detailed determinations of population welfare beyond what is possible with total wealth and inequality metrics. Estate taxes in the model appear quite effective in reducing inequality without reducing total wealth. Significant income inequality emerges for the model for a population of equally capable individuals presented with equal opportunities. Stochastic population instability at both the high and low ends of infertility are considered."}, "https://arxiv.org/abs/2406.13816": {"title": "The Dangerous Allure of Low Fertility", "link": "https://arxiv.org/abs/2406.13816", "description": "arXiv:2406.13816v1 Announce Type: new \nAbstract: Stochastic population and wealth trajectories for societies as functions of fertility are modeled with a minimal, endogenous model of a simple foraging economy. The model is scaled to match human lifespans and overall death rates. Stochastic population instability at both the high and low ends of fertility are considered. Lower population levels, caused by low fertility, generate concerns on economic growth, military security, and international political power; while also seen by some as reducing ecological and environmental damage. The model shows that increasingly low fertility leads to both higher wealth and lower population levels. As society is encouraged by increasing per capita wealth to continue to decrease fertility, dangerous population regimes are reached where stochastic extinction becomes more and more likely."}, "https://arxiv.org/abs/2406.14460": {"title": "Podcast Outcasts: Understanding Rumble's Podcast Dynamics", "link": "https://arxiv.org/abs/2406.14460", "description": "arXiv:2406.14460v1 Announce Type: new \nAbstract: Podcasting on Rumble, an alternative video-sharing platform, attracts controversial figures known for spreading divisive and often misleading content, which sharply contrasts with YouTube's more regulated environment. Motivated by the growing impact of podcasts on political discourse, as seen with figures like Joe Rogan and Andrew Tate, this paper explores the political biases and content strategies used by these platforms. In this paper, we conduct a comprehensive analysis of over 13K podcast videos from both YouTube and Rumble, focusing on their political content and the dynamics of their audiences. Using advanced speech-to-text transcription, topic modeling, and contrastive learning techniques, we explore three critical aspects: the presence of political bias in podcast channels, the nature of content that drives podcast views, and the usage of visual elements in these podcasts. Our findings reveal a distinct right-wing orientation in Rumble's podcasts, contrasting with YouTube's more diverse and apolitical content."}, "https://arxiv.org/abs/2406.14522": {"title": "Learning thresholds lead to stable language coexistence", "link": "https://arxiv.org/abs/2406.14522", "description": "arXiv:2406.14522v1 Announce Type: new \nAbstract: We introduce a language competition model that incorporates the effects of memory and learning on the language shift dynamics, using the Abrams-Strogatz model as a starting point. On a coarse grained time scale, the effects of memory and learning can be expressed as thresholds on the speakers fractions. In its simplest form, the resulting model is exactly solvable. Besides the consensus on one of the two languages, the model describes additional equilibrium states that are not present in the Abrams-Strogatz model: a stable coexistence of the two languages, if both thresholds are low enough, so that the language shift processes in the two opposite directions compensate each other, and a frozen state coinciding with the initial state, when both thresholds are too high for any language shift to take place. We show numerically that these results are preserved for threshold functions of a more general shape."}, "https://arxiv.org/abs/2406.12892": {"title": "Synthesis and Characterization of NiCoMn MOFs for Wastewater Treatment", "link": "https://arxiv.org/abs/2406.12892", "description": "arXiv:2406.12892v1 Announce Type: cross \nAbstract: Water pollution has become a global problem. Sources of wastewater majorly include industrial and commercial sectors. To cater to the exponential increase in clean water, efficient technologies are needed to treat wastewater. Several techniques such as redox reactions, membrane filtrations, mechanical processes, chemical treatment and adsorption techniques have been employed. However, their cost and effectiveness is still a major problem. In this study, we employed an effective wastewater treatment technique by synthesizing NiCoMn MOFs using a simple hydrothermal technique and characterized the properties using XRD and SEM for their possible characteristics. XRD analysis confirmed the successful synthesis of NiCoMn MOFs. Sufficient information regarding the surface morphology and topology was given by the SEM analysis which proved a nanoporous structure with high surface area effective for adsorption and oxidative catalysis of contaminants in wastewater. Moreover, a high electrostatic attraction between the MOFs was observed which could attract oppositely charged contaminants. The results showed a high potential for the synthesized NiCoMn MOFs for wastewater treatment applications."}, "https://arxiv.org/abs/2406.13201": {"title": "Toward Structure Fairness in Dynamic Graph Embedding: A Trend-aware Dual Debiasing Approach", "link": "https://arxiv.org/abs/2406.13201", "description": "arXiv:2406.13201v1 Announce Type: cross \nAbstract: Recent studies successfully learned static graph embeddings that are structurally fair by preventing the effectiveness disparity of high- and low-degree vertex groups in downstream graph mining tasks. However, achieving structure fairness in dynamic graph embedding remains an open problem. Neglecting degree changes in dynamic graphs will significantly impair embedding effectiveness without notably improving structure fairness. This is because the embedding performance of high-degree and low-to-high-degree vertices will significantly drop close to the generally poorer embedding performance of most slightly changed vertices in the long-tail part of the power-law distribution. We first identify biased structural evolutions in a dynamic graph based on the evolving trend of vertex degree and then propose FairDGE, the first structurally Fair Dynamic Graph Embedding algorithm. FairDGE learns biased structural evolutions by jointly embedding the connection changes among vertices and the long-short-term evolutionary trend of vertex degrees. Furthermore, a novel dual debiasing approach is devised to encode fair embeddings contrastively, customizing debiasing strategies for different biased structural evolutions. This innovative debiasing strategy breaks the effectiveness bottleneck of embeddings without notable fairness loss. Extensive experiments demonstrate that FairDGE achieves simultaneous improvement in the effectiveness and fairness of embeddings."}, "https://arxiv.org/abs/2406.13303": {"title": "Integration of Policy and Reputation based Trust Mechanisms in e-Commerce Industry", "link": "https://arxiv.org/abs/2406.13303", "description": "arXiv:2406.13303v1 Announce Type: cross \nAbstract: The e-commerce systems are being tackled from commerce behavior and internet technologies. Therefore, trust aspect between buyer-seller transactions is a potential element which needs to be addressed in competitive e-commerce industry. The e-commerce industry is currently handling two different trust approaches. First approach consists on centralized mechanism where digital credentials/set of rules assembled, called Policy based trust mechanisms . Second approach consists on decentralized trust mechanisms where reputation, points assembled and shared, called Reputation based trust mechanisms. The difference between reputation and policy based trust mechanism will be analyzed and recommendations would be proposed to increase trust between buyer and seller in e-commerce industry. The integration of trust mechanism is proposed through mapping process, strength of one mechanism with the weakness of other. The proposed model for integrated mechanism will be presented and illustrated how the proposed model will be used in real world e-commerce industry."}, "https://arxiv.org/abs/2406.13369": {"title": "Effective Edge-wise Representation Learning in Edge-Attributed Bipartite Graphs", "link": "https://arxiv.org/abs/2406.13369", "description": "arXiv:2406.13369v1 Announce Type: cross \nAbstract: Graph representation learning (GRL) is to encode graph elements into informative vector representations, which can be used in downstream tasks for analyzing graph-structured data and has seen extensive applications in various domains. However, the majority of extant studies on GRL are geared towards generating node representations, which cannot be readily employed to perform edge-based analytics tasks in edge-attributed bipartite graphs (EABGs) that pervade the real world, e.g., spam review detection in customer-product reviews and identifying fraudulent transactions in user-merchant networks. Compared to node-wise GRL, learning edge representations (ERL) on such graphs is challenging due to the need to incorporate the structure and attribute semantics from the perspective of edges while considering the separate influence of two heterogeneous node sets U and V in bipartite graphs. To our knowledge, despite its importance, limited research has been devoted to this frontier, and existing workarounds all suffer from sub-par results.\n  Motivated by this, this paper designs EAGLE, an effective ERL method for EABGs. Building on an in-depth and rigorous theoretical analysis, we propose the factorized feature propagation (FFP) scheme for edge representations with adequate incorporation of long-range dependencies of edges/features without incurring tremendous computation overheads. We further ameliorate FFP as a dual-view FFP by taking into account the influences from nodes in U and V severally in ERL. Extensive experiments on 5 real datasets showcase the effectiveness of the proposed EAGLE models in semi-supervised edge classification tasks. In particular, EAGLE can attain a considerable gain of at most 38.11% in AP and 1.86% in AUC when compared to the best baselines."}, "https://arxiv.org/abs/2406.13452": {"title": "Quantum Networks: from Multipartite Entanglement to Hypergraph Immersion", "link": "https://arxiv.org/abs/2406.13452", "description": "arXiv:2406.13452v1 Announce Type: cross \nAbstract: Multipartite entanglement, a higher-order interaction unique to quantum information, offers various advantages over bipartite entanglement in quantum network (QN) applications. Establishing multipartite entanglement across remote parties in QN requires entanglement routing, which irreversibly transforms the QN topology at the cost of existing entanglement links. Here, we address the question of whether a QN can be topologically transformed into another via entanglement routing. Our key result is an exact mapping from multipartite entanglement routing to Nash-Williams's graph immersion problem, extended to hypergraphs. This generalized hypergraph immersion problem introduces a partial order between QN topologies, permitting certain topological transformations while precluding others, offering discerning insights into the design and manipulation of higher-order network topologies in QNs."}, "https://arxiv.org/abs/2406.13605": {"title": "Nicer Than Humans: How do Large Language Models Behave in the Prisoner's Dilemma?", "link": "https://arxiv.org/abs/2406.13605", "description": "arXiv:2406.13605v1 Announce Type: cross \nAbstract: The behavior of Large Language Models (LLMs) as artificial social agents is largely unexplored, and we still lack extensive evidence of how these agents react to simple social stimuli. Testing the behavior of AI agents in classic Game Theory experiments provides a promising theoretical framework for evaluating the norms and values of these agents in archetypal social situations. In this work, we investigate the cooperative behavior of Llama2 when playing the Iterated Prisoner's Dilemma against random adversaries displaying various levels of hostility. We introduce a systematic methodology to evaluate an LLM's comprehension of the game's rules and its capability to parse historical gameplay logs for decision-making. We conducted simulations of games lasting for 100 rounds, and analyzed the LLM's decisions in terms of dimensions defined in behavioral economics literature. We find that Llama2 tends not to initiate defection but it adopts a cautious approach towards cooperation, sharply shifting towards a behavior that is both forgiving and non-retaliatory only when the opponent reduces its rate of defection below 30%. In comparison to prior research on human participants, Llama2 exhibits a greater inclination towards cooperative behavior. Our systematic approach to the study of LLMs in game theoretical scenarios is a step towards using these simulations to inform practices of LLM auditing and alignment."}, "https://arxiv.org/abs/2406.13920": {"title": "Explainable AI Security: Exploring Robustness of Graph Neural Networks to Adversarial Attacks", "link": "https://arxiv.org/abs/2406.13920", "description": "arXiv:2406.13920v1 Announce Type: cross \nAbstract: Graph neural networks (GNNs) have achieved tremendous success, but recent studies have shown that GNNs are vulnerable to adversarial attacks, which significantly hinders their use in safety-critical scenarios. Therefore, the design of robust GNNs has attracted increasing attention. However, existing research has mainly been conducted via experimental trial and error, and thus far, there remains a lack of a comprehensive understanding of the vulnerability of GNNs. To address this limitation, we systematically investigate the adversarial robustness of GNNs by considering graph data patterns, model-specific factors, and the transferability of adversarial examples. Through extensive experiments, a set of principled guidelines is obtained for improving the adversarial robustness of GNNs, for example: (i) rather than highly regular graphs, the training graph data with diverse structural patterns is crucial for model robustness, which is consistent with the concept of adversarial training; (ii) the large model capacity of GNNs with sufficient training data has a positive effect on model robustness, and only a small percentage of neurons in GNNs are affected by adversarial attacks; (iii) adversarial transfer is not symmetric and the adversarial examples produced by the small-capacity model have stronger adversarial transferability. This work illuminates the vulnerabilities of GNNs and opens many promising avenues for designing robust GNNs."}, "https://arxiv.org/abs/2406.14290": {"title": "Examining the Implications of Deepfakes for Election Integrity", "link": "https://arxiv.org/abs/2406.14290", "description": "arXiv:2406.14290v1 Announce Type: cross \nAbstract: It is becoming cheaper to launch disinformation operations at scale using AI-generated content, in particular 'deepfake' technology. We have observed instances of deepfakes in political campaigns, where generated content is employed to both bolster the credibility of certain narratives (reinforcing outcomes) and manipulate public perception to the detriment of targeted candidates or causes (adversarial outcomes). We discuss the threats from deepfakes in politics, highlight model specifications underlying different types of deepfake generation methods, and contribute an accessible evaluation of the efficacy of existing detection methods. We provide this as a summary for lawmakers and civil society actors to understand how the technology may be applied in light of existing policies regulating its use. We highlight the limitations of existing detection mechanisms and discuss the areas where policies and regulations are required to address the challenges of deepfakes."}, "https://arxiv.org/abs/2303.01934": {"title": "CONTAIN: A Community-based Algorithm for Network Immunization", "link": "https://arxiv.org/abs/2303.01934", "description": "arXiv:2303.01934v2 Announce Type: replace \nAbstract: Network immunization is an automated task in the field of network analysis that involves protecting a network (modeled as a graph) from being infected by an undesired arbitrary diffusion. In this article, we consider the spread of harmful content in social networks, and we propose CONTAIN, a novel COmmuNiTy-based Algorithm for network ImmuNization. Our solution uses the network information to (1) detect harmful content spreaders, and (2) generate partitions and rank them for immunization using the subgraphs induced by each spreader, i.e., employing CONTAIN. The experimental results obtained on real-world datasets show that CONTAIN outperforms state-of-the-art solutions, i.e., NetShield and SparseShield, by immunizing the network in fewer iterations, thus, converging significantly faster than the state-of-the-art algorithms. We also compared our solution in terms of scalability with the state-of-the-art tree-based mitigation algorithm MCWDST, as well as with NetShield and SparseShield. We can conclude that our solution outperforms MCWDST and NetShield."}, "https://arxiv.org/abs/2307.13520": {"title": "Using power system modelling outputs to identify weather-induced extreme events in highly renewable systems", "link": "https://arxiv.org/abs/2307.13520", "description": "arXiv:2307.13520v2 Announce Type: replace \nAbstract: In highly renewable power systems the increased weather dependence can result in new resilience challenges, such as renewable energy droughts, or a lack of sufficient renewable generation at times of high demand. The weather conditions responsible for these challenges have been well-studied in the literature. However, in reality multi-day resilience challenges are triggered by complex interactions between high demand, low renewable availability, electricity transmission constraints and storage dynamics. We show these challenges cannot be rigorously understood from an exclusively power systems, or meteorological, perspective. We propose a new method that uses electricity shadow prices - obtained by a European power system model based on 40 years of reanalysis data - to identify the most difficult periods driving system investments. Such difficult periods are driven by large-scale weather conditions such as low wind and cold temperature periods of various lengths associated with stationary high pressure over Europe. However, purely meteorological approaches fail to identify which events lead to the largest system stress over the multi-decadal study period due to the influence of subtle transmission bottlenecks and storage issues across multiple regions. These extreme events also do not relate strongly to traditional weather patterns (such as Euro-Atlantic weather regimes or the North Atlantic Oscillation index). We therefore compile a new set of weather patterns to define energy system stress events which include the impacts of electricity storage and large-scale interconnection. Without interdisciplinary studies combining state-of-the-art energy meteorology and modelling, further strive for adequate renewable power systems will be hampered."}, "https://arxiv.org/abs/2311.06436": {"title": "Augmented Degree Correction for Bipartite Networks with Applications to Recommender Systems", "link": "https://arxiv.org/abs/2311.06436", "description": "arXiv:2311.06436v2 Announce Type: replace \nAbstract: In recommender systems, users rate items, and are subsequently served other product recommendations based on these ratings. Even though users usually rate a tiny percentage of the available items, the system tries to estimate unobserved preferences by finding similarities across users and across items. In this work, we treat the observed ratings data as partially observed, dense, weighted, bipartite networks. For a class of systems without outside information, we adapt an approach developed for dense, weighted networks to account for unobserved edges and the bipartite nature of the problem. This approach allows for community structure, and for local estimation of flexible patterns of ratings across different pairs of communities. We compare the performance of our proposed approach to existing methods on a simulated data set, as well as on a data set of joke ratings, examining model performance in both cases at differing levels of sparsity."}, "https://arxiv.org/abs/2403.10277": {"title": "Delayed interactions in the noisy voter model through the periodic polling mechanism", "link": "https://arxiv.org/abs/2403.10277", "description": "arXiv:2403.10277v3 Announce Type: replace \nAbstract: We investigate the effects of delayed interactions on the stationary distribution of the noisy voter model. We assume that the delayed interactions occur through the periodic polling mechanism and replace the original instantaneous two-agent interactions. In our analysis, we require that the polling period aligns with the delay in announcing poll outcomes. As expected, when the polling period is relatively short, the model with delayed interactions is almost equivalent to the original model. As the polling period increases, oscillatory behavior emerges, but the model with delayed interactions still converges to stationary distribution. The stationary distribution resembles a Beta-binomial distribution, with its shape parameters scaling with the polling period. The observed scaling behavior is non-monotonic. Namely, the shape parameters peak at some intermediate polling period."}, "https://arxiv.org/abs/2311.09086": {"title": "The Uli Dataset: An Exercise in Experience Led Annotation of oGBV", "link": "https://arxiv.org/abs/2311.09086", "description": "arXiv:2311.09086v2 Announce Type: replace-cross \nAbstract: Online gender based violence has grown concomitantly with adoption of the internet and social media. Its effects are worse in the Global majority where many users use social media in languages other than English. The scale and volume of conversations on the internet has necessitated the need for automated detection of hate speech, and more specifically gendered abuse. There is, however, a lack of language specific and contextual data to build such automated tools. In this paper we present a dataset on gendered abuse in three languages- Hindi, Tamil and Indian English. The dataset comprises of tweets annotated along three questions pertaining to the experience of gender abuse, by experts who identify as women or a member of the LGBTQIA community in South Asia. Through this dataset we demonstrate a participatory approach to creating datasets that drive AI systems."}, "https://arxiv.org/abs/2406.14681": {"title": "Intercity Connectivity and Innovation", "link": "https://arxiv.org/abs/2406.14681", "description": "arXiv:2406.14681v1 Announce Type: new \nAbstract: Urban outputs, from economy to innovation, are known to grow as a power of a city's population. But, since large cities tend to be central in transportation and communication networks, the effects attributed to city size may be confounded with those of intercity connectivity. Here, we map intercity networks for the world's two largest economies (the United States and China) to explore whether a city's position in the networks of communication, human mobility, and scientific collaboration explains variance in a city's patenting activity that is unaccounted for by its population. We find evidence that models incorporating intercity connectivity outperform population-based models and exhibit stronger predictive power for patenting activity, particularly for technologies of more recent vintage (which we expect to be more complex or sophisticated). The effects of intercity connectivity are more robust in China, even after controlling for population, GDP, and education, but not in the United States once adjusted for GDP and education. This divergence suggests distinct urban network dynamics driving innovation in these regions. In China, models with social media and mobility networks explain more heterogeneity in the scaling of innovation, whereas in the United States, scientific collaboration plays a more significant role. These findings support the significance of a city's position within the intercity network in shaping its success in innovative activities."}, "https://arxiv.org/abs/2406.14692": {"title": "A Review of Spatial Network Insights and Methods in the Context of Planning: Applications, Challenges, and Opportunities", "link": "https://arxiv.org/abs/2406.14692", "description": "arXiv:2406.14692v1 Announce Type: new \nAbstract: With the rise of geospatial big data, new narratives of cities based on spatial networks and flows have replaced the traditional focus on locations. While plenty of research have empirically analyzed network structures, there lacks a state-of-the-art synthesis of applicable insights and methods of spatial networks in the planning context. In this chapter, we reviewed the theories, concepts, methods, and applications of spatial network analysis in cities and their insights for planners from four areas of concern: spatial structures, urban infrastructure optimizations, indications of economic wealth, social capital, and residential mobility, and public health control (especially COVID-19). We also outlined four challenges that planners face when taking the planning knowledge from spatial networks to actions: data openness and privacy, linkage to direct policy implications, lack of civic engagement, and the difficulty to visualize and integrate with GIS. Finally, we envisioned how spatial networks can be integrated into a collaborative planning framework."}, "https://arxiv.org/abs/2406.14698": {"title": "Generating geographically and economically realistic large-scale synthetic contact networks: A general method using publicly available data", "link": "https://arxiv.org/abs/2406.14698", "description": "arXiv:2406.14698v1 Announce Type: new \nAbstract: Synthetic contact networks are useful for modeling epidemic spread and social transmission, but data to infer realistic contact patterns that take account of assortative connections at the geographic and economic levels is limited. We developed a method to generate synthetic contact networks for any region of the United States based on publicly available data. First, we generate a synthetic population of individuals within households from US census data using combinatorial optimization. Then, individuals are assigned to workplaces and schools using commute data, employment statistics, and school enrollment data. The resulting population is then connected into a realistic contact network using graph generation algorithms. We test the method on two census regions and show that the synthetic populations accurately reflect the source data. We further show that the contact networks have distinct properties compared to networks generated without a synthetic population, and that those differences affect the rate of disease transmission in an epidemiological simulation. We provide open-source software to generate a synthetic population and contact network for any area within the US."}, "https://arxiv.org/abs/2406.14751": {"title": "Complex network community discovery using fast local move iterated greedy algorithm", "link": "https://arxiv.org/abs/2406.14751", "description": "arXiv:2406.14751v1 Announce Type: new \nAbstract: Examining the community structures within intricate networks is crucial for comprehending their intrinsic dynamics and functionality. The paper presents the Fast Local Move Iterated Greedy (FLMIG) algorithm, a novel method designed to effectively identify community structures in intricate networks. The FLMIG algorithm improves the modularity optimization process by including a rapid local move heuristic and an iterated greedy mechanism that switches between destructive and constructive phases to strengthen the community partitions. The main innovation is the integration of random neighbor moves with an enhanced Prune Louvain algorithm, which guarantees fast convergence while maintaining the connection of the identified communities. The results of our comprehensive studies, conducted on both synthetic and and real-world networks, clearly show that FLMIG surpasses existing cutting-edge techniques in terms of both accuracy and computing efficiency. This algorithm not only provides a strong tool for identifying communities, but also makes a valuable contribution to the broader field of network analysis by offering a method that can effectively handle large-scale and dynamically evolving networks."}, "https://arxiv.org/abs/2406.14913": {"title": "Cooperative bots exhibit nuanced effects on cooperation across strategic frameworks", "link": "https://arxiv.org/abs/2406.14913", "description": "arXiv:2406.14913v1 Announce Type: new \nAbstract: The positive impact of cooperative bots on cooperation within evolutionary game theory is well documented; however, existing studies have predominantly used discrete strategic frameworks, focusing on deterministic actions with a fixed probability of one. This paper extends the investigation to continuous and mixed strategic approaches. Continuous strategies employ intermediate probabilities to convey varying degrees of cooperation and focus on expected payoffs. In contrast, mixed strategies calculate immediate payoffs from actions chosen at a given moment within these probabilities. Using the prisoner's dilemma game, this study examines the effects of cooperative bots on human cooperation within hybrid populations of human players and simple bots, across both well-mixed and structured populations. Our findings reveal that cooperative bots significantly enhance cooperation in both population types across these strategic approaches under weak imitation scenarios, where players are less concerned with material gains. However, under strong imitation scenarios, while cooperative bots do not alter the defective equilibrium in well-mixed populations, they have varied impacts in structured populations across these strategic approaches. Specifically, they disrupt cooperation under discrete and continuous strategies but facilitate it under mixed strategies. These results highlight the nuanced effects of cooperative bots within different strategic frameworks and underscore the need for careful deployment, as their effectiveness is highly sensitive to how humans update their actions and their chosen strategic approach."}, "https://arxiv.org/abs/2406.14922": {"title": "Social learning with complex contagion", "link": "https://arxiv.org/abs/2406.14922", "description": "arXiv:2406.14922v1 Announce Type: new \nAbstract: We introduce a mathematical model that combines the concepts of complex contagion with payoff-biased imitation, to describe how social behaviors spread through a population. Traditional models of social learning by imitation are based on simple contagion -- where an individual may imitate a more successful neighbor following a single interaction. Our framework generalizes this process to incorporate complex contagion, which requires multiple exposures before an individual considers adopting a different behavior. We formulate this as a discrete time and state stochastic process in a finite population, and we derive its continuum limit as an ordinary differential equation that generalizes the replicator equation, the most widely used dynamical model in evolutionary game theory. When applied to linear frequency-dependent games, our social learning with complex contagion produces qualitatively different outcomes than traditional imitation dynamics: it can shift the Prisoner's Dilemma from a unique all-defector equilibrium to either a stable mixture of cooperators and defectors in the population, or a bistable system; it changes the Snowdrift game from a single to a bistable equilibrium; and it can alter the Coordination game from bistability at the boundaries to two internal equilibria. The long-term outcome depends on the balance between the complexity of the contagion process and the strength of selection that biases imitation towards more successful types. Our analysis intercalates the fields of evolutionary game theory with complex contagions, and it provides a synthetic framework that describes more realistic forms of behavioral change in social systems."}, "https://arxiv.org/abs/2406.15185": {"title": "Percolation transition of k-frequent destinations network for urban mobility", "link": "https://arxiv.org/abs/2406.15185", "description": "arXiv:2406.15185v1 Announce Type: new \nAbstract: Urban spatial interactions are a complex aggregation of routine visits and random explorations by individuals. The inherent uncertainty of these random visitations poses significant challenges to understanding urban structures and socioeconomic developments. To capture the core dynamics of urban interaction networks, we analyze the percolation structure of the $k$-most frequented destinations of intracity place-to-place flows from mobile phone data of eight major U.S. cities at a Census Block Group (CBG) level. Our study reveals a consistent percolation transition at $k^* = 130$, a critical threshold for the number of frequently visited destinations necessary to maintain a cohesive urban network. This percolation threshold proves remarkably consistent across diverse urban configurations, sizes, and geographical settings over a 48-month study period, and can largely be interpreted as the joint effect of the emergence of hubness and the level of mixing of residents. Furthermore, we examine the socioeconomic profiles of residents from different origin areas categorized by the fulfillment level of $k^*=130$ principal destinations, revealing a pronounced distinction in the origins' socioeconomic advantages. These insights offer a nuanced understanding of how urban spaces are interconnected and the determinants of travel behavior. Our findings contribute to a deeper comprehension of the structural dynamics that govern urban spatial interactions."}, "https://arxiv.org/abs/2406.14772": {"title": "Consistent community detection in multi-layer networks with heterogeneous differential privacy", "link": "https://arxiv.org/abs/2406.14772", "description": "arXiv:2406.14772v1 Announce Type: cross \nAbstract: As network data has become increasingly prevalent, a substantial amount of attention has been paid to the privacy issue in publishing network data. One of the critical challenges for data publishers is to preserve the topological structures of the original network while protecting sensitive information. In this paper, we propose a personalized edge flipping mechanism that allows data publishers to protect edge information based on each node's privacy preference. It can achieve differential privacy while preserving the community structure under the multi-layer degree-corrected stochastic block model after appropriately debiasing, and thus consistent community detection in the privatized multi-layer networks is achievable. Theoretically, we establish the consistency of community detection in the privatized multi-layer network and show that better privacy protection of edges can be obtained for a proportion of nodes while allowing other nodes to give up their privacy. Furthermore, the advantage of the proposed personalized edge-flipping mechanism is also supported by its numerical performance on various synthetic networks and a real-life multi-layer network."}, "https://arxiv.org/abs/2406.14894": {"title": "Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition", "link": "https://arxiv.org/abs/2406.14894", "description": "arXiv:2406.14894v1 Announce Type: cross \nAbstract: Verbs form the backbone of language, providing the structure and meaning to sentences. Yet, their intricate semantic nuances pose a longstanding challenge. Understanding verb relations through the concept of lexical entailment is crucial for comprehending sentence meanings and grasping verb dynamics. This work investigates the capabilities of eight Large Language Models in recognizing lexical entailment relations among verbs through differently devised prompting strategies and zero-/few-shot settings over verb pairs from two lexical databases, namely WordNet and HyperLex. Our findings unveil that the models can tackle the lexical entailment recognition task with moderately good performance, although at varying degree of effectiveness and under different conditions. Also, utilizing few-shot prompting can enhance the models' performance. However, perfectly solving the task arises as an unmet challenge for all examined LLMs, which raises an emergence for further research developments on this topic."}, "https://arxiv.org/abs/2406.14928": {"title": "Autonomous Agents for Collaborative Task under Information Asymmetry", "link": "https://arxiv.org/abs/2406.14928", "description": "arXiv:2406.14928v1 Announce Type: cross \nAbstract: Large Language Model Multi-Agent Systems (LLM-MAS) have achieved great progress in solving complex tasks. It performs communication among agents within the system to collaboratively solve tasks, under the premise of shared information. However, when agents' communication is leveraged to enhance human cooperation, a new challenge arises due to information asymmetry, since each agent can only access the information of its human user. Previous MAS struggle to complete tasks under this condition. To address this, we propose a new MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems. In iAgents, the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry. iAgents employs a novel agent reasoning mechanism, InfoNav, to navigate agents' communication towards effective information exchange. Together with InfoNav, iAgents organizes human information in a mixed memory to provide agents with accurate and comprehensive information for exchange. Additionally, we introduce InformativeBench, the first benchmark tailored for evaluating LLM agents' task-solving ability under information asymmetry. Experimental results show that iAgents can collaborate within a social network of 140 individuals and 588 relationships, autonomously communicate over 30 turns, and retrieve information from nearly 70,000 messages to complete tasks within 3 minutes."}, "https://arxiv.org/abs/2406.15038": {"title": "Online detection and infographic explanation of spam reviews with data drift adaptation", "link": "https://arxiv.org/abs/2406.15038", "description": "arXiv:2406.15038v1 Announce Type: cross \nAbstract: Spam reviews are a pervasive problem on online platforms due to its significant impact on reputation. However, research into spam detection in data streams is scarce. Another concern lies in their need for transparency. Consequently, this paper addresses those problems by proposing an online solution for identifying and explaining spam reviews, incorporating data drift adaptation. It integrates (i) incremental profiling, (ii) data drift detection & adaptation, and (iii) identification of spam reviews employing Machine Learning. The explainable mechanism displays a visual and textual prediction explanation in a dashboard. The best results obtained reached up to 87 % spam F-measure."}, "https://arxiv.org/abs/2406.15216": {"title": "A Highly Granular Temporary Migration Dataset Derived From Mobile Phone Data in Senegal", "link": "https://arxiv.org/abs/2406.15216", "description": "arXiv:2406.15216v1 Announce Type: cross \nAbstract: Understanding temporary migration is crucial for addressing various socio-economic and environmental challenges in developing countries. However, traditional surveys often fail to capture such movements effectively, leading to a scarcity of reliable data, particularly in sub-Saharan Africa. This article introduces a detailed and open-access dataset that leverages mobile phone data to capture temporary migration in Senegal with unprecedented spatio-temporal detail. The dataset provides measures of migration flows and stock across 151 locations across the country and for each half-month period from 2013 to 2015, with a specific focus on movements lasting between 20 and 180 days. The article presents a suite of methodological tools that not only include algorithmic methods for the detection of temporary migration events in digital traces, but also addresses key challenges in aggregating individual trajectories into coherent migration statistics. These methodological advancements are not only pivotal for the intrinsic value of the dataset but also adaptable for generating systematic migration statistics from other digital trace datasets in other contexts."}, "https://arxiv.org/abs/2406.15311": {"title": "The disruption index suffers from citation inflation and is confounded by shifts in scholarly citation practice", "link": "https://arxiv.org/abs/2406.15311", "description": "arXiv:2406.15311v1 Announce Type: cross \nAbstract: Measuring the rate of innovation in academia and industry is fundamental to monitoring the efficiency and competitiveness of the knowledge economy. To this end, a disruption index (CD) was recently developed and applied to publication and patent citation networks (Wu et al., Nature 2019; Park et al., Nature 2023). Here we show that CD systematically decreases over time due to secular growth in research and patent production, following two distinct mechanisms unrelated to innovation -- one behavioral and the other structural. Whereas the behavioral explanation reflects shifts associated with techno-social factors (e.g. self-citation practices), the structural explanation follows from `citation inflation' (CI), an inextricable feature of real citation networks attributable to increasing reference list lengths, which causes CD to systematically decrease. We demonstrate this causal link by way of mathematical deduction, computational simulation, multi-variate regression, and quasi-experimental comparison of the disruptiveness of PNAS versus PNAS Plus articles, which differ only in their lengths. Accordingly, we analyze CD data available in the SciSciNet database and find that disruptiveness incrementally increased from 2005-2015, and that the negative relationship between disruption and team-size is remarkably small in overall magnitude effect size, and shifts from negative to positive for team size $\\geq$ 8 coauthors."}, "https://arxiv.org/abs/2405.00053": {"title": "A new understanding on the history of developing MRI for cancer detection", "link": "https://arxiv.org/abs/2405.00053", "description": "arXiv:2405.00053v2 Announce Type: replace \nAbstract: Science is about facts and truth. Yet sometimes the truth and facts are not obvious. For example, in the field of MRI (Magnetic Resonance Imaging), there has been a long-lasting debate about who were the major contributors in its development. Particularly, there was a strong dispute between the followers of two scientists, R. Damadian and P. Lauterbur. In this review, we carefully trace the major developments in applying NMR for cancer detection starting almost 50 years ago. The research records show that the truth was beyond the claims of either research camps. The development of NMR for cancer detection involved multiple research groups, who made critical contributions at different junctures."}, "https://arxiv.org/abs/2405.03701": {"title": "QxEAI: Quantum-like evolutionary algorithm for automated probabilistic forecasting", "link": "https://arxiv.org/abs/2405.03701", "description": "arXiv:2405.03701v2 Announce Type: replace \nAbstract: Forecasting, to estimate future events, is crucial for business and decision-making. This paper proposes QxEAI, a methodology that produces a probabilistic forecast that utilizes a quantum-like evolutionary algorithm based on training a quantum-like logic decision tree and a classical value tree on a small number of related time series. We demonstrate how the application of our quantum-like evolutionary algorithm to forecasting can overcome the challenges faced by classical and other machine learning approaches. By using three real-world datasets (Dow Jones Index, retail sales, gas consumption), we show how our methodology produces accurate forecasts while requiring little to none manual work."}, "https://arxiv.org/abs/2307.13206": {"title": "Transferability of Graph Neural Networks using Graphon and Sampling Theories", "link": "https://arxiv.org/abs/2307.13206", "description": "arXiv:2307.13206v2 Announce Type: replace-cross \nAbstract: Graph neural networks (GNNs) have become powerful tools for processing graph-based information in various domains. A desirable property of GNNs is transferability, where a trained network can swap in information from a different graph without retraining and retain its accuracy. A recent method of capturing transferability of GNNs is through the use of graphons, which are symmetric, measurable functions representing the limit of large dense graphs. In this work, we contribute to the application of graphons to GNNs by presenting an explicit two-layer graphon neural network (WNN) architecture. We prove its ability to approximate bandlimited graphon signals within a specified error tolerance using a minimal number of network weights. We then leverage this result, to establish the transferability of an explicit two-layer GNN over all sufficiently large graphs in a convergent sequence. Our work addresses transferability between both deterministic weighted graphs and simple random graphs and overcomes issues related to the curse of dimensionality that arise in other GNN results. The proposed WNN and GNN architectures offer practical solutions for handling graph data of varying sizes while maintaining performance guarantees without extensive retraining."}, "https://arxiv.org/abs/2406.15428": {"title": "Assortativity in networks", "link": "https://arxiv.org/abs/2406.15428", "description": "arXiv:2406.15428v1 Announce Type: new \nAbstract: The degree-degree correlation is crucial in understanding the structural properties of and dynamics occurring upon network, and is often measured by the assortativity coefficient $r$. In this paper, we first study this measure in detail and conclude that $r$ belongs to an asymmetric range $[-1,1)$ rather than the widely-cited $[-1,1]$. Among which, we verify that star is the unique tree network that achieves the lower bound of index $r$. Next, we obtain that all the resultant networks based on several widely-used kinds of edge-based iterative operations are disassortative if seed model has negative $r$, and also generate a family of growing neutral networks. Then, we propose an edge-based iterative operation to construct growing assortative network when seed is assortative, and further extend it to work well in general setting. Lastly, we establish a sufficient condition for existence of neutral tree network, accordingly, not only find out a representative of any order neutral tree network for the first time, but also are the first to create growing neutral tree networks as well. Also, we obtain $8n/9$ neutral non-tree graphs of distinct order as $n\\rightarrow\\infty$."}, "https://arxiv.org/abs/2406.15439": {"title": "Heterogeneous peer effects of college roommates on academic performance", "link": "https://arxiv.org/abs/2406.15439", "description": "arXiv:2406.15439v1 Announce Type: new \nAbstract: Understanding how student peers influence learning outcomes is crucial for effective education management in complex social systems. The complexities of peer selection and evolving peer relationships, however, pose challenges for identifying peer effects using static observational data. Here we use both null-model and regression approaches to examine peer effects using longitudinal data from 5,272 undergraduates, where roommate assignments are plausibly random upon enrollment and roommate relationships persist until graduation. Specifically, we construct a roommate null model by randomly shuffling students among dorm rooms and introduce an assimilation metric to quantify similarities in roommate academic performance. We find significantly larger assimilation in actual data than in the roommate null model, suggesting roommate peer effects, whereby roommates have more similar performance than expected by chance alone. Moreover, assimilation exhibits an overall increasing trend over time, suggesting that peer effects become stronger the longer roommates live together. Our regression analysis further reveals the moderating role of peer heterogeneity. In particular, when roommates perform similarly, the positive relationship between a student's future performance and their roommates' average prior performance is more pronounced, and their ordinal rank in the dorm room has an independent effect. Our findings contribute to understanding the role of college roommates in influencing student academic performance."}, "https://arxiv.org/abs/2406.15449": {"title": "Rate of epidemic spreading on complex networks", "link": "https://arxiv.org/abs/2406.15449", "description": "arXiv:2406.15449v1 Announce Type: new \nAbstract: The initial phase of an epidemic is often characterised by an exponential increase in the number of infected individuals. In well-mixed populations, this exponential increase is controlled by the basic reproduction number R0 and the distribution of times between consecutive infection along an infection chain. However, we are still lacking a general understanding of how epidemics spread when individual interactions form a complex network. Here, we derive an expression for the rate of exponential spread of an epidemic on a complex network. We find that this rate is affected by the degree distribution, the network assortativity, and the level of clustering. Our result holds for a broad range of networks, aside from networks with very broad degree distribution, where no clear exponential regime is present. The theory presented in this paper bridges the gap between classic epidemiology and the theory of complex network, with broad implications for model inference and policy making."}, "https://arxiv.org/abs/2406.15452": {"title": "A-TEAM: Advanced Traffic Event Analysis and Management Platform for Transportation Data-Driven Problem Solving", "link": "https://arxiv.org/abs/2406.15452", "description": "arXiv:2406.15452v1 Announce Type: new \nAbstract: The rapid growth in terms of the availability of transportation data provides great potential for the introduction of emerging data-driven methodologies into transportation-related research and development efforts. However, advanced data-driven models, such as artificial-intelligence based approaches, usually contain complicated modeling structures and require strict data formats along with a very complex execution environment. It is thus often challenging to deploy and implement such data-driven models in a real-world environment. Moreover, a full-fledged application requires not only well developed and calibrated models, but also efficient connections with back end infrastructure such as large databases and front end utilities, such as a user-interface. This paper introduces a novel platform which provides an integrated architecture for deploying multi-purpose real-time traffic management applications. Inspired by the concept of modular design in software system development, this paper proposes a modular platform allowing users to customize their mission specific needs and preferences. The developed platform is capable of incorporating flexible user-provided models and/or data with the ultimate goal of deploying them as a complete application ready for real-world use. To illustrate this novel modular software system concept, this paper presents a work zone management and coordination application that is built upon the developed implementation platform to provide useful decision support to traffic engineers."}, "https://arxiv.org/abs/2406.15453": {"title": "Revisiting Monetarism: influence of Entropic Models", "link": "https://arxiv.org/abs/2406.15453", "description": "arXiv:2406.15453v1 Announce Type: new \nAbstract: This paper introduces an approach to gas-like models, from the concept of entropy, using the money stock data of two economic agents, in this case of two countries, which carry out market actions (trading) in two theoretical scenarios: in the absence of debt and with debt. The exercise deals exclusively with a no debt scenario and the data and results show that the bounded model generates low $P_{(m)}$, values that the results of the regressions between the two countries show an advantageous position of the stock country $m_i$ over the stock country $m_j$. About the rationale, it is found that these models can provide meaningful information regarding the behavior of monetary variables, -- taking into account the different conceptual positions proposed in the manuscript -- using analogies derived from other fields of study ranging from molecules in rarefied gases or particles collisions, bringing the data to the interaction of economic actors."}, "https://arxiv.org/abs/2406.15455": {"title": "Determination of the mean center of a region: A physics-based approach", "link": "https://arxiv.org/abs/2406.15455", "description": "arXiv:2406.15455v1 Announce Type: new \nAbstract: The mean center of a geographical region, including continents and countries, has been mostly determined to study the trend of population migration, the shift of economic hubs, and the spatial change of extreme climate events. However, the determination of the mean center is a formidable task as it deals with the curvature of the earth's surface. Here, we report a physics-based model to determine the mean center of a region. Our method provides the analytical expression for the location of the mean center for both flat and curved spaces, such as straight lines, circles, planes, three-dimensional space, cylinders, and spheres. Some of these expressions are often used to compute the center of mass of the physical system. Therefore, the implication of our model in the physical system extends the general validity of the model. Furthermore, we have computed various mean centers of India, such as the geographical, population, and crime centers. We have also assessed the year-wise movement of the crime center and found a spatial trend towards the north."}, "https://arxiv.org/abs/2406.15463": {"title": "Emergent Complexity in the Decision-Making Process of Chess Players", "link": "https://arxiv.org/abs/2406.15463", "description": "arXiv:2406.15463v1 Announce Type: new \nAbstract: In this article, we study the decision-making process of chess players by using a chess engine to evaluate the moves across different pools of games. We quantify the decisiveness of each move during the games using a metric called $\\Delta$, which is derived from the engine's evaluation of the positions. We then performed a comparative analysis across players of varying competitive levels. Firstly, we observed that players face a wide spectrum of $\\Delta$ values, evidencing the complexity of the process. By examining groups of winning and losing players, we found evidence where a decrease in complexity may be associated with a drop in player performance levels. Secondly, we observed that players' accuracy increases in positions with high $\\Delta$ values regardless of competitive level. Complementing this information with a null model where players make completely random legal moves allowed us to characterize the decision-making process under the simple strategy of making moves that minimize $\\Delta$ values. Finally, based on this idea, we proposed a simple model that approximately replicates the global emergent properties of the system."}, "https://arxiv.org/abs/2406.15464": {"title": "How Does Culture Evolve?", "link": "https://arxiv.org/abs/2406.15464", "description": "arXiv:2406.15464v1 Announce Type: new \nAbstract: This chapter synthesizes evidence from cognitive science, evolutionary theory, anthropology, psychological studies, and computational models for a complex systems inspired theory of creativity, and its role in cultural evolution. Creativity is guided by the global shape of one's integrated network of memories, concepts, and beliefs: one's worldview. This integrated structure and its dynamical change over time are described using autocatalytic networks. Autocatalytic networks can interact with each other, and they can grow and evolve; through interactions between their components, they generate novel components. Thus, they are used to describe cultural change both within and between individuals, as well as across cultural lineages. The chapter outlines autocatalytic network models of the origin of culture, the cognitive developmental process by which each child becomes a participant in cultural evolution, and the role of imitation, leadership, and social media on cultural evolution, as well as the trade-off between creativity and continuity."}, "https://arxiv.org/abs/2406.15492": {"title": "On the Principles behind Opinion Dynamics in Multi-Agent Systems of Large Language Models", "link": "https://arxiv.org/abs/2406.15492", "description": "arXiv:2406.15492v1 Announce Type: new \nAbstract: We study the evolution of opinions inside a population of interacting large language models (LLMs). Every LLM needs to decide how much funding to allocate to an item with three initial possibilities: full, partial, or no funding. We identify biases that drive the exchange of opinions based on the LLM's tendency to (i) find consensus with the other LLM's opinion, (ii) display caution when specifying funding, and (iii) consider ethical concerns in its opinion. We find these biases are affected by the perceived absence of compelling reasons for opinion change, the perceived willingness to engage in discussion, and the distribution of allocation values. Moreover, tensions among biases can lead to the survival of funding for items with negative connotations. We also find that the final distribution of full, partial, and no funding opinions is more diverse when an LLM freely forms its opinion after an interaction than when its opinion is a multiple-choice selection among the three allocation options. In the latter case, consensus or polarization is generally attained. When agents are aware of past opinions, they seek to maintain consistency with them, and more diverse updating rules emerge. Our study is performed using a Llama 3 LLM."}, "https://arxiv.org/abs/2406.15493": {"title": "Exploring Study Abroad with Traditionally Underrepresented Populations: Impacts of Institutional Types", "link": "https://arxiv.org/abs/2406.15493", "description": "arXiv:2406.15493v1 Announce Type: new \nAbstract: The study investigated roles of institutional types and ethnic/racial background on academic credit among the traditionally underrepresented population of the U.S. study abroad program. Using archival data, the study sampled the students' enrollment and academic credit information spanning a period of 20 years (2003 - 2022). Data analysis Using One-Way Analysis of Variance (ANOVA) indicates significant main influence of institutional type (p<.001) and significant main influence of ethnic/racial identity (p<.001) on students' academic credit. The result was discussed in terms of its relevance in educational policy re-evaluations, improvement of the study conditions of the underrepresented students, and enhancement of the enrollment opportunities of these minority population across all U.S. institutions of learning"}, "https://arxiv.org/abs/2406.15511": {"title": "On the number of freeway lanes and its positive or negative effect on safety", "link": "https://arxiv.org/abs/2406.15511", "description": "arXiv:2406.15511v1 Announce Type: new \nAbstract: We address the 80-year-old question of whether a freeway with more lanes results in fewer or more accidents. For finding the optimally safe number of lanes, in particular, we look at three types of accidents that are prevalent on urban freeways, namely \"following too closely\", \"driver inattention\", and \"unsafe change of lanes\". To do so we extend the intelligent driver model (IDM) to create a microscopic traffic flow model which is capable of producing accidents. We study the rate of accidents relative to a baseline 2-lane unidirectional freeway via Monte Carlo simulation. For each simulation instance we create a starting configuration involving only a few cars over a short segment of the freeway and simulate the dynamics thereafter. Furthermore, we look at the number of shoulders present, and show that the presence of shoulders can positively or negatively affect the accident rate depending on the type of accident."}, "https://arxiv.org/abs/2406.15514": {"title": "How big does a population need to be before demographers can ignore individual-level randomness in demographic events?", "link": "https://arxiv.org/abs/2406.15514", "description": "arXiv:2406.15514v1 Announce Type: new \nAbstract: When studying a national-level population, demographers can safely ignore the effect of individual-level randomness on age-sex structure. When studying a single community, or group of communities, however, the potential importance of individual-level randomness is less clear. We seek to measure the effect of individual-level randomness in births and deaths on standard summary indicators of age-sex structure, for populations of different sizes, focusing on on demographic conditions typical of historical populations. We conduct a microsimulation experiment where we simulate events and age-sex structure under a range of settings for demographic rates and population size. The experiment results suggest that individual-level randomness strongly affects age-sex structure for populations of about 100, but has a much smaller effect on populations of 1,000, and a negligible effect on populations of 10,000. Our conclusion is that analyses of age-sex structure in historical populations with sizes on the order 100 must account for individual-level randomness in demographic events. Analyses of populations with sizes on the order of 1,000 may need to make some allowance for individual-level variation, but other issues, such as measurement error, probably deserve more attention. Analyses of populations of 10,000 can safely ignore individual-level variation."}, "https://arxiv.org/abs/2406.15517": {"title": "Multidimensional representation of semantic relations between physical theories, fundamental constants and units of measurement with formal concept analysis", "link": "https://arxiv.org/abs/2406.15517", "description": "arXiv:2406.15517v1 Announce Type: new \nAbstract: We propose several hierarchical graphs that represent the semantic relations between physical theories, their fundamental constants and units of measurement. We begin with an alternative representation of Zelmanov's cube of fundamental constants as a concept lattice. We then propose the inclusion of a new fundamental constant: Milgrom's critical acceleration and discuss the implications of such analysis. We then look for the same fundamental constants in a graph that relates magnitudes and units of measurement in the International System of Units. This exercise shows the potential of visualizing hierarchical networks as a tool to better comprehend the interrelations and dependencies of physical magnitudes, units and theories. New regimes of application may be deduced, as well as an interesting reflection on our ontologies and corresponding theoretical objects."}, "https://arxiv.org/abs/2406.15533": {"title": "Food Pairing Unveiled: Exploring Recipe Creation Dynamics through Recommender Systems", "link": "https://arxiv.org/abs/2406.15533", "description": "arXiv:2406.15533v1 Announce Type: new \nAbstract: In the early 2000s, renowned chef Heston Blumenthal formulated his \"food pairing\" hypothesis, positing that if foods share many flavor compounds, then they tend to taste good when eaten together. In 2011, Ahn et al. conducted a study using a dataset of recipes, ingredients, and flavor compounds, finding that, in Western cuisine, ingredients in recipes often share more flavor compounds than expected by chance, indicating a natural tendency towards food pairing. Building upon Ahn's research, our work applies state-of-the-art collaborative filtering techniques to the dataset, providing a tool that can recommend new foods to add in recipes, retrieve missing ingredients and advise against certain combinations. We create our recommender in two ways, by taking into account ingredients appearances in recipes or shared flavor compounds between foods. While our analysis confirms the existence of food pairing, the recipe-based recommender performs significantly better than the flavor-based one, leading to the conclusion that food pairing is just one of the principles to take into account when creating recipes. Furthermore, and more interestingly, we find that food pairing in data is mostly due to trivial couplings of very similar ingredients, leading to a reconsideration of its current role in recipes, from being an already existing feature to a key to open up new scenarios in gastronomy. Our flavor-based recommender can thus leverage this novel concept and provide a new tool to lead culinary innovation."}, "https://arxiv.org/abs/2406.15595": {"title": "The impact of fear and behaviour response to established and novel diseases", "link": "https://arxiv.org/abs/2406.15595", "description": "arXiv:2406.15595v1 Announce Type: new \nAbstract: We analyze a disease transmission model that allows individuals to acquire fear and change their behaviour to reduce transmission. Fear is acquired through contact with infected individuals and through the influence of fearful individuals. We analyze the model in two limits: First, an Established Disease Limit (EDL), where the spread of the disease is much faster than the spread of fear, and second, a Novel Disease Limit (NDL), where the spread of the disease is comparable to that of fear. For the EDL, we show that the relative rate of fear acquisition to disease transmission controls the size of the fearful population at the end of a disease outbreak, and that the fear-induced contact reduction behaviour has very little impact on disease burden. Conversely, we show that in the NDL, disease burden can be controlled by fear-induced behaviour depending on the rate of fear loss. Specifically, fear-induced behaviour introduces a contact parameter $p$, which if too large prevents the contact reduction from effectively managing the epidemic. We analytically identify a critical prophylactic behaviour parameter $p=p_c$ where this happens leading to a discontinuity in epidemic prevalence. We show that this change in disease burden introduces multiple epidemic waves."}, "https://arxiv.org/abs/2406.15636": {"title": "Simple Games on Complex Networks", "link": "https://arxiv.org/abs/2406.15636", "description": "arXiv:2406.15636v1 Announce Type: new \nAbstract: The relationship between topology and dynamics of complex systems has motivated continuing interest from the scientific community. In the present work, we address this interesting topic from the perspective of simple games, involving two teams playing according to a small set of simple rules, taking place on four types of complex networks. Starting from a minimalist game, characterized by full symmetry always leading to ties, four other games are described in progressive order of complexity, taking into account the presence of neighbors as well as strategies. Each of these five games, as well as their specific changes when implemented in four types of networks, are studied in terms of statistics of the total duration of the game as well as the number of victories and ties, with several interesting results that substantiate, in some cases, the importance of the network topology on the respective dynamics. As a subsidiary result, the visualization of relationships between the data elements in terms of coincidence similarity networks allowed a more complete and direct interpretation of the obtained results."}, "https://arxiv.org/abs/2406.15679": {"title": "Iterative Service-Learning: A Computing-Based Case-study Applied to Small Rural Organizations", "link": "https://arxiv.org/abs/2406.15679", "description": "arXiv:2406.15679v1 Announce Type: new \nAbstract: This paper describes the iterative use of service learning to develop, review, and improve computing-based artifacts. It is well-known that computing students benefit from service-learning experiences as do the community partners. It is also well-known that computing artifacts rarely function well long-term without versioning and updates. Service-learning projects are often one-time engagements, completed by single teams of students over the course of a semester course. This limits the benefit for community partners that do not have the expertise or resources to review and update a project on their own.\n  Over several years, teams of undergraduate students in a capstone course created tailored social media plans for numerous small rural organizations. The projects were required to meet client specific needs, with identified audiences, measurable goals, and strategies and tactics to reach the identified goals. This paper builds on previously results for 60 projects conducted over several years. Nine clients were selected to participate in the iterative follow-up process, where new student teams conducted client interviews, reviewed the initial plans, and analyzed metrics from the current strategies and tactics to provide updated, improved artifacts. Using ABET learning objectives as a basis, clients reviewed the student teams and artifacts. This longitudinal study discusses the impact of this intervention to increase implementation and sustained use rates of computing artifacts developed through service learning. Both students and clients reported high satisfaction levels, and clients were particularly satisfied with the iterative improvement process. This research demonstrates an innovative practice for creating and maintaining computing artifacts through iterative service learning, while addressing the resource constraints of small organizations."}, "https://arxiv.org/abs/2406.15780": {"title": "Triple Helix synergy and patent dynamics", "link": "https://arxiv.org/abs/2406.15780", "description": "arXiv:2406.15780v1 Announce Type: new \nAbstract: We use a computationally efficient technique of Logistic Continuous Wavelet transform (CWT) to analyze patent data for Switzerland, Germany, USA, and Brszil for the period 1980-2000. We found that patent growth dynamics follows the dynamics of innovation system synergy in the framework of Triple Helix model of innovations where observed non-linear actors' interactions are provided by biased information exchange between heterogenious actors. Suggested approach reveals the latent trend structure in patent and innovation dynamics and may help policymakers identify the potential drivers of patent and innovation activity and form informed policy for boosting innovation development. The paper also privides a foundation for future research in differnt fields studying complex systems of interacting heterogenious agents."}, "https://arxiv.org/abs/2406.15814": {"title": "Simulation-Optimization Approaches for the Network Immunization Problem with Quarantining", "link": "https://arxiv.org/abs/2406.15814", "description": "arXiv:2406.15814v1 Announce Type: new \nAbstract: Vaccination has played an important role in preventing the spread of infectious diseases. However, the limited availability of vaccines and personnel at the roll-out of a new vaccine, as well as the costs of vaccination campaigns, might limit how many people can be vaccinated. Network immunization thus focuses on selecting a fixed-size subset of individuals to vaccinate so as to minimize the disease spread. In this paper, we consider simulation-optimization approaches for this selection problem. Here, the simulation of disease spread in an activity-based contact graph allows us to consider the effect of contact tracing and a limited willingness to test and quarantine. First, we develop a stochastic programming algorithm based on sampling infection forests from the simulation. Second, we propose a genetic algorithm that is tailored to the immunization problem and combines simulation runs of different sizes to balance the time needed to find promising solutions with the uncertainty resulting from simulation. Both approaches are tested on data from a major university in Denmark and disease characteristics representing those of COVID-19. Our results show that the proposed methods are competitive with a large number of centrality-based measures over a range of disease parameters and that the proposed methods are able to outperform them for a considerable number of these instances. Finally, we compare network immunization against our previously proposed approach of limiting distinct contacts. Although, independently, network immunization has a larger impact in reducing disease spread, we show that the combination of both methods reduces the disease spread even further."}, "https://arxiv.org/abs/2406.15894": {"title": "Persuasion, Betrayal and Regret in Election Campaigns", "link": "https://arxiv.org/abs/2406.15894", "description": "arXiv:2406.15894v1 Announce Type: new \nAbstract: Elections play a fundamental role in democratic societies, however they are often characterized by unexpected results. Here we discuss an election campaign model inspired by the compartmental epidemiology, and we show that the model captures the main characteristics of an election campaign: persuasion, betrayal and regret. All of these three factors can be used together or independently to influence the campaign, and to determine the winner. We include results for both the deterministic and the stochastic versions of the model, and we show that the decision to not vote significantly increases the fluctuations in the model, amplifying the chance of controversial results, in agreement with the well known \"paradox of not voting\"."}, "https://arxiv.org/abs/2406.16051": {"title": "Entropy-driven decision-making dynamics sheds light on the emergence of the \"paradox of choice\"", "link": "https://arxiv.org/abs/2406.16051", "description": "arXiv:2406.16051v1 Announce Type: new \nAbstract: Decision making is the cognitive process of selecting a course of action among multiple alternatives. As the decision maker belongs to a complex microenvironment (which contains multiple decision makers), has to make a decision where multiple options are present which often leads to a phenomenon known as the \"paradox of choices\". The latter refers to the case where too many options can lead to negative outcomes, such as increased uncertainty, decision paralysis, and frustration. Here, we employ an entropy driven mechanism within a statistical physics framework to explain the premises of the paradox. In turn, we focus on the emergence of a collective \"paradox of choice\", in the case of interacting decision-making agents, quantified as the decision synchronization time. Our findings reveal a trade-off between synchronization time and the sensing radius, indicating the optimal conditions for information transfer among group members, which significantly depends the individual sensitivity parameters. Interestingly, when agents sense their microenvironment in a biased way or their decisions are influenced by their past choices, then the collective \"paradox of choice\" does not occur. In a nutshell, our theory offers a low-dimensional and unified statistical explanation of the \"paradox of choice\" at the individual and at the collective level."}, "https://arxiv.org/abs/2406.16092": {"title": "Quantitative Global Carbon Inequality Network", "link": "https://arxiv.org/abs/2406.16092", "description": "arXiv:2406.16092v1 Announce Type: new \nAbstract: International trading networks significantly influence global economic conditions and environmental outcomes. A notable imbalance between economic gains and emissions transfers persists, manifesting as carbon inequality. This study introduces a novel metric, the Ecological Economic Equality Index, integrated with complex network dynamics analysis, to quantitatively evaluate the evolving roles within the global trading network and to pinpoint inequities in trade relationships from 1995 to 2022. Utilising high spatiotemporal resolution data from the Environmentally Extended Multi-regional Input-output model, our findings reveal a widening disparity in carbon inequality and dynamic patterns. This analysis emphasises the gap in regional carbon inequality and identifies unequal trade. The study underscores that carbon inequality is a critical challenge affecting both developing and developed regions, demanding widespread attention and action."}, "https://arxiv.org/abs/2406.16175": {"title": "The Persistence of Contrarianism on Twitter: Mapping users' sharing habits for the Ukraine war, COVID-19 vaccination, and the 2020 Midterm Elections", "link": "https://arxiv.org/abs/2406.16175", "description": "arXiv:2406.16175v1 Announce Type: new \nAbstract: Empirical studies of online disinformation emphasize matters of public concern such as the COVID-19 pandemic, foreign election interference, and the Russo-Ukraine war, largely in studies that treat the topics separately. Comparatively fewer studies attempt to relate such disparate topics and address the extent to which they share behaviors. In this study, we compare three samples of Twitter data on COVID-19 vaccination, the Ukraine war and the 2020 midterm elections, to ascertain how distinct ideological stances of users across the three samples might be related. Our results indicate the emergence of a broad contrarian stance that is defined by its opposition to public health narratives/policies along with the Biden administration's foreign policy stances. Sharing activity within the contrarian position falls on a spectrum with outright conspiratorial content on one end. We confirm the existence of ideologically coherent cross-subject stances among Twitter users, but in a manner not squarely aligned with right-left political orientations."}, "https://arxiv.org/abs/2406.16186": {"title": "Philosophical views of justice and their implications in energy systems modelling", "link": "https://arxiv.org/abs/2406.16186", "description": "arXiv:2406.16186v1 Announce Type: new \nAbstract: What constitutes socially just or unjust energy systems or transitions can be derived from the philosophy and theories of justice. Assessments of justice and utilising them in modelling lead to great differences based on which justice principles are applied. We find that comparisons between the two principles of utilitarianism and egalitarianism dominate in assessments of distributive justice, with the latter most often considered representing a \"just energy system\". The lack of recognition of alternative and equally valid principles of justice, resting on e.g. capabilities, responsibilities and/or opportunities, leads to a narrow understanding of justice that fails to align with the views of different individuals, stakeholders and societies. More importantly, it can lead to the unjust design of future energy systems and energy systems analysis. In this work, we contribute to the growing amount of research on justice in energy systems modelling by assessing the implications of different philosophical views on justice on modelling results. Through a modelling exercise with a power system model for Europe, we explore different designs of a future net-zero European energy system, and its distributional implications based on the application of different justice principles. In addition to the utilitarian and egalitarian approach, we include, among others, principles of \"polluters pay\" and \"ability-to-pay\", which take historical contributions of GHG and the socio-economic conditions of a region into account. We find that socially just energy systems look significantly different depending on the justice principles applied. The results may stimulate a greater discussion among researchers and policymakers on the implications of different constructions of justice in modelling, expansion of approaches, and demonstrate the importance of transparency and assumptions when communicating such results"}, "https://arxiv.org/abs/2406.16560": {"title": "GNNTAL:A Novel Model for Identifying Critical Nodes in Complex Networks", "link": "https://arxiv.org/abs/2406.16560", "description": "arXiv:2406.16560v1 Announce Type: new \nAbstract: Identification of critical nodes is a prominent topic in the study of complex networks. Numerous methods have been proposed, yet most exhibit inherent limitations. Traditional approaches primarily analyze specific structural features of the network; however, node influence is typically the result of a combination of multiple factors. Machine learning-based methods struggle to effectively represent the complex characteristics of network structures through suitable embedding techniques and require substantial data for training, rendering them prohibitively costly for large-scale networks. To address these challenges, this paper presents an active learning model based on GraphSAGE and Transformer, named GNNTAL. This model is initially pre-trained on random or synthetic networks and subsequently fine-tuned on real-world networks by selecting a few representative nodes using K-Means clustering and uncertainty sampling. This approach offers two main advantages: (1) it significantly reduces training costs; (2) it simultaneously incorporates both local and global features. A series of comparative experiments conducted on twelve real-world networks demonstrate that GNNTAL achieves superior performance. Additionally, this paper proposes an influence maximization method based on the predictions of the GNNTAL model, which achieves optimal performance without the need for complex computations. Finally, the paper analyses certain limitations of the GNNTAL model and suggests potential solutions."}, "https://arxiv.org/abs/2406.16676": {"title": "Unveiling Cognitive Constraints in Language Production: Extracting and Validating the Active Ego Network of Words", "link": "https://arxiv.org/abs/2406.16676", "description": "arXiv:2406.16676v1 Announce Type: new \nAbstract: The \"ego network of words\" model captures structural properties in language production associated with cognitive constraints. While previous research focused on the layer-based structure and its semantic properties, this paper argues that an essential element, the concept of an active network, is missing. The active part of the ego network of words only includes words that are regularly used by individuals, akin to the ego networks in the social domain, where the active part includes relationships regularly nurtured by individuals and hence demanding cognitive effort. In this work, we define a methodology for extracting the active part of the ego network of words and validate it using interview transcripts and tweets. The robustness of our method to varying input data sizes and temporal stability is demonstrated. We also demonstrate that without the active network concept (and a tool for properly extracting the active network from data), the \"ego network of words\" model is not able to properly estimate the cognitive effort involved and it becomes vulnerable to the amount of data considered (leading to the disappearance of the layered structure in large datasets). Our results are well-aligned with prior analyses of the ego network of words, where the limitation of the data collected led automatically (and implicitly) to approximately consider the active part of the network only. Moreover, the validation on the transcripts dataset (MediaSum) highlights the generalizability of the model across diverse domains and the ingrained cognitive constraints in language usage."}, "https://arxiv.org/abs/2406.16762": {"title": "Adaptive Payoff-driven Interaction in Networked Snowdrift Games", "link": "https://arxiv.org/abs/2406.16762", "description": "arXiv:2406.16762v1 Announce Type: new \nAbstract: In social dilemmas, most interactions are transient and susceptible to restructuring, leading to continuous changes in social networks over time. Typically, agents assess the rewards of their current interactions and adjust their connections to optimize outcomes. In this paper, we introduce an adaptive network model in the snowdrift game to examine dynamic levels of cooperation and network topology, involving the potential for both the termination of existing connections and the establishment of new ones. In particular, we define the agent's asymmetric disassociation tendency toward their neighbors, which fundamentally determines the probability of edge dismantlement. The mechanism allows agents to selectively sever and rewire their connections to alternative individuals to refine partnerships. Our findings reveal that adaptive networks are particularly effective in promoting a robust evolution toward states of either pure cooperation or complete defection, especially under conditions of extreme cost-benefit ratios, as compared to static network models. Moreover, the dynamic restructuring of connections and the distribution of network degrees among agents are closely linked to the levels of cooperation in stationary states. Specifically, cooperators tend to seek broader neighborhoods when confronted with the invasion of multiple defectors."}, "https://arxiv.org/abs/2406.16787": {"title": "Parrondo's paradox in susceptible-infectious-susceptible dynamics over periodic temporal networks", "link": "https://arxiv.org/abs/2406.16787", "description": "arXiv:2406.16787v1 Announce Type: new \nAbstract: Many social and biological networks periodically change over time with daily, weekly, and other cycles. Thus motivated, we formulate and analyze susceptible-infectious-susceptible (SIS) epidemic models over temporal networks with periodic schedules. More specifically, we assume that the temporal network consists of a cycle of alternately used static networks, each with a given duration. We show Parrondo's paradox behavior in this model, with which the periodic alternation of two static networks that are supra-threshold (i.e., above the epidemic threshold of the SIS model) yields a sub-threshold dynamics (i.e., with the number of infectious nodes exponentially decaying over time) in many cases. We find community structure to play an important role in shaping this phenomenon, and we study its dependence on the connectivity between and number of communities. Our findings associate such paradoxical behavior with anti-phase oscillatory behavior of the number of infectious nodes in different communities of the network."}, "https://arxiv.org/abs/2406.15470": {"title": "Mental Disorder Classification via Temporal Representation of Text", "link": "https://arxiv.org/abs/2406.15470", "description": "arXiv:2406.15470v1 Announce Type: cross \nAbstract: Mental disorders pose a global challenge, aggravated by the shortage of qualified mental health professionals. Mental disorder prediction from social media posts by current LLMs is challenging due to the complexities of sequential text data and the limited context length of language models. Current language model-based approaches split a single data instance into multiple chunks to compensate for limited context size. The predictive model is then applied to each chunk individually, and the most voted output is selected as the final prediction. This results in the loss of inter-post dependencies and important time variant information, leading to poor performance. We propose a novel framework which first compresses the large sequence of chronologically ordered social media posts into a series of numbers. We then use this time variant representation for mental disorder classification. We demonstrate the generalization capabilities of our framework by outperforming the current SOTA in three different mental conditions: depression, self-harm, and anorexia, with an absolute improvement of 5% in the F1 score. We investigate the situation where current data instances fall within the context length of language models and present empirical results highlighting the importance of temporal properties of textual data. Furthermore, we utilize the proposed framework for a cross-domain study, exploring commonalities across disorders and the possibility of inter-domain data usage."}, "https://arxiv.org/abs/2406.15498": {"title": "An Integration of policy and reputation based trust mechanisms", "link": "https://arxiv.org/abs/2406.15498", "description": "arXiv:2406.15498v1 Announce Type: cross \nAbstract: Due to popularization of internet and e-commerce, more and more people getting involved in online shopping market. A large number of companies have been transferred to the internet where online customers have been increased due to easy access. The online business facilitates people to communicate without knowing each other. The e-commerce systems are the combination of commerce behavior and internet technologies. Therefore, trust aspects are positive elements in buyer-seller transactions and a potential source of competitive e-commerce industry. There are two different approaches to handle the trust. The first approach has a solid authentication set of rules where decisions are made on some digital or logical rules called policy based trust mechanism. The second approach is a decentralized trust approach where reputation assembled and shared in distributed environment called reputation based trust mechanism. Objectives: In this thesis, the strengths and weaknesses of policy and reputation based trust mechanisms have been identified through systematic literature review and industrial interviews. Furthermore, the process of integrated trust mechanism has been proposed. The integrated trust mechanism is proposed through mapping process, weakness of one mechanism with the strength of other. The proposed integrated trust mechanism was validated by conducting experiment with buyer/seller scenario in auction system. The analysis of collected results indicated that proposed integrated trust mechanism improved the trust of buyer against eBay and Tradera. At the end, we have discussed some key points that may affect trust relationship between seller and buyer. Furthermore, there is a need for further validation of proposed trust mechanism in auction system/e-commerce industry."}, "https://arxiv.org/abs/2406.15957": {"title": "Weak recovery, hypothesis testing, and mutual information in stochastic block models and planted factor graphs", "link": "https://arxiv.org/abs/2406.15957", "description": "arXiv:2406.15957v1 Announce Type: cross \nAbstract: The stochastic block model is a canonical model of communities in random graphs. It was introduced in the social sciences and statistics as a model of communities, and in theoretical computer science as an average case model for graph partitioning problems under the name of the ``planted partition model.'' Given a sparse stochastic block model, the two standard inference tasks are: (i) Weak recovery: can we estimate the communities with non trivial overlap with the true communities? (ii) Detection/Hypothesis testing: can we distinguish if the sample was drawn from the block model or from a random graph with no community structure with probability tending to $1$ as the graph size tends to infinity?\n  In this work, we show that for sparse stochastic block models, the two inference tasks are equivalent except at a critical point. That is, weak recovery is information theoretically possible if and only if detection is possible. We thus find a strong connection between these two notions of inference for the model. We further prove that when detection is impossible, an explicit hypothesis test based on low degree polynomials in the adjacency matrix of the observed graph achieves the optimal statistical power. This low degree test is efficient as opposed to the likelihood ratio test, which is not known to be efficient. Moreover, we prove that the asymptotic mutual information between the observed network and the community structure exhibits a phase transition at the weak recovery threshold.\n  Our results are proven in much broader settings including the hypergraph stochastic block models and general planted factor graphs. In these settings we prove that the impossibility of weak recovery implies contiguity and provide a condition which guarantees the equivalence of weak recovery and detection."}, "https://arxiv.org/abs/2406.16357": {"title": "Towards Lightweight Graph Neural Network Search with Curriculum Graph Sparsification", "link": "https://arxiv.org/abs/2406.16357", "description": "arXiv:2406.16357v1 Announce Type: cross \nAbstract: Graph Neural Architecture Search (GNAS) has achieved superior performance on various graph-structured tasks. However, existing GNAS studies overlook the applications of GNAS in resource-constraint scenarios. This paper proposes to design a joint graph data and architecture mechanism, which identifies important sub-architectures via the valuable graph data. To search for optimal lightweight Graph Neural Networks (GNNs), we propose a Lightweight Graph Neural Architecture Search with Graph SparsIfication and Network Pruning (GASSIP) method. In particular, GASSIP comprises an operation-pruned architecture search module to enable efficient lightweight GNN search. Meanwhile, we design a novel curriculum graph data sparsification module with an architecture-aware edge-removing difficulty measurement to help select optimal sub-architectures. With the aid of two differentiable masks, we iteratively optimize these two modules to efficiently search for the optimal lightweight architecture. Extensive experiments on five benchmarks demonstrate the effectiveness of GASSIP. Particularly, our method achieves on-par or even higher node classification performance with half or fewer model parameters of searched GNNs and a sparser graph."}, "https://arxiv.org/abs/2406.16552": {"title": "Inference of Sequential Patterns for Neural Message Passing in Temporal Graphs", "link": "https://arxiv.org/abs/2406.16552", "description": "arXiv:2406.16552v1 Announce Type: cross \nAbstract: The modelling of temporal patterns in dynamic graphs is an important current research issue in the development of time-aware GNNs. Whether or not a specific sequence of events in a temporal graph constitutes a temporal pattern not only depends on the frequency of its occurrence. We consider whether it deviates from what is expected in a temporal graph where timestamps are randomly shuffled. While accounting for such a random baseline is important to model temporal patterns, it has mostly been ignored by current temporal graph neural networks. To address this issue we propose HYPA-DBGNN, a novel two-step approach that combines (i) the inference of anomalous sequential patterns in time series data on graphs based on a statistically principled null model, with (ii) a neural message passing approach that utilizes a higher-order De Bruijn graph whose edges capture overrepresented sequential patterns. Our method leverages hypergeometric graph ensembles to identify anomalous edges within both first- and higher-order De Bruijn graphs, which encode the temporal ordering of events. The model introduces an inductive bias that enhances model interpretability. We evaluate our approach for static node classification using benchmark datasets and a synthetic dataset that showcases its ability to incorporate the observed inductive bias regarding over- and under-represented temporal edges. We demonstrate the framework's effectiveness in detecting similar patterns within empirical datasets, resulting in superior performance compared to baseline methods in node classification tasks. To the best of our knowledge, our work is the first to introduce statistically informed GNNs that leverage temporal and causal sequence anomalies. HYPA-DBGNN represents a path for bridging the gap between statistical graph inference and neural graph representation learning, with potential applications to static GNNs."}, "https://arxiv.org/abs/2406.16816": {"title": "On the Impact of Sample Size in Reconstructing Noisy Graph Signals: A Theoretical Characterisation", "link": "https://arxiv.org/abs/2406.16816", "description": "arXiv:2406.16816v1 Announce Type: cross \nAbstract: Reconstructing a signal on a graph from noisy observations of a subset of the vertices is a fundamental problem in the field of graph signal processing. This paper investigates how sample size affects reconstruction error in the presence of noise via an in-depth theoretical analysis of the two most common reconstruction methods in the literature, least-squares reconstruction (LS) and graph-Laplacian regularised reconstruction (GLR). Our theorems show that at sufficiently low signal-to-noise ratios (SNRs), under these reconstruction methods we may simultaneously decrease sample size and decrease average reconstruction error. We further show that at sufficiently low SNRs, for LS reconstruction we have a $\\Lambda$-shaped error curve and for GLR reconstruction, a sample size of $ \\mathcal{O}(\\sqrt{N})$, where $N$ is the total number of vertices, results in lower reconstruction error than near full observation. We present thresholds on the SNRs, $\\tau$ and $\\tau_{GLR}$, below which the error is non-monotonic, and illustrate these theoretical results with experiments across multiple random graph models, sampling schemes and SNRs. These results demonstrate that any decision in sample-size choice has to be made in light of the noise levels in the data."}, "https://arxiv.org/abs/2211.00880": {"title": "DeepTrace: Learning to Optimize Contact Tracing in Epidemic Networks with Graph Neural Networks", "link": "https://arxiv.org/abs/2211.00880", "description": "arXiv:2211.00880v3 Announce Type: replace \nAbstract: Digital contact tracing aims to curb epidemics by identifying and mitigating public health emergencies through technology. Backward contact tracing, which tracks the sources of infection, proved crucial in places like Japan for identifying COVID-19 infections from superspreading events. This paper presents a novel perspective of digital contact tracing as online graph exploration and addresses the forward and backward contact tracing problem as a maximum-likelihood (ML) estimation problem using iterative epidemic network data sampling. The challenge lies in the combinatorial complexity and rapid spread of infections. We introduce DeepTrace, an algorithm based on a Graph Neural Network (GNN) that iteratively updates its estimations as new contact tracing data is collected, learning to optimize the maximum likelihood estimation by utilizing topological features to accelerate learning and improve convergence. The contact tracing process combines either BFS or DFS to expand the network and trace the infection source, ensuring comprehensive and efficient exploration. Additionally, the GNN model is fine-tuned through a two-phase approach: pre-training with synthetic networks to approximate likelihood probabilities and fine-tuning with high-quality data to refine the model. Using COVID-19 variant data, we illustrate that DeepTrace surpasses current methods in identifying superspreaders, providing a robust basis for a scalable digital contact tracing strategy."}, "https://arxiv.org/abs/2307.01915": {"title": "Using mathematics to study how people influence each other's opinions", "link": "https://arxiv.org/abs/2307.01915", "description": "arXiv:2307.01915v3 Announce Type: replace \nAbstract: People sometimes change their opinions when they discuss things with other people. Researchers study mathematical models of opinions to explore how people influence each other through their social interactions. In today's digital world, these models can help us learn how to promote accurate information and reduce unwanted influence. In this article, we discuss a simple mathematical model that looks at opinion changes from social interactions. We briefly describe what opinion models can tell us and how researchers try to make them more realistic."}, "https://arxiv.org/abs/2308.05247": {"title": "TUBERAIDER: Attributing Coordinated Hate Attacks on YouTube Videos to their Source Communities", "link": "https://arxiv.org/abs/2308.05247", "description": "arXiv:2308.05247v2 Announce Type: replace \nAbstract: Alas, coordinated hate attacks, or raids, are becoming increasingly common online. In a nutshell, these are perpetrated by a group of aggressors who organize and coordinate operations on a platform (e.g., 4chan) to target victims on another community (e.g., YouTube). In this paper, we focus on attributing raids to their source community, paving the way for moderation approaches that take the context (and potentially the motivation) of an attack into consideration. We present TUBERAIDER, an attribution system achieving over 75% accuracy in detecting and attributing coordinated hate attacks on YouTube videos. We instantiate it using links to YouTube videos shared on 4chan's /pol/ board, r/The_Donald, and 16 Incels-related subreddits. We use a peak detector to identify a rise in the comment activity of a YouTube video, which signals that an attack may be occurring. We then train a machine learning classifier based on the community language (i.e., TF-IDF scores of relevant keywords) to perform the attribution. We test TUBERAIDER in the wild and present a few case studies of actual aggression attacks identified by it to showcase its effectiveness."}, "https://arxiv.org/abs/2401.17890": {"title": "Followers do not dictate the virality of news outlets on social media", "link": "https://arxiv.org/abs/2401.17890", "description": "arXiv:2401.17890v2 Announce Type: replace \nAbstract: Initially conceived for entertainment, social media platforms have profoundly transformed the dissemination of information and consequently reshaped the dynamics of agenda-setting. In this scenario, understanding the factors that capture audience attention and drive viral content is crucial. Employing Gibrat's Law, which posits that an entity's growth rate is unrelated to its size, we examine the engagement growth dynamics of news outlets on social media. Our analysis encloses the Facebook historical data of over a thousand news outlets, encompassing approximately 57 million posts in four European languages from 2008 to the end of 2022. We discover universal growth dynamics according to which news virality is independent of the traditional size or engagement with the outlet. Moreover, our analysis reveals a significant long-term impact of news source reliability on engagement growth, with engagement induced by unreliable sources decreasing over time. We conclude the paper by presenting a statistical model replicating the observed growth dynamics."}, "https://arxiv.org/abs/2402.01940": {"title": "Generalized Naming Game and Bayesian Naming Game as Dynamical Systems", "link": "https://arxiv.org/abs/2402.01940", "description": "arXiv:2402.01940v3 Announce Type: replace \nAbstract: We study the $\\beta$-model ($\\beta$-NG) and the Bayesian Naming Game (BNG) as dynamical systems. By applying linear stability analysis to the dynamical system associated with the $\\beta$-model, we demonstrate the existence of a non-generic bifurcation with a bifurcation point $\\beta_c = 1/3$. As $\\beta$ passes through $\\beta_c$, the stability of isolated fixed points changes, giving rise to a one-dimensional manifold of fixed points. Notably, this attracting invariant manifold forms an arc of an ellipse. In the context of the BNG, we propose modeling the Bayesian learning probabilities $p_A$ and $p_B$ as logistic functions. This modeling approach allows us to establish the existence of fixed points without relying on the overly strong assumption that $p_A = p_B = p$, where $p$ is a constant."}, "https://arxiv.org/abs/2401.07115": {"title": "Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models", "link": "https://arxiv.org/abs/2401.07115", "description": "arXiv:2401.07115v2 Announce Type: replace-cross \nAbstract: The emergence of unveiling human-like behaviors in Large Language Models (LLMs) has led to a closer connection between NLP and human psychology. Scholars have been studying the inherent personalities exhibited by LLMs and attempting to incorporate human traits and behaviors into them. However, these efforts have primarily focused on commercially-licensed LLMs, neglecting the widespread use and notable advancements seen in Open LLMs. This work aims to address this gap by employing a set of 12 LLM Agents based on the most representative Open models and subject them to a series of assessments concerning the Myers-Briggs Type Indicator (MBTI) test and the Big Five Inventory (BFI) test. Our approach involves evaluating the intrinsic personality traits of Open LLM agents and determining the extent to which these agents can mimic human personalities when conditioned by specific personalities and roles. Our findings unveil that $(i)$ each Open LLM agent showcases distinct human personalities; $(ii)$ personality-conditioned prompting produces varying effects on the agents, with only few successfully mirroring the imposed personality, while most of them being ``closed-minded'' (i.e., they retain their intrinsic traits); and $(iii)$ combining role and personality conditioning can enhance the agents' ability to mimic human personalities. Our work represents a step up in understanding the dense relationship between NLP and human psychology through the lens of Open LLMs."}, "https://arxiv.org/abs/2404.11869": {"title": "Node-like as a Whole: Structure-aware Searching and Coarsening for Graph Classification", "link": "https://arxiv.org/abs/2404.11869", "description": "arXiv:2404.11869v2 Announce Type: replace-cross \nAbstract: Graph Transformers (GTs) have made remarkable achievements in graph-level tasks. However, most existing works regard graph structures as a form of guidance or bias for enhancing node representations, which focuses on node-central perspectives and lacks explicit representations of edges and structures. One natural question is, can we treat graph structures node-like as a whole to learn high-level features? Through experimental analysis, we explore the feasibility of this assumption. Based on our findings, we propose a novel multi-view graph representation learning model via structure-aware searching and coarsening (GRLsc) on GT architecture for graph classification. Specifically, we build three unique views, original, coarsening, and conversion, to learn a thorough structural representation. We compress loops and cliques via hierarchical heuristic graph coarsening and restrict them with well-designed constraints, which builds the coarsening view to learn high-level interactions between structures. We also introduce line graphs for edge embeddings and switch to edge-central perspective to construct the conversion view. Experiments on eight real-world datasets demonstrate the improvements of GRLsc over 28 baselines from various architectures."}, "https://arxiv.org/abs/2404.16066": {"title": "Social Media Use is Predictable from App Sequences: Using LSTM and Transformer Neural Networks to Model Habitual Behavior", "link": "https://arxiv.org/abs/2404.16066", "description": "arXiv:2404.16066v2 Announce Type: replace-cross \nAbstract: The present paper introduces a novel approach to studying social media habits through predictive modeling of sequential smartphone user behaviors. While much of the literature on media and technology habits has relied on self-report questionnaires and simple behavioral frequency measures, we examine an important yet understudied aspect of media and technology habits: their embeddedness in repetitive behavioral sequences. Leveraging Long Short-Term Memory (LSTM) and transformer neural networks, we show that (i) social media use is predictable at the within and between-person level and that (ii) there are robust individual differences in the predictability of social media use. We examine the performance of several modeling approaches, including (i) global models trained on the pooled data from all participants, (ii) idiographic person-specific models, and (iii) global models fine-tuned on person-specific data. Neither person-specific modeling nor fine-tuning on person-specific data substantially outperformed the global models, indicating that the global models were able to represent a variety of idiosyncratic behavioral patterns. Additionally, our analyses reveal that the person-level predictability of social media use is not substantially related to the frequency of smartphone use in general or the frequency of social media use, indicating that our approach captures an aspect of habits that is distinct from behavioral frequency. Implications for habit modeling and theoretical development are discussed."}, "https://arxiv.org/abs/2406.17043": {"title": "The hidden architecture of connections: How do multidimensional identities shape our social networks?", "link": "https://arxiv.org/abs/2406.17043", "description": "arXiv:2406.17043v1 Announce Type: new \nAbstract: Our multidimensional identities determine how we interact with each other, shaping social networks through group-based connection preferences. While interactions along single dimensions have been extensively studied, the dynamics driving multidimensional connection preferences remain largely unexplored. In this work, we develop a network model of multidimensional social interactions to tackle two crucial questions: What is the structure of our latent connection preferences, and how do we integrate information from our multidimensional identities to connect with others? To answer these questions, we systematically model different latent preference structures and preference aggregation mechanisms. Then, we compare them using Bayesian model selection by fitting empirical data from high school friendship networks. We find that a simple latent preference model consistently outperforms more complex alternatives. The calibrated model provides robust measures of latent connection preferences in real-world networks, bringing insights into how one- and multidimensional groups interact. Finally, we develop natural operationalizations of dimension salience, revealing which aspects of identity are most relevant for individuals when forming connections."}, "https://arxiv.org/abs/2406.17135": {"title": "Testing network clustering algorithms with Natural Language Processing", "link": "https://arxiv.org/abs/2406.17135", "description": "arXiv:2406.17135v1 Announce Type: new \nAbstract: The advent of online social networks has led to the development of an abundant literature on the study of online social groups and their relationship to individuals' personalities as revealed by their textual productions. Social structures are inferred from a wide range of social interactions. Those interactions form complex -- sometimes multi-layered -- networks, on which community detection algorithms are applied to extract higher order structures. The choice of the community detection algorithm is however hardily questioned in relation with the cultural production of the individual they classify. In this work, we assume the entangled nature of social networks and their cultural production to propose a definition of cultural based online social groups as sets of individuals whose online production can be categorized as social group-related. We take advantage of this apparently self-referential description of online social groups with a hybrid methodology that combines a community detection algorithm and a natural language processing classification algorithm. A key result of this analysis is the possibility to score community detection algorithms using their agreement with the natural language processing classification. A second result is that we can assign the opinion of a random user at >85% accuracy."}, "https://arxiv.org/abs/2406.17435": {"title": "Echo chamber effects in signed networks", "link": "https://arxiv.org/abs/2406.17435", "description": "arXiv:2406.17435v1 Announce Type: new \nAbstract: Echo chamber effects in social networks are generally attributed to the prevalence of interactions among like-minded peers. However, recent evidence has emphasized the role of hostile interactions between opposite-minded groups. Here, we model information propagation between such groups by generalizing popular contagion models to signed networks. We show that echo chambers spontaneously emerge in balanced networks, and in antibalanced ones for specific parameters. The robustness of our results is confirmed through simulations on various network topologies, including a real-world dataset."}, "https://arxiv.org/abs/2406.17552": {"title": "A Weighted-Median Model of Opinion Dynamics on Networks", "link": "https://arxiv.org/abs/2406.17552", "description": "arXiv:2406.17552v1 Announce Type: new \nAbstract: Social interactions influence people's opinions. In some situations, these interactions result in a consensus opinion; in others, they result in opinion fragmentation and the formation of different opinion groups in the form of \"echo chambers\". Consider a social network of individuals, who hold continuous-valued scalar opinions and change their opinions when they interact with each other. In such an opinion model, it is common for an opinion-update rule to depend on the mean opinion of interacting individuals. However, we consider an alternative update rule - which may be more realistic in some situations - that instead depends on a weighted median opinion of interacting individuals. Through numerical simulations of our opinion model, we investigate how the limit opinion distribution depends on network structure. For configuration-model networks, we also derive a mean-field approximation for the asymptotic dynamics of the opinion distribution when there are infinitely many individuals in a network."}, "https://arxiv.org/abs/2406.17556": {"title": "Modularity Based Community Detection in Hypergraphs", "link": "https://arxiv.org/abs/2406.17556", "description": "arXiv:2406.17556v1 Announce Type: new \nAbstract: In this paper, we propose a scalable community detection algorithm using hypergraph modularity function, h-Louvain. It is an adaptation of the classical Louvain algorithm in the context of hypergraphs. We observe that a direct application of the Louvain algorithm to optimize the hypergraph modularity function often fails to find meaningful communities. We propose a solution to this issue by adjusting the initial stage of the algorithm via carefully and dynamically tuned linear combination of the graph modularity function of the corresponding two-section graph and the desired hypergraph modularity function. The process is guided by Bayesian optimization of the hyper-parameters of the proposed procedure. Various experiments on synthetic as well as real-world networks are performed showing that this process yields improved results in various regimes."}, "https://arxiv.org/abs/2406.17724": {"title": "Spatiotemporal statistical features of velocity responses to traffic congestions in a local motorway network", "link": "https://arxiv.org/abs/2406.17724", "description": "arXiv:2406.17724v1 Announce Type: new \nAbstract: The causal connection between congestions and velocity changes at different locations induces various statistical features, which we identify and measure in detail. We carry out an empirical analysis of large-scale traffic data on a local motorway network around the Breitscheid intersection in the North Rhine-Westphalia, Germany. We put forward a response function which measures the velocity change at a certain location versus time conditioned on a congestion at another location. We use a novel definition of the corresponding congestion indicator to ensure causality. We find that the response of velocities to the congestion exhibits phase changes in time. A negative response at smaller time lags transforms into positive one at larger time lags, implying a certain traffic mechanism. The response decays as a power law with the distance. We also identify a scaling property leading to a collapse of the response functions on one curve."}, "https://arxiv.org/abs/2406.17736": {"title": "Fairness in Social Influence Maximization via Optimal Transport", "link": "https://arxiv.org/abs/2406.17736", "description": "arXiv:2406.17736v1 Announce Type: new \nAbstract: We study fairness in social influence maximization, whereby one seeks to select seeds that spread a given information throughout a network, ensuring balanced outreach among different communities (e.g. demographic groups). In the literature, fairness is often quantified in terms of the expected outreach within individual communities. In this paper, we demonstrate that such fairness metrics can be misleading since they ignore the stochastic nature of information diffusion processes. When information diffusion occurs in a probabilistic manner, multiple outreach scenarios can occur. As such, outcomes such as \"in 50% of the cases, no one of group 1 receives the information and everyone in group 2 receives it and in other 50%, the opposite happens\", which always results in largely unfair outcomes, are classified as fair by a variety of fairness metrics in the literature. We tackle this problem by designing a new fairness metric, mutual fairness, that captures variability in outreach through optimal transport theory. We propose a new seed selection algorithm that optimizes both outreach and mutual fairness, and we show its efficacy on several real datasets. We find that our algorithm increases fairness with only a minor decrease (and at times, even an increase) in efficiency."}, "https://arxiv.org/abs/2406.17752": {"title": "Connectivity and Community Structure of Online and Register-based Social Networks", "link": "https://arxiv.org/abs/2406.17752", "description": "arXiv:2406.17752v1 Announce Type: new \nAbstract: The dominance of online social media data as a source of population-scale social network studies has recently been challenged by networks constructed from government-curated register data. In this paper, we investigate how the two compare, focusing on aggregations of the Dutch online social network (OSN) Hyves and a register-based social network (RSN) of the Netherlands. First and foremost, we find that the connectivity of the two population-scale networks is strikingly similar, especially between closeby municipalities, with more long-distance ties captured by the OSN. This result holds when correcting for population density and geographical distance, notwithstanding that these two patterns appear to be the main drivers of connectivity. Second, we show that the community structure of neither network follows strict administrative geographical delineations (e.g., provinces). Instead, communities appear to either center around large metropolitan areas or, outside of the country's most urbanized area, are comprised of large blocks of interdependent municipalities. Interestingly, beyond population and distance-related patterns, communities also highlight the persistence of deeply rooted historical and sociocultural communities based on religion. The results of this study suggest that both online social networks and register-based social networks are valuable resources for insights into the social network structure of an entire population."}, "https://arxiv.org/abs/2406.16963": {"title": "Large Language Models for Link Stealing Attacks Against Graph Neural Networks", "link": "https://arxiv.org/abs/2406.16963", "description": "arXiv:2406.16963v1 Announce Type: cross \nAbstract: Graph data contains rich node features and unique edge information, which have been applied across various domains, such as citation networks or recommendation systems. Graph Neural Networks (GNNs) are specialized for handling such data and have shown impressive performance in many applications. However, GNNs may contain of sensitive information and susceptible to privacy attacks. For example, link stealing is a type of attack in which attackers infer whether two nodes are linked or not. Previous link stealing attacks primarily relied on posterior probabilities from the target GNN model, neglecting the significance of node features. Additionally, variations in node classes across different datasets lead to different dimensions of posterior probabilities. The handling of these varying data dimensions posed a challenge in using a single model to effectively conduct link stealing attacks on different datasets. To address these challenges, we introduce Large Language Models (LLMs) to perform link stealing attacks on GNNs. LLMs can effectively integrate textual features and exhibit strong generalizability, enabling attacks to handle diverse data dimensions across various datasets. We design two distinct LLM prompts to effectively combine textual features and posterior probabilities of graph nodes. Through these designed prompts, we fine-tune the LLM to adapt to the link stealing attack task. Furthermore, we fine-tune the LLM using multiple datasets and enable the LLM to learn features from different datasets simultaneously. Experimental results show that our approach significantly enhances the performance of existing link stealing attack tasks in both white-box and black-box scenarios. Our method can execute link stealing attacks across different datasets using only a single model, making link stealing attacks more applicable to real-world scenarios."}, "https://arxiv.org/abs/2406.17144": {"title": "An information-geometric approach for network decomposition using the q-state Potts model", "link": "https://arxiv.org/abs/2406.17144", "description": "arXiv:2406.17144v1 Announce Type: cross \nAbstract: Complex networks are critical in many scientific, technological, and societal contexts due to their ability to represent and analyze intricate systems with interdependent components. Often, after labeling the nodes of a network with a community detection algorithm, its modular organization emerges, allowing a better understanding of the underlying structure by uncovering hidden relationships. In this paper, we introduce a novel information-geometric framework for the filtering and decomposition of networks whose nodes have been labeled. Our approach considers the labeled network as the outcome of a Markov random field modeled by a q-state Potts model. According to information geometry, the first and second order Fisher information matrices are related to the metric and curvature tensor of the parametric space of a statistical model. By computing an approximation to the local shape operator, the proposed methodology is able to identify low and high information nodes, allowing the decomposition of the labeled network in two complementary subgraphs. Hence, we call this method as the LO-HI decomposition. Experimental results with several kinds of networks show that the high information subgraph is often related to edges and boundaries, while the low information subgraph is a smoother version of the network, in the sense that the modular structure is improved."}, "https://arxiv.org/abs/2406.17518": {"title": "Enhancing Explainability of Knowledge Learning Paths: Causal Knowledge Networks", "link": "https://arxiv.org/abs/2406.17518", "description": "arXiv:2406.17518v1 Announce Type: cross \nAbstract: A reliable knowledge structure is a prerequisite for building effective adaptive learning systems and intelligent tutoring systems. Pursuing an explainable and trustworthy knowledge structure, we propose a method for constructing causal knowledge networks. This approach leverages Bayesian networks as a foundation and incorporates causal relationship analysis to derive a causal network. Additionally, we introduce a dependable knowledge-learning path recommendationHuman-Centric eXplainable AI in Education technique built upon this framework, improving teaching and learning quality while maintaining transparency in the decision-making process."}, "https://arxiv.org/abs/2406.17572": {"title": "Ranking nodes in bipartite systems with a non-linear iterative map", "link": "https://arxiv.org/abs/2406.17572", "description": "arXiv:2406.17572v1 Announce Type: cross \nAbstract: This paper introduces a method based on a non-linear iterative map to evaluate node relevance in bipartite networks. By tuning a single parameter gamma, the method captures different concepts of node importance, including established measures like degree centrality, eigenvector centrality and the fitness-complexity ranking used in economics. The algorithm's flexibility allows for efficient ranking optimization tailored to specific tasks. As an illustrative example, we apply this method to ecological mutualistic networks, where ranking quality can be assessed by the extinction area - the rate at which the system collapses when species are removed in a certain order. The map with the optimal gamma value, which is dataset-specific, surpasses existing ranking methods on this task. Additionally, our method excels in evaluating nestedness, another crucial structural property of ecological systems, requiring specific node rankings. The final part of the paper explores the theoretical aspects of the map, revealing a phase transition at a critical $\\gamma$ value dependent on the data structure that can be characterized analytically for random networks. Near the critical point, the map exhibits unique features and a distinctive triangular packing pattern of the adjacency matrix."}, "https://arxiv.org/abs/2207.01830": {"title": "Optimal Verification of Rumors in Networks", "link": "https://arxiv.org/abs/2207.01830", "description": "arXiv:2207.01830v2 Announce Type: replace-cross \nAbstract: We study the diffusion of a true and a false message when agents are biased and able to verify messages. As a recipient of a rumor who verifies it becomes informed of the truth, a higher rumor prevalence can increase the prevalence of the truth. We uncover conditions such that this happens and discuss policy implications. Specifically, a planner aiming to maximize the prevalence of the truth should allow rumors to circulate if: verification overcomes ignorance of messages, transmission of information is relatively low, and the planner's budget to induce verification is neither too low nor too high."}, "https://arxiv.org/abs/2210.04359": {"title": "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates", "link": "https://arxiv.org/abs/2210.04359", "description": "arXiv:2210.04359v2 Announce Type: replace-cross \nAbstract: Solidarity is a crucial concept to understand social relations in societies. In this paper, we explore fine-grained solidarity frames to study solidarity towards women and migrants in German parliamentary debates between 1867 and 2022. Using 2,864 manually annotated text snippets (with a cost exceeding 18k Euro), we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and GPT-4. We find that GPT-4 outperforms other LLMs, approaching human annotation quality. Using GPT-4, we automatically annotate more than 18k further instances (with a cost of around 500 Euro) across 155 years and find that solidarity with migrants outweighs anti-solidarity but that frequencies and solidarity types shift over time. Most importantly, group-based notions of (anti-)solidarity fade in favor of compassionate solidarity, focusing on the vulnerability of migrant groups, and exchange-based anti-solidarity, focusing on the lack of (economic) contribution. Our study highlights the interplay of historical events, socio-economic needs, and political ideologies in shaping migration discourse and social cohesion. We also show that powerful LLMs, if carefully prompted, can be cost-effective alternatives to human annotation for hard social scientific tasks."}, "https://arxiv.org/abs/2401.11116": {"title": "Effects of Research Paper Promotion via ArXiv and X", "link": "https://arxiv.org/abs/2401.11116", "description": "arXiv:2401.11116v2 Announce Type: replace-cross \nAbstract: In the evolving landscape of scientific publishing, it is important to understand the drivers of high-impact research, to equip scientists with actionable strategies to enhance the reach of their work, and to understand trends in the use of modern scientific publishing tools to inform their further development. Here, we study trends in the use of early preprint publications and revisions on ArXiv and the use of X (formerly Twitter) for promotion of such papers in computer science and physics. We find that early submissions to ArXiv and promotion on X have soared in recent years. Estimating the effect that the use of each of these modern affordances has on the number of citations of scientific publications, we find that peer-reviewed conference papers in computer science that are submitted early to ArXiv gain on average $21.1 \\pm 17.4$ more citations, revised on ArXiv gain $18.4 \\pm 17.6$ more citations, and promoted on X gain $44.4 \\pm 8$ more citations in the first 5 years from an initial publication. In contrast, journal articles in physics experience comparatively lower boosts in citation counts, with increases of $3.9 \\pm 1.1$, $4.3 \\pm 0.9$, and $6.9 \\pm 3.5$ citations respectively for the same interventions. Our results show that promoting one's work on ArXiv or X has a large impact on the number of citations, as well as the number of influential citations computed by Semantic Scholar, and thereby on the career of researchers. These effects are present also for publications in physics, but they are relatively smaller. The larger relative effect sizes, effects of promotion accumulating over time, and elevated unpredictability of the number of citations in computer science than in physics suggest a greater role of world-of-mouth spreading in computer science than in physics."}, "https://arxiv.org/abs/2403.14431": {"title": "Breaking Consensus in Kinetic Opinion Formation Models on Graphons", "link": "https://arxiv.org/abs/2403.14431", "description": "arXiv:2403.14431v2 Announce Type: replace-cross \nAbstract: In this work we propose and investigate a strategy to prevent consensus in kinetic models for opinion formation. We consider a large interacting agent system, and assume that agent interactions are driven by compromise as well as self-thinking dynamics and also modulated by an underlying static social network. This network structure is included using so-called graphons, which modulate the interaction frequency in the corresponding kinetic formulation. We then derive the corresponding limiting Fokker Planck equation, and analyze its large time behavior. This microscopic setting serves as a starting point for the proposed control strategy, which steers agents away from mean opinion and is characterised by a suitable penalization depending on the properties of the graphon. We show that this minimalist approach is very effective by analyzing the quasi-stationary solutions mean-field model in a plurality of graphon structures. Several numerical experiments are also provided to show the effectiveness of the approach in preventing the formation of consensus steering the system towards a declustered state."}, "https://arxiv.org/abs/2406.17904": {"title": "Application of Liquid Rank Reputation System for Twitter Trend Analysis on Bitcoin", "link": "https://arxiv.org/abs/2406.17904", "description": "arXiv:2406.17904v1 Announce Type: new \nAbstract: Analyzing social media trends can create a win-win situation for both creators and consumers. Creators can receive fair compensation, while consumers gain access to engaging, relevant, and personalized content. This paper proposes a new model for analyzing Bitcoin trends on Twitter by incorporating a 'liquid democracy' approach based on user reputation. This system aims to identify the most impactful trends and their influence on Bitcoin prices and trading volume. It uses a Twitter sentiment analysis model based on a reputation rating system to determine the impact on Bitcoin price change and traded volume. In addition, the reputation model considers the users' higher-order friends on the social network (the initial Twitter input channels in our case study) to improve the accuracy and diversity of the reputation results. We analyze Bitcoin-related news on Twitter to understand how trends and user sentiment, measured through our Liquid Rank Reputation System, affect Bitcoin price fluctuations and trading activity within the studied time frame. This reputation model can also be used as an additional layer in other trend and sentiment analysis models. The paper proposes the implementation, challenges, and future scope of the liquid rank reputation model."}, "https://arxiv.org/abs/2406.17993": {"title": "Anatomizing Societal Recovery at the Microscale: Heterogeneity in Household Lifestyle Activities Rebounding after Disasters", "link": "https://arxiv.org/abs/2406.17993", "description": "arXiv:2406.17993v1 Announce Type: new \nAbstract: This study presents a granular analysis of societal recovery from disasters at the individual level, focusing on the aftermath of Hurricane Harvey and Hurricane Ida. Societal recovery is defined as the restoration of the societal functioning of the affected community to its normal/steady-state level. It evaluates the recovery of impacted residents based on fluctuations in their lifestyle patterns in visits to points of interest. The analysis focuses on: (1) the extent of heterogeneity in lifestyle recovery of residents in the same spatial area; and (2) the extent to which variations in lifestyle recovery and its heterogeneity among users can be explained based on hazard impact extent and social vulnerability. As lifestyle recovery progresses, heterogeneity diminishes, indicating that lower lifestyle recovery rates correlate with higher heterogeneity within a spatial area. This relationship between lifestyle recovery and heterogeneity can lead to the misestimation of recovery timelines, potentially resulting in the inefficient allocation of resources and disproportionate attention to already recovering communities. Key contributions of the study are fourfold: First, it characterizes societal recovery at the finest scale by examining fluctuations in individual lifestyles, revealing heterogeneity even among neighbors. Second, it proposes using individual lifestyle as an indicator of societal functioning to measure, more human centrically, disaster impacts and recovery speeds. Third, it introduces a method for quantifying lifestyle recovery that enables near-real-time monitoring, departing from traditional survey-based methods. Fourth, it provides empirical insights into the relationship between disaster impacts and societal recovery, showing that the severity of disaster impacts and resident income levels and percentage of minority populations influence recovery durations."}, "https://arxiv.org/abs/2406.18168": {"title": "Emergence of social hierarchies in a society with two competitive classes", "link": "https://arxiv.org/abs/2406.18168", "description": "arXiv:2406.18168v1 Announce Type: new \nAbstract: Agent-based models describing social interactions among individuals can help to better understand emerging macroscopic patterns in societies. One of the topics which is worth tackling is the formation of different kinds of hierarchies that emerge in social spaces such as cities. Here we propose a Bonabeau-like model by adding a second class of agents. The fundamental particularity of our model is that only a pairwise interaction between agents of the opposite class is allowed. Agent fitness can thus only change by competition among the two classes, while the total fitness in the society remains constant. The main result is that for a broad range of values of the model parameters, the fitness of the agents of each class show a decay in time except for one or very few agents which capture almost all the fitness in the society. Numerical simulations also reveal a singular shift from egalitarian to hierarchical society for each class. This behaviour depends on the control parameter $\\eta$, playing the role of the inverse of the temperature of the system. Results are invariant with regard to the system size, contingent solely on the quantity of agents within each class. Finally, a couple of scaling laws are provided thus showing a data collapse from different model parameters and they follow a shape which can be related to the presence of a phase transition in the model."}, "https://arxiv.org/abs/2406.18503": {"title": "From Tweet to Theft: Tracing the Flow of Stolen Cryptocurrency", "link": "https://arxiv.org/abs/2406.18503", "description": "arXiv:2406.18503v1 Announce Type: new \nAbstract: This paper presents a case study of a cryptocurrency scam that utilized coordinated and inauthentic behavior on Twitter. In 2020, 143 accounts sold by an underground merchant were used to orchestrate a fake giveaway. Tweets pointing to a fake blog post lured victims into sending Uniswap tokens (UNI) to designated addresses on the Ethereum blockchain, with the false promise of receiving more tokens in return. Using one of the scammer's addresses and leveraging the transparency and immutability of the Ethereum blockchain, we traced the flow of stolen funds through various addresses, revealing the tactics adopted to obfuscate traceability. The final destination of the funds involved two deposit addresses. The first, managed by a well-known cryptocurrency exchange, was likely associated with the scammer's own account on that platform and saw deposits exceeding $3.5 million. The second address was linked to a popular cryptocurrency swap service. These findings highlight the critical need for more stringent measures to verify the source of funds and prevent illicit activities."}, "https://arxiv.org/abs/2406.17836": {"title": "A Moonshot for AI Oracles in the Sciences", "link": "https://arxiv.org/abs/2406.17836", "description": "arXiv:2406.17836v1 Announce Type: cross \nAbstract: Nobel laureate Philip Anderson and Elihu Abrahams once stated that, \"even if machines did contribute to normal science, we see no mechanism by which they could create a Kuhnian revolution and thereby establish a new physical law.\" In this Perspective, we draw upon insights from the philosophies of science and artificial intelligence (AI) to propose necessary conditions of precisely such a mechanism for generating revolutionary mathematical theories. Recent advancements in AI suggest that satisfying the proposed necessary conditions by machines may be plausible; thus, our proposed necessary conditions also define a moonshot challenge. We also propose a heuristic definition of the intelligibility of mathematical theories to accelerate the development of machine theorists."}, "https://arxiv.org/abs/2406.17918": {"title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and Retrieval", "link": "https://arxiv.org/abs/2406.17918", "description": "arXiv:2406.17918v1 Announce Type: cross \nAbstract: In our recent research, we have developed a framework called GraphSnapShot, which has been proven an useful tool for graph learning acceleration. GraphSnapShot is a framework for fast cache, storage, retrieval and computation for graph learning. It can quickly store and update the local topology of graph structure and allows us to track patterns in the structure of graph networks, just like take snapshots of the graphs. In experiments, GraphSnapShot shows efficiency, it can achieve up to 30% training acceleration and 73% memory reduction for lossless graph ML training compared to current baselines such as dgl.This technique is particular useful for large dynamic graph learning tasks such as social media analysis and recommendation systems to process complex relationships between entities."}, "https://arxiv.org/abs/2406.17963": {"title": "Empowering Interdisciplinary Insights with Dynamic Graph Embedding Trajectories", "link": "https://arxiv.org/abs/2406.17963", "description": "arXiv:2406.17963v1 Announce Type: cross \nAbstract: We developed DyGETViz, a novel framework for effectively visualizing dynamic graphs (DGs) that are ubiquitous across diverse real-world systems. This framework leverages recent advancements in discrete-time dynamic graph (DTDG) models to adeptly handle the temporal dynamics inherent in dynamic graphs. DyGETViz effectively captures both micro- and macro-level structural shifts within these graphs, offering a robust method for representing complex and massive dynamic graphs. The application of DyGETViz extends to a diverse array of domains, including ethology, epidemiology, finance, genetics, linguistics, communication studies, social studies, and international relations. Through its implementation, DyGETViz has revealed or confirmed various critical insights. These include the diversity of content sharing patterns and the degree of specialization within online communities, the chronological evolution of lexicons across decades, and the distinct trajectories exhibited by aging-related and non-related genes. Importantly, DyGETViz enhances the accessibility of scientific findings to non-domain experts by simplifying the complexities of dynamic graphs. Our framework is released as an open-source Python package for use across diverse disciplines. Our work not only addresses the ongoing challenges in visualizing and analyzing DTDG models but also establishes a foundational framework for future investigations into dynamic graph representation and analysis across various disciplines."}, "https://arxiv.org/abs/2403.06641": {"title": "Socio-spatial segregation and human mobility: A review of empirical evidence", "link": "https://arxiv.org/abs/2403.06641", "description": "arXiv:2403.06641v3 Announce Type: replace \nAbstract: Social segregation, the spatial and social separation between individuals from different backgrounds, can affect sustainable urban development and social cohesion. The literature has traditionally focused on residential segregation, examining how individuals' residential locations are distributed differently across neighborhoods based on income, ethnicity, and education. However, this approach overlooks the complexity of spatial segregation because daily activities often extend far beyond residential areas. Since the 2010s, emerging mobility data sources have enabled a new understanding of socio-spatial segregation by considering daily activities such as work, school, shopping, and leisure visits. From traditional surveys to GPS trajectories, diverse data sources reveal that day-to-day mobility can impact segregation by reducing or amplifying segregation levels obtained when considering residential aspects alone. This literature review focuses on three critical questions: (a) How do human mobility patterns relate to individuals' segregation experiences? (b) What key factors explain the relationship between one's mobility patterns and segregation experiences? and (c) What are the strengths and limitations of segregation research that incorporates extensive mobility data? Our literature review enhances the understanding of socio-spatial segregation at the individual level and clarifies core concepts and methodological challenges in the field. By incorporating studies from computational social science, urban science, and transportation, our review aims to provide actionable insights for reducing segregation and addressing research gaps in this increasingly interdisciplinary area."}, "https://arxiv.org/abs/2302.08829": {"title": "Great year, bad Sharpe? A note on the joint distribution of performance and risk-adjusted return", "link": "https://arxiv.org/abs/2302.08829", "description": "arXiv:2302.08829v2 Announce Type: replace-cross \nAbstract: Returns distributions are heavy-tailed across asset classes. In this note, I examine the implications of this well-known stylized fact for the joint statistics of performance (absolute return) and Sharpe ratio (risk-adjusted return). Using both synthetic and real data, I show that, all other things being equal, the investments with the best in-sample performance are never associated with the best in-sample Sharpe ratios (and vice versa). This counter-intuitive effect is unrelated to the risk-return tradeoff familiar from portfolio theory: it is, rather, a consequence of asymptotic correlations between the sample mean and sample standard deviation of heavy-tailed variables. In addition to its large sample noise, this non-monotonic association of the Sharpe ratio with performance puts into question its status as the gold standard metric of investment quality."}, "https://arxiv.org/abs/2311.00721": {"title": "Empathy Detection from Text, Audiovisual, Audio or Physiological Signals: Task Formulations and Machine Learning Methods", "link": "https://arxiv.org/abs/2311.00721", "description": "arXiv:2311.00721v2 Announce Type: replace-cross \nAbstract: Empathy indicates an individual's ability to understand others. Over the past few years, empathy has drawn attention from various disciplines, including but not limited to Affective Computing, Cognitive Science and Psychology. Detecting empathy has potential applications in society, healthcare and education. Despite being a broad and overlapping topic, the avenue of empathy detection leveraging Machine Learning remains underexplored from a systematic literature review perspective. We collected 828 papers from 10 well-known databases, systematically screened them and analysed the final 61 papers. Our analyses reveal several prominent task formulations $-$ including empathy on localised utterances or overall expressions, unidirectional or parallel empathy, and emotional contagion $-$ in monadic, dyadic and group interactions. Empathy detection methods are summarised based on four input modalities $-$ text, audiovisual, audio and physiological signals $-$ thereby presenting modality-specific network architecture design protocols. We discuss challenges, research gaps and potential applications in the Affective Computing-based empathy domain, which can facilitate new avenues of exploration. We further enlist the public availability of datasets and codes. We believe that our work is a stepping stone to developing a robust empathy detection system that can be deployed in practice to enhance the overall well-being of human life."}, "https://arxiv.org/abs/2406.18617": {"title": "Similarities among top one day batters: physics-based quantification", "link": "https://arxiv.org/abs/2406.18617", "description": "arXiv:2406.18617v1 Announce Type: new \nAbstract: Assessment of the performance of a player in any sport is very much needed to determine the ranking of players and make a solid team with the best players. Besides these, fans, journalists, sports persons, and sports councils often analyse the performances of current and retired players to identify the best players of all time. Here, we study the performance of all-time top batters in one-day cricket using physics-based statistical methods. The batters are selected in this study who possess either higher total runs or a high number of centuries. It is found that the total runs increases linearly with the innings number at the later stage of the batter carrier, and the runs rate estimated from the linear regression analysis also increases linearly with the average runs. The probability of non-scoring innings is found to be a negligibly small number (i.e., $\\leq 0.1$ ) for each batter. Furthermore, based on innings-wise runs, we have computed the six-dimensional probability distribution vector for each player. Two components of the probability distribution vector vary linearly with average runs. The component representing the probability of scoring runs less than 50 linearly decreases with the average runs. In contrast, the probability of scoring runs greater than or equal to 100 and less than 150 linearly increases with the average runs. We have also estimated the entropy to assess the diversity of a player. Interestingly, the entropy varies linearly with the average runs, giving rise to two clusters corresponding to the old and recent players. Furthermore, the angle between two probability vectors is calculated for each pair of players to measure the similarities among the players. It is found that some of the players are almost identical to each other."}, "https://arxiv.org/abs/2406.18761": {"title": "Why Teach Quantum In Your Own Time: The Values of Grassroots Organizations Involved in Quantum Technologies Education and Outreach", "link": "https://arxiv.org/abs/2406.18761", "description": "arXiv:2406.18761v1 Announce Type: new \nAbstract: This paper examines the intersection of goals and values within grassroots organizations operating in the realm of quantum technologies (QT) education. It delineates a fundamental distinction between the objective to provide education and the drive to democratize learning through principles of inclusivity, accessibility, and diversity. The analysis reveals how these organizations navigate their nascent stages, grappling with the dual challenge of adhering to their foundational values while aspiring for sustainable growth and development in the highly specialized field of QT. The study uncovers the strategic approaches adopted by these entities, including efforts to create educational ecosystems and foster community engagement. The research underscores the potential vulnerabilities of these grassroots organizations, particularly in relation to the longevity and evolution of their initiatives as members transition into professional roles within the quantum sector. Through this investigation, the paper contributes to a nuanced understanding of how emerging educational organizations in the QT field balance their ideological commitments with practical growth considerations, highlighting the critical factors that influence their trajectory and impact."}, "https://arxiv.org/abs/2406.18780": {"title": "Investigation on centrality measures and opinion dynamics in two-layer networks with replica nodes", "link": "https://arxiv.org/abs/2406.18780", "description": "arXiv:2406.18780v1 Announce Type: new \nAbstract: We examine two-layer networks and centrality measures defined on them. The propose two fast and accurate algorithms to approximate the game-theoretic centrality measures and examine connection between centrality measures and characteristics of opinion dynamic processes on such networks. As an example, we consider a Zachary's karate club social network and extend it by adding the second (internal) layer of communication. Internal layer represents the idea that individuals can share their real opinions with their close friends. The structures of the external and internal layers may be different. As characteristics of of opinion dynamic processes we mean consensus time and winning rate of a particular opinion. We find significantly strong positive correlation between internal graph density and consensus time, and significantly strong negative correlation between centrality of authoritative nodes and consensus time."}, "https://arxiv.org/abs/2406.18792": {"title": "A data-driven assessment of biomedical terminology evolution using information theoretical and network analysis approaches", "link": "https://arxiv.org/abs/2406.18792", "description": "arXiv:2406.18792v1 Announce Type: new \nAbstract: The Medical Subject Headings (MeSH), one of the main knowledge organization systems in the biomedical domain, is constantly evolving following the latest scientific discoveries in health and life sciences. Previous research focused on quantifying information in MeSH using its hierarchical structure. In this work, we propose a data-driven approach based on information theory and network analyses to quantify the knowledge evolution in MeSH and the relevance of its individual concepts. Our approach leverages article annotations and their citation networks to compute the level of informativeness, usefulness, disruptiveness, and influence of MeSH concepts over time. The citation network includes the instances of MeSH concepts or MeSH headings, and the concept relevance is calculated individually. Then, this computation is propagated to the hierarchy to establish the relevance of a concept. We quantitatively evaluated our approach using changes in the MeSH terminology and showed that it effectively captures the evolution of the terminology. Moreover, we validated the ability of our framework to characterize retracted articles and show that concepts used to annotate retracted articles differ substantially from those used to annotate non-retracted. The proposed framework provides an effective method to rank concept relevance and can be useful in maintaining evolving knowledge organization systems."}, "https://arxiv.org/abs/2406.19079": {"title": "Oligopoly Game Stabilisation Through Multilayer Congestion Dynamics", "link": "https://arxiv.org/abs/2406.19079", "description": "arXiv:2406.19079v1 Announce Type: new \nAbstract: International trade and logistics are subject to factors including geopolitical instability, climate change, and black swan events such as the unforeseen closure of the Suez Canal. The problem of predicting local price change under modification of an underlying transport network or change in supply characteristics unites elements of game theory, network theory and transport. The Cournot Oligopoly models economic actors as rational players attempting to maximise profit by optimising supply quantities with analytical results now consolidated about equilibrium characteristics where transport conditions are fixed. Similarly, where supply and demand are fixed, the routing of goods in a transport network can be analytically solved through a traffic assignment problem. Hence we can solve the coupled Cournot-congestion problem by means of a 2-layer network. Where the layers are linked, inter-layer feedback wherein players attempt to maximise their utility occurs. In this respect we find players benefit from taking advantage of non-simultaneous responses to the market rather than moving to a new equilibrium. We draw conclusions about the nature of equilibria, finding that the concave utility curve property results in unique and stable equilibrium for each uncoupled layer, while linked layers have a non-unique stable equilibria for which general solutions are stated."}, "https://arxiv.org/abs/2406.19149": {"title": "\"A network of mutualities of being\": socio-material archaeological networks and biological ties at \\c{C}atalh\\\"oy\\\"uk", "link": "https://arxiv.org/abs/2406.19149", "description": "arXiv:2406.19149v1 Announce Type: new \nAbstract: Recent advances in archaeogenomics have granted access to previously unavailable biological information with the potential to further our understanding of past social dynamics at a range of scales. However, to properly integrate these data within archaeological narratives, new methodological and theoretical tools are required. Effort must be put into finding new methods for weaving together different datasets where material culture and archaeogenomic data are both constitutive elements. This is true on a small scale, when we study relationships at the individual level, and at a larger scale when we deal with social and population dynamics. Specifically, in the study of kinship systems it is essential to contextualize and make sense of biological relatedness through social relations, which, in archaeology, is achieved by using material culture as a proxy. In this paper we propose a Network Science framework to integrate archaeogenomic data and material culture at an intrasite scale to study biological relatedness and social organization at the Neolithic site of \\c{C}atalh\\\"oy\\\"uk. Methodologically, we propose the use of network variance to investigate the concentration of biological relatedness and material culture within networks of houses. This approach allowed us to observe how material culture similarity between buildings gives valuable information on potential biological relationships between individuals and how biogenetic ties concentrate at specific localities on site."}, "https://arxiv.org/abs/2406.19204": {"title": "CoDiNG -- Naming Game with Continuous Latent State of Agents", "link": "https://arxiv.org/abs/2406.19204", "description": "arXiv:2406.19204v1 Announce Type: new \nAbstract: Understanding the mechanisms behind opinion formation is crucial for gaining insight into the processes that shape political beliefs, cultural attitudes, consumer choices, and social movements. This work aims to explore a nuanced model that captures the intricacies of real-world opinion dynamics by synthesizing principles from cognitive science and employing social network analysis. The proposed model is a hybrid continuous-discrete extension of the well-known Naming Game opinion model. The added latent continuous layer of opinion strength follows cognitive processes in the human brain, akin to memory imprints. The discrete layer allows for the conversion of intrinsic continuous opinion into discrete form, which often occurs when we publicly verbalize our opinions. We evaluated our model using real data as ground truth and demonstrated that the proposed mechanism outperforms the classic Naming Game model in many cases, reflecting that our model is closer to the real process of opinion formation."}, "https://arxiv.org/abs/2406.19277": {"title": "The Emergence of Threads: The Birth of a New Social Network", "link": "https://arxiv.org/abs/2406.19277", "description": "arXiv:2406.19277v1 Announce Type: new \nAbstract: Threads, a new microblogging platform from Meta, was launched in July 2023. In contrast to prior new platforms, Threads was borne out of an existing parent platform, Instagram, for which all users must already possess an account. This offers a unique opportunity to study platform evolution, to understand how one existing platform can support the \"birth\" of another. With this in mind, this paper provides an initial exploration of Threads, contrasting it with its parent, Instagram. We compare user behaviour within and across the two social media platforms, focusing on posting frequency, content preferences, and engagement patterns. Utilising a temporal analysis framework, we identify consistent daily posting trends on the parent platform and uncover contrasting behaviours when comparing intra-platform and cross-platform activities. Our findings reveal that Threads engages more with political and AI-related topics, compared to Instagram which focuses more on lifestyle and fashion topics. Our analysis also shows that user activities align more closely on weekends across both platforms. Engagement analysis suggests that users prefer to post about topics that garner more likes and that topic consistency is maintained when users transition from Instagram to Threads. Our research provides insights into user behaviour and offers a basis for future studies on Threads."}, "https://arxiv.org/abs/2406.18596": {"title": "Uniform Stability of Dynamic SICA HIV Transmission Models on Time Scales", "link": "https://arxiv.org/abs/2406.18596", "description": "arXiv:2406.18596v1 Announce Type: cross \nAbstract: We consider a SICA model for HIV transmission on time scales. We prove permanence of solutions and we derive sufficient conditions for the existence and uniform asymptotic stability of a unique positive almost periodic solution of the system in terms of a Lyapunov function."}, "https://arxiv.org/abs/2406.18854": {"title": "What Is Missing In Homophily? Disentangling Graph Homophily For Graph Neural Networks", "link": "https://arxiv.org/abs/2406.18854", "description": "arXiv:2406.18854v1 Announce Type: cross \nAbstract: Graph homophily refers to the phenomenon that connected nodes tend to share similar characteristics. Understanding this concept and its related metrics is crucial for designing effective Graph Neural Networks (GNNs). The most widely used homophily metrics, such as edge or node homophily, quantify such \"similarity\" as label consistency across the graph topology. These metrics are believed to be able to reflect the performance of GNNs, especially on node-level tasks. However, many recent studies have empirically demonstrated that the performance of GNNs does not always align with homophily metrics, and how homophily influences GNNs still remains unclear and controversial. Then, a crucial question arises: What is missing in our current understanding of homophily? To figure out the missing part, in this paper, we disentangle the graph homophily into $3$ aspects: label, structural, and feature homophily, providing a more comprehensive understanding of GNN performance. To investigate their synergy, we propose a Contextual Stochastic Block Model with $3$ types of Homophily (CSBM-3H), where the topology and feature generation are controlled by the $3$ metrics. Based on the theoretical analysis of CSBM-3H, we derive a new composite metric, named Tri-Hom, that considers all $3$ aspects and overcomes the limitations of conventional homophily metrics. The theoretical conclusions and the effectiveness of Tri-Hom have been verified through synthetic experiments on CSBM-3H. In addition, we conduct experiments on $31$ real-world benchmark datasets and calculate the correlations between homophily metrics and model performance. Tri-Hom has significantly higher correlation values than $17$ existing metrics that only focus on a single homophily aspect, demonstrating its superiority and the importance of homophily synergy. Our code is available at \\url{https://github.com/zylMozart/Disentangle_GraphHom}."}, "https://arxiv.org/abs/2406.19222": {"title": "The myth of declining competitive balance in the UEFA Champions League group stage", "link": "https://arxiv.org/abs/2406.19222", "description": "arXiv:2406.19222v1 Announce Type: cross \nAbstract: According to previous studies, competitive balance has significantly declined in the UEFA Champions League group stage over the recent decades. Our paper introduces six alternative indices for measuring ex ante and ex post competitive balance in order to explore the robustness of these results. The ex ante measures are based on Elo ratings, while the ex post measures compare the group ranking to reasonable benchmarks. We find no evidence of any trend in the competitive balance of the UEFA Champions League group stage between the 2003/04 and 2023/24 seasons."}, "https://arxiv.org/abs/2307.08564": {"title": "Shaping New Norms for AI", "link": "https://arxiv.org/abs/2307.08564", "description": "arXiv:2307.08564v2 Announce Type: replace \nAbstract: As Artificial Intelligence (AI) becomes increasingly integrated into our lives, the need for new norms is urgent. However, AI evolves at a much faster pace than the characteristic time of norm formation, posing an unprecedented challenge to our societies. This paper examines possible criticalities of the processes of norm formation surrounding AI. Thus, it focuses on how new norms can be established, rather than on what these norms should be. It distinguishes different scenarios based on the centralisation or decentralisation of the norm formation process, analysing the cases where new norms are shaped by formal authorities, informal institutions, or emerge spontaneously in a bottom-up fashion. On the latter point, the paper reports a conversation with ChatGPT in which the LLM discusses some of the emerging norms it has observed. Far from seeking exhaustiveness, this article aims to offer readers interpretive tools to understand society's response to the growing pervasiveness of AI. An outlook on how AI could influence the formation of future social norms emphasises the importance for open societies to anchor their formal deliberation process in an open, inclusive, and transparent public discourse."}, "https://arxiv.org/abs/2311.16360": {"title": "Geometrics of the Adjacent Possible: Harvesting Values at the Curvature", "link": "https://arxiv.org/abs/2311.16360", "description": "arXiv:2311.16360v2 Announce Type: replace \nAbstract: Novelty alone is not sufficient for innovation. For new ideas and products to thrive, they must find their place within the existing societal fabric, such as institutions, conventions, and infrastructures that have been built over time. Past successes create inertia, favoring conservative advances. Here, we develop a quantitative framework to map the contours of the adjacent possible in the presence of the power of typicality. Typical assemblies, frequently combined building blocks in past innovations, compress and curve the space of possibilities toward what is imaginable, accessible, and implementable, much like gravitational forces on new ideas and actions. We demonstrate that these curvatures in the space of possibilities are not just abstract constructs but empirically measurable through two complementary studies. We first show that Edison's inventions are primarily located in areas of high curvature, aligning with his strategy of building upon institutionalized domains. In contrast, Tesla's inventions are mainly found in low-curvature areas, indicating his propensity for exploring new territories and pushing innovation boundaries. Further analysis of the entire U.S. patent database reveals that innovations in high-curvature areas are more likely to yield monetary value. High-curvature areas indicate windows of opportunity through the interplay between innovation and convention, explaining why commercially successful ideas often emerge at the fringes of institutionalized domains."}, "https://arxiv.org/abs/2302.03228": {"title": "Heterophily-Aware Graph Attention Network", "link": "https://arxiv.org/abs/2302.03228", "description": "arXiv:2302.03228v2 Announce Type: replace-cross \nAbstract: Graph Neural Networks (GNNs) have shown remarkable success in graph representation learning. Unfortunately, current weight assignment schemes in standard GNNs, such as the calculation based on node degrees or pair-wise representations, can hardly be effective in processing the networks with heterophily, in which the connected nodes usually possess different labels or features. Existing heterophilic GNNs tend to ignore the modeling of heterophily of each edge, which is also a vital part in tackling the heterophily problem. In this paper, we firstly propose a heterophily-aware attention scheme and reveal the benefits of modeling the edge heterophily, i.e., if a GNN assigns different weights to edges according to different heterophilic types, it can learn effective local attention patterns, which enable nodes to acquire appropriate information from distinct neighbors. Then, we propose a novel Heterophily-Aware Graph Attention Network (HA-GAT) by fully exploring and utilizing the local distribution as the underlying heterophily, to handle the networks with different homophily ratios. To demonstrate the effectiveness of the proposed HA-GAT, we analyze the proposed heterophily-aware attention scheme and local distribution exploration, by seeking for an interpretation from their mechanism. Extensive results demonstrate that our HA-GAT achieves state-of-the-art performances on eight datasets with different homophily ratios in both the supervised and semi-supervised node classification tasks."}, "https://arxiv.org/abs/2310.17171": {"title": "Estimating True Beliefs in Opinion Dynamics with Social Pressure", "link": "https://arxiv.org/abs/2310.17171", "description": "arXiv:2310.17171v2 Announce Type: replace-cross \nAbstract: Social networks often exert social pressure, causing individuals to adapt their expressed opinions to conform to their peers. An agent in such systems can be modeled as having a (true and unchanging) inherent belief while broadcasting a declared opinion at each time step based on her inherent belief and the past declared opinions of her neighbors. An important question in this setting is parameter estimation: how to disentangle the effects of social pressure to estimate inherent beliefs from declared opinions. This is useful for forecasting when agents' declared opinions are influenced by social pressure while real-world behavior only depends on their inherent beliefs. To address this, Jadbabaie et al. formulated the Interacting P\\'olya Urn model of opinion dynamics under social pressure and studied it on complete-graph social networks using an aggregate estimator, and found that their estimator converges to the inherent beliefs unless majority pressure pushes the network to consensus.\n  In this work, we studythis model on arbitrary networks, providing an estimator which converges to the inherent beliefs even in consensus situations. Finally, we bound the convergence rate of our estimator in both consensus and non-consensus scenarios; to get the bound for consensus scenarios (which converge slower than non-consensus) we additionally found how quickly the system converges to consensus."}, "https://arxiv.org/abs/2406.19554": {"title": "A Network-Based Measure of Cosponsorship Influence on Bill Passing in the United States House of Representatives", "link": "https://arxiv.org/abs/2406.19554", "description": "arXiv:2406.19554v1 Announce Type: new \nAbstract: Each year, the United States Congress considers {thousands of legislative proposals to select bills} to present to the US President to sign into law. Naturally, the decision processes of members of Congress are subject to peer influence. In this paper, we examine the effect on bill passage of accrued influence between US Congress members in the US House of Representatives. We explore how the influence of a bill's cosponsors affects the bill's outcome (specifically, whether or not it passes in the House). We define a notion of influence by analyzing the structure of a network that we construct {using} cosponsorship dynamics. We award `influence' between a pair of Congress members when they cosponsor a bill that achieves some amount of legislative success. We find that properties of the bill cosponsorship network can be a useful signal to examine influence in Congress; they help explain why some bills pass and others fail. We compare our measure of influence to off-the-shelf centrality measures and conclude that our influence measure is more indicative of bill passage."}, "https://arxiv.org/abs/2406.19571": {"title": "Reranking Social Media Feeds: A Practical Guide for Field Experiments", "link": "https://arxiv.org/abs/2406.19571", "description": "arXiv:2406.19571v1 Announce Type: new \nAbstract: Social media plays a central role in shaping public opinion and behavior, yet performing experiments on these platforms and, in particular, on feed algorithms is becoming increasingly challenging. This article offers practical recommendations to researchers developing and deploying field experiments focused on real-time re-ranking of social media feeds. This article is organized around two contributions. First, we overview an experimental method using web browser extensions that intercepts and re-ranks content in real-time, enabling naturalistic re-ranking field experiments. We then describe feed interventions and measurements that this paradigm enables on participants' actual feeds, without requiring the involvement of social media platforms. Second, we offer concrete technical recommendations for intercepting and re-ranking social media feeds with minimal user-facing delay, and provide an open-source implementation. This document aims to summarize lessons learned, provide concrete implementation details, and foster the ecosystem of independent social media research."}, "https://arxiv.org/abs/2406.19692": {"title": "Steering cooperation: Adversarial attacks on prisoner's dilemma in complex networks", "link": "https://arxiv.org/abs/2406.19692", "description": "arXiv:2406.19692v1 Announce Type: new \nAbstract: This study examines the application of adversarial attack concepts to control the evolution of cooperation in the prisoner's dilemma game in complex networks. Specifically, it proposes a simple adversarial attack method that drives players' strategies towards a target state by adding small perturbations to social networks. The proposed method is evaluated on both model and real-world networks. Numerical simulations demonstrate that the proposed method can effectively promote cooperation with significantly smaller perturbations compared to other techniques. Additionally, this study shows that adversarial attacks can also be useful in inhibiting cooperation (promoting defection). The findings reveal that adversarial attacks on social networks can be potent tools for both promoting and inhibiting cooperation, opening new possibilities for controlling cooperative behavior in social systems while also highlighting potential risks."}, "https://arxiv.org/abs/2406.19867": {"title": "Sampled Datasets Risk Substantial Bias in the Identification of Political Polarization on Social Media", "link": "https://arxiv.org/abs/2406.19867", "description": "arXiv:2406.19867v1 Announce Type: new \nAbstract: Following recent policy changes by X (Twitter) and other social media platforms, user interaction data has become increasingly difficult to access. These restrictions are impeding robust research pertaining to social and political phenomena online, which is critical due to the profound impact social media platforms may have on our societies. Here, we investigate the reliability of polarization measures obtained from different samples of social media data by studying the structural polarization of the Polish political debate on Twitter over a 24-hour period. First, we show that the political discussion on Twitter is only a small subset of the wider Twitter discussion. Second, we find that large samples can be representative of the whole political discussion on a platform, but small samples consistently fail to accurately reflect the true structure of polarization online. Finally, we demonstrate that keyword-based samples can be representative if keywords are selected with great care, but that poorly selected keywords can result in substantial political bias in the sampled data. Our findings demonstrate that it is not possible to measure polarization in a reliable way with small, sampled datasets, highlighting why the current lack of research data is so problematic, and providing insight into the practical implementation of the European Union's Digital Service Act which aims to improve researchers' access to social media data."}, "https://arxiv.org/abs/2406.19878": {"title": "A political radicalization framework based on Moral Foundations Theory", "link": "https://arxiv.org/abs/2406.19878", "description": "arXiv:2406.19878v1 Announce Type: new \nAbstract: Moral Foundations Theory proposes that individuals with conflicting political views base their behavior on different principles chosen from a small group of universal moral foundations. This study proposes using a set of widely accepted moral foundations (Fairness, Ingroup loyalty, Authority, and Purity) as proxies to determine the degree of radicalization of online communities. The fifth principle, Care, is generally surpassed by others, which are higher in the radicalized groups' moral hierarchy. Moreover, the presented data-driven methodological framework proposes an alternative way to measure whether a community complies with some moral principle or foundation: not evaluating its speech, but its behavior through interactions of its individuals, establishing a bridge between structural features of the interaction network and the intensity of communities' radicalization regarding the considered moral foundations. Two foundations may be assessed using the network's structural characteristics: Ingroup loyalty measured by group-level modularity, and Authority evaluated using group domination for detecting potential hierarchical substructures within the network. By analyzing the set of Pareto-optimal groups regarding a multidimensional moral relevance scale, the most radicalized communities are identified among those considered extreme in some of their attitudes or views. The application of the proposed framework is illustrated using real-world datasets. The radicalized communities' behavior exhibits increasing isolation, and its authorities and leaders show growing domination over their audience. There were also detected differences between users' behavior and speech, showing that individuals tend to share more 'extreme' ingroup content than that they publish: extreme views get more likes on social media."}, "https://arxiv.org/abs/2406.19953": {"title": "Uncovering the hidden core-periphery structure in hyperbolic networks", "link": "https://arxiv.org/abs/2406.19953", "description": "arXiv:2406.19953v1 Announce Type: new \nAbstract: The hyperbolic network models exhibit very fundamental and essential features, like small-worldness, scale-freeness, high-clustering coefficient, and community structure. In this paper, we comprehensively explore the presence of an important feature, the core-periphery structure, in the hyperbolic network models, which is often exhibited by real-world networks. We focused on well-known hyperbolic models such as popularity-similarity optimization model (PSO) and S1/H2 models and studied core-periphery structures using a well-established method that is based on standard random walk Markov chain model. The observed core-periphery centralization values indicate that the core-periphery structure can be very pronounced under certain conditions. We also validate our findings by statistically testing for the significance of the observed core-periphery structure in the network geometry. This study extends network science and reveals core-periphery insights applicable to various domains, enhancing network performance and resiliency in transportation and information systems."}, "https://arxiv.org/abs/2406.19543": {"title": "Demarked: A Strategy for Enhanced Abusive Speech Moderation through Counterspeech, Detoxification, and Message Management", "link": "https://arxiv.org/abs/2406.19543", "description": "arXiv:2406.19543v1 Announce Type: cross \nAbstract: Despite regulations imposed by nations and social media platforms, such as recent EU regulations targeting digital violence, abusive content persists as a significant challenge. Existing approaches primarily rely on binary solutions, such as outright blocking or banning, yet fail to address the complex nature of abusive speech. In this work, we propose a more comprehensive approach called Demarcation scoring abusive speech based on four aspect -- (i) severity scale; (ii) presence of a target; (iii) context scale; (iv) legal scale -- and suggesting more options of actions like detoxification, counter speech generation, blocking, or, as a final measure, human intervention. Through a thorough analysis of abusive speech regulations across diverse jurisdictions, platforms, and research papers we highlight the gap in preventing measures and advocate for tailored proactive steps to combat its multifaceted manifestations. Our work aims to inform future strategies for effectively addressing abusive speech online."}, "https://arxiv.org/abs/2406.19679": {"title": "Statistical Analysis on Scale and Regional Distribution of Undergraduate Physics Programs in Korean Universities", "link": "https://arxiv.org/abs/2406.19679", "description": "arXiv:2406.19679v1 Announce Type: cross \nAbstract: We report on the temporal changes in undergraduate-level physics programs at Korean universities from 1915 to 2023 by analyzing data on physics-related departments and their students using basic statistics and the scaling theory of statistical physics. Our analysis reveals that the number of departments peaked around the turn of the 21st century, and it has been steadily decreasing ever since, with particularly severe declines in private universities located outside the capital region. Besides the change in the overall numbers, we also show the change in the self-identity of physics-related departments reflected in department names, which reveals a recent trend of emphasizing more application-side such as semiconductors and data. As a sophisticated measure to quantify regional imbalances relative to the population eligible for higher education, we present scaling exponents from the scaling theory, which shows a shift from sublinear to linear for departments and a shift from linear to superlinear for students. The result indicates the exacerbation of the regional imbalance of university-level physics education in Korea."}, "https://arxiv.org/abs/2304.12559": {"title": "Attraction by pairwise coherence explains the emergence of ideological sorting", "link": "https://arxiv.org/abs/2304.12559", "description": "arXiv:2304.12559v4 Announce Type: replace \nAbstract: Political polarization has become a growing concern in democratic societies, as it drives tribal alignments and erodes civic deliberation among citizens. Given its prevalence across different countries, previous research has sought to understand under which conditions people tend to endorse extreme opinions. However, in polarized contexts, citizens not only adopt more extreme views but also become correlated across issues that are, a priori, seemingly unrelated. This phenomenon, known as \"ideological sorting\", has been receiving greater attention in recent years but the micro-level mechanisms underlying its emergence remain poorly understood. Here, we study the conditions under which a social dynamic system is expected to become ideologically sorted as a function of the mechanisms of interaction between its individuals. To this end, we developed and analyzed a multidimensional agent-based model that incorporates two mechanisms: homophily (where people tend to interact with those holding similar opinions) and pairwise-coherence favoritism (where people tend to interact with ingroups holding politically coherent opinions). We numerically integrated the model's master equations that perfectly describe the system's dynamics and found that ideological sorting only emerges in models that include pairwise-coherence favoritism. We then compared the model's outcomes with empirical data from 24,035 opinions across 67 topics and found that pairwise-coherence favoritism is significantly present in datasets that measure political attitudes but absent across topics not considered related to politics. Overall, this work combines theoretical approaches from system dynamics with model-based analyses of empirical data to uncover a potential mechanism underlying the pervasiveness of ideological sorting."}, "https://arxiv.org/abs/2403.00195": {"title": "The evolution of pluralistic ignorance", "link": "https://arxiv.org/abs/2403.00195", "description": "arXiv:2403.00195v2 Announce Type: replace \nAbstract: Pluralistic ignorance is a social-psychological phenomenon that occurs when individuals privately hold beliefs that differ from perceived group norms. Traditional models, based on opinion dynamics with private and public states, fail to account for a key aspect: when nonexpression aligns with normative behavior, initial social pressure can induce pluralistic ignorance. We show that pluralistic ignorance persists under infrequent imitation and strong initial minority influence. Although individuals can overcome this ignorance by the end of interactions, it reemerges in subsequent meetings. However, excessive imitation erases pluralistic ignorance, leading to a uniform state in which internal and external states align. Furthermore, incorporating memory into the internalization process shows that pluralistic ignorance peaks at moderate imitation levels."}, "https://arxiv.org/abs/2303.17001": {"title": "The G-invariant graph Laplacian", "link": "https://arxiv.org/abs/2303.17001", "description": "arXiv:2303.17001v4 Announce Type: replace-cross \nAbstract: Graph Laplacian based algorithms for data lying on a manifold have been proven effective for tasks such as dimensionality reduction, clustering, and denoising. In this work, we consider data sets whose data points lie on a manifold that is closed under the action of a known unitary matrix Lie group G. We propose to construct the graph Laplacian by incorporating the distances between all the pairs of points generated by the action of G on the data set. We deem the latter construction the ``G-invariant Graph Laplacian'' (G-GL). We show that the G-GL converges to the Laplace-Beltrami operator on the data manifold, while enjoying a significantly improved convergence rate compared to the standard graph Laplacian which only utilizes the distances between the points in the given data set. Furthermore, we show that the G-GL admits a set of eigenfunctions that have the form of certain products between the group elements and eigenvectors of certain matrices, which can be estimated from the data efficiently using FFT-type algorithms. We demonstrate our construction and its advantages on the problem of filtering data on a noisy manifold closed under the action of the special unitary group SU(2)."}, "https://arxiv.org/abs/2305.05833": {"title": "A Statistical Model of Bipartite Networks: Application to Cosponsorship in the United States Senate", "link": "https://arxiv.org/abs/2305.05833", "description": "arXiv:2305.05833v2 Announce Type: replace-cross \nAbstract: Many networks in political and social research are bipartite, with edges connecting exclusively across two distinct types of nodes. A common example includes cosponsorship networks, in which legislators are connected indirectly through the bills they support. Yet most existing network models are designed for unipartite networks, where edges can arise between any pair of nodes. However, using a unipartite network model to analyze bipartite networks, as often done in practice, can result in aggregation bias and artificially high-clustering -- a particularly insidious problem when studying the role groups play in network formation. To address these methodological problems, we develop a statistical model of bipartite networks theorized to be generated through group interactions by extending the popular mixed-membership stochastic blockmodel. Our model allows researchers to identify the groups of nodes, within each node type in the bipartite structure, that share common patterns of edge formation. The model also incorporates both node and dyad-level covariates as the predictors of group membership and of observed dyadic relations. We develop an efficient computational algorithm for fitting the model, and apply it to cosponsorship data from the United States Senate. We show that legislators in a Senate that was perfectly split along party lines were able to remain productive and pass major legislation by forming non-partisan, power-brokering coalitions that found common ground through their collaboration on low-stakes bills. We also find evidence for norms of reciprocity, and uncover the substantial role played by policy expertise in the formation of cosponsorships between senators and legislation. We make an open-source software package available that makes it possible for other researchers to uncover similar insights from bipartite networks."}, "https://arxiv.org/abs/2306.02766": {"title": "Networked Communication for Decentralised Agents in Mean-Field Games", "link": "https://arxiv.org/abs/2306.02766", "description": "arXiv:2306.02766v3 Announce Type: replace-cross \nAbstract: We introduce networked communication to the mean-field game framework, in particular to oracle-free settings where $N$ decentralised agents learn along a single, non-episodic run of the empirical system. We prove that our architecture, with only a few reasonable assumptions about network structure, has sample guarantees bounded between those of the centralised- and independent-learning cases. We discuss how the sample guarantees of the three theoretical algorithms do not actually result in practical convergence. We therefore show that in practical settings where the theoretical parameters are not observed (leading to poor estimation of the Q-function), our communication scheme significantly accelerates convergence over the independent case (and often even the centralised case), without relying on the assumption of a centralised learner. We contribute further practical enhancements to all three theoretical algorithms, allowing us to present their first empirical demonstrations. Our experiments confirm that we can remove several of the theoretical assumptions of the algorithms, and display the empirical convergence benefits brought by our new networked communication. We additionally show that the networked approach has significant advantages, over both the centralised and independent alternatives, in terms of robustness to unexpected learning failures and to changes in population size."}, "https://arxiv.org/abs/2307.14270": {"title": "Socioeconomic agents as active matter in nonequilibrium Sakoda-Schelling models", "link": "https://arxiv.org/abs/2307.14270", "description": "arXiv:2307.14270v2 Announce Type: replace-cross \nAbstract: How robust are socioeconomic agent-based models with respect to the details of the agents' decision rule? We tackle this question by considering an occupation model in the spirit of the Sakoda-Schelling model, historically introduced to shed light on segregation dynamics among human groups. For a large class of utility functions and decision rules, we pinpoint the nonequilibrium nature of the agent dynamics, while recovering the equilibrium-like phase separation phenomenology. Within the mean field approximation we show how the model can be mapped, to some extent, onto an active matter field description. Finally, we consider non-reciprocal interactions between two populations, and show how they can lead to non-steady macroscopic behavior. We believe our approach provides a unifying framework to further study geography-dependent agent-based models, notably paving the way for joint consideration of population and price dynamics within a field theoretic approach."}, "https://arxiv.org/abs/2407.00145": {"title": "Co-evolving networks for opinion and social dynamics in agent-based models", "link": "https://arxiv.org/abs/2407.00145", "description": "arXiv:2407.00145v1 Announce Type: new \nAbstract: The rise of digital social media has strengthened the coevolution of public opinions and social interactions, that shape social structures and collective outcomes in increasingly complex ways. Existing literature often explores this interplay as a one-directional influence, focusing on how opinions determine social ties within adaptive networks. However, this perspective overlooks the intrinsic dynamics driving social interactions, which can significantly influence how opinions form and evolve. In this work, we address this gap, by introducing the co-evolving opinion and social dynamics using stochastic agent-based models. Agents' mobility in a social space is governed by both their social and opinion similarity with others. Similarly, the dynamics of opinion formation is driven by the opinions of agents in their social vicinity. We analyze the underlying social and opinion interaction networks and explore the mechanisms influencing the appearance of emerging phenomena, like echo chambers and opinion consensus. To illustrate the model's potential for real-world analysis, we apply it to General Social Survey data on political identity and public opinion regarding governmental issues. Our findings highlight the model's strength in capturing the coevolution of social connections and individual opinions over time."}, "https://arxiv.org/abs/2407.00213": {"title": "Targeting influence in a harmonic opinion model", "link": "https://arxiv.org/abs/2407.00213", "description": "arXiv:2407.00213v1 Announce Type: new \nAbstract: Influence propagation in social networks is a central problem in modern social network analysis, with important societal applications in politics and advertising. A large body of work has focused on cascading models, viral marketing, and finite-horizon diffusion. There is, however, a need for more developed, mathematically principled \\emph{adversarial models}, in which multiple, opposed actors strategically select nodes whose influence will maximally sway the crowd to their point of view.\n  In the present work, we develop and analyze such a model based on harmonic functions and linear diffusion. We prove that our general problem is NP-hard and that the objective function is monotone and submodular; consequently, we can greedily approximate the solution within a constant factor. Introducing and analyzing a convex relaxation, we show that the problem can be approximately solved using smooth optimization methods. We illustrate the effectiveness of our approach on a variety of example networks."}, "https://arxiv.org/abs/2407.00254": {"title": "An Exhaustive Study of Two-Node McCulloch-Pitts Networks", "link": "https://arxiv.org/abs/2407.00254", "description": "arXiv:2407.00254v1 Announce Type: new \nAbstract: Boolean networks are widely used in computational biology, evolutionary studies, and social sciences. However, the set of all Boolean-function-defined networks are harder to study as a whole. On the other hand, McCulloch-Pitts gates are sparsely parameterized using only a few number of link strengths, making it possible to study and compare different networks models. We treat two-node McCulloch-Pitts systems as a minimal complex system. When the link strengths are discretized, $3^4=81$ network models or rules are organized in the rule space The limiting dynamics of each rule may depend on the choice of binary state value ([-1,1] or [0,1]), and on the treatment at the threshold point, leading to at least six variants. One variant with [-1,1] as the binary state value (V1 model) tends to have a more diverse dynamical behaviors with a mixture of multiple cycles and fixed points at the limiting state, whereas other variants tend to fall only to fixed-point limiting dynamics. We use V1 models to study the organization of rules with different dynamics in the rule space and robustness of limiting dynamics with respect to a mutation in the rule table, as well as the related phenomena of phase transition and edge-of-chaos. We use another variant (V4 models) with only the fixed-point limiting dynamics to study the robustness of limiting state with respect to perturbation of initial states. The two types of robustness do not seem to be associated with each other. Other aspects of fully discretized two-node MaCulloch-Pitts networks are also studied, including: the proposal of a seventh variant based on a difference equation; relation to Rene Thomas' two types of feedback loops; spectrum properties of state space transition matrix; and asynchronous updating. Our works also expand the concept of network motifs by allowing more finer details."}, "https://arxiv.org/abs/2407.00258": {"title": "Graph Simplification Solutions to the Street Intersection Miscount Problem", "link": "https://arxiv.org/abs/2407.00258", "description": "arXiv:2407.00258v1 Announce Type: new \nAbstract: Street intersection counts and densities are ubiquitous measures in transport geography and planning. However, typical street network data and typical street network analysis tools can substantially overcount them. This paper explains why this happens and introduces solutions to this problem. It presents the OSMnx package's algorithms to automatically simplify graph models of urban street networks -- via edge simplification and node consolidation -- resulting in faster, parsimonious models and more accurate network measures like intersection counts/densities, street segment lengths, and node degrees. Then it validates these algorithms and conducts a worldwide empirical assessment of count bias to quantify the motivating problem's prevalence. A full accounting of this bias and better methods to attenuate misrepresentations of intersections are necessary for data-driven, evidence-informed transport planning."}, "https://arxiv.org/abs/2407.00340": {"title": "The Echoes of the 'I': Tracing Identity with Demographically Enhanced Word Embeddings", "link": "https://arxiv.org/abs/2407.00340", "description": "arXiv:2407.00340v1 Announce Type: new \nAbstract: Identity is one of the most commonly studied constructs in social science. However, despite extensive theoretical work on identity, there remains a need for additional empirical data to validate and refine existing theories. This paper introduces a novel approach to studying identity by enhancing word embeddings with socio-demographic information. As a proof of concept, we demonstrate that our approach successfully reproduces and extends established findings regarding gendered self-views. Our methodology can be applied in a wide variety of settings, allowing researchers to tap into a vast pool of naturally occurring data, such as social media posts. Unlike similar methods already introduced in computer science, our approach allows for the study of differences between social groups. This could be particularly appealing to social scientists and may encourage the faster adoption of computational methods in the field."}, "https://arxiv.org/abs/2407.00355": {"title": "Global decomposition of networks into multiple cores formed by local hubs", "link": "https://arxiv.org/abs/2407.00355", "description": "arXiv:2407.00355v1 Announce Type: new \nAbstract: Networks are ubiquitous in various fields, representing systems where nodes and their interconnections constitute their intricate structures. We introduce a network decomposition scheme to reveal multiscale core-periphery structures lurking inside, using the concept of locally defined nodal hub centrality and edge-pruning techniques built upon it. We demonstrate that the hub-centrality-based edge pruning reveals a series of breaking points in network decomposition, which effectively separates a network into its backbone and shell structures. Our local-edge decomposition method iteratively identifies and removes locally least important nodes, and uncovers an onion-like hierarchical structure as a result. Compared with the conventional $k$-core decomposition method, our method based on relative information residing in local structures exhibits a clear advantage in terms of discovering locally crucial substructures. Furthermore, we introduce the core-periphery score to properly separate the core and periphery with our decomposition scheme. By extending the method combined with the network community structure, we successfully detect multiple core-periphery structures by decomposition inside each community. Moreover, the application of our decomposition to supernode networks defined from the communities reveals the intricate relation between the two representative mesoscale structures."}, "https://arxiv.org/abs/2407.00404": {"title": "The Uneven Impact of Mobility on the Segregation of Native and Foreign-born Individuals", "link": "https://arxiv.org/abs/2407.00404", "description": "arXiv:2407.00404v1 Announce Type: new \nAbstract: Segregation is a key challenge in promoting more diverse and inclusive cities. Research based on smartphone data has revealed that segregation can extend beyond residential areas into everyday activities like visiting shops and restaurants. The impact of these activities on segregation, however, is unclear. Some studies suggest that they promote mixing, while others indicate they reinforce segregation. Here, we elucidate how day-to-day mobility shapes overall segregation levels, looking at the distinctive segregation experienced by native and foreign-born individuals. Our study is based on ~320,000 smartphone trajectories collected in Sweden, where immigration creates profound divides. We find that while mobility levels generally promote mixing for native-born individuals, foreign-born individuals remain segregated in their out-of-home activities. Using counterfactual simulations, we show that this heterogeneous effect of mobility on experienced segregation results mainly from two mechanisms: homophily and limited travel, i.e., foreign-born individuals (i) prefer destinations visited by similar individuals, and (ii) have limited mobility ranges. We show that homophily plays a minor role, while limited mobility, associated with reduced transport access, limits opportunities for foreign-born to diversify their encounters. Our findings reconcile conflicting literature and suggest that enhancing transport accessibility in foreign-born areas could reduce social segregation."}, "https://arxiv.org/abs/2407.01106": {"title": "Indirect social influence and diffusion of innovations: An experimental approach", "link": "https://arxiv.org/abs/2407.01106", "description": "arXiv:2407.01106v1 Announce Type: new \nAbstract: A fundamental feature for understanding the diffusion of innovations through a social group is the manner in which we are influenced by our own social interactions. It is usually assumed that only direct interactions, those that form our social network, determine the dynamics of adopting innovations. Here, we put this assumption to the test by experimentally and theoretically studying the role of direct and indirect influences in the adoption of innovations. We perform experiments specifically designed to capture the influence that an individual receives from their direct social ties as well as from those socially close to them, as a function of the separation they have in their social network. The results of 21 experimental sessions with more than 590 participants show that the rate of adoption of an innovation is significantly influenced not only by our nearest neighbors but also by the second and third levels of influences an adopter has. Using a mathematical model that accounts for both direct and indirect interactions in a network, we fit the experimental results and determine the way in which influences decay with social distance. The results indicate that the strength of peer pressure on an adopter coming from its second and third circles of influence is approximately 2/3 and 1/3, respectively, relative to their closest neighbors. Our results strongly suggest that innovation adoption is a complex process in which an individual feels significant pressure not only from their direct ties but also by those socially close to them."}, "https://arxiv.org/abs/2407.01213": {"title": "EMIF: Evidence-aware Multi-source Information Fusion Network for Explainable Fake News Detection", "link": "https://arxiv.org/abs/2407.01213", "description": "arXiv:2407.01213v1 Announce Type: new \nAbstract: Extensive research on automatic fake news detection has been conducted due to the significant detrimental effects of fake news proliferation. Most existing approaches rely on a single source of evidence, such as comments or relevant news, to derive explanatory evidence for decision-making, demonstrating exceptional performance. However, their single evidence source suffers from two critical drawbacks: (i) noise abundance, and (ii) resilience deficiency. Inspired by the natural process of fake news identification, we propose an Evidence-aware Multi-source Information Fusion (EMIF) network that jointly leverages user comments and relevant news to make precise decision and excavate reliable evidence. To accomplish this, we initially construct a co-attention network to capture general semantic conflicts between comments and original news. Meanwhile, a divergence selection module is employed to identify the top-K relevant news articles with content that deviates the most from the original news, which ensures the acquisition of multiple evidence with higher objectivity. Finally, we utilize an inconsistency loss function within the evidence fusion layer to strengthen the consistency of two types of evidence, both negating the authenticity of the same news. Extensive experiments and ablation studies on real-world dataset FibVID show the effectiveness of our proposed model. Notably, EMIF shows remarkable robustness even in scenarios where a particular source of information is inadequate."}, "https://arxiv.org/abs/2407.01279": {"title": "Finding Hidden Swing Voters in the 2022 Italian Elections Twitter Discourse", "link": "https://arxiv.org/abs/2407.01279", "description": "arXiv:2407.01279v1 Announce Type: new \nAbstract: The global proliferation of social media platforms has transformed political communication, making the study of online interactions between politicians and voters crucial for understanding contemporary political discourse. In this work, we examine the dynamics of political messaging and voter behavior on Twitter during the 2022 Italian general elections. Specifically, we focus on voters who changed their political preferences over time (swing voters), identifying significant patterns of migration and susceptibility to propaganda messages. Our analysis reveals that during election periods, the popularity of politicians increases, and there is a notable variation in the use of persuasive language techniques, including doubt, loaded language, appeals to values, and slogans. Swing voters are more vulnerable to these propaganda techniques compared to non-swing voters, with differences in vulnerability patterns across various types of political shifts. These findings highlight the nuanced impact of social media on political opinion in Italy."}, "https://arxiv.org/abs/2407.01293": {"title": "Applying the Ego Network Model to Cross-Target Stance Detection", "link": "https://arxiv.org/abs/2407.01293", "description": "arXiv:2407.01293v1 Announce Type: new \nAbstract: Understanding human interactions and social structures is an incredibly important task, especially in such an interconnected world. One task that facilitates this is Stance Detection, which predicts the opinion or attitude of a text towards a target entity. Traditionally, this has often been done mainly via the use of text-based approaches, however, recent work has produced a model (CT-TN) that leverages information about a user's social network to help predict their stance, outperforming certain cross-target text-based approaches. Unfortunately, the data required for such graph-based approaches is not always available. This paper proposes two novel tools for Stance Detection: the Ego Network Model (ENM) and the Signed Ego Network Model (SENM). These models are founded in anthropological and psychological studies and have been used within the context of social network analysis and related tasks (e.g., link prediction). Stance Detection predictions obtained using these features achieve a level of accuracy similar to the graph-based features used by CT-TN while requiring less and more easily obtainable data. In addition to this, the performances of the inner and outer circles of the ENM, representing stronger and weaker social ties, respectively are compared. Surprisingly, the outer circles, which contain more numerous but less intimate connections, are more useful for predicting stance."}, "https://arxiv.org/abs/2407.01405": {"title": "Social Isolation, Digital Connection: COVID-19's Impact on Twitter Ego Networks", "link": "https://arxiv.org/abs/2407.01405", "description": "arXiv:2407.01405v1 Announce Type: new \nAbstract: One of the most impactful measures to fight the COVID-19 pandemic in its early first years was the lockdown, implemented by governments to reduce physical contact among people and minimize opportunities for the virus to spread. As people were compelled to limit their physical interactions and stay at home, they turned to online social platforms to alleviate feelings of loneliness. Ego networks represent how people organize their relationships due to human cognitive constraints that impose limits on meaningful interactions among people. Physical contacts were disrupted during the lockdown, causing socialization to shift entirely online, leading to a shift in socialization into online platforms. Our research aimed to investigate the impact of lockdown measures on online ego network structures potentially caused by the increase of cognitive expenses in online social networks. In particular, we examined a large Twitter dataset of users, covering 7 years of their activities. We found that during the lockdown, there was an increase in network sizes and a richer structure in social circles, with relationships becoming more intimate. Moreover, we observe that, after the lockdown measures were relaxed, these features returned to their pre-lockdown values."}, "https://arxiv.org/abs/2407.01460": {"title": "How Clustering Affects the Convergence of Decentralized Optimization over Networks: A Monte-Carlo-based Approach", "link": "https://arxiv.org/abs/2407.01460", "description": "arXiv:2407.01460v1 Announce Type: new \nAbstract: Decentralized algorithms have gained substantial interest owing to advancements in cloud computing, Internet of Things (IoT), intelligent transportation networks, and parallel processing over sensor networks. The convergence of such algorithms is directly related to specific properties of the underlying network topology. Specifically, the clustering coefficient is known to affect, for example, the controllability/observability and the epidemic growth over networks. In this work, we study the effects of the clustering coefficient on the convergence rate of networked optimization approaches. In this regard, we model the structure of large-scale distributed systems by random scale-free (SF) and clustered scale-free (CSF) networks and compare the convergence rate by tuning the network clustering coefficient. This is done by keeping other relevant network properties (such as power-law degree distribution, number of links, and average degree) unchanged. Monte-Carlo-based simulations are used to compare the convergence rate over many trials of SF graph topologies. Furthermore, to study the convergence rate over real case studies, we compare the clustering coefficient of some real-world networks with the eigenspectrum of the underlying network (as a measure of convergence rate). The results interestingly show higher convergence rate over low-clustered networks. This is significant as one can improve the learning rate of many existing decentralized machine-learning scenarios by tuning the network clustering."}, "https://arxiv.org/abs/2407.01471": {"title": "Tracking the 2024 US Presidential Election Chatter on Tiktok: A Public Multimodal Dataset", "link": "https://arxiv.org/abs/2407.01471", "description": "arXiv:2407.01471v1 Announce Type: new \nAbstract: This paper documents our release of a large-scale data collection of TikTok posts related to the upcoming 2024 U.S. Presidential Election. Our current data comprises 1.8 million videos published between November 1, 2023, and May 26, 2024. Its exploratory analysis identifies the most common keywords, hashtags, and bigrams in both Spanish and English posts, focusing on the election and the two main Presidential candidates, President Joe Biden and Donald Trump.\n  We utilized the TikTok Research API, incorporating various election-related keywords and hashtags, to capture the full scope of relevant content. To address the limitations of the TikTok Research API, we also employed third-party scrapers to expand our dataset. The dataset is publicly available at https://github.com/gabbypinto/US2024PresElectionTikToks"}, "https://arxiv.org/abs/2407.00042": {"title": "Module control of network analysis in psychopathology", "link": "https://arxiv.org/abs/2407.00042", "description": "arXiv:2407.00042v1 Announce Type: cross \nAbstract: The network approach to characterizing psychopathology departs from traditional latent categorical and dimensional approaches. Causal interplay among symptoms contributed to dynamic psychopathology system. Therefore, analyzing the symptom clusters is critical for understanding mental disorders. Furthermore, despite extensive research studying the topological features of symptom networks, the control relationships between symptoms remain largely unclear. Here, we present a novel systematizing concept, module control, to analyze the control principle of the symptom network at a module level. We introduce Module Control Network (MCN) to identify key modules that regulate the network's behavior. By applying our approach to a multivariate psychological dataset, we discover that non-emotional modules, such as sleep-related and stress-related modules, are the primary controlling modules in the symptom network. Our findings indicate that module control can expose central symptom cluster governing psychopathology network, offering novel insights into the underlying mechanisms of mental disorders and individualized approach to psychological interventions."}, "https://arxiv.org/abs/2407.00167": {"title": "Can GPT-4 Help Detect Quit Vaping Intentions? An Exploration of Automatic Data Annotation Approach", "link": "https://arxiv.org/abs/2407.00167", "description": "arXiv:2407.00167v1 Announce Type: cross \nAbstract: In recent years, the United States has witnessed a significant surge in the popularity of vaping or e-cigarette use, leading to a notable rise in cases of e-cigarette and vaping use-associated lung injury (EVALI) that caused hospitalizations and fatalities during the EVALI outbreak in 2019, highlighting the urgency to comprehend vaping behaviors and develop effective strategies for cessation. Due to the ubiquity of social media platforms, over 4.7 billion users worldwide use them for connectivity, communications, news, and entertainment with a significant portion of the discourse related to health, thereby establishing social media data as an invaluable organic data resource for public health research. In this study, we extracted a sample dataset from one vaping sub-community on Reddit to analyze users' quit-vaping intentions. Leveraging OpenAI's latest large language model GPT-4 for sentence-level quit vaping intention detection, this study compares the outcomes of this model against layman and clinical expert annotations. Using different prompting strategies such as zero-shot, one-shot, few-shot and chain-of-thought prompting, we developed 8 prompts with varying levels of detail to explain the task to GPT-4 and also evaluated the performance of the strategies against each other. These preliminary findings emphasize the potential of GPT-4 in social media data analysis, especially in identifying users' subtle intentions that may elude human detection."}, "https://arxiv.org/abs/2407.00347": {"title": "Resource Allocation and Secure Wireless Communication in the Large Model-based Mobile Edge Computing System", "link": "https://arxiv.org/abs/2407.00347", "description": "arXiv:2407.00347v1 Announce Type: cross \nAbstract: With the rapid advancement of large models and mobile edge computing, transfer learning, particularly through fine-tuning, has become crucial for adapting models to downstream tasks. Traditionally, this requires users to share their data with model owners for fine-tuning, which is not only costly but also raises significant privacy concerns. Furthermore, fine-tuning large-scale models is computationally intensive and often impractical for many users. To tackle these challenges, we introduce a system that combines offsite-tuning with physical-layer security, which provides local data owners with a lightweight adapter and a compressed emulator. Data owners then fine-tune the adapter locally and securely send it back to the model owners through a confidential channel for integration, ensuring privacy and resource conservation. Our paper focuses on optimizing computational resource allocation among data owners and the large model owner deployed on edge, and on the compression ratio of adapters. We incorporate a secrecy uplink channel to maximize the utility that we defined while minimizing system costs like energy consumption and delay. The optimization uses the Dinkelbach algorithm, fractional programming, successive convex approximation and alternating optimization. Experiments demonstrate our algorithm's superiority over existing methods."}, "https://arxiv.org/abs/2407.00940": {"title": "Unifying thermophotovoltaic performance metrics with technoeconomics", "link": "https://arxiv.org/abs/2407.00940", "description": "arXiv:2407.00940v1 Announce Type: cross \nAbstract: Thermophotovoltaics (TPV) are becoming a promising new heat engine with rapid recent gains in performance. Their performance is characterized by two metrics: efficiency and power density. As we bridge the gap between lab-scale and system-scale devices, we need to understand how each of these metrics impacts the technoeconomics of a TPV system. In this work, we develop a technoeconomic metric based on the levelized cost of electricity (LCOE) to understand how the metrics should be weighted relative to each other in terms of importance. We find that systems with high infrastructure and fuel costs should prioritize TPV efficiency, while systems where the TPV cell cost dominates should prioritize power density. We then evaluate how concrete cell improvements could improve the technoeconomics of five example systems, identifying the most impactful specific properties. Namely, improving spectral control with sub-bandgap reflectance is the most effective at reducing LCOE in systems with high infrastructure cost, while increasing view factor and reducing series resistance are most critical in systems with high TPV cell cost. Improving just 1-2 of these properties can reduce the LCOE by 30-50%. This study therefore helps researchers understand which performance metric is more important for their application and how to achieve high values of this performance metric."}, "https://arxiv.org/abs/2307.10279": {"title": "A conjecture on demographic mortality at high ages", "link": "https://arxiv.org/abs/2307.10279", "description": "arXiv:2307.10279v4 Announce Type: replace \nAbstract: The possibility of modeling and therefore predicting the trend of demographic mortality is of great scientific and social interest. The article presents and discusses the hypothesis that the demographic distribution of mortality in advanced ages converges asymptotically to an S-system distribution as lifespan increases. The statistical distribution of the S-system was introduced by the author in a 2022 paper and was derived by applying the methods of Fermi statistics to a cellular automaton acting as an \"arbitrary oscillator\". This distribution is here recalled and formalized analytically and its characteristics are described. The conjecture is based on two case studies: mortality in the United States from 1900 to 2017 and mortality in Italy from 1974 to 2019. The conjecture, applied to both case studies, appears reasonable. Tables and comparison figures are provided to support this. Finally, an attempt to predict demographic mortality behavior and limitations for the years to come is provided."}, "https://arxiv.org/abs/2401.08832": {"title": "From News Sharers to Post Viewers: How Topic Diversity and Conspiracy Theories Shape Engagement With Misinformation During a Health Crisis?", "link": "https://arxiv.org/abs/2401.08832", "description": "arXiv:2401.08832v2 Announce Type: replace \nAbstract: Engagement with misinformation on social media poses unprecedented threats to societal well-being, particularly during health crises when susceptibility to misinformation is heightened in a multi-topic context. This paper focuses on the COVID-19 pandemic and addresses a critical gap in understanding online engagement with multi-topic misinformation at two user levels: news sharers who share source news items on social media and post viewers who engage with online news posts. To this end, we conduct a comprehensive analysis of 7273 fact-checked source news claims related to COVID-19 and their associated posts on X, through the lens of topic diversity and conspiracy theories. We find that false news, particularly when accompanied by conspiracy theories, exhibits higher topic diversity than true news. At the news sharer level, false news has a longer lifetime and receives more posts on X than true news. Additionally, the integration of conspiracy theories is significantly associated with a longer lifetime for COVID-19 misinformation. However, topic diversity has no significant association with news sharer engagement in terms of news lifetime and the number of posts. At the post viewer level, contrary to the news sharer level, news posts characterized by heightened topic diversity receive more reposts, likes, and replies. Notably, post viewers tend to engage more with misinformation containing conspiracy narratives: false news posts that contain conspiracy theories, on average, receive 40.8% more reposts, 45.2% more likes, and 44.1% more replies compared to false news posts without conspiracy theories. Our findings suggest that news sharers and post viewers exhibit different engagement patterns on social media regarding topic diversity and conspiracy theories, offering valuable insights into designing targeted misinformation intervention strategies at both user levels."}, "https://arxiv.org/abs/2401.13054": {"title": "Frustrated Random Walks: A Fast Method to Compute Node Distances on Hypergraphs", "link": "https://arxiv.org/abs/2401.13054", "description": "arXiv:2401.13054v2 Announce Type: replace \nAbstract: A hypergraph is a generalization of a graph that arises naturally when attribute-sharing among entities is considered. Compared to graphs, hypergraphs have the distinct advantage that they contain explicit communities and are more convenient to manipulate. An open problem in hypergraph research is how to accurately and efficiently calculate node distances on hypergraphs. Estimating node distances enables us to find a node's nearest neighbors, which has important applications in such areas as recommender system, targeted ads, etc. In this paper, we propose using expected hitting times of random walks to compute hypergraph node distances. We note that simple random walks (SRW) cannot accurately compute node distances on highly complex real-world hypergraphs, which motivates us to introduce frustrated random walks (FRW) for this task. We further benchmark our method against DeepWalk, and show that while the latter can achieve comparable results, FRW has a distinct computational advantage in cases where the number of targets is fairly small. For such cases, we show that FRW runs in significantly shorter time than DeepWalk. Finally, we analyze the time complexity of our method, and show that for large and sparse hypergraphs, the complexity is approximately linear, rendering it superior to the DeepWalk alternative."}, "https://arxiv.org/abs/2402.03358": {"title": "A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation", "link": "https://arxiv.org/abs/2402.03358", "description": "arXiv:2402.03358v4 Announce Type: replace \nAbstract: Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction, or graph summarization, has gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at \\url{https://github.com/Emory-Melody/awesome-graph-reduction}. We hope this survey will bridge literature gaps and propel the advancement of this promising field."}, "https://arxiv.org/abs/2403.13450": {"title": "Complex Networks characterization of Indian Water Dam Systems and its topographic response", "link": "https://arxiv.org/abs/2403.13450", "description": "arXiv:2403.13450v2 Announce Type: replace \nAbstract: In this paper a Complex Network approach is taken to understand the salient features of Indian Water Dam Networks. Detailed analysis of 15 river basin networks have been carried out. The data has been taken from \"River Basin Atlas of India\" compiled by the Indian Space Research Organisation (ISRO) and Central Water Commission (CWC), Ministry of Water Resources, Government of India. The paper also investigates the correlation between various structural properties of the networks like total number of nodes, Link Density, Clustering Coefficient amongst each other and also with the Irrigation Potential and topographical features like the Elevation gradient of the region measured in meters. A mathematical model has also been proposed to understand the relation between irrigation potential measured in thousand hectares unit with the number of nodes, i.e. dams and barrages, to get a more quantitative understanding of the system. The paper also tries to observe the response of the network properties to actual topographical features of the region. This lays down a basic foundational work in understanding these water dam networks through a complex network approach over which further work can be done to make the predictions more efficient."}, "https://arxiv.org/abs/2404.02205": {"title": "A Holistic Indicator of Polarization to Measure Online Sexism", "link": "https://arxiv.org/abs/2404.02205", "description": "arXiv:2404.02205v2 Announce Type: replace \nAbstract: The online trend of the manosphere and feminist discourse on social networks requires a holistic measure of the level of sexism in an online community. This indicator is important for policymakers and moderators of online communities (e.g., subreddits) and computational social scientists, either to revise moderation strategies based on the degree of sexism or to match and compare the temporal sexism across different platforms and communities with real-time events and infer social scientific insights.\n  In this paper, we build a model that can provide a comparable holistic indicator of toxicity targeted toward male and female identity and male and female individuals. Despite previous supervised NLP methods that require annotation of toxic comments at the target level (e.g. annotating comments that are specifically toxic toward women) to detect targeted toxic comments, our indicator uses supervised NLP to detect the presence of toxicity and unsupervised word embedding association test to detect the target automatically.\n  We apply our model to gender discourse communities (e.g., r/TheRedPill, r/MGTOW, r/FemaleDatingStrategy) to detect the level of toxicity toward genders (i.e., sexism). Our results show that our framework accurately and consistently (93% correlation) measures the level of sexism in a community. We finally discuss how our framework can be generalized in the future to measure qualities other than toxicity (e.g. sentiment, humor) toward general-purpose targets and turn into an indicator of different sorts of polarizations."}, "https://arxiv.org/abs/2304.07203": {"title": "On the convergence of nonlinear averaging dynamics with three-body interactions on hypergraphs", "link": "https://arxiv.org/abs/2304.07203", "description": "arXiv:2304.07203v2 Announce Type: replace-cross \nAbstract: Complex networked systems in fields such as physics, biology, and social sciences often involve interactions that extend beyond simple pairwise ones. Hypergraphs serve as powerful modeling tools for describing and analyzing the intricate behaviors of systems with multi-body interactions. Herein, we investigate a discrete-time nonlinear averaging dynamics with three-body interactions: an underlying hypergraph, comprising triples as hyperedges, delineates the structure of these interactions, while the vertices update their states through a weighted, state-dependent average of neighboring pairs' states. This dynamics captures reinforcing group effects, such as peer pressure, and exhibits higher-order dynamical effects resulting from a complex interplay between initial states, hypergraph topology, and nonlinearity of the update. Differently from linear averaging dynamics on graphs with two-body interactions, this model does not converge to the average of the initial states but rather induces a shift. By assuming random initial states and by making some regularity and density assumptions on the hypergraph, we prove that the dynamics converges to a multiplicatively-shifted average of the initial states, with high probability. We further characterize the shift as a function of two parameters describing the initial state and interaction strength, as well as the convergence time as a function of the hypergraph structure."}, "https://arxiv.org/abs/2308.09790": {"title": "A Two-Part Machine Learning Approach to Characterizing Network Interference in A/B Testing", "link": "https://arxiv.org/abs/2308.09790", "description": "arXiv:2308.09790v2 Announce Type: replace-cross \nAbstract: The reliability of controlled experiments, commonly referred to as \"A/B tests,\" is often compromised by network interference, where the outcomes of individual units are influenced by interactions with others. Significant challenges in this domain include the lack of accounting for complex social network structures and the difficulty in suitably characterizing network interference. To address these challenges, we propose a machine learning-based method. We introduce \"causal network motifs\" and utilize transparent machine learning models to characterize network interference patterns underlying an A/B test on networks. Our method's performance has been demonstrated through simulations on both a synthetic experiment and a large-scale test on Instagram. Our experiments show that our approach outperforms conventional methods such as design-based cluster randomization and conventional analysis-based neighborhood exposure mapping. Our approach provides a comprehensive and automated solution to address network interference for A/B testing practitioners. This aids in informing strategic business decisions in areas such as marketing effectiveness and product customization."}, "https://arxiv.org/abs/2310.05070": {"title": "CO-ASnet :A Smart Contract Architecture Design based on Blockchain Technology with Active Sensor Networks", "link": "https://arxiv.org/abs/2310.05070", "description": "arXiv:2310.05070v2 Announce Type: replace-cross \nAbstract: The influence of opinion leaders impacts different aspects of social finance. How to analyse the utility of opinion leaders' influence in realizing assets on the blockchain and adopt a compliant regulatory scheme is worth exploring and pondering. Taking Musk's call on social media to buy Dogecoin as an example, this paper uses an event study to empirically investigate the phenomenon in which opinion leaders use ICOs (initial coin offerings) to exert influence. The results show that opinion leaders can use ICOs to influence the price of token assets with money and data traffic in their social network. They can obtain excess returns and reduce the cost of realization so that the closed loop of influence realization will be accelerated. Based on this phenomenon and the results of its impact, we use the ChainLink Oracle with Active Sensor Networks(CO-ASnet) to design a safe and applicable decentralized regulatory scheme that can constructively provide risk assessment strategies and early warning measures for token issuance. The influence realization of opinion leaders in blockchain issuance is bound to receive widespread attention, and this paper will provide an exemplary reference for regulators and enterprises to explore the boundaries of blockchain financial product development and governance."}, "https://arxiv.org/abs/2404.01216": {"title": "Novel Node Category Detection Under Subpopulation Shift", "link": "https://arxiv.org/abs/2404.01216", "description": "arXiv:2404.01216v2 Announce Type: replace-cross \nAbstract: In real-world graph data, distribution shifts can manifest in various ways, such as the emergence of new categories and changes in the relative proportions of existing categories. It is often important to detect nodes of novel categories under such distribution shifts for safety or insight discovery purposes. We introduce a new approach, Recall-Constrained Optimization with Selective Link Prediction (RECO-SLIP), to detect nodes belonging to novel categories in attributed graphs under subpopulation shifts. By integrating a recall-constrained learning framework with a sample-efficient link prediction mechanism, RECO-SLIP addresses the dual challenges of resilience against subpopulation shifts and the effective exploitation of graph structure. Our extensive empirical evaluation across multiple graph datasets demonstrates the superior performance of RECO-SLIP over existing methods. The experimental code is available at https://github.com/hsinghuan/novel-node-category-detection."}, "https://arxiv.org/abs/2407.01799": {"title": "Disentangling individual-level from location-based income uncovers socioeconomic preferential mobility and impacts segregation estimates", "link": "https://arxiv.org/abs/2407.01799", "description": "arXiv:2407.01799v1 Announce Type: new \nAbstract: Segregation encodes information about society, such as social cohesion, mixing, and inequality. However, most past and current studies tackled socioeconomic (SE) segregation by analyzing static aggregated mobility networks, often without considering further individual features beyond income and, most importantly, without distinguishing individual-level from location-based income. Accessing individual-level income may help mapping macroscopic behavior into more granular mobility patterns, hence impacting segregation estimates. Here we combine a mobile phone dataset of daily mobility flows across Spanish districts stratified and adjusted by age, gender and income with census data of districts median income. We build mobility-based SE assortativity matrices for multiple demographics and observe mobility patterns of three income groups with respect to location-based SE classes. We find that SE assortativity differs when isolating the mobility of specific income groups: we observe that groups prefer to visit areas with higher average income than their own, which we call preferential mobility. Our analysis suggests substantial differences between weekdays and weekends SE assortativity by age class, with weekends characterized by higher SE assortativity. Our modeling approach shows that the radiation model, which typically performs best at reproducing inter-municipal population mobility, best fits middle income and middle-aged flows, while performing worse on young and low income groups. Our double-sided approach, focusing on assortativity patterns and mobility modeling, suggests that state of the art mobility models fail at capturing preferential mobility behavior. Overall, our work indicates that mobility models considering the interplay of SE preferential behavior, age and gender gaps may sensibly improve the state of the art models performance."}, "https://arxiv.org/abs/2407.01820": {"title": "Exploring the Role of Randomization on Belief Rigidity in Online Social Networks", "link": "https://arxiv.org/abs/2407.01820", "description": "arXiv:2407.01820v1 Announce Type: new \nAbstract: People often stick to their existing beliefs, ignoring contradicting evidence or only interacting with those who reinforce their views. Social media platforms often facilitate such tendencies of homophily and echo-chambers as they promote highly personalized content to maximize user engagement. However, increased belief rigidity can negatively affect real-world policy decisions such as leading to climate change inaction and increased vaccine hesitancy. To understand and effectively tackle belief rigidity on online social networks, designing and evaluating various intervention strategies is crucial, and increasing randomization in the network can be considered one such intervention. In this paper, we empirically quantify the effects of a randomized social network structure on belief rigidity, specifically examining the potential benefits of introducing randomness into the network. We show that individuals' beliefs are positively influenced by peer opinions, regardless of whether those opinions are similar to or differ from their own by passively sensing belief rigidity through our experimental framework. Moreover, people incorporate a slightly higher variety of different peers (based on their opinions) into their networks when the recommendation algorithm provides them with diverse content, compared to when it provides them with similar content. Our results indicate that in some cases, there might be benefits to randomization, providing empirical evidence that a more randomized network could be a feasible way of helping people get out of their echo-chambers. Our findings have broader implications in computing and platform design of social media, and can help combat overly rigid beliefs in online social networks."}, "https://arxiv.org/abs/2407.02074": {"title": "CGAP: Urban Region Representation Learning with Coarsened Graph Attention Pooling", "link": "https://arxiv.org/abs/2407.02074", "description": "arXiv:2407.02074v1 Announce Type: new \nAbstract: The explosion of massive urban data recently has provided us with a valuable opportunity to gain deeper insights into urban regions and the daily lives of residents. Urban region representation learning emerges as a crucial realm for fulfilling this task. Among deep learning approaches, graph neural networks (GNNs) have shown promise, given that city elements can be naturally represented as nodes with various connections between them as edges. However, many existing GNN approaches encounter challenges such as over-smoothing and limitations in capturing information from nodes in other regions, resulting in the loss of crucial urban information and a decline in region representation performance. To address these challenges, we leverage urban graph structure information and introduce a hierarchical graph pooling process called Coarsened Graph Attention Pooling (CGAP). CGAP features local attention units to create coarsened intermediate graphs and global features. Additionally, by incorporating urban region graphs and global features into a global attention layer, we harness relational information to enhance representation effectiveness. Furthermore, CGAP integrates region attributes such as Points of Interest (POIs) and inter-regional contexts like human mobility, enabling the exploitation of multi-modal urban data for more comprehensive representation learning. Experiments on three downstream tasks related to the UN Sustainable Development Goals validate the effectiveness of region representations learned by our approach. Experimental results and analyses demonstrate that CGAP excels in various socioeconomic prediction tasks compared to competitive baselines."}, "https://arxiv.org/abs/2407.02290": {"title": "A systematic comparison of measures for k-anonymity in networks", "link": "https://arxiv.org/abs/2407.02290", "description": "arXiv:2407.02290v1 Announce Type: new \nAbstract: Privacy-aware sharing of network data is a difficult task due to the interconnectedness of individuals in networks. An important part of this problem is the inherently difficult question of how in a particular situation the privacy of an individual node should be measured. To that end, in this paper we propose a set of aspects that one should consider when choosing a measure for privacy. These aspects include the type of desired privacy and attacker scenario against which the measure protects, utility of the data, the type of desired output, and the computational complexity of the chosen measure. Based on these aspects, we provide a systematic overview of existing approaches in the literature. We then focus on a set of measures that ultimately enables our objective: sharing the anonymized full network dataset with limited disclosure risk. The considered measures, each based on the concept of k-anonymity, account for the structure of the surroundings of a certain node and differ in completeness and reach of the structural information taken into account. We present a comprehensive theoretical characterization as well as comparative empirical experiments on a wide range of real-world network datasets with up to millions of edges. We find that the choice of the measure has an enormous effect on aforementioned aspects. Most interestingly, we find that the most effective measures consider a greater node vicinity, yet utilize minimal structural information and thus use minimal computational resources. This finding has important implications for researchers and practitioners, who may, based on the recommendations given in this paper, make an informed choice on how to safely share large-scale network data in a privacy-aware manner."}, "https://arxiv.org/abs/2407.01788": {"title": "Impact of the Network Size and Frequency of Information Receipt on Polarization in Social Networks", "link": "https://arxiv.org/abs/2407.01788", "description": "arXiv:2407.01788v1 Announce Type: cross \nAbstract: Opinion Dynamics is an interdisciplinary area of research. Psychology and Sociology have proposed models of how individuals form opinions and how social interactions influence this process. Socio-Physicists have interpreted patterns in opinion formation as arising from non-linearity in the underlying process, shaping the models. Agent-based modeling has offered a platform to study the Opinion Dynamics of large groups. This paper recasts recent models in opinion formation into a proper dynamical system, injecting the idea of clock time into evolving opinions. The time interval between successive receipts of new information (frequency of information receipts) becomes a factor to study. Social media has shrunk time intervals between information receipts, increasing their frequency. The recast models show that shorter intervals and larger networks increase an individual's propensity for polarization, defined as an inability to hold a neutral opinion. A Polarization number based on sociological parameters is proposed, with critical values beyond which individuals are prone to polarization, depending on psychological parameters. Reduced time intervals and larger interacting groups can push the Polarization number to critical values, contributing to polarization. The Extent of Polarization is defined as the width of the region around neutral within which an individual cannot hold an opinion. Results are reported for model parameters found in the literature. The findings offer an opportunity to adjust model parameters to align with empirical evidence, aiding the study of Opinion Dynamics in large social networks using Agent-Based Modeling."}, "https://arxiv.org/abs/2407.01852": {"title": "Early-Career Researchers' Perspective on Future Colliders", "link": "https://arxiv.org/abs/2407.01852", "description": "arXiv:2407.01852v1 Announce Type: cross \nAbstract: Since its inception, the Large Hadron Collider (LHC) has significantly advanced particle physics and will continue to do so in the context of the High Luminosity LHC (HL-LHC) program to collect $3000$ fb$^{-1}$ by the end of 2041. The particle physics community worldwide is discussing which future collider could follow in the footsteps of the LHC and uncover yet inaccessible phenomena.\n  To foster the discussion on this important topic among the young particle physicist community, the Early-Career Researchers (ECR) panel of the European Committee for Future Colliders (ECFA) has organized the Future Colliders for Early-Career Researchers workshop at CERN in September 2023. This document aims to summarise this event and present the ECR perspective, outline the key questions that came up during the discussions, and explore how ECRs can influence the decision process of future colliders community and beyond."}, "https://arxiv.org/abs/2407.02018": {"title": "A Proposal for a FAIR Management of 3D Data in Cultural Heritage: The Aldrovandi Digital Twin Case", "link": "https://arxiv.org/abs/2407.02018", "description": "arXiv:2407.02018v1 Announce Type: cross \nAbstract: In this article we analyse 3D models of cultural heritage with the aim of answering three main questions: what processes can be put in place to create a FAIR-by-design digital twin of a temporary exhibition? What are the main challenges in applying FAIR principles to 3D data in cultural heritage studies and how are they different from other types of data (e.g. images) from a data management perspective? We begin with a comprehensive literature review touching on: FAIR principles applied to cultural heritage data; representation models; both Object Provenance Information (OPI) and Metadata Record Provenance Information (MRPI), respectively meant as, on the one hand, the detailed history and origin of an object, and - on the other hand - the detailed history and origin of the metadata itself, which describes the primary object (whether physical or digital); 3D models as cultural heritage research data and their creation, selection, publication, archival and preservation. We then describe the process of creating the Aldrovandi Digital Twin, by collecting, storing and modelling data about cultural heritage objects and processes. We detail the many steps from the acquisition of the Digital Cultural Heritage Objects (DCHO), through to the upload of the optimised DCHO onto a web-based framework (ATON), with a focus on open technologies and standards for interoperability and preservation. Using the FAIR Principles for Heritage Library, Archive and Museum Collections as a framework, we look in detail at how the Digital Twin implements FAIR principles at the object and metadata level. We then describe the main challenges we encountered and we summarise what seem to be the peculiarities of 3D cultural heritage data and the possible directions for further research in this field."}, "https://arxiv.org/abs/2407.02057": {"title": "HC-GLAD: Dual Hyperbolic Contrastive Learning for Unsupervised Graph-Level Anomaly Detection", "link": "https://arxiv.org/abs/2407.02057", "description": "arXiv:2407.02057v1 Announce Type: cross \nAbstract: Unsupervised graph-level anomaly detection (UGAD) has garnered increasing attention in recent years due to its significance. However, most existing methods only rely on traditional graph neural networks to explore pairwise relationships but such kind of pairwise edges are not enough to describe multifaceted relationships involving anomaly. There is an emergency need to exploit node group information which plays a crucial role in UGAD. In addition, most previous works ignore the global underlying properties (e.g., hierarchy and power-law structure) which are common in real-world graph datasets and therefore are indispensable factors on UGAD task. In this paper, we propose a novel Dual Hyperbolic Contrastive Learning for Unsupervised Graph-Level Anomaly Detection (HC-GLAD in short). To exploit node group connections, we construct hypergraphs based on gold motifs and subsequently perform hypergraph convolution. Furthermore, to preserve the hierarchy of real-world graphs, we introduce hyperbolic geometry into this field and conduct both graph and hypergraph embedding learning in hyperbolic space with hyperboloid model. To the best of our knowledge, this is the first work to simultaneously apply hypergraph with node group connections and hyperbolic geometry into this field. Extensive experiments on several real world datasets of different fields demonstrate the superiority of HC-GLAD on UGAD task. The code is available at https://github.com/Yali-F/HC-GLAD."}, "https://arxiv.org/abs/2407.02143": {"title": "Counterfactual Data Augmentation with Denoising Diffusion for Graph Anomaly Detection", "link": "https://arxiv.org/abs/2407.02143", "description": "arXiv:2407.02143v1 Announce Type: cross \nAbstract: A critical aspect of Graph Neural Networks (GNNs) is to enhance the node representations by aggregating node neighborhood information. However, when detecting anomalies, the representations of abnormal nodes are prone to be averaged by normal neighbors, making the learned anomaly representations less distinguishable. To tackle this issue, we propose CAGAD -- an unsupervised Counterfactual data Augmentation method for Graph Anomaly Detection -- which introduces a graph pointer neural network as the heterophilic node detector to identify potential anomalies whose neighborhoods are normal-node-dominant. For each identified potential anomaly, we design a graph-specific diffusion model to translate a part of its neighbors, which are probably normal, into anomalous ones. At last, we involve these translated neighbors in GNN neighborhood aggregation to produce counterfactual representations of anomalies. Through aggregating the translated anomalous neighbors, counterfactual representations become more distinguishable and further advocate detection performance. The experimental results on four datasets demonstrate that CAGAD significantly outperforms strong baselines, with an average improvement of 2.35% on F1, 2.53% on AUC-ROC, and 2.79% on AUC-PR."}, "https://arxiv.org/abs/2407.02396": {"title": "A refractory density approach to a multi-scale SEIRS epidemic model", "link": "https://arxiv.org/abs/2407.02396", "description": "arXiv:2407.02396v1 Announce Type: cross \nAbstract: We propose a novel multi-scale modeling framework for infectious disease spreading, borrowing ideas and modeling tools from the so-called Refractory Density (RD) approach. We introduce a microscopic model that describes the probability of infection for a single individual and the evolution of the disease within their body. From the individual-level description, we then present the corresponding population-level model of epidemic spreading on the mesoscopic and macroscopic scale. We conclude with numerical illustrations taking into account either a white Gaussian noise or an escape noise to showcase the potential of our approach in producing both transient and asymptotic complex dynamics as well as finite-size fluctuations consistently across multiple scales. A comparison with the epidemiology of coronaviruses is also given to corroborate the qualitative relevance of our new approach."}, "https://arxiv.org/abs/2303.08424": {"title": "Emergence of economic and social disparities through competitive gift-giving", "link": "https://arxiv.org/abs/2303.08424", "description": "arXiv:2303.08424v2 Announce Type: replace \nAbstract: Several tiers of social organization with varying economic and social disparities have been observed. However, a quantitative characterization of the types and the causal mechanisms for the transitions have hardly been explained. While anthropologists have emphasized that gift exchange, rather than market exchange, prevails in traditional societies and shapes social relations, few mathematical studies have explored its consequences for social organizations. In this study, we present a simple model of competitive gift-giving that describes how gifts bring goods to the recipient and honor to the donor, and simulate social change. Numerical simulations and an analysis of the corresponding mean-field theory demonstrate the transitions between the following four phases with different distribution shapes of wealth and social reputation: the band, without economic or social disparities; the tribe, with economic but without social disparities; the chiefdom, with both; and the kingdom, with economic disparity and weak social disparity except for an outlier, namely, the ``monarch''. The emergence of strong disparities is characterized by power law distributions and is attributed to the ``rich get richer'' process. In contrast, the absence of such a process leads to exponential distributions due to random fluctuations. The phases depend on the parameters characterizing the frequency and scale of gift interactions. Our findings provide quantitative criteria for classifying social organizations based on economic and social disparities, consistent with anthropological theory and empirical observations. Thus, we propose empirically measurable explanatory variables and characteristics for the evolution of social organizations. The constructive model, guided by social scientific theory, can provide the basic mechanistic explanation of social evolution and integrate theories of the social sciences."}, "https://arxiv.org/abs/2309.15176": {"title": "Robust Stance Detection: Understanding Public Perceptions in Social Media", "link": "https://arxiv.org/abs/2309.15176", "description": "arXiv:2309.15176v2 Announce Type: replace-cross \nAbstract: The abundance of social media data has presented opportunities for accurately determining public and group-specific stances around policy proposals or controversial topics. In contrast with sentiment analysis which focuses on identifying prevailing emotions, stance detection identifies precise positions (i.e., supportive, opposing, neutral) relative to a well-defined topic, such as perceptions toward specific global health interventions during the COVID-19 pandemic. Traditional stance detection models, while effective within their specific domain (e.g., attitudes towards masking protocols during COVID-19), often lag in performance when applied to new domains and topics due to changes in data distribution. This limitation is compounded by the scarcity of domain-specific, labeled datasets, which are expensive and labor-intensive to create. A solution we present in this paper combines counterfactual data augmentation with contrastive learning to enhance the robustness of stance detection across domains and topics of interest. We evaluate the performance of current state-of-the-art stance detection models, including a prompt-optimized large language model, relative to our proposed framework succinctly called STANCE-C3 (domain-adaptive Cross-target STANCE detection via Contrastive learning and Counterfactual generation). Empirical evaluations demonstrate STANCE-C3's consistent improvements over the baseline models with respect to accuracy across domains and varying focal topics. Despite the increasing prevalence of general-purpose models such as generative AI, specialized models such as STANCE-C3 provide utility in safety-critical domains wherein precision is highly valued, especially when a nuanced understanding of the concerns of different population segments could result in crafting more impactful public policies."}, "https://arxiv.org/abs/2310.05212": {"title": "Semiotics Networks Representing Perceptual Inference", "link": "https://arxiv.org/abs/2310.05212", "description": "arXiv:2310.05212v4 Announce Type: replace-cross \nAbstract: Every day, humans perceive objects and communicate these perceptions through various channels. In this paper, we present a computational model designed to track and simulate the perception of objects, as well as their representations as conveyed in communication.\n  We delineate two fundamental components of our internal representation, termed \"observed\" and \"seen\", which we correlate with established concepts in computer vision, namely encoding and decoding. These components are integrated into semiotic networks, which simulate perceptual inference of object perception and human communication.\n  Our model of object perception by a person allows us to define object perception by {\\em a network}. We demonstrate this with an example of an image baseline classifier by constructing a new network that includes the baseline classifier and an additional layer. This layer produces the images \"perceived\" by the entire network, transforming it into a perceptualized image classifier. This facilitates visualization of the acquired network.\n  Within our network, the image representations become more efficient for classification tasks when they are assembled and randomized. In our experiments, the perceptualized network outperformed the baseline classifier on MNIST training databases consisting of a restricted number of images.\n  Our model is not limited to persons and can be applied to any system featuring a loop involving the processing from \"internal\" to \"external\" representations."}, "https://arxiv.org/abs/2407.02548": {"title": "Speed-accuracy tradeoff and its effect in the game of cricket: predictive modeling from statistical mechanics perspective", "link": "https://arxiv.org/abs/2407.02548", "description": "arXiv:2407.02548v1 Announce Type: new \nAbstract: The speed-accuracy tradeoffs are prevalent in a wide range of physical systems. In this paper, we demonstrate speed-accuracy tradeoffs in the game of cricket, where 'batters' score runs on the balls bowled by the 'bowlers'. It is shown that the run scoring rate by a batter and the probability of dismissal follow a power-law relation. Due to availability of extensive data, game of cricket is an excellent model for the study of the effect of speed-accuracy tradeoff on the overall performance of the system. It is shown that the exponent of the power-law governs the nature of the adaptability of the player in different conditions and can be used for their assessment. Further, it is demonstrated that the players with extreme values of the power-law exponent are better suited for different playing conditions as compared to the ones with moderate values. These findings can be utilized to identify the potential of the cricket players for different game formats and can further help team management in devising strategies for the best outcomes with a given set of players."}, "https://arxiv.org/abs/2407.02603": {"title": "Heider balance on Archimedean lattices", "link": "https://arxiv.org/abs/2407.02603", "description": "arXiv:2407.02603v1 Announce Type: new \nAbstract: The phenomenon of Heider (structural) balance is known for a long time (P. Bonacich and P. Lu, Introduction to Mathematical Sociology, Princeton UP, 2012). Yet it attracts attention of numerous computational scholars, as it is an example of a macroscopic ordering which emerges as a consequence of local interactions. In this paper, we investigate the thermal evolution (driven by thermal noise level $T$) of the work function $U(T)$ for Heider balance on several Archimedean lattices that contain separated triangles, pairs of triangles, chains of triangles and complex structures of triangles. To that end, the heat-bath algorithm is applied. Two schemes of link values updating are considered: synchronous and asynchronous. In the latter case, the analytical formula $U(T)=-\\tanh(1/T)$ based on the partition function is provided. The Archimedean lattices are encoded with adjacency matrices, and Fortran procedures for their construction are provided. Finally, we present the mathematical proof that for any two-dimensional lattice, perfect structural (Heider) balance is unreachable at $T>0$."}, "https://arxiv.org/abs/2407.02612": {"title": "Women for Quantum -- Manifesto of Values", "link": "https://arxiv.org/abs/2407.02612", "description": "arXiv:2407.02612v1 Announce Type: new \nAbstract: Data show that the presence of women in quantum science is affected by a number of detriments and their percentage decreases even further for higher positions. Beyond data, from our shared personal experiences as female tenured quantum physics professors, we believe that the current model of scientific leadership, funding, and authority fails to represent many of us. It is time for a real change that calls for a different kind of force and for the participation of everyone. Women for quantum calls for a joint effort and aims with this initiative to contribute to such a transformation."}, "https://arxiv.org/abs/2407.02662": {"title": "Supporters and Skeptics: LLM-based Analysis of Engagement with Mental Health (Mis)Information Content on Video-sharing Platforms", "link": "https://arxiv.org/abs/2407.02662", "description": "arXiv:2407.02662v1 Announce Type: new \nAbstract: Over one in five adults in the US lives with a mental illness. In the face of a shortage of mental health professionals and offline resources, online short-form video content has grown to serve as a crucial conduit for disseminating mental health help and resources. However, the ease of content creation and access also contributes to the spread of misinformation, posing risks to accurate diagnosis and treatment. Detecting and understanding engagement with such content is crucial to mitigating their harmful effects on public health. We perform the first quantitative study of the phenomenon using YouTube Shorts and Bitchute as the sites of study. We contribute MentalMisinfo, a novel labeled mental health misinformation (MHMisinfo) dataset of 739 videos (639 from Youtube and 100 from Bitchute) and 135372 comments in total, using an expert-driven annotation schema. We first found that few-shot in-context learning with large language models (LLMs) are effective in detecting MHMisinfo videos. Next, we discover distinct and potentially alarming linguistic patterns in how audiences engage with MHMisinfo videos through commentary on both video-sharing platforms. Across the two platforms, comments could exacerbate prevailing stigma with some groups showing heightened susceptibility to and alignment with MHMisinfo. We discuss technical and public health-driven adaptive solutions to tackling the \"epidemic\" of mental health misinformation online."}, "https://arxiv.org/abs/2407.02807": {"title": "Regional and Temporal Patterns of Partisan Polarization during the COVID-19 Pandemic in the United States and Canada", "link": "https://arxiv.org/abs/2407.02807", "description": "arXiv:2407.02807v1 Announce Type: new \nAbstract: Public health measures were among the most polarizing topics debated online during the COVID-19 pandemic. Much of the discussion surrounded specific events, such as when and which particular interventions came into practise. In this work, we develop and apply an approach to measure subnational and event-driven variation of partisan polarization and explore how these dynamics varied both across and within countries. We apply our measure to a dataset of over 50 million tweets posted during late 2020, a salient period of polarizing discourse in the early phase of the pandemic. In particular, we examine regional variations in both the United States and Canada, focusing on three specific health interventions: lockdowns, masks, and vaccines. We find that more politically conservative regions had higher levels of partisan polarization in both countries, especially in the US where a strong negative correlation exists between regional vaccination rates and degree of polarization in vaccine related discussions. We then analyze the timing, context, and profile of spikes in polarization, linking them to specific events discussed on social media across different regions in both countries. These typically last only a few days in duration, suggesting that online discussions reflect and could even drive changes in public opinion, which in the context of pandemic response impacts public health outcomes across different regions and over time."}, "https://arxiv.org/abs/2407.02980": {"title": "Modelling the mitigation of anti-vaccine opinion propagation to suppress epidemic spread: A computational approach", "link": "https://arxiv.org/abs/2407.02980", "description": "arXiv:2407.02980v1 Announce Type: new \nAbstract: Information regarding vaccines from sources such as health services, media, and social networks can significantly shape vaccination decisions. In particular, the dissemination of negative information can contribute to vaccine hesitancy, thereby exacerbating infectious disease outbreaks. This study investigates strategies to mitigate anti-vaccine social contagion through effective counter-campaigns that disseminate positive vaccine information and encourage vaccine uptake, aiming to reduce the size of epidemics. In a coupled agent-based model that consists of opinion and disease diffusion processes, we explore and compare different heuristics to design positive campaigns based on the network structure and local presence of negative vaccine attitudes. We examine two campaigning regimes: a static regime with a fixed set of targets, and a dynamic regime in which targets can be updated over time. We demonstrate that strategic targeting and engagement with the dynamics of anti-vaccine influence diffusion in the network can effectively mitigate the spread of anti-vaccine sentiment, thereby reducing the epidemic size. However, the effectiveness of the campaigns differs across different targeting strategies and is impacted by a range of factors. We find that the primary advantage of static campaigns lies in their capacity to act as an obstacle, preventing the clustering of emerging anti-vaccine communities, thereby resulting in smaller and unconnected anti-vaccine groups. On the other hand, dynamic campaigns reach a broader segment of the population and adapt to the evolution of anti-vaccine diffusion, not only protecting susceptible agents from negative influence but also fostering positive propagation within negative regions."}, "https://arxiv.org/abs/2407.03074": {"title": "Dynamics of An Information Theoretic Analog of Two Masses on a Spring", "link": "https://arxiv.org/abs/2407.03074", "description": "arXiv:2407.03074v1 Announce Type: new \nAbstract: In this letter we investigate an information theoretic analogue of the classic two masses on spring system, arising from a physical interpretation of Friston's free energy principle in the theory of learning in a system of agents. Using methods from classical mechanics on manifolds, we define a kinetic energy term using the Fisher metric on distributions and a potential energy function defined in terms of stress on the agents' beliefs. The resulting Lagrangian (Hamiltonian) produces a variation of the classic DeGroot dynamics. In the two agent case, the potential function is defined using the Jeffrey's divergence and the resulting dynamics are characterized by a non-linear spring. These dynamics produce trajectories that resemble flows on tori but are shown numerically to produce chaos near the boundary of the space. We then investigate persuasion as an information theoretic control problem where analysis indicates that manipulating peer pressure with a fixed target is a more stable approach to altering an agent's belief than providing a slowly changing belief state that approaches the target."}, "https://arxiv.org/abs/2407.03117": {"title": "A 72h exploration of the co-evolution of food insecurity and international migration", "link": "https://arxiv.org/abs/2407.03117", "description": "arXiv:2407.03117v1 Announce Type: new \nAbstract: Food insecurity, defined as the lack of physical or economic access to safe, nutritious and sufficient food, remains one of the main challenges of the 2030 Agenda for Sustainable Development. Food insecurity is a complex phenomenon, resulting from the interplay of environmental, socio-demographic, and political events. Previous work has investigated the nexus between climate change, conflict, migration and food security at the household level, however these relations are still largely unexplored at national scales. In this context, during the Complexity72h workshop, held at the Universidad Carlos III de Madrid in June 2024, we explored the co-evolution of international migration flows and food insecurity at the national scale, accounting for remittances, as well as for changes in the economic, conflict, and climate situation. To this aim, we gathered data from several publicly available sources (Food and Agriculture Organization, World Bank, and UN Department of Economic and Social Affairs) and analyzed the association between food insecurity and migration, migration and remittances, and remittances and food insecurity. We then propose a framework linking together these associations to model the co-evolution of food insecurity and international migrations."}, "https://arxiv.org/abs/2407.03159": {"title": "Protection Degree and Migration in the Stochastic SIRS Model: A Queueing System Perspective", "link": "https://arxiv.org/abs/2407.03159", "description": "arXiv:2407.03159v1 Announce Type: new \nAbstract: With the prevalence of COVID-19, the modeling of epidemic propagation and its analyses have played a significant role in controlling epidemics. However, individual behaviors, in particular the self-protection and migration, which have a strong influence on epidemic propagation, were always neglected in previous studies. In this paper, we mainly propose two models from the individual and population perspectives. In the first individual model, we introduce the individual protection degree that effectively suppresses the epidemic level as a stochastic variable to the SIRS model. In the alternative population model, an open Markov queueing network is constructed to investigate the individual number of each epidemic state, and we present an evolving population network via the migration of people. Besides, stochastic methods are applied to analyze both models. In various simulations, the infected probability, the number of individuals in each state and its limited distribution are demonstrated."}, "https://arxiv.org/abs/2407.03255": {"title": "How Similar Are Elected Politicians and Their Constituents? Quantitative Evidence From Online Social Network", "link": "https://arxiv.org/abs/2407.03255", "description": "arXiv:2407.03255v1 Announce Type: new \nAbstract: How similar are politicians to those who vote for them? This is a critical question at the heart of democratic representation and particularly relevant at times when political dissatisfaction and populism are on the rise. To answer this question we compare the online discourse of elected politicians and their constituents. We collect a two and a half years (September 2020 - February 2023) constituency-level dataset for USA and UK that includes: (i) the Twitter timelines (5.6 Million tweets) of elected political representatives (595 UK Members of Parliament and 433 USA Representatives), (ii) the Nextdoor posts (21.8 Million posts) of the constituency (98.4% USA and 91.5% UK constituencies). We find that elected politicians tend to be equally similar to their constituents in terms of content and style regardless of whether a constituency elects a right or left-wing politician. The size of the electoral victory and the level of income of a constituency shows a nuanced picture. The narrower the electoral victory, the more similar the style and the more dissimilar the content is. The lower the income of a constituency, the more similar the content is. In terms of style, poorer constituencies tend to have a more similar sentiment and more dissimilar psychological text traits (i.e. measured with LIWC categories)."}, "https://arxiv.org/abs/2407.02624": {"title": "Optimizing Information Access in Networks via Edge Augmentation", "link": "https://arxiv.org/abs/2407.02624", "description": "arXiv:2407.02624v1 Announce Type: cross \nAbstract: Given a graph $G = (V, E)$ and a model of information flow on that network, a fundamental question is to understand if all the nodes have sufficient access to information generated at other nodes in the graph. If not, we can ask if a small set of edge additions improve information access. Formally, the broadcast value of a network is defined to be the minimum over pairs $u,v \\in V$ of the probability that an information cascade starting at $u$ reaches $v$. Recent work in the algorithmic fairness literature has focused on heuristics for adding a few edges to a graph to improve its broadcast. Our goal is to formally study the approximability of the Broadcast Improvement problem: given $G$ and a parameter $k$, find the best set of $k$ edges to add to $G$ in order to maximize the broadcast value of the resulting graph.\n  We develop efficient bicriteria approximation algorithms. If the optimal solution adds $k$ edges and achieves a broadcast of $\\beta^*$, we develop algorithms that can (a) add $2k-1$ edges and achieve a broadcast value roughly $(\\beta^*)^4$, or (b) add $O(k\\log n)$ edges and achieve a broadcast roughly $\\beta^*$. We also provide other trade-offs, that can be better depending on $k$ and the parameter associated with propagation in the cascade model. We complement our results by proving that unless P = NP, any algorithm that adds $O(k)$ edges must lose significantly in the approximation of $\\beta^*$, resolving an open question.\n  Our techniques are inspired by connections between Broadcast Improvement and problems such as Metric $k$-Center and Diameter Reduction. However, since the objective involves information cascades, we need to develop novel probabilistic tools to reason about the existence of paths in edge-sampled graphs. Finally, we show that our techniques extend to a single-source variant, for which we show both bicriteria algorithms and inapproximability results."}, "https://arxiv.org/abs/2407.02710": {"title": "WARNING This Contains Misinformation: The Effect of Cognitive Factors, Beliefs, and Personality on Misinformation Warning Tag Attitudes", "link": "https://arxiv.org/abs/2407.02710", "description": "arXiv:2407.02710v1 Announce Type: cross \nAbstract: Social media platforms enhance the propagation of online misinformation by providing large user bases with a quick means to share content. One way to disrupt the rapid dissemination of misinformation at scale is through warning tags, which label content as potentially false or misleading. Past warning tag mitigation studies yield mixed results for diverse audiences, however. We hypothesize that personalizing warning tags to the individual characteristics of their diverse users may enhance mitigation effectiveness. To reach the goal of personalization, we need to understand how people differ and how those differences predict a person's attitudes and self-described behaviors toward tags and tagged content. In this study, we leverage Amazon Mechanical Turk (n = 132) and undergraduate students (n = 112) to provide this foundational understanding. Specifically, we find attitudes towards warning tags and self-described behaviors are positively influenced by factors such as Personality Openness and Agreeableness, Need for Cognitive Closure (NFCC), Cognitive Reflection Test (CRT) score, and Trust in Medical Scientists. Conversely, Trust in Religious Leaders, Conscientiousness, and political conservatism were negatively correlated with these attitudes and behaviors. We synthesize our results into design insights and a future research agenda for more effective and personalized misinformation warning tags and misinformation mitigation strategies more generally."}, "https://arxiv.org/abs/2407.02758": {"title": "Differential Encoding for Improved Representation Learning over Graphs", "link": "https://arxiv.org/abs/2407.02758", "description": "arXiv:2407.02758v1 Announce Type: cross \nAbstract: Combining the message-passing paradigm with the global attention mechanism has emerged as an effective framework for learning over graphs. The message-passing paradigm and the global attention mechanism fundamentally generate node embeddings based on information aggregated from a node's local neighborhood or from the whole graph. The most basic and commonly used aggregation approach is to take the sum of information from a node's local neighbourhood or from the whole graph. However, it is unknown if the dominant information is from a node itself or from the node's neighbours (or the rest of the graph nodes). Therefore, there exists information lost at each layer of embedding generation, and this information lost could be accumulated and become more serious when more layers are used in the model. In this paper, we present a differential encoding method to address the issue of information lost. The idea of our method is to encode the differential representation between the information from a node's neighbours (or the rest of the graph nodes) and that from the node itself. The obtained differential encoding is then combined with the original aggregated local or global representation to generate the updated node embedding. By integrating differential encodings, the representational ability of generated node embeddings is improved. The differential encoding method is empirically evaluated on different graph tasks on seven benchmark datasets. The results show that it is a general method that improves the message-passing update and the global attention update, advancing the state-of-the-art performance for graph representation learning on these datasets."}, "https://arxiv.org/abs/2407.02885": {"title": "CogErgLLM: Exploring Large Language Model Systems Design Perspective Using Cognitive Ergonomics", "link": "https://arxiv.org/abs/2407.02885", "description": "arXiv:2407.02885v1 Announce Type: cross \nAbstract: Integrating cognitive ergonomics with LLMs is essential for enhancing safety, reliability, and user satisfaction in human-AI interactions. Current LLM design often lacks this integration, leading to systems that may not fully align with human cognitive capabilities and limitations. Insufficient focus on incorporating cognitive science methods exacerbates biases in LLM outputs, while inconsistent application of user-centered design principles results in sub-optimal user experiences. To address these challenges, our position paper explores the critical integration of cognitive ergonomics principles into LLM design, aiming to provide a comprehensive framework and practical guidelines for ethical LLM development. Through our contributions, we seek to advance understanding and practice in integrating cognitive ergonomics into LLM systems, fostering safer, more reliable, and ethically sound human-AI interactions."}, "https://arxiv.org/abs/2407.02992": {"title": "Scientific Text Analysis with Robots applied to observatory proposals", "link": "https://arxiv.org/abs/2407.02992", "description": "arXiv:2407.02992v1 Announce Type: cross \nAbstract: To test the potential disruptive effect of Artificial Intelligence (AI) transformers (e.g., ChatGPT) and their associated Large Language Models on the time allocation process, both in proposal reviewing and grading, an experiment has been set-up at ESO for the P112 Call for Proposals. The experiment aims at raising awareness in the ESO community and build valuable knowledge by identifying what future steps ESO and other observatories might need to take to stay up to date with current technologies. We present here the results of the experiment, which may further be used to inform decision-makers regarding the use of AI in the proposal review process. We find that the ChatGPT-adjusted proposals tend to receive lower grades compared to the original proposals. Moreover, ChatGPT 3.5 can generally not be trusted in providing correct scientific references, while the most recent version makes a better, but far from perfect, job. We also studied how ChatGPT deals with assessing proposals. It does an apparent remarkable job at providing a summary of ESO proposals, although it doesn't do so good to identify weaknesses. When looking at how it evaluates proposals, however, it appears that ChatGPT systematically gives a higher mark than humans, and tends to prefer proposals written by itself."}, "https://arxiv.org/abs/2407.03126": {"title": "Game-Theoretic Protection Adoption Against Networked SIS Epidemics", "link": "https://arxiv.org/abs/2407.03126", "description": "arXiv:2407.03126v1 Announce Type: cross \nAbstract: In this paper, we investigate game-theoretic strategies for containing spreading processes on large-scale networks. Specifically, we consider the class of networked susceptible-infected-susceptible (SIS) epidemics where a large population of agents strategically choose whether to adopt partially effective protection. We define the utilities of the agents which depends on the degree of the agent, its individual infection status and action, as well as the the overall prevalence of the epidemic and strategy profile of the entire population. We further present the coupled dynamics of epidemic evolution as well as strategy update which is assumed to follow the replicator dynamics. By relying on timescale separation arguments, we first derive the optimal strategy of protection adoption by the agents for a given epidemic state, and then present the reduced epidemic dynamics. The existence and uniqueness of endemic equilibrium is rigorously characterized and forms the main result of this paper. Finally, we present extensive numerical results to highlight the impacts of heterogeneous node degrees, infection rates, cost of protection adoption, and effectiveness of protection on the epidemic prevalence at the equilibrium."}, "https://arxiv.org/abs/2407.03190": {"title": "Cutting through the noise to motivate people: A comprehensive analysis of COVID-19 social media posts de/motivating vaccination", "link": "https://arxiv.org/abs/2407.03190", "description": "arXiv:2407.03190v1 Announce Type: cross \nAbstract: The COVID-19 pandemic exposed significant weaknesses in the healthcare information system. The overwhelming volume of misinformation on social media and other socioeconomic factors created extraordinary challenges to motivate people to take proper precautions and get vaccinated. In this context, our work explored a novel direction by analyzing an extensive dataset collected over two years, identifying the topics de/motivating the public about COVID-19 vaccination. We analyzed these topics based on time, geographic location, and political orientation. We noticed that while the motivating topics remain the same over time and geographic location, the demotivating topics rapidly. We also identified that intrinsic motivation, rather than external mandate, is more advantageous to inspire the public. This study addresses scientific communication and public motivation in social media. It can help public health officials, policymakers, and social media platforms develop more effective messaging strategies to cut through the noise of misinformation and educate the public about scientific findings."}, "https://arxiv.org/abs/2303.09590": {"title": "Visual Analytics of Multivariate Networks with Representation Learning and Composite Variable Construction", "link": "https://arxiv.org/abs/2303.09590", "description": "arXiv:2303.09590v3 Announce Type: replace \nAbstract: Multivariate networks are commonly found in real-world data-driven applications. Uncovering and understanding the relations of interest in multivariate networks is not a trivial task. This paper presents a visual analytics workflow for studying multivariate networks to extract associations between different structural and semantic characteristics of the networks (e.g., what are the combinations of attributes largely relating to the density of a social network?). The workflow consists of a neural-network-based learning phase to classify the data based on the chosen input and output attributes, a dimensionality reduction and optimization phase to produce a simplified set of results for examination, and finally an interpreting phase conducted by the user through an interactive visualization interface. A key part of our design is a composite variable construction step that remodels nonlinear features obtained by neural networks into linear features that are intuitive to interpret. We demonstrate the capabilities of this workflow with multiple case studies on networks derived from social media usage and also evaluate the workflow with qualitative feedback from experts."}, "https://arxiv.org/abs/2308.02755": {"title": "Multi-topic belief formation through bifurcations over signed social networks", "link": "https://arxiv.org/abs/2308.02755", "description": "arXiv:2308.02755v2 Announce Type: replace \nAbstract: We propose and analyze a nonlinear dynamic model of continuous-time multi-dimensional belief formation over signed social networks. Our model accounts for the effects of a structured belief system, self-appraisal, internal biases, and various sources of cognitive dissonance posited by recent theories in social psychology. We prove that agents become opinionated as a consequence of a bifurcation. We analyze how the balance of social network effects in the model controls the nature of the bifurcation and, therefore, the belief-forming limit-set solutions. Our analysis provides constructive conditions on how multi-stable network belief equilibria and belief oscillations emerging at a belief-forming bifurcation depend on the communication network graph and belief system network graph. Our model and analysis provide new theoretical insights on the dynamics of social systems and a new principled framework for designing decentralized decision-making on engineered networks in the presence of structured relationships among alternatives."}, "https://arxiv.org/abs/2001.05452": {"title": "The Gossiping Insert-Eliminate Algorithm for Multi-Agent Bandits", "link": "https://arxiv.org/abs/2001.05452", "description": "arXiv:2001.05452v4 Announce Type: replace-cross \nAbstract: We consider a decentralized multi-agent Multi Armed Bandit (MAB) setup consisting of $N$ agents, solving the same MAB instance to minimize individual cumulative regret. In our model, agents collaborate by exchanging messages through pairwise gossip style communications on an arbitrary connected graph. We develop two novel algorithms, where each agent only plays from a subset of all the arms. Agents use the communication medium to recommend only arm-IDs (not samples), and thus update the set of arms from which they play. We establish that, if agents communicate $\\Omega(\\log(T))$ times through any connected pairwise gossip mechanism, then every agent's regret is a factor of order $N$ smaller compared to the case of no collaborations. Furthermore, we show that the communication constraints only have a second order effect on the regret of our algorithm. We then analyze this second order term of the regret to derive bounds on the regret-communication tradeoffs. Finally, we empirically evaluate our algorithm and conclude that the insights are fundamental and not artifacts of our bounds. We also show a lower bound which gives that the regret scaling obtained by our algorithm cannot be improved even in the absence of any communication constraints. Our results thus demonstrate that even a minimal level of collaboration among agents greatly reduces regret for all agents."}, "https://arxiv.org/abs/2301.10844": {"title": "Diameter of Compact Riemann Surfaces", "link": "https://arxiv.org/abs/2301.10844", "description": "arXiv:2301.10844v4 Announce Type: replace-cross \nAbstract: Diameter is one of the most basic properties of a geometric object, while Riemann surfaces are one of the most basic geometric objects. Surprisingly, the diameter of compact Riemann surfaces is known exactly only for the sphere and the torus. For higher genuses, only very general but loose upper and lower bounds are available. The problem of calculating the diameter exactly has been intractable since there is no simple expression for the distance between a pair of points on a high-genus surface. Here we prove that the diameters of a class of simple Riemann surfaces known as generalized Bolza surfaces of any genus greater than $1$ are equal to the radii of their fundamental polygons. This is the first exact result for the diameter of a compact hyperbolic manifold."}, "https://arxiv.org/abs/2305.18784": {"title": "Collaborative Multi-Agent Heterogeneous Multi-Armed Bandits", "link": "https://arxiv.org/abs/2305.18784", "description": "arXiv:2305.18784v2 Announce Type: replace-cross \nAbstract: The study of collaborative multi-agent bandits has attracted significant attention recently. In light of this, we initiate the study of a new collaborative setting, consisting of $N$ agents such that each agent is learning one of $M$ stochastic multi-armed bandits to minimize their group cumulative regret. We develop decentralized algorithms which facilitate collaboration between the agents under two scenarios. We characterize the performance of these algorithms by deriving the per agent cumulative regret and group regret upper bounds. We also prove lower bounds for the group regret in this setting, which demonstrates the near-optimal behavior of the proposed algorithms."}, "https://arxiv.org/abs/2311.12702": {"title": "\"There Has To Be a Lot That We're Missing\": Moderating AI-Generated Content on Reddit", "link": "https://arxiv.org/abs/2311.12702", "description": "arXiv:2311.12702v4 Announce Type: replace-cross \nAbstract: Generative AI has begun to alter how we work, learn, communicate, and participate in online communities. How might our online communities be changed by generative AI? To start addressing this question, we focused on online community moderators' experiences with AI-generated content (AIGC). We performed fifteen in-depth, semi-structured interviews with community moderators on the social sharing site Reddit to understand their attitudes towards AIGC and how their communities are responding. Our study finds communities are choosing to enact rules restricting use of AIGC for both ideological and practical reasons. We find that, despite the absence of foolproof tools for detecting AIGC, moderators were able to somewhat limit the disruption caused by this new phenomenon by working with their communities to clarify norms about AIGC use. However, moderators found enforcing AIGC restrictions challenging, and had to rely on time-intensive and inaccurate detection heuristics in their efforts. Our results highlight the importance of supporting community autonomy and self-determination in the face of this sudden technological change, and suggest potential design solutions that may help."}, "https://arxiv.org/abs/2401.15090": {"title": "Maximum entropy in dynamic complex networks", "link": "https://arxiv.org/abs/2401.15090", "description": "arXiv:2401.15090v2 Announce Type: replace-cross \nAbstract: The field of complex networks studies a wide variety of interacting systems by representing them as networks. To understand their properties and mutual relations, the randomisation of network connections is a commonly used tool. However, information theoretic-based randomisation methods with well-established foundations mostly provide a stationary description of these systems, while stochastic randomisation methods that account for their dynamic nature lack such general foundations and require extensive repetition of the stochastic process to measure statistical properties. In this work, we extend the applicability of information-theoretic methods beyond stationary network models. By using the information-theoretic principle of maximum caliber we construct dynamic network ensemble distributions based on constraints representing statistical properties with known values throughout the evolution. We focus on the particular cases of dynamics constrained by the average number of connections of the whole network and each node, comparing each evolution to simulations of stochastic randomisation that obey the same constraints. We find that ensemble distributions estimated from simulations match those calculated with maximum caliber and that the equilibrium distributions to which they converge agree with known results of maximum entropy given the same constraints. Finally, we discuss further the connections to other maximum entropy approaches to network dynamics and conclude by proposing some possible avenues of future research."}, "https://arxiv.org/abs/2407.03375": {"title": "Asymptotic and stability analysis of kinetic models for opinion formation on networks: an Allen-Cahn approach", "link": "https://arxiv.org/abs/2407.03375", "description": "arXiv:2407.03375v1 Announce Type: new \nAbstract: We present the analysis of the stationary equilibria and their stability in case of an opinion formation process in presence of binary opposite opinions evolving according to majority-like rules on social networks. The starting point is a kinetic Boltzmann-type model derived from microscopic interactions rules for the opinion exchange among individuals holding a certain degree of connectivity. The key idea is to derive from the kinetic model an Allen-Cahn type equation for the fraction of individuals holding one of the two opinions. The latter can be studied by means of a linear stability analysis and by exploiting integral operator analysis. While this is true for ternary interactions, for binary interactions the derived equation of interest is a linear scattering equation, that can be studied by means of General Relative Entropy tools and integral operators."}, "https://arxiv.org/abs/2407.03376": {"title": "The Complex Interplay Between Risk Tolerance and the Spread of Infectious Diseases", "link": "https://arxiv.org/abs/2407.03376", "description": "arXiv:2407.03376v1 Announce Type: new \nAbstract: Risk-driven behavior provides a feedback mechanism through which individuals both shape and are collectively affected by an epidemic. We introduce a general and flexible compartmental model to study the effect of heterogeneity in the population with regards to risk tolerance. The interplay between behavior and epidemiology leads to a rich set of possible epidemic dynamics. Depending on the behavioral composition of the population, we find that increasing heterogeneity in risk tolerance can either increase or decrease the epidemic size. We find that multiple waves of infection can arise due to the interplay between transmission and behavior, even without the replenishment of susceptibles. We find that increasing protective mechanisms such as the effectiveness of interventions, the number of risk-averse people in the population, and the duration of intervention usage reduces the epidemic overshoot. When the protection is pushed past a critical threshold, the epidemic dynamics enter an underdamped regime where the epidemic size exactly equals the herd immunity threshold. Lastly, we can find regimes where epidemic size does not monotonically decrease with a population that becomes increasingly risk-averse."}, "https://arxiv.org/abs/2407.03388": {"title": "Passenger Route and Departure Time Guidance under Disruptions in Oversaturated Urban Rail Transit Networks", "link": "https://arxiv.org/abs/2407.03388", "description": "arXiv:2407.03388v1 Announce Type: new \nAbstract: The urban rail transit (URT) system attracts many commuters with its punctuality and convenience. However, it is vulnerable to disruptions caused by factors like extreme weather and temporary equipment failures, which greatly impact passengers' journeys and diminish the system's service quality. In this study, we propose targeted travel guidance for passengers at different space-time locations by devising passenger rescheduling strategies during disruptions. This guidance not only offers insights into route changes but also provides practical recommendations for delaying departure times when required. We present a novel three-feature four-group passenger classification principle, integrating temporal, spatial, and spatio-temporal features to classify passengers in disrupted URT networks. This approach results in the creation of four distinct solution spaces based on passenger groups. A mixed integer programming model is built based on individual level considering the First-in-First-out (FIFO) rule in oversaturated networks. Additionally, we present a two-stage solution approach for handling the complex issues in large-scale networks. Experimental results from both small-scale artificial networks and the real-world Beijing URT network validate the efficacy of our proposed passenger rescheduling strategies in mitigating disruptions. Specifically, when compared to scenarios with no travel guidance during disruptions, our strategies achieve a substantial reduction in total passenger travel time by 29.7% and 50.9% respectively, underscoring the effectiveness in managing unexpected disruptions."}, "https://arxiv.org/abs/2407.03484": {"title": "Visualizing the Evolution of Twitter (X", "link": "https://arxiv.org/abs/2407.03484", "description": "arXiv:2407.03484v1 Announce Type: new \nAbstract: With the rise of social media platforms, especially X.com (formerly Twitter), there is a growing interest in understanding digital social networks and human digital interactions. This paper presents a comprehensive methodology for extracting, processing, and visually analyzing data from X.com, using a combination of Python and R packages, enhanced by our publicly accessible, customizable code. Our approach compiles a dynamic dataset that captures various interactions: replies, retweets, and mentions. To explore deeper insights, the data is subjected to sentiment analysis and keyword coding, indicating shifts in discourse over time. Our method is structured in three primary phases. Initially, R is employed for pulling data and the formation of social network datasets. Following this, the combination of Python and R is utilized for sentiment analysis and keyword coding, aiming to uncover the underlying emotional shifts and language transitions within topics of discussion. The final phase employs R to visualize the dynamic shifts within these social networks. These visualization tools highlight changes in user interactions and patterns of influence. For a practical demonstration, we analyzed conversations on X.com regarding the controversial proposal to halt AI development, focusing specifically on discussions about ChatGPT. By using keyword searches, leading voices in the debate were identified. Our analysis of sentiment and keywords revealed patterns in emotions and language, while visual tools illustrated the development of network connections and their influence. This study emphasizes the vital role of visual tools in understanding online social dynamics in the digital age."}, "https://arxiv.org/abs/2407.03551": {"title": "Feelings about Bodies: Emotions on Diet and Fitness Forums Reveal Gendered Stereotypes and Body Image Concerns", "link": "https://arxiv.org/abs/2407.03551", "description": "arXiv:2407.03551v1 Announce Type: new \nAbstract: The gendered expectations about ideal body types can lead to body image concerns, dissatisfaction, and in extreme cases, disordered eating and other psychopathologies across the gender spectrum. While research has focused on pro-anorexia online communities that glorify the 'thin ideal', less attention has been given to the broader spectrum of body image concerns or how emerging disorders like muscle dysmorphia ('bigorexia') present in online discussions. To address these gaps, we analyze 46 Reddit discussion forums related to diet, fitness, and associated mental health challenges. Using membership structure analysis and transformer-based language models, we project these communities along gender and body ideal axes, revealing complex interactions between gender, body ideals, and emotional expression. Our findings show that feminine-oriented communities generally express more negative emotions, particularly in thinness-promoting forums. Conversely, communities focused on the muscular ideal exhibit less negativity, regardless of gender orientation. We also uncover a gendered pattern in emotional indicators of mental health challenges, with communities discussing serious issues aligning more closely with thinness-oriented, predominantly feminine-leaning communities. By revealing the gendered emotional dynamics of online communities, our findings can inform the development of more effective content moderation approaches that facilitate supportive interactions, while minimizing exposure to potentially harmful content."}, "https://arxiv.org/abs/2407.03568": {"title": "When LLM Meets Hypergraph: A Sociological Analysis on Personality via Online Social Networks", "link": "https://arxiv.org/abs/2407.03568", "description": "arXiv:2407.03568v1 Announce Type: new \nAbstract: Individual personalities significantly influence our perceptions, decisions, and social interactions, which is particularly crucial for gaining insights into human behavior patterns in online social network analysis. Many psychological studies have observed that personalities are strongly reflected in their social behaviors and social environments. In light of these problems, this paper proposes a sociological analysis framework for one's personality in an environment-based view instead of individual-level data mining. Specifically, to comprehensively understand an individual's behavior from low-quality records, we leverage the powerful associative ability of LLMs by designing an effective prompt. In this way, LLMs can integrate various scattered information with their external knowledge to generate higher-quality profiles, which can significantly improve the personality analysis performance. To explore the interactive mechanism behind the users and their online environments, we design an effective hypergraph neural network where the hypergraph nodes are users and the hyperedges in the hypergraph are social environments. We offer a useful dataset with user profile data, personality traits, and several detected environments from the real-world social platform. To the best of our knowledge, this is the first network-based dataset containing both hypergraph structure and social information, which could push forward future research in this area further. By employing the framework on this dataset, we can effectively capture the nuances of individual personalities and their online behaviors, leading to a deeper understanding of human interactions in the digital world."}, "https://arxiv.org/abs/2407.03773": {"title": "Decoding Political Polarization in Social Media Interactions", "link": "https://arxiv.org/abs/2407.03773", "description": "arXiv:2407.03773v1 Announce Type: new \nAbstract: Social media platforms significantly influence ideological divisions by enabling users to select information that aligns with their beliefs and avoid opposing viewpoints. Analyzing approximately 47 million Facebook posts, this study investigates the interactions of around 170 million users with news pages, revealing distinct patterns based on political orientations. While users generally prefer content that reflects their political biases, the extent of engagement varies even among individuals with similar ideological leanings. Specifically, political biases heavily influence commenting behaviors, particularly among users leaning towards the center-left and the right. Conversely, the 'likes' from center-left and centrist users are more indicative of their political affiliations. This research illuminates the complex relationship between social media behavior and political polarization, offering new insights into the manifestation of ideological divisions online."}, "https://arxiv.org/abs/2407.03904": {"title": "Asymmetric Iterated Prisoner's Dilemma on BA Scale-Free Network", "link": "https://arxiv.org/abs/2407.03904", "description": "arXiv:2407.03904v1 Announce Type: new \nAbstract: In real-world scenarios, individuals often cooperate for mutual benefit. However, differences in wealth can lead to varying outcomes for similar actions. In complex social networks, individuals' choices are also influenced by their neighbors. To explore the evolution of strategies in realistic settings, we conducted repeated asymmetric prisoners dilemma experiments on a weighted BA scale-free network. Our analysis highlighted how the four components of memory-one strategies affect win rates, found two special strategies in the evolutionary process, and increased the cooperation levels among individuals. These findings offer practical insights for addressing real-world problems."}, "https://arxiv.org/abs/2407.03968": {"title": "Academic Freedom and International Research Collaboration: A Longitudinal Analysis of Global Network Evolution", "link": "https://arxiv.org/abs/2407.03968", "description": "arXiv:2407.03968v1 Announce Type: new \nAbstract: The topic of academic freedom has come to the fore as nations around the world experience a wave of democratic backsliding. Institutions of higher education are often targets of autocrats who seek to suppress intellectual sources of social and political resistance. At the same time, international collaboration in scientific research has reached an all-time high, and the network of global science grows larger and denser every year. This research analyzes the effects of academic freedom on international research collaboration (IRC) among a sample of 166 countries. Global international collaboration data are drawn from articles in Web of Science across a 33-year time frame (1993-2022) and used to construct three separate IRC networks in science and technology (S&amp;T), social sciences (SocSci), and arts and humanities (A&amp;H). The Academic Freedom Index, covering the same time frame, is drawn from the Varieties of Democracy Project. Stochastic actor-oriented models (SAOM) are used to analyze the networks, implemented using the R package RSiena. Since IRC networks are naturally weighted by frequency of international co-authorship instances for each year, the R package backbone is used to binarize and trim the ties. Numerous endogenous network control variables are included in the model, as are exogenous country level factors, including geographic distance, number of authors, and GDP. The results show positive significant estimates for direct effects, non-linear direct effects, and homophily effects of academic freedom on tie creation and maintenance over time. These estimates increase in strength moving from the S&amp;T network, to the SocSci network, and is strongest in the A\\&amp;H network. This research provides support for the theory that academic research flourishes within environments of intellectual openness and freedom."}, "https://arxiv.org/abs/2407.04503": {"title": "When LLMs Play the Telephone Game: Cumulative Changes and Attractors in Iterated Cultural Transmissions", "link": "https://arxiv.org/abs/2407.04503", "description": "arXiv:2407.04503v1 Announce Type: new \nAbstract: As large language models (LLMs) start interacting with each other and generating an increasing amount of text online, it becomes crucial to better understand how information is transformed as it passes from one LLM to the next. While significant research has examined individual LLM behaviors, existing studies have largely overlooked the collective behaviors and information distortions arising from iterated LLM interactions. Small biases, negligible at the single output level, risk being amplified in iterated interactions, potentially leading the content to evolve towards attractor states. In a series of telephone game experiments, we apply a transmission chain design borrowed from the human cultural evolution literature: LLM agents iteratively receive, produce, and transmit texts from the previous to the next agent in the chain. By tracking the evolution of text toxicity, positivity, difficulty, and length across transmission chains, we uncover the existence of biases and attractors, and study their dependence on the initial text, the instructions, language model, and model size. For instance, we find that more open-ended instructions lead to stronger attraction effects compared to more constrained tasks. We also find that different text properties display different sensitivity to attraction effects, with toxicity leading to stronger attractors than length. These findings highlight the importance of accounting for multi-step transmission dynamics and represent a first step towards a more comprehensive understanding of LLM cultural dynamics."}, "https://arxiv.org/abs/2407.03446": {"title": "Towards Asimov's Psychohistory: Harnessing Topological Data Analysis, Artificial Intelligence and Social Media data to Forecast Societal Trends", "link": "https://arxiv.org/abs/2407.03446", "description": "arXiv:2407.03446v1 Announce Type: cross \nAbstract: In the age of big data and advanced computational methods, the prediction of large-scale social behaviors, reminiscent of Isaac Asimov's fictional science of Psychohistory, is becoming increasingly feasible. This paper consists of a theoretical exploration of the integration of computational power and mathematical frameworks, particularly through Topological Data Analysis (TDA) (Carlsson, Vejdemo-Johansson, 2022) and Artificial Intelligence (AI), to forecast societal trends through social media data analysis. By examining social media as a reflective surface of collective human behavior through the systematic behaviorist approach (Glenn, et al., 2016), I argue that these tools provide unprecedented clarity into the dynamics of large communities. This study dialogues with Asimov's work, drawing parallels between his visionary concepts and contemporary methodologies, illustrating how modern computational techniques can uncover patterns and predict shifts in social behavior, contributing to the emerging field of digital sociology -- or even, Psychohistory itself."}, "https://arxiv.org/abs/2407.03665": {"title": "Heterogeneous Hypergraph Embedding for Recommendation Systems", "link": "https://arxiv.org/abs/2407.03665", "description": "arXiv:2407.03665v1 Announce Type: cross \nAbstract: Recent advancements in recommender systems have focused on integrating knowledge graphs (KGs) to leverage their auxiliary information. The core idea of KG-enhanced recommenders is to incorporate rich semantic information for more accurate recommendations. However, two main challenges persist: i) Neglecting complex higher-order interactions in the KG-based user-item network, potentially leading to sub-optimal recommendations, and ii) Dealing with the heterogeneous modalities of input sources, such as user-item bipartite graphs and KGs, which may introduce noise and inaccuracies. To address these issues, we present a novel Knowledge-enhanced Heterogeneous Hypergraph Recommender System (KHGRec). KHGRec captures group-wise characteristics of both the interaction network and the KG, modeling complex connections in the KG. Using a collaborative knowledge heterogeneous hypergraph (CKHG), it employs two hypergraph encoders to model group-wise interdependencies and ensure explainability. Additionally, it fuses signals from the input graphs with cross-view self-supervised learning and attention mechanisms. Extensive experiments on four real-world datasets show our model's superiority over various state-of-the-art baselines, with an average 5.18\\% relative improvement. Additional tests on noise resilience, missing data, and cold-start problems demonstrate the robustness of our KHGRec framework. Our model and evaluation datasets are publicly available at \\url{https://github.com/viethungvu1998/KHGRec}."}, "https://arxiv.org/abs/2407.03953": {"title": "Generalizing Graph Transformers Across Diverse Graphs and Tasks via Pre-Training on Industrial-Scale Data", "link": "https://arxiv.org/abs/2407.03953", "description": "arXiv:2407.03953v1 Announce Type: cross \nAbstract: Graph pre-training has been concentrated on graph-level on small graphs (e.g., molecular graphs) or learning node representations on a fixed graph. Extending graph pre-trained models to web-scale graphs with billions of nodes in industrial scenarios, while avoiding negative transfer across graphs or tasks, remains a challenge. We aim to develop a general graph pre-trained model with inductive ability that can make predictions for unseen new nodes and even new graphs. In this work, we introduce a scalable transformer-based graph pre-training framework called PGT (Pre-trained Graph Transformer). Specifically, we design a flexible and scalable graph transformer as the backbone network. Meanwhile, based on the masked autoencoder architecture, we design two pre-training tasks: one for reconstructing node features and the other one for reconstructing local structures. Unlike the original autoencoder architecture where the pre-trained decoder is discarded, we propose a novel strategy that utilizes the decoder for feature augmentation. We have deployed our framework on Tencent's online game data. Extensive experiments have demonstrated that our framework can perform pre-training on real-world web-scale graphs with over 540 million nodes and 12 billion edges and generalizes effectively to unseen new graphs with different downstream tasks. We further conduct experiments on the publicly available ogbn-papers100M dataset, which consists of 111 million nodes and 1.6 billion edges. Our framework achieves state-of-the-art performance on both industrial datasets and public datasets, while also enjoying scalability and efficiency."}, "https://arxiv.org/abs/2407.04366": {"title": "Nash epidemics", "link": "https://arxiv.org/abs/2407.04366", "description": "arXiv:2407.04366v1 Announce Type: cross \nAbstract: Faced with a dangerous epidemic humans will spontaneously social distance to reduce their risk of infection at a socio-economic cost. Compartmentalised epidemic models have been extended to include this endogenous decision making: Individuals choose their behaviour to optimise a utility function, self-consistently giving rise to population behaviour. Here we study the properties of the resulting Nash equilibria, in which no member of the population can gain an advantage by unilaterally adopting different behaviour. We leverage a new analytic solution to obtain, (1) a simple relationship between rational social distancing behaviour and the current number of infections; (2) new scaling results for how the infection peak and number of total cases depend on the cost of contracting the disease; (3) characteristic infection costs that divide regimes of strong and weak behavioural response and depend only on the basic reproduction number of the disease; (4) a closed form expression for the value of the utility. We discuss how these analytic results provide a deep and intuitive understanding into the disease dynamics, useful for both individuals and policymakers. In particular the relationship between social distancing and infections represents a heuristic that could be communicated to the population to encourage, or \"bootstrap\", rational behaviour."}, "https://arxiv.org/abs/2407.04465": {"title": "Learning Patterns from Biological Networks: A Compounded Burr Probability Model", "link": "https://arxiv.org/abs/2407.04465", "description": "arXiv:2407.04465v1 Announce Type: cross \nAbstract: Complex biological networks, comprising metabolic reactions, gene interactions, and protein interactions, often exhibit scale-free characteristics with power-law degree distributions. However, empirical studies have revealed discrepancies between observed biological network data and ideal power-law fits, highlighting the need for improved modeling approaches. To address this challenge, we propose a novel family of distributions, building upon the baseline Burr distribution. Specifically, we introduce the compounded Burr (CBurr) distribution, derived from a continuous probability distribution family, enabling flexible and efficient modeling of node degree distributions in biological networks. This study comprehensively investigates the general properties of the CBurr distribution, focusing on parameter estimation using the maximum likelihood method. Subsequently, we apply the CBurr distribution model to large-scale biological network data, aiming to evaluate its efficacy in fitting the entire range of node degree distributions, surpassing conventional power-law distributions and other benchmarks. Through extensive data analysis and graphical illustrations, we demonstrate that the CBurr distribution exhibits superior modeling capabilities compared to traditional power-law distributions. This novel distribution model holds great promise for accurately capturing the complex nature of biological networks and advancing our understanding of their underlying mechanisms."}, "https://arxiv.org/abs/2307.04700": {"title": "Strength and weakness of disease-induced herd immunity in networks", "link": "https://arxiv.org/abs/2307.04700", "description": "arXiv:2307.04700v2 Announce Type: replace \nAbstract: When a fraction of a population becomes immune to an infectious disease, the population-wide infection risk decreases nonlinearly due to collective protection known as herd immunity. Studies based on mean-field models suggest that natural infection in a heterogeneous population may induce herd immunity more efficiently than homogeneous immunization. Here, we use network epidemic models to show that the opposite can also be the case. We identify two competing mechanisms driving disease-induced herd immunity in networks: the high density of immunity among socially active individuals enhances the herd immunity effect, while the topological localization of immune individuals weakens it. The effect of localization is stronger in networks embedded in low-dimensional space, which can make disease-induced immunity less effective than random immunization. Our results highlight the role of networks in shaping herd immunity and call for careful examination of model predictions that inform public health policies."}, "https://arxiv.org/abs/2310.06461": {"title": "Phosphorus recycling from human excreta in French agroecosystems and potential for food self-sufficiency", "link": "https://arxiv.org/abs/2310.06461", "description": "arXiv:2310.06461v2 Announce Type: replace \nAbstract: Phosphorus (P) is an essential constituent of life but large P losses from agroecosystems and sanitation systems are a major source of eutrophication in water bodies.These losses are doubly detrimental as P in human excretions can be used for crop fertilization.Through a unique dataset of 20,000 French WasteWater Treatment Plants (WWTPs) operational measurements over two decades and a P mass balance, we assess the fate of human excretions and their agricultural potential.Despite 75% of French WWTPs sludge being spread on crops, only 50% of the excreted P makes it back to agroecosystems. This is among the highest rate in Western countries where assessments have been made.Meanwhile, another 35% of the excreted P ends up in surface waters or the environment through WWTP discharge, individual autonomous systems diffuse losses, and sewers leaks.The remaining 15% is incinerated or sent to landfills.Moreover, while WWTP removal efficiency increased in the 2000s, reaching an 80% national average, it has been followed by a decade of stagnation in every French basin.The final removal efficiency for each basin, from 65% to 85%, closely matches whether the area was defined as P-sensitive in the European directive.Our results suggest that recycling all P in excretions could help supply 7 to 34% of French food supply without changing the current food system.Reshaping agricultural systems (shifting to more plant-based diets, decreasing P losses and food waste) would enable to go even further on the road to food sufficiency."}, "https://arxiv.org/abs/2401.02121": {"title": "Characterizing pedestrian contact interaction trajectories to understand spreading risk in human crowds", "link": "https://arxiv.org/abs/2401.02121", "description": "arXiv:2401.02121v2 Announce Type: replace \nAbstract: A spreading process can be observed when particular information, substances, or diseases spread through a population over time in social and biological systems. It is widely believed that contact interactions among individual entities play an essential role in the spreading process. Although contact interactions are often influenced by geometrical conditions, little attention has been paid to understand their effects, especially on contact duration among pedestrians. To examine how the pedestrian flow setups affect contact duration distribution, we have analyzed trajectories of pedestrians in contact interactions collected from pedestrian flow experiments of uni-, bi- and multi-directional setups. Based on turning angle entropy and efficiency, we have classified the type of motion observed in the contact interactions. We have found that the majority of contact interactions in the unidirectional flow setup can be categorized as confined motion, hinting at the possibility of long-lived contact duration. However, ballistic motion is more frequently observed in the other flow conditions, yielding frequent, brief contact interactions. Our results demonstrate that observing more confined motions is likely associated with the increase of parallel contact interactions regardless of pedestrian flow setups. This study highlights that the confined motions tend to yield longer contact duration, suggesting that the infectious disease transmission risk would be considerable even for low transmissibility. These results have important implications for crowd management in the context of minimizing spreading risk. This work is an extended version of Kwak et al. (2023) presented at the 2023 International Conference on Computational Science (ICCS)."}, "https://arxiv.org/abs/2404.10837": {"title": "The CHEPA model: assessing the impact of HEPA filter units in classrooms using a fast-running coupled indoor air quality and dynamic thermal model", "link": "https://arxiv.org/abs/2404.10837", "description": "arXiv:2404.10837v2 Announce Type: replace \nAbstract: The quality of the classroom environment, including ventilation, air quality and thermal conditions, has an important impact on children's health and academic achievements. The use of portable HEPA filter air cleaners is widely suggested as a strategy to mitigate exposure to particulate matter and airborne viruses. However, there is a need to quantify the relative benefits of such devices including the impacts on energy use. We present a simple coupled dynamic thermal and air quality model and apply it to naturally ventilated classrooms, representative of modern and Victorian era construction. We consider the addition of HEPA filters with, and without, reduced opening of windows, and explore concentrations of carbon dioxide (\\co), \\PM, airborne viral RNA, classroom temperature and energy use. Results indicate the addition of HEPA filters was predicted to reduce \\PM~ by 40--60\\% and viral RNA by 30--50\\% depending on the classroom design and window opening behaviour. The energy cost of running HEPA filters is likely to be only 1\\%--2\\% of the classroom heating costs. In scenarios when HEPA filters were on and window opening was reduced (to account for the additional clean air delivery rate of the filters), the heating cost was predicted to be reduced by as much as -13\\%, and these maximum reductions grew to -46\\% in wintertime simulations. In these scenarios the HEPA filters result in a notable reduction in \\PM~and viral RNA, but the \\co\\ concentration is significantly higher. The model provides a mechanism for exploring the relative impact of ventilation and air cleaning strategies on both exposures and energy costs, enabling an understanding of where trade-offs lie."}, "https://arxiv.org/abs/2404.16862": {"title": "Edge Importance in Complex Networks", "link": "https://arxiv.org/abs/2404.16862", "description": "arXiv:2404.16862v2 Announce Type: replace \nAbstract: Complex networks are made up of vertices and edges. The latter connect the vertices. There are several ways to measure the importance of the vertices, e.g., by counting the number of edges that start or end at each vertex, or by using the subgraph centrality of the vertices. It is more difficult to assess the importance of the edges. One approach is to consider the line graph associated with the given network and determine the importance of the vertices of the line graph, but this is fairly complicated except for small networks. This paper compares two approaches to estimate the importance of edges of medium-sized to large networks. One approach computes partial derivatives of the total communicability of the weights of the edges, where a partial derivative of large magnitude indicates that the corresponding edge may be important. Our second approach computes the Perron sensitivity of the edges. A high sensitivity signals that the edge may be important. The performance of these methods and some computational aspects are discussed. Applications of interest include to determine whether a network can be replaced by a network with fewer edges with about the same communicability."}, "https://arxiv.org/abs/2401.02627": {"title": "Characteristics and prevalence of fake social media profiles with AI-generated faces", "link": "https://arxiv.org/abs/2401.02627", "description": "arXiv:2401.02627v2 Announce Type: replace-cross \nAbstract: Recent advancements in generative artificial intelligence (AI) have raised concerns about their potential to create convincing fake social media accounts, but empirical evidence is lacking. In this paper, we present a systematic analysis of Twitter (X) accounts using human faces generated by Generative Adversarial Networks (GANs) for their profile pictures. We present a dataset of 1,420 such accounts and show that they are used to spread scams, spam, and amplify coordinated messages, among other inauthentic activities. Leveraging a feature of GAN-generated faces -- consistent eye placement -- and supplementing it with human annotation, we devise an effective method for identifying GAN-generated profiles in the wild. Applying this method to a random sample of active Twitter users, we estimate a lower bound for the prevalence of profiles using GAN-generated faces between 0.021% and 0.044% -- around 10K daily active accounts. These findings underscore the emerging threats posed by multimodal generative AI. We release the source code of our detection method and the data we collect to facilitate further investigation. Additionally, we provide practical heuristics to assist social media users in recognizing such accounts."}, "https://arxiv.org/abs/2403.06559": {"title": "Waiting times for sea level variations in the Port of Trieste: a computational data-driven study", "link": "https://arxiv.org/abs/2403.06559", "description": "arXiv:2403.06559v2 Announce Type: replace-cross \nAbstract: We report here a series of detailed statistical analyses on the sea level variations in the Port of Trieste using one of the largest existing catalogues that covers more than a century of measurements. We show that the distribution of waiting times, which are defined here akin to econophysics, namely the series of shortest time spans between a given sea level L and the next sea level of at least L + {\\delta} in the catalogue, exhibits a distinct scale-free character for small values of {\\delta}, while for larger values of {\\delta} the distribution is very similar to the exponential distribution. The distribution of waiting times for small values of {\\delta} is typical for complex systems exhibiting criticality and is reported abundantly in the literature, while the exponential-like distribution observed for large values of {\\delta} has been observed in contexts as diverse as magnetic systems and light sleep duration."}, "https://arxiv.org/abs/2407.05083": {"title": "Exploring agent interaction patterns in the comment sections of fake and real news", "link": "https://arxiv.org/abs/2407.05083", "description": "arXiv:2407.05083v1 Announce Type: new \nAbstract: User comments on social media have been recognized as a crucial factor in distinguishing between fake and real news, with many studies focusing on the textual content of user reactions. However, the interactions among agents in the comment sections for fake and real news have not been fully explored. In this study, we analyze a dataset comprising both fake and real news from Reddit to investigate agent interaction patterns, considering both the network structure and the sentiment of the nodes. Our findings reveal that (i) comments on fake news are more likely to form groups, (ii) compared to fake news, where users generate more negative sentiment, real news tend to elicit more neutral and positive sentiments. Additionally, nodes with similar sentiments cluster together more tightly than anticipated. From a dynamic perspective, we found that the sentiment distribution among nodes stabilizes early and remains stable over time. These findings have both theoretical and practical implications, particularly for the early detection of real and fake news within social networks."}, "https://arxiv.org/abs/2407.05161": {"title": "A Survey of Datasets for Information Diffusion Tasks", "link": "https://arxiv.org/abs/2407.05161", "description": "arXiv:2407.05161v1 Announce Type: new \nAbstract: Information diffusion across various new media platforms gradually influences perceptions, decisions, and social behaviors of individual users. In communication studies, the famous Five W's of Communication model (5W Model) has displayed the process of information diffusion clearly. At present, although plenty of studies and corresponding datasets about information diffusion have emerged, a systematic categorization of tasks and an integration of datasets are still lacking. To address this gap, we survey a systematic taxonomy of information diffusion tasks and datasets based on the \"5W Model\" framework. We first categorize the information diffusion tasks into ten subtasks with definitions and datasets analysis, from three main tasks of information diffusion prediction, social bot detection, and misinformation detection. We also collect the publicly available dataset repository of information diffusion tasks with the available links and compare them based on six attributes affiliated to users and content: user information, social network, bot label, propagation content, propagation network, and veracity label. In addition, we discuss the limitations and future directions of current datasets and research topics to advance the future development of information diffusion. The dataset repository can be accessed at our website https://github.com/fuxiaG/Information-Diffusion-Datasets."}, "https://arxiv.org/abs/2407.05198": {"title": "Medfluencer: A Network Representation of Medical Influencers' Identities and Discourse on Social Media", "link": "https://arxiv.org/abs/2407.05198", "description": "arXiv:2407.05198v1 Announce Type: new \nAbstract: In our study, we first constructed a dataset from the tweets of the top 100 medical influencers with the highest Influencer Score during the COVID-19 pandemic. This dataset was then used to construct a socio-semantic network, mapping both their identities and key topics, which are crucial for understanding their impact on public health discourse. To achieve this, we developed a few-shot multi-label classifier to identify influencers and their network actors' identities, employed BERTopic for extracting thematic content, and integrated these components into a network model to analyze their impact on health discourse."}, "https://arxiv.org/abs/2407.05669": {"title": "Fractional Budget Allocation for Influence Maximization under General Marketing Strategies", "link": "https://arxiv.org/abs/2407.05669", "description": "arXiv:2407.05669v1 Announce Type: new \nAbstract: We consider the fractional influence maximization problem, i.e., identifying users on a social network to be incentivized with potentially partial discounts to maximize the influence on the network. The larger the discount given to a user, the higher the likelihood of its activation (adopting a new product or innovation), who then attempts to activate its neighboring users, causing a cascade effect of influence through the network. Our goal is to devise efficient algorithms that assign initial discounts to the network's users to maximize the total number of activated users at the end of the cascade, subject to a constraint on the total sum of discounts given. In general, the activation likelihood could be any non-decreasing function of the discount, whereas, our focus lies on the case when the activation likelihood is an affine function of the discount, potentially varying across different users. As this problem is shown to be NP-hard, we propose and analyze an efficient (1-1/e)-approximation algorithm. Furthermore, we run experiments on real-world social networks to show the performance and scalability of our method."}, "https://arxiv.org/abs/2407.05929": {"title": "Multiplexity is temporal: effects of social times on network structure", "link": "https://arxiv.org/abs/2407.05929", "description": "arXiv:2407.05929v1 Announce Type: new \nAbstract: Large-scale social networks constructed using contact metadata have been invaluable tools for understanding and testing social theories of society-wide social structures. However, multiplex relationships explaining different social contexts have been out of reach of this methodology, limiting our ability to understand this crucial aspect of social systems. We propose a method that infers latent social times from the weekly activity of large-scale contact metadata, and reconstruct multilayer networks where layers correspond to social times. We then analyze the temporal multiplexity of ties in a society-wide communication network of millions of individuals. This allows us to test the propositions of Feld's social focus theory across a society-wide network: We show that ties favour their own social times regardless of contact intensity, suggesting they reflect underlying social foci. We present a result on strength of monoplex ties, which indicates that monoplex ties are bridging and even more important for global network connectivity than the weak, low-contact ties. Finally, we show that social times are transitive, so that when egos use a social time for a small subset of alters, the alters use the social time among themselves as well. Our framework opens up a way to analyse large-scale communication as multiplex networks and uncovers society-level patterns of multiplex connectivity."}, "https://arxiv.org/abs/2407.05956": {"title": "Untangling the Furball: A Practice Mapping Approach to the Analysis of Multimodal Interactions in Social Networks", "link": "https://arxiv.org/abs/2407.05956", "description": "arXiv:2407.05956v1 Announce Type: new \nAbstract: This article introduces the analytical approach of practice mapping, using vector embeddings of network actions and interactions to map commonalities and disjunctures in the practices of social media users, as a framework for methodological advancement beyond the limitations of conventional network analysis and visualisation. In particular, the methodological framework we outline here has the potential to incorporate multiple distinct modes of interaction into a single practice map, can be further enriched with account-level attributes such as information gleaned from textual analysis, profile information, available demographic details, and other features, and can be applied even to a cross-platform analysis of communicative patterns and practices.\n  The article presents practice mapping as an analytical framework and outlines its key methodological considerations. Given its prominence in past social media research, we draw on examples and data from the platform formerly known as Twitter in order to enable experienced scholars to translate their approaches to a practice mapping paradigm more easily, but point out how data from other platforms may be used in equivalent ways in practice mapping studies. We illustrate the utility of the approach by applying it to a dataset where the application of conventional network analysis and visualisation approaches has produced few meaningful insights."}, "https://arxiv.org/abs/2407.05988": {"title": "Minimum-regret hydrogen supply chain strategies to foster the energy transition of European hard-to-abate industries", "link": "https://arxiv.org/abs/2407.05988", "description": "arXiv:2407.05988v1 Announce Type: new \nAbstract: Low-carbon hydrogen (H2) is envisioned to play a central role in decarbonizing European hard-to-abate industries, such as refineries, ammonia, methanol, steel, and cement. To enable its widespread use, H2 supply chain (HSC) infrastructure is required. Mature and economically viable low-carbon H2 production pathways include steam methane reforming (SMR) of natural gas coupled with carbon dioxide capture and storage (CCS), water-electrolysis from renewable electricity, biomethane reforming, and biomass gasification. However, uncertainties surrounding demand and feedstock availabilities hamper their proliferation. Here, we investigate the impact of uncertainty in H2 demand and biomass availability on the optimal HSC design. The HSC is modeled as a network of H2 production and consumption sites that are interconnected with H2 and biomass transport technologies. A CCS supply chain is modeled alongside the HSC. The cost-optimal HSC design is determined based on a linear optimization problem that considers a regional resolution and a multi-year time horizon (2022-2050). We adopt a scenario-based uncertainty quantification approach and define discrete H2 demand and biomass availability scenarios. Applying a minimum-regret strategy, we show that sufficiently large low-carbon H2 production capacities (about 9.6 Mt/a by 2030) are essential to flexibly scale up HSCs and accommodate H2 demands of up to 35 Mt/a by 2050. While biomass-based H2 production emerges as the most cost-efficient low-carbon H2 production pathway, investments are not recommended unless the availability of biomass feedstocks is guaranteed. Instead, investments in SMR-CCS and electrolysis often offer greater flexibility. In addition, we highlight the importance of CCS infrastructure, which is required across scenarios."}, "https://arxiv.org/abs/2407.06149": {"title": "WIBACong: An Argument-centric Framework for Understanding US Congressional Hearings", "link": "https://arxiv.org/abs/2407.06149", "description": "arXiv:2407.06149v1 Announce Type: new \nAbstract: How can we utilize state-of-the-art NLP tools to better understand legislative deliberation? Committee hearings are a core feature of any legislature, and they create an institutional setting to promote the exchange of arguments and reasoning that directly impact and shape legislation. We develop WIBACong, which is an application of the WIBA NLP framework for quantifying and qualifying the deliberation dynamics that we previously developed, applied to US Congressional Hearings. WIBA is a pipeline for extracting and analyzing argumentation communicated within text corpora, enabling a focused attention on the dynamics of debates occurring in democratic settings. With our framework, we are able to uncover the nuances of how deliberative discourse actually is. In WIBACong, we propose a variety of summary statistics and useful visualizations to the WIBA output in order to analyze argumentation in U.S. committee hearing testimony, and in so doing we reveal potential biases in the committee system, and how political parties control the flow of information in 'hot topic' hearings."}, "https://arxiv.org/abs/2407.04701": {"title": "A convenient trick to compute cluster sizes in a Network", "link": "https://arxiv.org/abs/2407.04701", "description": "arXiv:2407.04701v1 Announce Type: cross \nAbstract: We present a convenient trick for computing the sizes of clusters within a network. The rationale relies on the mathematics of the geometric series and the fundamental matrix of a Markov Chain."}, "https://arxiv.org/abs/2407.05126": {"title": "Consistency and Discrepancy-Based Contrastive Tripartite Graph Learning for Recommendations", "link": "https://arxiv.org/abs/2407.05126", "description": "arXiv:2407.05126v1 Announce Type: cross \nAbstract: Tripartite graph-based recommender systems markedly diverge from traditional models by recommending unique combinations such as user groups and item bundles. Despite their effectiveness, these systems exacerbate the longstanding cold-start problem in traditional recommender systems, because any number of user groups or item bundles can be formed among users or items. To address this issue, we introduce a Consistency and Discrepancy-based graph contrastive learning method for tripartite graph-based Recommendation. This approach leverages two novel meta-path-based metrics consistency and discrepancy to capture nuanced, implicit associations between the recommended objects and the recommendees. These metrics, indicative of high-order similarities, can be efficiently calculated with infinite graph convolutional networks layers under a multi-objective optimization framework, using the limit theory of GCN."}, "https://arxiv.org/abs/2407.05186": {"title": "Understanding Political Communication and Political Communicators on Twitch", "link": "https://arxiv.org/abs/2407.05186", "description": "arXiv:2407.05186v1 Announce Type: cross \nAbstract: As new technologies rapidly reshape patterns of political communication, platforms like Twitch are transforming how people consume political information. This entertainment-oriented live streaming platform allows us to observe the impact of technologies such as ``live-streaming'' and ``streaming-chat'' on political communication. Despite its entertainment focus, Twitch hosts a variety of political actors, including politicians and pundits. This study explores Twitch politics by addressing three main questions: 1) Who are the political Twitch streamers? 2) What content is covered in political streams? 3) How do audiences of political streams interact with each other? To identify political streamers, I leveraged the Twitch API and supervised machine-learning techniques, identifying 574 political streamers. I used topic modeling to analyze the content of political streams, revealing seven broad categories of political topics and a unique pattern of communication involving context-specific ``emotes.'' Additionally, I created user-reference networks to examine interaction patterns, finding that a small number of users dominate the communication network. This research contributes to our understanding of how new social media technologies influence political communication, particularly among younger audiences."}, "https://arxiv.org/abs/2407.05627": {"title": "New Directions in Text Classification Research: Maximizing The Performance of Sentiment Classification from Limited Data", "link": "https://arxiv.org/abs/2407.05627", "description": "arXiv:2407.05627v1 Announce Type: cross \nAbstract: The stakeholders' needs in sentiment analysis for various issues, whether positive or negative, are speed and accuracy. One new challenge in sentiment analysis tasks is the limited training data, which often leads to suboptimal machine learning models and poor performance on test data. This paper discusses the problem of text classification based on limited training data (300 to 600 samples) into three classes: positive, negative, and neutral. A benchmark dataset is provided for training and testing data on the issue of Kaesang Pangarep's appointment as Chairman of PSI. External data for aggregation and augmentation purposes are provided, consisting of two datasets: the topic of Covid Vaccination sentiment and an open topic. The official score used is the F1-score, which balances precision and recall among the three classes, positive, negative, and neutral. A baseline score is provided as a reference for researchers for unoptimized classification methods. The optimized score is provided as a reference for the target score to be achieved by any proposed method. Both scoring (baseline and optimized) use the SVM method, which is widely reported as the state-of-the-art in conventional machine learning methods. The F1-scores achieved by the baseline and optimized methods are 40.83% and 51.28%, respectively."}, "https://arxiv.org/abs/2407.05801": {"title": "Design of a multisensory planetarium", "link": "https://arxiv.org/abs/2407.05801", "description": "arXiv:2407.05801v1 Announce Type: cross \nAbstract: We present the design and the prototype of a multisensory planetarium. The goal of this project is to offer a common perceptual and cognitive framework to all users, both sighted, deaf, and blind or visually impaired, concerning the experience of observing the night sky, but also to provide all equal access to scientific data regarding the observed objects, going beyond what can be sensed. The planetarium will consist of a Plexiglas hemisphere on which stars up to the fourth magnitude are represented by a brass bar that, when touched, activates visual, haptic, and acoustic stimuli. We mapped the magnitude of stars on acoustic and visual stimuli, while the distance of the star from us is mapped on a vibration. All the stimuli have been evaluated in pilot experiments using Plexiglas tablets representing some constellations, to assess their usability, intelligibility, and consistency with possible intuitive interpretations."}, "https://arxiv.org/abs/2407.05963": {"title": "6GSoft: Software for Edge-to-Cloud Continuum", "link": "https://arxiv.org/abs/2407.05963", "description": "arXiv:2407.05963v1 Announce Type: cross \nAbstract: In the era of 6G, developing and managing software requires cutting-edge software engineering (SE) theories and practices tailored for such complexity across a vast number of connected edge devices. Our project aims to lead the development of sustainable methods and energy-efficient orchestration models specifically for edge environments, enhancing architectural support driven by AI for contemporary edge-to-cloud continuum computing. This initiative seeks to position Finland at the forefront of the 6G landscape, focusing on sophisticated edge orchestration and robust software architectures to optimize the performance and scalability of edge networks. Collaborating with leading Finnish universities and companies, the project emphasizes deep industry-academia collaboration and international expertise to address critical challenges in edge orchestration and software architecture, aiming to drive significant advancements in software productivity and market impact."}, "https://arxiv.org/abs/2306.06683": {"title": "To be a pro-vax or not, the COVID-19 vaccine conundrum on Twitter", "link": "https://arxiv.org/abs/2306.06683", "description": "arXiv:2306.06683v2 Announce Type: replace \nAbstract: The most surprising observation reported by the study in (arXiv:2208.13523), involving stance detection of COVID-19 vaccine related tweets during the first year of pandemic, is the presence of a significant number of users (~2 million) who posted tweets with both anti-vax and pro-vax stances. This is a sizable cohort even when the stance detection noise is considered. In this paper, we tried to get deeper understanding of this 'dual-stance' group. Out of this group, 60% of users have more pro-vax tweets than anti-vax tweets and 17% have the same number of tweets in both classes. The rest have more anti-vax tweets, and they were highly active in expressing concerns about mandate and safety of a fast-tracked vaccine, while also tweeted some updates about vaccine development. The leaning pro-vax group have opposite composition: more vaccine updates and some posts about concerns. It is important to note that vaccine concerns were not always genuine and had a large dose of misinformation. 43% of the balanced group have only tweeted one tweet of each type during our study period and are the less active participants in the vaccine discourse. Our temporal study also shows that the change-of-stance behaviour became really significant once the trial results of COVID-19 vaccine were announced to the public, and it appears as the change of stance towards pro-vax is a reaction to people changing their opinion towards anti-vax. Our study finished at Mar 23, 2021 when the conundrum was still going strong. The dilemma might be a reflection of the uncertain and stressful times, but it also highlights the importance of building public trust to combat prevalent misinformation."}, "https://arxiv.org/abs/2308.04700": {"title": "BOPIM: Bayesian Optimization for influence maximization on temporal networks", "link": "https://arxiv.org/abs/2308.04700", "description": "arXiv:2308.04700v2 Announce Type: replace \nAbstract: The goal of influence maximization (IM) is to select a small set of seed nodes which maximizes the spread of influence on a network. In this work, we propose BOPIM, a Bayesian Optimization (BO) algorithm for IM on temporal networks. The IM task is well-suited for a BO solution due to its expensive and complicated objective function. We propose a simple surrogate function to model the objective function and leverage Gaussian Process regression with shrinkage priors to fit the model. A major difficulty in combinatorial BO is constructing an appropriate covariance matrix. We overcome this challenge with a kernel function based on the amount of overlap in seed set neighbors, thus tailoring the solution to our IM application. This also allows us to use the Expected Improvement acquisition function to choose the next point to evaluate. In numerical experiments on real-world networks, we prove that BOPIM outperforms competing methods and yields comparable influence spreads to a gold-standard greedy algorithm while being as much as five times faster. We also demonstrate the proposed method's ability to quantify uncertainty in optimal seed sets. To our knowledge, this is the first attempt to look at uncertainty in the seed sets for IM."}, "https://arxiv.org/abs/2309.15837": {"title": "Energy and environmental impacts of air-to-air heat pumps in a mid-latitude city", "link": "https://arxiv.org/abs/2309.15837", "description": "arXiv:2309.15837v3 Announce Type: replace \nAbstract: Heat pumps (HPs) have emerged as a key technology for reducing energy use and greenhouse gas emissions. This study evaluates the potential switch to air-to-air HPs (AAHPs) in Toulouse, France, where conventional space heating is split between electric and gas sources. In this context, we find that AAHPs reduce heating energy consumption by 57% to 76%, with electric heating energy consumption decreasing by 6% to 47%, resulting in virtually no local heating-related CO$_{2}$ emissions. We observe a slight reduction in near-surface air temperature of up to 0.5 {\\deg}C during cold spells, attributable to a reduction in sensible heat flux, which is unlikely to compromise AAHPs operational efficiency. While Toulouse's heating energy mix facilitates large energy savings, electric energy consumption may increase in cities where gas or other fossil fuel sources prevail. Furthermore, as AAHPs efficiency varies with internal and external conditions, their impact on the electrical grid is more complex than conventional heating systems. The results underscore the importance of matching heating system transitions with sustainable electricity generation to maximize environmental benefits. The study highlights the intricate balance between technological advancements in heating and their broader environmental and policy implications, offering key insights for urban energy policy and sustainability efforts."}, "https://arxiv.org/abs/2402.03100": {"title": "Inter-city infections and the role of size heterogeneity in containment strategies", "link": "https://arxiv.org/abs/2402.03100", "description": "arXiv:2402.03100v2 Announce Type: replace \nAbstract: We study the effectiveness of regional lockdown strategies to mitigate the spread of a pathogen across regional units, in the following called cities, within a country or region for a single infection wave. The heterogeneity in the epidemically relevant connectivity is defined via a random network model with cities as nodes, where the city's sizes determine their connectivity via a gravity type kernel function. Isolation of a whole city is initiated when infection numbers surpass defined thresholds. We consider two basic strategies for the lockdowns. Strategy~$(P)$ isolates cities based on a proportional threshold of infections, while stra\\-tegy~\\((U)\\) uses a uniform infection threshold for all cities. Given the heavy-tailed distribution of city sizes, strategy \\((P)\\) can potentially result in more secondary infections from larger cities than strategy \\((U)\\). As an efficiency measure we use the ratio of individuals under lockdown and the number of infected individuals. Additionally, we analytically derive formulas for the basic reproduction numbers and prevalences. Our model is fitted to mobility data from France, Japan, and Poland, and validated through simulations. The findings indicate that while the model aligns well with data from France and Poland, it deviates in Japan, highlighting the importance of geographical nuances in pathogen spread modeling. Furthermore, it suggests that for France (and Japan) both strategies perform equally well, while for Poland strategy \\((U)\\) outperforms strategy \\((P)\\)."}, "https://arxiv.org/abs/2310.00394": {"title": "Connectivity Aware and Energy Efficient Self-Organizing Distributed IoT Topology Control", "link": "https://arxiv.org/abs/2310.00394", "description": "arXiv:2310.00394v2 Announce Type: replace-cross \nAbstract: Internet of Things has pervaded every area of modern life. From a research and industry standpoint, there has been an increasing demand and desire in recent years to develop Internet of Things networks with distributed structure. Wireless communication under emergency circumstances is one of the important applications that distributed Internet of Things can have. In order for a network to be functional in this scenario, it must be developed without the aid of a pre-established or centralized structure and operated in a self-organized manner to accommodate the communication requirements of the time. Although the design and development of such networks can be highly advantageous, they frequently confront difficulties, the most significant of which is attaining and maintaining effective connectivity to have reliable communications despite the requirement to optimize energy usage. In this study, we present a model for self-organizing topology control for ad hoc-based Internet of Things networks that can address the aforementioned challenges. The model that will be presented employs the notion of the Hamiltonian function in classical mechanics and has two key objectives: regulating the network's topology and dynamics to enhance connectivity to a desirable level while requiring the least amount of energy possible. The results of the simulation indicate that the proposed model satisfactorily fulfills the goals of the problem."}, "https://arxiv.org/abs/2312.06441": {"title": "Revisiting Graph-Based Fraud Detection in Sight of Heterophily and Spectrum", "link": "https://arxiv.org/abs/2312.06441", "description": "arXiv:2312.06441v3 Announce Type: replace-cross \nAbstract: Graph-based fraud detection (GFD) can be regarded as a challenging semi-supervised node binary classification task. In recent years, Graph Neural Networks (GNN) have been widely applied to GFD, characterizing the anomalous possibility of a node by aggregating neighbor information. However, fraud graphs are inherently heterophilic, thus most of GNNs perform poorly due to their assumption of homophily. In addition, due to the existence of heterophily and class imbalance problem, the existing models do not fully utilize the precious node label information. To address the above issues, this paper proposes a semi-supervised GNN-based fraud detector SEC-GFD. This detector includes a hybrid filtering module and a local environmental constraint module, the two modules are utilized to solve heterophily and label utilization problem respectively. The first module starts from the perspective of the spectral domain, and solves the heterophily problem to a certain extent. Specifically, it divides the spectrum into various mixed-frequency bands based on the correlation between spectrum energy distribution and heterophily. Then in order to make full use of the node label information, a local environmental constraint module is adaptively designed. The comprehensive experimental results on four real-world fraud detection datasets denote that SEC-GFD outperforms other competitive graph-based fraud detectors. We release our code at https://github.com/Sunxkissed/SEC-GFD."}, "https://arxiv.org/abs/2407.06197": {"title": "On the Relation between Graph Ricci Curvature and Community Structure", "link": "https://arxiv.org/abs/2407.06197", "description": "arXiv:2407.06197v1 Announce Type: new \nAbstract: The connection between curvature and topology is a very well-studied theme in the subject of differential geometry. By suitably defining curvature on networks, the study of this theme has been extended into the domain of network analysis as well. In particular, this has led to curvature-based community detection algorithms. In this paper, we reveal the relation between community structure of a network and the curvature of its edges. In particular, we give apriori bounds on the curvature of intercommunity edges of a graph."}, "https://arxiv.org/abs/2407.06198": {"title": "Time-dependent Personalized PageRank for temporal networks: discrete and continuous scales", "link": "https://arxiv.org/abs/2407.06198", "description": "arXiv:2407.06198v1 Announce Type: new \nAbstract: In this paper we explore the PageRank of temporal networks on both discrete and continuous time scales in the presence of personalization vectors that vary over time. Also the underlying interplay between the discrete and continuous settings arising from discretization is highlighted. Additionally, localization results that set bounds to the estimated influence of the personalization vector on the ranking of a particular node are given. The theoretical results are illustrated by means of some real and synthetic examples."}, "https://arxiv.org/abs/2407.06205": {"title": "Chronological Analysis of Rigvedic Mandalas using Social Networks", "link": "https://arxiv.org/abs/2407.06205", "description": "arXiv:2407.06205v1 Announce Type: new \nAbstract: Establishing the chronology of the Vedas has interested scholars for the last two centuries. The oldest among them is Rig-Veda which has ten Mandalas, each composed separately. In this paper, we look at deciphering plausible pointers to the internal chronology of the Mandalas, by focusing on Gods and Goddesses worshiped in different Mandalas. We apply text analysis to the Mandalas using Clustering Techniques based on Cosine Similarity. Then we represent the association of deities with Mandalas using a grid-based Social Network that is amenable to chronological analysis and demonstrates the benefits of using Social Network Analysis for the problem at hand. Further, we analyze references to rivers to arrive at additional correlations. The approach used can be deployed generically to analyze other kinds of references and mentions and arrive at more substantive inferences."}, "https://arxiv.org/abs/2407.06351": {"title": "Bounded confidence modeling predicts how group work affects student math anxiety", "link": "https://arxiv.org/abs/2407.06351", "description": "arXiv:2407.06351v1 Announce Type: new \nAbstract: Math anxiety is ubiquitous. It not only affects student performance and confidence, but also can lead to avoidance of further math/STEM classes and careers. Cooperative learning (i.e., group work) is a proven strategy that can reduce math anxiety and has additional social and pedagogical benefits. However, depending on the individuals involved, some peer interactions may mitigate anxiety while others exacerbate it. Mathematical modeling is one approach to help untangle this complex dynamic. In this work we introduce a bounded confidence model to evaluate how math anxiety levels are affected by student group work. Although the model is quite simple, it captures non-obvious phenomena including how varying group sizes and frequency of switching groups can affect anxiety levels. The model is easily adaptable to incorporate additional personal and societal factors making it ripe for future research."}, "https://arxiv.org/abs/2407.06359": {"title": "Topological Determinants of Resilience in Urban Rail Networks Facing Multi-Hazard Disruptions", "link": "https://arxiv.org/abs/2407.06359", "description": "arXiv:2407.06359v1 Announce Type: new \nAbstract: This study examines the failure and recovery, two key components of resilience of nine major urban rail networks - Washington DC, Boston, Chicago, Delhi, Tokyo, Paris, Shanghai, London, and New York - against multi-hazard scenarios utilizing a quantitative approach focused on topological parameters to evaluate network resilience. Employing percolation-based network dismantling approach like Sequential Removal of Nodes and Giant Connected Component analysis, alongside random, centrality-based targeted attacks and flooding failure, findings reveal Domirank centrality's superior resilience in disruption and recovery phases. Kendall's tau coefficient's application further elucidates the relationships between network properties and resilience, underscoring larger networks' vulnerability yet faster recovery due to inherent redundancy and connectivity. Key attributes like average degree and path length consistently influence recovery effectiveness, while the clustering coefficient's positive correlation with recovery highlights the benefits of local interconnectivity. This analysis emphasizes the critical role of select nodes and the importance of balancing network design for enhanced resilience, offering insights for future urban rail system planning against multi-hazard threats."}, "https://arxiv.org/abs/2407.06558": {"title": "Sampling-Based Attack for Centrality Disruption in Complex Networks", "link": "https://arxiv.org/abs/2407.06558", "description": "arXiv:2407.06558v1 Announce Type: new \nAbstract: Many mobile networks are represented as graphs to obtain insight to their connectivity and transmission properties. Among these properties centrality resilience, that is, how well centralities, such as closeness and betweennesss, are maintained under attacks is a critical factor for proper functioning of a network. In this paper, we study the centrality resilience of complex networks by developing attack models to disrupt the rank of the top path-based centrality vertices. To develop our attack models, we extend the concept of rich clubs of influential vertices to the more general framework of scattered rich clubs. We define scattered rich clubs as dense subgraphs of high centrality vertices that are spread (scattered) across the network. Finding scattered rich clubs, although of polynomial time complexity, is extremely expensive computationally. We use snowball sampling to identify these important substructures as well as to identify which edges to target in our proposed attack models. Our results over a set of real world networks demonstrate that our proposed algorithm is effective in finding the single or scattered rich clubs efficiently and in successfully disrupting the centrality rankings of the network. To summarize, we propose sampling-based attack models for testing the resilience of networks with respect to centrality rankings. As part of this process, we introduce scattered rich clubs, a generalized form of the rich club model, efficient algorithms to detect them, and demonstrate their relation to network resilience."}, "https://arxiv.org/abs/2407.06631": {"title": "A Systematic Review of Echo Chamber Research: Comparative Analysis of Conceptualizations, Operationalizations, and Varying Outcomes", "link": "https://arxiv.org/abs/2407.06631", "description": "arXiv:2407.06631v1 Announce Type: new \nAbstract: This systematic review synthesizes current research on echo chambers and filter bubbles to highlight the reasons for the dissent in echo chamber research on the existence, antecedents, and effects of the phenomenon. The review of 112 studies reveals that the lack of consensus in echo chamber research is based on different conceptualizations and operationalizations of echo chambers. While studies that have conceptualized echo chambers with homophily and utilized data-driven computational social science (CSS) methods have confirmed the echo chamber hypothesis and polarization effects in social media, content exposure studies and surveys that have explored the full spectrum of media exposure have rejected it.\n  Most of these studies have been conducted in the United States, and the review emphasizes the need for a more comprehensive understanding of how echo chambers work in systems with more than two parties and outside the Global North. To advance our understanding of this phenomenon, future research should prioritize conducting more cross-platform studies, considering algorithmic filtering changes through continuous auditing, and examining the causal direction of the association between polarization, fragmentation, and the establishment of online echo chambers. The review also provides the advantages and disadvantages of different operationalizations and makes recommendations for studies in the European Union (EU), which will become possible with the upcoming Digital Services Act (DSA). Overall, this systematic review contributes to the ongoing scholarly discussion on the existence, antecedents, and effects of echo chambers and filter bubbles."}, "https://arxiv.org/abs/2407.06793": {"title": "Paradise-disorder transition in structural balance dynamics on Erd\\\"os-R\\'enyi graphs", "link": "https://arxiv.org/abs/2407.06793", "description": "arXiv:2407.06793v1 Announce Type: new \nAbstract: Structural balance has been posited as one of the factors influencing how friendly and hostile relations of social actors evolve over time. This study investigates the behavior of the Heider balance model in Erd\\\"os-R\\'enyi random graphs in the presence of a noisy environment, particularly the transition from an initially entirely positively polarized paradise state to a disordered phase. We examine both single-layer and bilayer network configurations and provide a mean-field solution for the average link polarization that predicts a first-order transition where the critical temperature scales with the connection probability $p$ as $p^2$ for a monolayer system and in a more complex way for a bilayer. We show that to mimic the dynamics observed in complete graphs, the intralayer Heider interaction strengths should be scaled as $p^{-2}$, while the interlayer interaction strengths should be scaled as $p^{-1}$ for random graphs. Numerical simulations have been performed, and their results confirm our analytical predictions, provided that graphs are dense enough."}, "https://arxiv.org/abs/2407.06820": {"title": "From Contact to Threat: A Social Network Perspective on Perceptions of Immigration", "link": "https://arxiv.org/abs/2407.06820", "description": "arXiv:2407.06820v1 Announce Type: new \nAbstract: Our perceptions are shaped by the social networks we are embedded in. Despite the acknowledged influence of close contacts on how we perceive the world, the role of the broader social environment remains opaque. Here, we leverage a unique combination of population-scale social network and survey data on perceptions of immigration. We find that both direct contacts and a wider social network exposure to migrants matter. Notably, for natives, network exposure shows a shift from positive to negative association with perceptions of immigration beyond a certain exposure threshold. The multi-layer nature of our data highlights this tipping point for next-door neighbors, with private social contexts exhibiting a positive relationship between exposure and immigration perceptions. Furthermore, it shows that contacts spanning multiple contexts also strengthen this relationship. The provided insights on the interplay between network composition and attitudes toward immigration highlight generic patterns shaping public opinion on pressing societal issues."}, "https://arxiv.org/abs/2407.06998": {"title": "Changepoint Detection in Highly-Attributed Dynamic Graphs", "link": "https://arxiv.org/abs/2407.06998", "description": "arXiv:2407.06998v1 Announce Type: new \nAbstract: Detecting anomalous behavior in dynamic networks remains a constant challenge. This problem is further exacerbated when the underlying topology of these networks is affected by individual highly-dimensional node attributes. We address this issue by tracking a network's modularity as a proxy of its community structure. We leverage Graph Neural Networks (GNNs) to estimate each snapshot's modularity. GNNs can account for both network structure and high-dimensional node attributes, providing a comprehensive approach for estimating network statistics. Our method is validated through simulations that demonstrate its ability to detect changes in highly-attributed networks by analyzing shifts in modularity. Moreover, we find our method is able to detect a real-world event within the \\#Iran Twitter reply network, where each node has high-dimensional textual attributes."}, "https://arxiv.org/abs/2407.06813": {"title": "Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy", "link": "https://arxiv.org/abs/2407.06813", "description": "arXiv:2407.06813v1 Announce Type: cross \nAbstract: Diplomacy is one of the most sophisticated activities in human society. The complex interactions among multiple parties/ agents involve various abilities like social reasoning, negotiation arts, and long-term strategy planning. Previous AI agents surely have proved their capability of handling multi-step games and larger action spaces on tasks involving multiple agents. However, diplomacy involves a staggering magnitude of decision spaces, especially considering the negotiation stage required. Recently, LLM agents have shown their potential for extending the boundary of previous agents on a couple of applications, however, it is still not enough to handle a very long planning period in a complex multi-agent environment. Empowered with cutting-edge LLM technology, we make the first stab to explore AI's upper bound towards a human-like agent for such a highly comprehensive multi-agent mission by combining three core and essential capabilities for stronger LLM-based societal agents: 1) strategic planner with memory and reflection; 2) goal-oriented negotiate with social reasoning; 3) augmenting memory by self-play games to self-evolving without any human in the loop."}, "https://arxiv.org/abs/2407.07026": {"title": "Resolving Sentiment Discrepancy for Multimodal Sentiment Detection via Semantics Completion and Decomposition", "link": "https://arxiv.org/abs/2407.07026", "description": "arXiv:2407.07026v1 Announce Type: cross \nAbstract: With the proliferation of social media posts in recent years, the need to detect sentiments in multimodal (image-text) content has grown rapidly. Since posts are user-generated, the image and text from the same post can express different or even contradictory sentiments, leading to potential \\textbf{sentiment discrepancy}. However, existing works mainly adopt a single-branch fusion structure that primarily captures the consistent sentiment between image and text. The ignorance or implicit modeling of discrepant sentiment results in compromised unimodal encoding and limited performances. In this paper, we propose a semantics Completion and Decomposition (CoDe) network to resolve the above issue. In the semantics completion module, we complement image and text representations with the semantics of the OCR text embedded in the image, helping bridge the sentiment gap. In the semantics decomposition module, we decompose image and text representations with exclusive projection and contrastive learning, thereby explicitly capturing the discrepant sentiment between modalities. Finally, we fuse image and text representations by cross-attention and combine them with the learned discrepant sentiment for final classification. Extensive experiments conducted on four multimodal sentiment datasets demonstrate the superiority of CoDe against SOTA methods."}, "https://arxiv.org/abs/2105.09191": {"title": "Structural dynamics of plant-pollinator mutualistic networks", "link": "https://arxiv.org/abs/2105.09191", "description": "arXiv:2105.09191v3 Announce Type: replace \nAbstract: The discourse surrounding the structural organization of mutualistic interactions mostly revolves around modularity and nestedness. The former is known to enhance the stability of communities, while the latter is related to their feasibility, albeit compromising the stability. However, it has recently been shown that the joint emergence of these structures poses challenges that can eventually lead to limitations in the dynamic properties of mutualistic communities. We hypothesize that considering compound arrangements -- modules with internal nested organization -- can offer valuable insights in this debate. We analyze the temporal structural dynamics of 20 plant-pollinator interaction networks and observe large structural variability throughout the year. Compound structures are particularly prevalent during the peak of the pollination season, often coexisting with nested and modular arrangements in varying degrees. Motivated by these empirical findings, we synthetically investigate the dynamics of the structural patterns across two control parameters -- community size and connectance levels -- mimicking the progression of the pollination season. Our analysis reveals contrasting impacts on the stability and feasibility of these mutualistic communities. We characterize the consistent relationship between network structure and stability, which follows a monotonic pattern. But, in terms of feasibility, we observe non-linear relationships. Compound structures exhibit a favorable balance between stability and feasibility, particularly in mid-sized ecological communities, suggesting they may effectively navigate the simultaneous requirements of stability and feasibility. These findings may indicate that the assembly process of mutualistic communities is driven by a delicate balance among multiple properties, rather than the dominance of a single one."}, "https://arxiv.org/abs/2306.09967": {"title": "The temporal dynamics of group interactions in higher-order social networks", "link": "https://arxiv.org/abs/2306.09967", "description": "arXiv:2306.09967v3 Announce Type: replace \nAbstract: Representing social systems as networks, starting from the interactions between individuals, sheds light on the mechanisms governing their dynamics. However, networks encode only pairwise interactions, while most social interactions occur among groups of individuals, requiring higher-order network representations. Despite the recent interest in higher-order networks, little is known about the mechanisms that govern the formation and evolution of groups, and how people move between groups. Here, we leverage empirical data on social interactions among children and university students to study their temporal dynamics at both individual and group levels, characterising how individuals navigate groups and how groups form and disaggregate. We find robust patterns across contexts and propose a dynamical model that closely reproduces empirical observations. These results represent a further step in understanding social systems, and open up research directions to study the impact of group dynamics on dynamical processes that evolve on top of them."}, "https://arxiv.org/abs/2403.01065": {"title": "Perspective: Challenges and opportunities for high-quality battery production at scale", "link": "https://arxiv.org/abs/2403.01065", "description": "arXiv:2403.01065v3 Announce Type: replace \nAbstract: As the impacts of climate change become increasingly apparent, the need for widespread electrification is now internationally recognized. As a result, global battery production is set to dramatically expand over the next decade. Unfortunately, however, batteries are both immensely difficult to produce at the gigawatt-hour scale and inordinately sensitive to minor manufacturing variation. As a result, the battery industry has already experienced a number of both highly-visible safety incidents and under-the-radar reliability issues -- a trend that will only worsen if left unaddressed. In this perspective, we highlight both the challenges and opportunities to enable battery quality at scale. We first describe the interplay between various battery failure modes and their numerous root causes. We then discuss the tensions at play to manage and improve battery quality during cell production. We hope our perspective brings greater visibility to the battery quality challenge to enable safe global electrification."}, "https://arxiv.org/abs/2407.07115": {"title": "Analysis of Seawater Quality Parameter and Treatment with Hydrodynamic Cavitation Method", "link": "https://arxiv.org/abs/2407.07115", "description": "arXiv:2407.07115v1 Announce Type: new \nAbstract: The aim of the research was to compare the quality parameters of Seawater before and after hydrodynamic cavitation treatment. The Hydrodynamic Cavitation Method for water treatment gives the highest reduction in Turbidity (100%), the second highest reduction in TSS (83.86%), and the lowest reduction in Na+ (8.47%), according to the study and analysis of various quality parameters. When compared to CPCB Water Quality Criteria, treated water is suitable for outdoor bathing; it again satisfies the standards for classes SW-I, SW-II, SW-III, SW-IV and SW-V, i.e., treated sea water can be used for a variety of purposes, including bathing, contact water sports, commercial fishing, mariculture, ecologically sensitive zones, aesthetics, harbour, waters Navigation and Controlled water disposal. The SAR value of treated water, which is 1.72, indicates it is appropriate for all types of soil and crops. The treated water's WQI, which is 65, showed that it is of Fair quality and may be used for industrial and irrigation uses. Water quality can be improved by recycling the treated water for an additional 24 hours using the hydrodynamic cavitation method. Recycling of one time treated sea water will result in higher-quality water that can be used for a variety of purposes.\n  The Hydrodynamic Cavitation method by using venturi - orifice is proven to be the most effective over the other methods. Because it does not require any chemical reagent, hence does not produce any hazardous chemical waste, and maintains an eco-friendly and economically sustainable, environment benign technique for the treatment of Seawater."}, "https://arxiv.org/abs/2407.07118": {"title": "Parameter estimation of epidemic spread in two-layer random graphs by classical and machine learning methods", "link": "https://arxiv.org/abs/2407.07118", "description": "arXiv:2407.07118v1 Announce Type: new \nAbstract: Our main goal in this paper is to quantitatively compare the performance of classical methods to XGBoost and convolutional neural networks in a parameter estimation problem for epidemic spread. As we use flexible two-layer random graphs as the underlying network, we can also study how much the structure of the graphs in the training set and the test set can differ while to get a reasonably good estimate. In addition, we also examine whether additional information (such as the average degree of infected vertices) can help improving the results, compared to the case when we only know the time series consisting of the number of susceptible and infected individuals. Our simulation results also show which methods are most accurate in the different phases of the epidemic."}, "https://arxiv.org/abs/2407.07119": {"title": "Optimal bias of utility function between two-layer network for the evolution of prosocial behavior in two-order game and higher-order game", "link": "https://arxiv.org/abs/2407.07119", "description": "arXiv:2407.07119v1 Announce Type: new \nAbstract: Cooperation is an important research object in economics, sociology, and biology, and the evolution of cooperation in structured populations is a interesting research topic. We mainly focus on the evolution of cooperation with two-order and higher-order game in two-layer network. We introduce a bias coefficient of utility function and study the influence of bias coefficient on the evolution of cooperation in two-layer network. We firstly provide theoretical analysis of fixation probabilities of two-order and higher-order game under weak selection in two-layer network.Secondly,based on the expression of fixation probability, we obtain the critical value of the two different games by comparing the size relationship of fixation probability under weak selection condition and neutral selection condition. Finally, by comparing the relationship between the critical value of single-layer and two-layer network in two-order game and higher-order game, when the nonlinear factor satisfies certain conditions, it is concluded that when the optimal bias coefficient tends towards 0 is met, some two-layer networks promote the evolution of cooperative behavior more than some single-layer networks."}, "https://arxiv.org/abs/2407.07127": {"title": "A non-conservative kinetic framework for a closed-market society subject to shock events", "link": "https://arxiv.org/abs/2407.07127", "description": "arXiv:2407.07127v1 Announce Type: new \nAbstract: Recently, several events have shockingly impacted society, carrying tough consequences. This was evident in the recent COVID-19 pandemic. However, not all individuals are affected by shock events in the same way. Among other factors, the consequences can vary depending on the wealth class. In our presented work, the approach typical of kinetic theory is used to analyze the dynamics of a closed-market society exposed to various types of shock events. To achieve this, we introduce non-conservative equations, incorporating proliferative and destructive binary interactions as well as external actions. Specifically, the latter term reproduces the shock events, and to accomplish this, we introduce an appropriate external force field into the kinetic framework, modeled using Gaussian functions. Several numerical simulations exploring different scenarios are presented to illustrate the behavior of the solution predicted by the model and to gain some insights when complex situations are investigated."}, "https://arxiv.org/abs/2407.07138": {"title": "Support and Scandals in GameFi dApps: A Network Analysis of The Sandbox Transactions", "link": "https://arxiv.org/abs/2407.07138", "description": "arXiv:2407.07138v1 Announce Type: new \nAbstract: We explore the burgeoning field of GameFi through a detailed network analysis of The Sandbox, a prominent decentralized application (dApp) in this domain. Utilizing the bow-tie model, we map out transaction data within The Sandbox, providing a novel perspective on its operational dynamics. Our study investigates the varying impacts of external support, uncovering a surprising absence of enduring effects on network activity. We also investigate the network's response to several notable incidents, including the Ronin Hack and the United States Securities and Exchange Commission's hearing on cryptocurrencies, revealing a generally resilient structure with limited long-term disturbances. A critical aspect of our analysis focuses on the \"whales,\" or major stakeholders in The Sandbox, where we uncover their pivotal role in influencing network trends, noting a significant shift in their engagement over time. This research sheds light on the intricate workings of GameFi ecosystems and contributes to the broader discourse on the intersection of the Web, AI, and society, particularly in understanding the resilience and dynamics of emerging digital economies. We particularly note the parallels of the long-tail behavior we see in web-based ecosystems appearing in this niche domain of GameFi. Our findings hold significant implications for the future development of equitable and sustainable GameFi dApps, offering insights into stakeholder behavior and network resilience in the face of external challenges and opportunities."}, "https://arxiv.org/abs/2407.07227": {"title": "Uncovering the Interaction Equation: Quantifying the Effect of User Interactions on Social Media Homepage Recommendations", "link": "https://arxiv.org/abs/2407.07227", "description": "arXiv:2407.07227v1 Announce Type: new \nAbstract: Social media platforms depend on algorithms to select, curate, and deliver content personalized for their users. These algorithms leverage users' past interactions and extensive content libraries to retrieve and rank content that personalizes experiences and boosts engagement. Among various modalities through which this algorithmically curated content may be delivered, the homepage feed is the most prominent. This paper presents a comprehensive study of how prior user interactions influence the content presented on users' homepage feeds across three major platforms: YouTube, Reddit, and X (formerly Twitter). We use a series of carefully designed experiments to gather data capable of uncovering the influence of specific user interactions on homepage content. This study provides insights into the behaviors of the content curation algorithms used by each platform, how they respond to user interactions, and also uncovers evidence of deprioritization of specific topics."}, "https://arxiv.org/abs/2407.07301": {"title": "Higher-order Fuzzy Membership in Motif Modularity Optimization", "link": "https://arxiv.org/abs/2407.07301", "description": "arXiv:2407.07301v1 Announce Type: new \nAbstract: Higher-order community detection (HCD) reveals both mesoscale structures and functional characteristics of real-life networks. Although many methods have been developed from diverse perspectives, to our knowledge, none can provide fine-grained higher-order fuzzy community information. This study presents a novel concept of higher-order fuzzy memberships that quantify the membership grades of motifs to crisp higher-order communities, thereby revealing the partial community affiliations. Furthermore, we employ higher-order fuzzy memberships to enhance HCD via a general framework called fuzzy memberships assisted motif-based evolutionary modularity (FMMEM). In FFMEM, on the one hand, a fuzzy membership-based neighbor community modification (FM-NCM) strategy is designed to correct misassigned bridge nodes, thereby improving partition quality. On the other hand, a fuzzy membership-based local community merging (FM-LCM) strategy is also proposed to combine excessively fragmented communities for enhancing local search ability. Experimental results indicate that the FMMEM framework outperforms state-of-the-art methods in both synthetic and real-world datasets, particularly in the networks with ambiguous and complex structures."}, "https://arxiv.org/abs/2407.07599": {"title": "Can social media shape the security of next-generation connected vehicles?", "link": "https://arxiv.org/abs/2407.07599", "description": "arXiv:2407.07599v1 Announce Type: new \nAbstract: The increasing adoption of connectivity and electronic components in vehicles makes these systems valuable targets for attackers. While automotive vendors prioritize safety, there remains a critical need for comprehensive assessment and analysis of cyber risks. In this context, this paper proposes a Social Media Automotive Threat Intelligence (SOCMATI) framework, specifically designed for the emerging field of automotive cybersecurity. The framework leverages advanced intelligence techniques and machine learning models to extract valuable insights from social media. Four use cases illustrate the framework's potential by demonstrating how it can significantly enhance threat assessment procedures within the automotive industry."}, "https://arxiv.org/abs/2407.07096": {"title": "Spectral Toolkit of Algorithms for Graphs: Technical Report (2)", "link": "https://arxiv.org/abs/2407.07096", "description": "arXiv:2407.07096v1 Announce Type: cross \nAbstract: Spectral Toolkit of Algorithms for Graphs (STAG) is an open-source library for efficient graph algorithms. This technical report presents the newly implemented component on locality sensitive hashing, kernel density estimation, and fast spectral clustering. The report includes a user's guide to the newly implemented algorithms, experiments and demonstrations of the new functionality, and several technical considerations behind our development."}, "https://arxiv.org/abs/2407.07123": {"title": "Differential Equations and Applications to COVID-19", "link": "https://arxiv.org/abs/2407.07123", "description": "arXiv:2407.07123v1 Announce Type: cross \nAbstract: This paper focuses on the application of the Verhulst logistic equation to model in retrospective the total COVID-19 cases in Senegal during the period from April 2022 to April 2023. Our predictions for April 2023 are compared with the real COVID-19 data for April 2023 to assess the accuracy of the model. The data analysis is conducted using Python programming language, which allows for efficient data processing and prediction generation."}, "https://arxiv.org/abs/2407.07128": {"title": "Modularity aided consistent attributed graph clustering via coarsening", "link": "https://arxiv.org/abs/2407.07128", "description": "arXiv:2407.07128v1 Announce Type: cross \nAbstract: Graph clustering is an important unsupervised learning technique for partitioning graphs with attributes and detecting communities. However, current methods struggle to accurately capture true community structures and intra-cluster relations, be computationally efficient, and identify smaller communities. We address these challenges by integrating coarsening and modularity maximization, effectively leveraging both adjacency and node features to enhance clustering accuracy. We propose a loss function incorporating log-determinant, smoothness, and modularity components using a block majorization-minimization technique, resulting in superior clustering outcomes. The method is theoretically consistent under the Degree-Corrected Stochastic Block Model (DC-SBM), ensuring asymptotic error-free performance and complete label recovery. Our provably convergent and time-efficient algorithm seamlessly integrates with graph neural networks (GNNs) and variational graph autoencoders (VGAEs) to learn enhanced node features and deliver exceptional clustering performance. Extensive experiments on benchmark datasets demonstrate its superiority over existing state-of-the-art methods for both attributed and non-attributed graphs."}, "https://arxiv.org/abs/2407.07159": {"title": "Finding Fake News Websites in the Wild", "link": "https://arxiv.org/abs/2407.07159", "description": "arXiv:2407.07159v1 Announce Type: cross \nAbstract: The battle against the spread of misinformation on the Internet is a daunting task faced by modern society. Fake news content is primarily distributed through digital platforms, with websites dedicated to producing and disseminating such content playing a pivotal role in this complex ecosystem. Therefore, these websites are of great interest to misinformation researchers. However, obtaining a comprehensive list of websites labeled as producers and/or spreaders of misinformation can be challenging, particularly in developing countries. In this study, we propose a novel methodology for identifying websites responsible for creating and disseminating misinformation content, which are closely linked to users who share confirmed instances of fake news on social media. We validate our approach on Twitter by examining various execution modes and contexts. Our findings demonstrate the effectiveness of the proposed methodology in identifying misinformation websites, which can aid in gaining a better understanding of this phenomenon and enabling competent entities to tackle the problem in various areas of society."}, "https://arxiv.org/abs/2407.07712": {"title": "Deep-Graph-Sprints: Accelerated Representation Learning in Continuous-Time Dynamic Graphs", "link": "https://arxiv.org/abs/2407.07712", "description": "arXiv:2407.07712v1 Announce Type: cross \nAbstract: Continuous-time dynamic graphs (CTDGs) are essential for modeling interconnected, evolving systems. Traditional methods for extracting knowledge from these graphs often depend on feature engineering or deep learning. Feature engineering is limited by the manual and time-intensive nature of crafting features, while deep learning approaches suffer from high inference latency, making them impractical for real-time applications. This paper introduces Deep-Graph-Sprints (DGS), a novel deep learning architecture designed for efficient representation learning on CTDGs with low-latency inference requirements. We benchmark DGS against state-of-the-art feature engineering and graph neural network methods using five diverse datasets. The results indicate that DGS achieves competitive performance while improving inference speed up to 12x compared to other deep learning approaches on our tested benchmarks. Our method effectively bridges the gap between deep representation learning and low-latency application requirements for CTDGs."}, "https://arxiv.org/abs/2402.18470": {"title": "Higher-order null models as a lens for social systems", "link": "https://arxiv.org/abs/2402.18470", "description": "arXiv:2402.18470v4 Announce Type: replace \nAbstract: Despite the widespread adoption of higher-order mathematical structures such as hypergraphs, methodological tools for their analysis lag behind those for traditional graphs. This work addresses a critical gap in this context by proposing two micro-canonical random null models for directed hypergraphs: the Directed Hypergraph Configuration Model (DHCM) and the Directed Hypergraph JOINT Model (DHJM). These models preserve essential structural properties of directed hypergraphs such as node in- and out-degree sequences and hyperedge head and tail size sequences, or their joint tensor. We also describe two efficient MCMC algorithms, NuDHy-Degs and NuDHy-JOINT, to sample random hypergraphs from these ensembles.\n  To showcase the interdisciplinary applicability of the proposed null models, we present three distinct use cases in sociology, epidemiology, and economics. First, we reveal the oscillatory behavior of increased homophily in opposition parties in the US Congress over a 40-year span, emphasizing the role of higher-order structures in quantifying political group homophily. Second, we investigate non-linear contagion in contact hyper-networks, demonstrating that disparities between simulations and theoretical predictions can be explained by considering higher-order joint degree distributions. Last, we examine the economic complexity of countries in the global trade network, showing that local network properties preserved by NuDHy explain the main structural economic complexity indexes.\n  This work advances the development of null models for directed hypergraphs, addressing the intricate challenges posed by their complex entity relations, and providing a versatile suite of tools for researchers across various domains."}, "https://arxiv.org/abs/2203.07655": {"title": "Joint Time-Vertex Fractional Fourier Transform", "link": "https://arxiv.org/abs/2203.07655", "description": "arXiv:2203.07655v2 Announce Type: replace-cross \nAbstract: Graph signal processing (GSP) facilitates the analysis of high-dimensional data on non-Euclidean domains by utilizing graph signals defined on graph vertices. In addition to static data, each vertex can provide continuous time-series signals, transforming graph signals into time-series signals on each vertex. The joint time-vertex Fourier transform (JFT) framework offers spectral analysis capabilities to analyze these joint time-vertex signals. Analogous to the fractional Fourier transform (FRT) extending the ordinary Fourier transform (FT), we introduce the joint time-vertex fractional Fourier transform (JFRT) as a generalization of JFT. The JFRT enables fractional analysis for joint time-vertex processing by extending Fourier analysis to fractional orders in both temporal and vertex domains. We theoretically demonstrate that JFRT generalizes JFT and maintains properties such as index additivity, reversibility, reduction to identity, and unitarity for specific graph topologies. Additionally, we derive Tikhonov regularization-based denoising in the JFRT domain, ensuring robust and well-behaved solutions. Comprehensive numerical experiments on synthetic and real-world datasets highlight the effectiveness of JFRT in denoising and clustering tasks that outperform state-of-the-art approaches."}, "https://arxiv.org/abs/2402.11136": {"title": "Interbank network reconstruction enforcing density and reciprocity", "link": "https://arxiv.org/abs/2402.11136", "description": "arXiv:2402.11136v2 Announce Type: replace-cross \nAbstract: Networks of financial exposures are the key propagators of risk and distress among banks, but their empirical structure is not publicly available because of confidentiality. This limitation has triggered the development of methods of network reconstruction from partial, aggregate information. Unfortunately, even the best methods available fail in replicating the number of directed cycles, which on the other hand play a crucial role in determining graph spectra and hence the degree of network stability and systemic risk. Here we address this challenge by exploiting the hypothesis that the statistics of higher-order cycles is strongly constrained by that of the shortest ones, i.e. by the amount of dyads with reciprocated links. First, we provide a detailed analysis of link reciprocity on the e-MID dataset of Italian banks, finding that correlations between reciprocal links systematically increase with the temporal resolution, typically changing from negative to positive around a timescale of up to 50 days. Then, we propose a new network reconstruction method capable of enforcing, only from the knowledge of aggregate interbank assets and liabilities, both a desired sparsity and a desired link reciprocity. We confirm that the addition of reciprocity dramatically improves the prediction of several structural and spectral network properties, including the largest real eigenvalue and the eccentricity of the elliptical distribution of the other eigenvalues in the complex plane. These results illustrate the importance of correctly addressing the temporal resolution and the resulting level of reciprocity in the reconstruction of financial networks."}, "https://arxiv.org/abs/2407.08171": {"title": "Examining (Political) Content Consumption on Facebook Through Data Donation", "link": "https://arxiv.org/abs/2407.08171", "description": "arXiv:2407.08171v1 Announce Type: new \nAbstract: This paper investigates the usage patterns of Facebook among different demographics in the United States, focusing on the consumption of political information and its variability across age, gender, and ethnicity. Employing a novel data donation model, we developed a tool that allows users to voluntarily share their interactions with public Facebook groups and pages, which we subsequently enrich using CrowdTangle. This approach enabled the collection and analysis of a dataset comprising over 1,200 American users. Our findings indicate that political content consumption on Facebook is relatively low, averaging around 17%, and exhibits significant demographic variations. Additionally, we provide insights into the temporal trends of these interactions. The main contributions of this research include a methodological framework for studying social media usage in a privacy-preserving manner, a comprehensive dataset reflective of current engagement patterns, and descriptive insights that highlight demographic disparities and trends over time. This study enhances our understanding of social media's role in information dissemination and its implications for political engagement, offering a valuable resource for researchers and policymakers in a landscape where direct data access is diminishing."}, "https://arxiv.org/abs/2407.08172": {"title": "Deciphering Viral Trends in WhatsApp: A Case Study From a Village in Rural India", "link": "https://arxiv.org/abs/2407.08172", "description": "arXiv:2407.08172v1 Announce Type: new \nAbstract: This research studies the nature and spread of WhatsApp content among everyday users in a rural Indian village. Leveraging a dataset of hundreds of private WhatsApp groups collected with consent from participants, our study uncovers the kinds of WhatsApp groups users are part of, marking the first such categorization. The dataset comprises tens of thousands of messages, out of which we manually classified 604 pieces of content designated as 'forwarded many times'-indicating their viral status.\n  Our key findings indicate a high prevalence of content focused on national politics, with the viral messages overwhelmingly supporting a specific political party and disparaging the opposition. Significantly, these messages were fraught with misinformation, engendering hate against Muslims and promoting a narrative of Hindus being under threat. This trend was particularly noticeable within caste-based groups, which were dominated by misinformation, pro-BJP rhetoric, anti-Congress content, and Hindutva propaganda. Remarkably, much of the misinformation circulating had previously been discredited by established fact-checking organizations. This suggests not only a recurring cycle of debunked information reappearing but also that fact-checks are failing to penetrate these specific groups.\n  As the first quantitative analysis of everyday WhatsApp use in a rural context, this research has far-reaching implications for understanding the unique challenges posed by end-to-end encrypted platforms. It serves as a crucial baseline for designing more effective moderation policies aimed at combating misinformation and fostering a more responsible use of encrypted communication channels."}, "https://arxiv.org/abs/2407.08299": {"title": "Evolving Network Modeling Driven by the Degree Increase and Decrease Mechanism", "link": "https://arxiv.org/abs/2407.08299", "description": "arXiv:2407.08299v1 Announce Type: new \nAbstract: Ever since the Barab\\'{a}si-Albert (BA) scale-free network has been proposed, network modeling has been studied intensively in light of the network growth and the preferential attachment (PA). However, numerous real systems are featured with a dynamic evolution including network reduction in addition to network growth. In this paper, we propose a novel mechanism for evolving networks from the perspective of vertex degree. We construct a queueing system to describe the increase and decrease of vertex degree, which drives the network evolution. In our mechanism, the degree increase rate is regarded as a function positively correlated to the degree of a vertex, ensuring the preferential attachment in a new way. Degree distributions are investigated under two expressions of the degree increase rate, one of which manifests a ``long tail'', and another one varies with different values of parameters. In simulations, we compare our theoretical distributions with simulation results and also apply them to real networks, which presents the validity and applicability of our model."}, "https://arxiv.org/abs/2407.08604": {"title": "Agglomerative Clustering in Uniform and Proportional Feature Spaces", "link": "https://arxiv.org/abs/2407.08604", "description": "arXiv:2407.08604v1 Announce Type: new \nAbstract: Pattern comparison represents a fundamental and crucial aspect of scientific modeling, artificial intelligence, and pattern recognition. Three main approaches have typically been applied for pattern comparison: (i) distances; (ii) statistical joint variation; (iii) projections; and (iv) similarity indices, each with their specific characteristics. In addition to arguing for intrinsic interesting properties of multiset-based similarity approaches, the present work describes a respectively based hierarchical agglomerative clustering approach which inherits the several interesting characteristics of the coincidence similarity index -- including strict comparisons allowing distinguishing between closely similar patterns, inherent normalization, as well as substantial robustness to the presence of noise and outliers in datasets. Two other hierarchical clustering approaches are considered, namely a multiset-based method as well as the traditional Ward's approach. After characterizing uniform and proportional features spaces and presenting the main basic concepts and methods, a comparison of relative performance between the three considered hierarchical methods is reported and discussed, with several interesting and important results. In particular, though intrinsically suitable for implementing proportional comparisons, the coincidence similarity methodology also works effectively in several types of data in uniform feature spaces"}, "https://arxiv.org/abs/2407.08697": {"title": "Patterns of link reciprocity in directed, signed networks", "link": "https://arxiv.org/abs/2407.08697", "description": "arXiv:2407.08697v1 Announce Type: new \nAbstract: Most of the analyses concerning signed networks have focused on the balance theory, hence identifying frustration with undirected, triadic motifs having an odd number of negative edges; much less attention has been paid to their directed counterparts. To fill this gap, we focus on signed, directed connections, with the aim of exploring the notion of frustration in such a context. When dealing with signed, directed edges, frustration is a multi-faceted concept, admitting different definitions at different scales: if we limit ourselves to consider cycles of length two, frustration is related to reciprocity, i.e. the tendency of edges to admit the presence of partners pointing in the opposite direction. As the reciprocity of signed networks is still poorly understood, we adopt a principled approach for its study, defining quantities and introducing models to consistently capture empirical patterns of the kind. In order to quantify the tendency of empirical networks to form either mutualistic or antagonistic cycles of length two, we extend the Exponential Random Graphs framework to binary, directed, signed networks with global and local constraints and, then, compare the empirical abundance of the aforementioned patterns with the one expected under each model. We find that the (directed extension of the) balance theory is not capable of providing a consistent explanation of the patterns characterising the directed, signed networks considered in this work. Although part of the ambiguities can be solved by adopting a coarser definition of balance, our results call for a different theory, accounting for the directionality of edges in a coherent manner. In any case, the evidence that the empirical, signed networks can be highly reciprocated leads us to recommend to explicitly account for the role played by bidirectional dyads in determining frustration at higher levels (e.g. the triadic one)."}, "https://arxiv.org/abs/2407.07925": {"title": "Enhancing Social Media Personalization: Dynamic User Profile Embeddings and Multimodal Contextual Analysis Using Transformer Models", "link": "https://arxiv.org/abs/2407.07925", "description": "arXiv:2407.07925v1 Announce Type: cross \nAbstract: This study investigates the impact of dynamic user profile embedding on personalized context-aware experiences in social networks. A comparative analysis of multilingual and English transformer models was performed on a dataset of over twenty million data points. The analysis included a wide range of metrics and performance indicators to compare dynamic profile embeddings versus non-embeddings (effectively static profile embeddings). A comparative study using degradation functions was conducted. Extensive testing and research confirmed that dynamic embedding successfully tracks users' changing tastes and preferences, providing more accurate recommendations and higher user engagement. These results are important for social media platforms aiming to improve user experience through relevant features and sophisticated recommendation engines."}, "https://arxiv.org/abs/2407.08175": {"title": "Generalized Diffusive Epidemic Process with Permanent Immunity in Two Dimensions", "link": "https://arxiv.org/abs/2407.08175", "description": "arXiv:2407.08175v1 Announce Type: cross \nAbstract: We introduce the generalized diffusive epidemic process, which is a metapopulation model for an epidemic outbreak where a non-sedentary population of walkers can jump along lattice edges with diffusion rates $D_S$ or $D_I$ if they are susceptible or infected, respectively, and recovered individuals possess permanent immunity. Individuals can be contaminated with rate $\\mu_c$ if they share the same lattice node with an infected individual and recover with rate $\\mu_r$, being removed from the dynamics. Therefore, the model does not have the conservation of the active particles composed of susceptible and infected individuals. The reaction-diffusion dynamics are separated into two stages: (i) Brownian diffusion, where the particles can jump to neighboring nodes, and (ii) contamination and recovery reactions. The dynamics are mapped into a growing process by activating lattice nodes with successful contaminations where activated nodes are interpreted as infection sources. In all simulations, the epidemic starts with one infected individual in a lattice filled with susceptibles. Our results indicate a phase transition in the dynamic percolation universality class controlled by the population size, irrespective of diffusion rates $D_S$ and $D_I$ and a subexponential growth of the epidemics in the percolation threshold."}, "https://arxiv.org/abs/2407.08552": {"title": "Authenticity and exclusion: social media recommendation algorithms and the dynamics of belonging in professional networks", "link": "https://arxiv.org/abs/2407.08552", "description": "arXiv:2407.08552v1 Announce Type: cross \nAbstract: Homophily - the attraction of similarity - profoundly influences social interactions, affecting associations, information disclosure, and the dynamics of social exchanges. Organizational studies reveal that when professional and personal boundaries overlap, individuals from minority backgrounds often encounter a dilemma between authenticity and inclusion due to these homophily-driven dynamics: if they disclose their genuine interests, they risk exclusion from the broader conversation. Conversely, to gain inclusion, they might feel pressured to assimilate. How might the nature and design of social media platforms, where different conversational contexts frequently collapse, and the recommender algorithms that are at the heart of these platforms, which can prioritize content based on network structure and historical user engagement, impact these dynamics? In this paper, we employ agent-based simulations to investigate this question. Our findings indicate a decline in the visibility of professional content generated by minority groups, a trend that is exacerbated over time by recommendation algorithms. Within these minority communities, users who closely resemble the majority group tend to receive greater visibility. We examine the philosophical and design implications of our results, discussing their relevance to questions of informational justice, inclusion, and the epistemic benefits of diversity."}, "https://arxiv.org/abs/2210.05401": {"title": "MiDe22: An Annotated Multi-Event Tweet Dataset for Misinformation Detection", "link": "https://arxiv.org/abs/2210.05401", "description": "arXiv:2210.05401v2 Announce Type: replace \nAbstract: The rapid dissemination of misinformation through online social networks poses a pressing issue with harmful consequences jeopardizing human health, public safety, democracy, and the economy; therefore, urgent action is required to address this problem. In this study, we construct a new human-annotated dataset, called MiDe22, having 5,284 English and 5,064 Turkish tweets with their misinformation labels for several recent events between 2020 and 2022, including the Russia-Ukraine war, COVID-19 pandemic, and Refugees. The dataset includes user engagements with the tweets in terms of likes, replies, retweets, and quotes. We also provide a detailed data analysis with descriptive statistics and the experimental results of a benchmark evaluation for misinformation detection."}, "https://arxiv.org/abs/2303.03152": {"title": "U-Park: A User-Centric Smart Parking Recommendation System for Electric Shared Micromobility Services", "link": "https://arxiv.org/abs/2303.03152", "description": "arXiv:2303.03152v2 Announce Type: replace \nAbstract: Electric Shared Micromobility Services (ESMS) has become a vital element within the Mobility as a Service framework, contributing to sustainable transportation systems. However, existing ESMS face notable design challenges such as shortcomings in integration, transparency, and user-centred approaches, resulting in increased operational costs and decreased service quality. A key operational issue for ESMS revolves around parking, particularly ensuring the availability of parking spaces as users approach their destinations. For instance, a recent study illustrated that nearly 13% of shared E-Bike users in Dublin, Ireland, encounter difficulties parking their E-Bikes due to inadequate planning and guidance. In response, we introduce U-Park, a user-centric smart parking recommendation system designed for ESMS, providing tailored recommendations to users by analysing their historical mobility data, trip trajectory, and parking space availability. We present the system architecture, implement it, and evaluate its performance using real-world data from an Irish-based shared E-Bike provider, MOBY Bikes. Our results illustrate U-Park's ability to predict a user's destination within a shared E-Bike system, achieving an approximate accuracy rate of over 97.60%, all without requiring direct user input. Experiments have proven that this predictive capability empowers U-Park to suggest the optimal parking station to users based on the availability of predicted parking spaces, improving the probability of obtaining a parking spot by 24.91% on average and 29.66% on maximum when parking availability is limited."}, "https://arxiv.org/abs/2304.14518": {"title": "The Death of Renaissance Scientist", "link": "https://arxiv.org/abs/2304.14518", "description": "arXiv:2304.14518v2 Announce Type: replace \nAbstract: Scholars are often categorized into two types: hedgehogs (specialists), who focus on working within a specific research field, and foxes (generalists), who actively contribute to a variety of fields. Despite the familiar anecdotes and popularity of this distinction, its empirical foundation has remained largely unexamined. We examine whether the research style of being a fox or a hedgehog is a stable personal trait or an evolving strategy over a scientist's career. Analyzing 2.3 million scholars' publication records over a century, we find that research styles exhibit remarkable stability. Notably, the proportion of fox-like scientists has dramatically declined in the past century, a phenomenon we term \"the death of Renaissance scientists.\" This decline is particularly significant as science shifts toward team collaboration. Teams of foxes consistently outperform teams of hedgehogs in generating new ideas and directions, as confirmed by two emerging innovation metrics for papers: atypicality and disruption. Our research is the first to quantify the process and consequences of the decline of Renaissance scientists. By doing so, we establish a universal link between research styles, demographic shifts, and innovative output."}, "https://arxiv.org/abs/2311.13579": {"title": "Nucleation phenomena and extreme vulnerability of spatial k-core systems", "link": "https://arxiv.org/abs/2311.13579", "description": "arXiv:2311.13579v2 Announce Type: replace \nAbstract: K-core percolation is a fundamental dynamical process in complex networks with applications that span numerous real-world systems. Earlier studies focus primarily on random networks without spatial constraints and reveal intriguing mixed-order transitions. However, real-world systems, ranging from transportation and communication networks to complex brain networks, are not random but are spatially embedded. Here, we study k-core percolation on two-dimensional spatially embedded networks and show that, in contrast to regular percolation, the length of connections can control the transition type, leading to four different types of phase transitions associated with novel phenomena and a rich phase diagram. A key finding is the existence of a metastable phase in which microscopic localized damage, independent of system size, can cause a macroscopic phase transition, a result which cannot be achieved in traditional percolation. In this case, local failures can spontaneously propagate the damage radially until the system entirely collapses, a phenomenon analogous to the nucleation process. These findings suggest novel features and extreme vulnerabilities of spatially embedded k-core network systems, and highlight the necessity to take into account the characteristic length of links when designing robust spatial networks. Furthermore, our insight about the microscopic processes and their origin during the mixed order and first order abrupt transitions in k-core networks could shed light on the mechanisms of many systems where such transitions occur."}, "https://arxiv.org/abs/2312.12251": {"title": "Fairness and Consensus in Opinion Models (Technical Report)", "link": "https://arxiv.org/abs/2312.12251", "description": "arXiv:2312.12251v2 Announce Type: replace \nAbstract: We introduce a DeGroot-based model for opinion dynamics in social networks. A community of agents is represented as a weighted directed graph whose edges indicate how much agents influence one another. The model is formalized using labeled transition systems, henceforth called opinion transition systems (OTS), whose states represent the agents' opinions and whose actions are the edges of the influence graph. If a transition labeled $(i,j)$ is performed, agent $j$ updates their opinion taking into account the opinion of agent $i$ and the influence $i$ has over $j$. We study (convergence to) opinion consensus among the agents of strongly-connected graphs with influence values in the interval $(0,1)$. We show that consensus cannot be guaranteed under the standard strong fairness assumption on transition systems. We derive that consensus is guaranteed under a stronger notion from the literature of concurrent systems; bounded fairness. We argue that bounded-fairness is too strong of a notion for consensus as it almost surely rules out random runs and it is not a constructive liveness property. We introduce a weaker fairness notion, called $m$-bounded fairness, and show that it guarantees consensus. The new notion includes almost surely all random runs and it is a constructive liveness property. Finally, we consider OTS with dynamic influence and show convergence to consensus holds under $m$-bounded fairness if the influence changes within a fixed interval $[L,U]$ with $0<u><1$. We illustrate OTS with examples and simulations, offering insights into opinion formation under fairness and dynamic influence."}, "https://arxiv.org/abs/2401.13098": {"title": "Enhancing Global Maritime Traffic Network Forecasting with Gravity-Inspired Deep Learning Models", "link": "https://arxiv.org/abs/2401.13098", "description": "arXiv:2401.13098v3 Announce Type: replace-cross \nAbstract: Aquatic non-indigenous species (NIS) pose significant threats to biodiversity, disrupting ecosystems and inflicting substantial economic damages across agriculture, forestry, and fisheries. Due to the fast growth of global trade and transportation networks, NIS has been introduced and spread unintentionally in new environments. This study develops a new physics-informed model to forecast maritime shipping traffic between port regions worldwide. The predicted information provided by these models, in turn, is used as input for risk assessment of NIS spread through transportation networks to evaluate the capability of our solution. Inspired by the gravity model for international trades, our model considers various factors that influence the likelihood and impact of vessel activities, such as shipping flux density, distance between ports, trade flow, and centrality measures of transportation hubs. Accordingly, this paper introduces transformers to gravity models to rebuild the short- and long-term dependencies that make the risk analysis feasible. Thus, we introduce a physics-inspired framework that achieves an 89% binary accuracy for existing and non-existing trajectories and an 84.8% accuracy for the number of vessels flowing between key port areas, representing more than 10% improvement over the traditional deep-gravity model. Along these lines, this research contributes to a better understanding of NIS risk assessment. It allows policymakers, conservationists, and stakeholders to prioritize management actions by identifying high-risk invasion pathways. Besides, our model is versatile and can include new data sources, making it suitable for assessing international vessel traffic flow in a changing global landscape."}, "https://arxiv.org/abs/2403.12560": {"title": "The SIS process on Erd\\\"os-R\\'enyi graphs: determining the infected fraction", "link": "https://arxiv.org/abs/2403.12560", "description": "arXiv:2403.12560v2 Announce Type: replace-cross \nAbstract: There are many methods to estimate the quasi-stationary infected fraction of the SIS process on (random) graphs. A challenge is to adequately incorporate correlations, which is especially important in sparse graphs. Methods typically are either significantly biased in sparse graphs, or computationally very demanding already for small network sizes. In this paper we present a new method to determine the infected fraction in sparse graphs, which we test on Erd\\H{o}s-R\\'enyi graphs. Our method does take into account correlations and gives accurate predictions. At the same time, computations are very feasible and can easily be done even for large networks."}, "https://arxiv.org/abs/2407.08749": {"title": "Remembering Steven Weinberg", "link": "https://arxiv.org/abs/2407.08749", "description": "arXiv:2407.08749v1 Announce Type: new \nAbstract: Contribution to the volume \"In Memory of Steven Weinberg\" to appear in Nuclear Physics B."}, "https://arxiv.org/abs/2407.08762": {"title": "Commute-Time-Optimised Graphs for GNNs", "link": "https://arxiv.org/abs/2407.08762", "description": "arXiv:2407.08762v1 Announce Type: new \nAbstract: We explore graph rewiring methods that optimise commute time. Recent graph rewiring approaches facilitate long-range interactions in sparse graphs, making such rewirings commute-time-optimal $\\textit{on average}$. However, when an expert prior exists on which node pairs should or should not interact, a superior rewiring would favour short commute times between these privileged node pairs. We construct two synthetic datasets with known priors reflecting realistic settings, and use these to motivate two bespoke rewiring methods that incorporate the known prior. We investigate the regimes where our rewiring improves test performance on the synthetic datasets. Finally, we perform a case study on a real-world citation graph to investigate the practical implications of our work."}, "https://arxiv.org/abs/2407.08938": {"title": "Social dilemmas, network reciprocity and the small-world property", "link": "https://arxiv.org/abs/2407.08938", "description": "arXiv:2407.08938v1 Announce Type: new \nAbstract: We revisit two evolutionary game theory models, namely the Prisoner and the Snowdrift dilemmas, on top of small-world networks. These dynamics on networked populations (individuals occupying nodes of a graph) are mainly concerning on the competition between to cooperate or to defect, by allowing some process of revision of strategies. Cooperators avoid defectors by forming clusters in a process known as network reciprocity. This defense strategy is based on the fact that any individual interact only with its nearest neighbors. The minimum cluster, in turn, is formed by a set of three completely connected nodes and the bulk of these triplets is associated with the transitivity property of a network. Particularly, we show that the transitivity increases eventually assuming a constant behavior when observed as a function of the number of contacts of an individual. We investigate the influence of the network reciprocity on that transitivity increasing regime on the promotion of a cooperative behavior. The dynamics on small-world networks are compared with those random regular, and annealed networks, the later typically studied as the well-mixed approach. We observe that the Snowdrift Game converge to an annealed scenario as randonness and coordination number increase, whereas the Prisoner's Dilemma becomes more severe against the cooperative behavior under the regime of an increasing network reciprocity."}, "https://arxiv.org/abs/2407.09019": {"title": "Heterogeneous Subgraph Network with Prompt Learning for Interpretable Depression Detection on Social Media", "link": "https://arxiv.org/abs/2407.09019", "description": "arXiv:2407.09019v1 Announce Type: new \nAbstract: Massive social media data can reflect people's authentic thoughts, emotions, communication, etc., and therefore can be analyzed for early detection of mental health problems such as depression. Existing works about early depression detection on social media lacked interpretability and neglected the heterogeneity of social media data. Furthermore, they overlooked the global interaction among users. To address these issues, we develop a novel method that leverages a Heterogeneous Subgraph Network with Prompt Learning(HSNPL) and contrastive learning mechanisms. Specifically, prompt learning is employed to map users' implicit psychological symbols with excellent interpretability while deep semantic and diverse behavioral features are incorporated by a heterogeneous information network. Then, the heterogeneous graph network with a dual attention mechanism is constructed to model the relationships among heterogeneous social information at the feature level. Furthermore, the heterogeneous subgraph network integrating subgraph attention and self-supervised contrastive learning is developed to explore complicated interactions among users and groups at the user level. Extensive experimental results demonstrate that our proposed method significantly outperforms state-of-the-art methods for depression detection on social media."}, "https://arxiv.org/abs/2407.09289": {"title": "How buildings change the fundamental allometry", "link": "https://arxiv.org/abs/2407.09289", "description": "arXiv:2407.09289v1 Announce Type: new \nAbstract: We demonstrate that the original fundamental allometry alone cannot accurately describe the relationship between urban area and population size. Instead, building height is a third factor that interplays with area and population. To illustrate this, we propose a straightforward model based on the idea that city area is the result of people's desire to live close to one another while also having sufficient living space. This leads to a more general form of fundamental allometry (relating area, population, and building height). Our argument is supported by empirical data from different countries."}, "https://arxiv.org/abs/2407.09326": {"title": "Team careers in science: formation, composition and success of persistent collaborations", "link": "https://arxiv.org/abs/2407.09326", "description": "arXiv:2407.09326v1 Announce Type: new \nAbstract: Teams are the fundamental units propelling innovation and advancing modern science. A rich literature links the fundamental features of teams, such as their size and diversity, to academic success. However, such analyses fail to capture temporal patterns, treating each group of co-authors as a distinct unit and neglecting the existence of persistent collaborations. By contrast, teams are dynamical entities, made of core members who consistently work together, surrounded by transient members who sporadically participate. Leveraging on a large dataset of over 205 million scientific papers published since 1900, we extract 511,550 core teams of statistically significant persistent collaborations of pairs and larger groups of scientists. We look into `team careers' investigating their trajectories in time, characterizing their formation, productivity and eventual dissolution. We characterize team composition along multiple dimensions, including age, academic affiliation and scientific disciplines. Finally, we investigate the academic impact of persistent collaborations, hallmarking the key compositional features underlying their success. Our work sheds light on the nature of persistent teams, informing researchers, institutions and funding agencies about the dynamics of their formation, evolution and success."}, "https://arxiv.org/abs/2407.09365": {"title": "Tracking Patterns in Toxicity and Antisocial Behavior Over User Lifetimes on Large Social Media Platforms", "link": "https://arxiv.org/abs/2407.09365", "description": "arXiv:2407.09365v1 Announce Type: new \nAbstract: An increasing amount of attention has been devoted to the problem of \"toxic\" or antisocial behavior on social media. In this paper we analyze such behavior at very large scales: we analyze toxicity over a 14-year time span on nearly 500 million comments from Reddit and Wikipedia, grounded in two different proxies for toxicity.\n  At the individual level, we analyze users' toxicity levels over the course of their time on the site, and find a striking reversal in trends: both Reddit and Wikipedia users tended to become less toxic over their life cycles on the site in the early (pre-2013) history of the site, but more toxic over their life cycles in the later (post-2013) history of the site. We also find that toxicity on Reddit and Wikipedia differ in a key way, with the most toxic behavior on Reddit exhibited in aggregate by the most active users, and the most toxic behavior on Wikipedia exhibited in aggregate by the least active users. Finally, we consider the toxicity of discussion around widely-shared pieces of content, and find that the trends for toxicity in discussion about content bear interesting similarities with the trends for toxicity in discussion by users."}, "https://arxiv.org/abs/2407.09202": {"title": "Influencer Self-Disclosure Practices on Instagram: A Multi-Country Longitudinal Study", "link": "https://arxiv.org/abs/2407.09202", "description": "arXiv:2407.09202v1 Announce Type: cross \nAbstract: This paper presents a longitudinal study of more than ten years of activity on Instagram consisting of over a million posts by 400 content creators from four countries: the US, Brazil, Netherlands and Germany. Our study shows differences in the professionalisation of content monetisation between countries, yet consistent patterns; significant differences in the frequency of posts yet similar user engagement trends; and significant differences in the disclosure of sponsored content in some countries, with a direct connection with national legislation. We analyse shifts in marketing strategies due to legislative and platform feature changes, focusing on how content creators adapt disclosure methods to different legal environments. We also analyse the impact of disclosures and sponsored posts on engagement and conclude that, although sponsored posts have lower engagement on average, properly disclosing ads does not reduce engagement further. Our observations stress the importance of disclosure compliance and can guide authorities in developing and monitoring them more effectively."}, "https://arxiv.org/abs/2407.09364": {"title": "Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text", "link": "https://arxiv.org/abs/2407.09364", "description": "arXiv:2407.09364v1 Announce Type: cross \nAbstract: The significant progress in the development of Large Language Models has contributed to blurring the distinction between human and AI-generated text. The increasing pervasiveness of AI-generated text and the difficulty in detecting it poses new challenges for our society. In this paper, we tackle the problem of detecting and attributing AI-generated text by proposing WhosAI, a triplet-network contrastive learning framework designed to predict whether a given input text has been generated by humans or AI and to unveil the authorship of the text. Unlike most existing approaches, our proposed framework is conceived to learn semantic similarity representations from multiple generators at once, thus equally handling both detection and attribution tasks. Furthermore, WhosAI is model-agnostic and scalable to the release of new AI text-generation models by incorporating their generated instances into the embedding space learned by our framework. Experimental results on the TuringBench benchmark of 200K news articles show that our proposed framework achieves outstanding results in both the Turing Test and Authorship Attribution tasks, outperforming all the methods listed in the TuringBench benchmark leaderboards."}, "https://arxiv.org/abs/2407.09404": {"title": "CAACS: A Carbon Aware Ant Colony System", "link": "https://arxiv.org/abs/2407.09404", "description": "arXiv:2407.09404v1 Announce Type: cross \nAbstract: In an era where sustainability is becoming increasingly crucial, we introduce a new Carbon-Aware Ant Colony System (CAACS) Algorithm that addresses the Generalized Traveling Salesman Problem (GTSP) while minimizing carbon emissions. This novel approach leverages the natural efficiency of ant colony pheromone trails to find optimal routes, balancing both environmental and economic objectives. By integrating sustainability into transportation models, CAACS provides a powerful tool for real-world applications, including network design, delivery route planning, and commercial aircraft logistics. Our algorithm's unique bi-objective optimization advances the study of sustainable transportation solutions."}, "https://arxiv.org/abs/2302.02533": {"title": "Linking Datasets on Organizations Using Half A Billion Open-Collaborated Records", "link": "https://arxiv.org/abs/2302.02533", "description": "arXiv:2302.02533v4 Announce Type: replace \nAbstract: Scholars studying organizations often work with multiple datasets lacking shared identifiers or covariates. In such situations, researchers usually use approximate string (\"fuzzy\") matching methods to combine datasets. String matching, although useful, faces fundamental challenges. Even where two strings appear similar to humans, fuzzy matching often struggles because it fails to adapt to the informativeness of the character combinations. In response, a number of machine learning methods have been developed to refine string matching. Yet, the effectiveness of these methods is limited by the size and diversity of training data. This paper introduces data from a prominent employment networking site (LinkedIn) as a massive training corpus to address these limitations. By leveraging information from the LinkedIn corpus regarding organizational name-to-name links, we incorporate trillions of name pair examples into various methods to enhance existing matching benchmarks and performance by explicitly maximizing match probabilities. We also show how relationships between organization names can be modeled using a network representation of the LinkedIn data. In illustrative merging tasks involving lobbying firms, we document improvements when using the LinkedIn corpus in matching calibration and make all data and methods open source."}, "https://arxiv.org/abs/2309.11798": {"title": "A Comprehensive Review of Community Detection in Graphs", "link": "https://arxiv.org/abs/2309.11798", "description": "arXiv:2309.11798v5 Announce Type: replace \nAbstract: The study of complex networks has significantly advanced our understanding of community structures which serves as a crucial feature of real-world graphs. Detecting communities in graphs is a challenging problem with applications in sociology, biology, and computer science. Despite the efforts of an interdisciplinary community of scientists, a satisfactory solution to this problem has not yet been achieved. This review article delves into the topic of community detection in graphs, which serves as a thorough exposition of various community detection methods from perspectives of modularity-based method, spectral clustering, probabilistic modelling, and deep learning. Along with the methods, a new community detection method designed by us is also presented. Additionally, the performance of these methods on the datasets with and without ground truth is compared. In conclusion, this comprehensive review provides a deep understanding of community detection in graphs."}, "https://arxiv.org/abs/2402.14410": {"title": "Human-machine social systems", "link": "https://arxiv.org/abs/2402.14410", "description": "arXiv:2402.14410v2 Announce Type: replace \nAbstract: From fake social media accounts and generative-AI chatbots to financial trading algorithms and self-driving vehicles, robots, bots, and algorithms are proliferating and permeating our communication channels, social interactions, economic transactions, and transportation arteries. Networks of multiple interdependent and interacting humans and autonomous machines constitute complex social systems where the collective outcomes cannot be deduced from either human or machine behavior alone. Under this paradigm, we review recent research from across a range of disciplines and identify general dynamics and patterns in situations of competition, coordination, cooperation, contagion, and collective decision-making, with context-rich examples from high-frequency trading markets, a social media platform, an open-collaboration community, and a discussion forum. To ensure more robust and resilient human-machine communities, researchers should study them using complex-system methods, engineers should explicitly design AI for human-machine and machine-machine interactions, and regulators should govern the ecological diversity and social co-evolution of humans and machines."}, "https://arxiv.org/abs/2302.03239": {"title": "Calibrated Recommendations for Users with Decaying Attention", "link": "https://arxiv.org/abs/2302.03239", "description": "arXiv:2302.03239v2 Announce Type: replace-cross \nAbstract: Recommendation systems capable of providing diverse sets of results are a focus of increasing importance, with motivations ranging from fairness to novelty and other aspects of optimizing user experience. One form of diversity of recent interest is calibration, the notion that personalized recommendations should reflect the full distribution of a user's interests, rather than a single predominant category -- for instance, a user who mainly reads entertainment news but also wants to keep up with news on the environment and the economy would prefer to see a mixture of these genres, not solely entertainment news. Existing work has formulated calibration as a subset selection problem; this line of work observes that the formulation requires the unrealistic assumption that all recommended items receive equal consideration from the user, but leaves as an open question the more realistic setting in which user attention decays as they move down the list of results.\n  In this paper, we consider calibration with decaying user attention under two different models. In both models, there is a set of underlying genres that items can belong to. In the first setting, where items are represented by fine-grained mixtures of genre percentages, we provide a $(1-1/e)$-approximation algorithm by extending techniques for constrained submodular optimization. In the second setting, where items are coarsely binned into a single genre each, we surpass the $(1-1/e)$ barrier imposed by submodular maximization and give a $2/3$-approximate greedy algorithm. Our work thus addresses the problem of capturing ordering effects due to decaying attention, allowing for the extension of near-optimal calibration from recommendation sets to recommendation lists."}, "https://arxiv.org/abs/2407.09657": {"title": "Analyzing X's Web of Influence: Dissecting News Sharing Dynamics through Credibility and Popularity with Transfer Entropy and Multiplex Network Measures", "link": "https://arxiv.org/abs/2407.09657", "description": "arXiv:2407.09657v1 Announce Type: new \nAbstract: The dissemination of news articles on social media platforms significantly impacts the public's perception of global issues, with the nature of these articles varying in credibility and popularity. The challenge of measuring this influence and identifying key propagators is formidable. Traditional graph-based metrics such as different centrality measures and node degree methods offer some insights into information flow but prove insufficient for identifying hidden influencers in large-scale social media networks such as X (previously known as Twitter). This study adopts and enhances a non-parametric framework based on Transfer Entropy to elucidate the influence relationships among X users. It further categorizes the distribution of influence exerted by these actors through the innovative use of multiplex network measures within a social media context, aiming to pinpoint influential actors during significant world events. The methodology was applied to three distinct events, and the findings revealed that actors in different events leveraged different types of news articles and influenced distinct sets of actors based on the news category. Notably, we found that actors disseminating trustworthy news articles to influence others occasionally resort to untrustworthy sources. However, the converse scenario, wherein actors predominantly using untrustworthy news types switch to trustworthy sources for influence, is less prevalent. This asymmetry suggests a discernible pattern in the strategic use of news articles for influence across social media networks, highlighting the nuanced roles of trustworthiness and popularity in the spread of information and influence."}, "https://arxiv.org/abs/2407.09691": {"title": "EVOLVE: Predicting User Evolution and Network Dynamics in Social Media Using Fine-Tuned GPT-like Model", "link": "https://arxiv.org/abs/2407.09691", "description": "arXiv:2407.09691v1 Announce Type: new \nAbstract: Social media platforms are extensively used for sharing personal emotions, daily activities, and various life events, keeping people updated with the latest happenings. From the moment a user creates an account, they continually expand their network of friends or followers, freely interacting with others by posting, commenting, and sharing content. Over time, user behavior evolves based on demographic attributes and the networks they establish. In this research, we propose a predictive method to understand how a user evolves on social media throughout their life and to forecast the next stage of their evolution. We fine-tune a GPT-like decoder-only model (we named it E-GPT: Evolution-GPT) to predict the future stages of a user's evolution in online social media. We evaluate the performance of these models and demonstrate how user attributes influence changes within their network by predicting future connections and shifts in user activities on social media, which also addresses other social media challenges such as recommendation systems."}, "https://arxiv.org/abs/2407.09894": {"title": "Transferring Structure Knowledge: A New Task to Fake news Detection Towards Cold-Start Propagation", "link": "https://arxiv.org/abs/2407.09894", "description": "arXiv:2407.09894v1 Announce Type: new \nAbstract: Many fake news detection studies have achieved promising performance by extracting effective semantic and structure features from both content and propagation trees. However, it is challenging to apply them to practical situations, especially when using the trained propagation-based models to detect news with no propagation data. Towards this scenario, we study a new task named cold-start fake news detection, which aims to detect content-only samples with missing propagation. To achieve the task, we design a simple but effective Structure Adversarial Net (SAN) framework to learn transferable features from available propagation to boost the detection of content-only samples. SAN introduces a structure discriminator to estimate dissimilarities among learned features with and without propagation, and further learns structure-invariant features to enhance the generalization of existing propagation-based methods for content-only samples. We conduct qualitative and quantitative experiments on three datasets. Results show the challenge of the new task and the effectiveness of our SAN framework."}, "https://arxiv.org/abs/2407.10035": {"title": "Opinion formation under mass media influence on the Barabasi-Albert network", "link": "https://arxiv.org/abs/2407.10035", "description": "arXiv:2407.10035v1 Announce Type: new \nAbstract: We study numerically the dynamics of opinion formation under the influence of mass media using the $q$-voter model on a Barabasi-Albert network. We investigate the scenario where a voter adopts the mass media's opinion with a probability $p$ when there is no unanimity among a group of $q$ agents. Through numerical simulation, we identify a critical probability threshold, $p_t$, at which the system consistently reaches complete consensus. This threshold probability $p_t$ decreases as the group size $q$ increases, following a power-law relation $p_t \\propto q^{\\gamma}$ with $\\gamma \\approx -1.187$. Additionally, we analyze the system's relaxation time, the time required to reach a complete consensus state. This relaxation time increases with the population size $N$, following a power-law $\\tau \\propto N^{\\nu}$, where $\\nu \\approx 1.093$. Conversely, an increase in the probability $p$ results in a decrease in relaxation time following a power-law relationship $\\tau \\propto p^{\\delta}$, with $\\delta \\approx -0.596$. The value of the exponent \\( \\nu \\) is similar to the exponents obtained in the voter and $q$-voter models across various network topologies."}, "https://arxiv.org/abs/2407.10100": {"title": "Constraints on Meso-Scale Structure in Complex Networks", "link": "https://arxiv.org/abs/2407.10100", "description": "arXiv:2407.10100v1 Announce Type: new \nAbstract: A key topic in network science is the detection of intermediate or meso-scale structures. Community, core-periphery, disassortative and other partitions allow us to understand the organisation and function of large networks. In this work we study under what conditions certain common meso-scale structures are detectable using the idea of block modularity. We find that the configuration model imposes strong restrictions on core-periphery and related structures in directed networks. We derive inequalities expressing when such structures can be detected under the configuration model. Nestedness is closely related to core-periphery and is similarly restricted to only be detectable under certain conditions. We show that these conditions are a generalisation of the resolution limit to structures other than assortative communities. We show how block modularity is related to the degree corrected Stochastic Block Model and that optimisation of one can be made equivalent to the other in general. Finally, we discuss these issues in inferential versus descriptive approaches to meso-scale structure detection."}, "https://arxiv.org/abs/2407.10553": {"title": "Formation of human kinship structures depending on population size and cultural mutation rate", "link": "https://arxiv.org/abs/2407.10553", "description": "arXiv:2407.10553v1 Announce Type: new \nAbstract: How does social complexity depend on population size and cultural transmission? Kinship structures in traditional societies provide a fundamental illustration, where cultural rules between clans determine people's marriage possibilities. Here we propose a simple model of kinship interactions that considers kin and in-law cooperation and sexual rivalry. In this model, multiple societies compete. Societies consist of multiple families with different cultural traits and mating preferences. These values determine interactions and hence the growth rate of families and are transmitted to offspring with mutations. Through a multilevel evolutionary simulation, family traits and preferences are grouped into multiple clans with inter-clan mating preferences. It illustrates the emergence of kinship structures as the spontaneous formation of interdependent cultural associations. Emergent kinship structures are characterized by the cycle length of marriage exchange and the number of cycles in society. We numerically and analytically clarify their parameter dependence. The relative importance of cooperation versus rivalry determines whether attraction or repulsion exists between families. Different structures evolve as locally stable attractors. The probabilities of formation and collapse of complex structures depend on the number of families and the mutation rate, showing characteristic scaling relationships. It is now possible to explore macroscopic kinship structures based on microscopic interactions, together with their environmental dependence and the historical causality of their evolution. We propose the basic causal mechanism of the formation of typical human social structures by referring to ethnographic observations and concepts from statistical physics and multilevel evolution. Such interdisciplinary collaboration will unveil universal features in human societies."}, "https://arxiv.org/abs/2407.10614": {"title": "Investigating shocking events in the Ethereum stablecoin ecosystem through temporal multilayer graph structure", "link": "https://arxiv.org/abs/2407.10614", "description": "arXiv:2407.10614v1 Announce Type: new \nAbstract: In the dynamic landscape of the Web, we are witnessing the emergence of the Web3 paradigm, which dictates that platforms should rely on blockchain technology and cryptocurrencies to sustain themselves and their profitability. Cryptocurrencies are characterised by high market volatility and susceptibility to substantial crashes, issues that require temporal analysis methodologies able to tackle the high temporal resolution, heterogeneity and scale of blockchain data. While existing research attempts to analyse crash events, fundamental questions persist regarding the optimal time scale for analysis, differentiation between long-term and short-term trends, and the identification and characterisation of shock events within these decentralised systems. This paper addresses these issues by examining cryptocurrencies traded on the Ethereum blockchain, with a spotlight on the crash of the stablecoin TerraUSD and the currency LUNA designed to stabilise it. Utilising complex network analysis and a multi-layer temporal graph allows the study of the correlations between the layers representing the currencies and system evolution across diverse time scales. The investigation sheds light on the strong interconnections among stablecoins pre-crash and the significant post-crash transformations. We identify anomalous signals before, during, and after the collapse, emphasising their impact on graph structure metrics and user movement across layers. This paper pioneers temporal, cross-chain graph analysis to explore a cryptocurrency collapse. It emphasises the importance of temporal analysis for studies on web-derived data and how graph-based analysis can enhance traditional econometric results. Overall, this research carries implications beyond its field, for example for regulatory agencies aiming to safeguard users from shocks and monitor investment risks for citizens and clients."}, "https://arxiv.org/abs/2407.10680": {"title": "Friedkin-Johnsen Model for Opinion Dynamics on Signed Graphs", "link": "https://arxiv.org/abs/2407.10680", "description": "arXiv:2407.10680v1 Announce Type: new \nAbstract: A signed graph offers richer information than an unsigned graph, since it describes both collaborative and competitive relationships in social networks. In this paper, we study the opinion dynamics on a signed graph, based on the Friedkin-Johnsen model. We first interpret the equilibrium opinion in terms of a defined random walk on an augmented signed graph, by representing the equilibrium opinion of every node as a combination of all nodes' internal opinions, with the coefficient of the internal opinion for each node being the difference of two absorbing probabilities. We then quantify some relevant social phenomena and express them in terms of the $\\ell_2$ norms of vectors. We also design a nearly-linear time signed Laplacian solver for assessing these quantities, by establishing a connection between the absorbing probability of random walks on a signed graph and that on an associated unsigned graph. We further study the opinion optimization problem by changing the initial opinions of a fixed number of nodes, which can be optimally solved in cubic time. We provide a nearly-linear time algorithm with error guarantee to approximately solve the problem. Finally, we execute extensive experiments on sixteen real-life signed networks, which show that both of our algorithms are effective and efficient, and are scalable to massive graphs with over 20 million nodes."}, "https://arxiv.org/abs/2407.10771": {"title": "Growth of Science: How long will the United States uphold its position?", "link": "https://arxiv.org/abs/2407.10771", "description": "arXiv:2407.10771v1 Announce Type: new \nAbstract: Policymakers often assess the growth of science in a country and compare it with that of other countries to set future planning for scientific research focusing on the sustainable development and economic growth of the country. Here, we study the growth of science for the period of 1996-2020 corresponding to the top fifty countries with the highest publications in 2020. It is found that the annual growth rates of scientific and technical journal publications exhibit Taylor's power law behavior indicating the dependence of the variance on the mean growth rate and the distributions of annual growth rates follow skew-symmetric distributions. Furthermore, we have computed the entropy based on annual publication numbers among the countries to assess the spatial disparity in the system. The entropy is found to increase mostly linear with time reducing the disparity among the countries. By performing the linear regression analysis, we predict that around the year 2046, all the countries excluding China may equally contribute towards the growth of science. We have also assessed the stability of the current USA ranking by computing the entropy between the USA and other countries. Based on the regression analysis, it is estimated that three potential countries such as Indonesia, India, and Iran may take the ranks ahead of the USA around the years 2024, 2029, and 2041 respectively."}, "https://arxiv.org/abs/2407.10952": {"title": "A Network Lens on Social Costs: Demolishing a Historic Street for a New Subway Station", "link": "https://arxiv.org/abs/2407.10952", "description": "arXiv:2407.10952v1 Announce Type: new \nAbstract: Urban redevelopment often involves a delicate balance between enhancing regional connectivity and preserving local social fabric. Through a case study in Guangzhou, China, we argue that demolishing a historic street to construct a new subway station shows competing interests between local government's priority to facilitate spatial connectivity and locals' priority to maintain a place for social interaction and memories. We measure the social costs of the new subway station through a network lens, focusing on the loss of social ties and memories and low travel benefits of the new station for the local populations. We find that 1) the demolition will remove many small businesses that support locals' daily activities, social ties, and memories, and 2) the new station reduces travel distance and increases route options for passengers from other areas of the city more than locals nearby the demolition site. Our results contribute to a network-based framework and methodology to understand and contest inequality in expanding transportation network infrastructure in cities."}, "https://arxiv.org/abs/2407.09546": {"title": "A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading", "link": "https://arxiv.org/abs/2407.09546", "description": "arXiv:2407.09546v1 Announce Type: cross \nAbstract: The utilization of Large Language Models (LLMs) in financial trading has primarily been concentrated within the stock market, aiding in economic and financial decisions. Yet, the unique opportunities presented by the cryptocurrency market, noted for its on-chain data's transparency and the critical influence of off-chain signals like news, remain largely untapped by LLMs. This work aims to bridge the gap by developing an LLM-based trading agent, CryptoTrade, which uniquely combines the analysis of on-chain and off-chain data. This approach leverages the transparency and immutability of on-chain data, as well as the timeliness and influence of off-chain signals, providing a comprehensive overview of the cryptocurrency market. CryptoTrade incorporates a reflective mechanism specifically engineered to refine its daily trading decisions by analyzing the outcomes of prior trading decisions. This research makes two significant contributions. Firstly, it broadens the applicability of LLMs to the domain of cryptocurrency trading. Secondly, it establishes a benchmark for cryptocurrency trading strategies. Through extensive experiments, CryptoTrade has demonstrated superior performance in maximizing returns compared to traditional trading strategies and time-series baselines across various cryptocurrencies and market conditions. Our code and data are available at \\url{https://anonymous.4open.science/r/CryptoTrade-Public-92FC/}."}, "https://arxiv.org/abs/2407.09747": {"title": "SocialRec: User Activity Based Post Weighted Dynamic Personalized Post Recommendation System in Social Media", "link": "https://arxiv.org/abs/2407.09747", "description": "arXiv:2407.09747v1 Announce Type: cross \nAbstract: User activities can influence their subsequent interactions with a post, generating interest in the user. Typically, users interact with posts from friends by commenting and using reaction emojis, reflecting their level of interest on social media such as Facebook, Twitter, and Reddit. Our objective is to analyze user history over time, including their posts and engagement on various topics. Additionally, we take into account the user's profile, seeking connections between their activities and social media platforms. By integrating user history, engagement, and persona, we aim to assess recommendation scores based on relevant item sharing by Hit Rate (HR) and the quality of the ranking system by Normalized Discounted Cumulative Gain (NDCG), where we achieve the highest for NeuMF 0.80 and 0.6 respectively. Our hybrid approach solves the cold-start problem when there is a new user, for new items cold-start problem will never occur, as we consider the post category values. To improve the performance of the model during cold-start we introduce collaborative filtering by looking for similar users and ranking the users based on the highest similarity scores."}, "https://arxiv.org/abs/2407.10206": {"title": "Dominant Design Prediction with Phylogenetic Networks", "link": "https://arxiv.org/abs/2407.10206", "description": "arXiv:2407.10206v1 Announce Type: cross \nAbstract: This study proposes an effective method to predict technology development from an evolutionary perspective. Product evolution is the result of technological evolution and market selection. A phylogenetic network is the main method to study product evolution. The formation of the dominant design determines the trajectory of technology development. How to predict future dominant design has become a key issue in technology forecasting and new product development. We define the dominant product and use machine learning methods, combined with product evolutionary theory, to construct a Fully Connected Phylogenetic Network dataset to effectively predict the future dominant design."}, "https://arxiv.org/abs/2407.10216": {"title": "Applications of Particle Accelerators", "link": "https://arxiv.org/abs/2407.10216", "description": "arXiv:2407.10216v1 Announce Type: cross \nAbstract: Of the tens of thousands of particle accelerators in operation worldwide, the vast majority are not used for particle physics, but instead for applications. Some applications such as radiotherapy for cancer treatment are well-known, while others are more surprising: food irradiation using electron beams, or the hardening of road tarmac. The uses of particle beams are constantly growing in number including in medicine, industry, security, environment, and cultural heritage preservation. This lecture aims to give a broad sweep of the many uses of particle accelerators, covering technologies ranging in size from a few centimetres for industrial electron linacs through to large synchrotron light sources of hundreds of metres circumference operating as national and international facilities. We finish by discussing some of the challenges facing accelerators used in wider society."}, "https://arxiv.org/abs/2407.10304": {"title": "Systematic analysis of the effectiveness of adding human mobility data to covid-19 case prediction linear models", "link": "https://arxiv.org/abs/2407.10304", "description": "arXiv:2407.10304v1 Announce Type: cross \nAbstract: Human mobility data has been extensively used in covid-19 case prediction models. Nevertheless, related work has questioned whether mobility data really helps that much. We present a systematic analysis across mobility datasets and prediction lookaheads and reveal that adding mobility data to predictive models improves model performance only for about two months at the onset of the testing period, and that performance improvements -- measured as predicted vs. actual correlation improvement over non-mobility baselines -- are at most 0.3."}, "https://arxiv.org/abs/2407.10321": {"title": "Public Discourse about COVID-19 Vaccinations: A Computational Analysis of the Relationship between Public Concerns and Policies", "link": "https://arxiv.org/abs/2407.10321", "description": "arXiv:2407.10321v1 Announce Type: cross \nAbstract: Societies worldwide have witnessed growing rifts separating advocates and opponents of vaccinations and other COVID-19 countermeasures. With the rollout of vaccination campaigns, German-speaking regions exhibited much lower vaccination uptake than other European regions. While Austria, Germany, and Switzerland (the DACH region) caught up over time, it remains unclear which factors contributed to these changes. Scrutinizing public discourses can help shed light on the intricacies of vaccine hesitancy and inform policy-makers tasked with making far-reaching decisions: policies need to effectively curb the spread of the virus while respecting fundamental civic liberties and minimizing undesired consequences. This study draws on Twitter data to analyze the topics prevalent in the public discourse. It further maps the topics to different phases of the pandemic and policy changes to identify potential drivers of change in public attention. We use a hybrid pipeline to detect and analyze vaccination-related tweets using topic modeling, sentiment analysis, and a minimum of social scientific domain knowledge to analyze the discourse about vaccinations in the light of the COVID-19 pandemic in the DACH region. We show that skepticism regarding the severity of the COVID-19 virus and towards efficacy and safety of vaccines were among the prevalent topics in the discourse on Twitter but that the most attention was given to debating the theme of freedom and civic liberties. Especially during later phases of the pandemic, when implemented policies restricted the freedom of unvaccinated citizens, increased vaccination uptake could be observed. At the same time, increasingly negative and polarized sentiments emerge in the discourse. This suggests that these policies might have effectively attenuated vaccination hesitancy but were not successfully dispersing citizens' doubts and concerns."}, "https://arxiv.org/abs/2407.10340": {"title": "Mapping the Scholarship of Dark Pattern Regulation: A Systematic Review of Concepts, Regulatory Paradigms, and Solutions from an Interdisciplinary Perspective", "link": "https://arxiv.org/abs/2407.10340", "description": "arXiv:2407.10340v1 Announce Type: cross \nAbstract: Dark patterns, design tricks used on online interfaces to manipulate users decision-making process, have raised public concerns. However, research on regulation of dark pattern remains underdeveloped and scattered, particularly regarding scholars views on the concept, regulatory paradigms, and solutions. Following PRISMA guidelines, this paper systematically reviews the formats and content of regulatory discussions on dark patterns from the interdisciplinary scholarship of Law and Human-Computer Interaction. A total of 65 studies were analysed through content and thematic analysis. This study synthesises the unique trends and characteristics of legal scholarship on dark patterns, identifying five root problems and triple layered harms. It critiques current regulations in terms of legal theories and sectoral legislations, highlighting their inadequacies in addressing dark patterns. The paper also critically examines existing proposed solutions, including paradigmatic shifts in legal doctrines, refinements to existing frameworks, technical design-embedded solutions, and accountability measures for design practices. This research critically discusses the current barriers to effective dark pattern regulations and explores promising regulatory solutions. The difficulty in identifying the normative nature of various forms of dark patterns, in identifying evident and actionable harm, and the expanding scope of dark patterns connotation inherently hinders effective regulation. However, technical design-embedded solutions, accountability frameworks, and practical design guidelines offer potential routes for more proactive regulation, while legal pluralism stands as a promising macro-level change in regulatory paradigms for dark pattern regulation."}, "https://arxiv.org/abs/2407.10640": {"title": "Error Bounds for the Network Scale-Up Method", "link": "https://arxiv.org/abs/2407.10640", "description": "arXiv:2407.10640v1 Announce Type: cross \nAbstract: Epidemiologists and social scientists have used the Network Scale-Up Method (NSUM) for over thirty years to estimate the size of a hidden sub-population within a social network. This method involves querying a subset of network nodes about the number of their neighbours belonging to the hidden sub-population. In general, NSUM assumes that the social network topology and the hidden sub-population distribution are well-behaved; hence, the NSUM estimate is close to the actual value. However, bounds on NSUM estimation errors have not been analytically proven. This paper provides analytical bounds on the error incurred by the two most popular NSUM estimators. These bounds assume that the queried nodes accurately provide their degree and the number of neighbors belonging to the hidden population. Our key findings are twofold. First, we show that when an adversary designs the network and places the hidden sub-population, then the estimate can be a factor of $\\Omega(\\sqrt{n})$ off from the real value (in a network with $n$ nodes). Second, we also prove error bounds when the underlying network is randomly generated, showing that a small constant factor can be achieved with high probability using samples of logarithmic size $O(\\log{n})$. We present improved analytical bounds for Erdos-Renyi and Scale-Free networks. Our theoretical analysis is supported by an extensive set of numerical experiments designed to determine the effect of the sample size on the accuracy of the estimates in both synthetic and real networks."}, "https://arxiv.org/abs/2407.10755": {"title": "Socioeconomic factors of national representation in the global film festival circuit: skewed toward the large and wealthy, but small countries can beat the odds", "link": "https://arxiv.org/abs/2407.10755", "description": "arXiv:2407.10755v1 Announce Type: cross \nAbstract: This study analyzes how economic, demographic, and geographic factors predict the representation of different countries in the global film festival circuit. It relies on the combination of several open access datasets, including festival programming information from the Cinando platform of the Cannes Film Market, covering more than 30,000 screenings of over 20,000 films in almost 600 festivals across the world over a decade. It is shown that while the festival screen is indeed dominated by films from large affluent countries, the bias is nevertheless not fully proportional to the large demographic and economic disparities across the world, and that several small countries perform better than expected. It is further analyzed via computational simulations how much including films from smaller countries contributes to cultural diversity, and how countries differ in cultural \"trade balance\" dynamics, revealing differences between net exporters and importers of festival films. This research underscores the importance of balanced representation in film festivals and the public value of increasing cultural diversity. The data-driven insights and approaches to quantitative festival program and cultural event analytics are hoped to be useful for both the academic community as well as film festival organizers and policymakers aiming to foster more inclusive and diverse cultural landscapes."}, "https://arxiv.org/abs/2407.10816": {"title": "Monotone convergence of spreading processes on networks", "link": "https://arxiv.org/abs/2407.10816", "description": "arXiv:2407.10816v1 Announce Type: cross \nAbstract: We analyze the Bass and SI models for the spreading of innovations and epidemics, respectively, on homogeneous complete networks, circular networks, and heterogeneous complete networks with two homogeneous groups. We allow the network parameters to be time dependent, which is a prerequisite for the analysis of optimal strategies on networks. Using a novel top-down analysis of the master equations, we present a simple proof for the monotone convergence of these models to their respective infinite-population limits. This leads to explicit expressions for the expected adoption or infection level in the Bass and SI models, respectively, on infinite homogeneous complete and circular networks, and on heterogeneous complete networks with two homogeneous groups with time-dependent parameters."}, "https://arxiv.org/abs/2407.10821": {"title": "Astro-COLIBRI: Empowering Citizen Scientists in Time Domain Astronomy", "link": "https://arxiv.org/abs/2407.10821", "description": "arXiv:2407.10821v1 Announce Type: cross \nAbstract: In recent decades, astronomy and astrophysics have experienced several fundamental changes. On one hand, there has been a significant increase in the observation of transient phenomena, which are short-lived events such as supernova explosions, fast radio bursts, and gamma-ray bursts. In addition, the detection of a growing number of different cosmic messengers provides researchers with crucial information about these objects. For example, the detection of high-energy neutrinos and gravitational waves regularly complements traditional astronomical observations in the electromagnetic spectrum. This trend is expected to intensify in the coming years with the commissioning of a wide variety of next-generation observatories, which will enable more in-depth studies of the transient sky.\n  To enhance our understanding and optimize the observations of these phenomena, we have developed the Astro-COLIBRI platform. It is freely available to amateur and professional astronomers in the form of a smartphone application (iOS and Android), a web interface, an API, and a chatbot 'Astro-COLIBRI GPT', among many other features. Astro-COLIBRI serves as a central access point for information on astrophysical sources and transient events, allowing a wide network of observers to track and receive real-time alerts.\n  Here we highlight the key features of Astro-COLIBRI, with a particular emphasis on recent innovations. These include a discussion forum that facilitates user interactions and our strengthened collaboration with various networks of amateur astronomers."}, "https://arxiv.org/abs/2310.01731": {"title": "The impact of social noise on the majority-rule model across various network topologies", "link": "https://arxiv.org/abs/2310.01731", "description": "arXiv:2310.01731v3 Announce Type: replace \nAbstract: We explore the impact of social noise, characterized by nonconformist behavior, on the phase transition within the framework of the majority-rule model. The order-disorder transition can reflect the consensus-polarization state in a social context. This study covers various network topologies, including complete graphs, two-dimensional (2-D) square lattices, three-dimensional (3-D) square lattices, and heterogeneous or complex networks such as Watts-Strogatz (W-S), Barab\\'asi-Albert (B-A), and Erdos-R\\'enyi (E-R) networks, as well as their combinations. Social behavior is represented by the parameter $p$, which indicates the probability of agents exhibiting nonconformist behavior. Our results show that the model exhibits a continuous phase transition across all networks. Through finite-size scaling analysis and evaluation of critical exponents, our results suggest that the model falls into the same universality class as the Ising model."}, "https://arxiv.org/abs/2402.03910": {"title": "Understanding Trends, Patterns, and Dynamics in Global Company Acquisitions: A Network Perspective", "link": "https://arxiv.org/abs/2402.03910", "description": "arXiv:2402.03910v3 Announce Type: replace \nAbstract: Studying acquisitions offers invaluable insights into startup trends, aiding informed investment decisions for businesses. However, the scarcity of studies in this domain prompts our focus on shedding light in this area. Employing Crunchbase data, our study delves into the global network of company acquisitions using diverse network analysis techniques. Our findings unveil an acquisition network characterized by a primarily sparse structure comprising localized dense connections. We reveal a prevalent tendency among organizations to acquire companies within their own country and industry, as well as those within the same age bracket. Furthermore, we show that the country, region, city, and category of the companies can affect the formation of acquisition relationships between them. Our temporal analysis indicates a growth in the number of weakly connected components of the network over time, accompanied by a trend toward a sparser network. Through centrality metrics computation in the cross-city acquisition network, we identify New York, London, and San Francisco as pivotal and central hubs in the global economic landscape. Finally, we show that the United States, United Kingdom, and Germany are predominant countries in international acquisitions. The insights from our research assist policymakers in crafting better regulations to foster global economic growth, and aid businesses in deciding which startups to acquire and which markets to target for expansion."}, "https://arxiv.org/abs/2310.20335": {"title": "Uplifting edges in higher order networks: spectral centralities for non-uniform hypergraphs", "link": "https://arxiv.org/abs/2310.20335", "description": "arXiv:2310.20335v2 Announce Type: replace-cross \nAbstract: Spectral analysis of networks states that many structural properties of graphs, such as centrality of their nodes, are given in terms of their adjacency matrices. The natural extension of such spectral analysis to higher order networks is strongly limited by the fact that a given hypergraph could have several different adjacency hypermatrices, hence the results obtained so far are mainly restricted to the class of uniform hypergraphs, which leaves many real systems unattended. A new method for analysing non-linear eigenvector-like centrality measures of non-uniform hypergraphs is presented in this paper that could be useful for studying properties of $\\mathcal{H}$-eigenvectors and $\\mathcal{Z}$-eigenvectors in the non-uniform case. In order to do so, a new operation - the $\\textit{uplift}$ - is introduced, incorporating auxiliary nodes in the hypergraph to allow for a uniform-like analysis. We later argue why this is a mathematically sound operation, and we furthermore use it to classify a whole family of hypergraphs with unique Perron-like $\\mathcal{Z}$-eigenvectors. We supplement the theoretical analysis with several examples and numerical simulations on synthetic and real datasets."}, "https://arxiv.org/abs/2402.11040": {"title": "Surpassing legacy approaches to PWR core reload optimization with single-objective Reinforcement learning", "link": "https://arxiv.org/abs/2402.11040", "description": "arXiv:2402.11040v2 Announce Type: replace-cross \nAbstract: Optimizing the fuel cycle cost through the optimization of nuclear reactor core loading patterns involves multiple objectives and constraints, leading to a vast number of candidate solutions that cannot be explicitly solved. To advance the state-of-the-art in core reload patterns, we have developed methods based on Deep Reinforcement Learning (DRL) for both single- and multi-objective optimization. Our previous research has laid the groundwork for these approaches and demonstrated their ability to discover high-quality patterns within a reasonable time frame. On the other hand, stochastic optimization (SO) approaches are commonly used in the literature, but there is no rigorous explanation that shows which approach is better in which scenario. In this paper, we demonstrate the advantage of our RL-based approach, specifically using Proximal Policy Optimization (PPO), against the most commonly used SO-based methods: Genetic Algorithm (GA), Parallel Simulated Annealing (PSA) with mixing of states, and Tabu Search (TS), as well as an ensemble-based method, Prioritized Replay Evolutionary and Swarm Algorithm (PESA). We found that the LP scenarios derived in this paper are amenable to a global search to identify promising research directions rapidly, but then need to transition into a local search to exploit these directions efficiently and prevent getting stuck in local optima. PPO adapts its search capability via a policy with learnable weights, allowing it to function as both a global and local search method. Subsequently, we compared all algorithms against PPO in long runs, which exacerbated the differences seen in the shorter cases. Overall, the work demonstrates the statistical superiority of PPO compared to the other considered algorithms."}, "https://arxiv.org/abs/2403.10707": {"title": "Discovering Latent Themes in Social Media Messaging: A Machine-in-the-Loop Approach Integrating LLMs", "link": "https://arxiv.org/abs/2403.10707", "description": "arXiv:2403.10707v2 Announce Type: replace-cross \nAbstract: Grasping the themes of social media content is key to understanding the narratives that influence public opinion and behavior. The thematic analysis goes beyond traditional topic-level analysis, which often captures only the broadest patterns, providing deeper insights into specific and actionable themes such as \"public sentiment towards vaccination\", \"political discourse surrounding climate policies,\" etc. In this paper, we introduce a novel approach to uncovering latent themes in social media messaging. Recognizing the limitations of the traditional topic-level analysis, which tends to capture only overarching patterns, this study emphasizes the need for a finer-grained, theme-focused exploration. Traditional theme discovery methods typically involve manual processes and a human-in-the-loop approach. While valuable, these methods face challenges in scalability, consistency, and resource intensity in terms of time and cost. To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs). To demonstrate our approach, we apply our framework to contentious topics, such as climate debate and vaccine debate. We use two publicly available datasets: (1) the climate campaigns dataset of 21k Facebook ads and (2) the COVID-19 vaccine campaigns dataset of 9k Facebook ads. Our quantitative and qualitative analysis shows that our methodology yields more accurate and interpretable results compared to the baselines. Our results not only demonstrate the effectiveness of our approach in uncovering latent themes but also illuminate how these themes are tailored for demographic targeting in social media contexts. Additionally, our work sheds light on the dynamic nature of social media, revealing the shifts in the thematic focus of messaging in response to real-world events."}, "https://arxiv.org/abs/2404.10259": {"title": "Uncovering Latent Arguments in Social Media Messaging by Employing LLMs-in-the-Loop Strategy", "link": "https://arxiv.org/abs/2404.10259", "description": "arXiv:2404.10259v2 Announce Type: replace-cross \nAbstract: The widespread use of social media has led to a surge in popularity for automated methods of analyzing public opinion. Supervised methods are adept at text categorization, yet the dynamic nature of social media discussions poses a continual challenge for these techniques due to the constant shifting of the focus. On the other hand, traditional unsupervised methods for extracting themes from public discourse, such as topic modeling, often reveal overarching patterns that might not capture specific nuances. Consequently, a significant portion of research into social media discourse still depends on labor-intensive manual coding techniques and a human-in-the-loop approach, which are both time-consuming and costly. In this work, we study the problem of discovering arguments associated with a specific theme. We propose a generic LLMs-in-the-Loop strategy that leverages the advanced capabilities of Large Language Models (LLMs) to extract latent arguments from social media messaging. To demonstrate our approach, we apply our framework to contentious topics. We use two publicly available datasets: (1) the climate campaigns dataset of 14k Facebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of 9k Facebook ads with 14 themes. Additionally, we design a downstream task as stance prediction by leveraging talking points in climate debates. Furthermore, we analyze demographic targeting and the adaptation of messaging based on real-world events."}, "https://arxiv.org/abs/2407.11163": {"title": "Exact Label Recovery in Euclidean Random Graphs", "link": "https://arxiv.org/abs/2407.11163", "description": "arXiv:2407.11163v1 Announce Type: new \nAbstract: In this paper, we propose a family of label recovery problems on weighted Euclidean random graphs. The vertices of a graph are embedded in $\\mathbb{R}^d$ according to a Poisson point process, and are assigned to a discrete community label. Our goal is to infer the vertex labels, given edge weights whose distributions depend on the vertex labels as well as their geometric positions. Our general model provides a geometric extension of popular graph and matrix problems, including submatrix localization and $\\mathbb{Z}_2$-synchronization, and includes the Geometric Stochastic Block Model (proposed by Sankararaman and Baccelli) as a special case. We study the fundamental limits of exact recovery of the vertex labels. Under a mild distinctness of distributions assumption, we determine the information-theoretic threshold for exact label recovery, in terms of a Chernoff-Hellinger divergence criterion. Impossibility of recovery below the threshold is proven by a unified analysis using a Cram\\'er lower bound. Achievability above the threshold is proven via an efficient two-phase algorithm, where the first phase computes an almost-exact labeling through a local propagation scheme, while the second phase refines the labels. The information-theoretic threshold is dictated by the performance of the so-called genie estimator, which decodes the label of a single vertex given all the other labels. This shows that our proposed models exhibit the local-to-global amplification phenomenon."}, "https://arxiv.org/abs/2407.11261": {"title": "Competition between group interactions and nonlinearity in voter dynamics on hypergraphs", "link": "https://arxiv.org/abs/2407.11261", "description": "arXiv:2407.11261v1 Announce Type: new \nAbstract: Social dynamics are often driven by both pairwise (i.e., dyadic) relationships and higher-order (i.e., polyadic) group relationships, which one can describe using hypergraphs. To gain insight into the impact of polyadic relationships on dynamical processes on networks, we formulate and study a polyadic voter process, which we call the group-driven voter model (GVM), in which we incorporate the effect of group interactions by nonlinear interactions that are subject to a group (i.e., hyperedge) constraint. By examining the competition between nonlinearity and group sizes, we show that the GVM achieves consensus faster than standard voter-model dynamics, with an optimum minimizing exit time {\\tau} . We substantiate this finding by using mean-field theory on annealed uniform hypergraphs with N nodes, for which {\\tau} scales as A ln N, where the prefactor A depends both on the nonlinearity and on group-constraint factors. Our results reveal how competition between group interactions and nonlinearity shapes GVM dynamics. We thereby highlight the importance of such competing effects in complex systems with polyadic interactions."}, "https://arxiv.org/abs/2407.11521": {"title": "Introducing Total Harmonic Resistance for Graph Robustness under Edge Deletions", "link": "https://arxiv.org/abs/2407.11521", "description": "arXiv:2407.11521v1 Announce Type: new \nAbstract: Assessing and improving the robustness of a graph $G$ are critical steps in network design and analysis. To this end, we consider the optimisation problem of removing $k$ edges from $G$ such that the resulting graph has minimal robustness, simulating attacks or failures. In this paper, we propose total harmonic resistance as a new robustness measure for this purpose - and compare it to the recently proposed forest index [Zhu et al., IEEE Trans.\\ Inf.\\ Forensics and Security, 2023]. Both measures are related to the established total effective resistance measure, but their advantage is that they can handle disconnected graphs. This is also important for originally connected graphs due to the removal of the $k$ edges. To compare our measure with the forest index, we first investigate exact solutions for small examples. The best $k$ edges to select when optimizing for the forest index lie at the periphery. Our proposed measure, in turn, prioritizes more central edges, which should be beneficial for most applications. Furthermore, we adapt a generic greedy algorithm to our optimization problem with the total harmonic resistance. With this algorithm, we perform a case study on the Berlin road network and also apply the algorithm to established benchmark graphs. The results are similar as for the small example graphs above and indicate the higher suitability of the new measure."}, "https://arxiv.org/abs/2407.11690": {"title": "Using Causality to Infer Coordinated Attacks in Social Media", "link": "https://arxiv.org/abs/2407.11690", "description": "arXiv:2407.11690v1 Announce Type: new \nAbstract: The rise of social media has been accompanied by a dark side with the ease of creating fake accounts and disseminating misinformation through coordinated attacks. Existing methods to identify such attacks often rely on thematic similarities or network-based approaches, overlooking the intricate causal relationships that underlie coordinated actions. This work introduces a novel approach for detecting coordinated attacks using Convergent Cross Mapping (CCM), a technique that infers causality from temporal relationships between user activity. We build on the theoretical framework of CCM by incorporating topic modelling as a basis for further optimizing its performance. We apply CCM to real-world data from the infamous IRA attack on US elections, achieving F1 scores up to 75.3% in identifying coordinated accounts. Furthermore, we analyse the output of our model to identify the most influential users in a community. We apply our model to a case study involving COVID-19 anti-vax related discussions on Twitter. Our results demonstrate the effectiveness of our model in uncovering causal structures of coordinated behaviour, offering a promising avenue for mitigating the threat of malicious campaigns on social media platforms."}, "https://arxiv.org/abs/2407.11697": {"title": "Identifying Coordinated Activities on Online Social Networks Using Contrast Pattern Mining", "link": "https://arxiv.org/abs/2407.11697", "description": "arXiv:2407.11697v1 Announce Type: new \nAbstract: The proliferation of misinformation and disinformation on social media networks has become increasingly concerning. With a significant portion of the population using social media on a regular basis, there are growing efforts by malicious organizations to manipulate public opinion through coordinated campaigns. Current methods for identifying coordinated user accounts typically rely on either similarities in user behaviour, latent coordination in activity traces, or classification techniques. In our study, we propose a framework based on the hypothesis that coordinated users will demonstrate abnormal growth in their behavioural patterns over time relative to the wider population. Specifically, we utilize the EPClose algorithm to extract contrasting patterns of user behaviour during a time window of malicious activity, which we then compare to a historical time window. We evaluated the effectiveness of our approach using real-world data, and our results show a minimum increase of 10% in the F1 score compared to existing approaches."}, "https://arxiv.org/abs/2407.11772": {"title": "User Behavior Analysis and Clustering in Peace Elite: Insights and Recommendations", "link": "https://arxiv.org/abs/2407.11772", "description": "arXiv:2407.11772v1 Announce Type: new \nAbstract: This study presents a comprehensive analysis of user behavior and clustering in Peace Elite, a popular mobile battle royale game, employing temporal and static data mining techniques to uncover distinct player segments. Our methodology encompasses time series K-means clustering, graph-based algorithms (DeepWalk and LINE), and static attribute clustering, visualized through innovative hybrid charts. Key findings reveal significant variations in player engagement, skill levels, and social interactions across five primary user segments, ranging from highly active and skilled players to inactive or new users. We also analyze the impact of external factors on user retention and the network structure within clusters, uncovering correlations between cluster cohesion and player activity levels. This research provides valuable insights for game developers and marketers, offering data-driven recommendations for personalized game experiences, targeted marketing strategies, and improved player retention in online gaming environments."}, "https://arxiv.org/abs/2407.11794": {"title": "What's in a Niche? Migration Patterns in Online Communities", "link": "https://arxiv.org/abs/2407.11794", "description": "arXiv:2407.11794v1 Announce Type: new \nAbstract: Broad topics in online platforms represent a type of meso-scale between individual user-defined communities and the whole platform; they typically consist of related communities that address different facets of a shared topic. Users often engage with the topic by moving among the communities within a single category. We find that there are strong regularities in the aggregate pattern of user migration, in that the communities comprising a topic can be ordered in a partial order such that there is more migration in the direction defined by the partial order than against it. Ordered along this overall direction, we find that communities in aggregate become smaller, less toxic, and more linguistically distinctive, suggesting a picture consistent with specialization. We study directions defined not just in the movement of users but also by the movement of URLs and by the direction of mentions from one community to another; each of these produces a consistent direction, but the directions all differ from each other.\n  We show how, collectively, these distinct trends help organize the structure of large online topics and compare our findings across both Reddit and Wikipedia and in simulations."}, "https://arxiv.org/abs/2407.11909": {"title": "Cumulative Advantage of Brokerage in Academia", "link": "https://arxiv.org/abs/2407.11909", "description": "arXiv:2407.11909v1 Announce Type: new \nAbstract: Science is a collaborative endeavor in which \"who collaborates with whom\" profoundly influences scientists' career trajectories and success. Despite its relevance, little is known about how scholars facilitate new collaborations among their peers. In this study, we quantify brokerage in academia and study its effect on the careers of physicists worldwide. We find that early-career participation in brokerage increases later-stage involvement for all researchers, with increasing participation rates and greater career impact among more successful scientists. This cumulative advantage process suggests that brokerage contributes to the unequal distribution of success in academia. Surprisingly, this affects both women and men equally, despite women being more junior in all brokerage roles and lagging behind men's participation due to their late and slow arrival to physics. Because of its cumulative nature, promoting brokerage opportunities to early career scientists might help reduce the inequalities in academic success."}, "https://arxiv.org/abs/2407.06479": {"title": "Interaction Matters: An Evaluation Framework for Interactive Dialogue Assessment on English Second Language Conversations", "link": "https://arxiv.org/abs/2407.06479", "description": "arXiv:2407.06479v1 Announce Type: cross \nAbstract: We present an evaluation framework for interactive dialogue assessment in the context of English as a Second Language (ESL) speakers. Our framework collects dialogue-level interactivity labels (e.g., topic management; 4 labels in total) and micro-level span features (e.g., backchannels; 17 features in total). Given our annotated data, we study how the micro-level features influence the (higher level) interactivity quality of ESL dialogues by constructing various machine learning-based models. Our results demonstrate that certain micro-level features strongly correlate with interactivity quality, like reference word (e.g., she, her, he), revealing new insights about the interaction between higher-level dialogue quality and lower-level linguistic signals. Our framework also provides a means to assess ESL communication, which is useful for language assessment."}, "https://arxiv.org/abs/2407.11013": {"title": "Quantum-tunnelling deep neural networks for sociophysical neuromorphic AI", "link": "https://arxiv.org/abs/2407.11013", "description": "arXiv:2407.11013v1 Announce Type: cross \nAbstract: The discovery of the quantum tunnelling effect -- the transmission of particles through a high potential barrier -- was one of the most impressive achievements of quantum mechanics made in the 1920s. Responding to the contemporary challenges, I introduce a novel deep neural network (DNN) architecture that processes information using the effect of quantum tunnelling. I demonstrate the ability of the quantum tunnelling DNN (QT-DNN) to recognise optical illusions like a human. Hardware implementation of QT-DNN is expected to result in an inexpensive and energy-efficient neuromorphic chip suitable for applications in autonomous vehicles. The optical illusions recognition tests developed in this paper should lay foundations for cognitive benchmarking tasks for AI systems of the future, benefiting the fields of sociophysics and behavioural science."}, "https://arxiv.org/abs/2407.11138": {"title": "Lessons from a human-in-the-loop machine learning approach for identifying vacant, abandoned, and deteriorated properties in Savannah, Georgia", "link": "https://arxiv.org/abs/2407.11138", "description": "arXiv:2407.11138v1 Announce Type: cross \nAbstract: Addressing strategies for managing vacant, abandoned, and deteriorated (VAD) properties is important for maintaining healthy communities. Yet, the process of identifying these properties can be difficult. Here, we create a human-in-the-loop machine learning (HITLML) model called VADecide and apply it to a parcel-level case study in Savannah, Georgia. The results show a higher prediction accuracy than was achieved when using a machine learning model without human input in the training. The HITLML approach also reveals differences between machine vs. human-generated results. Our findings contribute to knowledge about the advantages and challenges of HITLML in urban planning.\n  [Accepted for Publication at a Peer Review Journal]"}, "https://arxiv.org/abs/2407.11254": {"title": "Conquering images and the basis of transformative action", "link": "https://arxiv.org/abs/2407.11254", "description": "arXiv:2407.11254v1 Announce Type: cross \nAbstract: Our rapid immersion into online life has made us all ill. Through the generation, personalization, and dissemination of enchanting imagery, artificial technologies commodify the minds and hearts of the masses with nauseating precision and scale. Online networks, artificial intelligence (AI), social media, and digital news feeds fine-tune our beliefs and pursuits by establishing narratives that subdivide and polarize our communities and identities. Meanwhile those commanding these technologies conquer the final frontiers of our interior lives, social relations, earth, and cosmos. In the Attention Economy, our agency is restricted and our vitality is depleted for their narcissistic pursuits and pleasures. Generative AI empowers the forces that homogenize and eradicate life, not through some stupid \"singularity\" event, but through devaluing human creativity, labor, and social life. Using a fractured lens, we will examine how narratives and networks influence us on mental, social, and algorithmic levels. We will discuss how atomizing imagery -- ideals and pursuits that alienate, rather than invigorate the individual -- hijack people's agency to sustain the forces that destroy them. We will discover how empires build digital networks that optimize society and embolden narcissists to enforce social binaries that perpetuate the ceaseless expansion of consumption, exploitation, and hierarchy. Structural hierarchy in the world is reified through hierarchy in our beliefs and thinking. Only by seeing images as images and appreciating the similarity shared by opposing narratives can we facilitate transformative action and break away from the militaristic systems plaguing our lives."}, "https://arxiv.org/abs/2407.11361": {"title": "Graph Structure Prompt Learning: A Novel Methodology to Improve Performance of Graph Neural Networks", "link": "https://arxiv.org/abs/2407.11361", "description": "arXiv:2407.11361v1 Announce Type: cross \nAbstract: Graph neural networks (GNNs) are widely applied in graph data modeling. However, existing GNNs are often trained in a task-driven manner that fails to fully capture the intrinsic nature of the graph structure, resulting in sub-optimal node and graph representations. To address this limitation, we propose a novel Graph structure Prompt Learning method (GPL) to enhance the training of GNNs, which is inspired by prompt mechanisms in natural language processing. GPL employs task-independent graph structure losses to encourage GNNs to learn intrinsic graph characteristics while simultaneously solving downstream tasks, producing higher-quality node and graph representations. In extensive experiments on eleven real-world datasets, after being trained by GPL, GNNs significantly outperform their original performance on node classification, graph classification, and edge prediction tasks (up to 10.28%, 16.5%, and 24.15%, respectively). By allowing GNNs to capture the inherent structural prompts of graphs in GPL, they can alleviate the issue of over-smooth and achieve new state-of-the-art performances, which introduces a novel and effective direction for GNN research with potential applications in various domains."}, "https://arxiv.org/abs/2407.11907": {"title": "GraphFM: A Scalable Framework for Multi-Graph Pretraining", "link": "https://arxiv.org/abs/2407.11907", "description": "arXiv:2407.11907v1 Announce Type: cross \nAbstract: Graph neural networks are typically trained on individual datasets, often requiring highly specialized models and extensive hyperparameter tuning. This dataset-specific approach arises because each graph dataset often has unique node features and diverse connectivity structures, making it difficult to build a generalist model. To address these challenges, we introduce a scalable multi-graph multi-task pretraining approach specifically tailored for node classification tasks across diverse graph datasets from different domains. Our method, Graph Foundation Model (GraphFM), leverages a Perceiver-based encoder that employs learned latent tokens to compress domain-specific features into a common latent space. This approach enhances the model's ability to generalize across different graphs and allows for scaling across diverse data. We demonstrate the efficacy of our approach by training a model on 152 different graph datasets comprising over 7.4 million nodes and 189 million edges, establishing the first set of scaling laws for multi-graph pretraining on datasets spanning many domains (e.g., molecules, citation and product graphs). Our results show that pretraining on a diverse array of real and synthetic graphs improves the model's adaptability and stability, while performing competitively with state-of-the-art specialist models. This work illustrates that multi-graph pretraining can significantly reduce the burden imposed by the current graph training paradigm, unlocking new capabilities for the field of graph neural networks by creating a single generalist model that performs competitively across a wide range of datasets and tasks."}, "https://arxiv.org/abs/2407.11932": {"title": "Impossibility of latent inner product recovery via rate distortion", "link": "https://arxiv.org/abs/2407.11932", "description": "arXiv:2407.11932v1 Announce Type: cross \nAbstract: In this largely expository note, we present an impossibility result for inner product recovery in a random geometric graph or latent space model using the rate-distortion theory. More precisely, suppose that we observe a graph $A$ on $n$ vertices with average edge density $p$ generated from Gaussian or spherical latent locations $z_1, \\dots, z_n \\in \\mathbb{R}^d$ associated with the $n$ vertices. It is of interest to estimate the inner products $\\langle z_i, z_j \\rangle$ which represent the geometry of the latent points. We prove that it is impossible to recover the inner products if $d \\gtrsim n h(p)$ where $h(p)$ is the binary entropy function. This matches the condition required for positive results on inner product recovery in the literature. The proof follows the well-established rate-distortion theory with the main technical ingredient being a lower bound on the rate-distortion function of the Wishart distribution which is interesting in its own right."}, "https://arxiv.org/abs/2309.13309": {"title": "Independence role in the generalized Sznajd model", "link": "https://arxiv.org/abs/2309.13309", "description": "arXiv:2309.13309v2 Announce Type: replace \nAbstract: The Sznajd model is one of sociophysics's well-known opinion dynamics models. Based on social validation, it has found application in diverse social systems and remains an intriguing subject of study, particularly in scenarios where interacting agents deviate from prevailing norms. This paper investigates the generalized Sznajd model, featuring independent agents on a complete graph and a two-dimensional square lattice. Agents in the network act independently with a probability $p$, signifying a change in their opinion or state without external influence. This model defines a paired agent size $r$, influencing a neighboring agent size $n$ to adopt their opinion. This study incorporates analytical and numerical approaches, especially on the complete graph. Our results show that the macroscopic state of the system remains unaffected by the neighbor size $n$ but is contingent solely on the number of paired agents $r$. Additionally, the time required to reach a stationary state is inversely proportional to the number of neighboring agents $n$. For the two-dimensional square lattice, two critical points $p = p_c$ emerge based on the configuration of agents. The results indicate that the universality class of the model on the complete graph aligns with the mean-field Ising universality class. Furthermore, the universality class of the model on the two-dimensional square lattice, featuring two distinct configurations, is identical and falls within the two-dimensional Ising universality class."}, "https://arxiv.org/abs/2309.16442": {"title": "A Small World of Bad Guys: Investigating the Behavior of Hacker Groups in Cyber-Attacks", "link": "https://arxiv.org/abs/2309.16442", "description": "arXiv:2309.16442v2 Announce Type: replace \nAbstract: This paper explores the behaviour of malicious hacker groups operating in cyberspace and how they organize themselves in structured networks. To better understand these groups, the paper uses Social Network Analysis (SNA) to analyse the interactions and relationships among several malicious hacker groups. The study uses a tested dataset as its primary source, providing an empirical analysis of the cooperative behaviours exhibited by these groups. The study found that malicious hacker groups tend to form close-knit networks where they consult, coordinate with, and assist each other in carrying out their attacks. The study also identified a \"small world\" phenomenon within the population of malicious actors, which suggests that these groups establish interconnected relationships to facilitate their malicious operations. The small world phenomenon indicates that the actor-groups are densely connected, but they also have a small number of connections to other groups, allowing for efficient communication and coordination of their activities."}, "https://arxiv.org/abs/2405.03266": {"title": "Efficient computation of Katz centrality for very dense networks via negative parameter Katz", "link": "https://arxiv.org/abs/2405.03266", "description": "arXiv:2405.03266v2 Announce Type: replace \nAbstract: Katz centrality (and its limiting case, eigenvector centrality) is a frequently used tool to measure the importance of a node in a network, and to rank the nodes accordingly. One reason for its popularity is that Katz centrality can be computed very efficiently when the network is sparse, i.e., having only $O(n)$ edges between its $n$ nodes. While sparsity is common in practice, in some applications one faces the opposite situation of a very dense network, where only $O(n)$ potential edges are missing with respect to a complete graph. We explain why and how, even for very dense networks, it is possible to efficiently compute the ranking stemming from Katz centrality for unweighted graphs, possibly directed and possibly with loops, by working on the complement graph. Our approach also provides an interpretation, regardless of sparsity, of \"Katz centrality with negative parameter\" as usual Katz centrality on the complement graph. For weighted graphs, we provide instead an approximation method that is based on removing sufficiently many edges from the network (or from its complement), and we give sufficient conditions for this approximation to provide the correct ranking. We include numerical experiments to illustrate the advantages of the proposed approach."}, "https://arxiv.org/abs/2402.14947": {"title": "An Avalanche of Images on Telegram Preceded Russia's Full-Scale Invasion of Ukraine", "link": "https://arxiv.org/abs/2402.14947", "description": "arXiv:2402.14947v2 Announce Type: replace-cross \nAbstract: Governments use propaganda, including through visual content -- or Politically Salient Image Patterns (PSIP) -- on social media, to influence and manipulate public opinion. In the present work, we collected Telegram post-history of from 989 Russian milbloggers to better understand the social and political narratives that circulated online in the months surrounding Russia's 2022 full-scale invasion of Ukraine. Overall, we found an 8,925% increase (p<0.001) in the number of posts and a 5,352% increase (p<0.001) in the number of images posted by these accounts in the two weeks prior to the invasion. We also observed a similar increase in the number and intensity of politically salient manipulated images that circulated on Telegram. Although this paper does not evaluate malice or coordination in these activities, we do conclude with a call for further research into the role that manipulated visual media has in the lead-up to instability events and armed conflict."}, "https://arxiv.org/abs/2403.02011": {"title": "Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks", "link": "https://arxiv.org/abs/2403.02011", "description": "arXiv:2403.02011v2 Announce Type: replace-cross \nAbstract: We propose a method to represent bipartite networks using graph embeddings tailored to tackle the challenges of studying ecological networks, such as the ones linking plants and pollinators, where many covariates need to be accounted for, in particular to control for sampling bias. We adapt the variational graph auto-encoder approach to the bipartite case, which enables us to generate embeddings in a latent space where the two sets of nodes are positioned based on their probability of connection. We translate the fairness framework commonly considered in sociology in order to address sampling bias in ecology. By incorporating the Hilbert-Schmidt independence criterion (HSIC) as an additional penalty term in the loss we optimize, we ensure that the structure of the latent space is independent of continuous variables, which are related to the sampling process. Finally, we show how our approach can change our understanding of ecological networks when applied to the Spipoll data set, a citizen science monitoring program of plant-pollinator interactions to which many observers contribute, making it prone to sampling bias."}, "https://arxiv.org/abs/2407.12063": {"title": "Unveiling Scaling Laws in the Regulatory Functions of Reddit", "link": "https://arxiv.org/abs/2407.12063", "description": "arXiv:2407.12063v1 Announce Type: new \nAbstract: Online platforms like Reddit, Wikipedia, and Facebook are integral to modern life, enabling content creation and sharing through posts, comments, and discussions. Despite their virtual and often anonymous nature, these platforms need rules and oversight to maintain a safe and productive environment. As these communities grow, a key question arises: how does the need for regulatory functions scale? Do larger groups require more regulatory actions and oversight per person, or can they manage with less? Our analysis of Reddit's regulatory functions reveals robust scaling relationships across different subreddits, suggesting universal patterns between community size and the amount of regulation needed. We found that the number of comments and moderator actions, such as comment removals, grew faster than the community size, with superlinear exponents of 1.12 and 1.18, respectively. However, bot-based rule enforcement did not keep pace with community growth, exhibiting a slightly sublinear exponent of 0.95. Further analysis of the residuals from these scaling behaviors identified a 'trade-off axis,' where one-way coordination mechanisms (bots and moderators) counteract two-way interactions (comments) and vice versa. Our findings suggest that a more proactive moderation approach, characterized by increased bot activity and moderator comment removals, tends to result in less user engagement under the scaling framework. Understanding these natural scaling patterns and interactions can help platform administrators and policymakers foster healthy online communities while mitigating harmful behaviors such as harassment, doxxing, and misinformation. Without proper regulation, these negative behaviors can proliferate and cause significant damage. Targeted interventions based on these insights are key to ensuring online platforms remain safe and beneficial spaces."}, "https://arxiv.org/abs/2407.12123": {"title": "Explicit solutions of the Bass and SI models on hypernetworks", "link": "https://arxiv.org/abs/2407.12123", "description": "arXiv:2407.12123v1 Announce Type: new \nAbstract: We analyze the Bass model and the Susceptible-Infected (SI) model on hypergraphs with 3-body interactions. We derive the master equations for general hypernetworks, and use them to obtain explicit expressions for the expected adoption/infection level on infinite complete hypernetworks, infinite Erd\\H{o}s-R\\'enyi hypernetworks, and on infinite hyperlines. These expressions are exact, as they are derived without making any approximation."}, "https://arxiv.org/abs/2407.12146": {"title": "Physical partisan proximity outweighs online ties in predicting US voting outcomes", "link": "https://arxiv.org/abs/2407.12146", "description": "arXiv:2407.12146v1 Announce Type: new \nAbstract: Affective polarization and increasing social divisions affect social mixing and the spread of information across online and physical spaces, reinforcing social and electoral cleavages and influencing political outcomes. Here, using aggregated and de-identified co-location and online network data, we investigate the relationship between partisan exposure and voting patterns in the USA by comparing three dimensions of partisan exposure: physical proximity and exposure to the same social contexts, online social ties, and residential sorting. By leveraging various statistical modeling approaches, we consistently find that partisan exposure in the physical space, as captured by co-location patterns, more accurately predicts electoral outcomes in US counties, outperforming online and residential exposures across metropolitan and non-metro areas. Moreover, our results show that physical partisan proximity is the best predictor of voting patterns in swing counties, where the election results are most uncertain. We also estimate county-level experienced partisan segregation and examine its relationship with individuals' demographic and socioeconomic characteristics. Focusing on metropolitan areas, our results confirm the presence of extensive partisan segregation in the US and show that offline partisan isolation, both considering physical encounters or residential sorting, is higher than online segregation and is primarily associated with educational attainment. Our findings emphasize the importance of physical space in understanding the relationship between social networks and political behavior, in contrast to the intense scrutiny focused on online social networks and elections."}, "https://arxiv.org/abs/2407.12612": {"title": "American cities are defined by isolated rings and pockets characterized by limited socio-economic mixing", "link": "https://arxiv.org/abs/2407.12612", "description": "arXiv:2407.12612v1 Announce Type: new \nAbstract: Cities generate gains from interaction, but citizens often experience segregation as they move around the urban environment. Using GPS location data, we identify four distinct patterns of experienced segregation across US cities. Most common are affluent or poor neighborhoods where visitors lack diversity and residents have limited exposure to diversity elsewhere. Less frequent are majority-minority areas where residents must travel for diverse encounters, and wealthy urban zones with diverse visitors but where locals sort into homogeneous amenities. By clustering areas with similar mobility signatures, we uncover rings around cities and internal pockets where intergroup interaction is limited. Using a decision tree, we show that demography and location interact to create these zones. Our findings, persistent across time and prevalent across US cities, highlight the importance of considering both who is mixing and where in urban environments. Understanding the mesoscopic patterns that define experienced segregation in America illuminates neighborhood advantage and disadvantage, enabling interventions to foster economic opportunity and urban dynamism."}, "https://arxiv.org/abs/2407.12681": {"title": "Dynamics of Cities", "link": "https://arxiv.org/abs/2407.12681", "description": "arXiv:2407.12681v1 Announce Type: new \nAbstract: This study investigates city dynamics employing a nonextensive diffusion equation suited for addressing diffusion within a fractal medium, where the nonadditive parameter, $q$, plays a relevant role. The findings demonstrate the efficacy of this approach in determining the relation between the fractal dimension of the city, the allometric exponent and $q$, and elucidating the stationary phase of urban evolution. The dynamic methodology facilitates the correlation of the fractal dimension with both the entropic index and the urban scaling exponent identified in data analyses. The results reveal that the scaling behaviour observed in cities aligns with the fractal dimension measured through independent methods. Moreover, the interpretation of these findings underscores the intimate connection between the fractal dimension and social interactions within the urban context. This research contributes to a deeper comprehension of the intricate interplay between human behaviour, urban dynamics, and the underlying fractal nature of cities."}, "https://arxiv.org/abs/2407.12728": {"title": "Exploring the interplay of individual traits and interaction dynamics in preschool social networks", "link": "https://arxiv.org/abs/2407.12728", "description": "arXiv:2407.12728v1 Announce Type: new \nAbstract: Several studies have investigated human interaction using modern tracking techniques for face-to-face encounters across various settings and age groups. However, little attention has been given to understanding how individual characteristics relate to social behavior. This is particularly important in younger age groups due to its potential effects on early childhood development. In this study, conducted during the Complexity 72h Workshop, we analyze human social interactions in a French preschool, where children's face-to-face interactions were monitored using proximity sensors over an academic year. We use metadata from parent surveys and preschool linguistic tests, covering demographic information and home habits, to examine the interplay between individual characteristics and contact patterns. Using a mixture of approaches, from random forest classifiers to network-based metrics at both dyadic and higher-order (group) levels, we identify sex, age, language scores, and number of siblings as the variables displaying the most significant associations with interaction patterns. We explore these variables' relationships to interactions within and outside classrooms and across mixed and single-grade classes. At the group level, we investigate how group affinity affects group persistence. We also find that higher-order network centrality (hypercoreness) is higher among children with siblings, indicating different group embedding despite similar total contact duration. This study aligns with existing literature on early social development and highlights the importance of integrating individual traits into the study of human interactions. Focusing on 2-5-year-olds offers insights into emerging social preferences during critical phases of cognitive development. Future research could use these findings to enhance mechanistic models of complex social systems by incorporating individual traits."}, "https://arxiv.org/abs/2407.12771": {"title": "The Role of Network and Identity in the Diffusion of Hashtags", "link": "https://arxiv.org/abs/2407.12771", "description": "arXiv:2407.12771v1 Announce Type: new \nAbstract: Although the spread of behaviors is influenced by many social factors, existing literature tends to study the effects of single factors -- most often, properties of the social network -- on the final cascade. In order to move towards a more integrated view of cascades, this paper offers the first comprehensive investigation into the role of two social factors in the diffusion of 1,337 popular hashtags representing the production of novel culture on Twitter: 1) the topology of the Twitter social network and 2) performance of each user's probable demographic identity. Here, we show that cascades are best modeled using a combination of network and identity, rather than either factor alone. This combined model best reproduces a composite index of ten cascade properties across all 1,337 hashtags. However, there is important heterogeneity in what social factors are required to reproduce different properties of hashtag cascades. For instance, while a combined network+identity model best predicts the popularity of cascades, a network-only model has better performance in predicting cascade growth and an identity-only model in adopter composition. We are able to predict what type of hashtag is best modeled by each combination of features and use this to further improve performance. Additionally, consistent with prior literature on the combined network+identity model most outperforms the single-factor counterfactuals among hashtags used for expressing racial or regional identity, stance-taking, talking about sports, or variants of existing cultural trends with very slow- or fast-growing communicative need. In sum, our results imply the utility of multi-factor models in predicting cascades, in order to account for the varied ways in which network, identity, and other social factors play a role in the diffusion of hashtags on Twitter."}, "https://arxiv.org/abs/2407.12112": {"title": "A Benchmark for Fairness-Aware Graph Learning", "link": "https://arxiv.org/abs/2407.12112", "description": "arXiv:2407.12112v1 Announce Type: cross \nAbstract: Fairness-aware graph learning has gained increasing attention in recent years. Nevertheless, there lacks a comprehensive benchmark to evaluate and compare different fairness-aware graph learning methods, which blocks practitioners from choosing appropriate ones for broader real-world applications. In this paper, we present an extensive benchmark on ten representative fairness-aware graph learning methods. Specifically, we design a systematic evaluation protocol and conduct experiments on seven real-world datasets to evaluate these methods from multiple perspectives, including group fairness, individual fairness, the balance between different fairness criteria, and computational efficiency. Our in-depth analysis reveals key insights into the strengths and limitations of existing methods. Additionally, we provide practical guidance for applying fairness-aware graph learning methods in applications. To the best of our knowledge, this work serves as an initial step towards comprehensively understanding representative fairness-aware graph learning methods to facilitate future advancements in this area."}, "https://arxiv.org/abs/2407.12269": {"title": "UTG: Towards a Unified View of Snapshot and Event Based Models for Temporal Graphs", "link": "https://arxiv.org/abs/2407.12269", "description": "arXiv:2407.12269v1 Announce Type: cross \nAbstract: Temporal graphs have gained increasing importance due to their ability to model dynamically evolving relationships. These graphs can be represented through either a stream of edge events or a sequence of graph snapshots. Until now, the development of machine learning methods for both types has occurred largely in isolation, resulting in limited experimental comparison and theoretical crosspollination between the two. In this paper, we introduce Unified Temporal Graph (UTG), a framework that unifies snapshot-based and event-based machine learning models under a single umbrella, enabling models developed for one representation to be applied effectively to datasets of the other. We also propose a novel UTG training procedure to boost the performance of snapshot-based models in the streaming setting. We comprehensively evaluate both snapshot and event-based models across both types of temporal graphs on the temporal link prediction task. Our main findings are threefold: first, when combined with UTG training, snapshotbased models can perform competitively with event-based models such as TGN and GraphMixer even on event datasets. Second, snapshot-based models are at least an order of magnitude faster than most event-based models during inference. Third, while event-based methods such as NAT and DyGFormer outperforms snapshotbased methods on both types of temporal graphs, this is because they leverage joint neighborhood structural features thus emphasizing the potential to incorporate these features into snapshot-based models as well. These findings highlight the importance of comparing model architectures independent of the data format and suggest the potential of combining the efficiency of snapshot-based models with the performance of event-based models in the future."}, "https://arxiv.org/abs/2407.12451": {"title": "Across Platforms and Languages: Dutch Influencers and Legal Disclosures on Instagram, YouTube and TikTok", "link": "https://arxiv.org/abs/2407.12451", "description": "arXiv:2407.12451v1 Announce Type: cross \nAbstract: Content monetization on social media fuels a growing influencer economy. Influencer marketing remains largely undisclosed or inappropriately disclosed on social media. Non-disclosure issues have become a priority for national and supranational authorities worldwide, who are starting to impose increasingly harsher sanctions on them. This paper proposes a transparent methodology for measuring whether and how influencers comply with disclosures based on legal standards. We introduce a novel distinction between disclosures that are legally sufficient (green) and legally insufficient (yellow). We apply this methodology to an original dataset reflecting the content of 150 Dutch influencers publicly registered with the Dutch Media Authority based on recently introduced registration obligations. The dataset consists of 292,315 posts and is multi-language (English and Dutch) and cross-platform (Instagram, YouTube and TikTok). We find that influencer marketing remains generally underdisclosed on social media, and that bigger influencers are not necessarily more compliant with disclosure standards."}, "https://arxiv.org/abs/2407.12545": {"title": "Conspiracy theories and where to find them on TikTok", "link": "https://arxiv.org/abs/2407.12545", "description": "arXiv:2407.12545v1 Announce Type: cross \nAbstract: TikTok has skyrocketed in popularity over recent years, especially among younger audiences, thanks to its viral trends and social challenges. However, concerns have been raised about the potential of this platform to promote and amplify online harmful and dangerous content. Leveraging the official TikTok Research API and collecting a longitudinal dataset of 1.5M videos shared in the US over a period of 3 years, our study analyzes the presence of videos promoting conspiracy theories, providing a lower-bound estimate of their prevalence (approximately 0.1% of all videos) and assessing the effects of the new Creator Program, which provides new ways for creators to monetize, on the supply of conspiratorial content. We evaluate the capabilities of state-of-the-art open Large Language Models to identify conspiracy theories after extracting audio transcriptions of videos, finding that they can detect harmful content with high precision but with overall performance comparable to fine-tuned traditional language models such as RoBERTa. Our findings are instrumental for content moderation strategies that aim to understand and mitigate the spread of harmful content on rapidly evolving social media platforms like TikTok."}, "https://arxiv.org/abs/2407.12604": {"title": "Exact Graph Matching in Correlated Gaussian-Attributed Erd\\H{o}s-R\\'enyi Model", "link": "https://arxiv.org/abs/2407.12604", "description": "arXiv:2407.12604v1 Announce Type: cross \nAbstract: Graph matching problem aims to identify node correspondence between two or more correlated graphs. Previous studies have primarily focused on models where only edge information is provided. However, in many social networks, not only the relationships between users, represented by edges, but also their personal information, represented by features, are present. In this paper, we address the challenge of identifying node correspondence in correlated graphs, where additional node features exist, as in many real-world settings. We propose a two-step procedure, where we initially match a subset of nodes only using edge information, and then match the remaining nodes using node features. We derive information-theoretic limits for exact graph matching on this model. Our approach provides a comprehensive solution to the real-world graph matching problem by providing systematic ways to utilize both edge and node information for exact matching of the graphs."}, "https://arxiv.org/abs/2407.12690": {"title": "The Dual Imperative: Innovation and Regulation in the AI Era", "link": "https://arxiv.org/abs/2407.12690", "description": "arXiv:2407.12690v1 Announce Type: cross \nAbstract: This article addresses the societal costs associated with the lack of regulation in Artificial Intelligence and proposes a framework combining innovation and regulation. Over fifty years of AI research, catalyzed by declining computing costs and the proliferation of data, have propelled AI into the mainstream, promising significant economic benefits. Yet, this rapid adoption underscores risks, from bias amplification and labor disruptions to existential threats posed by autonomous systems. The discourse is polarized between accelerationists, advocating for unfettered technological advancement, and doomers, calling for a slowdown to prevent dystopian outcomes. This piece advocates for a middle path that leverages technical innovation and smart regulation to maximize the benefits of AI while minimizing its risks, offering a pragmatic approach to the responsible progress of AI technology. Technical invention beyond the most capable foundation models is needed to contain catastrophic risks. Regulation is required to create incentives for this research while addressing current issues."}, "https://arxiv.org/abs/2407.12761": {"title": "The ECFA Early-Career Researchers Panel: Report for the year 2023", "link": "https://arxiv.org/abs/2407.12761", "description": "arXiv:2407.12761v1 Announce Type: cross \nAbstract: The European Committee for Future Accelerators (ECFA) Early-Career Researcher (ECR) panel, which represents the interests of the ECR community to ECFA, presents in this document its initiatives and activities in the year 2023. This report summarises the process of the first big turnover in the panel composition at the start of 2023 and reports on the activities of the active working groups - either pursued from before or newly established. The overarching goal of the ECFA-ECR panel is to better understand and support the diverse interests of early-career researchers in the ECFA community and beyond."}, "https://arxiv.org/abs/2307.04284": {"title": "Effects of Network Connectivity and Functional Diversity Distribution on Human Collective Ideation", "link": "https://arxiv.org/abs/2307.04284", "description": "arXiv:2307.04284v3 Announce Type: replace \nAbstract: Human collective tasks in teams and organizations increasingly require participation of members with diverse backgrounds working in networked social environments. However, little is known about how network structure and the functional diversity of member backgrounds would interact with each other and affect collective processes. Here we conducted three sets of human-subject experiments which involved 617 university students who collaborated anonymously in a collective ideation task on a custom-made online social network platform. We found that spatially clustered collectives with assortative background distribution tended to explore more diverse ideas than in other conditions, whereas collectives with random background distribution consistently generated ideas with the highest utility. We also found that higher network connectivity may improve individuals' overall experience but may not improve the collective performance regarding idea generation, idea diversity, and final idea quality."}, "https://arxiv.org/abs/2307.11398": {"title": "The precursor of the critical transitions in majority vote model with the noise feedback from the vote layer", "link": "https://arxiv.org/abs/2307.11398", "description": "arXiv:2307.11398v4 Announce Type: replace \nAbstract: In this paper, we investigate phase transitions in the Majority-Vote model coupled with noise layers of different structures. We examine the Square lattice and Random-regular networks, as well as their combinations, for both vote layers and noise layers. Our findings reveal the presence of independent third-order transitions in all cases and dependent third-order transitions when critical transitions occur. This suggests that dependent third-order transitions may serve as precursors to critical transitions in non-equilibrium systems. Furthermore, we observe that when the structure of vote layers is local, the coupling between the vote layer and the noise layer leads to the absence of critical phenomena."}, "https://arxiv.org/abs/2308.02623": {"title": "Epidemic spreading under game-based self-quarantine behaviors: The different effects of local and global information", "link": "https://arxiv.org/abs/2308.02623", "description": "arXiv:2308.02623v2 Announce Type: replace \nAbstract: During the outbreak of an epidemic, individuals may modify their behaviors in response to external (including local and global) infection-related information. However, the difference between local and global information in influencing the spread of diseases remains inadequately explored. Here we study a simple epidemic model that incorporates the game-based self-quarantine behavior of individuals, taking into account the influence of local infection status, global disease prevalence and node heterogeneity (non-identical degree distribution). Our findings reveal that local information can effectively contain an epidemic, even with only a small proportion of individuals opting for self-quarantine. On the other hand, global information can cause infection evolution curves shaking during the declining phase of an epidemic, owing to the synchronous release of nodes with the same degree from the quarantined state. In contrast, the releasing pattern under the local information appears to be more random. This shaking phenomenon can be observed in various types of networks associated with different characteristics. Moreover, it is found that under the proposed game-epidemic framework, a disease is more difficult to spread in heterogeneous networks than in homogeneous networks, which differs from conventional epidemic models."}, "https://arxiv.org/abs/2312.12651": {"title": "Toxic Bias: Perspective API Misreads German as More Toxic", "link": "https://arxiv.org/abs/2312.12651", "description": "arXiv:2312.12651v3 Announce Type: replace \nAbstract: Proprietary public APIs play a crucial and growing role as research tools among social scientists. Among such APIs, Google's machine learning-based Perspective API is extensively utilized for assessing the toxicity of social media messages, providing both an important resource for researchers and automatic content moderation. However, this paper exposes an important bias in Perspective API concerning German language text. Through an in-depth examination of several datasets, we uncover intrinsic language biases within the multilingual model of Perspective API. We find that the toxicity assessment of German content produces significantly higher toxicity levels than other languages. This finding is robust across various translations, topics, and data sources, and has significant consequences for both research and moderation strategies that rely on Perspective API. For instance, we show that, on average, four times more tweets and users would be moderated when using the German language compared to their English translation. Our findings point to broader risks associated with the widespread use of proprietary APIs within the computational social sciences."}, "https://arxiv.org/abs/2407.12864": {"title": "Clustering Time-Evolving Networks Using the Dynamic Graph Laplacian", "link": "https://arxiv.org/abs/2407.12864", "description": "arXiv:2407.12864v1 Announce Type: new \nAbstract: Time-evolving graphs arise frequently when modeling complex dynamical systems such as social networks, traffic flow, and biological processes. Developing techniques to identify and analyze communities in these time-varying graph structures is an important challenge. In this work, we generalize existing spectral clustering algorithms from static to dynamic graphs using canonical correlation analysis (CCA) to capture the temporal evolution of clusters. Based on this extended canonical correlation framework, we define the dynamic graph Laplacian and investigate its spectral properties. We connect these concepts to dynamical systems theory via transfer operators, and illustrate the advantages of our method on benchmark graphs by comparison with existing methods. We show that the dynamic graph Laplacian allows for a clear interpretation of cluster structure evolution over time for directed and undirected graphs."}, "https://arxiv.org/abs/2407.12876": {"title": "Exploring the Use of Abusive Generative AI Models on Civitai", "link": "https://arxiv.org/abs/2407.12876", "description": "arXiv:2407.12876v1 Announce Type: new \nAbstract: The rise of generative AI is transforming the landscape of digital imagery, and exerting a significant influence on online creative communities. This has led to the emergence of AI-Generated Content (AIGC) social platforms, such as Civitai. These distinctive social platforms allow users to build and share their own generative AI models, thereby enhancing the potential for more diverse artistic expression. Designed in the vein of social networks, they also provide artists with the means to showcase their creations (generated from the models), engage in discussions, and obtain feedback, thus nurturing a sense of community. Yet, this openness also raises concerns about the abuse of such platforms, e.g., using models to disseminate deceptive deepfakes or infringe upon copyrights. To explore this, we conduct the first comprehensive empirical study of an AIGC social platform, focusing on its use for generating abusive content. As an exemplar, we construct a comprehensive dataset covering Civitai, the largest available AIGC social platform. Based on this dataset of 87K models and 2M images, we explore the characteristics of content and discuss strategies for moderation to better govern these platforms."}, "https://arxiv.org/abs/2407.12968": {"title": "Multi-Platform Framing Analysis: A Case Study of Kristiansand Quran Burning", "link": "https://arxiv.org/abs/2407.12968", "description": "arXiv:2407.12968v1 Announce Type: new \nAbstract: The framing of events in various media and discourse spaces is crucial in the era of misinformation and polarization. Many studies, however, are limited to specific media or networks, disregarding the importance of cross-platform diffusion. This study overcomes that limitation by conducting a multi-platform framing analysis on Twitter, YouTube, and traditional media analyzing the 2019 Koran burning in Kristiansand, Norway. It examines media and policy frames and uncovers network connections through shared URLs. The findings show that online news emphasizes the incident's legality, while social media focuses on its morality, with harsh hate speech prevalent in YouTube comments. Additionally, YouTube is identified as the most self-contained community, whereas Twitter is the most open to external inputs."}, "https://arxiv.org/abs/2407.13161": {"title": "How to quantify an examination? Evidence from physics examinations via complex networks", "link": "https://arxiv.org/abs/2407.13161", "description": "arXiv:2407.13161v1 Announce Type: new \nAbstract: Given the untapped potential for continuous improvement of examinations, quantitative investigations of examinations could guide efforts to considerably improve learning efficiency and evaluation and thus greatly help both learners and educators. However, there is a general lack of quantitative methods for investigating examinations. To address this gap, we propose a new metric via complex networks; i.e., the knowledge point network (KPN) of an examination is constructed by representing the knowledge points (concepts, laws, etc.) as nodes and adding links when these points appear in the same question. Then, the topological quantities of KPNs, such as degree, centrality, and community, can be employed to systematically explore the structural properties and evolution of examinations. In this work, 35 physics examinations from the NCEE examination spanning from 2006 to 2020 were investigated as an evidence. We found that the constructed KPNs are scale-free networks that show strong assortativity and small-world effects in most cases. The communities within the KPNs are obvious, and the key nodes are mainly related to mechanics and electromagnetism. Different question types are related to specific knowledge points, leading to noticeable structural variations in KPNs. Moreover, changes in the KPN topology between examinations administered in different years may offer insights guiding college entrance examination reforms. Based on topological quantities such as the average degree, network density, average clustering coefficient, and network transitivity, the Fd is proposed to evaluate examination difficulty. All the above results show that our approach can comprehensively quantify the knowledge structures and examination characteristics. These networks may elucidate comprehensive examination knowledge graphs for educators and guide improvements in teaching."}, "https://arxiv.org/abs/2407.13206": {"title": "Double stochastic opinion dynamics with fractional inflow of new opinions", "link": "https://arxiv.org/abs/2407.13206", "description": "arXiv:2407.13206v1 Announce Type: new \nAbstract: A recent analysis of empirical limit order flow data highlights the necessity for a more refined order flow model that integrates the power-law distribution of limit order cancellation times. These cancellation times follow a discrete probability mass function derived from the Tsallis $q$-exponential distribution, or equivalently, the second form of the Pareto distribution. By combining fractional L'{e}vy stable motion as the model for limit order inflow with the power-law distribution for cancellation times, we propose an innovative approach to modeling order imbalance in financial markets. We extend this model to a broader context, illustrating its applicability to opinion dynamics in social systems where opinions have a finite lifespan. This proposed model exemplifies a stochastic time series characterized by stationary increments and broken self-similarity. Consequently, it offers a novel framework for testing methods to evaluate long-range dependence in such time series."}, "https://arxiv.org/abs/2407.13549": {"title": "Evaluating the effect of viral news on social media engagement", "link": "https://arxiv.org/abs/2407.13549", "description": "arXiv:2407.13549v1 Announce Type: new \nAbstract: This study examines Facebook and YouTube content from over a thousand news outlets in four European languages from 2018 to 2023, using a Bayesian structural time-series model to evaluate the impact of viral posts. Our results show that most viral events do not significantly increase engagement and rarely lead to sustained growth. The virality effect usually depends on the engagement trend preceding the viral post, typically reversing it. When news emerges unexpectedly, viral events enhances users' engagement, reactivating the collective response process. In contrast, when virality manifests after a sustained growth phase, it represents the final burst of that growth process, followed by a decline in attention. Moreover, quick viral effects fade faster, while slower processes lead to more persistent growth. These findings highlight the transient effect of viral events and underscore the importance of consistent, steady attention-building strategies to establish a solid connection with the user base rather than relying on sudden visibility spikes."}, "https://arxiv.org/abs/2407.13714": {"title": "Mapping Inter-City Trade Networks to Maximum Entropy Models using Electronic Invoice Data", "link": "https://arxiv.org/abs/2407.13714", "description": "arXiv:2407.13714v1 Announce Type: new \nAbstract: We analyze the network of transactions among cities based on the electronic invoice database for the municipalities in the Cear\\'a state, Brazil. It consists of approximately 3.7 billion records, registered during the period between the years 2016 to 2019. All the transactions are grouped in a unique dataset and represented as an asymmetrical adjacency matrix corresponding to a directed graph with connections weighted by the number of transactions among cities. Due to the large size of Cear\\'a state, its unequal distribution of wealth, and spatially heterogeneous population, we initially determine communities of cities based on their mutual intensity of trades and verify to which extent their economic interests reflect a community cohesiveness. For the first task, we use the Infomap algorithm to detect the partition which provides the shortest description length and captures the optimal community structure of the network in terms of its associated flow dynamics. Surprisingly, the partition identified has five modules, whose two-dimensional geographical projections are all simply-connected domains. We proceed with the analysis of traded products by building bipartite structures represented in terms of adjacency matrices between municipalities and products, considering selling and buying. Using the revealed comparative advantage (RCA) concept, we define a non-monetary and binary activity index that can distinguish the RCA of a city in a class of goods or services as evidenced by trade flows. Finally, through the pairwise Maximum Entropy Model, we can associate to the largest communities their corresponding binary Ising-like Hamiltonian models. In an analogy with critical phenomena, our results reveal that each community operates at a \"temperature\" that is close to the corresponding \"critical point\", suggesting a high degree of \"economic cohesiveness\" in its trade network of cities."}, "https://arxiv.org/abs/2407.13071": {"title": "Analysing the Public Discourse around OpenAI's Text-To-Video Model 'Sora' using Topic Modeling", "link": "https://arxiv.org/abs/2407.13071", "description": "arXiv:2407.13071v1 Announce Type: cross \nAbstract: The recent introduction of OpenAI's text-to-video model Sora has sparked widespread public discourse across online communities. This study aims to uncover the dominant themes and narratives surrounding Sora by conducting topic modeling analysis on a corpus of 1,827 Reddit comments from five relevant subreddits (r/OpenAI, r/technology, r/singularity, r/vfx, and r/ChatGPT). The comments were collected over a two-month period following Sora's announcement in February 2024. After preprocessing the data, Latent Dirichlet Allocation (LDA) was employed to extract four key topics: 1) AI Impact and Trends in Sora Discussions, 2) Public Opinion and Concerns about Sora, 3) Artistic Expression and Video Creation with Sora, and 4) Sora's Applications in Media and Entertainment. Visualizations including word clouds, bar charts, and t-SNE clustering provided insights into the importance of topic keywords and the distribution of comments across topics. The results highlight prominent narratives around Sora's potential impact on industries and employment, public sentiment and ethical concerns, creative applications, and use cases in the media and entertainment sectors. While limited to Reddit data within a specific timeframe, this study offers a framework for understanding public perceptions of emerging generative AI technologies through online discourse analysis."}, "https://arxiv.org/abs/2407.13251": {"title": "Motif-Consistent Counterfactuals with Adversarial Refinement for Graph-Level Anomaly Detection", "link": "https://arxiv.org/abs/2407.13251", "description": "arXiv:2407.13251v1 Announce Type: cross \nAbstract: Graph-level anomaly detection is significant in diverse domains. To improve detection performance, counterfactual graphs have been exploited to benefit the generalization capacity by learning causal relations. Most existing studies directly introduce perturbations (e.g., flipping edges) to generate counterfactual graphs, which are prone to alter the semantics of generated examples and make them off the data manifold, resulting in sub-optimal performance. To address these issues, we propose a novel approach, Motif-consistent Counterfactuals with Adversarial Refinement (MotifCAR), for graph-level anomaly detection. The model combines the motif of one graph, the core subgraph containing the identification (category) information, and the contextual subgraph (non-motif) of another graph to produce a raw counterfactual graph. However, the produced raw graph might be distorted and cannot satisfy the important counterfactual properties: Realism, Validity, Proximity and Sparsity. Towards that, we present a Generative Adversarial Network (GAN)-based graph optimizer to refine the raw counterfactual graphs. It adopts the discriminator to guide the generator to generate graphs close to realistic data, i.e., meet the property Realism. Further, we design the motif consistency to force the motif of the generated graphs to be consistent with the realistic graphs, meeting the property Validity. Also, we devise the contextual loss and connection loss to control the contextual subgraph and the newly added links to meet the properties Proximity and Sparsity. As a result, the model can generate high-quality counterfactual graphs. Experiments demonstrate the superiority of MotifCAR."}, "https://arxiv.org/abs/2407.13325": {"title": "Can nuclear energy contribute to the energy transition?", "link": "https://arxiv.org/abs/2407.13325", "description": "arXiv:2407.13325v1 Announce Type: cross \nAbstract: In the course of the energy transition, energy generation from nuclear power - through nuclear fission and perhaps in the future through nuclear fusion - is often proposed as an alternative or supplement to renewable energy sources. There are already good reasons why electricity generation from nuclear energy is significantly more expensive than other forms of generation, while increasing dryness as a result of climate change is generally calling into question the reliability of thermal power plants. Nuclear energy is therefore unlikely to play a role in a future energy supply that relies on low costs and reliability."}, "https://arxiv.org/abs/2407.13566": {"title": "Decentralised Governance for Autonomous Cyber-Physical Systems", "link": "https://arxiv.org/abs/2407.13566", "description": "arXiv:2407.13566v1 Announce Type: cross \nAbstract: This paper examines the potential for Cyber-Physical Systems (CPS) to be governed in a decentralised manner, whereby blockchain-based infrastructure facilitates the communication between digital and physical domains through self-governing and self-organising principles. Decentralised governance paradigms that integrate computation in physical domains (such as 'Decentralised Autonomous Organisations' (DAOs)) represent a novel approach to autono-mous governance and operations. These have been described as akin to cybernetic systems. Through the lens of a case study of an autonomous cabin called \"no1s1\" which demonstrates self-ownership via blockchain-based control and feedback loops, this research explores the potential for blockchain infrastructure to be utilised in the management of physical systems. By highlighting the considerations and challenges of decentralised governance in managing autonomous physical spaces, the study reveals that autonomy in the governance of autonomous CPS is not merely a technological feat but also involves a complex mesh of functional and social dynamics. These findings underscore the importance of developing continuous feedback loops and adaptive governance frameworks within decentralised CPS to address both expected and emergent challenges. This investigation contributes to the fields of infra-structure studies and Cyber-Physical Systems engineering. It also contributes to the discourse on decentralised governance and autonomous management of physical spaces by offering both practical insights and providing a framework for future research."}, "https://arxiv.org/abs/2307.15401": {"title": "Breathing Green: Maximising Health and Environmental Benefits for Active Transportation Users Leveraging Large Scale Air Quality Data", "link": "https://arxiv.org/abs/2307.15401", "description": "arXiv:2307.15401v4 Announce Type: replace \nAbstract: Pollution in urban areas can have significant adverse effects on the health and well-being of citizens, with traffic-related air pollution being a major concern in many cities. Pollutants emitted by vehicles, such as nitrogen oxides, carbon monoxide, and particulate matter, can cause respiratory and cardiovascular problems, particularly for vulnerable road users like pedestrians and cyclists. Furthermore, recent research has indicated that individuals living in more polluted areas are at a greater risk of developing chronic illnesses such as asthma, allergies, and cancer. Addressing these problems is crucial to protecting public health and maximising environmental benefits. In this project, we explore the feasibility of tackling this challenge by leveraging big data analysis and data-driven methods. Specifically, we investigate the recently released Google Air Quality dataset and devise an optimisation strategy to suggest green travel routes for different types of active transportation users in Dublin. To demonstrate our achievement, we have developed a prototype and have shown that citizens who use our model to plan their outdoor activities can benefit notably, with a significant decrease of 17.87% on average in pollutant intake, from the environmental advantages it offers."}, "https://arxiv.org/abs/2403.14951": {"title": "Simple Graph Condensation", "link": "https://arxiv.org/abs/2403.14951", "description": "arXiv:2403.14951v2 Announce Type: replace-cross \nAbstract: The burdensome training costs on large-scale graphs have aroused significant interest in graph condensation, which involves tuning Graph Neural Networks (GNNs) on a small condensed graph for use on the large-scale original graph. Existing methods primarily focus on aligning key metrics between the condensed and original graphs, such as gradients, output distribution and trajectories of GNNs, yielding satisfactory performance on downstream tasks. However, these complex metrics necessitate intricate external parameters and can potentially disrupt the optimization process of the condensation graph, making the condensation process highly demanding and unstable. Motivated by the recent success of simplified models across various domains, we propose a simplified approach to metric alignment in graph condensation, aiming to reduce unnecessary complexity inherited from intricate metrics. We introduce the Simple Graph Condensation (SimGC) framework, which aligns the condensed graph with the original graph from the input layer to the prediction layer, guided by a pre-trained Simple Graph Convolution (SGC) model on the original graph. Importantly, SimGC eliminates external parameters and exclusively retains the target condensed graph during the condensation process. This straightforward yet effective strategy achieves a significant speedup of up to 10 times compared to existing graph condensation methods while performing on par with state-of-the-art baselines. Comprehensive experiments conducted on seven benchmark datasets demonstrate the effectiveness of SimGC in prediction accuracy, condensation time, and generalization capability. Our code is available at https://github.com/BangHonor/SimGC."}, "https://arxiv.org/abs/2407.13929": {"title": "Unmasking Social Bots: How Confident Are We?", "link": "https://arxiv.org/abs/2407.13929", "description": "arXiv:2407.13929v1 Announce Type: new \nAbstract: Social bots remain a major vector for spreading disinformation on social media and a menace to the public. Despite the progress made in developing multiple sophisticated social bot detection algorithms and tools, bot detection remains a challenging, unsolved problem that is fraught with uncertainty due to the heterogeneity of bot behaviors, training data, and detection algorithms. Detection models often disagree on whether to label the same account as bot or human-controlled. However, they do not provide any measure of uncertainty to indicate how much we should trust their results. We propose to address both bot detection and the quantification of uncertainty at the account level - a novel feature of this research. This dual focus is crucial as it allows us to leverage additional information related to the quantified uncertainty of each prediction, thereby enhancing decision-making and improving the reliability of bot classifications. Specifically, our approach facilitates targeted interventions for bots when predictions are made with high confidence and suggests caution (e.g., gathering more data) when predictions are uncertain."}, "https://arxiv.org/abs/2407.14014": {"title": "Asymmetric interaction preference induces cooperation in human-agent hybrid game", "link": "https://arxiv.org/abs/2407.14014", "description": "arXiv:2407.14014v1 Announce Type: new \nAbstract: With the development of artificial intelligence, human beings are increasingly interested in human-agent collaboration, which generates a series of problems about the relationship between agents and humans, such as trust and cooperation. This inevitably induces the inherent human characteristic that there are subjective interaction preferences for different groups, especially in human-agent hybrid systems where human-human interaction, agent-agent interaction, and human-agent interaction coexist. However, understanding how individual interaction preferences affect the cooperation of the system remains a major challenge. Therefore, this paper proposes a human-agent hybrid prisoner's dilemma game system under the framework of evolutionary game. In spatial networks, the most significant difference between agents and humans is the flexibility of decision, where humans have higher adaptive capabilities, follow link dynamics, and adopt free decision rules, which enable them to choose different strategies for different neighbors. However, agents follow node dynamics and adopt consistent decision rules, applying the same strategy to different neighbors. We give the subjective preferences of any individual to different groups, involving the interaction preferences between homogeneous groups and heterogeneous groups respectively. The simulation results show that both human and agent have asymmetric interaction preferences for groups with different identities, which can significantly improve the cooperative behavior of the system. In the hybrid system, human groups show more stable prosocial behavior. Agent groups can form highly cooperative clusters under the condition of strong interaction preference for human groups. In addition, giving agents the ability to identify opponents can effectively alleviate the interaction dilemma of agents."}, "https://arxiv.org/abs/2407.14353": {"title": "Human-in-the-loop MGA to generate energy system design options matching stakeholder needs", "link": "https://arxiv.org/abs/2407.14353", "description": "arXiv:2407.14353v1 Announce Type: new \nAbstract: Using cost-optimisation to support energy system design decisions hides from view many more feasible designs that, while economically comparable to the least-cost option, may have other practical advantages that stakeholders prefer. Increasingly, modellers look at Modelling to generate alternatives (MGA) as a way to go beyond few, optimal designs and provide stakeholders with broad portfolios of design options to appraise based on their preferences and knowledge of the problem. In practice, however, generating and exploring all the feasible system design options is not computationally possible. Modellers need to choose what design feature to generate diversity around, and yet, they cannot know what features may matter the most to heterogeneous real-world stakeholders. Stakeholders' knowledge is thus needed to guide the MGA search towards the most important trade-offs, but how to incorporate such knowledge in the MGA workflow is unclear. To address this gap, we propose a first-of-its-kind human-in-the-loop (HITL) approach that seamlessly and automatically integrates stakeholder preferences into an MGA workflow. In a nutshell, we elicit stakeholders' system design preferences via a first interaction between them and a tentative MGA design space. Hence, we decode those preferences so that it becomes possible to feed them back to the MGA algorithm to perform a guided search. This search produces an updated, human-trained design space with a higher share of designs that align with the elicited preferences. In a controlled experiment for the case study of the Portuguese energy system, we demonstrate that our HITL-MGA approach may facilitate the identification of a consensus solution that balances many conflicting stakeholder preferences. The approach shows promise for generalisation and real-world application to accelerate technically and socially feasible energy transition decisions."}, "https://arxiv.org/abs/2407.13880": {"title": "The Software Complexity of Nations", "link": "https://arxiv.org/abs/2407.13880", "description": "arXiv:2407.13880v1 Announce Type: cross \nAbstract: Despite the growing importance of the digital sector, research on economic complexity and its implications continues to rely mostly on administrative records, e.g. data on exports, patents, and employment, that fail to capture the nuances of the digital economy. In this paper we use data on the geography of programming languages used in open-source software projects to extend economic complexity ideas to the digital economy. We estimate a country's software economic complexity and show that it complements the ability of measures of complexity based on trade, patents, and research papers to account for international differences in GDP per capita, income inequality, and emissions. We also show that open-source software follows the principle of relatedness, meaning that a country's software entries and exits are explained by specialization in related programming languages. We conclude by exploring the diversification and development of countries in open-source software in the context of large language models. Together, these findings help extend economic complexity methods and their policy considerations to the digital sector."}, "https://arxiv.org/abs/2407.13909": {"title": "PRAGyan -- Connecting the Dots in Tweets", "link": "https://arxiv.org/abs/2407.13909", "description": "arXiv:2407.13909v1 Announce Type: cross \nAbstract: As social media platforms grow, understanding the underlying reasons behind events and statements becomes crucial for businesses, policymakers, and researchers. This research explores the integration of Knowledge Graphs (KGs) with Large Language Models (LLMs) to perform causal analysis of tweets dataset. The LLM aided analysis techniques often lack depth in uncovering the causes driving observed effects. By leveraging KGs and LLMs, which encode rich semantic relationships and temporal information, this study aims to uncover the complex interplay of factors influencing causal dynamics and compare the results obtained using GPT-3.5 Turbo. We employ a Retrieval-Augmented Generation (RAG) model, utilizing a KG stored in a Neo4j (a.k.a PRAGyan) data format, to retrieve relevant context for causal reasoning. Our approach demonstrates that the KG-enhanced LLM RAG can provide improved results when compared to the baseline LLM (GPT-3.5 Turbo) model as the source corpus increases in size. Our qualitative analysis highlights the advantages of combining KGs with LLMs for improved interpretability and actionable insights, facilitating informed decision-making across various domains. Whereas, quantitative analysis using metrics such as BLEU and cosine similarity show that our approach outperforms the baseline by 10\\%."}, "https://arxiv.org/abs/2407.13924": {"title": "Preparing Fermilab to Carry Out the P5 Plan", "link": "https://arxiv.org/abs/2407.13924", "description": "arXiv:2407.13924v1 Announce Type: cross \nAbstract: In the past several years, Fermi National Accelerator Laboratory (Fermilab) has suffered a number of troublesome setbacks. In some cases, failures to respond effectively have only made things worse, resulting in the current crisis atmosphere at the laboratory. This is reflected in part in the poorly rated performance appraisals (or PEMP) that the DOE provides to the lab every year. Three entities are currently responsible for supporting, managing, operating, and leading the lab: the Department of Energy (DOE), Fermi Research Alliance (FRA), and the Laboratory Director Office. Two recent events provide opportunities for interested parties to reverse the recent decline: the release in December 2023 of the P5 Report and the DOE decision to rebid the Management & Operating (M&amp;O) contract. The P5 Report a decadal update of recommendations for US particle physics, includes an ambitious and positive roadmap for projects and activities at Fermilab. Carrying out the P5 plan successfully, however, will also require addressing many of the other problems currently besetting the lab. The selection of a new M&amp;O contractor is under the responsibility of the Deputy Director for Operations of the DOE Office of Science Dr. Juston Fontaine. A new M&amp;O Contractor would replace FRA and name a new Director effective January 1, 2025. A new management team, one hopes, would be motivated to solve problems and would enjoy a honeymoon period, enabling them to make positive changes more easily. Stimulated by these recent developments, the authors of this document, all present or past employees, convened confidentially in order to collect testimonials on a series of events symptomatic of the lab problems, to provide constructive criticism and offer possible solutions. This document summarizes this work."}, "https://arxiv.org/abs/2407.13942": {"title": "Harmful Suicide Content Detection", "link": "https://arxiv.org/abs/2407.13942", "description": "arXiv:2407.13942v1 Announce Type: cross \nAbstract: Harmful suicide content on the Internet is a significant risk factor inducing suicidal thoughts and behaviors among vulnerable populations. Despite global efforts, existing resources are insufficient, specifically in high-risk regions like the Republic of Korea. Current research mainly focuses on understanding negative effects of such content or suicide risk in individuals, rather than on automatically detecting the harmfulness of content. To fill this gap, we introduce a harmful suicide content detection task for classifying online suicide content into five harmfulness levels. We develop a multi-modal benchmark and a task description document in collaboration with medical professionals, and leverage large language models (LLMs) to explore efficient methods for moderating such content. Our contributions include proposing a novel detection task, a multi-modal Korean benchmark with expert annotations, and suggesting strategies using LLMs to detect illegal and harmful content. Owing to the potential harm involved, we publicize our implementations and benchmark, incorporating an ethical verification process."}, "https://arxiv.org/abs/2407.14081": {"title": "DisenSemi: Semi-supervised Graph Classification via Disentangled Representation Learning", "link": "https://arxiv.org/abs/2407.14081", "description": "arXiv:2407.14081v1 Announce Type: cross \nAbstract: Graph classification is a critical task in numerous multimedia applications, where graphs are employed to represent diverse types of multimedia data, including images, videos, and social networks. Nevertheless, in real-world scenarios, labeled graph data can be limited or scarce. To address this issue, we focus on the problem of semi-supervised graph classification, which involves both supervised and unsupervised models learning from labeled and unlabeled data. In contrast to recent approaches that transfer the entire knowledge from the unsupervised model to the supervised one, we argue that an effective transfer should only retain the relevant semantics that align well with the supervised task. In this paper, we propose a novel framework named DisenSemi, which learns disentangled representation for semi-supervised graph classification. Specifically, a disentangled graph encoder is proposed to generate factor-wise graph representations for both supervised and unsupervised models. Then we train two models via supervised objective and mutual information (MI)-based constraints respectively. To ensure the meaningful transfer of knowledge from the unsupervised encoder to the supervised one, we further define an MI-based disentangled consistency regularization between two models and identify the corresponding rationale that aligns well with the current graph classification task. Experimental results on a range of publicly accessible datasets reveal the effectiveness of our DisenSemi."}, "https://arxiv.org/abs/2407.14179": {"title": "Bottlenecks in Occupational Transitions: A Data-driven Taxonomy", "link": "https://arxiv.org/abs/2407.14179", "description": "arXiv:2407.14179v1 Announce Type: cross \nAbstract: In an era of rapid technological advancements and macroeconomic shifts, worker reallocation is necessary, yet responses to labor market shocks remain sluggish, making it crucial to identify bottlenecks in occupational transitions to understand labor market dynamics and improve mobility. In this study, we analyze French occupational data to uncover patterns of worker mobility and pinpoint specific occupations that act as bottlenecks which impede rapid reallocation. We introduce two metrics, transferability and accessibility, to quantify the diversity of occupational transitions and find that bottlenecks can be explained by a condensation effect of occupations with high accessibility but low transferability. Transferability measures the variety of transitions from an occupation to others, while accessibility assesses the variety of transitions into an occupation. We provide a comprehensive framework for analyzing occupational complexity and mobility patterns, offering insights into potential barriers and pathways for efficient retraining programs. We argue that our approach can inform policymakers and stakeholders aiming to enhance labor market efficiency and support workforce adaptability."}, "https://arxiv.org/abs/2407.14213": {"title": "Higher-order triadic percolation on random hypergraphs", "link": "https://arxiv.org/abs/2407.14213", "description": "arXiv:2407.14213v1 Announce Type: cross \nAbstract: In this work, we propose a comprehensive theoretical framework combining percolation theory with nonlinear dynamics in order to study hypergraphs with a time-varying giant component. We consider in particular hypergraphs with higher-order triadic interactions that can upregulate or downregulate the hyperedges. Triadic interactions are a general type of signed regulatory interaction that occurs when a third node regulates the interaction between two other nodes. For example, in brain networks, the glia can facilitate or inhibit synaptic interactions between neurons. However, the regulatory interactions may not only occur between regulator nodes and pairwise interactions but also between regulator nodes and higher-order interactions (hyperedges), leading to higher-order triadic interactions. For instance, in biochemical reaction networks, the enzymes regulate the reactions involving multiple reactants. Here we propose and investigate higher-order triadic percolation on hypergraphs showing that the giant component can have a non-trivial dynamics. Specifically, we demonstrate that, under suitable conditions, the order parameter of this percolation problem, i.e., the fraction of nodes in the giant component, undergoes a route to chaos in the universality class of the logistic map. In hierarchical higher-order triadic percolation, we extend this paradigm in order to treat hierarchically nested triadic interactions demonstrating the non-trivial effect of their increased combinatorial complexity on the critical phenomena and the dynamical properties of the process. Finally, we consider other generalizations of the model studying the effect of considering interdependencies and node regulation instead of hyperedge regulation."}, "https://arxiv.org/abs/2306.02113": {"title": "What makes Individual I's a Collective We; Coordination mechanisms & costs", "link": "https://arxiv.org/abs/2306.02113", "description": "arXiv:2306.02113v3 Announce Type: replace \nAbstract: The collective effort exceeds the sum of its parts when individuals coordinate and regulate their activities and behaviors. This holds true even in self-organizing systems with open, voluntary participation where coordination occurs implicitly. Here, we analyze the non-functional actions of contributors, administrators, and bots on Wikipedia, categorizing them by their asymmetric authority: one-way oversight and two-way. This categorization helps us reveal comparable patterns. First, we find remarkably consistent scaling factors for each category relative to system size. Two-way coordination scales superlinearly (with an exponent of $1.3$), while oversight coordination grows sublinearly (with an exponent of $0.9$), suggesting an underlying mechanism for coordination across communities. Second, we identify the hierarchical modular structure of interactions as a key factor for the economy of scale in coordination, and we propose a mathematical model to explain these results. Finally, our temporal analysis shows a shift from two-way interactions to one-way oversight as system size increases. This suggests the emergence of a nascent hierarchical structure even in self-organizing systems, echoing Weber's theory of organizational evolution."}, "https://arxiv.org/abs/2306.15922": {"title": "Divide-and-rule policy in the Naming Game", "link": "https://arxiv.org/abs/2306.15922", "description": "arXiv:2306.15922v2 Announce Type: replace \nAbstract: The Naming Game is a classic model for studying the emergence and evolution of language within a population. In this paper, we extend the traditional Naming Game model to encompass multiple committed opinions and investigate the system dynamics on the complete graph with an arbitrarily large population and random networks of finite size. For the fully connected complete graph, the homogeneous mixing condition enables us to use mean-field theory to analyze the opinion evolution of the system. However, when the number of opinions increases, the number of variables describing the system grows exponentially. To mitigate this, we focus on a special scenario where the largest group of committed agents competes with a motley of committed groups, each of which is smaller than the largest one, while initially, most of uncommitted agents hold one unique opinion. This scenario is chosen for its recurrence in diverse societies and its potential for complexity reduction by unifying agents from smaller committed groups into one category. Our investigation reveals that when the size of the largest committed group reaches the critical threshold, most of uncommitted agents change their beliefs to this opinion, triggering a phase transition. Further, we derive the general formula for the multi-opinion evolution using a recursive approach, enabling investigation into any scenario. Finally, we employ agent-based simulations to reveal the opinion evolution and dominance transition in random graphs. Our results provide insights into the conditions under which the dominant opinion emerges in a population and the factors that influence these conditions."}, "https://arxiv.org/abs/2401.15161": {"title": "Individual and team performance in cricket", "link": "https://arxiv.org/abs/2401.15161", "description": "arXiv:2401.15161v2 Announce Type: replace \nAbstract: Advancements in technology have recently allowed us to collect and analyse large-scale fine-grained data about human performance, drastically changing the way we approach sports. Here, we provide the first comprehensive analysis of individual and team performance in One-Day International cricket, one of the most popular sports in the world. We investigate temporal patterns of individual success by quantifying the location of the best performance of a player and find that they can happen at any time in their career, surrounded by a burst of comparable top performances. Our analysis shows that long-term performance can be predicted from early observations and that temporary exclusions of players from teams are often due to declining performances but are also associated with strong comebacks. By computing the duration of streaks of winning performances compared to random expectations, we demonstrate that teams win and lose matches consecutively. We define the contributions of specialists such as openers, all-rounders and wicket-keepers and show that a balanced performance from multiple individuals is required to ensure team success. Finally, we measure how transitioning to captaincy in the team improves the performance of batsmen, but not that of bowlers. Our work emphasizes how individual endeavours and team dynamics interconnect and influence collective outcomes in sports."}, "https://arxiv.org/abs/2402.12623": {"title": "ECHO: Edge Centrality via Neighborhood-based Optimization", "link": "https://arxiv.org/abs/2402.12623", "description": "arXiv:2402.12623v3 Announce Type: replace \nAbstract: Given a network G, edge centrality is a metric used to evaluate the importance of edges in G, which is a key concept in analyzing networks and finds vast applications involving edge ranking. In spite of a wealth of research on devising edge centrality measures, they incur either prohibitively high computation costs or varied deficiencies that lead to sub-optimal ranking quality.\n  To overcome their limitations, this paper proposes ECHO, a new centrality measure for edge ranking that is formulated based on neighborhood-based optimization objectives. We provide in-depth theoretical analyses to unveil the mathematical definitions and intuitive interpretations of the proposed ECHO measure from diverse aspects. Based thereon, we present three linear-complexity algorithms for ECHO estimation with non-trivial theoretical accuracy guarantees for centrality values. Extensive experiments comparing ECHO against six existing edge centrality metrics in graph analytics tasks on real networks showcase that ECHO offers superior practical effectiveness while offering high computation efficiency."}, "https://arxiv.org/abs/2402.13244": {"title": "Are Fact-Checking Tools Helpful? An Evaluation of Google Fact Check", "link": "https://arxiv.org/abs/2402.13244", "description": "arXiv:2402.13244v3 Announce Type: replace \nAbstract: Google Fact Check is a promising search engine that facilitates fact-checking to combat misinformation on social media, especially during significant events such as the COVID-19 pandemic and the U.S. presidential elections. However, its usability has not been thoroughly studied. We evaluated its performance by analyzing the search results regarding 1,000 COVID-19-related false claims and found that only 15.8\\% of them obtained fact-checking results, and the results returned by the search engine are relatively reliable. We also found that the false claims receiving different fact-checking verdicts (i.e., ``False'', ``Partly False'', ``True'', and ``Unratable'') tend to reflect diverse emotional tones, and fact-checking sources tend to check the claims in different lengths and using dictionary words to various extents. Claim variations addressing the same issue yet described differently are likely to obtain disparate fact-checking results. Based on these, we suggested that the quantities of fact-checking results could be optimized and that slightly adjusting input claim wording may be the best practice for users to obtain more useful information. This study aims to contribute to understanding fact-checking tools and promote information integrity."}, "https://arxiv.org/abs/2403.12663": {"title": "Renormalization of networks with weak geometric coupling", "link": "https://arxiv.org/abs/2403.12663", "description": "arXiv:2403.12663v2 Announce Type: replace \nAbstract: The Renormalization Group is crucial for understanding systems across scales, including complex networks. Renormalizing networks via network geometry, a framework in which their topology is based on the location of nodes in a hidden metric space, is one of the foundational approaches. However, the current methods assume that the geometric coupling is strong, neglecting weak coupling in many real networks. This paper extends renormalization to weak geometric coupling, showing that geometric information is essential to preserve self-similarity. Our results underline the importance of geometric effects on network topology even when the coupling to the underlying space is weak."}, "https://arxiv.org/abs/2404.08098": {"title": "The Impact of School and Family Networks on COVID-19 Infections Among Dutch Students: A Study Using Population-Level Registry Data", "link": "https://arxiv.org/abs/2404.08098", "description": "arXiv:2404.08098v2 Announce Type: replace \nAbstract: Understanding the impact of different social interactions is key to improving epidemic models. Here, we use extensive registry data -- including PCR test results and population-level networks -- to investigate the impact of school, family, and other social contacts on SARS-CoV-2 transmission in the Netherlands (June 2020--October 2021). We isolate and compare different contexts of potential SARS-CoV-2 transmission by matching pairs of students based on their attendance at the same or different primary school (in 2020) and secondary school (in 2021) and their geographic proximity. We then calculated the probability of temporally associated infections -- i.e. the probability of both students testing positive within a 14-day period.\n  Our results highlight the relative importance of household and family transmission in the spread of SARS-CoV-2 compared to school settings. The probability of temporally associated infections for siblings and parent-child pairs living in the same household was 22.6--23.2\\%, and 4.7--7.9\\% for family members living in different household. In contrast, the probability of temporally associated infections was 0.52\\% for pairs of students living nearby but not attending the same primary or secondary school, 0.66\\% for pairs attending different secondary schools but having attended the same primary school, and 1.65\\% for pairs attending the same secondary school. Finally, we used multilevel regression analyses to examine how individual, school, and geographic factors contribute to transmission risk. We found that the largest differences in transmission probabilities were due to unobserved individual (60\\%) and school-level (34\\%) factors. Only a small proportion (3\\%) could be attributed to geographic proximity of students or to school size, denomination, or the median income of the school area."}, "https://arxiv.org/abs/2310.04259": {"title": "Safety-Oriented Calibration and Evaluation of the Intelligent Driver Model", "link": "https://arxiv.org/abs/2310.04259", "description": "arXiv:2310.04259v2 Announce Type: replace-cross \nAbstract: Many car-following models like the Intelligent Driver Model (IDM) incorporate important aspects of safety in their definitions, such as collision-free driving and keeping safe distances, implying that drivers are safety conscious when driving. Despite their safety-oriented nature, when calibrating and evaluating these models, the main objective of most studies is to find model parameters that minimize the error in observed measurements like spacing and speed while studies specifically focused on calibrating and evaluating unobserved safe behavior captured by the parameters of the model are scarce. Most studies on calibration and evaluation of the IDM do not check if the observed driving behavior (i.e. spacing) are within the model estimated unobserved safety thresholds (i.e. desired safety spacing) or what parameters are important for safety. This limits their application for safety driven traffic simulations. To fill this gap, this paper first proposes a simple metric to evaluate driver compliance with the safety thresholds of the IDM model. Specifically, we evaluate driver compliance to their desired safety spacing, speed and safe time gap. Next, a method to enforce compliance to the safety threshold during model calibration is proposed. The proposed compliance metric and the calibration approach is tested using Dutch highway trajectory data obtained from a driving simulator experiment and two drones. The results show that compliance to the IDM safety threshold greatly depends on braking capability with a median compliance between 38% and 90% of driving time, indicating that drivers can only partially follow the IDM safety threshold in reality."}, "https://arxiv.org/abs/2404.17804": {"title": "Flock2: A model for orientation-based social flocking", "link": "https://arxiv.org/abs/2404.17804", "description": "arXiv:2404.17804v2 Announce Type: replace-cross \nAbstract: The aerial flocking of birds, or murmurations, has fascinated observers while presenting many challenges to behavioral study and simulation. We examine how the periphery of murmurations remain well bounded and cohesive. We also investigate agitation waves, which occur when a flock is disturbed, developing a plausible model for how they might emerge spontaneously. To understand these behaviors a new model is presented for orientation-based social flocking. Previous methods model inter-bird dynamics by considering the neighborhood around each bird, and introducing forces for avoidance, alignment, and cohesion as three dimensional vectors that alter acceleration. Our method introduces orientation-based social flocking that treats social influences from neighbors more realistically as a desire to turn, indirectly controlling the heading in an aerodynamic model. While our model can be applied to any flocking social bird we simulate flocks of starlings, Sturnus vulgaris, and demonstrate the possibility of orientation waves in the absence of predators. Our model exhibits spherical and ovoidal flock shapes matching observation. Comparisons of our model to Reynolds' on energy consumption and frequency analysis demonstrates more realistic motions, significantly less energy use in turning, and a plausible mechanism for emergent orientation waves."}, "https://arxiv.org/abs/2407.14510": {"title": "A Review on Response Strategies in Infrastructure Network Restoration", "link": "https://arxiv.org/abs/2407.14510", "description": "arXiv:2407.14510v1 Announce Type: new \nAbstract: This paper reviews the literature on response strategies for restoring infrastructure networks in the aftermath of a disaster. Our motivation for this review is twofold. First, the frequency and magnitude of natural and man-made disasters (e.g., wild fires, tornadoes, global pandemics, terrorist attacks) have been increasing. These events disrupt the operation of infrastructure networks, preventing the delivery of vital goods and services such as power and food. Therefore, it is critical to understand the state-of-the-art in responding to network disruptions in order to develop efficient strategies to mitigate their impacts. Second, it is critical to enable timely decisions in a rapidly changing and unpredictable environment while accounting for numerous interrelated factors. Because the vast majority of response decision problems are computationally challenging, quickly finding solutions that are compatible with real-time decision making is a difficult task. Hence, it is important to understand the nature of response activities and decisions, as well as the available solution methodologies and inherent trade-offs between computation time and solution quality. We review quantitative response methodologies developed for infrastructure network restoration, classifying relevant studies based on the properties of the underlying network. In particular, we focus on resource allocation, scheduling, routing and repair efforts within the domain of power, road, and water, oil and gas network restoration. We also discuss open research questions and future research directions."}, "https://arxiv.org/abs/2407.14537": {"title": "Small but not least changes: The Art of Creating Disruptive Innovations", "link": "https://arxiv.org/abs/2407.14537", "description": "arXiv:2407.14537v1 Announce Type: new \nAbstract: In the ever-evolving landscape of technology, product innovation thrives on replacing outdated technologies with groundbreaking ones or through the ingenious recombination of existing technologies. Our study embarks on a revolutionary journey by genetically representing products, extracting their chromosomal data, and constructing a comprehensive phylogenetic network of automobiles. We delve deep into the technological features that shape innovation, pinpointing the ancestral roots of products and mapping out intricate product-family triangles. By leveraging the similarities within these triangles, we introduce a pioneering \"Product Disruption Index\"-inspired by the CD index (Funk and Owen-Smith, 2017)-to quantify a product's disruptiveness. Our approach is rigorously validated against the scientifically recognized trend of decreasing disruptiveness over time (Park et al., 2023) and through compelling case studies. Our statistical analysis reveals a fascinating insight: disruptive product innovations often stem from minor, yet crucial, modifications."}, "https://arxiv.org/abs/2407.14541": {"title": "Mitigating biases in big mobility data: a case study of monitoring large-scale transit systems", "link": "https://arxiv.org/abs/2407.14541", "description": "arXiv:2407.14541v1 Announce Type: new \nAbstract: Big mobility datasets (BMD) have shown many advantages in studying human mobility and evaluating the performance of transportation systems. However, the quality of BMD remains poorly understood. This study evaluates biases in BMD and develops mitigation methods. Using Google and Apple mobility data as examples, this study compares them with benchmark data from governmental agencies. Spatio-temporal discrepancies between BMD and benchmark are observed and their impacts on transportation applications are investigated, emphasizing the urgent need to address these biases to prevent misguided policymaking. This study further proposes and tests a bias mitigation method. It is shown that the mitigated BMD could generate valuable insights into large-scale public transit systems across 100+ US counties, revealing regional disparities of the recovery of transit systems from the COVID-19. This study underscores the importance of caution when using BMD in transportation research and presents effective mitigation strategies that would benefit practitioners."}, "https://arxiv.org/abs/2407.14546": {"title": "The Ethical Aspects of Choosing a Nuclear Fuel Cycle", "link": "https://arxiv.org/abs/2407.14546", "description": "arXiv:2407.14546v1 Announce Type: new \nAbstract: In this paper, we addressed the problem of choosing a nuclear fuel cycle. Ethical problems related to the choice of a nuclear fuel cycle, such as the depletion of natural uranium reserves, the accumulation of nuclear waste, and the connection with the problems of nonidentity and distributive justice are considered. We examined cultural differences in attitudes toward nuclear safety and the associated ambiguities in the choice of a nuclear fuel cycle. We suggested that the reduction in consumption of natural uranium does not seem to be a feasible way of reducing nuclear waste because of the nonidentity problem."}, "https://arxiv.org/abs/2407.14629": {"title": "Astronomy's relationship with the lands and communities of Maunakea", "link": "https://arxiv.org/abs/2407.14629", "description": "arXiv:2407.14629v1 Announce Type: new \nAbstract: Astronomy is at a turning point in its history and in its relations with the Indigenous peoples who are the generational stewards of land where several of our main observatories are located. The controversy regarding the further development of astronomy facilities on Maunakea is probably the most significant and publicized conflict about the use of such land in the name of science. Thousands have stood in resistance, elders were arrested, and the community is divided. Astronomy's access to one of its most emblematic sites is at risk. This situation challenges our professional practice, the projects we build on Indigenous lands, and our relationships with the people who live within these lands and with society in general. This paper attempts to share the perspective of the authors on the historical events, including the very recent past, through the lens of our understanding and opinions; to provide transparency, with humility, into our process of introspection and transformation; and to share our hopes and ambitions as leaders from Maunakea Observatories for the future of astronomy in Hawai'i, as advocated by the Astro2020 report from the U.S. National Academies of Sciences, Engineering, and Medicine; and to suggest ways for the profession to commit to this long-term vision."}, "https://arxiv.org/abs/2407.14860": {"title": "Modeling the effects of natural disasters, wars, and migrations on sustainability or collapse of pre-industrial societies: Random perturbations of the Human and Nature Dynamics (HANDY) model", "link": "https://arxiv.org/abs/2407.14860", "description": "arXiv:2407.14860v1 Announce Type: new \nAbstract: We study the effect of random perturbations in the Human and Nature Dynamics (HANDY) model. HANDY models the interactions between human population, depletion, and consumption of natural resources. HANDY explains how endogenous human--nature interactions could lead to sustainability or collapse in past societies. We introduce a Gaussian random noise perturbation on the population change to represent generic external perturbations. The robustness of the results is investigated with statistical analysis based on probability distributions of specific events. Our study shows that the results of the unperturbed HANDY model are robust under small perturbations of $\\lesssim$ 10\\% of the Human population. Our results confirm that endogenous dynamics drive the societal cycles. However, exogenous perturbations, such as floods, droughts, earthquakes, volcanic eruptions, infectious disease, epidemics, and wars, can accelerate or delay a collapse cycle."}, "https://arxiv.org/abs/2407.14977": {"title": "Multifractal analysis of racially-constrained population patterns and residential segregation in the US cities", "link": "https://arxiv.org/abs/2407.14977", "description": "arXiv:2407.14977v1 Announce Type: new \nAbstract: A phenomenon of racial segregation in U.S. cities is a multifaceted area of study. A recent advancement in this field is the development of a methodology that transforms census population count-by-race data into a grid of monoracial cells. This format enables assessment of heterogeneity of segregation within a city. This paper leverages such a grid for the quantification of race-constrained population patterns, allowing for the calculation and mapping of binary segregation patterns within arbitrary region. A key innovation is the application of Multifractal Analysis (MFA) to quantify the residency patterns of race-constrained populations. The residency pattern is characterized by a multifractal spectrum function, where the independent variable is a local metric of pattern's \"gappiness\", and the dependent variable is proportional to the size of the sub-pattern consisting of all locations having the same value of this metric. In the context of binary populations, the gappiness of the race-constrained population's pattern is intrinsically linked to its segregation. This paper provides a comprehensive description of the methodology, illustrated with examples focusing on the residency pattern of Black population in the central region of Washington, DC. Further, the methodology is demonstrated using a sample of residency patterns of Black population in fourteen large U.S. cities. By numerically describing each pattern through a multifractal spectrum, the fourteen patterns are clustered into three distinct categories, each having unique characteristics. Maps of local gappiness and segregation for each city are provided to show the connection between the nature of the multifractal spectrum and the corresponding residency and segregation patterns. This method offers an excellent quantification of race-restricted residency and residential segregation patterns within U.S. cities."}, "https://arxiv.org/abs/2407.15025": {"title": "Digital Twin-based Driver Risk-Aware Intelligent Mobility Analytics for Urban Transportation Management", "link": "https://arxiv.org/abs/2407.15025", "description": "arXiv:2407.15025v1 Announce Type: new \nAbstract: Traditional mobility management strategies emphasize macro-level mobility oversight from traffic-sensing infrastructures, often overlooking safety risks that directly affect road users. To address this, we propose a Digital Twin-based Driver Risk-Aware Intelligent Mobility Analytics (DT-DIMA) system. The DT-DIMA system integrates real-time traffic information from pan-tilt-cameras (PTCs), synchronizes this data into a digital twin to accurately replicate the physical world, and predicts network-wide mobility and safety risks in real time. The system's innovation lies in its integration of spatial-temporal modeling, simulation, and online control modules. Tested and evaluated under normal traffic conditions and incidental situations (e.g., unexpected accidents, pre-planned work zones) in a simulated testbed in Brooklyn, New York, DT-DIMA demonstrated mean absolute percentage errors (MAPEs) ranging from 8.40% to 15.11% in estimating network-level traffic volume and MAPEs from 0.85% to 12.97% in network-level safety risk prediction. In addition, the highly accurate safety risk prediction enables PTCs to preemptively monitor road segments with high driving risks before incidents take place. Such proactive PTC surveillance creates around a 5-minute lead time in capturing traffic incidents. The DT-DIMA system enables transportation managers to understand mobility not only in terms of traffic patterns but also driver-experienced safety risks, allowing for proactive resource allocation in response to various traffic situations. To the authors' best knowledge, DT-DIMA is the first urban mobility management system that considers both mobility and safety risks based on digital twin architecture."}, "https://arxiv.org/abs/2407.15127": {"title": "A Model of Proactive Safety Based on Knowledge Graph", "link": "https://arxiv.org/abs/2407.15127", "description": "arXiv:2407.15127v1 Announce Type: new \nAbstract: In contemporary safety management, despite the abundance of safety data gathered from routine operation tasks and safety management activities, actions cannot prevent all accidents effectively due to a lack of effective utilization of these data as safety knowledge. To bridge this gap, this paper proposes a hybrid proactive safety model integrating data-driven and knowledge-driven approaches. The model comprises three main steps: proactive safety actions to generate safety data, data-driven approaches to mine safety data, and knowledge-driven approaches to depicting risk knowledge graphs. Application of this model to a continuous stirred tank reactor (CSTR) scenario demonstrates its efficacy in identifying and addressing safety issues proactively. The results demonstrate the effectiveness and practicality of the proposed proactive safety model, suggesting its endorsement within both academic and industrial applications."}, "https://arxiv.org/abs/2407.15165": {"title": "Reinforcement Learning Optimizes Power Dispatch in Decentralized Power Grid", "link": "https://arxiv.org/abs/2407.15165", "description": "arXiv:2407.15165v1 Announce Type: new \nAbstract: Effective frequency control in power grids has become increasingly important with the increasing demand for renewable energy sources. Here, we propose a novel strategy for resolving this challenge using graph convolutional proximal policy optimization (GC-PPO). The GC-PPO method can optimally determine how much power individual buses dispatch to reduce frequency fluctuations across a power grid. We demonstrate its efficacy in controlling disturbances by applying the GC-PPO to the power grid of the UK. The performance of GC-PPO is outstanding compared to the classical methods. This result highlights the promising role of GC-PPO in enhancing the stability and reliability of power systems by switching lines or decentralizing grid topology."}, "https://arxiv.org/abs/2407.15431": {"title": "Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs", "link": "https://arxiv.org/abs/2407.15431", "description": "arXiv:2407.15431v1 Announce Type: new \nAbstract: The text-attributed graph (TAG) is one kind of important real-world graph-structured data with each node associated with raw texts. For TAGs, traditional few-shot node classification methods directly conduct training on the pre-processed node features and do not consider the raw texts. The performance is highly dependent on the choice of the feature pre-processing method. In this paper, we propose P2TAG, a framework designed for few-shot node classification on TAGs with graph pre-training and prompting. P2TAG first pre-trains the language model (LM) and graph neural network (GNN) on TAGs with self-supervised loss. To fully utilize the ability of language models, we adapt the masked language modeling objective for our framework. The pre-trained model is then used for the few-shot node classification with a mixed prompt method, which simultaneously considers both text and graph information. We conduct experiments on six real-world TAGs, including paper citation networks and product co-purchasing networks. Experimental results demonstrate that our proposed framework outperforms existing graph few-shot learning methods on these datasets with +18.98% ~ +35.98% improvements."}, "https://arxiv.org/abs/2407.15643": {"title": "Link Polarity Prediction from Sparse and Noisy Labels via Multiscale Social Balance", "link": "https://arxiv.org/abs/2407.15643", "description": "arXiv:2407.15643v1 Announce Type: new \nAbstract: Signed Graph Neural Networks (SGNNs) have recently gained attention as an effective tool for several learning tasks on signed networks, i.e., graphs where edges have an associated polarity. One of these tasks is to predict the polarity of the links for which this information is missing, starting from the network structure and the other available polarities. However, when the available polarities are few and potentially noisy, such a task becomes challenging.\n  In this work, we devise a semi-supervised learning framework that builds around the novel concept of \\emph{multiscale social balance} to improve the prediction of link polarities in settings characterized by limited data quantity and quality. Our model-agnostic approach can seamlessly integrate with any SGNN architecture, dynamically reweighting the importance of each data sample while making strategic use of the structural information from unlabeled edges combined with social balance theory.\n  Empirical validation demonstrates that our approach outperforms established baseline models, effectively addressing the limitations imposed by noisy and sparse data. This result underlines the benefits of incorporating multiscale social balance into SGNNs, opening new avenues for robust and accurate predictions in signed network analysis."}, "https://arxiv.org/abs/2407.15823": {"title": "A Large-scale Benchmark Dataset for Commuting Origin-destination Matrix Generation", "link": "https://arxiv.org/abs/2407.15823", "description": "arXiv:2407.15823v1 Announce Type: new \nAbstract: The commuting origin-destination~(OD) matrix is a critical input for urban planning and transportation, providing crucial information about the population residing in one region and working in another within an interested area. Despite its importance, obtaining and updating the matrix is challenging due to high costs and privacy concerns. This has spurred research into generating commuting OD matrices for areas lacking historical data, utilizing readily available information via computational models. In this regard, existing research is primarily restricted to only a single or few large cities, preventing these models from being applied effectively in other areas with distinct characteristics, particularly in towns and rural areas where such data is urgently needed. To address this, we propose a large-scale dataset comprising commuting OD matrices for 3,233 diverse areas around the U.S. For each area, we provide the commuting OD matrix, combined with regional attributes including demographics and point-of-interests of each region in that area. We believe this comprehensive dataset will facilitate the development of more generalizable commuting OD matrix generation models, which can capture various patterns of distinct areas. Additionally, we use this dataset to benchmark a set of commuting OD generation models, including physical models, element-wise predictive models, and matrix-wise generative models. Surprisingly, we find a new paradigm, which considers the whole area combined with its commuting OD matrix as an attributed directed weighted graph and generates the weighted edges based on the node attributes, can achieve the optimal. This may inspire a new research direction from graph learning in this field."}, "https://arxiv.org/abs/2407.14732": {"title": "Meta-GPS++: Enhancing Graph Meta-Learning with Contrastive Learning and Self-Training", "link": "https://arxiv.org/abs/2407.14732", "description": "arXiv:2407.14732v1 Announce Type: cross \nAbstract: Node classification is an essential problem in graph learning. However, many models typically obtain unsatisfactory performance when applied to few-shot scenarios. Some studies have attempted to combine meta-learning with graph neural networks to solve few-shot node classification on graphs. Despite their promising performance, some limitations remain. First, they employ the node encoding mechanism of homophilic graphs to learn node embeddings, even in heterophilic graphs. Second, existing models based on meta-learning ignore the interference of randomness in the learning process. Third, they are trained using only limited labeled nodes within the specific task, without explicitly utilizing numerous unlabeled nodes. Finally, they treat almost all sampled tasks equally without customizing them for their uniqueness. To address these issues, we propose a novel framework for few-shot node classification called Meta-GPS++. Specifically, we first adopt an efficient method to learn discriminative node representations on homophilic and heterophilic graphs. Then, we leverage a prototype-based approach to initialize parameters and contrastive learning for regularizing the distribution of node embeddings. Moreover, we apply self-training to extract valuable information from unlabeled nodes. Additionally, we adopt S$^2$ (scaling & shifting) transformation to learn transferable knowledge from diverse tasks. The results on real-world datasets show the superiority of Meta-GPS++. Our code is available here."}, "https://arxiv.org/abs/2407.14844": {"title": "Political Leanings in Web3 Betting: Decoding the Interplay of Political and Profitable Motives", "link": "https://arxiv.org/abs/2407.14844", "description": "arXiv:2407.14844v1 Announce Type: cross \nAbstract: Harnessing the transparent blockchain user behavior data, we construct the Political Betting Leaning Score (PBLS) to measure political leanings based on betting within Web3 prediction markets. Focusing on Polymarket and starting from the 2024 U.S. Presidential Election, we synthesize behaviors over 15,000 addresses across 4,500 events and 8,500 markets, capturing the intensity and direction of their political leanings by the PBLS. We validate the PBLS through internal consistency checks and external comparisons. We uncover relationships between our PBLS and betting behaviors through over 800 features capturing various behavioral aspects. A case study of the 2022 U.S. Senate election further demonstrates the ability of our measurement while decoding the dynamic interaction between political and profitable motives. Our findings contribute to understanding decision-making in decentralized markets, enhancing the analysis of behaviors within Web3 prediction environments. The insights of this study reveal the potential of blockchain in enabling innovative, multidisciplinary studies and could inform the development of more effective online prediction markets, improve the accuracy of forecast, and help the design and optimization of platform mechanisms. The data and code for the paper are accessible at the following link: https://github.com/anonymous."}, "https://arxiv.org/abs/2407.15227": {"title": "A Community-Centric Perspective for Characterizing and Detecting Anti-Asian Violence-Provoking Speech", "link": "https://arxiv.org/abs/2407.15227", "description": "arXiv:2407.15227v1 Announce Type: cross \nAbstract: Violence-provoking speech -- speech that implicitly or explicitly promotes violence against the members of the targeted community, contributed to a massive surge in anti-Asian crimes during the pandemic. While previous works have characterized and built tools for detecting other forms of harmful speech, like fear speech and hate speech, our work takes a community-centric approach to studying anti-Asian violence-provoking speech. Using data from ~420k Twitter posts spanning a 3-year duration (January 1, 2020 to February 1, 2023), we develop a codebook to characterize anti-Asian violence-provoking speech and collect a community-crowdsourced dataset to facilitate its large-scale detection using state-of-the-art classifiers. We contrast the capabilities of natural language processing classifiers, ranging from BERT-based to LLM-based classifiers, in detecting violence-provoking speech with their capabilities to detect anti-Asian hateful speech. In contrast to prior work that has demonstrated the effectiveness of such classifiers in detecting hateful speech ($F_1 = 0.89$), our work shows that accurate and reliable detection of violence-provoking speech is a challenging task ($F_1 = 0.69$). We discuss the implications of our findings, particularly the need for proactive interventions to support Asian communities during public health crises. The resources related to the study are available at https://claws-lab.github.io/violence-provoking-speech/."}, "https://arxiv.org/abs/2407.15249": {"title": "Hurricane Evacuation Analysis with Large-scale Mobile Device Location Data during Hurricane Ian", "link": "https://arxiv.org/abs/2407.15249", "description": "arXiv:2407.15249v1 Announce Type: cross \nAbstract: Hurricane Ian is the deadliest and costliest hurricane in Florida's history, with 2.5 million people ordered to evacuate. As we witness increasingly severe hurricanes in the context of climate change, mobile device location data offers an unprecedented opportunity to study hurricane evacuation behaviors. With a terabyte-level GPS dataset, we introduce a holistic hurricane evacuation behavior algorithm with a case study of Ian: we infer evacuees' departure time and categorize them into different behavioral groups, including self, voluntary, mandatory, shadow and in-zone evacuees. Results show the landfall area (Fort Myers, Lee County) had lower out-of-zone but higher overall evacuation rate, while the predicted landfall area (Tampa, Hillsborough County) had the opposite, suggesting the effects of delayed evacuation order. Out-of-zone evacuation rates would increase from shore to inland. Spatiotemporal analysis identified three evacuation waves: during formation, before landfall, and after landfall. These insights are valuable for enhancing future disaster planning and management."}, "https://arxiv.org/abs/2407.15452": {"title": "GraphScale: A Framework to Enable Machine Learning over Billion-node Graphs", "link": "https://arxiv.org/abs/2407.15452", "description": "arXiv:2407.15452v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs) have emerged as powerful tools for supervised machine learning over graph-structured data, while sampling-based node representation learning is widely utilized in unsupervised learning. However, scalability remains a major challenge in both supervised and unsupervised learning for large graphs (e.g., those with over 1 billion nodes). The scalability bottleneck largely stems from the mini-batch sampling phase in GNNs and the random walk sampling phase in unsupervised methods. These processes often require storing features or embeddings in memory. In the context of distributed training, they require frequent, inefficient random access to data stored across different workers. Such repeated inter-worker communication for each mini-batch leads to high communication overhead and computational inefficiency.\n  We propose GraphScale, a unified framework for both supervised and unsupervised learning to store and process large graph data distributedly. The key insight in our design is the separation of workers who store data and those who perform the training. This separation allows us to decouple computing and storage in graph training, thus effectively building a pipeline where data fetching and data computation can overlap asynchronously. Our experiments show that GraphScale outperforms state-of-the-art methods for distributed training of both GNNs and node embeddings. We evaluate GraphScale both on public and proprietary graph datasets and observe a reduction of at least 40% in end-to-end training times compared to popular distributed frameworks, without any loss in performance. While most existing methods don't support billion-node graphs for training node embeddings, GraphScale is currently deployed in production at TikTok enabling efficient learning over such large graphs."}, "https://arxiv.org/abs/2407.15532": {"title": "Large-scale Time-Varying Portfolio Optimisation using Graph Attention Networks", "link": "https://arxiv.org/abs/2407.15532", "description": "arXiv:2407.15532v1 Announce Type: cross \nAbstract: Apart from assessing individual asset performance, investors in financial markets also need to consider how a set of firms performs collectively as a portfolio. Whereas traditional Markowitz-based mean-variance portfolios are widespread, network-based optimisation techniques have built upon these developments. However, most studies do not contain firms at risk of default and remove any firms that drop off indices over a certain time. This is the first study to incorporate risky firms and use all the firms in portfolio optimisation. We propose and empirically test a novel method that leverages Graph Attention networks (GATs), a subclass of Graph Neural Networks (GNNs). GNNs, as deep learning-based models, can exploit network data to uncover nonlinear relationships. Their ability to handle high-dimensional features and accommodate customised layers for specific purposes makes them particularly appealing for large-scale problems such as mid- and small-cap portfolio optimization. This study utilises 30 years of data on mid-cap firms, creating graphs of firms using distance correlation and the Triangulated Maximally Filtered Graph approach. These graphs are the inputs to a GAT model that we train using custom layers which impose weight and allocation constraints and a loss function derived from the Sharpe ratio, thus directly maximising portfolio risk-adjusted returns. This new model is benchmarked against a network characteristic-based portfolio, a mean variance-based portfolio, and an equal-weighted portfolio. The results show that the portfolio produced by the GAT-based model outperforms all benchmarks and is consistently superior to other strategies over a long period while also being informative of market dynamics."}, "https://arxiv.org/abs/2301.11290": {"title": "Graph Encoder Ensemble for Simultaneous Vertex Embedding and Community Detection", "link": "https://arxiv.org/abs/2301.11290", "description": "arXiv:2301.11290v3 Announce Type: replace \nAbstract: In this paper, we introduce a novel and computationally efficient method for vertex embedding, community detection, and community size determination. Our approach leverages a normalized one-hot graph encoder and a rank-based cluster size measure. Through extensive simulations, we demonstrate the excellent numerical performance of our proposed graph encoder ensemble algorithm."}, "https://arxiv.org/abs/2302.12295": {"title": "Information cascade on networks and phase transitions", "link": "https://arxiv.org/abs/2302.12295", "description": "arXiv:2302.12295v2 Announce Type: replace \nAbstract: Herein, we consider a voting model for information cascades on several types of networks -- a random graph, the Barab\\'{a}si-Albert(BA) model, and lattice networks -- by using one parameter $\\omega$; $\\omega=1,0, -1$ respectively correspond to these networks. $\\omega$ is related to the size of hubs. We discuss the differences between the phases in which the networks depend. In $\\omega\\ne -1$, without, the following two types of phase transitions can be observed: information cascade transition and super-normal transition. The first is the transition between a state where most voters make correct choices and a state where most of them are wrong. This is an absorption transition that belongs to the non-equilibrium transition. In the symmetric case, the phase transition is continuous and the universality class is the same as nonlinear P\\'{o}lya model. In contrast, in the asymmetric case, there is a discontinuous phase transition, where the gap depends on the network. The super-normal transition is the transition of the convergence speed, and the critical point of the convergence speed transition depends on $\\omega$. At $\\omega=1$, in the BA model, this transition disappears. Both phase transitions disappear at $\\omega=-1$ in the lattice case. In conclusion, as the performance near the lattice case, $\\omega\\sim-1$ exhibits the best performance of the voting in all networks. As the hub size decreases, the performance improves."}, "https://arxiv.org/abs/2306.00037": {"title": "BotArtist: Generic approach for bot detection in Twitter via semi-automatic machine learning pipeline", "link": "https://arxiv.org/abs/2306.00037", "description": "arXiv:2306.00037v4 Announce Type: replace \nAbstract: Twitter, as one of the most popular social networks, provides a platform for communication and online discourse. Unfortunately, it has also become a target for bots and fake accounts, resulting in the spread of false information and manipulation. This paper introduces a semi-automatic machine learning pipeline (SAMLP) designed to address the challenges correlated with machine learning model development. Through this pipeline, we develop a comprehensive bot detection model named BotArtist, based on user profile features. SAMLP leverages nine distinct publicly available datasets to train the BotArtist model. To assess BotArtist's performance against current state-of-the-art solutions, we select 35 existing Twitter bot detection methods, each utilizing a diverse range of features. Our comparative evaluation of BotArtist and these existing methods, conducted across nine public datasets under standardized conditions, reveals that the proposed model outperforms existing solutions by almost 10%, in terms of F1-score, achieving an average score of 83.19 and 68.5 over specific and general approaches respectively. As a result of this research, we provide a dataset of the extracted features combined with BotArtist predictions over the 10.929.533 Twitter user profiles, collected via Twitter API during the 2022 Russo-Ukrainian War, over a 16-month period. This dataset was created in collaboration with [Shevtsov et al., 2022a] where the original authors share anonymized tweets on the discussion of the Russo-Ukrainian war with a total amount of 127.275.386 tweets. The combination of the existing text dataset and the provided labeled bot and human profiles will allow for the future development of a more advanced bot detection large language model in the post-Twitter API era."}, "https://arxiv.org/abs/2309.15968": {"title": "Dynamics of Ideological Biases of Social Media Users", "link": "https://arxiv.org/abs/2309.15968", "description": "arXiv:2309.15968v2 Announce Type: replace \nAbstract: Humanity for centuries has perfected skills of interpersonal interactions and evolved patterns that enable people to detect lies and deceiving behavior of others in face-to-face settings. Unprecedented growth of people's access to mobile phones and social media raises an important question: How does this new technology influence people's interactions and support the use of traditional patterns? In this article, we answer this question for homophily-driven patterns in social media. In our previous studies, we found that, on a university campus, changes in student opinions were driven by the desire to hold popular opinions. Here, we demonstrate that the evolution of online platform-wide opinion groups is driven by the same desire. We focus on two social media: Twitter and Parler, on which we tracked the political biases of their users. On Parler, an initially stable group of Right-biased users evolved into a permanent Right-leaning echo chamber dominating weaker, transient groups of members with opposing political biases. In contrast, on Twitter, the initial presence of two large opposing bias groups led to the evolution of a bimodal bias distribution, with a high degree of polarization. We capture the movement of users from the initial to final bias groups during the tracking period. We also show that user choices are influenced by side-effects of homophily. Users entering the platform attempt to find a sufficiently large group whose members hold political biases within the range sufficiently close to their own. If successful, they stabilize their biases and become permanent members of the group. Otherwise, they leave the platform. We believe that the dynamics of users' behavior uncovered in this article create a foundation for technical solutions supporting social groups on social media and socially aware networks."}, "https://arxiv.org/abs/2311.08919": {"title": "FCS-HGNN: Flexible Multi-type Community Search in Heterogeneous Information Networks", "link": "https://arxiv.org/abs/2311.08919", "description": "arXiv:2311.08919v3 Announce Type: replace \nAbstract: Community search is a personalized community discovery problem designed to identify densely connected subgraphs containing the query node. Recently, community search in heterogeneous information networks (HINs) has received considerable attention. Existing methods typically focus on modeling relationships in HINs through predefined meta-paths or user-specified relational constraints. However, metapath-based methods are primarily designed to identify single-type communities with nodes of the same type rather than multi-type communities involving nodes of different types. Constraint-based methods require users to have a good understanding of community patterns to define a suitable set of relational constraints, which increases the burden on users. In this paper, we propose FCS-HGNN, a novel method for flexibly identifying both single-type and multi-type communities in HINs. Specifically, FCS-HGNN extracts complementary information from different views and dynamically considers the contribution of each relation instead of treating them equally, thereby capturing more fine-grained heterogeneous information. Furthermore, to improve efficiency on large-scale graphs, we further propose LS-FCS-HGNN, which incorporates i) the neighbor sampling strategy to improve training efficiency, and ii) the depth-based heuristic search strategy to improve query efficiency. We conducted extensive experiments to demonstrate the superiority of our proposed methods over state-of-the-art methods, achieving average improvements of 14.3% and 11.1% on single-type and multi-type communities, respectively."}, "https://arxiv.org/abs/2311.18705": {"title": "Quantifying metadata-block structure relationships in networks using description length", "link": "https://arxiv.org/abs/2311.18705", "description": "arXiv:2311.18705v3 Announce Type: replace \nAbstract: Network analysis is often enriched by including an examination of node metadata. In the context of understanding the mesoscale of networks it is often assumed that node groups based on metadata and node groups based on connectivity patterns are intrinsically linked. Recently, this assumption has been challenged and it has been demonstrated that metadata might be entirely unrelated to structure or, similarly, multiple sets of metadata might be relevant to the structure of a network in different ways. We propose the metablox tool to quantify the relationship between a network's node metadata and its mesoscale structure, measuring the strength of the relationship and the type of structural arrangement exhibited by the metadata. Our tool incorporates a way to distinguish significantly relevant relationships and can be used as part of systematic meta analyses comparing large numbers of networks, which we demonstrate on a number of synthetic and empirical networks."}, "https://arxiv.org/abs/2404.04175": {"title": "Interplay of network structure and talent configuration on wealth dynamics", "link": "https://arxiv.org/abs/2404.04175", "description": "arXiv:2404.04175v2 Announce Type: replace \nAbstract: The economic success of individuals is often determined by a combination of talent, luck, and assistance from others. We introduce a new agent-based model that simultaneously considers talent, luck, and social interaction. This model allows us to explore how network structure (how agents interact) and talent distribution among agents affect the dynamics of capital accumulation through analytical and numerical methods. We identify a phenomenon as ``talent configuration effect\", which refers to the influence of how talent is allocated to individuals (nodes) in the network. We analyze this effect through two key properties: talent assortativity (TA) and talent-degree correlation (TD). In particular, we focus on three economic indicators: growth rate ($n_{\\rm rate}$), Gini coefficient (inequality: $n_{\\rm Gini}$), and meritocratic fairness ($n_{LT}$). This investigation helps us understand the interplay between talent configuration and network structure on capital dynamics. We find that, in the short term, positive correlations exist between TA and TD for all three economic indicators. Furthermore, the dominant factor influencing capital dynamics depends on the network topology. In scale-free networks, TD has a stronger influence on the economic indices than TA. Conversely, in lattice-like networks, TA plays a more significant role. Our findings address that high socioeconomic homophily can create a dilemma between growth and equality, and that hub monopolization by few highly talented agents makes economic growth strongly dependent on their performances."}, "https://arxiv.org/abs/2008.03073": {"title": "Degree distributions in networks: beyond the power law", "link": "https://arxiv.org/abs/2008.03073", "description": "arXiv:2008.03073v5 Announce Type: replace-cross \nAbstract: The power law is useful in describing count phenomena such as network degrees and word frequencies. With a single parameter, it captures the main feature that the frequencies are linear on the log-log scale. Nevertheless, there have been criticisms of the power law, for example that a threshold needs to be pre-selected without its uncertainty quantified, that the power law is simply inadequate, and that subsequent hypothesis tests are required to determine whether the data could have come from the power law. We propose a modelling framework that combines two different generalisations of the power law, namely the generalised Pareto distribution and the Zipf-polylog distribution, to resolve these issues. The proposed mixture distributions are shown to fit the data well and quantify the threshold uncertainty in a natural way. A model selection step embedded in the Bayesian inference algorithm further answers the question whether the power law is adequate."}, "https://arxiv.org/abs/2203.10560": {"title": "Who Shares Fake News? Uncovering Insights from Social Media Users' Post Histories", "link": "https://arxiv.org/abs/2203.10560", "description": "arXiv:2203.10560v3 Announce Type: replace-cross \nAbstract: We propose that social-media users' own post histories are an underused yet valuable resource for studying fake-news sharing. By extracting textual cues from their prior posts, and contrasting their prevalence against random social-media users and others (e.g., those with similar socio-demographics, political news-sharers, and fact-check sharers), researchers can identify cues that distinguish fake-news sharers, predict those most likely to share fake news, and identify promising constructs to build interventions. Our research includes studies along these lines. In Study 1, we explore the distinctive language patterns of fake-news sharers, highlighting elements such as their higher use of anger and power-related words. In Study 2, we show that adding textual cues into predictive models enhances their accuracy in predicting fake-news sharers. In Study 3, we explore the contrasting role of trait and situational anger, and show trait anger is associated with a greater propensity to share both true and fake news. In Study 4, we introduce a way to authenticate Twitter accounts in surveys, before using it to explore how crafting an ad copy that resonates with users' sense of power encourages the adoption of fact-checking tools. We hope to encourage the use of novel research methods for marketers and misinformation researchers."}, "https://arxiv.org/abs/2310.17251": {"title": "Price of information in games of chance: a statistical physics approach", "link": "https://arxiv.org/abs/2310.17251", "description": "arXiv:2310.17251v2 Announce Type: replace-cross \nAbstract: Information in the form of data, which can be stored and transferred between users, can be viewed as an intangible commodity, which can be traded in exchange for money. Determining the fair price at which a string of data should be traded is an important and open problem in many settings. In this work we develop a statistical physics framework that allows to determine analytically the fair price of information exchanged between players in a game of chance. For definiteness, we consider a game where $N$ players bet on the binary outcome of a stochastic process and share the entry fees pot if successful. We assume that one player holds information about past outcomes of the game, which they may either use exclusively to improve their betting strategy or offer to sell to another player. We find a sharp transition as the number of players $N$ is tuned across a critical value, between a phase where the transaction is always profitable for the seller and one where it may not be. In both phases, different regimes are possible, depending on the \"quality\" of information being put up for sale: we observe symbiotic regimes, where both parties collude effectively to rig the game in their favor, competitive regimes, where the transaction is unappealing to the data holder as it overly favors a competitor for scarce resources, and even prey-predator regimes, where an exploitative data holder could be giving away bad-quality data to undercut a competitor. Our analytical framework can be generalized to more complex settings and constitutes a flexible tool to address the rich and timely problem of pricing information in games of chance."}, "https://arxiv.org/abs/2401.04436": {"title": "A Payne-Whitham model of urban traffic networks in the presence of traffic lights and its application to traffic optimisation", "link": "https://arxiv.org/abs/2401.04436", "description": "arXiv:2401.04436v2 Announce Type: replace-cross \nAbstract: Urban road transport is a major civilisational and economic challenge, affecting the quality of life and economic activity. Addressing these challenges requires a multidisciplinary approach and sustainable urban planning strategies to mitigate the negative effects of traffic in cities. In this paper, we introduce an extension of one of the most popular macroscopic traffic simulation models, the Payne-Whitham model. We investigate how this model, originally designed to model highway traffic on straight road segments, can be adapted to more realistic conditions with arbitrary road network graphs and multiple intersections with traffic signals. Furthermore, we showcase the practical application of this extension in experiments aimed at optimising traffic signal settings. For computational reasons, these experiments involve the adoption of surrogate models for approximating our extended Payne-Whitham model, and subsequently, we utilise the Differential Evolution optimization algorithm, resulting in the identification of traffic signal settings that enhance the average speed of cars and decrease the total length of queues, thereby facilitating smoother traffic flow."}, "https://arxiv.org/abs/2402.17073": {"title": "Hyperdimensional Computing for Node Classification and Link Prediction", "link": "https://arxiv.org/abs/2402.17073", "description": "arXiv:2402.17073v2 Announce Type: replace-cross \nAbstract: We introduce a novel method for transductive learning on graphs using hyperdimensional representations. The proposed approach encodes data samples using random projections into a very high-dimensional space (hyperdimensional or HD space for short). It obviates the need for expensive iterative training of the sort required by deep learning methods. Specifically, we propose a Hyperdimensional Graph Learning (HDGL) algorithm. HDGL leverages the \\emph{injectivity} property of node representations of a family of Graph Neural Networks (GNNs) to map node features to the HD space and then uses HD operators such as bundling and binding to aggregate information from the local neighborhood of each node. The resulting latent node representations support both node classification and link prediction tasks, unlike typical deep learning methods, which often require separate models for these tasks.\n  We report results of experiments using widely used benchmark datasets which demonstrate that, on the node classification task, HDGL is competitive with the SOTA GNN methods with respect to accuracy, at substantially reduced computational cost. Furthermore, HDGL is well-suited for class incremental learning where the model has to learn to effectively discriminate between a growing number of classes. Our experiments also show that the HD representation constructed by HDGL supports link prediction at accuracies comparable to that of DeepWalk and related methods, although it falls short of SOTA Graph Neural Network (GNN) methods that rely on computationally expensive iterative training. We conclude that HDGL offers a computationally efficient alternative to graph neural networks for node classification, especially in settings that call for class-incremental learning or in applications that demand high accuracy models at significantly lower computational cost and learning time than possible with the SOTA GNNs."}, "https://arxiv.org/abs/2407.15946": {"title": "Universal emergence of local Zipf's law", "link": "https://arxiv.org/abs/2407.15946", "description": "arXiv:2407.15946v1 Announce Type: new \nAbstract: A plethora of natural and socio-economic phenomena share a striking statistical regularity, that is the magnitude of elements decreases with a power law as a function of their position in a ranking of magnitude. Such regularity is commonly known as Zipf's law, and plenty of problem-specific explanations for its emergence have been provided in different fields. Yet, an explanation for Zipf's law ubiquity is currently lacking. In this paper, we demonstrate from first principles that Zipf's behavior naturally emerges as a local approximation to the order statistics generated by any ranking process. We validate our results against several relevant examples."}, "https://arxiv.org/abs/2407.16011": {"title": "Scenarios of future annual carbon footprints of astronomical research infrastructures", "link": "https://arxiv.org/abs/2407.16011", "description": "arXiv:2407.16011v1 Announce Type: new \nAbstract: Research infrastructures have been identified as an important source of greenhouse gas emissions of astronomical research. Based on a comprehensive inventory of 1,211 ground-based observatories and space missions, we assessed the evolution of the number of astronomical facilities and their carbon footprint from 1945 to 2022. We found that space missions dominate greenhouse gas emissions in astronomy, showing an important peak at the end of the 1960ies, followed by a decrease that has turned again into a rise over the last decade. Extrapolating past trends, we predict that greenhouse gas emissions from astronomical facilities will experience no strong decline in the future, and may even rise substantially, unless research practices are changed. We demonstrate that a continuing growth in the number of operating astronomical facilities is not environmentally sustainable. These findings should motivate the astronomical community to reflect about the necessary evolutions that would put astronomical research on a sustainable path."}, "https://arxiv.org/abs/2407.16014": {"title": "Political Elites in the Attention Economy: Visibility Over Civility and Credibility?", "link": "https://arxiv.org/abs/2407.16014", "description": "arXiv:2407.16014v1 Announce Type: new \nAbstract: Elected officials have privileged roles in public communication. In contrast to national politicians, whose posting content is more likely to be closely scrutinized by a robust ecosystem of nationally focused media outlets, sub-national politicians are more likely to openly disseminate harmful content with limited media scrutiny. In this paper, we analyze the factors that explain the online visibility of over 6.5K unique state legislators in the US and how their visibility might be impacted by posting low-credibility or uncivil content. We conducted a study of posting on Twitter and Facebook (FB) during 2020-21 to analyze how legislators engage with users on these platforms. The results indicate that distributing content with low-credibility information attracts greater attention from users on FB and Twitter for Republicans. Conversely, posting content that is considered uncivil on Twitter receives less attention. A noticeable scarcity of posts containing uncivil content was observed on FB, which may be attributed to the different communication patterns of legislators on these platforms. In most cases, the effect is more pronounced among the most ideologically extreme legislators. Our research explores the influence exerted by state legislators on online political conversations, with Twitter and FB serving as case studies. Furthermore, it sheds light on the differences in the conduct of political actors on these platforms. This study contributes to a better understanding of the role that political figures play in shaping online political discourse."}, "https://arxiv.org/abs/2407.16051": {"title": "ElectionRumors2022: A Dataset of Election Rumors on Twitter During the 2022 US Midterms", "link": "https://arxiv.org/abs/2407.16051", "description": "arXiv:2407.16051v1 Announce Type: new \nAbstract: Understanding the spread of online rumors is a pressing societal challenge and an active area of research across domains. In the context of the 2022 U.S. midterm elections, one influential social media platform for sharing information -- including rumors that may be false, misleading, or unsubstantiated -- was Twitter (now renamed X). To increase understanding of the dynamics of online rumors about elections, we present and analyze a dataset of 1.81 million Twitter posts corresponding to 135 distinct rumors which spread online during the midterm election season (September 5 to December 1, 2022). We describe how this data was collected, compiled, and supplemented, and provide a series of exploratory analyses along with comparisons to a previously-published dataset on 2020 election rumors. We also conduct a mixed-methods analysis of three distinct rumors about the election in Arizona, a particularly prominent focus of 2022 election rumoring. Finally, we provide a set of potential future directions for how this dataset could be used to facilitate future research into online rumors, misinformation, and disinformation."}, "https://arxiv.org/abs/2407.16152": {"title": "Discovering overlapping communities in multi-layer directed networks", "link": "https://arxiv.org/abs/2407.16152", "description": "arXiv:2407.16152v1 Announce Type: new \nAbstract: This article explores the challenging problem of detecting overlapping communities in multi-layer directed networks. Our goal is to understand the underlying asymmetric overlapping community structure by analyzing the mixed memberships of nodes. We introduce a new model, the multi-layer mixed membership stochastic co-block model (multi-layer MM-ScBM), to model multi-layer directed networks in which nodes can belong to multiple communities. We develop a spectral procedure to estimate nodes' memberships in both sending and receiving patterns. Our method uses a successive projection algorithm on a few leading eigenvectors of two debiased aggregation matrices. To our knowledge, this is the first work to detect asymmetric overlapping communities in multi-layer directed networks. We demonstrate the consistent estimation properties of our method by providing per-node error rates under the multi-layer MM-ScBM framework. Our theoretical analysis reveals that increasing the overall sparsity, the number of nodes, or the number of layers can improve the accuracy of overlapping community detection. Extensive numerical experiments are conducted to validate these theoretical findings. We also apply our method to one real-world multi-layer directed network, gaining insightful results."}, "https://arxiv.org/abs/2407.16183": {"title": "Random walks on bifractal networks", "link": "https://arxiv.org/abs/2407.16183", "description": "arXiv:2407.16183v1 Announce Type: new \nAbstract: It has recently been shown that networks possessing the scale-free and fractal properties may exhibit the bifractal nature, in which local structures are described by two different fractal dimensions. In this study, we investigate random walks on fractal scale-free networks (FSFNs) by examining the walk dimension $d_{\\text{w}}$ and the spectral dimension $d_{\\text{s}}$, to understand how the bifractality of FSFNs affects their dynamical properties. The walk dimension is found to be unaffected by the difference in local fractality of an FSFN and remains constant regardless of the starting node of a random walk, whereas the spectral dimension takes two values, $d_{\\text{s}}^{\\text{min}}$ and $d_{\\text{s}}^{\\text{max}}(> d_{\\text{s}}^{\\text{min}})$, depending on the starting node. The dimension $d_{\\text{s}}^{\\text{min}}$ characterizes the return probability of a random walker starting from an infinite-degree hub node in the thermodynamic limit, while $d_{\\text{s}}^{\\text{max}}$ describes that of a random walker starting from a finite-degree non-hub node infinitely distant from hub nodes and is equal to the global spectral dimension $D_{\\text{s}}$. The existence of two local spectral dimensions is a direct consequence of the bifractality of the FSFN. Furthermore, analytical expressions of $d_{\\text{w}}$, $d_{\\text{s}}^{\\text{min}}$, and $d_{\\text{s}}^{\\text{max}}$ are presented for FSFNs formed by the generator model and the giant components of critical scale-free random graphs, and are numerically confirmed."}, "https://arxiv.org/abs/2407.16380": {"title": "q-deformed evolutionary dynamics in simple matrix games", "link": "https://arxiv.org/abs/2407.16380", "description": "arXiv:2407.16380v1 Announce Type: new \nAbstract: We consider evolutionary games in which the agent selected for update compares their payoff to q neighbours, rather than a single neighbour as in standard evolutionary game theory. Through studying fixed point stability and fixation times for 2x2 games with all-to-all interactions, we find that the flow changes significantly as a function of q. Further, we investigate the effects of changing the underlying topology from an all-to-all interacting system to an uncorrelated graph via the pair approximation. We also develop the framework for studying games with more than two strategies, such as the rock-paper-scissors game where we show that changing q leads to the emergence of new types of flow."}, "https://arxiv.org/abs/2407.15867": {"title": "The new science of COVID-19: A Bibliographic and Network Analysis", "link": "https://arxiv.org/abs/2407.15867", "description": "arXiv:2407.15867v1 Announce Type: cross \nAbstract: Since the outbreak of the COVID-19, there have been many scientific publications studying the COVID-19. The purpose of this study is to identify the research trend, collaboration pattern, most influential elements, etc. from scientific publications related to COVID-19 in 2020, by using bibliographic analysis and network analysis. In Chapter 1 and Chapter 2, motivation behind this paper is introduced. Some previous similar studies are discussed. Comparisons are made in different aspects, such as data collection methods, data analysis tools and methods, etc. Their advantages and limitations compared to this paper are also addressed. In Chapter 3, important concepts used in this paper related to bibliographic analysis such as h-index and g-index, and network analysis such as centrality measures and assortativity are introduced. Networks with small-world property and scale-free property will also be studied. In Chapter 4 and Chapter 5, the way the data are obtained for the analysis of this paper is introduced step by step. Full result is shown. In Chapter 6, conclusions are arrived. A general growing trend of the number of the publications is observed, due to the efforts made by scientific researchers. Meanwhile, measures should also be taken to encourage future study in this field."}, "https://arxiv.org/abs/2407.15956": {"title": "Future of Home-living: Designing Smart Spaces for Modern Domestic Life", "link": "https://arxiv.org/abs/2407.15956", "description": "arXiv:2407.15956v1 Announce Type: cross \nAbstract: The evolution of smart home technologies, particularly agentic ones such as conversational agents, robots, and virtual avatars, is reshaping our understanding of home and domestic life. This shift highlights the complexities of modern domestic life, with the household landscape now featuring diverse cohabiting units like co-housing and communal living arrangements. These agentic technologies present specific design challenges and opportunities as they become integrated into everyday routines and activities. Our workshop envisions smart homes as dynamic, user-shaped spaces, focusing on the integration of these technologies into daily life. We aim to explore how these technologies transform household dynamics, especially through boundary fluidity, by uniting researchers and practitioners from fields such as design, sociology, and ethnography. Together, we will develop a taxonomy of challenges and opportunities, providing a structured perspective on the integration of agentic technologies and their impact on contemporary living arrangements."}, "https://arxiv.org/abs/2407.16594": {"title": "GenRec: A Flexible Data Generator for Recommendations", "link": "https://arxiv.org/abs/2407.16594", "description": "arXiv:2407.16594v1 Announce Type: cross \nAbstract: The scarcity of realistic datasets poses a significant challenge in benchmarking recommender systems and social network analysis methods and techniques. A common and effective solution is to generate synthetic data that simulates realistic interactions. However, although various methods have been proposed, the existing literature still lacks generators that are fully adaptable and allow easy manipulation of the underlying data distributions and structural properties. To address this issue, the present work introduces GenRec, a novel framework for generating synthetic user-item interactions that exhibit realistic and well-known properties observed in recommendation scenarios. The framework is based on a stochastic generative process based on latent factor modeling. Here, the latent factors can be exploited to yield long-tailed preference distributions, and at the same time they characterize subpopulations of users and topic-based item clusters. Notably, the proposed framework is highly flexible and offers a wide range of hyper-parameters for customizing the generation of user-item interactions. The code used to perform the experiments is publicly available at https://anonymous.4open.science/r/GenRec-DED3."}, "https://arxiv.org/abs/2306.14735": {"title": "Highly engaging events reveal semantic and temporal compression in online community discourse", "link": "https://arxiv.org/abs/2306.14735", "description": "arXiv:2306.14735v2 Announce Type: replace \nAbstract: People nowadays express their opinions in online spaces, using different forms of interactions such as posting, sharing and discussing with one another. How do these digital traces change in response to events happening in the real world? We leverage Reddit conversation data, exploiting its community-based structure, to elucidate how offline events influence online user interactions and behavior. Online conversations, as posts and comments, are analysed along their temporal and semantic dimensions. Conversations tend to become repetitive with a more limited vocabulary, develop at a faster pace and feature heightened emotions. As the event approaches, the shifts occurring in conversations are reflected in the users' dynamics. Users become more active and they exchange information with a growing audience, despite using a less rich vocabulary and repetitive messages. The recurring patterns we discovered are persistent across a wide range of events and several contexts, representing a fingerprint of how online dynamics change in response to real-world occurrences."}, "https://arxiv.org/abs/2401.14999": {"title": "The dynamics of the Reddit collective action leading to the GameStop short squeeze", "link": "https://arxiv.org/abs/2401.14999", "description": "arXiv:2401.14999v3 Announce Type: replace \nAbstract: In early 2021, the stock prices of GameStop, AMC, Nokia, and BlackBerry experienced dramatic increases, triggered by short squeeze operations that have been largely attributed to Reddit's retail investors. These events showcased, for the first time, the potential of online social networks to catalyze financial collective action. How, when and to what extent Reddit users played a role in driving up these prices, however, remains unclear. We address these questions by statistical analysis of time series with high temporal resolution, about social activity on Reddit and Twitter as well as stock trading volumes. We find that increasing Reddit discussions anticipated high trading volume before the GameStop short squeeze, with their predictive power being particularly strong on hourly time scales. This effect emerged abruptly a few weeks before the event, but waned once the community of investors gained widespread visibility through Twitter. Meanwhile, the collective investment of the Reddit community, quantified through each user's financial position on GameStop, closely mirrored the market capitalization of the stock. These evidences suggest a coordinated action by Reddit users in developing a shared financial strategy through social media. Towards the end of January, users talking about GameStop contributed to raise the popularity of BlackBerry, AMC and Nokia, which emerged as the most popular stocks as the community gained global recognition. Overall, our results shed light on the dynamics behind the first large-scale financial collective action driven by social media users."}, "https://arxiv.org/abs/2309.12232": {"title": "Identifying vegetation patterns for a qualitative assessment of land degradation using a cellular automata model and satellite imagery", "link": "https://arxiv.org/abs/2309.12232", "description": "arXiv:2309.12232v2 Announce Type: replace-cross \nAbstract: We aim to identify the spatial distribution of vegetation and its growth dynamics with the purpose of obtaining a qualitative assessment of vegetation health and land degradation. To do so, we compare a statistical model of vegetation growth and real-world Earth observation data. Specifically, we analyze a stochastic cellular automata model and data obtained from satellite images, namely using the Normalized Difference Vegetation Index (NDVI) and the Leaf Area Index (LAI). In the experimental data, we look for areas where vegetation is broken into small patches and qualitatively compare it to the percolating, fragmented, and degraded states that appear in the cellular automata model. We model the periodic effect of seasons, finding numerical evidence of a periodic fragmentation and recovery phenomenology if the model parameters are sufficiently close to the model's percolation transition. We qualitatively recognize these effects in real-world vegetation images and consider them a signal of increased environmental stress and vulnerability. Finally, we show an estimation of the environmental stress in land images by considering both the vegetation density and its clusterization."}, "https://arxiv.org/abs/2401.13782": {"title": "Position: AI/ML Influencers Have a Place in the Academic Process", "link": "https://arxiv.org/abs/2401.13782", "description": "arXiv:2401.13782v3 Announce Type: replace-cross \nAbstract: As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside controls precisely matched by 9 key covariates. Our statistical and causal inference analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. Given these findings, we advocate for a responsible approach to curation, encouraging influencers to uphold the journalistic standard that includes showcasing diverse research topics, authors, and institutions."}, "https://arxiv.org/abs/2407.16850": {"title": "Covering a Graph with Dense Subgraph Families, via Triangle-Rich Sets", "link": "https://arxiv.org/abs/2407.16850", "description": "arXiv:2407.16850v1 Announce Type: new \nAbstract: Graphs are a fundamental data structure used to represent relationships in domains as diverse as the social sciences, bioinformatics, cybersecurity, the Internet, and more. One of the central observations in network science is that real-world graphs are globally sparse, yet contains numerous \"pockets\" of high edge density. A fundamental task in graph mining is to discover these dense subgraphs. Most common formulations of the problem involve finding a single (or a few) \"optimally\" dense subsets. But in most real applications, one does not care for the optimality. Instead, we want to find a large collection of dense subsets that covers a significant fraction of the input graph.\n  We give a mathematical formulation of this problem, using a new definition of regularly triangle-rich (RTR) families. These families capture the notion of dense subgraphs that contain many triangles and have degrees comparable to the subgraph size. We design a provable algorithm, RTRExtractor, that can discover RTR families that approximately cover any RTR set. The algorithm is efficient and is inspired by recent results that use triangle counts for community testing and clustering.\n  We show that RTRExtractor has excellent behavior on a large variety of real-world datasets. It is able to process graphs with hundreds of millions of edges within minutes. Across many datasets, RTRExtractor achieves high coverage using high edge density datasets. For example, the output covers a quarter of the vertices with subgraphs of edge density more than (say) $0.5$, for datasets with 10M+ edges. We show an example of how the output of RTRExtractor correlates with meaningful sets of similar vertices in a citation network, demonstrating the utility of RTRExtractor for unsupervised graph discovery tasks."}, "https://arxiv.org/abs/2407.16852": {"title": "Re-routing game: The inadequacy of mean-field approach in modeling the herd behavior in path switching", "link": "https://arxiv.org/abs/2407.16852", "description": "arXiv:2407.16852v1 Announce Type: new \nAbstract: Coordination of vehicle routes is a feasible way to ease traffic congestions amid a fixed road infrastructure. Nevertheless, even the optimal route configurations are provided to individual drivers, it is hard to achieve as greedy drivers may switch to other routes for a lower individual cost. Recent research uses mean-field cavity approach from the studies of spin glasses to analyze the impact of path switching in optimized transportation networks. However, this method only provides a mean-field approximation, which does not take into account the collective herd behavior in path switching due to un-coordinated individual decisions. In this study, we propose an exhaustive cavity approach to investigate the impact of un-coordinated path switching in a re-routing game and reveal that greedy drivers' decision can be highly correlated which leads to the failure of mean-fielded approaches. Our theoretical results fits well with simulations, and our developed framework can be generalized to analyze other games with multiple players and rounds. Our results shed light on the impact of herd behavior of un-coordinated human drivers in suppressing congestions through path coordination."}, "https://arxiv.org/abs/2407.17441": {"title": "Gender disparities in the dissemination and acquisition of scientific knowledge", "link": "https://arxiv.org/abs/2407.17441", "description": "arXiv:2407.17441v1 Announce Type: new \nAbstract: Recent research has challenged the widespread belief that gender inequities in academia would disappear simply by increasing the number of women. More complex causes might be at play, embodied in the networked structure of scientific collaborations. Here, we aim to understand the structural inequality between male and female scholars in the dissemination of scientific knowledge. We use a large-scale dataset of academic publications from the American Physical Society (APS) to build a time-varying network of collaborations from 1970 to 2020. We model knowledge dissemination as a contagion process in which scientists become informed based on the propagation of knowledge through their collaborators. We quantify the fairness of the system in terms of how women acquire and diffuse knowledge compared to men. Our results indicate that knowledge acquisition and diffusion are slower for women than expected. We find that the main determinant of women's disadvantage is the gap in the cumulative number of collaborators, highlighting how time creates structural disadvantages that contribute to marginalize women in physics. Our work sheds light on how the dynamics of scientific collaborations shape gender disparities in knowledge dissemination and calls for a deeper understanding on how to intervene to improve fairness and diversity in the scientific community."}, "https://arxiv.org/abs/2407.17451": {"title": "BlueTempNet: A Temporal Multi-network Dataset of Social Interactions in Bluesky Social", "link": "https://arxiv.org/abs/2407.17451", "description": "arXiv:2407.17451v1 Announce Type: new \nAbstract: Decentralized social media platforms like Bluesky Social (Bluesky) have made it possible to publicly disclose some user behaviors with millisecond-level precision. Embracing Bluesky's principles of open-source and open-data, we present the first collection of the temporal dynamics of user-driven social interactions. BlueTempNet integrates multiple types of networks into a single multi-network, including user-to-user interactions (following and blocking users) and user-to-community interactions (creating and joining communities). Communities are user-formed groups in custom Feeds, where users subscribe to posts aligned with their interests. Following Bluesky's public data policy, we collect existing Bluesky Feeds, including the users who liked and generated these Feeds, and provide tools to gather users' social interactions within a date range. This data-collection strategy captures past user behaviors and supports the future data collection of user behavior."}, "https://arxiv.org/abs/2407.16863": {"title": "Balanced Multi-Relational Graph Clustering", "link": "https://arxiv.org/abs/2407.16863", "description": "arXiv:2407.16863v1 Announce Type: cross \nAbstract: Multi-relational graph clustering has demonstrated remarkable success in uncovering underlying patterns in complex networks. Representative methods manage to align different views motivated by advances in contrastive learning. Our empirical study finds the pervasive presence of imbalance in real-world graphs, which is in principle contradictory to the motivation of alignment. In this paper, we first propose a novel metric, the Aggregation Class Distance, to empirically quantify structural disparities among different graphs. To address the challenge of view imbalance, we propose Balanced Multi-Relational Graph Clustering (BMGC), comprising unsupervised dominant view mining and dual signals guided representation learning. It dynamically mines the dominant view throughout the training process, synergistically improving clustering performance with representation learning. Theoretical analysis ensures the effectiveness of dominant view mining. Extensive experiments and in-depth analysis on real-world and synthetic datasets showcase that BMGC achieves state-of-the-art performance, underscoring its superiority in addressing the view imbalance inherent in multi-relational graphs. The source code and datasets are available at https://github.com/zxlearningdeep/BMGC."}, "https://arxiv.org/abs/2407.17132": {"title": "Exploring Covid-19 Spatiotemporal Dynamics: Non-Euclidean Spatially Aware Functional Registration", "link": "https://arxiv.org/abs/2407.17132", "description": "arXiv:2407.17132v1 Announce Type: cross \nAbstract: When it came to Covid-19, timing was everything. This paper considers the spatiotemporal dynamics of the Covid-19 pandemic via a developed methodology of non-Euclidean spatially aware functional registration. In particular, the daily SARS-CoV-2 incidence in each of 380 local authorities in the UK from March to June 2020 is analysed to understand the phase variation of the waves when considered as curves. This is achieved by adapting a traditional registration method (that of local variation analysis) to account for the clear spatial dependencies in the data. This adapted methodology is shown via simulation studies to perform substantially better for the estimation of the registration functions than the non-spatial alternative. Moreover, it is found that the driving time between locations represents the spatial dependency in the Covid-19 data better than geographical distance. However, since driving time is non-Euclidean, the traditional spatial frameworks break down; to solve this, a methodology inspired by multidimensional scaling is developed to approximate the driving times by a Euclidean distance which enables the established theory to be applied. Finally, the resulting estimates of the registration/warping processes are analysed by taking functionals to understand the qualitatively observable earliness/lateness and sharpness/flatness of the Covid-19 waves quantitatively."}, "https://arxiv.org/abs/2306.00833": {"title": "When Does Bottom-up Beat Top-down in Hierarchical Community Detection?", "link": "https://arxiv.org/abs/2306.00833", "description": "arXiv:2306.00833v2 Announce Type: replace \nAbstract: Hierarchical clustering of networks consists in finding a tree of communities, such that lower levels of the hierarchy reveal finer-grained community structures. There are two main classes of algorithms tackling this problem. Divisive ($\\textit{top-down}$) algorithms recursively partition the nodes into two communities, until a stopping rule indicates that no further split is needed. In contrast, agglomerative ($\\textit{bottom-up}$) algorithms first identify the smallest community structure and then repeatedly merge the communities using a $\\textit{linkage}$ method. In this article, we establish theoretical guarantees for the recovery of the hierarchical tree and community structure of a Hierarchical Stochastic Block Model by a bottom-up algorithm. We also establish that this bottom-up algorithm attains the information-theoretic threshold for exact recovery at intermediate levels of the hierarchy. Notably, these recovery conditions are less restrictive compared to those existing for top-down algorithms. This shows that bottom-up algorithms extend the feasible region for achieving exact recovery at intermediate levels. Numerical experiments on both synthetic and real data sets confirm the superiority of bottom-up algorithms over top-down algorithms. We also observe that top-down algorithms can produce dendrograms with inversions. These findings contribute to a better understanding of hierarchical clustering techniques and their applications in network analysis."}, "https://arxiv.org/abs/2401.09769": {"title": "Learning from Graphs with Heterophily: Progress and Future", "link": "https://arxiv.org/abs/2401.09769", "description": "arXiv:2401.09769v3 Announce Type: replace \nAbstract: Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corresponding open-source codes can be accessed and will be continuously updated at our repositories:https://github.com/gongchenghua/Papers-Graphs-with-Heterophily."}, "https://arxiv.org/abs/2308.09195": {"title": "Architecture and Applications of IoT Devices in Socially Relevant Fields", "link": "https://arxiv.org/abs/2308.09195", "description": "arXiv:2308.09195v2 Announce Type: replace-cross \nAbstract: Number of IoT enabled devices are being tried and introduced every year and there is a healthy competition among researched and businesses to capitalize the space created by IoT, as these devices have a great market potential. Depending on the type of task involved and sensitive nature of data that the device handles, various IoT architectures, communication protocols and components are chosen and their performance is evaluated. This paper reviews such IoT enabled devices based on their architecture, communication protocols and functions in few key socially relevant fields like health care, farming, firefighting, women/individual safety/call for help/harm alert, home surveillance and mapping as these fields involve majority of the general public. It can be seen, to one's amazement, that already significant number of devices are being reported on these fields and their performance is promising. This paper also outlines the challenges involved in each of these fields that require solutions to make these devices reliable"}, "https://arxiv.org/abs/2402.03365": {"title": "Heterophily-Aware Fair Recommendation using Graph Convolutional Networks", "link": "https://arxiv.org/abs/2402.03365", "description": "arXiv:2402.03365v2 Announce Type: replace-cross \nAbstract: In recent years, graph neural networks (GNNs) have become a popular tool to improve the accuracy and performance of recommender systems. Modern recommender systems are not only designed to serve end users, but also to benefit other participants, such as items and items providers. These participants may have different or conflicting goals and interests, which raise the need for fairness and popularity bias considerations. GNN-based recommendation methods also face the challenges of unfairness and popularity bias and their normalization and aggregation processes suffer from these challenges. In this paper, we propose a fair GNN-based recommender system, called HetroFair, to improve items' side fairness. HetroFair uses two separate components to generate fairness-aware embeddings: i) fairnessaware attention which incorporates dot product in the normalization process of GNNs, to decrease the effect of nodes' degrees, and ii) heterophily feature weighting to assign distinct weights to different features during the aggregation process. In order to evaluate the effectiveness of HetroFair, we conduct extensive experiments over six real-world datasets. Our experimental results reveal that HetroFair not only alleviates the unfairness and popularity bias on items' side, but also achieves superior accuracy on users' side. Our implementation is publicly available at https://github.com/NematGH/HetroFair."}, "https://arxiv.org/abs/2402.11933": {"title": "SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning", "link": "https://arxiv.org/abs/2402.11933", "description": "arXiv:2402.11933v2 Announce Type: replace-cross \nAbstract: To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed. While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams. In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels. In this paper, we propose SLADE (Self-supervised Learning for Anomaly Detection in Edge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels. SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time. To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term ones. Failure in these tasks for a node signals its deviation from the norm. Notably, the neural network and tasks are carefully designed so that all required operations can be performed in constant time (w.r.t. the graph size) in response to each new edge in the input stream. In dynamic anomaly detection across four real-world datasets, SLADE outperforms nine competing methods, even those leveraging label supervision."}, "https://arxiv.org/abs/2407.17749": {"title": "Opinion dynamics on switching networks", "link": "https://arxiv.org/abs/2407.17749", "description": "arXiv:2407.17749v1 Announce Type: new \nAbstract: We study opinion dynamics over a directed multilayer network. In particular, we consider networks in which the impact of neighbors of agents on their opinions is proportional to their in-degree. Agents update their opinions over time to coordinate with their neighbors. However, the frequency of agents' interactions with neighbors in different network layers differs. Consequently, the multilayer network's adjacency matrices are time-varying. We aim to characterize how the frequency of activation of different layers impacts the convergence of the opinion dynamics process."}, "https://arxiv.org/abs/2407.17787": {"title": "HC-GST: Heterophily-aware Distribution Consistency based Graph Self-training", "link": "https://arxiv.org/abs/2407.17787", "description": "arXiv:2407.17787v1 Announce Type: new \nAbstract: Graph self-training (GST), which selects and assigns pseudo-labels to unlabeled nodes, is popular for tackling label sparsity in graphs. However, recent study on homophily graphs show that GST methods could introduce and amplify distribution shift between training and test nodes as they tend to assign pseudo-labels to nodes they are good at. As GNNs typically perform better on homophilic nodes, there could be potential shifts towards homophilic pseudo-nodes, which is underexplored. Our preliminary experiments on heterophilic graphs verify that these methods can cause shifts in homophily ratio distributions, leading to \\textit{training bias} that improves performance on homophilic nodes while degrading it on heterophilic ones. Therefore, we study a novel problem of reducing homophily ratio distribution shifts during self-training on heterophilic graphs. A key challenge is the accurate calculation of homophily ratios and their distributions without extensive labeled data. To tackle them, we propose a novel Heterophily-aware Distribution Consistency-based Graph Self-Training (HC-GST) framework, which estimates homophily ratios using soft labels and optimizes a selection vector to align pseudo-nodes with the global homophily ratio distribution. Extensive experiments on both homophilic and heterophilic graphs show that HC-GST effectively reduces training bias and enhances self-training performance."}, "https://arxiv.org/abs/2407.17825": {"title": "Blockchain Takeovers in Web 3", "link": "https://arxiv.org/abs/2407.17825", "description": "arXiv:2407.17825v1 Announce Type: new \nAbstract: A fundamental goal of Web 3.0 is to establish a decentralized network and application ecosystem, thereby enabling users to retain control over their data while promoting value exchange. However, the recent Tron-Steem takeover incident poses a significant threat to this vision. In this paper, we present a thorough empirical analysis of the Tron-Steem takeover incident. By conducting a fine-grained reconstruction of the stake and election snapshots within the Steem blockchain, one of the most prominent social-oriented blockchains, we quantify the marked shifts in decentralization pre and post the takeover incident, highlighting the severe threat that blockchain network takeovers pose to the decentralization principle of Web 3.0. Moreover, by employing heuristic methods to identify anomalous voters and conducting clustering analyses on voter behaviors, we unveil the underlying mechanics of takeover strategies employed in the Tron-Steem incident and suggest potential mitigation strategies, which contribute to the enhanced resistance of Web 3.0 networks against similar threats in the future. We believe the insights gleaned from this research help illuminate the challenges imposed by blockchain network takeovers in the Web 3.0 era, suggest ways to foster the development of decentralized technologies and governance, as well as to enhance the protection of Web 3.0 user rights."}, "https://arxiv.org/abs/2407.18086": {"title": "Revealing urban area from mobile positioning data", "link": "https://arxiv.org/abs/2407.18086", "description": "arXiv:2407.18086v1 Announce Type: new \nAbstract: Researchers face the trade-off between publishing mobility data along with their papers while simultaneously protecting the privacy of the individuals. In addition to the fundamental anonymization process, other techniques, such as spatial discretization and, in certain cases, location concealing or complete removal, are applied to achieve these dual objectives. The primary research question is whether concealing the observation area is an adequate form of protection or whether human mobility patterns in urban areas are inherently revealing of location. The characteristics of the mobility data, such as the number of activity records or the number of unique users in a given spatial unit, reveal the silhouette of the urban landscape, which can be used to infer the identity of the city in question. It was demonstrated that even without disclosing the exact location, the patterns of human mobility can still reveal the urban area from which the data was collected. The presented locating method was tested on other cities using different open data sets and against coarser spatial discretization units. While publishing mobility data is essential for research, it was demonstrated that concealing the observation area is insufficient to prevent the identification of the urban area. Furthermore, using larger discretization units alone is an ineffective solution to the problem of the observation area re-identification. Instead of obscuring the observation area, noise should be added to the trajectories to prevent user identification."}, "https://arxiv.org/abs/2407.17522": {"title": "Mapping the Technological Future: A Topic, Sentiment, and Emotion Analysis in Social Media Discourse", "link": "https://arxiv.org/abs/2407.17522", "description": "arXiv:2407.17522v1 Announce Type: cross \nAbstract: People worldwide are currently confronted with a number of technological challenges, which act as a potent source of uncertainty. The uncertainty arising from the volatility and unpredictability of technology (such as AI) and its potential consequences is widely discussed on social media. This study uses BERTopic modelling along with sentiment and emotion analysis on 1.5 million tweets from 2021 to 2023 to identify anticipated tech-driven futures and capture the emotions communicated by 400 key opinion leaders (KOLs). Findings indicate positive sentiment significantly outweighs negative, with a prevailing dominance of positive anticipatory emotions. Specifically, the 'Hope' score is approximately 10.33\\% higher than the median 'Anxiety' score. KOLs emphasize 'Optimism' and benefits over 'Pessimism' and challenges. The study emphasizes the important role KOLs play in shaping future visions through anticipatory discourse and emotional tone during times of technological uncertainty."}, "https://arxiv.org/abs/2407.17679": {"title": "Instagram versus women of color: Why are women of color protesting Instagram's algorithmic changes?", "link": "https://arxiv.org/abs/2407.17679", "description": "arXiv:2407.17679v1 Announce Type: cross \nAbstract: Instagram has been appropriated by communities for several contemporary social struggles, often translating into real world action. Likewise, women of color (WOC) have used it to protest, share information and support one another through its various affordances. However, Instagram is known to have frequent updates, and recently the updates have been more drastic. The newest update changed the recommendation algorithm such that it showed video-oriented content (reels) from unknown accounts over static media from a user's own network. Several marginalized communities, and especially WOC resisted this change and others that led to it. Due to the backlash, Instagram rolled back its changes. Drawing from past HCI work on digital platforms for marginalised communities, I propose a qualitative study informed by the open research strategy to understand why WOC are resisting these changes, and eventually provide implications for design that can help implement changes in a more inclusive manner."}, "https://arxiv.org/abs/2407.17703": {"title": "Context-aware knowledge graph framework for traffic speed forecasting using graph neural network", "link": "https://arxiv.org/abs/2407.17703", "description": "arXiv:2407.17703v1 Announce Type: cross \nAbstract: Human mobility is intricately influenced by urban contexts spatially and temporally, constituting essential domain knowledge in understanding traffic systems. While existing traffic forecasting models primarily rely on raw traffic data and advanced deep learning techniques, incorporating contextual information remains underexplored due to the lack of effective integration frameworks and the complexity of urban contexts. This study proposes a novel context-aware knowledge graph (CKG) framework to enhance traffic speed forecasting by effectively modeling spatial and temporal contexts. Employing a relation-dependent integration strategy, the framework generates context-aware representations from the spatial and temporal units of CKG to capture spatio-temporal dependencies of urban contexts. A CKG-GNN model, combining the CKG, dual-view multi-head self-attention (MHSA), and graph neural network (GNN), is then designed to predict traffic speed using these context-aware representations. Our experiments demonstrate that CKG's configuration significantly influences embedding performance, with ComplEx and KG2E emerging as optimal for embedding spatial and temporal units, respectively. The CKG-GNN model surpasses benchmark models, achieving an average MAE of $3.46\\pm0.01$ and a MAPE of $14.76\\pm0.09\\%$ for traffic speed predictions from 10 to 120 minutes. The dual-view MHSA analysis reveals the crucial role of relation-dependent features from the context-based view and the model's ability to prioritize recent time slots in prediction from the sequence-based view. The CKG framework's model-agnostic nature suggests its potential applicability in various applications of intelligent transportation systems. Overall, this study underscores the importance of incorporating domain-specific contexts into traffic forecasting and merging context-aware knowledge graphs with neural networks to enhance accuracy."}, "https://arxiv.org/abs/2407.18098": {"title": "Unraveling the Web of Disinformation: Exploring the Larger Context of State-Sponsored Influence Campaigns on Twitter", "link": "https://arxiv.org/abs/2407.18098", "description": "arXiv:2407.18098v1 Announce Type: cross \nAbstract: Social media platforms offer unprecedented opportunities for connectivity and exchange of ideas; however, they also serve as fertile grounds for the dissemination of disinformation. Over the years, there has been a rise in state-sponsored campaigns aiming to spread disinformation and sway public opinion on sensitive topics through designated accounts, known as troll accounts. Past works on detecting accounts belonging to state-backed operations focus on a single campaign. While campaign-specific detection techniques are easier to build, there is no work done on developing systems that are campaign-agnostic and offer generalized detection of troll accounts unaffected by the biases of the specific campaign they belong to. In this paper, we identify several strategies adopted across different state actors and present a system that leverages them to detect accounts from previously unseen campaigns. We study 19 state-sponsored disinformation campaigns that took place on Twitter, originating from various countries. The strategies include sending automated messages through popular scheduling services, retweeting and sharing selective content and using fake versions of verified applications for pushing content. By translating these traits into a feature set, we build a machine learning-based classifier that can correctly identify up to 94% of accounts from unseen campaigns. Additionally, we run our system in the wild and find more accounts that could potentially belong to state-backed operations. We also present case studies to highlight the similarity between the accounts found by our system and those identified by Twitter."}, "https://arxiv.org/abs/2407.18108": {"title": "Graph Neural Ordinary Differential Equations for Coarse-Grained Socioeconomic Dynamics", "link": "https://arxiv.org/abs/2407.18108", "description": "arXiv:2407.18108v1 Announce Type: cross \nAbstract: We present a data-driven machine-learning approach for modeling space-time socioeconomic dynamics. Through coarse-graining fine-scale observations, our modeling framework simplifies these complex systems to a set of tractable mechanistic relationships -- in the form of ordinary differential equations -- while preserving critical system behaviors. This approach allows for expedited 'what if' studies and sensitivity analyses, essential for informed policy-making. Our findings, from a case study of Baltimore, MD, indicate that this machine learning-augmented coarse-grained model serves as a powerful instrument for deciphering the complex interactions between social factors, geography, and exogenous stressors, offering a valuable asset for system forecasting and resilience planning."}, "https://arxiv.org/abs/2407.18154": {"title": "Identification of a time-varying SIR Model for Covid-19", "link": "https://arxiv.org/abs/2407.18154", "description": "arXiv:2407.18154v1 Announce Type: cross \nAbstract: Throughout human history, epidemics have been a constant presence. Understanding their dynamics is essential to predict scenarios and make substantiated decisions. Mathematical models are powerful tools to describe an epidemic behavior. Among the most used, the compartmental ones stand out, dividing population into classes with well-defined characteristics. One of the most known is the $SIR$ model, based on a set of differential equations describing the rates of change of three categories over time. These equations take into account parameters such as the disease transmission rate and the recovery rate, which both change over time. However, classical models use constant parameters and can not describe the behavior of a disease over long periods. In this work, it is proposed a $SIR$ model with time-varying transmission rate parameter with a method to estimate this parameter based on an optimization problem, which minimizes the sum of the squares of the errors between the model and historical data. Additionally, based on the infection rates determined by the algorithm, the model's ability to predict disease activity in future scenarios was also investigated. Epidemic data released by the government of the State of Rio Grande do Sul in Brazil was used to evaluate the models, where the models shown a very good forecasting ability, resulting in errors for predicting the total number of accumulated infected persons of 0.13% for 7 days ahead and 0.6% for 14 days ahead."}, "https://arxiv.org/abs/2309.16228": {"title": "Brand Network Booster: A new system for improving brand connectivity", "link": "https://arxiv.org/abs/2309.16228", "description": "arXiv:2309.16228v2 Announce Type: replace \nAbstract: This paper presents a new decision support system offered for an in-depth analysis of semantic networks, which can provide insights for a better exploration of a brand's image and the improvement of its connectivity. In terms of network analysis, we show that this goal is achieved by solving an extended version of the Maximum Betweenness Improvement problem, which includes the possibility of considering adversarial nodes, constrained budgets, and weighted networks - where connectivity improvement can be obtained by adding links or increasing the weight of existing connections. Our contribution includes a new algorithmic framework and the integration of this framework into a software system called Brand Network Booster (BNB), which supports brand connectivity evaluation and improvement. We present this new system together with three case studies, and we also discuss its performance. Our tool and approach are valuable to both network scholars and in facilitating strategic decision-making processes for marketing and communication managers across various sectors, be it public or private."}, "https://arxiv.org/abs/2201.07794": {"title": "A Non-Expert's Introduction to Data Ethics for Mathematicians", "link": "https://arxiv.org/abs/2201.07794", "description": "arXiv:2201.07794v5 Announce Type: replace-cross \nAbstract: I give a short introduction to data ethics. I begin with some background information and societal context for data ethics. I then discuss data ethics in mathematical-science education and indicate some available course material. I briefly highlight a few efforts -- at my home institution and elsewhere -- on data ethics, society, and social good. I then discuss open data in research, research replicability and some other ethical issues in research, and the tension between privacy and open data and code, and a few controversial studies and reactions to studies. I then discuss ethical principles, institutional review boards, and a few other considerations in the scientific use of human data. I then briefly survey a variety of research and lay articles that are relevant to data ethics and data privacy. I conclude with a brief summary and some closing remarks.\n  My focal audience is mathematicians, but I hope that this chapter will also be useful to others. I am not an expert about data ethics, and this chapter provides only a starting point on this wide-ranging topic. I encourage you to examine the resources that I discuss and to reflect carefully on data ethics, its role in mathematics education, and the societal implications of data and data analysis. As data and technology continue to evolve, I hope that such careful reflection will continue throughout your life."}, "https://arxiv.org/abs/2401.04280": {"title": "Predicting the structure of dynamic graphs", "link": "https://arxiv.org/abs/2401.04280", "description": "arXiv:2401.04280v2 Announce Type: replace-cross \nAbstract: Many aspects of graphs have been studied in depth. However, forecasting the structure of a graph at future time steps incorporating unseen, new nodes and edges has not gained much attention. In this paper, we present such an approach. Using a time series of graphs, we forecast graphs at future time steps. We use time series forecasting methods to predict the node degree at future time points and combine these forecasts with flux balance analysis -- a linear programming method used in biochemistry -- to obtain the structure of future graphs. We evaluate this approach using synthetic and real-world datasets and demonstrate its utility and applicability."}, "https://arxiv.org/abs/2407.18274": {"title": "Adaptive Differentially Private Structural Entropy Minimization for Unsupervised Social Event Detection", "link": "https://arxiv.org/abs/2407.18274", "description": "arXiv:2407.18274v1 Announce Type: new \nAbstract: Social event detection refers to extracting relevant message clusters from social media data streams to represent specific events in the real world. Social event detection is important in numerous areas, such as opinion analysis, social safety, and decision-making. Most current methods are supervised and require access to large amounts of data. These methods need prior knowledge of the events and carry a high risk of leaking sensitive information in the messages, making them less applicable in open-world settings. Therefore, conducting unsupervised detection while fully utilizing the rich information in the messages and protecting data privacy remains a significant challenge. To this end, we propose a novel social event detection framework, ADP-SEMEvent, an unsupervised social event detection method that prioritizes privacy. Specifically, ADP-SEMEvent is divided into two stages, i.e., the construction stage of the private message graph and the clustering stage of the private message graph. In the first stage, an adaptive differential privacy approach is used to construct a private message graph. In this process, our method can adaptively apply differential privacy based on the events occurring each day in an open environment to maximize the use of the privacy budget. In the second stage, to address the reduction in data utility caused by noise, a novel 2-dimensional structural entropy minimization algorithm based on optimal subgraphs is used to detect events in the message graph. The highlight of this process is unsupervised and does not compromise differential privacy. Extensive experiments on two public datasets demonstrate that ADP-SEMEvent can achieve detection performance comparable to state-of-the-art methods while maintaining reasonable privacy budget parameters."}, "https://arxiv.org/abs/2407.18275": {"title": "Relations between average clustering coefficient and another centralities in graphs", "link": "https://arxiv.org/abs/2407.18275", "description": "arXiv:2407.18275v1 Announce Type: new \nAbstract: Relations between average clustering coefficient and global clustering coefficient, local efficiency, radiality, closeness, betweenness and stress centralities were obtained for simple graphs."}, "https://arxiv.org/abs/2407.18277": {"title": "Online Social Network Data-Driven Early Detection on Short-Form Video Addiction", "link": "https://arxiv.org/abs/2407.18277", "description": "arXiv:2407.18277v1 Announce Type: new \nAbstract: Short-form video (SFV) has become a globally popular form of entertainment in recent years, appearing on major social media platforms. However, current research indicate that short video addiction can lead to numerous negative effects on both physical and psychological health, such as decreased attention span and reduced motivation to learn. Additionally, Short-form Video Addiction (SFVA) has been linked to other issues such as a lack of psychological support in real life, family or academic pressure, and social anxiety. Currently, the detection of SFVA typically occurs only after users experience negative consequences. Therefore, we aim to construct a short video addiction dataset based on social network behavior and design an early detection framework for SFVA. Previous mental health detection research on online social media has mostly focused on detecting depression and suicidal tendency. In this study, we propose the first early detection framework for SFVA EarlySD. We first introduce large language models (LLMs) to address the common issues of sparsity and missing data in graph datasets. Meanwhile, we categorize social network behavior data into different modalities and design a heterogeneous social network structure as the primary basis for detecting SFVA. We conduct a series of quantitative analysis on short video addicts using our self-constructed dataset, and perform extensive experiments to validate the effectiveness of our method EarlySD, using social data and heterogeneous social graphs in the detection of short video addiction."}, "https://arxiv.org/abs/2407.18278": {"title": "Talking Wikidata: Communication patterns and their impact on community engagement in collaborative knowledge graphs", "link": "https://arxiv.org/abs/2407.18278", "description": "arXiv:2407.18278v1 Announce Type: new \nAbstract: We study collaboration patterns of Wikidata, one of the world's largest collaborative knowledge graph communities. Wikidata lacks long-term engagement with a small group of priceless members, 0.8%, to be responsible for 80% of contributions. Therefore, it is essential to investigate their behavioural patterns and find ways to enhance their contributions and participation. Previous studies have highlighted the importance of discussions among contributors in understanding these patterns. To investigate this, we analyzed all the discussions on Wikidata and used a mixed methods approach, including statistical tests, network analysis, and text and graph embedding representations. Our research showed that the interactions between Wikidata editors form a small world network where the content of a post influences the continuity of conversations. We also found that the account age of Wikidata members and their conversations are significant factors in their long-term engagement with the project. Our findings can benefit the Wikidata community by helping them improve their practices to increase contributions and enhance long-term participation."}, "https://arxiv.org/abs/2407.18546": {"title": "Statistical Analysis of the Properties of Geometric Network with Node Mobility", "link": "https://arxiv.org/abs/2407.18546", "description": "arXiv:2407.18546v1 Announce Type: new \nAbstract: The movement changes the underlying spatial representation of the participated mobile objects or nodes. In real world scenario, such mobile nodes can be part of any biological network, transportation network, social network, human interaction, etc. The change in the geometry leads to the change in various desirable properties of real-world networks especially in human interaction networks. In real life, human movement is concerned for better lifestyle where they form their new connections due to the geographical changes. Therefore, in this paper, we design a model for geometric networks with mobile nodes (GNMN) and conduct a comprehensive statistical analysis of their properties. We analyze the effect of node mobility by evaluating key network metrics such as connectivity, node degree distribution, second hop neighbors, and centrality measures. Through extensive simulations, we observe significant variations in the behavior of geometric networks with mobile nodes."}, "https://arxiv.org/abs/2407.18653": {"title": "Managing CO2 under global and country-specific net-zero emissions targets in Europe", "link": "https://arxiv.org/abs/2407.18653", "description": "arXiv:2407.18653v1 Announce Type: new \nAbstract: The European Union (EU) is committed to achieving carbon neutrality by 2050. This requires capturing CO2, eventually transporting it to different regions, to then either convert it into valuable products or sequester it underground. While the target is set for the EU as a whole, a significant part of the governance and strategy remains in the individual member states. Using the networked sector-coupled model PyPSA-Eur, we explored the impacts of imposing net-zero emissions globally for the entire EU versus imposing carbon neutrality for each country. Under a global CO2 target, some countries remain net CO2 emitters, while others become net CO2 absorbers. Forcing net-zero emissions in every country increases system cost by 1.4%, demands varied CO2 prices, and triggers higher investment in direct air capture and renewable capacities. In both scenarios, a significant portion of the captured CO2 is transported across Europe either directly in CO2 pipelines or indirectly via solid biomass or synthetic methane gas, methanol, and oil."}, "https://arxiv.org/abs/2407.18849": {"title": "MNTD: An Efficient Dynamic Community Detector Based on Nonnegative Tensor Decomposition", "link": "https://arxiv.org/abs/2407.18849", "description": "arXiv:2407.18849v1 Announce Type: new \nAbstract: Dynamic community detection is crucial for elucidating the temporal evolution of social structures, information dissemination, and interactive behaviors within complex networks. Nonnegative matrix factorization provides an efficient framework for identifying communities in static networks but fall short in depicting temporal variations in community affiliations. To solve this problem, this paper proposes a Modularity maximization-incorporated Nonnegative Tensor RESCAL Decomposition (MNTD) model for dynamic community detection. This method serves two primary functions: a) Nonnegative tensor RESCAL decomposition extracts latent community structures in different time slots, highlighting the persistence and transformation of communities; and b) Incorporating an initial community structure into the modularity maximization algorithm, facilitating more precise community segmentations. Comparative analysis of real-world datasets shows that the MNTD is superior to state-of-the-art dynamic community detection methods in the accuracy of community detection."}, "https://arxiv.org/abs/2407.18900": {"title": "How Polarized are Online Conversations about Childhood?", "link": "https://arxiv.org/abs/2407.18900", "description": "arXiv:2407.18900v1 Announce Type: new \nAbstract: 2020 through 2023 were unusually tumultuous years for children in the United States, and children's welfare was prominent in political debate. Theories in moral psychology suggest that political parties would treat concerns for children using different moral frames, and that moral conflict might drive substantial polarization in discussions about children. However, such partisan frames may still differ very little if there is limited underlying disagreement about moral issues and everyday concerns in childhood when not explicitly referencing politics. We evaluate claims of universality and division in moral language using tweets from 2019-2023 linked to U.S. voter records, focusing on expressed morality. Our results show that mentions of children by Republicans and Democrats are usually similar, differing no more than mentions by women and men, and tend to contain no large differences in accompanying moral words. To the extent that mentions of children did differ across parties, these differences were constrained to topics polarized well before the pandemic -- and slightly heightened when co-mentioned with `kids' or `children'. These topics reflected a small fraction of conversations about children. Overall, polarization of online discussion around childhood appears to reflect escalated polarization on lines of existing partisan conflicts rather than concerns originating from new concerns about the welfare of children during and after the pandemic."}, "https://arxiv.org/abs/2407.18376": {"title": "Exploring Bengali Religious Dialect Biases in Large Language Models with Evaluation Perspectives", "link": "https://arxiv.org/abs/2407.18376", "description": "arXiv:2407.18376v1 Announce Type: cross \nAbstract: While Large Language Models (LLM) have created a massive technological impact in the past decade, allowing for human-enabled applications, they can produce output that contains stereotypes and biases, especially when using low-resource languages. This can be of great ethical concern when dealing with sensitive topics such as religion. As a means toward making LLMS more fair, we explore bias from a religious perspective in Bengali, focusing specifically on two main religious dialects: Hindu and Muslim-majority dialects. Here, we perform different experiments and audit showing the comparative analysis of different sentences using three commonly used LLMs: ChatGPT, Gemini, and Microsoft Copilot, pertaining to the Hindu and Muslim dialects of specific words and showcasing which ones catch the social biases and which do not. Furthermore, we analyze our findings and relate them to potential reasons and evaluation perspectives, considering their global impact with over 300 million speakers worldwide. With this work, we hope to establish the rigor for creating more fairness in LLMs, as these are widely used as creative writing agents."}, "https://arxiv.org/abs/2407.18564": {"title": "Unveiling Privacy Vulnerabilities: Investigating the Role of Structure in Graph Data", "link": "https://arxiv.org/abs/2407.18564", "description": "arXiv:2407.18564v1 Announce Type: cross \nAbstract: The public sharing of user information opens the door for adversaries to infer private data, leading to privacy breaches and facilitating malicious activities. While numerous studies have concentrated on privacy leakage via public user attributes, the threats associated with the exposure of user relationships, particularly through network structure, are often neglected. This study aims to fill this critical gap by advancing the understanding and protection against privacy risks emanating from network structure, moving beyond direct connections with neighbors to include the broader implications of indirect network structural patterns. To achieve this, we first investigate the problem of Graph Privacy Leakage via Structure (GPS), and introduce a novel measure, the Generalized Homophily Ratio, to quantify the various mechanisms contributing to privacy breach risks in GPS. Based on this insight, we develop a novel graph private attribute inference attack, which acts as a pivotal tool for evaluating the potential for privacy leakage through network structures under worst-case scenarios. To protect users' private data from such vulnerabilities, we propose a graph data publishing method incorporating a learnable graph sampling technique, effectively transforming the original graph into a privacy-preserving version. Extensive experiments demonstrate that our attack model poses a significant threat to user privacy, and our graph data publishing method successfully achieves the optimal privacy-utility trade-off compared to baselines."}, "https://arxiv.org/abs/2407.18772": {"title": "Learning production functions for supply chains with graph neural networks", "link": "https://arxiv.org/abs/2407.18772", "description": "arXiv:2407.18772v1 Announce Type: cross \nAbstract: The global economy relies on the flow of goods over supply chain networks, with nodes as firms and edges as transactions between firms. While we may observe these external transactions, they are governed by unseen production functions, which determine how firms internally transform the input products they receive into output products that they sell. In this setting, it can be extremely valuable to infer these production functions, to better understand and improve supply chains, and to forecast future transactions more accurately. However, existing graph neural networks (GNNs) cannot capture these hidden relationships between nodes' inputs and outputs. Here, we introduce a new class of models for this setting, by combining temporal GNNs with a novel inventory module, which learns production functions via attention weights and a special loss function. We evaluate our models extensively on real supply chains data, along with data generated from our new open-source simulator, SupplySim. Our models successfully infer production functions, with a 6-50% improvement over baselines, and forecast future transactions on real and synthetic data, outperforming baselines by 11-62%."}, "https://arxiv.org/abs/2404.01637": {"title": "Constructive agents nullify the ability of destructive agents to foster cooperation in public goods games", "link": "https://arxiv.org/abs/2404.01637", "description": "arXiv:2404.01637v2 Announce Type: replace \nAbstract: Existing studies have revealed a paradoxical phenomenon in public goods games, wherein destructive agents, harming both cooperators and defectors, can unexpectedly bolster cooperation. Building upon this intriguing premise, our paper introduces a novel concept: constructive agents, which confer additional benefits to both cooperators and defectors. We investigate the impact of these agents on cooperation dynamics within the framework of public goods games. Employing replicator dynamics, we find that unlike destructive agents, the mere presence of constructive agents does not significantly alter the defective equilibrium. However, when the benefits from constructive agents are outweighed by the damage inflicted by destructive agents, the addition of constructive agents does not affect the ability of destructive agents to sustain cooperation. In this scenario, cooperators can be maintained through a cyclic dominance between cooperators, defectors, and destructive agents, with constructive agents adding complexity but not fundamentally changing the equilibrium. Conversely, if the benefits from constructive agents surpass the harm caused by destructive agents, the presence of constructive agents nullifies the ability of destructive agents to foster cooperation. Our results highlight the nuanced role of constructive agents in cooperation dynamics, emphasizing the necessity of carefully assessing incentive balances when encouraging cooperation."}, "https://arxiv.org/abs/2310.19834": {"title": "AMIR: Automated MisInformation Rebuttal -- A COVID-19 Vaccination Datasets based Recommendation System", "link": "https://arxiv.org/abs/2310.19834", "description": "arXiv:2310.19834v2 Announce Type: replace-cross \nAbstract: Misinformation has emerged as a major societal threat in recent years in general; specifically in the context of the COVID-19 pandemic, it has wrecked havoc, for instance, by fuelling vaccine hesitancy. Cost-effective, scalable solutions for combating misinformation are the need of the hour. This work explored how existing information obtained from social media and augmented with more curated fact checked data repositories can be harnessed to facilitate automated rebuttal of misinformation at scale. While the ideas herein can be generalized and reapplied in the broader context of misinformation mitigation using a multitude of information sources and catering to the spectrum of social media platforms, this work serves as a proof of concept, and as such, it is confined in its scope to only rebuttal of tweets, and in the specific context of misinformation regarding COVID-19. It leverages two publicly available datasets, viz. FaCov (fact-checked articles) and misleading (social media Twitter) data on COVID-19 Vaccination."}, "https://arxiv.org/abs/2407.19172": {"title": "Bridging the gap between micro-economics and micro-mobility: a two-dimensional risk-based microscopic model of pedestrians' and bicyclists' operational behaviors", "link": "https://arxiv.org/abs/2407.19172", "description": "arXiv:2407.19172v1 Announce Type: new \nAbstract: Due to the inherent safety concerns associated with traffic movement in unconstrained two-dimensional settings, it is important that pedestrians' and other modes' movements such as bicyclists are modeled as a risk-taking stochastic dynamic process that may lead to errors and thus contacts and collisions. Among the existing models that may capture risk-taking behaviors are: 1) the social force models (through the interplay of the repulsion and the attraction force parameters); 2) and the discrete-choice models (through the rationality or the bounded rationality paradigm while weighing different alternatives). Given that the social force models may not readily capture the contact/collision dynamics through the Newtonian force framework, decision-making theories are hypothesized as a feasible approach to formulate a new model that can account for cognitive and behavioral dimensions such as uncertainty and risk. However, instead of relying on the bounded rationality theory, in this paper, a generalized Prospect Theory based microsimulation model is proposed. The model relies on the micro-economics Prospect Theory paradigm where pedestrians or bicyclists (i.e., micro-mobility users) evaluate their speed and directional alternatives while considering the possibility of colliding with other obstacles/users. A numerical analysis on the main model parameters is presented. The model is then calibrated and validated using two real-world data sets with trajectories recorded in naturalistic settings. With the calibrated parameters studied, simulation exercises and sensitivity analysis are conducted to recreate bottlenecks and lane formations in different conditions. The findings show that the proposed model's parameters reflect the risk-taking tendencies of different roadway users in mixed right-of-way's environments while showing realistic microscopic and macroscopic traffic flow characteristics."}, "https://arxiv.org/abs/2407.19288": {"title": "SignedLouvain: Louvain for signed networks", "link": "https://arxiv.org/abs/2407.19288", "description": "arXiv:2407.19288v1 Announce Type: new \nAbstract: In this article, we consider the problem of community detection in signed networks. We propose SignedLouvain, an adaptation of the Louvain method to maximise signed modularity, efficiently taking advantage of the structure induced by signed relations. We begin by identifying the inherent limitations of applying the standard Louvain algorithm to signed networks, before introducing a novel variant specifically engineered to overcome these challenges. Through extensive experiments on real-world datasets, we demonstrate that the proposed method not only maintains the speed and scalability of its predecessor but also significantly enhances accuracy in detecting communities within signed networks."}, "https://arxiv.org/abs/2407.19390": {"title": "Characterizing limit order books in call auctions of a stock market", "link": "https://arxiv.org/abs/2407.19390", "description": "arXiv:2407.19390v1 Announce Type: new \nAbstract: There has been little research on limit order books in call auctions of stock markets, where trades are executed at a single point in time, although there has been extensive research on continuous auctions, where trades are executed sequentially. This study focuses on the limit order books in the call auctions, with the aim of identifying their shapes and clarifying the relation between these shapes and the trading volume. Using data for all stocks listed on the Tokyo Stock Exchange, we found that the shape of the limit order books in call auctions are well fitted by the hyperbolic tangent function. We defined the median spread and the width of limit orders from the fitting parameters. When the median spread and the width were plotted for all stocks, we observed interesting distributions depending on the attributes of the stocks and the company performances. Furthermore, we showed that the trading volume can be represented by the median spread and the width. The main contribution of this research is to demonstrate that median spread and the width are useful indicators for characterizing the states of the limit order books in the call auctions."}, "https://arxiv.org/abs/2407.19498": {"title": "Independent fact-checking organizations exhibit a departure from political neutrality", "link": "https://arxiv.org/abs/2407.19498", "description": "arXiv:2407.19498v1 Announce Type: new \nAbstract: Independent fact-checking organizations have emerged as the crusaders to debunk fake news. However, they may not always remain neutral, as they can be selective in the false news they choose to expose and in how they present the information. They can deviate from neutrality by being selective in what false news they debunk and how the information is presented. Prompting the now popular large language model, GPT-3.5, with journalistic frameworks, we establish a longitudinal measure (2018-2023) for political neutrality that looks beyond the left-right spectrum. Specified on a range of -1 to 1 (with zero being absolute neutrality), we establish the extent of negative portrayal of political entities that makes a difference in the readers' perception in the USA and India. Here, we observe an average score of -0.17 and -0.24 in the USA and India, respectively. The findings indicate how seemingly objective fact-checking can still carry distorted political views, indirectly and subtly impacting the perception of consumers of the news."}, "https://arxiv.org/abs/2407.19607": {"title": "Gender and the influence of research environment in topic selection of early-career faculty in STEM", "link": "https://arxiv.org/abs/2407.19607", "description": "arXiv:2407.19607v1 Announce Type: new \nAbstract: We study the influence that research environments have in shaping careers of early-career faculty in terms of their research portfolio. We find that departments exert an attractive force over early-career newcomer faculty, who after their incorporation increase their within-department collaborations, and work on topics closer to those of incumbent faculty. However, these collaborations are not gender blind: Newcomers collaborate less than expected with female senior incumbents. The analysis of departments grouped by fraction of female incumbents reveals that female newcomers in departments with above the median fractions of female incumbents tend to select research topics farther from their department than female newcomers in the remaining departments -- a difference we do not observe for male newcomers. Our results suggest a relationship between the collaboration deficit with female incumbents and the selection of research topics of female early-faculty, thus highlighting the importance of studying research environments to fully understand gender differences in academia."}, "https://arxiv.org/abs/2407.19874": {"title": "Statistical Laws in Complex Systems", "link": "https://arxiv.org/abs/2407.19874", "description": "arXiv:2407.19874v1 Announce Type: new \nAbstract: Statistical laws describe regular patterns observed in diverse scientific domains, ranging from the magnitude of earthquakes (Gutenberg-Richter law) and metabolic rates in organisms (Kleiber's law), to the frequency distribution of words in texts (Zipf's and Herdan-Heaps' laws), and productivity metrics of cities (urban scaling laws). The origins of these laws, their empirical validity, and the insights they provide into underlying systems have been subjects of scientific inquiry for centuries. This monograph provides an unifying approach to the study of statistical laws, critically evaluating their role in the theoretical understanding of complex systems and the different data-analysis methods used to evaluate them. Through a historical review and a unified analysis, we uncover that the persistent controversies on the validity of statistical laws are predominantly rooted not in novel empirical findings but in the discordance among data-analysis techniques, mechanistic models, and the interpretations of statistical laws. Starting with simple examples and progressing to more advanced time-series and statistical methods, this monograph and its accompanying repository provide comprehensive material for researchers interested in analyzing data, testing and comparing different laws, and interpreting results in both existing and new datasets."}, "https://arxiv.org/abs/2407.19923": {"title": "Performance of Higher-Order Networks in Reconstructing Sequential Paths: from Micro to Macro Scale", "link": "https://arxiv.org/abs/2407.19923", "description": "arXiv:2407.19923v1 Announce Type: new \nAbstract: Activities such as the movement of passengers and goods, the transfer of physical or digital assets, web navigation and even successive passes in football, result in timestamped paths through a physical or virtual network. The need to analyse such paths has produced a new modelling paradigm in the form of higher-order networks which are able to capture temporal and topological characteristics of sequential data. This has been complemented by sequence mining approaches, a key example being sequential motifs measuring the prevalence of recurrent subsequences. Previous work on higher-order networks has focused on how to identify the optimal order for a path dataset, where the order can be thought of as the number of steps of memory encoded in the model. In this paper, we build on these approaches to consider which orders are necessary to reproduce different path characteristics, from path lengths to counts of sequential motifs, viewing paths generated from different higher-order models as null models which capture features of the data up to a certain order, and randomised otherwise. Furthermore, we provide an important extension to motif counting, whereby cases with self-loops, starting nodes, and ending nodes of paths are taken into consideration. Conducting a thorough analysis using path lengths and sequential motifs on a diverse range of path datasets, we show that our approach can shed light on precisely where models of different order overperform or underperform, and what this may imply about the original path data."}, "https://arxiv.org/abs/2407.19950": {"title": "A multilevel backbone extraction framework", "link": "https://arxiv.org/abs/2407.19950", "description": "arXiv:2407.19950v1 Announce Type: new \nAbstract: As networks grow in size and complexity, backbones become an essential network representation. Indeed, they provide a simplified yet informative overview of the underlying organization by retaining the most significant and structurally influential connections within a network. Network heterogeneity often results in complex and intricate structures, making it challenging to identify the backbone. In response, we introduce the Multilevel Backbone Extraction Framework, a novel approach that diverges from conventional backbone methodologies. This generic approach prioritizes the mesoscopic organization of networks. First, it splits the network into homogeneous-density components. Second, it extracts independent backbones for each component using any classical Backbone technique. Finally, the various backbones are combined. This strategy effectively addresses the heterogeneity observed in network groupings. Empirical investigations on real-world networks underscore the efficacy of the Multilevel Backbone approach in preserving essential network structures and properties. Experiments demonstrate its superiority over classical methods in handling network heterogeneity and enhancing network integrity. The framework is adaptable to various types of networks and backbone extraction techniques, making it a versatile tool for network analysis and backbone extraction across diverse network applications."}, "https://arxiv.org/abs/2407.20024": {"title": "Fairness Through Controlled (Un)Awareness in Node Embeddings", "link": "https://arxiv.org/abs/2407.20024", "description": "arXiv:2407.20024v1 Announce Type: new \nAbstract: Graph representation learning is central for the application of machine learning (ML) models to complex graphs, such as social networks. Ensuring `fair' representations is essential, due to the societal implications and the use of sensitive personal data. In this paper, we demonstrate how the parametrization of the \\emph{CrossWalk} algorithm influences the ability to infer a sensitive attributes from node embeddings. By fine-tuning hyperparameters, we show that it is possible to either significantly enhance or obscure the detectability of these attributes. This functionality offers a valuable tool for improving the fairness of ML systems utilizing graph embeddings, making them adaptable to different fairness paradigms."}, "https://arxiv.org/abs/2407.19196": {"title": "Why Misinformation is Created? Detecting them by Integrating Intent Features", "link": "https://arxiv.org/abs/2407.19196", "description": "arXiv:2407.19196v1 Announce Type: cross \nAbstract: Various social media platforms, e.g., Twitter and Reddit, allow people to disseminate a plethora of information more efficiently and conveniently. However, they are inevitably full of misinformation, causing damage to diverse aspects of our daily lives. To reduce the negative impact, timely identification of misinformation, namely Misinformation Detection (MD), has become an active research topic receiving widespread attention. As a complex phenomenon, the veracity of an article is influenced by various aspects. In this paper, we are inspired by the opposition of intents between misinformation and real information. Accordingly, we propose to reason the intent of articles and form the corresponding intent features to promote the veracity discrimination of article features. To achieve this, we build a hierarchy of a set of intents for both misinformation and real information by referring to the existing psychological theories, and we apply it to reason the intent of articles by progressively generating binary answers with an encoder-decoder structure. We form the corresponding intent features and integrate it with the token features to achieve more discriminative article features for MD. Upon these ideas, we suggest a novel MD method, namely Detecting Misinformation by Integrating Intent featuRes (DM-INTER). To evaluate the performance of DM-INTER, we conduct extensive experiments on benchmark MD datasets. The experimental results validate that DM-INTER can outperform the existing baseline MD methods."}, "https://arxiv.org/abs/2407.19311": {"title": "Can Modifying Data Address Graph Domain Adaptation?", "link": "https://arxiv.org/abs/2407.19311", "description": "arXiv:2407.19311v1 Announce Type: cross \nAbstract: Graph neural networks (GNNs) have demonstrated remarkable success in numerous graph analytical tasks. Yet, their effectiveness is often compromised in real-world scenarios due to distribution shifts, limiting their capacity for knowledge transfer across changing environments or domains. Recently, Unsupervised Graph Domain Adaptation (UGDA) has been introduced to resolve this issue. UGDA aims to facilitate knowledge transfer from a labeled source graph to an unlabeled target graph. Current UGDA efforts primarily focus on model-centric methods, such as employing domain invariant learning strategies and designing model architectures. However, our critical examination reveals the limitations inherent to these model-centric methods, while a data-centric method allowed to modify the source graph provably demonstrates considerable potential. This insight motivates us to explore UGDA from a data-centric perspective. By revisiting the theoretical generalization bound for UGDA, we identify two data-centric principles for UGDA: alignment principle and rescaling principle. Guided by these principles, we propose GraphAlign, a novel UGDA method that generates a small yet transferable graph. By exclusively training a GNN on this new graph with classic Empirical Risk Minimization (ERM), GraphAlign attains exceptional performance on the target graph. Extensive experiments under various transfer scenarios demonstrate the GraphAlign outperforms the best baselines by an average of 2.16%, training on the generated graph as small as 0.25~1% of the original training graph."}, "https://arxiv.org/abs/2407.19429": {"title": "FTF-ER: Feature-Topology Fusion-Based Experience Replay Method for Continual Graph Learning", "link": "https://arxiv.org/abs/2407.19429", "description": "arXiv:2407.19429v1 Announce Type: cross \nAbstract: Continual graph learning (CGL) is an important and challenging task that aims to extend static GNNs to dynamic task flow scenarios. As one of the mainstream CGL methods, the experience replay (ER) method receives widespread attention due to its superior performance. However, existing ER methods focus on identifying samples by feature significance or topological relevance, which limits their utilization of comprehensive graph data. In addition, the topology-based ER methods only consider local topological information and add neighboring nodes to the buffer, which ignores the global topological information and increases memory overhead. To bridge these gaps, we propose a novel method called Feature-Topology Fusion-based Experience Replay (FTF-ER) to effectively mitigate the catastrophic forgetting issue with enhanced efficiency. Specifically, from an overall perspective to maximize the utilization of the entire graph data, we propose a highly complementary approach including both feature and global topological information, which can significantly improve the effectiveness of the sampled nodes. Moreover, to further utilize global topological information, we propose Hodge Potential Score (HPS) as a novel module to calculate the topological importance of nodes. HPS derives a global node ranking via Hodge decomposition on graphs, providing more accurate global topological information compared to neighbor sampling. By excluding neighbor sampling, HPS significantly reduces buffer storage costs for acquiring topological information and simultaneously decreases training time. Compared with state-of-the-art methods, FTF-ER achieves a significant improvement of 3.6% in AA and 7.1% in AF on the OGB-Arxiv dataset, demonstrating its superior performance in the class-incremental learning setting."}, "https://arxiv.org/abs/2407.19749": {"title": "Mitigating Farmland Biodiversity Loss: A Bio-Economic Model of Land Consolidation and Pesticide Use", "link": "https://arxiv.org/abs/2407.19749", "description": "arXiv:2407.19749v1 Announce Type: cross \nAbstract: Biodiversity loss driven by agricultural intensification is a pressing global issue, with significant implications for ecosystem stability and human well-being. We design an integrated bio-economic agent-based model, informed by historical data from the French agricultural sector, to project future biodiversity trends and evaluate policy interventions. Our model predicts further biodiversity decline under a business-as-usual scenario, primarily due to intensified land consolidation. We evaluate two policy options: reducing pesticide use and subsidizing small farmers. While pesticide reduction rapidly benefits biodiversity in the beginning, it eventually leads to increased land consolidation and further biodiversity loss. In contrast, subsidizing small farmers by reallocating a small fraction of existing subsidies, stabilizes farm sizes and enhances biodiversity in the long run. The most effective strategy results from combining both policies, leveraging pesticide reduction alongside targeted subsidies to balance economic pressures and consistently improve biodiversity."}, "https://arxiv.org/abs/2303.07563": {"title": "Bounded-Confidence Models of Opinion Dynamics with Adaptive Confidence Bounds", "link": "https://arxiv.org/abs/2303.07563", "description": "arXiv:2303.07563v3 Announce Type: replace \nAbstract: People's opinions change with time as they interact with each other. In a bounded-confidence model (BCM) of opinion dynamics, individuals (which are represented by the nodes of a network) have continuous-valued opinions and are influenced by neighboring nodes whose opinions are sufficiently similar to theirs (i.e., are within a confidence bound). In this paper, we formulate and analyze discrete-time BCMs with heterogeneous and adaptive confidence bounds. We introduce two new models: (1) a BCM with synchronous opinion updates that generalizes the Hegselmann--Krause (HK) model and (2) a BCM with asynchronous opinion updates that generalizes the Deffuant--Weisbuch (DW) model. We analytically and numerically explore our adaptive BCMs' limiting behaviors, including the confidence-bound dynamics, the formation of clusters of nodes with similar opinions, and the time evolution of an \"effective graph\", which is a time-dependent subgraph of a network with edges between nodes that {are currently receptive to each other.} For a variety of networks and a wide range of values of the parameters that control the increase and decrease of confidence bounds, we demonstrate numerically that our adaptive BCMs result in fewer major opinion clusters and longer convergence times than the baseline (i.e., nonadaptive) BCMs. We also show that our adaptive BCMs can have adjacent nodes that converge to the same opinion but are not {receptive to each other.} This qualitative behavior does not occur in the associated baseline BCMs."}, "https://arxiv.org/abs/2310.15772": {"title": "Analyzing User Characteristics of Hate Speech Spreaders on Social Media", "link": "https://arxiv.org/abs/2310.15772", "description": "arXiv:2310.15772v2 Announce Type: replace \nAbstract: Hate speech on social media threatens the mental and physical well-being of individuals and contributes to real-world violence. Resharing is an important driver behind the spread of hate speech on social media. Yet, little is known about who reshares hate speech and what their characteristics are. In this paper, we analyze the role of user characteristics in hate speech resharing across different types of hate speech (e.g., political hate). For this, we proceed as follows: First, we cluster hate speech posts using large language models to identify different types of hate speech. Then we model the effects of user attributes on users' probability to reshare hate speech using an explainable machine learning model. To do so, we apply debiasing to control for selection bias in our observational social media data and further control for the latent vulnerability of users to hate speech. We find that, all else equal, users with fewer followers, fewer friends, fewer posts, and older accounts share more hate speech. This shows that users with little social influence tend to share more hate speech. Further, we find substantial heterogeneity across different types of hate speech. For example, racist and misogynistic hate is spread mostly by users with little social influence. In contrast, political anti-Trump and anti-right-wing hate is reshared by users with larger social influence. Overall, understanding the factors that drive users to share hate speech is crucial for detecting individuals at risk of engaging in harmful behavior and for designing effective mitigation strategies."}, "https://arxiv.org/abs/2311.17709": {"title": "Transmission grid stability with large interregional power flows", "link": "https://arxiv.org/abs/2311.17709", "description": "arXiv:2311.17709v2 Announce Type: replace \nAbstract: We propose a general methodology for identifying critical lines in the long-distance transmission of power across large electric grids. When the system is pushed to its operational limit, for instance by large power imbalances such as those generated by high penetration of variable renewable energy sources, the network gets destabilized and loses synchrony. We investigate a model of the synchronous AC grid of continental Europe under tunable large interregional power flows. When those flows exceed some critical value, we find that instabilities emerge due to topological constraints. We identify two different scenarios triggering these instabilities. In the first one, specific sets of lines reach their maximal load simultaneously, causing the grid to split into two desynchronized zones. In the second one, one or few lines become overloaded, which eventually renders one Lyapunov exponent positive. The first scenario is obviously less generic and we develop a numerical approach to force the splitting of the AC grid into disconnected areas. Remarkably, the critical lines identified in this way match those that triggered the separation of the synchronous grid of continental Europe in two instances in 2021. We further discuss how the modes of the system provide information on which areas are more susceptible to lose synchrony with each other."}, "https://arxiv.org/abs/2402.15988": {"title": "Towards Fair Graph Anomaly Detection: Problem, Benchmark Datasets, and Evaluation", "link": "https://arxiv.org/abs/2402.15988", "description": "arXiv:2402.15988v2 Announce Type: replace \nAbstract: The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect anomalous nodes in an input graph while avoiding biased predictions against individuals from sensitive subgroups. However, the current literature does not comprehensively discuss this problem, nor does it provide realistic datasets that encompass actual graph structures, anomaly labels, and sensitive attributes. To bridge this gap, we introduce a formal definition of the FairGAD problem and present two novel datasets constructed from the social media platforms Reddit and Twitter. These datasets comprise 1.2 million and 400,000 edges associated with 9,000 and 47,000 nodes, respectively, and leverage political leanings as sensitive attributes and misinformation spreaders as anomaly labels. We demonstrate that our FairGAD datasets significantly differ from the synthetic datasets used by the research community. Using our datasets, we investigate the performance-fairness trade-off in nine existing GAD and non-graph AD methods on five state-of-the-art fairness methods. Our code and datasets are available at https://github.com/nigelnnk/FairGAD"}, "https://arxiv.org/abs/2402.18203": {"title": "Exploring the space of graphs with fixed discrete curvatures", "link": "https://arxiv.org/abs/2402.18203", "description": "arXiv:2402.18203v2 Announce Type: replace \nAbstract: Discrete curvatures are quantities associated to the nodes and edges of a graph that reflect the local geometry around them. These curvatures have a rich mathematical theory and they have recently found success as a tool to analyze networks across a wide range of domains. In this work, we consider the problem of constructing graphs with a prescribed set of discrete edge curvatures, and explore the space of such graphs. We address this problem in two ways: first, we develop an evolutionary algorithm to sample graphs with discrete curvatures close to a given set. We use this algorithm to explore how other network statistics vary when constrained by the discrete curvatures in the network. Second, we solve the exact reconstruction problem for the specific case of Forman-Ricci curvature. By leveraging the theory of Markov bases, we obtain a finite set of rewiring moves that connects the space of all graphs with a fixed discrete curvature."}, "https://arxiv.org/abs/2403.08535": {"title": "Robustness of Random Networks with Selective Reinforcement against Attacks", "link": "https://arxiv.org/abs/2403.08535", "description": "arXiv:2403.08535v2 Announce Type: replace \nAbstract: We investigate the robustness of random networks reinforced by adding hidden edges against targeted attacks. This study focuses on two types of reinforcement: uniform reinforcement, where edges are randomly added to all nodes, and selective reinforcement, where edges are randomly added only to the minimum degree nodes of the given network. We use generating functions to derive the giant component size and the critical threshold for the targeted attacks on reinforced networks. Applying our analysis and Monte Carlo simulations to the targeted attacks on scale-free networks, it becomes clear that selective reinforcement significantly improves the robustness of networks against the targeted attacks."}, "https://arxiv.org/abs/2403.19333": {"title": "Three-dimensional shape and connectivity of physical networks", "link": "https://arxiv.org/abs/2403.19333", "description": "arXiv:2403.19333v2 Announce Type: replace \nAbstract: Data describing the three-dimensional structure of physical networks is increasingly available, leading to a surge of interest in network science to explore the relationship between the shape and connectivity of physical networks. We contribute to this effort by standardizing and analyzing 15 data sets from different domains. Each network is made of tube-like objects bound together at junction points, which we treat as nodes, with the connections between them considered as links. We divide these networks into three categories: lattice-like networks, trees, and linked trees. The degree distribution of these physical networks is bounded, with most nodes having degrees one or three. Characterizing the physical properties of links, we show that links have an elongated shape and tend to follow a nearly straight trajectory, while a small fraction of links follow a winding path. These typical node and link properties must be reflected by physical network models. We also measure how confined a link is in space by comparing its trajectory to a randomized null model, showing that links that are central in the abstract network tend to be physically confined by their neighbors. The fact that the shape and connectivity of the physical networks are intertwined highlights that their three-dimensional layout must be taken into account to understand the evolution and function of physical networks."}, "https://arxiv.org/abs/2404.10148": {"title": "Node Similarities under Random Projections: Limits and Pathological Cases", "link": "https://arxiv.org/abs/2404.10148", "description": "arXiv:2404.10148v2 Announce Type: replace \nAbstract: Random Projections have been widely used to generate embeddings for various graph learning tasks due to their computational efficiency. The majority of applications have been justified through the Johnson-Lindenstrauss Lemma. In this paper, we take a step further and investigate how well dot product and cosine similarity are preserved by random projections when these are applied over the rows of the graph matrix. Our analysis provides new asymptotic and finite-sample results, identifies pathological cases, and tests them with numerical experiments. We specialize our fundamental results to a ranking application by computing the probability of random projections flipping the node ordering induced by their embeddings. We find that, depending on the degree distribution, the method produces especially unreliable embeddings for the dot product, regardless of whether the adjacency or the normalized transition matrix is used. With respect to the statistical noise introduced by random projections, we show that cosine similarity produces remarkably more precise approximations."}, "https://arxiv.org/abs/2308.10862": {"title": "Mapping Election Polarization and Competitiveness using Election Results", "link": "https://arxiv.org/abs/2308.10862", "description": "arXiv:2308.10862v2 Announce Type: replace-cross \nAbstract: The simplified hypothesis that an election is polarized as an explanation of recent electoral outcomes worldwide is centered on perceptions of voting patterns rather than ideological data from the electorate. While the literature focuses on measuring polarization using ideological-like data from electoral studies-which are limited to economically advantageous countries and are representative mostly to national scales-we argue that, in fact, voting patterns can lead to mapping effective proxies of citizen divisions on election day. This paper perspectives two complementary concepts, Election Polarization (EP) and Election Competitiveness (EC), as a means to understand voting patterns on Election Day. We present an agnostic approach that relies solely on election data and validate it using synthetic and real-world election data across 13 countries in the Eurozone, North America, Latin America, and New Zealand. Overall, we find that we can label and distinguish expectations of polarized and competitive elections in these countries, and we report that EP positively correlates with a metric of political polarization in the U.S., unlocking opportunities for studies of polarization at the regional level and for lower/middle-income countries where electoral studies are available, but surveys are limited."}, "https://arxiv.org/abs/2401.00060": {"title": "Assessing your Observatory's Impact: Best Practices in Establishing and Maintaining Observatory Bibliographies", "link": "https://arxiv.org/abs/2401.00060", "description": "arXiv:2401.00060v2 Announce Type: replace-cross \nAbstract: Observatories need to measure and evaluate the scientific output and overall impact of their facilities. An observatory bibliography consists of the papers published using that observatory's data, typically gathered by searching the major journals for relevant keywords. Recently, the volume of literature and methods by which the publications pool is evaluated has increased. Efficient and standardized procedures are necessary to assign meaningful metadata; enable user-friendly retrieval; and provide the opportunity to derive reports, statistics, and visualizations to impart a deeper understanding of the research output. In 2021, a group of observatory bibliographers from around the world convened online to continue the discussions presented in Lagerstrom (2015). We worked to extract general guidelines from our experiences, techniques, and lessons learnt. The paper explores the development, application, and current status of telescope bibliographies and future trends. This paper briefly describes the methodologies employed in constructing databases, along with the various bibliometric techniques used to analyze and interpret them. We explain reasons for non-standardization and why it is essential for each observatory to identify metadata and metrics that are meaningful for them; caution the (over-)use of comparisons among facilities that are, ultimately, not comparable through bibliometrics; and highlight the benefits of telescope bibliographies, both for researchers within the astronomical community and for stakeholders beyond the specific observatories. There is tremendous diversity in the ways bibliographers track publications and maintain databases, due to parameters such as resources, type of observatory, historical practices, and reporting requirements to funders and outside agencies. However, there are also common sets of Best Practices."}, "https://arxiv.org/abs/2403.16303": {"title": "Large Language Models in Biomedical and Health Informatics: A Review with Bibliometric Analysis", "link": "https://arxiv.org/abs/2403.16303", "description": "arXiv:2403.16303v4 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have rapidly become important tools in Biomedical and Health Informatics (BHI), enabling new ways to analyze data, treat patients, and conduct research. This study aims to provide a comprehensive overview of LLM applications in BHI, highlighting their transformative potential and addressing the associated ethical and practical challenges. We reviewed 1,698 research articles from January 2022 to December 2023, categorizing them by research themes and diagnostic categories. Additionally, we conducted network analysis to map scholarly collaborations and research dynamics. Our findings reveal a substantial increase in the potential applications of LLMs to a variety of BHI tasks, including clinical decision support, patient interaction, and medical document analysis. Notably, LLMs are expected to be instrumental in enhancing the accuracy of diagnostic tools and patient care protocols. The network analysis highlights dense and dynamically evolving collaborations across institutions, underscoring the interdisciplinary nature of LLM research in BHI. A significant trend was the application of LLMs in managing specific disease categories such as mental health and neurological disorders, demonstrating their potential to influence personalized medicine and public health strategies. LLMs hold promising potential to further transform biomedical research and healthcare delivery. While promising, the ethical implications and challenges of model validation call for rigorous scrutiny to optimize their benefits in clinical settings. This survey serves as a resource for stakeholders in healthcare, including researchers, clinicians, and policymakers, to understand the current state and future potential of LLMs in BHI."}, "https://arxiv.org/abs/2405.03724": {"title": "GraphSL: An Open-Source Library for Graph Source Localization Approaches and Benchmark Datasets", "link": "https://arxiv.org/abs/2405.03724", "description": "arXiv:2405.03724v2 Announce Type: replace-cross \nAbstract: We introduce GraphSL, a new library for studying the graph source localization problem. graph diffusion and graph source localization are inverse problems in nature: graph diffusion predicts information diffusions from information sources, while graph source localization predicts information sources from information diffusions. GraphSL facilitates the exploration of various graph diffusion models for simulating information diffusions and enables the evaluation of cutting-edge source localization approaches on established benchmark datasets. The source code of GraphSL is made available at Github Repository (https://github.com/xianggebenben/GraphSL). Bug reports and feedback can be directed to the Github issues page (https://github.com/xianggebenben/GraphSL/issues)."}, "https://arxiv.org/abs/2407.20350": {"title": "Socioeconomic determinants of protective behaviors and contact patterns in the post-COVID-19 pandemic era: a cross-sectional study in Italy", "link": "https://arxiv.org/abs/2407.20350", "description": "arXiv:2407.20350v1 Announce Type: new \nAbstract: Socioeconomic inequalities significantly influence infectious disease outcomes, as seen with COVID-19, but the pathways through which socioeconomic conditions affect transmission dynamics remain unclear. To address this, we conducted a survey representative of the Italian population, stratified by age, gender, geographical area, city size, employment status, and education level. The survey's final aim was to estimate differences in contact and protective behaviors across various population strata, both being key components of transmission dynamics. Our initial insights based on the survey indicate that years after the pandemic began, the perceived impact of COVID-19 on professional, economic, social, and psychological dimensions varied across socioeconomic strata, extending beyond the heterogeneity observed in the epidemiological outcomes of the pandemic. This reinforces the need for approaches that systematically consider socioeconomic determinants. In this context, using generalized models, we identified associations between socioeconomic factors and vaccination status for both COVID-19 and influenza, as well as the influence of socioeconomic conditions on mask-wearing and social distancing. Importantly, we also observed differences in contact behaviors based on employment status while education level did not show a significant association. These findings highlight the complex interplay of socioeconomic and demographic factors in shaping individual responses to public health measures. Understanding these dynamics is essential for developing effective epidemic models and targeted public health strategies, particularly for vulnerable populations."}, "https://arxiv.org/abs/2407.20451": {"title": "Opinion response functions are key to understanding tipping of social conventions", "link": "https://arxiv.org/abs/2407.20451", "description": "arXiv:2407.20451v1 Announce Type: new \nAbstract: The extent to which committed minorities can overturn social conventions is an active area of research in the mathematical modelling of opinion dynamics. Researchers generally use simulations of agent-based models (ABMs) to compute approximate values for the minimum committed minority size needed to overturn a social convention. In this manuscript, we expand on previous work by studying an ABM's mean-field behaviour using ordinary differential equation (ODE) models and a new tool, opinion response functions. Using these methods allows for formal analysis of the deterministic model which can provide a theoretical explanation for observed behaviours, e.g., coexistence or overturning of opinions. In particular, opinion response functions are a method of characterizing the equilibria in our social model. Our analysis confirms earlier numerical results and supplements them with a precise formula for computing the minimum committed minority size required to overturn a social convention."}, "https://arxiv.org/abs/2407.20478": {"title": "Hidden high-risky states identification from routine urban traffic", "link": "https://arxiv.org/abs/2407.20478", "description": "arXiv:2407.20478v1 Announce Type: new \nAbstract: One of the core risk management tasks is to identify hidden high-risky states that may lead to system breakdown, which can provide valuable early warning knowledge. However, due to high dimensionality and nonlinear interaction embedded in large-scale complex systems like urban traffic, it remains challenging to identify hidden high-risky states from huge system state space where over 99% of possible system states are not yet visited in empirical data. Based on maximum entropy model, we infer the underlying interaction network from complicated dynamical processes of urban traffic, and construct system energy landscape. In this way, we can locate hidden high-risky states that have never been observed from real data. These states can serve as risk signals with high probability of entering hazardous minima in energy landscape, which lead to huge recovery cost. Our finding might provide insights for complex system risk management."}, "https://arxiv.org/abs/2407.20770": {"title": "Non-Bayesian Social Learning with Multiview Observations", "link": "https://arxiv.org/abs/2407.20770", "description": "arXiv:2407.20770v1 Announce Type: new \nAbstract: Non-Bayesian social learning enables multiple agents to conduct networked signal and information processing through observing environmental signals and information aggregating. Traditional non-Bayesian social learning models only consider single signals, limiting their applications in scenarios where multiple viewpoints of information are available. In this work, we exploit, in the information aggregation step, the independently learned results from observations taken from multiple viewpoints and propose a novel non-Bayesian social learning model for scenarios with multiview observations. We prove the convergence of the model under traditional assumptions and provide convergence conditions for the algorithm in the presence of misleading signals. Through theoretical analyses and numerical experiments, we validate the strong reliability and robustness of the proposed algorithm, showcasing its potential for real-world applications."}, "https://arxiv.org/abs/2407.20794": {"title": "Data-driven physics-based modeling of pedestrian dynamics", "link": "https://arxiv.org/abs/2407.20794", "description": "arXiv:2407.20794v1 Announce Type: new \nAbstract: Pedestrian crowds encompass a complex interplay of intentional movements aimed at reaching specific destinations, fluctuations due to personal and interpersonal variability, and interactions with each other and the environment. Previous work showed the effectiveness of Langevin-like equations in capturing the statistical properties of pedestrian dynamics in simple settings, such as almost straight trajectories. However, modeling more complex dynamics, e.g. when multiple routes and origin-destinations are involved, remains a significant challenge. In this work, we introduce a novel and generic framework to describe the dynamics of pedestrians in any geometric setting, significantly extending previous works. Our model is based on Langevin dynamics with two timescales. The fast timescale corresponds to the stochastic fluctuations present when a pedestrian is walking. The slow timescale is associated with the dynamics that a pedestrian plans to follow, thus a smoother path. Employing a data-driven approach inspired by statistical field theories, we learn the complex potentials directly from the data, namely a high-statistics database of real-life pedestrian trajectories. This approach makes the model generic as the potentials can be read from any trajectory data set and the underlying Langevin structure enables physics-based insights. We validate our model through a comprehensive statistical analysis, comparing simulated trajectories with actual pedestrian measurements across five complementary settings, including a real-life train platform scenario, underscoring its practical societal relevance. We show that our model effectively captures fluctuation statistics in pedestrian motion. Beyond providing fundamental insights and predictive capabilities in pedestrian dynamics, our model could be used to investigate generic active dynamics such as vehicular traffic and collective animal behavior."}, "https://arxiv.org/abs/2407.20380": {"title": "Inferring financial stock returns correlation from complex network analysis", "link": "https://arxiv.org/abs/2407.20380", "description": "arXiv:2407.20380v1 Announce Type: cross \nAbstract: Financial stock returns correlations have been studied in the prism of random matrix theory, to distinguish the signal from the \"noise\". Eigenvalues of the matrix that are above the rescaled Marchenko Pastur distribution can be interpreted as collective modes behavior while the modes under are usually considered as noise. In this analysis we use complex network analysis to simulate the \"noise\" and the \"market\" component of the return correlations, by introducing some meaningful correlations in simulated geometric Brownian motion for the stocks. We find that the returns correlation matrix is dominated by stocks with high eigenvector centrality and clustering found in the network. We then use simulated \"market\" random walks to build an optimal portfolio and find that the overall return performs better than using the historical mean-variance data, up to 50% on short time scale."}, "https://arxiv.org/abs/2407.20711": {"title": "Information index augmented eRG to model vaccination behaviour: A case study of COVID-19 in the US", "link": "https://arxiv.org/abs/2407.20711", "description": "arXiv:2407.20711v1 Announce Type: cross \nAbstract: Recent pandemics triggered the development of a number of mathematical models and computational tools apt at curbing the socio-economic impact of these and future pandemics. The need to acquire solid estimates from the data led to the introduction of effective approaches such as the \\emph{epidemiological Renormalization Group} (eRG). A recognized relevant factor impacting the evolution of pandemics is the feedback stemming from individuals' choices. The latter can be taken into account via the \\textit{information index} which accommodates the information--induced perception regarding the status of the disease and the memory of past spread. We, therefore, show how to augment the eRG by means of the information index. We first develop the {\\it behavioural} version of the eRG and then test it against the US vaccination campaign for COVID-19. We find that the behavioural augmented eRG improves the description of the pandemic dynamics of the US divisions for which the epidemic peak occurs after the start of the vaccination campaign. Our results strengthen the relevance of taking into account the human behaviour component when modelling pandemic evolution. To inform public health policies, the model can be readily employed to investigate the socio-epidemiological dynamics, including vaccination campaigns, for other regions of the world."}, "https://arxiv.org/abs/2309.10628": {"title": "Symmetric conformity functions make decision-making processes independent of the distribution of learning strategies", "link": "https://arxiv.org/abs/2309.10628", "description": "arXiv:2309.10628v2 Announce Type: replace \nAbstract: Two main procedures characterize the way in which social actors evaluate the qualities of the options in decision-making processes: they either seek to evaluate their intrinsic qualities (individual learners), or they rely on the opinion of the others (social learners). For the latter, social experiments have suggested that the mathematical form of the probability of adopting an option, called the conformity function, is symmetric in the adoption rate. However, the literature on decision-making includes models where social learners employ either symmetric or nonsymmetric conformity functions. We generalize a particular case studied in a previous work, and we show analytically that if the conformity function is symmetric, the details of the probability distribution of the propensity of the agents to behave as a social or an individual learner do not matter, only its expected value influences the determination of the steady state. We also show that in this case, the same steady state is reached for two extreme dynamical processes: one that considers propensities as idiosyncratic properties of the agents (each agent being an individual learner always with the same probability), and the opposite one, which allows them to change their propensity during the dynamics. This is not the case if the conformity function is nonsymmetric. This fact can inspire experiments that could shed light on the debate about mathematical properties of conformity functions."}, "https://arxiv.org/abs/2402.06753": {"title": "Shortest-path percolation on random networks", "link": "https://arxiv.org/abs/2402.06753", "description": "arXiv:2402.06753v2 Announce Type: replace \nAbstract: We propose a bond-percolation model intended to describe the consumption, and eventual exhaustion, of resources in transport networks. Edges forming minimum-length paths connecting demanded origin-destination nodes are removed if below a certain budget. As pairs of nodes are demanded and edges are removed, the macroscopic connected component of the graph disappears, i.e., the graph undergoes a percolation transition. Here, we study such a shortest-path-percolation transition in homogeneous random graphs where pairs of demanded origin-destination nodes are randomly generated, and fully characterize it by means of finite-size scaling analysis. If budget is finite, the transition is identical to the one of ordinary percolation, where a single giant cluster shrinks as edges are removed from the graph; for infinite budget, the transition becomes more abrupt than the one of ordinary percolation, being characterized by the sudden fragmentation of the giant connected component into a multitude of clusters of similar size."}, "https://arxiv.org/abs/2402.11351": {"title": "Modeling the amplification of epidemic spread by misinformed populations", "link": "https://arxiv.org/abs/2402.11351", "description": "arXiv:2402.11351v3 Announce Type: replace \nAbstract: Understanding how misinformation affects the spread of disease is crucial for public health, especially given recent research indicating that misinformation can increase vaccine hesitancy and discourage vaccine uptake. However, it is difficult to investigate the interaction between misinformation and epidemic outcomes due to the dearth of data-informed holistic epidemic models. Here, we employ an epidemic model that incorporates a large, mobility-informed physical contact network as well as the distribution of misinformed individuals across counties derived from social media data. The model allows us to simulate and estimate various scenarios to understand the impact of misinformation on epidemic spreading. Using this model, we present a worst-case scenario in which a heavily misinformed population would result in an additional 14% of the U.S. population becoming infected over the course of the COVID-19 epidemic, compared to a best-case scenario."}, "https://arxiv.org/abs/2404.02689": {"title": "Social clustering reinforces external influence on the majority opinion model", "link": "https://arxiv.org/abs/2404.02689", "description": "arXiv:2404.02689v2 Announce Type: replace \nAbstract: Public opinion is subject to peer interaction via social networks and external pressure from the media, advertising, and other actors. In this paper, we study the interaction between external and peer influence on the stochastic opinion dynamics of a majority vote model. We introduce a model where agents update their opinions based on the combined influence of their local neighbourhood (peers) and an external actor in the transition rates. In the first model, the external influence is only felt by agents non-aligned with the external actor (\"push strategy\"). In the second model, agents are affected by external influence, independently of their opinions (\"nudging strategy\"). In both cases, the external influence increases the possible macroscopic outcomes. These outcomes are determined by the chosen influence strategy. We also find that the social network structure affects the opinion dynamics, with social clustering positively reinforcing the external influence whereas degree heterogeneity weakens the external forces. These findings are relevant to businesses and policy making, helping to understand how groups of individuals collectively react to external actors."}, "https://arxiv.org/abs/2405.01510": {"title": "Reverse Influential Community Search Over Social Networks (Technical Report)", "link": "https://arxiv.org/abs/2405.01510", "description": "arXiv:2405.01510v4 Announce Type: replace \nAbstract: As an important fundamental task of numerous real-world applications such as social network analysis and online advertising/marketing, several prior works studied influential community search, which retrieves a community with high structural cohesiveness and maximum influences on other users in social networks. However, previous works usually considered the influences of the community on arbitrary users in social networks, rather than specific groups (e.g., customer groups, or senior communities). Inspired by this, we propose a novel Top-M Reverse Influential Community Search (TopM-RICS) problem, which obtains a seed community with the maximum influence on a user-specified target community, satisfying both structural and keyword constraints. To efficiently tackle the TopM-RICS problem, we design effective pruning strategies to filter out false alarms of candidate seed communities, and propose an effective index mechanism to facilitate the community retrieval. We also formulate and tackle a TopM-RICS variant, named Top-M Relaxed Reverse Influential Community Search} (TopM-R2ICS), which returns top-M subgraphs with relaxed structural constraints and having the maximum influence on a user-specified target community. Comprehensive experiments have been conducted to verify the efficiency and effectiveness of our TopM-RICS and TopM-R2ICS approaches on both real-world and synthetic social networks under various parameter settings."}, "https://arxiv.org/abs/2407.21063": {"title": "Network and Sentiment Analysis of Enron Emails", "link": "https://arxiv.org/abs/2407.21063", "description": "arXiv:2407.21063v1 Announce Type: new \nAbstract: The objective of the research was to analyze e-mails exchanged at Enron, a power company that declared bankruptcy in 2001 following an investigation into unethical operations regarding their financials. Like other researchers, we identify the most important employees and detect communities using network science methods. We find that the importance of a person depends on the centrality measure used; while the communities we detected resembled the formal organizational structure of the company. In addition, because previous work required that 10 e-mails be sent and received for an e-mail relationship to exist, we analyzed the effect of different thresholds on the results and found that results were very dependent on the threshold used. We also performed sentiment analyses on the e-mails to evaluate whether sentiment changed over time and found that the sentiments of the e-mails do not give insight into the financial wellbeing of Enron. Our results provide insight into how information flowed through Enron, who the key employees were, and e-mail sentiment before and after the crisis"}, "https://arxiv.org/abs/2407.21067": {"title": "Socio-cognitive Networks between Researchers", "link": "https://arxiv.org/abs/2407.21067", "description": "arXiv:2407.21067v1 Announce Type: new \nAbstract: Understanding why researchers cite each other has been a longstanding conjecture in studying scientific networks. Prior research suggests relevance, group cohesion, or honest source crediting as possible factors. However, the dual nature of cognitive and social dimensions underlying citation is often overlooked by not considering the intermediary steps leading up to a citation. For one work to be cited by another, it must first be published by a set of authors. Therefore, we investigate the reasons behind researchers' citations, explicitly examining the interplay of socio-cognitive ties through the interdependence of coauthorship and citation networks. We assess our claims in an empirical analysis by employing the Author-Oriented Relational HyperEvent Model (AuthRHEM) to study Chilean astronomers' citation and collaboration behavior between 2013 and 2015 in a joint framework. We find evidence that when deciding which work to cite, authors prefer other work with novelty and cognitive ties, such as work-to-work relations. At the same time, coherent groups are relevant because coauthors are cocited more frequently in subsequent publications."}, "https://arxiv.org/abs/2407.21537": {"title": "Modelling competition for space: Emergent inefficiency and inequality due to spatial self-organization among a group of crowd-avoiding agents", "link": "https://arxiv.org/abs/2407.21537", "description": "arXiv:2407.21537v1 Announce Type: new \nAbstract: Competition for a limited resource is the hallmark of many complex systems, and often, that resource turns out to be the physical space itself. In this work, we study a novel model designed to elucidate the dynamics and emergence in complex adaptive systems in which agents compete for some spatially spread resource. Specifically, in the model, the dynamics result from the agents trying to position themselves in the quest to avoid physical crowding experienced locally. We characterize in detail the dependence of the emergent behavior of the model on the population density of the system and the individual-level agent traits such as the extent of space an agent considers as her neighborhood, the limit of occupation density one tolerates within that neighborhood, and the information accessibility of the agents about neighborhood occupancy. We show that the efficiency with which the agents utilize the physical space shows transitions at two values of densities. The first of these transitions demarcates efficient and inefficient phases of the system, and the second one signifies the density at which the inefficiency is maximum. We show that the variation of inefficiency with respect to the information accessible to the agents shows opposing behavior above and below this second transition density. We also look into the inequality of resource sharing in the model and show that although inefficiency can be a non-monotonic function of information depending upon the parameters of the model, inequality, in general, decreases with information. Our study sheds light on the role of competition, spatial constraints, and agent traits within complex adaptive systems, offering insights into their emergent behaviors."}, "https://arxiv.org/abs/2407.21753": {"title": "Characterizing User Archetypes and Discussions on Scored", "link": "https://arxiv.org/abs/2407.21753", "description": "arXiv:2407.21753v1 Announce Type: new \nAbstract: In recent years, the proliferation of social platforms has drastically transformed the way individuals interact, organize, and share information. In this scenario, we experience an unprecedented increase in the scale and complexity of interactions and, at the same time, little to no research about some fringe social platforms. In this paper, we present a multi-dimensional framework for characterizing nodes and hyperedges in social hypernetworks, with a focus on the understudied alt-right platform Scored.co. Our approach integrates the possibility of studying higher-order interactions, thanks to the hypernetwork representation, and various node features such as user activity, sentiment, and toxicity, with the aim to define distinct user archetypes and understand their roles within the network. Utilizing a comprehensive dataset from Scored.co, we analyze the dynamics of these archetypes over time and explore their interactions and influence within the community. The framework's versatility allows for detailed analysis of both individual user behaviors and broader social structures. Our findings highlight the importance of higher-order interactions in understanding social dynamics, offering new insights into the roles and behaviors that emerge in complex online environments."}, "https://arxiv.org/abs/2407.21041": {"title": "They Look Like Each Other: Case-based Reasoning for Explainable Depression Detection on Twitter using Large Language Models", "link": "https://arxiv.org/abs/2407.21041", "description": "arXiv:2407.21041v1 Announce Type: cross \nAbstract: Depression is a common mental health issue that requires prompt diagnosis and treatment. Despite the promise of social media data for depression detection, the opacity of employed deep learning models hinders interpretability and raises bias concerns. We address this challenge by introducing ProtoDep, a novel, explainable framework for Twitter-based depression detection. ProtoDep leverages prototype learning and the generative power of Large Language Models to provide transparent explanations at three levels: (i) symptom-level explanations for each tweet and user, (ii) case-based explanations comparing the user to similar individuals, and (iii) transparent decision-making through classification weights. Evaluated on five benchmark datasets, ProtoDep achieves near state-of-the-art performance while learning meaningful prototypes. This multi-faceted approach offers significant potential to enhance the reliability and transparency of depression detection on social media, ultimately aiding mental health professionals in delivering more informed care."}, "https://arxiv.org/abs/2407.21056": {"title": "What Matters in Explanations: Towards Explainable Fake Review Detection Focusing on Transformers", "link": "https://arxiv.org/abs/2407.21056", "description": "arXiv:2407.21056v1 Announce Type: cross \nAbstract: Customers' reviews and feedback play crucial role on electronic commerce~(E-commerce) platforms like Amazon, Zalando, and eBay in influencing other customers' purchasing decisions. However, there is a prevailing concern that sellers often post fake or spam reviews to deceive potential customers and manipulate their opinions about a product. Over the past decade, there has been considerable interest in using machine learning (ML) and deep learning (DL) models to identify such fraudulent reviews. Unfortunately, the decisions made by complex ML and DL models - which often function as \\emph{black-boxes} - can be surprising and difficult for general users to comprehend. In this paper, we propose an explainable framework for detecting fake reviews with high precision in identifying fraudulent content with explanations and investigate what information matters most for explaining particular decisions by conducting empirical user evaluation. Initially, we develop fake review detection models using DL and transformer models including XLNet and DistilBERT. We then introduce layer-wise relevance propagation (LRP) technique for generating explanations that can map the contributions of words toward the predicted class. The experimental results on two benchmark fake review detection datasets demonstrate that our predictive models achieve state-of-the-art performance and outperform several existing methods. Furthermore, the empirical user evaluation of the generated explanations concludes which important information needs to be considered in generating explanations in the context of fake review identification."}, "https://arxiv.org/abs/2407.21294": {"title": "Decentralized and Uncoordinated Learning of Stable Matchings: A Game-Theoretic Approach", "link": "https://arxiv.org/abs/2407.21294", "description": "arXiv:2407.21294v1 Announce Type: cross \nAbstract: We consider the problem of learning stable matchings in a fully decentralized and uncoordinated manner. In this problem, there are $n$ men and $n$ women, each having preference over the other side. It is assumed that women know their preferences over men, but men are not aware of their preferences over women, and they only learn them if they propose and successfully get matched to women. A matching is called stable if no man and woman prefer each other over their current matches. When all the preferences are known a priori, the celebrated Deferred-Acceptance algorithm proposed by Gale and Shapley provides a decentralized and uncoordinated algorithm to obtain a stable matching. However, when the preferences are unknown, developing such an algorithm faces major challenges due to a lack of coordination. We achieve this goal by making a connection between stable matchings and learning Nash equilibria (NE) in noncooperative games. First, we provide a complete information game formulation for the stable matching problem with known preferences such that its set of pure NE coincides with the set of stable matchings, while its mixed NE can be rounded in a decentralized manner to a stable matching. Relying on such a game-theoretic formulation, we show that for hierarchical markets, adopting the exponential weight (EXP) learning algorithm for the stable matching game achieves logarithmic regret with polynomial dependence on the number of players, thus answering a question posed in previous literature. Moreover, we show that the same EXP learning algorithm converges locally and exponentially fast to a stable matching in general matching markets. We complement this result by introducing another decentralized and uncoordinated learning algorithm that globally converges to a stable matching with arbitrarily high probability, leveraging the weak acyclicity property of the stable matching game."}, "https://arxiv.org/abs/2407.21592": {"title": "Does the Source of a Warning Matter? Examining the Effectiveness of Veracity Warning Labels Across Warners", "link": "https://arxiv.org/abs/2407.21592", "description": "arXiv:2407.21592v1 Announce Type: cross \nAbstract: In this study, we conducted an online, between-subjects experiment (N = 2,049) to better understand the impact of warning label sources on information trust and sharing intentions. Across four warners (the social media platform, other social media users, Artificial Intelligence (AI), and fact checkers), we found that all significantly decreased trust in false information relative to control, but warnings from AI were modestly more effective. All warners significantly decreased the sharing intentions of false information, except warnings from other social media users. AI was again the most effective. These results were moderated by prior trust in media and the information itself. Most noteworthy, we found that warning labels from AI were significantly more effective than all other warning labels for participants who reported a low trust in news organizations, while warnings from AI were no more effective than any other warning label for participants who reported a high trust in news organizations."}, "https://arxiv.org/abs/1904.05327": {"title": "Endogenous Coalition Formation in Policy Debates", "link": "https://arxiv.org/abs/1904.05327", "description": "arXiv:1904.05327v2 Announce Type: replace \nAbstract: Political actors form coalitions around their joint normative beliefs in order to influence the policy process on contentious issues such as climate change or population ageing. Policy process theory maintains that learning within and across coalitions is a central predictor of coalition formation and policy change but has yet to explain how policy learning works. The present article explains the formation and maintenance of coalitions by focusing on the ways actors adopt policy beliefs from other actors in policy debates. A policy debate is a complex social system in which temporal network dependence guides how actors contribute ideological statements to the debate. Belief adoption matters in three complementary ways: bonding, which exploits cues within coalitions; bridging, which explores new beliefs outside one's perimeter in the debate; and repulsion, which reinforces polarization between coalitions and cements their belief systems. We formalize this theory of endogenous coalition formation in policy debates and test it on a micro-level empirical dataset using statistical network analysis and event history analysis."}, "https://arxiv.org/abs/2309.04727": {"title": "Optimal transport with constraints: from mirror descent to classical mechanics", "link": "https://arxiv.org/abs/2309.04727", "description": "arXiv:2309.04727v2 Announce Type: replace \nAbstract: Finding optimal trajectories for multiple traffic demands in a congested network is a challenging task. Optimal transport theory is a principled approach that has been used successfully to study various transportation problems. Its usage is limited by the lack of principled and flexible ways to incorporate realistic constraints. We propose a principled physics-based approach to impose constraints flexibly in such optimal transport problems. Constraints are included in mirror descent dynamics using the principle of D'Alembert-Lagrange from classical mechanics. This leads to a sparse, local and linear approximation of the feasible set leading in many cases to closed-form updates."}, "https://arxiv.org/abs/2311.11128": {"title": "Evolutionary game selection creates cooperative environments", "link": "https://arxiv.org/abs/2311.11128", "description": "arXiv:2311.11128v2 Announce Type: replace \nAbstract: The emergence of collective cooperation in competitive environments is a well-known phenomenon in biology, economics, and social systems. While most evolutionary game models focus on the evolution of strategies for a fixed game, how strategic decisions coevolve with the environment has so far mostly been overlooked. Here, we consider a game selection model where not only the strategies but also the game can change over time following evolutionary principles. Our results show that coevolutionary dynamics of games and strategies can induce novel collective phenomena, fostering the emergence of cooperative environments. When the model is taken on structured populations the architecture of the interaction network can significantly amplify pro-social behavior, with a critical role played by network heterogeneity and the presence of clustered groups of similar players, distinctive features observed in real-world populations. By unveiling the link between the evolution of strategies and games for different structured populations, our model sheds new light on the origin of social dilemmas ubiquitously observed in real-world social systems."}, "https://arxiv.org/abs/2401.12231": {"title": "Disentangled Condensation for Large-scale Graphs", "link": "https://arxiv.org/abs/2401.12231", "description": "arXiv:2401.12231v2 Announce Type: replace \nAbstract: Graph condensation has emerged as an intriguing technique to save the expensive training costs of Graph Neural Networks (GNNs) by substituting a condensed small graph with the original graph. Despite the promising results achieved, previous methods usually employ an entangled paradigm of redundant parameters (nodes, edges, GNNs), which incurs complex joint optimization during condensation. This paradigm has considerably impeded the scalability of graph condensation, making it challenging to condense extremely large-scale graphs and generate high-fidelity condensed graphs. Therefore, we propose to disentangle the condensation process into a two-stage GNN-free paradigm, independently condensing nodes and generating edges while eliminating the need to optimize GNNs at the same time. The node condensation module avoids the complexity of GNNs by focusing on node feature alignment with anchors of the original graph, while the edge translation module constructs the edges of the condensed nodes by transferring the original structure knowledge with neighborhood anchors. This simple yet effective approach achieves at least 10 times faster than state-of-the-art methods with comparable accuracy on medium-scale graphs. Moreover, the proposed DisCo can successfully scale up to the Ogbn-papers100M graph with flexible reduction rates. Extensive downstream tasks and ablation study on five common datasets further demonstrate the effectiveness of the proposed DisCo framework. The source code will be made publicly available."}, "https://arxiv.org/abs/2402.07964": {"title": "Can smartphone apps reveal fishing catch rates and durations?", "link": "https://arxiv.org/abs/2402.07964", "description": "arXiv:2402.07964v2 Announce Type: replace \nAbstract: Reliable angler behavior data is important for effective fisheries management. Traditionally, such data is gathered through surveys, but an innovative cost-effective approach involves utilizing online platforms and smartphone applications. Previous studies have identified correlations between citizen-reported data from these applications and conventional survey information. However, it remains unclear whether conventional survey data is directly related to citizen-reported data or mainly derived from \"intermediate\" variables. We applied Bayesian networks to data from conventional surveys, the Angler's Atlas website, the MyCatch smartphone application, and environmental data across Alberta and Ontario, Canada, to detect probabilistic dependencies. Using Bayesian model averaging, we measured the strength of connections between variables. In Ontario, aerial boat counts were directly related to waterbody webpage views, with a 51% probability. In Alberta, creel survey-reported catch rate was directly related to citizen-reported catch rate and fishing duration, though with low probabilities (12% and 6%, respectively). Daily fishing durations were indirectly related, with air temperature and solar radiation as intermediates. These findings suggest that citizen reports can complement traditional methods for evaluating angler behavior."}, "https://arxiv.org/abs/2308.16491": {"title": "In-class Data Analysis Replications: Teaching Students while Testing Science", "link": "https://arxiv.org/abs/2308.16491", "description": "arXiv:2308.16491v2 Announce Type: replace-cross \nAbstract: Science is facing a reproducibility crisis. Previous work has proposed incorporating data analysis replications into classrooms as a potential solution. However, despite the potential benefits, it is unclear whether this approach is feasible, and if so, what the involved stakeholders-students, educators, and scientists-should expect from it. Can students perform a data analysis replication over the course of a class? What are the costs and benefits for educators? And how can this solution help benchmark and improve the state of science?\n  In the present study, we incorporated data analysis replications in the project component of the Applied Data Analysis course (CS-401) taught at EPFL (N=354 students). Here we report pre-registered findings based on surveys administered throughout the course. First, we demonstrate that students can replicate previously published scientific papers, most of them qualitatively and some exactly. We find discrepancies between what students expect of data analysis replications and what they experience by doing them along with changes in expectations about reproducibility, which together serve as evidence of attitude shifts to foster students' critical thinking. Second, we provide information for educators about how much overhead is needed to incorporate replications into the classroom and identify concerns that replications bring as compared to more traditional assignments. Third, we identify tangible benefits of the in-class data analysis replications for scientific communities, such as a collection of replication reports and insights about replication barriers in scientific work that should be avoided going forward.\n  Overall, we demonstrate that incorporating replication tasks into a large data science class can increase the reproducibility of scientific work as a by-product of data science instruction, thus benefiting both science and students."}, "https://arxiv.org/abs/2403.16137": {"title": "A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective", "link": "https://arxiv.org/abs/2403.16137", "description": "arXiv:2403.16137v2 Announce Type: replace-cross \nAbstract: Graph self-supervised learning (SSL) is now a go-to method for pre-training graph foundation models (GFMs). There is a wide variety of knowledge patterns embedded in the graph data, such as node properties and clusters, which are crucial to learning generalized representations for GFMs. However, existing surveys of GFMs have several shortcomings: they lack comprehensiveness regarding the most recent progress, have unclear categorization of self-supervised methods, and take a limited architecture-based perspective that is restricted to only certain types of graph models. As the ultimate goal of GFMs is to learn generalized graph knowledge, we provide a comprehensive survey of self-supervised GFMs from a novel knowledge-based perspective. We propose a knowledge-based taxonomy, which categorizes self-supervised graph models by the specific graph knowledge utilized. Our taxonomy consists of microscopic (nodes, links, etc.), mesoscopic (context, clusters, etc.), and macroscopic knowledge (global structure, manifolds, etc.). It covers a total of 9 knowledge categories and more than 25 pretext tasks for pre-training GFMs, as well as various downstream task generalization strategies. Such a knowledge-based taxonomy allows us to re-examine graph models based on new architectures more clearly, such as graph language models, as well as provide more in-depth insights for constructing GFMs."}, "https://arxiv.org/abs/2408.00045": {"title": "Observing network dynamics through sentinel nodes", "link": "https://arxiv.org/abs/2408.00045", "description": "arXiv:2408.00045v1 Announce Type: new \nAbstract: A fundamental premise of statistical physics is that the particles in a physical system are interchangeable, and hence the state of each specific component is representative of the system as a whole. This assumption breaks down for complex networks, in which nodes may be extremely diverse, and no single component can truly represent the state of the entire system. It seems, therefore, that to observe the dynamics of social, biological or technological networks, one must extract the dynamic states of a large number of nodes -- a task that is often practically prohibitive. To overcome this challenge, we use machine learning techniques to detect the network's sentinel nodes, a set of network components whose combined states can help approximate the average dynamics of the entire network. The method allows us to assess the state of a large complex system by tracking just a small number of carefully selected nodes. The resulting sentinel node set offers a natural probe by which to practically observe complex network dynamics."}, "https://arxiv.org/abs/2408.00139": {"title": "Multiway Alignment of Political Attitudes", "link": "https://arxiv.org/abs/2408.00139", "description": "arXiv:2408.00139v1 Announce Type: new \nAbstract: The related concepts of partisan belief systems, issue alignment, and partisan sorting are central to our understanding of politics. These phenomena have been studied using measures of alignment between pairs of topics, or how much individuals' attitudes toward a topic reveal about their attitudes toward another topic. We introduce a higher-order measure that extends the assessment of alignment beyond pairs of topics by quantifying the amount of information individuals' opinions on one topic reveal about a set of topics simultaneously. Our multiway alignment measure indicates how much individuals' opinions on multiple topics align into a single ideological divide. Applying this approach to legislative voting behavior reveals that parliamentary systems typically exhibit similar multiway alignment characteristics, but can change in response to shifting intergroup dynamics. In American National Election Studies surveys, our approach reveals a growing significance of party identification together with a consistent rise in multiway alignment over time. Similarly, the growing multiway alignment among topical issues in Finnish online discussions suggests a trend towards a more ideologically driven political landscape. Our case studies demonstrate that the multiway alignment measure is a versatile tool for understanding societal polarization and partisan belief systems across diverse domains."}, "https://arxiv.org/abs/2408.00148": {"title": "The iterative persuasion-polarization opinion dynamics and its mean-field analysis", "link": "https://arxiv.org/abs/2408.00148", "description": "arXiv:2408.00148v1 Announce Type: new \nAbstract: In this paper, we introduce the Iterative Persuasion-Polarization (IPP) model to study the dynamics of opinion formation and change within a population. The IPP model integrates mechanisms of persuasion and repulsion, where individuals influence each other through interactions that can either align opinions incrementally or lead to greater divergence. The probability of each interaction type is governed by a parameter $\\alpha$, representing the population's receptiveness to persuasion. We investigate how these interaction dynamics shape the long-term distribution of opinions, examining conditions that promote consensus or polarization. By deriving a system of nonlinear and autonomous ordinary differential equations (ODEs), we provide a rigorous mathematical framework for analyzing the distributional behavior of opinions in large populations. Our findings contribute to a deeper understanding of social influence dynamics and their implications in complex social systems."}, "https://arxiv.org/abs/2408.00633": {"title": "DisTrack: a new Tool for Semi-automatic Misinformation Tracking in Online Social Networks", "link": "https://arxiv.org/abs/2408.00633", "description": "arXiv:2408.00633v1 Announce Type: new \nAbstract: Introduction: This article introduces DisTrack, a methodology and a tool developed for tracking and analyzing misinformation within Online Social Networks (OSNs). DisTrack is designed to combat the spread of misinformation through a combination of Natural Language Processing (NLP) Social Network Analysis (SNA) and graph visualization. The primary goal is to detect misinformation, track its propagation, identify its sources, and assess the influence of various actors within the network.\n  Methods: DisTrack's architecture incorporates a variety of methodologies including keyword search, semantic similarity assessments, and graph generation techniques. These methods collectively facilitate the monitoring of misinformation, the categorization of content based on alignment with known false claims, and the visualization of dissemination cascades through detailed graphs. The tool is tailored to capture and analyze the dynamic nature of misinformation spread in digital environments.\n  Results: The effectiveness of DisTrack is demonstrated through three case studies focused on different themes: discredit/hate speech, anti-vaccine misinformation, and false narratives about the Russia-Ukraine conflict. These studies show DisTrack's capabilities in distinguishing posts that propagate falsehoods from those that counteract them, and tracing the evolution of misinformation from its inception.\n  Conclusions: The research confirms that DisTrack is a valuable tool in the field of misinformation analysis. It effectively distinguishes between different types of misinformation and traces their development over time. By providing a comprehensive approach to understanding and combating misinformation in digital spaces, DisTrack proves to be an essential asset for researchers and practitioners working to mitigate the impact of false information in online social environments."}, "https://arxiv.org/abs/2408.00702": {"title": "Future Directions in Human Mobility Science", "link": "https://arxiv.org/abs/2408.00702", "description": "arXiv:2408.00702v1 Announce Type: new \nAbstract: We provide a brief review of human mobility science and present three key areas where we expect to see substantial advancements. We start from the mind and discuss the need to better understand how spatial cognition shapes mobility patterns. We then move to societies and argue the importance of better understanding new forms of transportation. We conclude by discussing how algorithms shape mobility behaviour and provide useful tools for modellers. Finally, we discuss how progress in these research directions may help us address some of the challenges our society faces today."}, "https://arxiv.org/abs/2408.00123": {"title": "Semantic Codebook Learning for Dynamic Recommendation Models", "link": "https://arxiv.org/abs/2408.00123", "description": "arXiv:2408.00123v1 Announce Type: cross \nAbstract: Dynamic sequential recommendation (DSR) can generate model parameters based on user behavior to improve the personalization of sequential recommendation under various user preferences. However, it faces the challenges of large parameter search space and sparse and noisy user-item interactions, which reduces the applicability of the generated model parameters. The Semantic Codebook Learning for Dynamic Recommendation Models (SOLID) framework presents a significant advancement in DSR by effectively tackling these challenges. By transforming item sequences into semantic sequences and employing a dual parameter model, SOLID compresses the parameter generation search space and leverages homogeneity within the recommendation system. The introduction of the semantic metacode and semantic codebook, which stores disentangled item representations, ensures robust and accurate parameter generation. Extensive experiments demonstrates that SOLID consistently outperforms existing DSR, delivering more accurate, stable, and robust recommendations."}, "https://arxiv.org/abs/2408.00312": {"title": "Adversarial Text Rewriting for Text-aware Recommender Systems", "link": "https://arxiv.org/abs/2408.00312", "description": "arXiv:2408.00312v1 Announce Type: cross \nAbstract: Text-aware recommender systems incorporate rich textual features, such as titles and descriptions, to generate item recommendations for users. The use of textual features helps mitigate cold-start problems, and thus, such recommender systems have attracted increased attention. However, we argue that the dependency on item descriptions makes the recommender system vulnerable to manipulation by adversarial sellers on e-commerce platforms. In this paper, we explore the possibility of such manipulation by proposing a new text rewriting framework to attack text-aware recommender systems. We show that the rewriting attack can be exploited by sellers to unfairly uprank their products, even though the adversarially rewritten descriptions are perceived as realistic by human evaluators. Methodologically, we investigate two different variations to carry out text rewriting attacks: (1) two-phase fine-tuning for greater attack performance, and (2) in-context learning for higher text rewriting quality. Experiments spanning 3 different datasets and 4 existing approaches demonstrate that recommender systems exhibit vulnerability against the proposed text rewriting attack. Our work adds to the existing literature around the robustness of recommender systems, while highlighting a new dimension of vulnerability in the age of large-scale automated text generation."}, "https://arxiv.org/abs/2408.00490": {"title": "Graph Representation Learning via Causal Diffusion for Out-of-Distribution Recommendation", "link": "https://arxiv.org/abs/2408.00490", "description": "arXiv:2408.00490v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs)-based recommendation algorithms typically assume that training and testing data are drawn from independent and identically distributed (IID) spaces. However, this assumption often fails in the presence of out-of-distribution (OOD) data, resulting in significant performance degradation. In this study, we construct a Structural Causal Model (SCM) to analyze interaction data, revealing that environmental confounders (e.g., the COVID-19 pandemic) lead to unstable correlations in GNN-based models, thus impairing their generalization to OOD data. To address this issue, we propose a novel approach, graph representation learning via causal diffusion (CausalDiffRec) for OOD recommendation. This method enhances the model's generalization on OOD data by eliminating environmental confounding factors and learning invariant graph representations. Specifically, we use backdoor adjustment and variational inference to infer the real environmental distribution, thereby eliminating the impact of environmental confounders. This inferred distribution is then used as prior knowledge to guide the representation learning in the reverse phase of the diffusion process to learn the invariant representation. In addition, we provide a theoretical derivation that proves optimizing the objective function of CausalDiffRec can encourage the model to learn environment-invariant graph representations, thereby achieving excellent generalization performance in recommendations under distribution shifts. Our extensive experiments validate the effectiveness of CausalDiffRec in improving the generalization of OOD data, and the average improvement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and 11.65% on Douban datasets."}, "https://arxiv.org/abs/2311.11749": {"title": "A causal intervention framework for synthesizing mobility data and evaluating predictive neural networks", "link": "https://arxiv.org/abs/2311.11749", "description": "arXiv:2311.11749v3 Announce Type: replace \nAbstract: Deep neural networks are increasingly utilized in mobility prediction tasks, yet their intricate internal workings pose challenges for interpretability, especially in comprehending how various aspects of mobility behavior affect predictions. This study introduces a causal intervention framework to assess the impact of mobility-related factors on neural networks designed for next location prediction -- a task focusing on predicting the immediate next location of an individual. To achieve this, we employ individual mobility models to synthesize location visit sequences and control behavior dynamics by intervening in their data generation process. We evaluate the interventional location sequences using mobility metrics and input them into well-trained networks to analyze performance variations. The results demonstrate the effectiveness in producing location sequences with distinct mobility behaviors, thereby facilitating the simulation of diverse yet realistic spatial and temporal changes. These changes result in performance fluctuations in next location prediction networks, revealing impacts of critical mobility behavior factors, including sequential patterns in location transitions, proclivity for exploring new locations, and preferences in location choices at population and individual levels. The gained insights hold value for the real-world application of mobility prediction networks, and the framework is expected to promote the use of causal inference to enhance the interpretability and robustness of neural networks in mobility applications."}, "https://arxiv.org/abs/2312.10269": {"title": "The DSA Transparency Database: Auditing Self-reported Moderation Actions by Social Media", "link": "https://arxiv.org/abs/2312.10269", "description": "arXiv:2312.10269v3 Announce Type: replace \nAbstract: Since September 2023, the Digital Services Act (DSA) obliges large online platforms to submit detailed data on each moderation action they take within the European Union (EU) to the DSA Transparency Database. From its inception, this centralized database has sparked scholarly interest as an unprecedented and potentially unique trove of data on real-world online moderation. Here, we thoroughly analyze all 353.12M records submitted by the eight largest social media platforms in the EU during the first 100 days of the database. Specifically, we conduct a platform-wise comparative study of their: volume of moderation actions, grounds for decision, types of applied restrictions, types of moderated content, timeliness in undertaking and submitting moderation actions, and use of automation. Furthermore, we systematically cross-check the contents of the database with the platforms' own transparency reports. Our analyses reveal that (i) the platforms adhered only in part to the philosophy and structure of the database, (ii) the structure of the database is partially inadequate for the platforms' reporting needs, (iii) the platforms exhibited substantial differences in their moderation actions, (iv) a remarkable fraction of the database data is inconsistent, (v) the platform X (formerly Twitter) presents the most inconsistencies. Our findings have far-reaching implications for policymakers and scholars across diverse disciplines. They offer guidance for future regulations that cater to the reporting needs of online platforms in general, but also highlight opportunities to improve and refine the database itself."}, "https://arxiv.org/abs/2306.04407": {"title": "A Mathematical Model for Co-infection Dynamics of Pneumocystis Pneumonia and HIV/AIDS with Treatment", "link": "https://arxiv.org/abs/2306.04407", "description": "arXiv:2306.04407v2 Announce Type: replace-cross \nAbstract: The control of opportunistic infections among HIV infected individuals should be one of the major public health concerns in reducing mortality rate of individuals living with HIV/AIDS. In this study a deterministic co-infection mathematical model is employed to provide a quantification of treatment at each contagious stage against Pneumocystis Pneumonia (PCP) among HIV infected individuals on ART. The disease-free equilibrium for the HIV/AIDS sub model, PCP sub model and the co-infection model are shown to be locally asymptotically stable when their associated disease threshold parameter is less than a unity. By use of suitable Lyapunov functions, the endemic equilibrium corresponding to HIV/AIDS and PCP sub models are globally asymptotically stable whenever $\\mathcal{R}_{0H}>1$ and $\\mathcal{R}_{0P}>1$ respectively. The sensitivity analysis results implicate that the effective contact rates are the main mechanisms fueling the proliferation of the two diseases and on the other hand treatment efforts play an important role in reducing the incidence. Numerical simulations show that treatment of PCP at all contagious stages reduces its burden on HIV/AIDS patients and dual treatment of the co-infected individuals significantly reduces the burden of the co-infection."}, "https://arxiv.org/abs/2408.00901": {"title": "A value-focused thinking approach to measure community resilience", "link": "https://arxiv.org/abs/2408.00901", "description": "arXiv:2408.00901v1 Announce Type: new \nAbstract: Community resilience refers to the ability to prepare for, absorb, recover from, and adapt to disruptive events, but specific definitions and measures for resilience can vary widely from researcher to researcher or from discipline to discipline. Community resilience is often measured using a set of indicators based on census, socioeconomic, and community organizational data, but these metrics and measures for community resilience provide little guidance for policymakers to determine how best to increase the community resilience. This article proposes to measure community resilience based on value focused thinking. We propose an objectives hierarchy that begins with a community decision makers' fundamental objective for resilience. Six high level objectives for community resilience, including social resilience, economic resilience, infrastructure resilience, environmental resilience, availability of resources, and functionality of critical services, are broken down into measurable attributes that focus on specific outcomes that a decision maker would like to achieve if a disruption occurs. This new way of assessing resilience is applied to measure the resilience of an illustrative community to an improvised explosive device, a cyberattack, a tornado, a flood, and a winter storm. Keywords: Community Resilience, Resiliency, Risk Analysis"}, "https://arxiv.org/abs/2408.00905": {"title": "High-Impact Innovations and Hidden Gender Disparities in Inventor-Evaluator Networks", "link": "https://arxiv.org/abs/2408.00905", "description": "arXiv:2408.00905v1 Announce Type: new \nAbstract: We study of millions of scientific, technological, and artistic innovations and find that the innovation gap faced by women is far from universal. No gap exists for conventional innovations. Rather, the gap is pervasively rooted in innovations that combine ideas in unexpected ways - innovations most critical to scientific breakthroughs. Further, at the USPTO we find that female examiners reject up to 33 percent more unconventional innovations by women inventors than do male examiners, suggesting that gender discrimination weakly explains this innovation gap. Instead, new data indicate that a configuration of institutional practices explains the innovation gap. These practices compromise the expertise women examiners need to accurately assess unconventional innovations and then \"over-assign\" women examiners to women innovators, undermining women's innovations. These institutional impediments negatively impact innovation rates in science but have the virtue of being more amenable to actionable policy changes than does culturally ingrained gender discrimination."}, "https://arxiv.org/abs/2408.01157": {"title": "A Note on Computing Betweenness Centrality from the 2-core", "link": "https://arxiv.org/abs/2408.01157", "description": "arXiv:2408.01157v1 Announce Type: new \nAbstract: A central task in network analysis is to identify important nodes in a graph. Betweenness centrality (BC) is a popular centrality measure that captures the significance of nodes based on the number of shortest paths each node intersects with. In this note, we derive a recursive formula to compute the betweenness centralities of a graph from the betweenness centralities of its 2-core.Furthermore, we analyze mathematically the significant impact of removing degree-one nodes on the estimation of betweenness centrality within the context of the popular pivot sampling scheme for Single-Source Shortest Path (SSSP) computations, as described in the Brandes-Pich approach and implemented in widely used software such as NetworkX. We demonstrate both theoretically and empirically that removing degree-1 nodes can reduce the sample complexity needed to achieve better accuracy, thereby decreasing the overall runtime."}, "https://arxiv.org/abs/2408.01172": {"title": "Cascading failures with group support in interdependent hypergraphs", "link": "https://arxiv.org/abs/2408.01172", "description": "arXiv:2408.01172v1 Announce Type: new \nAbstract: The functionality of an entity frequently necessitates the support of a group situated in another layer of the system. To unravel the profound impact of such group support on a system's resilience against cascading failures, we devise a framework comprising a double-layer interdependent hypergraph system, wherein nodes are capable of receiving support via hyperedges. Our central hypothesis posits that the failure may transcend to another layer when all support groups of each dependent node fail, thereby initiating a potentially iterative cascade across layers. Through rigorous analytical methods, we derive the critical threshold for the initial node survival probability that marks the second-order phase transition point. A salient discovery is that as the prevalence of dependent nodes escalates, the system dynamics shift from a second-order to a first-order phase transition. Notably, irrespective of the collapse pattern, systems characterized by scale-free hyperdegree distributions within both hypergraph layers consistently demonstrate superior robustness compared to those adhering to Poisson hyperdegree distributions. In summary, our research underscores the paramount significance of group support mechanisms and intricate network topologies in determining the resilience of interconnected systems against the propagation of cascading failures. By exploring the interplay between these factors, we have gained insights into how systems can be designed or optimized to mitigate the risk of widespread disruptions, ensuring their continued functionality and stability in the face of adverse events."}, "https://arxiv.org/abs/2408.01257": {"title": "Detection and Characterization of Coordinated Online Behavior: A Survey", "link": "https://arxiv.org/abs/2408.01257", "description": "arXiv:2408.01257v1 Announce Type: new \nAbstract: Coordination is a fundamental aspect of life. The advent of social media has made it integral also to online human interactions, such as those that characterize thriving online communities and social movements. At the same time, coordination is also core to effective disinformation, manipulation, and hate campaigns. This survey collects, categorizes, and critically discusses the body of work produced as a result of the growing interest on coordinated online behavior. We reconcile industry and academic definitions, propose a comprehensive framework to study coordinated online behavior, and review and critically discuss the existing detection and characterization methods. Our analysis identifies open challenges and promising directions of research, serving as a guide for scholars, practitioners, and policymakers in understanding and addressing the complexities inherent to online coordination."}, "https://arxiv.org/abs/2408.00422": {"title": "Ginzburg--Landau Functionals in the Large-Graph Limit", "link": "https://arxiv.org/abs/2408.00422", "description": "arXiv:2408.00422v1 Announce Type: cross \nAbstract: Ginzburg--Landau (GL) functionals on graphs, which are relaxations of graph-cut functionals on graphs, have yielded a variety of insights in image segmentation and graph clustering. In this paper, we study large-graph limits of GL functionals by taking a functional-analytic view of graphs as nonlocal kernels. For a graph $W_n$ with $n$ nodes, the corresponding graph GL functional $\\GL^{W_n}_\\ep$ is an energy for functions on $W_n$. We minimize GL functionals on sequences of growing graphs that converge to functions called graphons. For such sequences of graphs, we show that the graph GL functional $\\Gamma$-converges to a continuous and nonlocal functional that we call the \\emph{graphon GL functional}. We also investigate the sharp-interface limits of the graph GL and graphon GL functionals, and we relate these limits to a nonlocal total variation. We express the limiting GL functional in terms of Young measures and thereby obtain a probabilistic interpretation of the variational problem in the large-graph limit. Finally, to develop intuition about the graphon GL functional, we compute the GL minimizer for several example families of graphons."}, "https://arxiv.org/abs/2408.00781": {"title": "Hands-on STEM Learning Experiences using Digital Technologies", "link": "https://arxiv.org/abs/2408.00781", "description": "arXiv:2408.00781v1 Announce Type: cross \nAbstract: The facilitation of STEM education can be enhanced by the provision of opportunities for learners to gain a better understanding of science through the utilization of tangible and visual examples. The objective of this work is to present an account of our experiences and activities carried out in Italian schools with this novel approach. The selection of projects and experiences discussed --in which students develop a range of core competencies such as collaboration, creativity, critical thinking, experimentation, prototyping, communication and problem-solving; include tangible complex 3D printed structures, large micro-controller board replicas and the visualization of wind dynamics and tiny invisible elementary particles among others. These hands-on experiences demonstrate the benefits on the use of digital fabrication technologies implemented within a FabLab for STEM learning."}, "https://arxiv.org/abs/2408.00818": {"title": "Y Social: an LLM-powered Social Media Digital Twin", "link": "https://arxiv.org/abs/2408.00818", "description": "arXiv:2408.00818v1 Announce Type: cross \nAbstract: In this paper we introduce Y, a new-generation digital twin designed to replicate an online social media platform. Digital twins are virtual replicas of physical systems that allow for advanced analyses and experimentation. In the case of social media, a digital twin such as Y provides a powerful tool for researchers to simulate and understand complex online interactions. {\\tt Y} leverages state-of-the-art Large Language Models (LLMs) to replicate sophisticated agent behaviors, enabling accurate simulations of user interactions, content dissemination, and network dynamics. By integrating these aspects, Y offers valuable insights into user engagement, information spread, and the impact of platform policies. Moreover, the integration of LLMs allows Y to generate nuanced textual content and predict user responses, facilitating the study of emergent phenomena in online environments.\n  To better characterize the proposed digital twin, in this paper we describe the rationale behind its implementation, provide examples of the analyses that can be performed on the data it enables to be generated, and discuss its relevance for multidisciplinary research."}, "https://arxiv.org/abs/2408.01268": {"title": "Rumour Spreading Depends on the Latent Geometry and Degree Distribution in Social Network Models", "link": "https://arxiv.org/abs/2408.01268", "description": "arXiv:2408.01268v1 Announce Type: cross \nAbstract: We study push-pull rumour spreading in small-world models for social networks where the degrees follow a power-law. In a non-geometric setting Fountoulakis, Panagiotou and Sauerwald have shown that rumours always spread fast (SODA 2012). On the other hand, Janssen and Mehrabian have found that rumours spread slowly in a spatial preferential attachment model (SIDMA 2017). We study the question systematically for the model of geometric inhomogeneous random graphs (GIRGs), which has been found to be a good theoretical and empirical fit for social networks. Our result is two-fold: with classical Euclidean geometry both slow and fast rumour spreading may occur, depending on the exponent of the power law and the prevalence of weak ties in the networks, and we fully characterise the phase boundaries between those two regimes. Depending on the parameters, fast spreading may either mean polylogarithmic time or even doubly logarithmic time. Secondly, we show that rumour spreading is always fast in a non-metric geometry. The considered non-metric geometry allows to model social connections where resemblance of vertices in a single attribute, such as familial kinship, already strongly indicates the presence of an edge. Classical Euclidean Geometry fails to capture such ties.\n  For some regimes in the Euclidean setting, the efficient pathways for spreading rumours differ from previously identified paths. A vertex of degree $d$ can transmit the rumour efficiently to a vertex of larger degree by a chain of length $3$, where one of the two intermediaries has constant degree, and the other has degree $d^{c}$ for some constant $c<1$."}, "https://arxiv.org/abs/2408.01346": {"title": "Prompt Refinement or Fine-tuning? Best Practices for using LLMs in Computational Social Science Tasks", "link": "https://arxiv.org/abs/2408.01346", "description": "arXiv:2408.01346v1 Announce Type: cross \nAbstract: Large Language Models are expressive tools that enable complex tasks of text understanding within Computational Social Science. Their versatility, while beneficial, poses a barrier for establishing standardized best practices within the field. To bring clarity on the values of different strategies, we present an overview of the performance of modern LLM-based classification methods on a benchmark of 23 social knowledge tasks. Our results point to three best practices: select models with larger vocabulary and pre-training corpora; avoid simple zero-shot in favor of AI-enhanced prompting; fine-tune on task-specific data, and consider more complex forms instruction-tuning on multiple datasets only when only training data is more abundant."}, "https://arxiv.org/abs/2012.13617": {"title": "A New Perspective to Node Influence Evaluation in Complex Network Using Subgraph Tr-Centrality", "link": "https://arxiv.org/abs/2012.13617", "description": "arXiv:2012.13617v2 Announce Type: replace \nAbstract: There is great significance in evaluating a node's Influence ranking in complex networks. Over the years, many researchers have presented different measures for quantifying node interconnectedness within networks. Therefore, this paper introduces a centrality measure called Tr-centrality which focuses on using the node triangle structure and the node neighborhood information to define the strength of a node, which is defined as the summation of Gruebler's Equation of the node's one-hop triangle neighborhood to the number of all the edges in the subgraph. Furthermore, we socially consider it as the local trust of a node. To verify the validity of Tr-centrality [1], we apply it to four real-world networks with different densities and shapes, and Tr-centrality has proven to yield better results."}, "https://arxiv.org/abs/2402.08524": {"title": "A new framework for calibrating COVID-19 SEIR models with spatial-/time-varying coefficients using genetic and sliding window algorithms", "link": "https://arxiv.org/abs/2402.08524", "description": "arXiv:2402.08524v2 Announce Type: replace-cross \nAbstract: A susceptible-exposed-infected-removed (SEIR) model assumes spatial-/time-varying coefficients to model the effect of non-pharmaceutical interventions (NPIs) on the regional and temporal distribution of COVID-19 disease epidemics. A significant challenge in using such model is their fast and accurate calibration to observed data from geo-referenced hospitalized data, i.e., efficient estimation of the spatial-/time-varying parameters. In this work, a new calibration framework is proposed towards optimizing the spatial-/time-varying parameters of the SEIR model. We also devise a method for combing the overlapping sliding window technique (OSW) with a genetic algorithm (GA) calibration routine to automatically search the segmented parameter space. Parallelized GA is used to reduce the computational burden. Our framework abstracts the implementation complexity of the method away from the user. It provides high-level APIs for setting up a customized calibration system and consuming the optimized values of parameters. We evaluated the application of our method on the calibration of a spatial age-structured microsimulation model using a single objective function that comprises observed COVID-19-related ICU demand. The results reflect the effectiveness of the proposed method towards estimating the parameters in a changing environment."}, "https://arxiv.org/abs/2408.01549": {"title": "Reducing COVID-19 Misinformation Spread by Introducing Information Diffusion Delay Using Agent-based Modeling", "link": "https://arxiv.org/abs/2408.01549", "description": "arXiv:2408.01549v1 Announce Type: new \nAbstract: With the explosive growth of the Coronavirus Pandemic (COVID-19), misinformation on social media has developed into a global phenomenon with widespread and detrimental societal effects. Despite recent progress and efforts in detecting COVID-19 misinformation on social media networks, this task remains challenging due to the complexity, diversity, multi-modality, and high costs of fact-checking or annotation. In this research, we introduce a systematic and multidisciplinary agent-based modeling approach to limit the spread of COVID-19 misinformation and interpret the dynamic actions of users and communities in evolutionary online (or offline) social media networks. Our model was applied to a Twitter network associated with an armed protest demonstration against the COVID-19 lockdown in Michigan state in May, 2020. We implemented a one-median problem to categorize the Twitter network into six key communities (nodes) and identified information exchange (links) within the network. We measured the response time to COVID-19 misinformation spread in the network and employed a cybernetic organizational method to monitor the Twitter network. The overall misinformation mitigation strategy was evaluated, and agents were allocated to interact with the network based on the measured response time and feedback. The proposed model prioritized the communities based on the agents response times at the operational level. It then optimized agent allocation to limit the spread of COVID19 related misinformation from different communities, improved the information diffusion delay threshold to up to 3 minutes, and ultimately enhanced the mitigation process to reduce misinformation spread across the entire network."}, "https://arxiv.org/abs/2408.01631": {"title": "A Comparative Analysis of Wealth Index Predictions in Africa between three Multi-Source Inference Models", "link": "https://arxiv.org/abs/2408.01631", "description": "arXiv:2408.01631v1 Announce Type: new \nAbstract: Poverty map inference is a critical area of research, with growing interest in both traditional and modern techniques, ranging from regression models to convolutional neural networks applied to tabular data, images, and networks. Despite extensive focus on the validation of training phases, the scrutiny of final predictions remains limited. Here, we compare the Relative Wealth Index (RWI) inferred by Chi et al. (2021) with the International Wealth Index (IWI) inferred by Lee and Braithwaite (2022) and Esp\\'in-Noboa et al. (2023) across six Sub-Saharan African countries. Our analysis focuses on identifying trends and discrepancies in wealth predictions over time. Our results show that the predictions by Chi et al. and Esp\\'in-Noboa et al. align with general GDP trends, with differences expected due to the distinct time-frames of the training sets. However, predictions by Lee and Braithwaite diverge significantly, indicating potential issues with the validity of the model. These discrepancies highlight the need for policymakers and stakeholders in Africa to rigorously audit models that predict wealth, especially those used for decision-making on the ground. These and other techniques require continuous verification and refinement to enhance their reliability and ensure that poverty alleviation strategies are well-founded."}, "https://arxiv.org/abs/2408.01674": {"title": "Dynamical toy model of interacting $N$ agents robustly exhibiting Zipf's law", "link": "https://arxiv.org/abs/2408.01674", "description": "arXiv:2408.01674v1 Announce Type: new \nAbstract: We propose a dynamical toy model of agents which possess a quantity and have an interaction radius depending on the amount of the quantity. They exchange the quantity with agents existing within their interaction radii. It is shown in the paper that the distribution of the quantity of agents is robustly governed by Zipf's law for a small density of agents independent of the number of agents and the type of interaction, despite the simplicity of the rules. The model can exhibit other power laws with different exponents and the Gaussian distributions. The difference in the mechanism underlying Zipf's law and other power laws are studied by mapping the systems into graphs and investigating quantities characterizing the mapped graph. Thus, this model suggests one of the origins of Zipf's law, i.e., the most common fundamental characteristics necessary for Zipf's law to appear."}, "https://arxiv.org/abs/2408.01900": {"title": "Quantifying gendered citation imbalance in computer science conferences", "link": "https://arxiv.org/abs/2408.01900", "description": "arXiv:2408.01900v1 Announce Type: new \nAbstract: The number of citations received by papers often exhibits imbalances in terms of author attributes such as country of affiliation and gender. While recent studies have quantified citation imbalance in terms of the authors' gender in journal papers, the computer science discipline, where researchers frequently present their work at conferences, may exhibit unique patterns in gendered citation imbalance. Additionally, understanding how network properties in citations influence citation imbalances remains challenging due to a lack of suitable reference models. In this paper, we develop a family of reference models for citation networks and investigate gender imbalance in citations between papers published in computer science conferences. By deploying these reference models, we found that homophily in citations is strongly associated with gendered citation imbalance in computer science, whereas heterogeneity in the number of citations received per paper has a relatively minor association with it. Furthermore, we found that the gendered citation imbalance is most pronounced in papers published in the highest-ranked conferences, is present across different subfields, and extends to citation-based rankings of papers. Our study provides a framework for investigating associations between network properties and citation imbalances, aiming to enhance our understanding of the structure and dynamics of citations between research publications."}, "https://arxiv.org/abs/2408.02042": {"title": "Evolutionary dynamics in stochastic nonlinear public goods games", "link": "https://arxiv.org/abs/2408.02042", "description": "arXiv:2408.02042v1 Announce Type: new \nAbstract: Understanding the evolution of cooperation in multiplayer games is of vital significance for natural and social systems. An important challenge is that group interactions often leads to nonlinear synergistic effects. However, previous models mainly focus on deterministic nonlinearity where the arise of synergy or discounting effect is determined by certain conditions, ignoring uncertainty and stochasticity in real-world systems. Here, we develop a probabilistic framework to study the cooperative behavior in stochastic nonlinear public goods games. Through both analytical treatment and Monte Carlo simulations, we provide comprehensive understanding of social dilemmas with stochastic nonlinearity in both well-mixed and structured populations. We find that increasing the degree of nonlinearity makes synergy more advantageous when competing with discounting, thereby promoting cooperation. Interestingly, we show that network reciprocity loses effectiveness when the probability of synergy is small. Moreover, group size exhibits nonlinear effects on group cooperation regardless of the underlying structure. Our findings thus provide novel insights into how stochastic nonlinearity influences the emergence of prosocial behavior."}, "https://arxiv.org/abs/2408.02076": {"title": "Why distinctiveness centrality is distinctive", "link": "https://arxiv.org/abs/2408.02076", "description": "arXiv:2408.02076v1 Announce Type: new \nAbstract: This paper responds to a commentary by Neal (2024) regarding the Distinctiveness centrality metrics introduced by Fronzetti Colladon and Naldi (2020). Distinctiveness centrality offers a novel reinterpretation of degree centrality, particularly emphasizing the significance of direct connections to loosely connected peers within (social) networks. This response paper presents a more comprehensive analysis of the correlation between Distinctiveness and the Beta and Gamma measures. All five distinctiveness measures are considered, as well as a more meaningful range of the {\\alpha} parameter and different network topologies, distinguishing between weighted and unweighted networks. Findings indicate significant variability in correlations, supporting the viability of Distinctiveness as alternative or complementary metrics within social network analysis. Moreover, the paper presents computational complexity analysis and simplified R code for practical implementation. Encouraging initial findings suggest potential applications in diverse domains, inviting further exploration and comparative analyses."}, "https://arxiv.org/abs/2408.02092": {"title": "SEAtech: Deception Techniques in Social Engineering Attacks: An Analysis of Emerging Trends and Countermeasures", "link": "https://arxiv.org/abs/2408.02092", "description": "arXiv:2408.02092v1 Announce Type: new \nAbstract: Social Engineering is the act of manipulating individuals to perform actions or reveal information. Social engineering tactics are widely recognized as a significant risk to information security. The increasing digital environment has increased the prevalence of social engineering attacks, bringing huge threats to both people and organizations. This paper explores current deception techniques used during social engineering attacks to understand emerging trends and discuss effective countermeasures. It is always a good idea to have knowledge of counter measures and risks from these increasing cyber threats. We have also explored the types of deception attacks and role of social engineering in Advanced Persistent Threats. Today major concern for cybersecurity and other web related attacks is due to social engineering attacks that is also the driving force of increasing cybercrimes worldwide. By uncovering emerging trends and analyzing the psychological underpinnings of these attacks this paper highlights the known deception techniques, emerging trends and counter measures of social engineering attacks."}, "https://arxiv.org/abs/2408.02370": {"title": "Identifying influential node groups in networks with core-periphery structure", "link": "https://arxiv.org/abs/2408.02370", "description": "arXiv:2408.02370v1 Announce Type: new \nAbstract: Identifying influential spreaders is a crucial problem for practical applications in network science. The core-periphery(C-P) structure, common in many real-world networks, comprises a densely interconnected group of nodes(core) and the rest of the sparsely connected nodes subordinated to the core(periphery). Core nodes are expected to be more influential than periphery nodes generally, but recent studies suggest that this is not the case in some networks. In this work, we look for mesostructural conditions that arise when core nodes are significantly more influential than periphery nodes. In particular, we investigate the roles of the internal and external connectivity of cores in their relative influence. We observe that the internal and external connectivity of cores are broadly distributed, and the relative influence of the cores is also broadly distributed in real-world networks. Our key finding is that the internal connectivity of cores is positively correlated with their relative influence, whereas the relative influence increases up to a certain value of the external connectivity and decreases thereafter. Finally, results from the model-generated networks clarify the observations from the real-world networks. Our findings provide a structural condition for influential cores in networks and shed light on why some cores are influential and others are not."}, "https://arxiv.org/abs/2408.02389": {"title": "Fast Estimation of Percolation Centrality", "link": "https://arxiv.org/abs/2408.02389", "description": "arXiv:2408.02389v1 Announce Type: new \nAbstract: In this work, we present a new algorithm to approximate the percolation centrality of every node in a graph. Such a centrality measure quantifies the importance of the vertices in a network during a contagious process. In this paper, we present a randomized approximation algorithm that can compute probabilistically guaranteed high-quality percolation centrality estimates, generalizing techniques used by Pellegrina and Vandin (TKDD 2024) for the betweenness centrality. The estimation obtained by our algorithm is within $\\varepsilon$ of the value with probability at least $1-\\delta$, for fixed constants $\\varepsilon,\\delta \\in (0,1)$. We our theoretical results with an extensive experimental analysis on several real-world networks and provide empirical evidence that our algorithm improves the current state of the art in speed, and sample size while maintaining high accuracy of the percolation centrality estimates."}, "https://arxiv.org/abs/2408.02467": {"title": "Inferring firm-level supply chain networks with realistic systemic risk from industry sector-level data", "link": "https://arxiv.org/abs/2408.02467", "description": "arXiv:2408.02467v1 Announce Type: new \nAbstract: Production networks constitute the backbone of every economic system. They are inherently fragile as several recent crises clearly highlighted. Estimating the system-wide consequences of local disruptions (systemic risk) requires detailed information on the supply chain networks (SCN) at the firm-level, as systemic risk is associated with specific mesoscopic patterns. However, such information is usually not available and realistic estimates must be inferred from available sector-level data such as input-output tables and firm-level aggregate output data. Here we explore the ability of several maximum-entropy algorithms to infer realizations of SCNs characterized by a realistic level of systemic risk. We are in the unique position to test them against the actual Ecuadorian production network at the firm-level. Concretely, we compare various properties, including the Economic Systemic Risk Index, of the Ecuadorian production network with those from four inference models. We find that the most realistic systemic risk content at the firm-level is retrieved by the model that incorporates information about firm-specific input disaggregated by sector, indicating the importance of correctly accounting for firms' heterogeneous input profiles across sectors. Our results clearly demonstrate the minimal amount of empirical information at the sector level that is necessary to statistically generate synthetic SCNs that encode realistic firm-specific systemic risk."}, "https://arxiv.org/abs/2408.02482": {"title": "Gender differences in collaboration and career progression in physics", "link": "https://arxiv.org/abs/2408.02482", "description": "arXiv:2408.02482v1 Announce Type: new \nAbstract: We examine gender differences in collaboration networks and academic career progression in physics. We use the likelihood and time to become a principal investigator (PI) and the length of an author's career to measure career progression. Utilising logistic regression and accelerated failure time models, we examine whether the effect of collaboration behaviour varies by gender. We find that, controlling for the number of publications, the relationship between collaborative behaviour and career progression is almost the same for men and women. Specifically, we find that those who eventually reach principal investigator (PI) status, tend to have published with more unique collaborators. In contrast, publishing repeatedly with the same highly interconnected collaborators and/or larger number of coauthors per publication is characteristic of shorter career lengths and those that do not attain PI status. We observe that the tie strength is stronger for women than men, and women tend to collaborate in more tightly connected and larger groups than men. Finally, we observe that women are less likely to attain the status of PI throughout their careers and have a lower survival probability compared to men, which calls for policies to close this crucial gap."}, "https://arxiv.org/abs/2408.01464": {"title": "Summary Report on the 2024 African School of Physics Program for Learners", "link": "https://arxiv.org/abs/2408.01464", "description": "arXiv:2408.01464v1 Announce Type: cross \nAbstract: On April 15-19, 2024, as a part of the eighth African School of Physics, ASP2024, we organized a program for learners from selected high schools in the vicinity of Marrakesh, Morocco. In this event, within a week, we reached out to over a thousand high school students, aka learners, from many high schools in the region of Marrakesh. We present a summary report on these outreach activities."}, "https://arxiv.org/abs/2408.01594": {"title": "\"I don't see myself represented here at all\": User Experiences of Stable Diffusion Outputs Containing Representational Harms across Gender Identities and Nationalities", "link": "https://arxiv.org/abs/2408.01594", "description": "arXiv:2408.01594v1 Announce Type: cross \nAbstract: Though research into text-to-image generators (T2Is) such as Stable Diffusion has demonstrated their amplification of societal biases and potentials to cause harm, such research has primarily relied on computational methods instead of seeking information from real users who experience harm, which is a significant knowledge gap. In this paper, we conduct the largest human subjects study of Stable Diffusion, with a combination of crowdsourced data from 133 crowdworkers and 14 semi-structured interviews across diverse countries and genders. Through a mixed-methods approach of intra-set cosine similarity hierarchies (i.e., comparing multiple Stable Diffusion outputs for the same prompt with each other to examine which result is 'closest' to the prompt) and qualitative thematic analysis, we first demonstrate a large disconnect between user expectations for Stable Diffusion outputs with those generated, evidenced by a set of Stable Diffusion renditions of `a Person' providing images far away from such expectations. We then extend this finding of general dissatisfaction into highlighting representational harms caused by Stable Diffusion upon our subjects, especially those with traditionally marginalized identities, subjecting them to incorrect and often dehumanizing stereotypes about their identities. We provide recommendations for a harm-aware approach to (re)design future versions of Stable Diffusion and other T2Is."}, "https://arxiv.org/abs/2408.01782": {"title": "Are EU low-carbon structural funds efficient in reducing emissions?", "link": "https://arxiv.org/abs/2408.01782", "description": "arXiv:2408.01782v1 Announce Type: cross \nAbstract: This paper investigates the effectiveness of the ``low-carbon economy'' expenditures from European Structural and Investment Funds in fostering reductions in greenhouse gas emissions within European regions, focusing on the 2007-2013 and 2014-2020 programme periods. By decomposing emissions time series into trend and cycle components and considering them within a panel data framework, our research highlights that the impacts of low-carbon economy expenditures vary, qualitatively and quantitatively, with the targeted regions' development level. We find significant emissions reductions in developed and transition regions yet less favourable outcomes in less developed areas. Further analysis into specific greenhouse gas emissions types (CO$_2$, CH$_4$, and N$_2$O) reveals inconsistent impacts, underscoring the complexity of achieving emissions reductions. Our findings emphasise the need for tailored environmental strategies that accommodate the economic disparities of regions in the European Union."}, "https://arxiv.org/abs/2408.01911": {"title": "Brief state of the art in social information mining: Practical application in analysis of trends in French legislative 2024", "link": "https://arxiv.org/abs/2408.01911", "description": "arXiv:2408.01911v1 Announce Type: cross \nAbstract: The analysis of social media information has undergone significant evolution in the last decade due to advancements in artificial intelligence (AI) and machine learning (ML). This paper provides an overview of the state-of-the-art techniques in social media mining, with a practical application in analyzing trends in the 2024 French legislative elections. We leverage natural language processing (NLP) tools to gauge public opinion by extracting and analyzing comments and reactions from the AgoraVox platform. The study reveals that the National Rally party, led by Marine Le Pen, maintains a high level of engagement on social media, outperforming traditional parties. This trend is corroborated by user interactions, indicating a strong digital presence. The results highlight the utility of advanced AI models, such as transformers and large language models (LLMs), in capturing nuanced public sentiments and predicting political leanings, demonstrating their potential in real-time reputation management and crisis response."}, "https://arxiv.org/abs/2408.02097": {"title": "Optimal policy for control of epidemics with constrained time intervals and region-based interactions", "link": "https://arxiv.org/abs/2408.02097", "description": "arXiv:2408.02097v1 Announce Type: cross \nAbstract: We introduce a policy model coupled with the susceptible-infected-recovered (SIR) epidemic model to study interactions between policy-making and the dynamics of epidemics. We consider both single-region policies, as well as game-theoretic models involving interactions among several regions, and hierarchical interactions among policy-makers modeled as multi-layer games. We assume that the policy functions are piece-wise constant with a minimum time interval for each policy stage, considering policies cannot change frequently in time or they cannot be easily followed. The optimal policy is obtained by minimizing a cost function which consists of an implementation cost, an impact cost, and, in the case of multi-layer games, a non-compliance cost. We show in a case study of COVID-19 in France that when the cost function is reduced to the impact cost and is parameterized as the final epidemic size, the solution approximates that of the optimal control in Bliman et al, J. Optim. Theory Appl., 189, 2021, for sufficiently small minimum policy time interval. For a larger time interval however the optimal policy is a step down function, quite different from the step up structure typically deployed during the COVID-19 pandemic. In addition, we present a counterfactual study of how the pandemic would have evolved if herd immunity was reached during the second wave in the county of Los Angeles, California. Lastly, we study a case of three interacting counties with and without a governing state."}, "https://arxiv.org/abs/2408.02208": {"title": "Multi-level Traffic-Responsive Tilt Camera Surveillance through Predictive Correlated Online Learning", "link": "https://arxiv.org/abs/2408.02208", "description": "arXiv:2408.02208v1 Announce Type: cross \nAbstract: In urban traffic management, the primary challenge of dynamically and efficiently monitoring traffic conditions is compounded by the insufficient utilization of thousands of surveillance cameras along the intelligent transportation system. This paper introduces the multi-level Traffic-responsive Tilt Camera surveillance system (TTC-X), a novel framework designed for dynamic and efficient monitoring and management of traffic in urban networks. By leveraging widely deployed pan-tilt-cameras (PTCs), TTC-X overcomes the limitations of a fixed field of view in traditional surveillance systems by providing mobilized and 360-degree coverage. The innovation of TTC-X lies in the integration of advanced machine learning modules, including a detector-predictor-controller structure, with a novel Predictive Correlated Online Learning (PiCOL) methodology and the Spatial-Temporal Graph Predictor (STGP) for real-time traffic estimation and PTC control. The TTC-X is tested and evaluated under three experimental scenarios (e.g., maximum traffic flow capture, dynamic route planning, traffic state estimation) based on a simulation environment calibrated using real-world traffic data in Brooklyn, New York. The experimental results showed that TTC-X captured over 60\\% total number of vehicles at the network level, dynamically adjusted its route recommendation in reaction to unexpected full-lane closure events, and reconstructed link-level traffic states with best MAE less than 1.25 vehicle/hour. Demonstrating scalability, cost-efficiency, and adaptability, TTC-X emerges as a powerful solution for urban traffic management in both cyber-physical and real-world environments."}, "https://arxiv.org/abs/2402.03924": {"title": "Network Analysis of U", "link": "https://arxiv.org/abs/2402.03924", "description": "arXiv:2402.03924v2 Announce Type: replace \nAbstract: We present a nation-wide network analysis of non-fatal opioid-involved overdose journeys in the United States. Leveraging a unique proprietary dataset of Emergency Medical Services incidents, we construct a journey-to-overdose geospatial network capturing nearly half a million opioid-involved overdose events spanning 2018-2023. We analyze the structure and sociological profiles of the nodes, which are counties or their equivalents, characterize the distribution of overdose journey lengths, and investigate changes in the journey network between 2018 and 2023. Our findings include that authority and hub nodes identified by the HITS algorithm tend to be located in urban areas and involved in overdose journeys with particularly long geographical distances."}, "https://arxiv.org/abs/2303.01653": {"title": "Upper Bounds on Overshoot in SIR Models with Nonlinear Incidence", "link": "https://arxiv.org/abs/2303.01653", "description": "arXiv:2303.01653v5 Announce Type: replace-cross \nAbstract: We expand the calculation of the upper bound on epidemic overshoot in SIR models to account for nonlinear incidence. We lay out the general procedure and restrictions to perform the calculation analytically for nonlinear functions in the number of susceptibles. We demonstrate the procedure by working through several examples and also numerically study what happens to the upper bound on overshoot when nonlinear incidence manifests in the form of epidemic dynamics over a contact network. We find that both steeper incidence terms and larger contact heterogeneity can increase the range of communicable diseases at which the overshoot remains a relatively large public health hazard."}, "https://arxiv.org/abs/2305.09958": {"title": "SIGMA: Similarity-based Efficient Global Aggregation for Heterophilous Graph Neural Networks", "link": "https://arxiv.org/abs/2305.09958", "description": "arXiv:2305.09958v2 Announce Type: replace-cross \nAbstract: Graph neural networks (GNNs) realize great success in graph learning but suffer from performance loss when meeting heterophily, i.e. neighboring nodes are dissimilar, due to their local and uniform aggregation. Existing attempts of heterophilous GNNs incorporate long-range or global aggregations to distinguish nodes in the graph. However, these aggregations usually require iteratively maintaining and updating full-graph information, which limits their efficiency when applying to large-scale graphs. In this paper, we propose \\aggname{}, an efficient global heterophilous GNN aggregation integrating the structural similarity measurement SimRank. Our theoretical analysis illustrates that \\aggname{} inherently captures distant global similarity even under heterophily, that conventional approaches can only achieve after iterative aggregations. Furthermore, it enjoys efficient one-time computation with a complexity only linear to the node set size $\\mathcal{O}(n)$. Comprehensive evaluation demonstrates that \\aggname{} achieves state-of-the-art performance with superior aggregation and overall efficiency. Notably, it obtains 5$\\times$ acceleration on the large-scale heterophily dataset \\emph{pokec} with over 30 million edges compared to the best baseline aggregation."}, "https://arxiv.org/abs/2306.04802": {"title": "A Review on Knowledge Graphs for Healthcare: Resources, Applications, and Promises", "link": "https://arxiv.org/abs/2306.04802", "description": "arXiv:2306.04802v4 Announce Type: replace-cross \nAbstract: Healthcare knowledge graphs (HKGs) are valuable tools for organizing biomedical concepts and their relationships with interpretable structures. The recent advent of large language models (LLMs) has paved the way for building more comprehensive and accurate HKGs. This, in turn, can improve the reliability of generated content and enable better evaluation of LLMs. However, the challenges of HKGs such as regarding data heterogeneity and limited coverage are not fully understood, highlighting the need for detailed reviews. This work provides the first comprehensive review of HKGs. It summarizes the pipeline and key techniques for HKG construction, as well as the common utilization approaches, i.e., model-free and model-based. The existing HKG resources are also organized based on the data types they capture and application domains they cover, along with relevant statistical information (Resource available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase). At the application level, we delve into the successful integration of HKGs across various health domains, ranging from fine-grained basic science research to high-level clinical decision support and public health. Lastly, the paper highlights the opportunities for HKGs in the era of LLMs. This work aims to serve as a valuable resource for understanding the potential and opportunities of HKG in health research."}, "https://arxiv.org/abs/2310.11060": {"title": "Privacy-Preserving Graph Embedding based on Local Differential Privacy", "link": "https://arxiv.org/abs/2310.11060", "description": "arXiv:2310.11060v2 Announce Type: replace-cross \nAbstract: Graph embedding has become a powerful tool for learning latent representations of nodes in a graph. Despite its superior performance in various graph-based machine learning tasks, serious privacy concerns arise when the graph data contains personal or sensitive information. To address this issue, we investigate and develop graph embedding algorithms that satisfy local differential privacy (LDP). We introduce a novel privacy-preserving graph embedding framework, named PrivGE, to protect node data privacy. Specifically, we propose an LDP mechanism to obfuscate node data and utilize personalized PageRank as the proximity measure to learn node representations. Furthermore, we provide a theoretical analysis of the privacy guarantees and utility offered by the PrivGE framework. Extensive experiments on several real-world graph datasets demonstrate that PrivGE achieves an optimal balance between privacy and utility, and significantly outperforms existing methods in node classification and link prediction tasks."}, "https://arxiv.org/abs/2408.02675": {"title": "Key factors and network model for location-based cultural mobile game design", "link": "https://arxiv.org/abs/2408.02675", "description": "arXiv:2408.02675v1 Announce Type: new \nAbstract: The use of smart devices as media for digital learning constitutes a new-generation digital learning paradigm. Therefore, context-aware game-based learning has attracted considerable attention. Location-based games have not only positive effects on learning but also pronounced effects on culture and history. Accordingly, focusing on railway cultural heritages, we attempted to assess interdependent relationships between key factors crucial for the design of a location-based mobile game for cultural heritages. We adopted the analytic network process (ANP) for our assessment. We initially performed a literature review to generalize relevant criteria and elements and developed a questionnaire based on the fuzzy delphi method (FDM); thus, key factors, namely 3 criteria and 15 elements, were selected. We also applied an online ANP-based questionnaire; on the basis of the experts opinions, we established a network model and determined the priority order of the key factors. The results revealed that experts considered culture learning to be of the highest importance, with the most important three elements being prior knowledge, challenge levels, and cultural narrative. In addition, culture learning exhibited a strong interaction with content design. In each criterion, one element had a considerable influence on the remaining elements, as determined by an analysis of matrices."}, "https://arxiv.org/abs/2408.02959": {"title": "Enhancing Stability and Assessing Uncertainty in Community Detection through a Consensus-based Approach", "link": "https://arxiv.org/abs/2408.02959", "description": "arXiv:2408.02959v1 Announce Type: new \nAbstract: Complex data in social and natural sciences find effective representation through networks, wherein quantitative and categorical information can be associated with nodes and connecting edges. The internal structure of networks can be explored using unsupervised machine learning methods known as community detection algorithms. The process of community detection is inherently subject to uncertainty as algorithms utilize heuristic approaches and randomised procedures to explore vast solution spaces, resulting in non-deterministic outcomes and variability in detected communities across multiple runs. Moreover, many algorithms are not designed to identify outliers and may fail to take into account that a network is an unordered mathematical entity. The main aim of our work is to address these issues through a consensus-based approach by introducing a new framework called Consensus Community Detection (CCD). Our method can be applied to different community detection algorithms, allowing the quantification of uncertainty for the whole network as well as for each node, and providing three strategies for dealing with outliers: incorporate, highlight, or group. The effectiveness of our approach is evaluated on artificial benchmark networks."}, "https://arxiv.org/abs/2408.02986": {"title": "Structural evolution of international crop trade networks", "link": "https://arxiv.org/abs/2408.02986", "description": "arXiv:2408.02986v1 Announce Type: new \nAbstract: Food security is a critical issue closely linked to human being. With the increasing demand for food, international trade has become the main access to supplementing domestic food shortages, which not only alleviates local food shocks, but also exposes economies to global food crises. In this paper, we construct four temporal international crop trade networks (iCTNs) based on trade values of maize, rice, soybean and wheat, and describe the structural evolution of different iCTNs from{ {1993}} to 2018. We find that the size of all the four iCTNs expanded from{ {1993}} to 2018 with more participants and larger trade values. Our results show that the iCTNs not only become tighter according to the increasing in network density and clustering coefficient, but also get more similar. We also find that the iCTNs are not always disassortative, unlike the world cereal trade networks and other international commodity trade networks. The degree assortative coefficients depend on degree directions and crop types. The analysis about assortativity also indicates that economies with high out-degrees tend to connect with economies with low in-degrees and low out-degrees. Additionally, we compare the structure of the four iCTNs to enhance our understanding of the international food trade system. Although the overall evolutionary patterns of different iCTNs are similar, some crops exhibit idiosyncratic trade patterns. It highlights the need to consider different crop networks' idiosyncratic features while making food policies. Our findings about the dynamics of the iCTNs play an important role in understanding vulnerabilities in the global food system."}, "https://arxiv.org/abs/2408.03096": {"title": "Enhancing Twitter Bot Detection via Multimodal Invariant Representations", "link": "https://arxiv.org/abs/2408.03096", "description": "arXiv:2408.03096v1 Announce Type: new \nAbstract: Detecting Twitter Bots is crucial for maintaining the integrity of online discourse, safeguarding democratic processes, and preventing the spread of malicious propaganda. However, advanced Twitter Bots today often employ sophisticated feature manipulation and account farming techniques to blend seamlessly with genuine user interactions, posing significant challenges to existing detection models. In response to these challenges, this paper proposes a novel Twitter Bot Detection framework called BotSAI. This framework enhances the consistency of multimodal user features, accurately characterizing various modalities to distinguish between real users and bots. Specifically, the architecture integrates information from users, textual content, and heterogeneous network topologies, leveraging customized encoders to obtain comprehensive user feature representations. The heterogeneous network encoder efficiently aggregates information from neighboring nodes through oversampling techniques and local relationship transformers. Subsequently, a multi-channel representation mechanism maps user representations into invariant and specific subspaces, enhancing the feature vectors. Finally, a self-attention mechanism is introduced to integrate and refine the enhanced user representations, enabling efficient information interaction. Extensive experiments demonstrate that BotSAI outperforms existing state-of-the-art methods on two major Twitter Bot Detection benchmarks, exhibiting superior performance. Additionally, systematic experiments reveal the impact of different social relationships on detection accuracy, providing novel insights for the identification of social bots."}, "https://arxiv.org/abs/2408.03140": {"title": "Measuring interconnectedness of infectious diseases in funded and unfunded research: a temporal network analysis on bibliometric data 1995-2022", "link": "https://arxiv.org/abs/2408.03140", "description": "arXiv:2408.03140v1 Announce Type: new \nAbstract: Despite substantial investments in infectious disease research over the past decades, the field continues to struggle with inadequate long-term investment strategies and resource disparities, which highlights the critical need for a better understanding of funding and research landscapes to support evidence-based policymaking. Our study presents a novel perspective on the interconnectedness of evolving infectious disease knowledge. Through identifying publications based on funded and unfunded research, the analysis of temporal network of infectious disease associations reveals (i) growing compartmentalisation of funded research, i.e., it focuses on the groups of infectious diseases with readily established connections, and (ii) the growth in global integration in unfunded research, i.e., it tends to be more widely exploratory and links distant diseases. Moreover, we find that in both funded and unfunded research prominent diseases like HIV, malaria and tuberculosis have strong bridging effects facilitating global integration, while diphtheria, tetanus, and pertussis are characterised with strong local connectivity between themselves. We also find that although coronavirus has seen a surge in publications since COVID-19, its systemic impact on the interconnectedness of infectious disease knowledge remains relatively low. Our work highlights the importance of considering the interconnectedness of infectious diseases in health policy making and has potential to contribute to more efficient health resource allocation."}, "https://arxiv.org/abs/2408.03146": {"title": "The Dawn of Decentralized Social Media: An Exploration of Bluesky's Public Opening", "link": "https://arxiv.org/abs/2408.03146", "description": "arXiv:2408.03146v1 Announce Type: new \nAbstract: Bluesky is a Twitter-like decentralized social media platform that has recently grown in popularity. After an invite-only period, it opened to the public worldwide on February 6th, 2024. In this paper, we provide a longitudinal analysis of user activity in the two months around the opening, studying changes in the general characteristics of the platform due to the rapid growth of the user base. We observe a broad distribution of activity similar to more established platforms, but a higher volume of original than reshared content, and very low toxicity. After opening to the public, Bluesky experienced a large surge in new users and activity, especially posting English and Japanese content. In particular, several accounts entered the discussion with suspicious behavior, like following many accounts and sharing content from low-credibility news outlets. Some of these have already been classified as spam or suspended, suggesting effective moderation."}, "https://arxiv.org/abs/2408.02867": {"title": "Could a gravity inversion exist within the Hollow Earth of Legendary's Monsterverse?", "link": "https://arxiv.org/abs/2408.02867", "description": "arXiv:2408.02867v1 Announce Type: cross \nAbstract: One of the most popular movie franchises of late is Legendary's Monsterverse as is evidenced by the gross earnings of the series recently surpassing the two and a half billion dollar mark with the release of $\\it{Godzilla X Kong}$. As is typical with many movies of this genre, in order to enjoy them one must suspend their disbelief when it comes to the laws of physics. While there are a plethora of examples of violations of basic principles (the square-cube law being the prime example) the idea of a \"gravity inversion\" occurring inside of the \"Hollow Earth\" is among the most debated as well as the most relevant to the storylines of the recent movies. The intent of this paper is to show that, while typical scientific arguments refuting the possibility of such an inversion are definitely completely correct, a slight modification of the conditions assumed to exist inside the planet in these arguments may allow for the inversion to actually occur."}, "https://arxiv.org/abs/2408.02883": {"title": "\"Sharing, Not Showing Off\": How BeReal Approaches Authentic Self-Presentation on Social Media Through Its Design", "link": "https://arxiv.org/abs/2408.02883", "description": "arXiv:2408.02883v1 Announce Type: cross \nAbstract: Adolescents are particularly vulnerable to the pressures created by social media, such as heightened self-consciousness and the need for extensive self-presentation. In this study, we investigate how BeReal, a social media platform designed to counter some of these pressures, influences adolescents' self-presentation behaviors. We interviewed 29 users aged 13-18 to understand their experiences with BeReal. We found that BeReal's design focuses on spontaneous sharing, including randomly timed daily notifications and reciprocal posting, discourages staged posts, encourages careful curation of the audience, and reduces pressure on self-presentation. The space created by BeReal offers benefits such as validating an unfiltered life and reframing social comparison, but its approach to self-presentation is sometimes perceived as limited or unappealing and, at times, even toxic. Drawing on this empirical data, we distill a set of design guidelines for creating platforms that support authentic self-presentation online, such as scaffolding reciprocity and expanding beyond spontaneous photo-sharing to allow users to more accurately and comfortably portray themselves."}, "https://arxiv.org/abs/2304.12192": {"title": "Temporal rich club phenomenon and its formation mechanisms", "link": "https://arxiv.org/abs/2304.12192", "description": "arXiv:2304.12192v2 Announce Type: replace \nAbstract: The temporal rich club (TRC) phenomenon is widespread in real systems, forming a tight and continuous collection of the prominent nodes that control the system. However, there is still a lack of sufficient understanding of the mechanisms of TRC formation. Here we use the international N-nutrient trade network as an example of an in-depth identification, analysis, and modeling of its TRC phenomenon. The system exhibits a statistically significant TRC phenomenon, with eight economies forming the cornerstone club. Our analysis reveals that node degree is the most influential factor in TRC formation compared to other variables. The mathematical evolution models we constructed propose that the TRC in the N-nutrient trade network arises from the coexistence of degree-homophily and path-dependence mechanisms. By comprehending these mechanisms, we introduce a novel perspective on TRC formation. Although our analysis is limited to the international trade system, the methodology can be extended to analyze the mechanisms underlying TRC emergence in other systems."}, "https://arxiv.org/abs/2304.05760": {"title": "Visibility graph analysis of the grains and oilseeds indices", "link": "https://arxiv.org/abs/2304.05760", "description": "arXiv:2304.05760v2 Announce Type: replace-cross \nAbstract: The Grains and Oilseeds Index (GOI) and its sub-indices of wheat, maize, soyabeans, rice, and barley are daily price indexes reflect the price changes of the global spot markets of staple agro-food crops. In this paper, we carry out a visibility graph (VG) analysis of the GOI and its five sub-indices. Our findings reveal that the degree distributions of the VGs, except for rice, exhibit exponentially truncated power-law tails, while the rice VG conforms to a power-law tail. The average clustering coefficients of the six VGs are quite large ($>0.5$) and exhibit a nice power-law relation with respect to the average degrees of the VGs. For each VG, the clustering coefficients of nodes are inversely proportional to their degrees for large degrees and are correlated to their degrees as a power law for small degrees. All the six VGs exhibit small-world characteristics. The degree-degree correlation coefficients shows that the VGs for maize and soyabeans indices exhibit weak assortative mixing patterns, while the other four VGs are weakly disassortative. The average nearest neighbor degree functions have similar patterns, and each function shows a more complex mixing pattern which decreases for small degrees, increases for mediate degrees, and decreases again for large degrees."}, "https://arxiv.org/abs/2309.04637": {"title": "Network analysis of graduate program support structures through experiences of various demographic groups", "link": "https://arxiv.org/abs/2309.04637", "description": "arXiv:2309.04637v2 Announce Type: replace-cross \nAbstract: Physics graduate studies are substantial efforts on the part of individual students, departments, and institutions of higher education. Understanding the factors that lead to student success and attrition is crucial for improving these programs. One factor that has recently started to be investigated is the broadly defined students' experiences related to support structures. The Aspects of Student Experience Scale (ASES), a Likert-style survey, was developed by researchers to do just that. In this study, we leverage the network approach for Likert-style surveys (NALS) methodology to provide a unique interpretation of responses to the ASES instrument for well-defined demographic groups. We confirm the validity of our findings by studying the stability of the NALS themes and investigating how they are expressed within demographic-based networks. We find that for all four themes in the original ASES study, certain thematic trends capturing students' experiences vary across the demographic-based networks in meaningful ways. We also reveal that for some demographic groups, there is an interesting interplay between, and mixing of, the original themes. Finally, our study showcases how NALS can be applied to other Likert-style datasets."}, "https://arxiv.org/abs/2402.12629": {"title": "Television Discourse Decoded: Comprehensive Multimodal Analytics at Scale", "link": "https://arxiv.org/abs/2402.12629", "description": "arXiv:2402.12629v2 Announce Type: replace-cross \nAbstract: In this paper, we tackle the complex task of analyzing televised debates, with a focus on a prime time news debate show from India. Previous methods, which often relied solely on text, fall short in capturing the multimodal essence of these debates. To address this gap, we introduce a comprehensive automated toolkit that employs advanced computer vision and speech-to-text techniques for large-scale multimedia analysis. Utilizing state-of-the-art computer vision algorithms and speech-to-text methods, we transcribe, diarize, and analyze thousands of YouTube videos of a prime-time television debate show in India. These debates are a central part of Indian media but have been criticized for compromised journalistic integrity and excessive dramatization. Our toolkit provides concrete metrics to assess bias and incivility, capturing a comprehensive multimedia perspective that includes text, audio utterances, and video frames. Our findings reveal significant biases in topic selection and panelist representation, along with alarming levels of incivility. This work offers a scalable, automated approach for future research in multimedia analysis, with profound implications for the quality of public discourse and democratic debate. To catalyze further research in this area, we also release the code, dataset collected and supplemental pdf."}, "https://arxiv.org/abs/2404.02740": {"title": "Mixing Individual and Collective Behaviours to Predict Out-of-Routine Mobility", "link": "https://arxiv.org/abs/2404.02740", "description": "arXiv:2404.02740v2 Announce Type: replace-cross \nAbstract: Predicting human displacements is crucial for addressing various societal challenges, including urban design, traffic congestion, epidemic management, and migration dynamics. While predictive models like deep learning and Markov models offer insights into individual mobility, they often struggle with out-of-routine behaviours. Our study introduces an approach that dynamically integrates individual and collective mobility behaviours, leveraging collective intelligence to enhance prediction accuracy. Evaluating the model on millions of privacy-preserving trajectories across three US cities, we demonstrate its superior performance in predicting out-of-routine mobility, surpassing even advanced deep learning methods. Spatial analysis highlights the model's effectiveness near urban areas with a high density of points of interest, where collective behaviours strongly influence mobility. During disruptive events like the COVID-19 pandemic, our model retains predictive capabilities, unlike individual-based models. By bridging the gap between individual and collective behaviours, our approach offers transparent and accurate predictions, crucial for addressing contemporary mobility challenges."}, "https://arxiv.org/abs/2404.14244": {"title": "AI-Generated Faces in the Real World: A Large-Scale Case Study of Twitter Profile Images", "link": "https://arxiv.org/abs/2404.14244", "description": "arXiv:2404.14244v2 Announce Type: replace-cross \nAbstract: Recent advances in the field of generative artificial intelligence (AI) have blurred the lines between authentic and machine-generated content, making it almost impossible for humans to distinguish between such media. One notable consequence is the use of AI-generated images for fake profiles on social media. While several types of disinformation campaigns and similar incidents have been reported in the past, a systematic analysis has been lacking. In this work, we conduct the first large-scale investigation of the prevalence of AI-generated profile pictures on Twitter. We tackle the challenges of a real-world measurement study by carefully integrating various data sources and designing a multi-stage detection pipeline. Our analysis of nearly 15 million Twitter profile pictures shows that 0.052% were artificially generated, confirming their notable presence on the platform. We comprehensively examine the characteristics of these accounts and their tweet content, and uncover patterns of coordinated inauthentic behavior. The results also reveal several motives, including spamming and political amplification campaigns. Our research reaffirms the need for effective detection and mitigation strategies to cope with the potential negative effects of generative AI in the future."}, "https://arxiv.org/abs/2408.03331": {"title": "The Wasserstein Bipolarization Index: A New Measure of Public Opinion Polarization, with an Application to Cross-Country Attitudes toward COVID-19 Vaccination Mandates", "link": "https://arxiv.org/abs/2408.03331", "description": "arXiv:2408.03331v1 Announce Type: new \nAbstract: Although the topic of opinion polarization receives much attention from the media, public opinion researchers and political scientists, the phenomenon itself has not been adequately characterized in either the lay or academic literature. To study opinion polarization among the public, researchers compare the distributions of respondents to survey questions or track the distribution of responses to a question over time using ad-hoc methods and measures such as visual comparisons, variances, and bimodality coefficients. To remedy this situation, we build on the axiomatic approach in the economics literature on income bipolarization, specifying key properties a measure of bipolarization should satisfy: in particular, it should increase as the distribution spreads away from a center toward the poles and/or as clustering below or above this center increases. We then show that measures of bipolarization used in public opinion research fail to satisfy one or more of these axioms. Next, we propose a $p$-Wasserstein polarization index that satisfies the axioms we set forth. Our index measures the dissimilarity between an observed distribution and a distribution with all the mass clustered on the lower and upper endpoints of the scale. We use our index to examine bipolarization in attitudes toward governmental COVID-19 vaccine mandates across 11 countries, finding the U.S and U.K are most polarized, China, France and India the least polarized, while the others (Brazil, Australia, Colombia, Canada, Italy, Spain) occupy an intermediate position."}, "https://arxiv.org/abs/2408.03332": {"title": "Eliminating Bias in Pedestrian Density Estimation: A Voronoi Cell Perspective", "link": "https://arxiv.org/abs/2408.03332", "description": "arXiv:2408.03332v1 Announce Type: new \nAbstract: For pedestrians moving without spatial constraints, extensive research has been devoted to develop methods of density estimation. In this paper we present a new approach based on Voronoi cells, offering a means to estimate density for individuals in small, unbounded pedestrian groups. A thorough evaluation of existing methods, encompassing both Lagrangian and Eulerian approaches employed in similar contexts, reveals notable limitations. Specifically, these methods turn out to be ill-defined for realistic density estimation along a pedestrian's trajectory, exhibiting systematic biases and fluctuations that depend on the choice of parameters. There is thus a need for a parameter-independent method to eliminate this bias. We propose a modification of the widely used Voronoi-cell based density estimate to accommodate pedestrian groups, irrespective of their size. The advantages of this modified Voronoi method are that it is an instantaneous method that requires only knowledge of the pedestrians' positions at a give time, does not depend on the choice of parameter values, gives us a realistic estimate of density in an individual's neighborhood, and has appropriate physical meaning for both small and large human crowds in a wide variety of situations. We conclude with general remarks about the meaning of density measurements for small groups of pedestrians."}, "https://arxiv.org/abs/2408.03457": {"title": "Transit Rider Heat Stress in Atlanta, GA under Current and Future Climate Scenarios", "link": "https://arxiv.org/abs/2408.03457", "description": "arXiv:2408.03457v1 Announce Type: new \nAbstract: Transit is a crucial mode of transportation, especially in urban areas and for urban and rural disadvantaged communities. Because extreme temperatures often pose threats to the elderly, members of the disability community, and other vulnerable populations, this study seeks to understand the level of influence that extreme temperatures may have on transit users across different demographic groups. In this case study for Atlanta, GA, heat stress is predicted for 2019 transit riders (using transit rider activity survey data) and for three future climate scenarios, SSP245, SSP370, and SSP585, into the year 2100. The HeatPath Analyzer and TransitSim 4.0 models were applied to predict cumulative heat exposure and trip-level risk for 35,999 trip equivalents for an average Atlanta area weekday in the summer of 2019. The analyses show that under 2019 weather conditions, 8.33% of summer trips were estimated to be conducted under extreme heat. With the projected future climate conditions, the percentage of trips under extreme heat risk grows steadily. By 2100, 37.1%, 56.1%, and 76.4% are projected to be under extreme heat risk for scenarios SSP245, SSP370, and SSP585, respectively. Under current weather conditions, Atlanta transit riders that own no vehicles and transit riders that are African American are disproportionately influenced by extreme heat. The disparity between these two groups and other groups of transit riders becomes wider as climate change continues to exacerbate. The findings of the study highlight an urgent need to implement heat mitigation and adaptation strategies in urban transit networks."}, "https://arxiv.org/abs/2408.03755": {"title": "Geographical Isolation as a Driver of Political Violence in African Cities", "link": "https://arxiv.org/abs/2408.03755", "description": "arXiv:2408.03755v1 Announce Type: new \nAbstract: Violence is commonly linked with large urban areas, and as a social phenomenon, it is presumed to scale super-linearly with population size. This study explores the hypothesis that smaller, isolated cities in Africa may experience a heightened intensity of violence against civilians. It aims to investigate the correlation between the risk of experiencing violence with a city's size and its geographical isolation. Over a 20-year period, the incidence of civilian casualties has been analysed to assess lethality in relation to varying degrees of isolation and city sizes. African cities are categorised by isolation (number of highway connections) and centrality (the estimated frequency of journeys). Findings suggest that violence against civilians exhibits a sub-linear pattern, with larger cities witnessing fewer casualties per 100,000 inhabitants. Remarkably, individuals in isolated cities face a quadrupled risk of a casualty compared to those in more connected cities."}, "https://arxiv.org/abs/2408.03794": {"title": "A universal framework for inclusive 15-minute cities", "link": "https://arxiv.org/abs/2408.03794", "description": "arXiv:2408.03794v1 Announce Type: new \nAbstract: Proximity-based cities have attracted much attention in recent years. The 15-minute city, in particular, heralded a new vision for cities where essential services must be easily accessible. Despite its undoubted merit in stimulating discussion on new organisations of cities, the 15-minute city cannot be applicable everywhere, and its very definition raises a few concerns. Here, we tackle the feasibility and practicability of the '15-minute city' model in many cities worldwide. We provide a worldwide quantification of how close cities are to the ideal of the 15-minute city. To this end, we measure the accessibility times to resources and services, and we reveal strong heterogeneity of accessibility within and across cities, with a significant role played by local population densities. We provide an online platform (\\href{whatif.sonycsl.it/15mincity}{whatif.sonycsl.it/15mincity}) to access and visualise accessibility scores for virtually all cities worldwide. The heterogeneity of accessibility within cities is one of the sources of inequality. We thus simulate how much a better redistribution of resources and services could heal inequity by keeping the same resources and services or by allowing for virtually infinite resources. We highlight pronounced discrepancies among cities in the minimum number of additional services needed to comply with the 15-minute city concept. We conclude that the proximity-based paradigm must be generalised to work on a wide range of local population densities. Finally, socio-economic and cultural factors should be included to shift from time-based to value-based cities."}, "https://arxiv.org/abs/2408.03828": {"title": "More than 'Left and Right': Revealing Multilevel Online Political Selective Exposure", "link": "https://arxiv.org/abs/2408.03828", "description": "arXiv:2408.03828v1 Announce Type: new \nAbstract: Selective exposure, individuals' inclination to seek out information that supports their beliefs while avoiding information that contradicts them, plays an important role in the emergence of polarization. In the political domain, selective exposure is usually measured on a left-right ideology scale, ignoring finer details. Here, we combine survey and Twitter data collected during the 2022 Brazilian Presidential Election and investigate selective exposure patterns between the survey respondents and political influencers. We analyze the followship network between survey respondents and political influencers and find a multilevel community structure that reveals a hierarchical organization more complex than a simple split between left and right. Moreover, depending on the level we consider, we find different associations between network indices of exposure patterns and 189 individual attributes of the survey respondents. For example, at finer levels, the number of influencer communities a survey respondent follows is associated with several factors, such as demographics, news consumption frequency, and incivility perception. In comparison, only their political ideology is a significant factor at coarser levels. Our work demonstrates that measuring selective exposure at a single level, such as left and right, misses important information necessary to capture this phenomenon correctly."}, "https://arxiv.org/abs/2408.03461": {"title": "When does the mean network capture the topology of a sample of networks?", "link": "https://arxiv.org/abs/2408.03461", "description": "arXiv:2408.03461v1 Announce Type: cross \nAbstract: The notion of Fr\\'echet mean (also known as \"barycenter\") network is the workhorse of most machine learning algorithms that require the estimation of a \"location\" parameter to analyse network-valued data. In this context, it is critical that the network barycenter inherits the topological structure of the networks in the training dataset. The metric - which measures the proximity between networks - controls the structural properties of the barycenter. This work is significant because it provides for the first time analytical estimates of the sample Fr\\'echet mean for the stochastic blockmodel, which is at the cutting edge of rigorous probabilistic analysis of random networks. We show that the mean network computed with the Hamming distance is unable to capture the topology of the networks in the training sample, whereas the mean network computed using the effective resistance distance recovers the correct partitions and associated edge density. From a practical standpoint, our work informs the choice of metrics in the context where the sample Fr\\'echet mean network is used to characterise the topology of networks for network-valued machine learning"}, "https://arxiv.org/abs/2408.03502": {"title": "Role Identification based Method for Cyberbullying Analysis in Social Edge Computing", "link": "https://arxiv.org/abs/2408.03502", "description": "arXiv:2408.03502v1 Announce Type: cross \nAbstract: Over the past few years, many efforts have been dedicated to studying cyberbullying in social edge computing devices, and most of them focus on three roles: victims, perpetrators, and bystanders. If we want to obtain a deep insight into the formation, evolution, and intervention of cyberbullying in devices at the edge of the Internet, it is necessary to explore more fine-grained roles. This paper presents a multi-level method for role feature modeling and proposes a differential evolution-assisted K-means (DEK) method to identify diverse roles. Our work aims to provide a role identification scheme for cyberbullying scenarios for social edge computing environments to alleviate the general safety issues that cyberbullying brings. The experiments on ten real-world datasets obtained from Weibo and five public datasets show that the proposed DEK outperforms the existing approaches on the method level. After clustering, we obtained nine roles and analyzed the characteristics of each role and their evolution trends under different cyberbullying scenarios. Our work in this paper can be placed in devices at the edge of the Internet, leading to better real-time identification performance and adapting to the broad geographic location and high mobility of mobile devices."}, "https://arxiv.org/abs/2304.09914": {"title": "The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning", "link": "https://arxiv.org/abs/2304.09914", "description": "arXiv:2304.09914v4 Announce Type: replace-cross \nAbstract: Populist rhetoric employed on online media is characterized as deeply impassioned and often imbued with strong emotions. The aim of this paper is to empirically investigate the differences in affective nonverbal communication of political leaders. We use a deep-learning approach to process a sample of 220 YouTube videos of political leaders from 15 different countries, analyze their facial expressions of emotion and then examine differences in average emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the YouTube video. Based on a sample of manually coded images, we find that this deep-learning approach has 53-60\\% agreement with human labels. We observe statistically significant differences in the average score of negative emotions between groups of leaders with varying degrees of populist rhetoric."}, "https://arxiv.org/abs/2312.01185": {"title": "A ripple in time: a discontinuity in American history", "link": "https://arxiv.org/abs/2312.01185", "description": "arXiv:2312.01185v5 Announce Type: replace-cross \nAbstract: In this technical note we suggest a novel approach to discover temporal (related and unrelated to language dilation) and personality (authorship attribution) in historical datasets. We exemplify our approach on the State of the Union speeches given by the past 42 US presidents: this dataset is known for its relatively small amount of data, and high variability of the amount and style of texts. Nevertheless we manage to achieve about 95\\% accuracy on the authorship attribution task, and pin down the date of writing to a single presidential term."}, "https://arxiv.org/abs/2404.18980": {"title": "Networks And Productivity -- A Study In Economic Scholars During COVID-19", "link": "https://arxiv.org/abs/2404.18980", "description": "arXiv:2404.18980v2 Announce Type: replace-cross \nAbstract: The COVID-19 pandemic has disrupted traditional academic collaboration patterns, offering a unique opportunity to analyze the influence of peer effects and collaboration dynamics on research productivity. Using a novel network dataset, this paper investigates the role of peer effects on the productivity of economists, as measured by their publication count, during both pre-pandemic and pandemic periods. The results indicate that peer effects were significant in the pre-pandemic period but not during the pandemic. Additionally, the study sheds light on gender and race differences. These findings enhance our understanding of how research collaboration impacts knowledge production and provide insights that may inform policies aimed at promoting collaboration and boosting research productivity within the academic community."}, "https://arxiv.org/abs/2408.03952": {"title": "The marginal majority effect: when social influence produces lock-in", "link": "https://arxiv.org/abs/2408.03952", "description": "arXiv:2408.03952v1 Announce Type: new \nAbstract: People are influenced by the choices of others, a phenomenon observed across contexts in the social and behavioral sciences. Social influence can lock in an initial popularity advantage of an option over a higher quality alternative. Yet several experiments designed to enable social influence have found that social systems self-correct rather than lock-in. Here we identify a behavioral phenomenon that makes inferior lock-in possible, which we call the 'marginal majority effect': A discontinuous increase in the choice probability of an option as its popularity exceeds that of a competing option. We demonstrate the existence of marginal majority effects in several recent experiments and show that lock-in always occurs when the effect is large enough to offset the quality effect on choice, but rarely otherwise. Our results reconcile conflicting past empirical evidence and connect a behavioral phenomenon to the possibility of social lock-in."}, "https://arxiv.org/abs/2408.03966": {"title": "Large-Scale Graphs Community Detection using Spark GraphFrames", "link": "https://arxiv.org/abs/2408.03966", "description": "arXiv:2408.03966v1 Announce Type: new \nAbstract: With the emergence of social networks, online platforms dedicated to different use cases, and sensor networks, the emergence of large-scale graph community detection has become a steady field of research with real-world applications. Community detection algorithms have numerous practical applications, particularly due to their scalability with data size. Nonetheless, a notable drawback of community detection algorithms is their computational intensity~\\cite{Apostol2014}, resulting in decreasing performance as data size increases. For this purpose, new frameworks that employ distributed systems such as Apache Hadoop and Apache Spark which can seamlessly handle large-scale graphs must be developed. In this paper, we propose a novel framework for community detection algorithms, i.e., K-Cliques, Louvain, and Fast Greedy, developed using Apache Spark GraphFrames. We test their performance and scalability on two real-world datasets. The experimental results prove the feasibility of developing graph mining algorithms using Apache Spark GraphFrames."}, "https://arxiv.org/abs/2408.04076": {"title": "Multi-scale structural complexity as a quantitative measure of visual complexity", "link": "https://arxiv.org/abs/2408.04076", "description": "arXiv:2408.04076v1 Announce Type: new \nAbstract: While intuitive for humans, the concept of visual complexity is hard to define and quantify formally. We suggest adopting the multi-scale structural complexity (MSSC) measure, an approach that defines structural complexity of an object as the amount of dissimilarities between distinct scales in its hierarchical organization. In this work, we apply MSSC to the case of visual stimuli, using an open dataset of images with subjective complexity scores obtained from human participants (SAVOIAS). We demonstrate that MSSC correlates with subjective complexity on par with other computational complexity measures, while being more intuitive by definition, consistent across categories of images, and easier to compute. We discuss objective and subjective elements inherently present in human perception of complexity and the domains where the two are more likely to diverge. We show how the multi-scale nature of MSSC allows further investigation of complexity as it is perceived by humans."}, "https://arxiv.org/abs/2408.04163": {"title": "Academic collaboration on large language model studies increases overall but varies across disciplines", "link": "https://arxiv.org/abs/2408.04163", "description": "arXiv:2408.04163v1 Announce Type: new \nAbstract: Interdisciplinary collaboration is crucial for addressing complex scientific challenges. Recent advancements in large language models (LLMs) have shown significant potential in benefiting researchers across various fields. To explore the application of LLMs in scientific disciplines and their implications for interdisciplinary collaboration, we collect and analyze 50,391 papers from OpenAlex, an open-source platform for scholarly metadata. We first employ Shannon entropy to assess the diversity of collaboration in terms of authors' institutions and departments. Our results reveal that most fields have exhibited varying degrees of increased entropy following the release of ChatGPT, with Computer Science displaying a consistent increase. Other fields such as Social Science, Decision Science, Psychology, Engineering, Health Professions, and Business, Management & Accounting have shown minor to significant increases in entropy in 2024 compared to 2023. Statistical testing further indicates that the entropy in Computer Science, Decision Science, and Engineering is significantly lower than that in health-related fields like Medicine and Biochemistry, Genetics & Molecular Biology. In addition, our network analysis based on authors' affiliation information highlights the prominence of Computer Science, Medicine, and other Computer Science-related departments in LLM research. Regarding authors' institutions, our analysis reveals that entities such as Stanford University, Harvard University, University College London, and Google are key players, either dominating centrality measures or playing crucial roles in connecting research networks. Overall, this study provides valuable insights into the current landscape and evolving dynamics of collaboration networks in LLM research."}, "https://arxiv.org/abs/2408.04373": {"title": "The role of central places in exposure segregation", "link": "https://arxiv.org/abs/2408.04373", "description": "arXiv:2408.04373v1 Announce Type: new \nAbstract: Here we show that \"exposure segregation\" - the degree to which individuals of one group are exposed to individuals of another in day-to-day mobility - is dependent on the structure of cities, and the importance of downtowns in particular. Recent work uses aggregated data to claim that the location of amenities can inhibit or facilitate interactions between groups: if a city is residentially segregated, as many American cities are, then amenities between segregated communities should encourage them to mix. We show that the relationship between \"bridging\" amenities and socio-economic mixing breaks down when we examine the amenities themselves, rather than the urban aggregates. For example, restaurants with locations that suggest low expected mixing do not, much of the time, have low mixing: there is only a weak correlation between bridging and mixing at the level of the restaurant, despite a strong correlation at the level of the supermarket. This is because downtowns - and the bundle of amenities that define them - tend not to be situated in bridge areas but play an important role in drawing diverse groups together."}, "https://arxiv.org/abs/2408.04456": {"title": "Modeling information spread across networks with communities using a multitype branching process framework", "link": "https://arxiv.org/abs/2408.04456", "description": "arXiv:2408.04456v1 Announce Type: new \nAbstract: The dynamics of information diffusion in complex networks is widely studied in an attempt to understand how individuals communicate and how information travels and reaches individuals through interactions. However, complex networks often present community structure, and tools to analyse information diffusion on networks with communities are needed. In this paper, we develop theoretical tools using multi-type branching processes to model and analyse simple contagion information spread on a broad class of networks with community structure. We show how, by using limited information about the network -- the degree distribution within and between communities -- we can calculate standard statistical characteristics of the dynamics of information diffusion, such as the extinction probability, hazard function, and cascade size distribution. These properties can be estimated not only for the entire network but also for each community separately. Furthermore, we estimate the probability of information spreading from one community to another where it is not currently spreading. We demonstrate the accuracy of our framework by applying it to two specific examples: the Stochastic Block Model and a log-normal network with community structure. We show how the initial seeding location affects the observed cascade size distribution on a heavy-tailed network and that our framework accurately captures this effect."}, "https://arxiv.org/abs/2408.04395": {"title": "Enhanced Semantic Graph Based Approach With Sentiment Analysis For User Interest Retrieval From Social Sites", "link": "https://arxiv.org/abs/2408.04395", "description": "arXiv:2408.04395v1 Announce Type: cross \nAbstract: Blogs and social networking sites serve as a platform to the users for expressing their interests, ideas and thoughts. Targeted marketing uses the recommendation systems for suggesting their services and products to the users or clients. So the method used by target marketing is extraction of keywords and main topics from the user generated texts. Most of conventional methods involve identifying the personal interests just on the basis of surveys and rating systems. But the proposed research differs in manner that it aim at using the user generated text as a source medium for identifying and analyzing the personal interest as a knowledge base area of users. Semantic graph based approach is proposed research work that identifies the references of clients and users by analyzing their own texts such as tweets. The keywords need to be extracted from the text generated by the user on the social networking sites. This can be made possible by using several algorithms that extracts the keywords automatically from the available content provided by the user. Based on frequency and degree it ranks the extracted keywords. Furthermore, semantic graph based model assists in providing useful suggestions just by extracting the interests of users by analyzing their contents from social media. In this approach graph comprises of nodes and edges where nodes represents the keywords extracted by the algorithm and edges shows the semantic connection between the nodes. The method does not require internet related user activities like surveys or ratings to gather user interest related information."}, "https://arxiv.org/abs/2408.04441": {"title": "Causal Inference in Social Platforms Under Approximate Interference Networks", "link": "https://arxiv.org/abs/2408.04441", "description": "arXiv:2408.04441v1 Announce Type: cross \nAbstract: Estimating the total treatment effect (TTE) of a new feature in social platforms is crucial for understanding its impact on user behavior. However, the presence of network interference, which arises from user interactions, often complicates this estimation process. Experimenters typically face challenges in fully capturing the intricate structure of this interference, leading to less reliable estimates. To address this issue, we propose a novel approach that leverages surrogate networks and the pseudo inverse estimator. Our contributions can be summarized as follows: (1) We introduce the surrogate network framework, which simulates the practical situation where experimenters build an approximation of the true interference network using observable data. (2) We investigate the performance of the pseudo inverse estimator within this framework, revealing a bias-variance trade-off introduced by the surrogate network. We demonstrate a tighter asymptotic variance bound compared to previous studies and propose an enhanced variance estimator outperforming the original estimator. (3) We apply the pseudo inverse estimator to a real experiment involving over 50 million users, demonstrating its effectiveness in detecting network interference when combined with the difference-in-means estimator. Our research aims to bridge the gap between theoretical literature and practical implementation, providing a solution for estimating TTE in the presence of network interference and unknown interference structures."}, "https://arxiv.org/abs/2408.04461": {"title": "Random Walk Diffusion for Efficient Large-Scale Graph Generation", "link": "https://arxiv.org/abs/2408.04461", "description": "arXiv:2408.04461v1 Announce Type: cross \nAbstract: Graph generation addresses the problem of generating new graphs that have a data distribution similar to real-world graphs. While previous diffusion-based graph generation methods have shown promising results, they often struggle to scale to large graphs. In this work, we propose ARROW-Diff (AutoRegressive RandOm Walk Diffusion), a novel random walk-based diffusion approach for efficient large-scale graph generation. Our method encompasses two components in an iterative process of random walk sampling and graph pruning. We demonstrate that ARROW-Diff can scale to large graphs efficiently, surpassing other baseline methods in terms of both generation time and multiple graph statistics, reflecting the high quality of the generated graphs."}, "https://arxiv.org/abs/2408.04784": {"title": "Eigenvector Localization and Universal Regime Transitions in Multiplex Networks: A Perturbative Approach", "link": "https://arxiv.org/abs/2408.04784", "description": "arXiv:2408.04784v1 Announce Type: new \nAbstract: In this work, we investigate the transition between layer-localized and delocalized regimes in a general contact-based social contagion model on multiplex networks. We begin by analyzing the layer-localization to delocalization transition through the inverse participation ratio (IPR). Utilizing perturbation analysis, we derive a new analytical approximation for the transition point and an expression for the IPR in the non-dominant layer within the localized regime. Additionally, we examine the transition from a non-dominant to a dominant regime, providing an analytical expression for the transition point. These transitions are further explored and validated through dynamical simulations."}, "https://arxiv.org/abs/2408.05050": {"title": "Social contagion under hybrid interactions", "link": "https://arxiv.org/abs/2408.05050", "description": "arXiv:2408.05050v1 Announce Type: new \nAbstract: Threshold-driven models and game theory are two fundamental paradigms for describing human interactions in social systems. However, in mimicking social contagion processes, models that simultaneously incorporate these two mechanisms have been largely overlooked. Here, we study a general model that integrates hybrid interaction forms by assuming that a part of nodes in a network are driven by the threshold mechanism, while the remaining nodes exhibit imitation behavior governed by their rationality (under the game-theoretic framework). Our results reveal that the spreading dynamics are determined by the payoff of adoption. For positive payoffs, increasing the density of highly rational nodes can promote the adoption process, accompanied by a hybrid phase transition. The degree of rationality can regulate the spreading speed, with less rational imitators slowing down the spread. We further find that the results are opposite for negative payoffs of adoption. This model may provide valuable insights into understanding the complex dynamics of social contagion phenomena in real-world social networks."}, "https://arxiv.org/abs/2408.05165": {"title": "Higher-Order Temporal Network Prediction and Interpretation", "link": "https://arxiv.org/abs/2408.05165", "description": "arXiv:2408.05165v1 Announce Type: new \nAbstract: A social interaction (so-called higher-order event/interaction) can be regarded as the activation of the hyperlink among the corresponding individuals. Social interactions can be, thus, represented as higher-order temporal networks, that record the higher-order events occurring at each time step over time. The prediction of higher-order interactions is usually overlooked in traditional temporal network prediction methods, where a higher-order interaction is regarded as a set of pairwise interactions. The prediction of future higher-order interactions is crucial to forecast and mitigate the spread the information, epidemics and opinion on higher-order social contact networks. In this paper, we propose novel memory-based models for higher-order temporal network prediction. By using these models, we aim to predict the higher-order temporal network one time step ahead, based on the network observed in the past. Importantly, we also intent to understand what network properties and which types of previous interactions enable the prediction. The design and performance analysis of these models are supported by our analysis of the memory property of networks, e.g., similarity of the network and activity of a hyperlink over time respectively. Our models assume that a target hyperlink's future activity (active or not) depends the past activity of the target link and of all or selected types of hyperlinks that overlap with the target. We then compare the performance of both models with a baseline utilizing a pairwise temporal network prediction method. In eight real-world networks, we find that both models consistently outperform the baseline and the refined model tends to perform the best. Our models also reveal how past interactions of the target hyperlink and different types of hyperlinks that overlap with the target contribute to the prediction of the target's future activity."}, "https://arxiv.org/abs/2307.13544": {"title": "A model for efficient dynamical ranking in networks", "link": "https://arxiv.org/abs/2307.13544", "description": "arXiv:2307.13544v2 Announce Type: replace \nAbstract: We present a physics-inspired method for inferring dynamic rankings in directed temporal networks - networks in which each directed and timestamped edge reflects the outcome and timing of a pairwise interaction. The inferred ranking of each node is real-valued and varies in time as each new edge, encoding an outcome like a win or loss, raises or lowers the node's estimated strength or prestige, as is often observed in real scenarios including sequences of games, tournaments, or interactions in animal hierarchies. Our method works by solving a linear system of equations and requires only one parameter to be tuned. As a result, the corresponding algorithm is scalable and efficient. We test our method by evaluating its ability to predict interactions (edges' existence) and their outcomes (edges' directions) in a variety of applications, including both synthetic and real data. Our analysis shows that in many cases our method's performance is better than existing methods for predicting dynamic rankings and interaction outcomes."}, "https://arxiv.org/abs/2408.05459": {"title": "A Versatile Framework for Attributed Network Clustering via K-Nearest Neighbor Augmentation", "link": "https://arxiv.org/abs/2408.05459", "description": "arXiv:2408.05459v1 Announce Type: new \nAbstract: Attributed networks containing entity-specific information in node attributes are ubiquitous in modeling social networks, e-commerce, bioinformatics, etc. Their inherent network topology ranges from simple graphs to hypergraphs with high-order interactions and multiplex graphs with separate layers. An important graph mining task is node clustering, aiming to partition the nodes of an attributed network into k disjoint clusters such that intra-cluster nodes are closely connected and share similar attributes, while inter-cluster nodes are far apart and dissimilar. It is highly challenging to capture multi-hop connections via nodes or attributes for effective clustering on multiple types of attributed networks. In this paper, we first present AHCKA as an efficient approach to attributed hypergraph clustering (AHC). AHCKA includes a carefully-crafted K-nearest neighbor augmentation strategy for the optimized exploitation of attribute information on hypergraphs, a joint hypergraph random walk model to devise an effective AHC objective, and an efficient solver with speedup techniques for the objective optimization. The proposed techniques are extensible to various types of attributed networks, and thus, we develop ANCKA as a versatile attributed network clustering framework, capable of attributed graph clustering (AGC), attributed multiplex graph clustering (AMGC), and AHC. Moreover, we devise ANCKA with algorithmic designs tailored for GPU acceleration to boost efficiency. We have conducted extensive experiments to compare our methods with 19 competitors on 8 attributed hypergraphs, 16 competitors on 6 attributed graphs, and 16 competitors on 3 attributed multiplex graphs, all demonstrating the superb clustering quality and efficiency of our methods."}, "https://arxiv.org/abs/2408.05469": {"title": "Temporal network modeling with online and hidden vertices based on the birth and death process", "link": "https://arxiv.org/abs/2408.05469", "description": "arXiv:2408.05469v1 Announce Type: new \nAbstract: Complex networks have played an important role in describing real complex systems since the end of the last century. Recently, research on real-world data sets reports intermittent interaction among social individuals. In this paper, we pay attention to this typical phenomenon of intermittent interaction by considering the state transition of network vertices between online and hidden based on the birth and death process. By continuous-time Markov theory, we show that both the number of each vertex's online neighbors and the online network size are stable and follow the homogeneous probability distribution in a similar form, inducing similar statistics as well. In addition, all propositions are verified via simulations. Moreover, we also present the degree distributions based on small-world and scale-free networks and find some regular patterns by simulations. The application in fitting real networks is discussed."}, "https://arxiv.org/abs/2408.05623": {"title": "Behavioral and Topological Heterogeneities in Network Versions of Schelling\\textquotesingle s Segregation Model", "link": "https://arxiv.org/abs/2408.05623", "description": "arXiv:2408.05623v1 Announce Type: new \nAbstract: Agent-based models of residential segregation have been of persistent interest to various research communities since their origin with James Sakoda and popularization by Thomas Schelling. Frequently, these models have sought to elucidate the extent to which the collective dynamics of individual preferences may cause segregation to emerge. This open question has sustained relevance in U.S. jurisprudence. Previous investigation of heterogeneity of behaviors (preferences) by Xie & Zhou has shown reductions in segregation. Meanwhile, previous investigation of heterogeneity of social network topologies by Gandica, Gargiulo, and Carletti has shown no significant impact to observed segregation levels. In the present study, we examined effects of the concurrent presence of both behavioral and topological heterogeneities in network segregation models. Simulations were conducted using both Schelling\\textquotesingle s and Xie & Zhou\\textquotesingle s preference models on 2D lattices with varied levels of densification to create topological heterogeneities (i.e., clusters, hubs). Results show a richer variety of outcomes, including novel differences in resultant segregation levels and hub composition. Notably, with concurrent increased representations of heterogeneous preferences and heterogenous topologies, reduced levels of segregation emerge. Simultaneously, we observe a novel dynamic of segregation between tolerance levels as highly tolerant nodes take residence in dense areas and push intolerant nodes to sparse areas mimicking the urban-rural divide."}, "https://arxiv.org/abs/2408.05700": {"title": "Quantification of the Self-Excited Emotion Dynamics in Online Interactions", "link": "https://arxiv.org/abs/2408.05700", "description": "arXiv:2408.05700v1 Announce Type: new \nAbstract: Emotions are essential for guiding human behavior, particularly in social interactions. In modern societies, a growing share of human interactions are taking place online which has been shown to amplify and distort the expression and perception of emotions. However, the entanglement across different emotions is not fully understood. We use a multivariate Hawkes self-excited point process to model and calibrate the temporal expressions of six basic emotions in YouTube live chats. This allows us to understand interdependencies among emotions, but also to disentangle the influence from the video content and social interactions with peers. Positive emotions are found to be more contagious, while negative emotions tend to leave a longer-lasting impression on users' memories. Furthermore, we quantify the endogeneity of online emotion dynamics and find that peer interactions drive user emotional expressions 3-5 times more than passive content consumption. This underscores the powerful incentives of social interactions and the potential risk of emotional manipulation through the use of modern chatbots."}, "https://arxiv.org/abs/2408.05755": {"title": "Effect of Perturbation and Topological Structure on Synchronization Dynamics in Multilayer Networks", "link": "https://arxiv.org/abs/2408.05755", "description": "arXiv:2408.05755v1 Announce Type: new \nAbstract: The way the topological structure transforms from a decoupled to a coupled state in multiplex networks has been extensively studied through both analytical and numerical approaches, often utilizing models of artificial networks. These studies typically assume uniform interconnections between layers to simplify the analytical treatment of structural properties in multiplex networks. However, this assumption is not applicable for real networks, where the heterogeneity of link weights is an intrinsic characteristic. Therefore, in this paper, link weights are calculated considering the node's reputation and the impact of the inter-layer link weights are assessed on the overall network's structural characteristics. These characteristics include synchronization time, stability of synchronization, and the second-smallest eigenvalue of the Laplacian matrix (algebraic connectivity). Our findings reveal that the perturbation in link weights (intra-layer) causes a transition in the algebraic connectivity whereas variation in inter-layer link weights has a significant impact on the synchronization stability and synchronization time in the multiplex networks. This analysis is different from the predictions made under the assumption of equal inter-layer link weights."}, "https://arxiv.org/abs/2408.06341": {"title": "Is it a work or leisure travel? Applying text classification to identify work-related travel on social networks", "link": "https://arxiv.org/abs/2408.06341", "description": "arXiv:2408.06341v1 Announce Type: new \nAbstract: In today's digital era, the use of Social Networks (SNs) and Location-Based SNs (LBSNs) has become integral for travelers seeking Points of Interest (POI) and sharing travel experiences. This trend is supported by the fact that a significant majority of American travelers utilize SNs during their trips. However, the abundance of information available on these platforms presents a challenge in identifying the best options. To address this issue, Recommender Systems (RS) are commonly employed to suggest POIs based on user history, with the integration of contextual information enhancing the quality of recommendations. Notably, incorporating user travel purpose, which is often overlooked but holds potential in characterizing travelers' behavior, can lead to more tailored recommendations. In this study, we propose a model to predict whether a trip is leisure or work-related, utilizing state-of-the-art Automatic Text Classification (ATC) models such as BERT, RoBERTa, and BART to enhance the understanding of user travel purposes and improve recommendation accuracy in specific travel scenarios."}, "https://arxiv.org/abs/2408.05242": {"title": "FLASH: Federated Learning-Based LLMs for Advanced Query Processing in Social Networks through RAG", "link": "https://arxiv.org/abs/2408.05242", "description": "arXiv:2408.05242v1 Announce Type: cross \nAbstract: Our paper introduces a novel approach to social network information retrieval and user engagement through a personalized chatbot system empowered by Federated Learning GPT. The system is designed to seamlessly aggregate and curate diverse social media data sources, including user posts, multimedia content, and trending news. Leveraging Federated Learning techniques, the GPT model is trained on decentralized data sources to ensure privacy and security while providing personalized insights and recommendations. Users interact with the chatbot through an intuitive interface, accessing tailored information and real-time updates on social media trends and user-generated content. The system's innovative architecture enables efficient processing of input files, parsing and enriching text data with metadata, and generating relevant questions and answers using advanced language models. By facilitating interactive access to a wealth of social network information, this personalized chatbot system represents a significant advancement in social media communication and knowledge dissemination."}, "https://arxiv.org/abs/2408.05243": {"title": "SocFedGPT: Federated GPT-based Adaptive Content Filtering System Leveraging User Interactions in Social Networks", "link": "https://arxiv.org/abs/2408.05243", "description": "arXiv:2408.05243v1 Announce Type: cross \nAbstract: Our study presents a multifaceted approach to enhancing user interaction and content relevance in social media platforms through a federated learning framework. We introduce personalized GPT and Context-based Social Media LLM models, utilizing federated learning for privacy and security. Four client entities receive a base GPT-2 model and locally collected social media data, with federated aggregation ensuring up-to-date model maintenance. Subsequent modules focus on categorizing user posts, computing user persona scores, and identifying relevant posts from friends' lists. A quantifying social engagement approach, coupled with matrix factorization techniques, facilitates personalized content suggestions in real-time. An adaptive feedback loop and readability score algorithm also enhance the quality and relevance of content presented to users. Our system offers a comprehensive solution to content filtering and recommendation, fostering a tailored and engaging social media experience while safeguarding user privacy."}, "https://arxiv.org/abs/2408.05245": {"title": "Improved Adaboost Algorithm for Web Advertisement Click Prediction Based on Long Short-Term Memory Networks", "link": "https://arxiv.org/abs/2408.05245", "description": "arXiv:2408.05245v1 Announce Type: cross \nAbstract: This paper explores an improved Adaboost algorithm based on Long Short-Term Memory Networks (LSTMs), which aims to improve the prediction accuracy of user clicks on web page advertisements. By comparing it with several common machine learning algorithms, the paper analyses the advantages of the new model in ad click prediction. It is shown that the improved algorithm proposed in this paper performs well in user ad click prediction with an accuracy of 92%, which is an improvement of 13.6% compared to the highest of 78.4% among the other three base models. This significant improvement indicates that the algorithm is more capable of capturing user behavioural characteristics and time series patterns. In addition, this paper evaluates the model's performance on other performance metrics, including accuracy, recall, and F1 score. The results show that the improved Adaboost algorithm based on LSTM is significantly ahead of the traditional model in all these metrics, which further validates its effectiveness and superiority. Especially when facing complex and dynamically changing user behaviours, the model is able to better adapt and make accurate predictions. In order to ensure the practicality and reliability of the model, this study also focuses on the accuracy difference between the training set and the test set. After validation, the accuracy of the proposed model on these two datasets only differs by 1.7%, which is a small difference indicating that the model has good generalisation ability and can be effectively applied to real-world scenarios."}, "https://arxiv.org/abs/2408.05313": {"title": "Discrete-time immunization number", "link": "https://arxiv.org/abs/2408.05313", "description": "arXiv:2408.05313v1 Announce Type: cross \nAbstract: We introduce a discrete-time immunization version of the SEIS compartment model of infection by a contagious disease, with an extended latency and protective period. The population is modeled by a graph $H$ where vertices represent individuals and edges exist between individuals with close connections. Our objective is to clear the population of infection while minimizing the maximum number of immunizations that occur at each time-step. We prove that this minimum is bounded above by a natural function of the pathwidth of $H$. In addition to our general results, we also focus on the case where the latency and protective periods last for one time-step. In this case, we characterize graphs that require only one immunization per time-step, provide a useful tool for proving lower bounds, and show that, for any tree $T$, there is a subdivision of $T$ that requires at most two immunizations per time-step."}, "https://arxiv.org/abs/2408.05474": {"title": "A Structural Feature-Based Approach for Comprehensive Graph Classification", "link": "https://arxiv.org/abs/2408.05474", "description": "arXiv:2408.05474v1 Announce Type: cross \nAbstract: The increasing prevalence of graph-structured data across various domains has intensified greater interest in graph classification tasks. While numerous sophisticated graph learning methods have emerged, their complexity often hinders practical implementation. In this article, we address this challenge by proposing a method that constructs feature vectors based on fundamental graph structural properties. We demonstrate that these features, despite their simplicity, are powerful enough to capture the intrinsic characteristics of graphs within the same class. We explore the efficacy of our approach using three distinct machine learning methods, highlighting how our feature-based classification leverages the inherent structural similarities of graphs within the same class to achieve accurate classification. A key advantage of our approach is its simplicity, which makes it accessible and adaptable to a broad range of applications, including social network analysis, bioinformatics, and cybersecurity. Furthermore, we conduct extensive experiments to validate the performance of our method, showing that it not only reveals a competitive performance but in some cases surpasses the accuracy of more complex, state-of-the-art techniques. Our findings suggest that a focus on fundamental graph features can provide a robust and efficient alternative for graph classification, offering significant potential for both research and practical applications."}, "https://arxiv.org/abs/2408.05587": {"title": "COARA will not save science from the tyranny of administrative evaluation", "link": "https://arxiv.org/abs/2408.05587", "description": "arXiv:2408.05587v1 Announce Type: cross \nAbstract: Could the generalized adoption of the COARA principles make administrative research evaluation socially desirable and solve all its problems? The answer to this question is no. To reach this conclusion, two main arguments are discussed. The first characterises COARA as a form of 'technocracy' perfectly consistent with a neo-liberal view of research. The second consists in the adoption of Philip Kitcher's idea of well-ordered science. It is argued that administrative evaluation of research, even if correct on the basis of COARA principles, is at odds with the principles of well-ordered science since it cannot escape neither the tyranny of expertise nor the tyranny of ignorance. These two arguments allow to suggest limiting administrative evaluation to the bare minimum (recruitment of researchers and funding of projects), and to focus attention mainly to the fairness of evaluation procedures."}, "https://arxiv.org/abs/2408.05832": {"title": "Compact majority-minority districts almost never exist", "link": "https://arxiv.org/abs/2408.05832", "description": "arXiv:2408.05832v1 Announce Type: cross \nAbstract: For a uniformly distributed population, we show that with high probability, any majority-minority voting district containing a fraction of the population necessarily exhibits a tiny Polsby-Popper score."}, "https://arxiv.org/abs/2403.18957": {"title": "Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models", "link": "https://arxiv.org/abs/2403.18957", "description": "arXiv:2403.18957v2 Announce Type: replace-cross \nAbstract: Online user generated content games (UGCGs) are increasingly popular among children and adolescents for social interaction and more creative online entertainment. However, they pose a heightened risk of exposure to explicit content, raising growing concerns for the online safety of children and adolescents. Despite these concerns, few studies have addressed the issue of illicit image-based promotions of unsafe UGCGs on social media, which can inadvertently attract young users. This challenge arises from the difficulty of obtaining comprehensive training data for UGCG images and the unique nature of these images, which differ from traditional unsafe content. In this work, we take the first step towards studying the threat of illicit promotions of unsafe UGCGs. We collect a real-world dataset comprising 2,924 images that display diverse sexually explicit and violent content used to promote UGCGs by their game creators. Our in-depth studies reveal a new understanding of this problem and the urgent need for automatically flagging illicit UGCG promotions. We additionally create a cutting-edge system, UGCG-Guard, designed to aid social media platforms in effectively identifying images used for illicit UGCG promotions. This system leverages recently introduced large vision-language models (VLMs) and employs a novel conditional prompting strategy for zero-shot domain adaptation, along with chain-of-thought (CoT) reasoning for contextual identification. UGCG-Guard achieves outstanding results, with an accuracy rate of 94% in detecting these images used for the illicit promotion of such games in real-world scenarios."}, "https://arxiv.org/abs/2408.06376": {"title": "Interrupted time series analysis of clickbait on worldwide news websites, 2016-2023", "link": "https://arxiv.org/abs/2408.06376", "description": "arXiv:2408.06376v1 Announce Type: new \nAbstract: Clickbait is deceptive text that can manipulate web browsing, creating an information gap between a link and target page that literally baits a user into clicking. Clickbait detection continues to be well studied, but analyses of clickbait overall on the web are limited. A dataset was built consisting of 451,033,388 clickbait scores produced by a clickbait detector which analyzed links and headings on primarily English news pages from the Common Crawl. On this data, 5 segmented regression models were fit on 5 major news events and averaged clickbait scores. COVID and the 2020 US Election appeared to influence clickbait levels."}, "https://arxiv.org/abs/2408.06393": {"title": "Leading by the Nodes: A Survey of Film Industry Network Analysis and Datasets", "link": "https://arxiv.org/abs/2408.06393", "description": "arXiv:2408.06393v1 Announce Type: new \nAbstract: This paper presents a comprehensive survey of network analysis research on the film industry, aiming to evaluate its emergence as a field of study and identify potential areas for further research. Many foundational network studies made use of the abundant data from the Internet Movie Database (IMDb) to test network methodologies. This survey focuses more specifically on examining research that employs network analysis to evaluate the film industry itself, revealing the social and business relationships involved in film production, distribution, and consumption. The paper adopts a classification approach based on node type and conducts network analyses of the reviewed literature, mapping citation and co-authorship networks. The findings provide insights into the structure and interconnectedness of the field, offering potential clusters of debates and shedding light on the institutional, geographical, and demographic characteristics shaping this field. In addition, this survey contributes to understanding film industry network analysis and informs researchers interested in network methods within the film industry and related cultural sectors."}, "https://arxiv.org/abs/2408.06658": {"title": "ComGPT: Detecting Local Community Structure with Large Language Models", "link": "https://arxiv.org/abs/2408.06658", "description": "arXiv:2408.06658v1 Announce Type: new \nAbstract: Large language models (LLMs), like GPT, have demonstrated the ability to understand graph structures and have achieved excellent performance in various graph reasoning tasks such as node classification. So far, how to leverage LLMs to better detect local communities remains underexplored. Local community detection algorithms based on seed expansion often face a seed-dependent problem, community diffusion, and free rider effect. Using LLMs to solve existing local community work problems faces the following challenges: existing graph encoding methods fail to provide LLMs with sufficient community-related graph information; LLMs lack domain knowledge in mining communities. To address these issues, we improve graph encoding by supplementing community knowledge to enhance the ability of graph encoding to express graph information. Additionally, we design the NSG (Node Selection Guide) prompt to enhance LLMs' understanding of community characteristics, aiming to alleviate the seed-dependent problem, community diffusion, and free rider effect. Based on the graph encoding and NSG prompt, we present a GPT-guided local community detection, called ComGPT. ComGPT iteratively selects potential nodes from the detected community's neighbors and subsequently employs GPT to choose the node that optimally integrates into the detected community from these selected potential nodes. Experimental results demonstrate that ComGPT outperforms the comparison algorithms, thereby confirming the effectiveness of the designed graph encoding and prompt."}, "https://arxiv.org/abs/2408.06344": {"title": "The Signatures of Ideal Flow Networks", "link": "https://arxiv.org/abs/2408.06344", "description": "arXiv:2408.06344v1 Announce Type: cross \nAbstract: An Ideal Flow Network (IFN) is a strongly connected network where relative flows are preserved (irreducible premagic matrix). IFN can be decomposed into canonical cycles to form a string code called network signature. A network signature can be composed back into an IFN by assignment and merging operations. Using string manipulations on network signatures, we can derive total flow, link values, sum of rows and columns, and probability matrices and test for irreducibility."}, "https://arxiv.org/abs/2408.06434": {"title": "New Coevolution Dynamic as an Optimization Strategy in Group Problem Solving", "link": "https://arxiv.org/abs/2408.06434", "description": "arXiv:2408.06434v1 Announce Type: cross \nAbstract: In society, it is common to face problems that require collaboration with other people, from everyday challenges to complex tasks, such as group projects at work. In this context, the search for more effective problem-solving strategies becomes a topic of great interest. This paper presents a new dynamic that integrates coevolution into the NK model together with an Erd\\\"os-R\\'enyi random network, allowing more than one neighbor update. We explore how this coevolution can be employed as an optimization strategy for solving group problems. In recent coevolution models, only one agent is removed and another is added to the neighborhood of influence. Here, we allow $L$ agents to be changed, which had a large impact on the system performance. In the analysis of the results, we consider rewiring as a way for the target agent to obtain information from individuals or groups that were not part of its neighborhood. Our simulations demonstrate that coevolution can produces gain on the computational cost."}, "https://arxiv.org/abs/2408.06797": {"title": "Stunned by Sleeping Beauty: How Prince Probability updates his forecast upon their fateful encounter", "link": "https://arxiv.org/abs/2408.06797", "description": "arXiv:2408.06797v1 Announce Type: cross \nAbstract: The Sleeping Beauty problem is a puzzle in probability theory that has gained much attention since Elga's discussion of it [Elga, Adam, Analysis 60 (2), p.143-147 (2000)]. Sleeping Beauty is put asleep, and a coin is tossed. If the outcome of the coin toss is Tails, Sleeping Beauty is woken up on Monday, put asleep again and woken up again on Tuesday (with no recollection of having woken up on Monday). If the outcome is Heads, Sleeping Beauty is woken up on Monday only. Each time Sleeping Beauty is woken up, she is asked what her belief is that the outcome was Heads. What should Sleeping Beauty reply? In literature arguments have been given for both 1/3 and 1/2 as the correct answer. In this short note we argue using simple Bayesian probability theory why 1/3 is the right answer, and not 1/2. Briefly, when Sleeping Beauty awakens, her being awake is nontrivial extra information that leads her to update her beliefs about Heads to 1/3. We strengthen our claim by considering an additional observer, Prince Probability, who may or may not meet Sleeping Beauty. If he meets Sleeping Beauty while she is awake, he lowers his credence in Heads to 1/3. We also briefly consider the credence in Heads of a Sleeping Beauty who knows that she is dreaming (and thus asleep)."}, "https://arxiv.org/abs/2408.06839": {"title": "Geotree of Geodetector: An Anatomy of Knowledge Diffusion of a Novel Statistic", "link": "https://arxiv.org/abs/2408.06839", "description": "arXiv:2408.06839v1 Announce Type: cross \nAbstract: The growing number of citations to original publications highlighted their utility across academia, but the dissemination of knowledge from tacit conceptualization to scientific publications and its global applications remains understudied, and the prediction of knowledge trends in a disciplinary context is rare. Addressing the gaps, this paper constructed a tree-like hierarchical model (Geotree) to dissect the knowledge evolution paths of the Geodetector theory (a case) using the Web of Science citation database. Our results revealed that the knowledge evolution of 932 citations to Geodetector was partitioned into periods: a budding period of initial theoretical exploration, a growing period for emerging topics in application, and a mature period marked by significant citation growth. Our test R2 of the predicting model over the next decade, considering the tree-like hierarchy across research directions and disciplines, was 100% higher than that of the other two (from 0.29 to 0.58). The knowledge spreading, from China to North America in 2011, Europe in 2012, Oceania in 2017, South America in 2018, and Africa in 2019, was more associated with a country s production of scientific publications (q-statistic = 0.307***) than its income level. The Geotree modeling of two other cases from space science and physics confirmed the reliability of the source publication-based approach in tracking knowledge diffusion. Our established research framework enriched the current methodology of information science and provided valuable references for policymakers and scholars to enhance their decision-making processes."}, "https://arxiv.org/abs/2408.06900": {"title": "Entendre, a Social Bot Detection Tool for Niche, Fringe, and Extreme Social Media", "link": "https://arxiv.org/abs/2408.06900", "description": "arXiv:2408.06900v1 Announce Type: cross \nAbstract: Social bots-automated accounts that generate and spread content on social media-are exploiting vulnerabilities in these platforms to manipulate public perception and disseminate disinformation. This has prompted the development of public bot detection services; however, most of these services focus primarily on Twitter, leaving niche platforms vulnerable. Fringe social media platforms such as Parler, Gab, and Gettr often have minimal moderation, which facilitates the spread of hate speech and misinformation. To address this gap, we introduce Entendre, an open-access, scalable, and platform-agnostic bot detection framework. Entendre can process a labeled dataset from any social platform to produce a tailored bot detection model using a random forest classification approach, ensuring robust social bot detection. We exploit the idea that most social platforms share a generic template, where users can post content, approve content, and provide a bio (common data features). By emphasizing general data features over platform-specific ones, Entendre offers rapid extensibility at the expense of some accuracy. To demonstrate Entendre's effectiveness, we used it to explore the presence of bots among accounts posting racist content on the now-defunct right-wing platform Parler. We examined 233,000 posts from 38,379 unique users and found that 1,916 unique users (4.99%) exhibited bot-like behavior. Visualization techniques further revealed that these bots significantly impacted the network, amplifying influential rhetoric and hashtags (e.g., #qanon, #trump, #antilgbt). These preliminary findings underscore the need for tools like Entendre to monitor and assess bot activity across diverse platforms."}, "https://arxiv.org/abs/2408.07052": {"title": "The News Comment Gap and Algorithmic Agenda Setting in Online Forums", "link": "https://arxiv.org/abs/2408.07052", "description": "arXiv:2408.07052v1 Announce Type: cross \nAbstract: The disparity between news stories valued by journalists and those preferred by readers, known as the \"News Gap\", is well-documented. However, the difference in expectations regarding news related user-generated content is less studied. Comment sections, hosted by news websites, are popular venues for reader engagement, yet still subject to editorial decisions. It is thus important to understand journalist vs reader comment preferences and how these are served by various comment ranking algorithms that represent discussions differently. We analyse 1.2 million comments from Austrian newspaper Der Standard to understand the \"News Comment Gap\" and the effects of different ranking algorithms. We find that journalists prefer positive, timely, complex, direct responses, while readers favour comments similar to article content from elite authors. We introduce the versatile Feature-Oriented Ranking Utility Metric (FORUM) to assess the impact of different ranking algorithms and find dramatic differences in how they prioritise the display of comments by sentiment, topical relevance, lexical diversity, and readability. Journalists can exert substantial influence over the discourse through both curatorial and algorithmic means. Understanding these choices' implications is vital in fostering engaging and civil discussions while aligning with journalistic objectives, especially given the increasing legal scrutiny and societal importance of online discourse."}, "https://arxiv.org/abs/2009.07525": {"title": "Detectability of hierarchical communities in networks", "link": "https://arxiv.org/abs/2009.07525", "description": "arXiv:2009.07525v2 Announce Type: replace \nAbstract: We study the problem of recovering a planted hierarchy of partitions in a network. The detectability of a single planted partition has previously been analysed in detail and a phase transition has been identified below which the partition cannot be detected. Here we show that, in the hierarchical setting, there exist additional phases in which the presence of multiple consistent partitions can either help or hinder detection. Accordingly, the detectability limit for non-hierarchical partitions typically provides insufficient information about the detectability of the complete hierarchical structure, as we highlight with several constructive examples."}, "https://arxiv.org/abs/2402.13033": {"title": "Enhancing Node Representations for Real-World Complex Networks with Topological Augmentation", "link": "https://arxiv.org/abs/2402.13033", "description": "arXiv:2402.13033v2 Announce Type: replace-cross \nAbstract: Graph augmentation methods play a crucial role in improving the performance and enhancing generalisation capabilities in Graph Neural Networks (GNNs). Existing graph augmentation methods mainly perturb the graph structures, and are usually limited to pairwise node relations. These methods cannot fully address the complexities of real-world large-scale networks, which often involve higher-order node relations beyond only being pairwise. Meanwhile, real-world graph datasets are predominantly modelled as simple graphs, due to the scarcity of data that can be used to form higher-order edges. Therefore, reconfiguring the higher-order edges as an integration into graph augmentation strategies lights up a promising research path to address the aforementioned issues. In this paper, we present Topological Augmentation (TopoAug), a novel graph augmentation method that builds a combinatorial complex from the original graph by constructing virtual hyperedges directly from the raw data. TopoAug then produces auxiliary node features by extracting information from the combinatorial complex, which are used for enhancing GNN performances on downstream tasks. We design three diverse virtual hyperedge construction strategies to accompany the construction of combinatorial complexes: (1) via graph statistics, (2) from multiple data perspectives, and (3) utilising multi-modality. Furthermore, to facilitate TopoAug evaluation, we provide 23 novel real-world graph datasets across various domains including social media, biology, and e-commerce. Our empirical study shows that TopoAug consistently and significantly outperforms GNN baselines and other graph augmentation methods, across a variety of application contexts, which clearly indicates that it can effectively incorporate higher-order node relations into the graph augmentation for real-world complex networks."}, "https://arxiv.org/abs/2408.07135": {"title": "The Adaptive Strategies of Anti-Kremlin Digital Dissent in Telegram during the Russian Invasion of Ukraine", "link": "https://arxiv.org/abs/2408.07135", "description": "arXiv:2408.07135v1 Announce Type: new \nAbstract: During Russia's invasion of Ukraine in February 2022, Telegram became an essential social media platform for Kremlin-sponsored propaganda dissemination. Over time, Anti-Kremlin Russian opposition channels have also emerged as a prominent voice of dissent against the state-sponsored propaganda. This study examines the dynamics of Anti-Kremlin content on Telegram over seven phases of the invasion, inspired by the concept of breach in narrative theory. A data-driven, computational analysis of emerging topics revealed the Russian economy, combat updates, international politics, and Russian domestic affairs, among others. Using a common set of statistical contrasts by phases of the invasion, a longitudinal analysis of topic prevalence allowed us to examine associations with documented offline events and viewer reactions, suggesting an adaptive breach-oriented communications strategy that maintained viewer interest. Viewer approval of those events that threaten Kremlin control suggests that Telegram levels the online playing field for the opposition, surprising given the Kremlin's suppression of free speech offline."}, "https://arxiv.org/abs/2408.07297": {"title": "Estimate collective cooperativeness of driving agents in mixed traffic flow", "link": "https://arxiv.org/abs/2408.07297", "description": "arXiv:2408.07297v1 Announce Type: new \nAbstract: Cooperation is a ubiquitous phenomenon in many natural, social, and engineered systems that contain multiple agents. Characterizing and quantifying cooperativeness of driving agents is of interest and significance for two reasons. Theoretically, it will enhance the understanding of micro-macro connections and emergence of cooperation in mixed traffic. Pragmatically, this understanding will benefit the design and operations of automated and mixed-autonomy transportation systems. However, it remains unclear how the cooperativeness can be accurately defined and quantified from empirical data, and it remains open when and to what extent collective cooperativeness exists. This paper is intended to fill the gap. We propose a unified conceptual framework to estimate collective cooperativeness of driving agents leveraging a recent behavioral equilibrium model of mixed autonomy traffic (Li et al. 2022a). This framework is interpretable, theoretically consistent, and enables quantifying collective cooperativeness of traffic agents from trajectory data. We apply the framework to multilane freeway traffic employing NGSIM I-80 trajectory data set and careful data selection. Our case study indicates the existence of collective cooperativeness between human-driven passenger cars and trucks in real-world traffic and reveals its other properties that are otherwise unknown."}, "https://arxiv.org/abs/2408.07300": {"title": "Deeply nested structure of mythological traditions worldwide", "link": "https://arxiv.org/abs/2408.07300", "description": "arXiv:2408.07300v1 Announce Type: new \nAbstract: All human societies present unique narratives that shape their customs and beliefs. Despite cultural differences, some symbolic elements (e.g., heroes and tricksters) are common across many cultures. Here, we reconcile these seemingly contradictory aspects by analyzing mythological themes and traditions at various scales. Our analysis revealed that global mythologies exhibit both geographic and thematic nesting across different scales, manifesting in a layered structure. The largest geographic clusters correspond to the New and Old Worlds, which further divide into smaller bioregions. This hierarchical manifestation closely aligns with historical human migration patterns at a large scale, suggesting that narrative themes were carried through deep history. At smaller scales, the correspondence with bioregions indicates that these themes are locally adapted and diffused into variations across cultures over time. Our approach, which treats myths and traditions as random variables without considering factors like geography, history, or story lineage, suggests that the manifestation of mythology has been well-preserved over time and thus opens exciting research avenues to reconstruct historical patterns and provide insight into human cultural narratives."}, "https://arxiv.org/abs/2408.07369": {"title": "ProCom: A Few-shot Targeted Community Detection Algorithm", "link": "https://arxiv.org/abs/2408.07369", "description": "arXiv:2408.07369v1 Announce Type: new \nAbstract: Targeted community detection aims to distinguish a particular type of community in the network. This is an important task with a lot of real-world applications, e.g., identifying fraud groups in transaction networks. Traditional community detection methods fail to capture the specific features of the targeted community and detect all types of communities indiscriminately. Semi-supervised community detection algorithms, emerged as a feasible alternative, are inherently constrained by their limited adaptability and substantial reliance on a large amount of labeled data, which demands extensive domain knowledge and manual effort.\n  In this paper, we address the aforementioned weaknesses in targeted community detection by focusing on few-shot scenarios. We propose ProCom, a novel framework that extends the ``pre-train, prompt'' paradigm, offering a low-resource, high-efficiency, and transferable solution. Within the framework, we devise a dual-level context-aware pre-training method that fosters a deep understanding of latent communities in the network, establishing a rich knowledge foundation for downstream task. In the prompt learning stage, we reformulate the targeted community detection task into pre-training objectives, allowing the extraction of specific knowledge relevant to the targeted community to facilitate effective and efficient inference. By leveraging both the general community knowledge acquired during pre-training and the specific insights gained from the prompt communities, ProCom exhibits remarkable adaptability across different datasets. We conduct extensive experiments on five benchmarks to evaluate the ProCom framework, demonstrating its SOTA performance under few-shot scenarios, strong efficiency, and transferability across diverse datasets."}, "https://arxiv.org/abs/2408.07549": {"title": "Stability of heterogeneous linear and nonlinear car-following models", "link": "https://arxiv.org/abs/2408.07549", "description": "arXiv:2408.07549v1 Announce Type: new \nAbstract: Stop-and-go waves in road traffic are complex collective phenomena with significant implications for traffic engineering, safety and the environment. Despite decades of research, understanding and controlling these dynamics remains challenging. This article examines two classes of heterogeneous car-following models with quenched disorder to shed light on the underlying mechanisms that drive traffic instability and stop-and-go dynamics. Specifically, a scaled heterogeneity model and an additive heterogeneity model are investigated, each of which affects the stability of linear and nonlinear car-following models differently. We derive general linear stability conditions which we apply to specific models and illustrate by simulation. The study provides insights into the role of individual heterogeneity in vehicle behaviour and its influence on traffic stability."}, "https://arxiv.org/abs/2408.07606": {"title": "Confrontation of capitalism and socialism in Wikipedia networks", "link": "https://arxiv.org/abs/2408.07606", "description": "arXiv:2408.07606v1 Announce Type: new \nAbstract: We introduce the Ising Network Opinion Formation (INOF) model and apply it for the analysis of networks of 6 Wikipedia language editions. In the model, Ising spins are placed at network nodes/articles and the steady-state opinion polarization of spins is determined from the Monte Carlo iterations in which a given spin orientation is determined by in-going links from other spins. The main consideration is done for opinion confrontation between {\\it capitalism, imperialism} (blue opinion) and {\\it socialism, communism} (red opinion). These nodes have fixed spin/opinion orientation while other nodes achieve their steady-state opinions in the process of Monte Carlo iterations. We find that the global network opinion favors {\\it socialism, communism} for all 6 editions. The model also determines the opinion preferences for world countries and political leaders, showing good agreement with heuristic expectations. We also present results for opinion competition between {\\it Christianity} and {\\it Islam}, and USA Democratic and Republican parties. We argue that the INOF approach can find numerous applications for directed complex networks."}, "https://arxiv.org/abs/2408.07161": {"title": "Fast and Accurate Algorithms to Calculate Expected Modularity in Probabilistic Networks", "link": "https://arxiv.org/abs/2408.07161", "description": "arXiv:2408.07161v1 Announce Type: cross \nAbstract: Modularity maximization is a widely used community detection technique for deterministic networks. However, little research has been performed to develop efficient modularity calculation algorithms for probabilistic networks. Particularly, it is challenging to efficiently calculate expected modularity when all possible worlds are considered. To address this problem, we propose two algorithms, namely $\\mathrm{PWP}^{\\mathrm{EMOD}}$ and $\\mathrm{APWP}^{\\mathrm{EMOD}}$, partitioning the possible worlds based on their modularities to significantly reduce the number of probability calculations. We evaluate the accuracy and time efficiency of our algorithms through comprehensive experiments."}, "https://arxiv.org/abs/2408.07191": {"title": "Joint Graph Rewiring and Feature Denoising via Spectral Resonance", "link": "https://arxiv.org/abs/2408.07191", "description": "arXiv:2408.07191v1 Announce Type: cross \nAbstract: Graph neural networks (GNNs) take as input the graph structure and the feature vectors associated with the nodes. Both contain noisy information about the labels. Here we propose joint denoising and rewiring (JDR)--an algorithm to jointly denoise the graph structure and features, which can improve the performance of any downstream algorithm. We do this by defining and maximizing the alignment between the leading eigenspaces of graph and feature matrices. To approximately solve this computationally hard problem, we propose a heuristic that efficiently handles real-world graph datasets with many classes and different levels of homophily or heterophily. We experimentally verify the effectiveness of our approach on synthetic data and real-world graph datasets. The results show that JDR consistently outperforms existing rewiring methods on node classification tasks using GNNs as downstream models."}, "https://arxiv.org/abs/2408.07237": {"title": "Neural embedding of beliefs reveals the role of relative dissonance in human decision-making", "link": "https://arxiv.org/abs/2408.07237", "description": "arXiv:2408.07237v1 Announce Type: cross \nAbstract: Beliefs serve as the foundation for human cognition and decision-making. They guide individuals in deriving meaning from their lives, shaping their behaviors, and forming social connections. Therefore, a model that encapsulates beliefs and their interrelationships is crucial for quantitatively studying the influence of beliefs on our actions. Despite its importance, research on the interplay between human beliefs has often been limited to a small set of beliefs pertaining to specific issues, with a heavy reliance on surveys or experiments. Here, we propose a method for extracting nuanced relations between thousands of beliefs by leveraging large-scale user participation data from an online debate platform and mapping these beliefs to an embedding space using a fine-tuned large language model (LLM). This belief embedding space effectively encapsulates the interconnectedness of diverse beliefs as well as polarization across various social issues. We discover that the positions within this belief space predict new beliefs of individuals. Furthermore, we find that the relative distance between one's existing beliefs and new beliefs can serve as a quantitative estimate of cognitive dissonance, allowing us to predict new beliefs. Our study highlights how modern LLMs, when combined with collective online records of human beliefs, can offer insights into the fundamental principles that govern human belief formation and decision-making processes."}, "https://arxiv.org/abs/2408.07298": {"title": "Improving the use of social contact studies in epidemic modelling", "link": "https://arxiv.org/abs/2408.07298", "description": "arXiv:2408.07298v1 Announce Type: cross \nAbstract: Social contact studies, investigating social contact patterns in a population sample, have been an important contribution for epidemic models to better fit real life epidemics. A contact matrix $M$, having the \\emph{mean} number of contacts between individuals of different age groups as its elements, is estimated and used in combination with a multitype epidemic model to produce better data fitting and also giving more appropriate expressions for $R_0$ and other model outcomes. However, $M$ does not capture \\emph{variation} in contacts \\emph{within} each age group, which is often large in empirical settings. Here such variation within age groups is included in a simple way by dividing each age group into two halves: the socially active and the socially less active. The extended contact matrix, and its associated epidemic model, empirically show that acknowledging variation in social activity within age groups has a substantial impact on modelling outcomes such as $R_0$ and the final fraction $\\tau$ getting infected. In fact, the variation in social activity within age groups is often more important for data fitting than the division into different age groups itself. However, a difficulty with heterogeneity in social activity is that social contact studies typically lack information on if mixing with respect to social activity is assortative or not, i.e.\\ do socially active tend to mix more with other socially active or more with socially less active? The analyses show that accounting for heterogeneity in social activity improves the analyses irrespective of if such mixing is assortative or not, but the different assumptions gives rather different output. Future social contact studies should hence also try to infer the degree of assortativity of contacts with respect to social activity."}, "https://arxiv.org/abs/2102.03569": {"title": "Opinion Dynamics Incorporating Higher-Order Interactions", "link": "https://arxiv.org/abs/2102.03569", "description": "arXiv:2102.03569v3 Announce Type: replace \nAbstract: The issue of opinion sharing and formation has received considerable attention in the academic literature, and a few models have been proposed to study this problem. However, existing models are limited to the interactions among nearest neighbors, ignoring those second, third, and higher-order neighbors, despite the fact that higher-order interactions occur frequently in real social networks. In this paper, we develop a new model for opinion dynamics by incorporating long-range interactions based on higher-order random walks. We prove that the model converges to a fixed opinion vector, which may differ greatly from those models without higher-order interactions. Since direct computation of the equilibrium opinion is computationally expensive, which involves the operations of huge-scale matrix multiplication and inversion, we design a theoretically convergence-guaranteed estimation algorithm that approximates the equilibrium opinion vector nearly linearly in both space and time with respect to the number of edges in the graph. We conduct extensive experiments on various social networks, demonstrating that the new algorithm is both highly efficient and effective."}, "https://arxiv.org/abs/2311.14739": {"title": "Finding the Fairest Voting System using Likelihood Analysis", "link": "https://arxiv.org/abs/2311.14739", "description": "arXiv:2311.14739v2 Announce Type: replace \nAbstract: This paper takes a statistical approach to determine which of 10 voting systems, or preference aggregation rules, is the fairest based on their probabilistic likelihood of violating Arrow's five social choice criteria. The voting systems considered are: Plurality, Borda, Dowdall, Top Two (Plurality Runoff), Instant Runoff, Coombs, Baldwin, Copeland, Pairwise Majority, and Minimax. This paper builds upon the work of Dougherty and Heckelman (2020) by computing violation frequencies for elections with a greater number of alternatives. Elections with up to 50,000 voters and between three and six alternatives are simulated using both Impartial Culture and Impartial Anonymous Culture. The results of these simulations produce new IIA violation likelihoods for each method and show that Pairwise Majority is the most likely to jointly satisfy all five of Arrow's criteria. Furthermore, of the systems that satisfy transitivity, the Baldwin method is most likely to jointly satisfy all five of Arrow's criteria in elections with three alternatives. As the number of alternatives increase, the joint satisfaction frequencies decrease rapidly for all systems."}, "https://arxiv.org/abs/2110.06382": {"title": "A Survey of Open Source User Activity Traces with Applications to User Mobility Characterization and Modeling", "link": "https://arxiv.org/abs/2110.06382", "description": "arXiv:2110.06382v3 Announce Type: replace-cross \nAbstract: The current state-of-the-art in user mobility research has extensively relied on open-source mobility traces captured from pedestrian and vehicular activity through a variety of communication technologies as users engage in a wide-range of applications, including connected healthcare, localization, social media, e-commerce, etc. Most of these traces are feature-rich and diverse, not only in the information they provide, but also in how they can be used and leveraged. This diversity poses two main challenges for researchers and practitioners who wish to make use of available mobility datasets. First, it is quite difficult to get a bird's eye view of the available traces without spending considerable time looking them up. Second, once they have found the traces, they still need to figure out whether the traces are adequate to their needs.\n  The purpose of this survey is three-fold. It proposes a taxonomy to classify open-source mobility traces including their mobility mode, data source and collection technology. It then uses the proposed taxonomy to classify existing open-source mobility traces and finally, highlights three case studies using popular publicly available datasets to showcase how our taxonomy can tease out feature sets in traces to help determine their applicability to specific use-cases."}, "https://arxiv.org/abs/2211.06028": {"title": "Dynamic Curing and Network Design in SIS Epidemic Processes", "link": "https://arxiv.org/abs/2211.06028", "description": "arXiv:2211.06028v2 Announce Type: replace-cross \nAbstract: This paper studies efficient algorithms for dynamic curing policies and the corresponding network design problems to guarantee the fast extinction of epidemic spread in a susceptible-infected-susceptible (SIS) model. We consider a Markov process-based SIS epidemic model. We provide a computationally efficient curing algorithm based on the curing policy proposed by Drakopoulos, Ozdaglar, and Tsitsiklis (2014). Since the corresponding optimization problem is NP-hard, finding optimal policies is intractable for large graphs. We provide approximation guarantees on the curing budget of the proposed dynamic curing algorithm. We also present a curing algorithm fair to demographic groups.\n  When the total infection rate is high, the original curing policy includes a waiting period in which no measure is taken to mitigate the spread until the rate slows down. To avoid the waiting period, we study network design problems to reduce the total infection rate by deleting edges or reducing the weight of edges. Then the curing processes become continuous since the total infection rate is restricted by network design. We provide algorithms with provable guarantees for the considered network design problems. In summary, the proposed curing and network design algorithms together provide an effective and computationally efficient approach that mitigates SIS epidemic spread in networks."}, "https://arxiv.org/abs/2310.17712": {"title": "Community Detection Guarantees Using Embeddings Learned by Node2Vec", "link": "https://arxiv.org/abs/2310.17712", "description": "arXiv:2310.17712v2 Announce Type: replace-cross \nAbstract: Embedding the nodes of a large network into an Euclidean space is a common objective in modern machine learning, with a variety of tools available. These embeddings can then be used as features for tasks such as community detection/node clustering or link prediction, where they achieve state of the art performance. With the exception of spectral clustering methods, there is little theoretical understanding for commonly used approaches to learning embeddings. In this work we examine the theoretical properties of the embeddings learned by node2vec. Our main result shows that the use of $k$-means clustering on the embedding vectors produced by node2vec gives weakly consistent community recovery for the nodes in (degree corrected) stochastic block models. We also discuss the use of these embeddings for node and link prediction tasks. We demonstrate this result empirically, and examine how this relates to other embedding tools for network data."}, "https://arxiv.org/abs/2408.07731": {"title": "Polarization dynamics: a study of individuals shifting between political communities on social media", "link": "https://arxiv.org/abs/2408.07731", "description": "arXiv:2408.07731v1 Announce Type: new \nAbstract: Individuals engaging on social media often tend to establish online communities where interactions predominantly occur among like-minded peers. While considerable efforts have been devoted to studying and delineating these communities, there has been limited attention directed towards individuals who diverge from these patterns. In this study, we examine the community structure of re-post networks within the context of a polarized political environment at two different times. We specifically identify individuals who consistently switch between opposing communities and analyze the key features that distinguish them. Our investigation focuses on two crucial aspects of these users: the topological properties of their interactions and the political bias in the content of their posts. Our analysis is based on a dataset comprising 2 million tweets related to US President Donald Trump, coupled with data from over 100 000 individual user accounts spanning the 2020 US presidential election year. Our findings indicate that individuals who switch communities exhibit disparities compared to those who remain within the same communities, both in terms of the topological aspects of their interaction patterns (pagerank, degree, betweenness centrality.) and in the sentiment bias of their content towards Donald Trump."}, "https://arxiv.org/abs/2408.07900": {"title": "Network analysis reveals news press landscape and asymmetric user polarization", "link": "https://arxiv.org/abs/2408.07900", "description": "arXiv:2408.07900v1 Announce Type: new \nAbstract: Unlike traditional media, online news platforms allow users to consume content that suits their tastes and to facilitate interactions with other people. However, as more personalized consumption of information and interaction with like-minded users increase, ideological bias can inadvertently increase and contribute to the formation of echo chambers, reinforcing the polarization of opinions. Although the structural characteristics of polarization among different ideological groups in online spaces have been extensively studied, research into how these groups emotionally interact with each other has not been as thoroughly explored. From this perspective, we investigate both structural and affective polarization between news media user groups on Naver News, South Korea's largest online news portal, during the period of 2022 Korean presidential election. By utilizing the dataset comprising 333,014 articles and over 36 million user comments, we uncover two distinct groups of users characterized by opposing political leanings and reveal significant bias and polarization among them. Additionally, we reveal the existence of echo chambers within co-commenting networks and investigate the asymmetric affective interaction patterns between the two polarized groups. Classification task of news media articles based on the distinct comment response patterns support the notion that different political groups may employ distinct communication strategies. Our approach based on network analysis on large-scale comment dataset offers novel insights into characteristics of user polarization in the online news platforms and the nuanced interaction nature between user groups."}, "https://arxiv.org/abs/2408.07927": {"title": "Motif analysis and passing behavior in football passing networks", "link": "https://arxiv.org/abs/2408.07927", "description": "arXiv:2408.07927v1 Announce Type: new \nAbstract: The strategic orchestration of football matchplays profoundly influences game outcomes, motivating a surge in research aimed at uncovering tactical nuances through social network analysis. In this paper, we delve into the microscopic intricacies of cooperative player interactions by focusing on triadic motifs within passing networks. Employing a dataset compiled from 3,199 matches across 18 premier football competitions, we identify successful passing activities and construct passing networks for both home and away teams. Our findings highlight a pronounced disparity in passing efficiency, with home teams demonstrating superior performance relative to away teams. Through the identification and analysis of 3-motifs, we find that the motifs with more bidirectional links are more significant. It reveals that footballers exhibit a strong tendency towards backward passes rather than direct forward attacks. Comparing the results of games, we find that some motifs are related to the goal difference. It indicates that direct and effective forward passing significantly amplifies a team's offensive capabilities, whereas an abundance of passbacks portends an elevated risk of offensive futility. These revelations affirm the efficacy of network motif analysis as a potent analytical tool for unveiling the foundational components of passing dynamics among footballers and for decoding the complex tactical behaviors and interaction modalities that underpin team performance."}, "https://arxiv.org/abs/2408.08025": {"title": "Disagreement as a way to study misinformation and its effects", "link": "https://arxiv.org/abs/2408.08025", "description": "arXiv:2408.08025v1 Announce Type: new \nAbstract: Misinformation - false or misleading information - is considered a significant societal concern due to its associated \"misinformation effects,\" such as political polarization, erosion of trust in institutions, problematic behavior, and public health challenges. However, the prevailing concept is misaligned with what is studied. While misinformation focuses on instances of information about factual matters, the broad spectrum of effects often manifests at a societal level and is shaped by a wide range of interdependent factors such as identity, values, opinions, epistemologies, and disagreements. Unsurprisingly, misinformation effects can occur without the prevalence of misinformation, and misinformation does not necessarily increase the effects studied. Here, we propose using disagreement - conflicting attitudes and beliefs between individuals and communities - as a way to study misinformation effects because it addresses the identified conceptual limitations of misinformation. Furthermore, unlike misinformation, disagreement does not require researchers to determine whether a given information is false or misleading. Thus, it can be studied and, more importantly, measured without the need to make a normative judgment about a given information, even when the specific topic is entirely removed, as we show in a longitudinal disagreement measurement. We demonstrate that disagreement, as a holistic concept, provides better explanations for the occurrence of misinformation effects, enhances precision in developing appropriate interventions, and offers a promising approach for evaluating them through quantification. Finally, we show how disagreement addresses current misinformation research questions and conclude with recommendations for research practice."}, "https://arxiv.org/abs/2408.08122": {"title": "Cluster Formations of Free and Congested Flows in Urban Road Networks", "link": "https://arxiv.org/abs/2408.08122", "description": "arXiv:2408.08122v1 Announce Type: new \nAbstract: Understanding traffic behavior is crucial for enhancing the stable functioning and safety of transportation systems. Previous percolation-based transportation studies have analyzed transition behaviors from free-flow to traffic-jam states, with a focus on robustness and resilience during congestion. However, relatively less attention is paid to the percolation analysis of the free-flow states, specifically how free-flow clusters form and grow. In this study, we investigate the percolation patterns of two opposing traffic scenarios -- traffic jam state and free-flow state -- within the same road network using Chengdu taxi data and compare their percolating behaviors. Our analysis reveals differences between the two scenarios in the growth patterns of the giant connected component (GCC), which is captured by a persistent gap between the GCC size curves, particularly during peak hours. We attribute these disparities to a long-range spatial correlation of traffic speed within a road network. Empirically, we find distinct long-range spatial correlations in traffic, using rescaled taxi speeds on roads, and we examine their relationship with each percolation pattern. Our analysis provides an integrated view of traffic dynamics and uncovers intrinsic traffic correlations within urban areas that drive these intriguing percolation patterns. Our findings also offer valuable metrics for effective traffic management and accident prevention strategies, aligning with urban transportation safety and reliability goals. These insights are beneficial for assessing and designing resilient urban road networks that maintain functionality under stress, ultimately improving the reliability of traffic assessments and reducing accidents."}, "https://arxiv.org/abs/2408.07977": {"title": "Cortical network reconfiguration aligns with shifts of basal ganglia and cerebellar influence", "link": "https://arxiv.org/abs/2408.07977", "description": "arXiv:2408.07977v1 Announce Type: cross \nAbstract: Mammalian functional architecture flexibly adapts, transitioning from integration where information is distributed across the cortex, to segregation where information is focal in densely connected communities of brain regions. This flexibility in cortical brain networks is hypothesized to be driven by control signals originating from subcortical pathways, with the basal ganglia shifting the cortex towards integrated processing states and the cerebellum towards segregated states. In a sample of healthy human participants (N=242), we used fMRI to measure temporal variation in global brain networks while participants performed two tasks with similar cognitive demands (Stroop and Multi-Source Inference Task (MSIT)). Using the modularity index, we determined cortical networks shifted from integration (low modularity) at rest to high modularity during easier i.e. congruent (segregation). Increased task difficulty (incongruent) resulted in lower modularity in comparison to the easier counterpart indicating more integration of the cortical network. Influence of basal ganglia and cerebellum was measured using eigenvector centrality. Results correlated with decreases and increases in cortical modularity respectively, with only the basal ganglia influence preceding cortical integration. Our results support the theory the basal ganglia shifts cortical networks to integrated states due to environmental demand. Cerebellar influence correlates with shifts to segregated cortical states, though may not play a causal role."}, "https://arxiv.org/abs/2408.08036": {"title": "Analysing pandemics in phase-space", "link": "https://arxiv.org/abs/2408.08036", "description": "arXiv:2408.08036v1 Announce Type: cross \nAbstract: Based on the SIRD-model a new model including time-delay is proposed for a description of the outbreak of the novel coronavirus Sars-CoV-2 pandemic. All data were analysed by representing all quantities as a function of the susceptible population, as opposed to the usual dependence on time. The total number of deaths could be predicted for the first, second and third wave of the pandemic in Germany with an accuracy of about 10\\%, shortly after the maximum of infectious people was reached. By using the presentation in phase space, it could be shown that a classical SEIRD- and SIRD-model with constant parameters will not be able to describe the first wave of the pandemic accurately."}, "https://arxiv.org/abs/2408.08217": {"title": "RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train and Deploy Edge Classifiers for Computational Social Science", "link": "https://arxiv.org/abs/2408.08217", "description": "arXiv:2408.08217v1 Announce Type: cross \nAbstract: Large language models (LLMs) have enhanced our ability to rapidly analyze and classify unstructured natural language data. However, concerns regarding cost, network limitations, and security constraints have posed challenges for their integration into work processes. In this study, we adopt a systems design approach to employing LLMs as imperfect data annotators for downstream supervised learning tasks, introducing novel system intervention measures aimed at improving classification performance. Our methodology outperforms LLM-generated labels in seven of eight tests, demonstrating an effective strategy for incorporating LLMs into the design and deployment of specialized, supervised learning models present in many industry use cases."}, "https://arxiv.org/abs/2302.06357": {"title": "Real-World Networks are Low-Dimensional: Theoretical and Practical Assessment", "link": "https://arxiv.org/abs/2302.06357", "description": "arXiv:2302.06357v3 Announce Type: replace \nAbstract: Detecting the dimensionality of graphs is a central topic in machine learning. While the problem has been tackled empirically as well as theoretically, existing methods have several drawbacks. On the one hand, empirical tools are computationally heavy and lack theoretical foundation. On the other hand, theoretical approaches do not apply to graphs with heterogeneous degree distributions, which is often the case for complex real-world networks.\n  To address these drawbacks, we consider geometric inhomogeneous random graphs (GIRGs) as a random graph model, which captures a variety of properties observed in practice. Our first result shows that the clustering coefficient of GIRGs scales inverse exponentially with respect to the number of dimensions, when the latter is at most logarithmic in $n$. This gives a first theoretical explanation for the low dimensionality of real-world networks as observed by Almagro et al. in 2022. We further use these insights to derive a linear-time algorithm for determining the dimensionality of a given GIRG and prove that our algorithm returns the correct number of dimensions with high probability GIRG. Our algorithm bridges the gap between theory and practice, as it not only comes with a rigorous proof of correctness but also yields results comparable to that of prior empirical approaches, as indicated by our experiments on real-world instances."}, "https://arxiv.org/abs/2408.08317": {"title": "Thermal and microclimatic behavior of OASIS schoolyard paving materials", "link": "https://arxiv.org/abs/2408.08317", "description": "arXiv:2408.08317v1 Announce Type: new \nAbstract: As part of its Resilience Strategy, the City of Paris' OASIS program aims to contribute ot its adaptation to heatwaves and climate change by transforming schoolyards into climate shelters, namely via desealing and greening. In this context, a variety of alternative pavement materials have been proposed to replace the initial schoolyard pavement, composed of an asphalt sidewalk structure. In the context of the EU-funded ERDF UIA OASIS project, the thermal performance of these alternative materials and their impact in terms of urban cooling was explored. To this aim, five samples of reference and innovative schoolyard pavements were studied in the lab under heat-wave conditions. Alternative green, biosourced, recycled and reflective pavement solutions were compared to standard fine-aggregate asphalt concrete. Their performance was evaluated with regards to their contribution to the urban heat island phenomenon and to pedestrian heat stress, account for the typical use schedule of schoolyards. Green and biosourced materials were found to perform well for both indicators, while the standard and recycled solutions had poor UHI performance but had limited negative effects on daytime heat stress. The reflective pavement had better UHI performance but had high radiosity during daytime which can negatively affect pedestrian heat stress."}, "https://arxiv.org/abs/2408.08504": {"title": "Percolation analysis of spatio-temporal distribution of population in Seoul and Helsinki", "link": "https://arxiv.org/abs/2408.08504", "description": "arXiv:2408.08504v1 Announce Type: new \nAbstract: Spatio-temporal distribution of urban population is crucial to understand the structure and dynamics of cities. Most studies, however, have focused on the microscopic structure of cities such as their few most crowded areas. In this work, we investigate the macroscopic structure of cities such as their clusters of highly populated areas. To do this, we analyze the spatial distribution of urban population and its intra-day dynamics in Seoul and Helsinki with a percolation framework. We observe that the growth patterns of the largest clusters in the real and randomly shuffled population data are significantly different, and highly populated areas during daytime are denser and form larger clusters than during nighttime. An analysis of the cluster size distributions at percolation criticality shows that their power-law exponents during daytime are lower than during nighttime, indicating that the spatial distributions of urban population during daytime and nighttime fall into different universality classes. Finally measuring the area-perimeter fractal dimension of the collection of clusters at criticality demonstrates that the fractal dimensions during daytime are higher than during nighttime, indicating that the perimeters of clusters during daytime are more rough than during nighttime. Our findings suggest that even the same city can have qualitatively different spatial distributions of population over time, and propose a way to quantitatively compare the macrostructure of cities based on population distribution data."}, "https://arxiv.org/abs/2408.08602": {"title": "Discrete-time SIS Social Contagion Processes on Hypergraphs", "link": "https://arxiv.org/abs/2408.08602", "description": "arXiv:2408.08602v1 Announce Type: new \nAbstract: Recent research on social contagion processes has revealed the limitations of traditional networks, which capture only pairwise relationships, to characterize complex multiparty relationships and group influences properly. Social contagion processes on higher-order networks (simplicial complexes and general hypergraphs) have therefore emerged as a novel frontier. In this work, we investigate discrete-time Susceptible-Infected-Susceptible (SIS) social contagion processes occurring on weighted and directed hypergraphs and their extensions to bivirus cases and general higher-order SIS processes with the aid of tensor algebra. Our focus lies in comprehensively characterizing the healthy state and endemic equilibria within this framework. The emergence of bistability or multistability behavior phenomena, where multiple equilibria coexist and are simultaneously locally asymptotically stable, is demonstrated in view of the presence of the higher-order interaction. The novel sufficient conditions of the appearance for system behaviors, which are determined by both (higher-order) network topology and transition rates, are provided to assess the likelihood of the SIS social contagion processes causing an outbreak. More importantly, given the equilibrium is locally stable, an explicit domain of attraction associated with the system parameters is constructed. Moreover, a learning method to estimate the transition rates is presented. In the end, the attained theoretical results are supplemented via numerical examples. Specifically, we evaluate the effectiveness of the networked SIS social contagion process by comparing it with the $2^n$-state Markov chain model. These numerical examples are given to highlight the performance of parameter learning algorithms and the system behaviors of the discrete-time SIS social contagion process."}, "https://arxiv.org/abs/2408.08685": {"title": "Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?", "link": "https://arxiv.org/abs/2408.08685", "description": "arXiv:2408.08685v1 Announce Type: cross \nAbstract: Graph neural networks (GNNs) are vulnerable to adversarial perturbations, especially for topology attacks, and many methods that improve the robustness of GNNs have received considerable attention. Recently, we have witnessed the significant success of large language models (LLMs), leading many to explore the great potential of LLMs on GNNs. However, they mainly focus on improving the performance of GNNs by utilizing LLMs to enhance the node features. Therefore, we ask: Will the robustness of GNNs also be enhanced with the powerful understanding and inference capabilities of LLMs? By presenting the empirical results, we find that despite that LLMs can improve the robustness of GNNs, there is still an average decrease of 23.1% in accuracy, implying that the GNNs remain extremely vulnerable against topology attack. Therefore, another question is how to extend the capabilities of LLMs on graph adversarial robustness. In this paper, we propose an LLM-based robust graph structure inference framework, LLM4RGNN, which distills the inference capabilities of GPT-4 into a local LLM for identifying malicious edges and an LM-based edge predictor for finding missing important edges, so as to recover a robust graph structure. Extensive experiments demonstrate that LLM4RGNN consistently improves the robustness across various GNNs. Even in some cases where the perturbation ratio increases to 40%, the accuracy of GNNs is still better than that on the clean graph."}, "https://arxiv.org/abs/2408.08744": {"title": "Noisy information channel mediated prevention of the tragedy of the commons", "link": "https://arxiv.org/abs/2408.08744", "description": "arXiv:2408.08744v1 Announce Type: cross \nAbstract: Synergy between evolutionary dynamics of cooperation and fluctuating state of shared resource being consumed by the cooperators is essential for averting the tragedy of the commons. Not only in humans, but also in the cognitively-limited organisms, this interplay between the resource and the cooperation is ubiquitously witnessed. The strategically interacting players engaged in such game-environment feedback scenarios naturally pick strategies based on their perception of the environmental state. Such perception invariably happens through some sensory information channels that the players are endowed with. The unfortunate reality is that any sensory channel must be noisy due to various factors; consequently, the perception of the environmental state becomes faulty rendering the players incapable of adopting the strategy that they otherwise would. Intriguingly, situation is not as bad as it sounds. Here we introduce the hitherto neglected information channel between players and the environment into the paradigm of stochastic evolutionary games with a view to bringing forward the counterintuitive possibility of emergence and sustenance of cooperation on account of the noise in the channel. Our primary study is in the simplest non-trivial setting of two-state stochastically fluctuating resource harnessed by a large unstructured population of cooperators and defectors adopting either memory-1 strategies or reactive strategies while engaged in repeated two-player interactions. The effect of noisy information channel in enhancing the cooperation in reactive-strategied population is unprecedented. We find that the propensity of cooperation in the population is inversely related to the mutual information (normalized by the channel capacity) of the corresponding information channel."}, "https://arxiv.org/abs/2408.08832": {"title": "Personalized graph feature-based multi-omics data integration for cancer subtype identification", "link": "https://arxiv.org/abs/2408.08832", "description": "arXiv:2408.08832v1 Announce Type: cross \nAbstract: Cancer is a highly heterogeneous disease with significant variability in molecular features and clinical outcomes, making diagnosis and treatment challenging. In recent years, high-throughput omic technologies have facilitated the discovery of mechanisms underlying various cancer subtypes by providing diverse omics data, such as gene expression, DNA methylation, and miRNA expression. However, the complexity and heterogeneity of multi-omics data present significant challenges for their integration in exploring cancer subtypes. Various methods have been proposed to address these challenges. In this paper, we propose a novel and straightforward approach for identifying cancer subtypes by integrating patient-specific subnetworks features from different omics data. We construct patient-specific induced subnetwork using a random walk with restart algorithm from patient similarity networks (PSNs) and compute nine structural properties that capture essential network topology. These features are integrated across the three omic datasets to form comprehensive patient profiles. K-means clustering is then applied for cancer subtype identification. We evaluate our approach on five cancer datasets, including breast invasive carcinoma, colon adenocarcinoma, glioblastoma multiforme, kidney renal clear cell carcinoma, and lung squamous cell carcinoma, for three different omic data types. The evaluation shows that our method produces promising and effective results, demonstrating competitive or superior performance compared to existing methods and underscoring its potential for advancing personalized cancer diagnosis and treatment."}, "https://arxiv.org/abs/2408.08861": {"title": "The computational power of a human society: a new model of social evolution", "link": "https://arxiv.org/abs/2408.08861", "description": "arXiv:2408.08861v1 Announce Type: cross \nAbstract: Social evolutionary theory seeks to explain increases in the scale and complexity of human societies, from origins to present. Over the course of the twentieth century, social evolutionary theory largely fell out of favor as a way of investigating human history, just as advances in complex systems science and computer science saw the emergence of powerful new conceptions of complex systems, and in particular new methods of measuring complexity. We propose that these advances in our understanding of complex systems and computer science should be brought to bear on our investigations into human history. To that end, we present a new framework for modeling how human societies co-evolve with their biotic environments, recognizing that both a society and its environment are computers. This leads us to model the dynamics of each of those two systems using the same, new kind of computational machine, which we define here. For simplicity, we construe a society as a set of interacting occupations and technologies. Similarly, under such a model, a biotic environment is a set of interacting distinct ecological and climatic processes. This provides novel ways to characterize social complexity, which we hope will cast new light on the archaeological and historical records. Our framework also provides a natural way to formalize both the energetic (thermodynamic) costs required by a society as it runs, and the ways it can extract thermodynamic resources from the environment in order to pay for those costs -- and perhaps to grow with any left-over resources."}, "https://arxiv.org/abs/2306.10759": {"title": "SGFormer: Simplifying and Empowering Transformers for Large-Graph Representations", "link": "https://arxiv.org/abs/2306.10759", "description": "arXiv:2306.10759v5 Announce Type: replace-cross \nAbstract: Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points. Transformers, as an emerging class of foundation encoders for graph-structured data, have shown promising performance on small graphs due to its global attention capable of capturing all-pair influence beyond neighboring nodes. Even so, existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated models by stacking deep multi-head attentions. In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. This encourages us to rethink the design philosophy for Transformers on large graphs, where the global attention is a computation overhead hindering the scalability. We frame the proposed scheme as Simplified Graph Transformers (SGFormer), which is empowered by a simple attention model that can efficiently propagate information among arbitrary nodes in one layer. SGFormer requires none of positional encodings, feature/graph pre-processing or augmented loss. Empirically, SGFormer successfully scales to the web-scale graph ogbn-papers100M and yields up to 141x inference acceleration over SOTA Transformers on medium-sized graphs. Beyond current results, we believe the proposed methodology alone enlightens a new technical path of independent interest for building Transformers on large graphs."}, "https://arxiv.org/abs/2402.11494": {"title": "Graph Out-of-Distribution Generalization via Causal Intervention", "link": "https://arxiv.org/abs/2402.11494", "description": "arXiv:2402.11494v2 Announce Type: replace-cross \nAbstract: Out-of-distribution (OOD) generalization has gained increasing attentions for learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation with distribution shifts. The challenge is that distribution shifts on graphs involve intricate interconnections between nodes, and the environment labels are often absent in data. In this paper, we adopt a bottom-up data-generative perspective and reveal a key observation through causal analysis: the crux of GNNs' failure in OOD generalization lies in the latent confounding bias from the environment. The latter misguides the model to leverage environment-sensitive correlations between ego-graph features and target nodes' labels, resulting in undesirable generalization on new unseen nodes. Built upon this analysis, we introduce a conceptually simple yet principled approach for training robust GNNs under node-level distribution shifts, without prior knowledge of environment labels. Our method resorts to a new learning objective derived from causal inference that coordinates an environment estimator and a mixture-of-expert GNN predictor. The new approach can counteract the confounding bias in training data and facilitate learning generalizable predictive relations. Extensive experiment demonstrates that our model can effectively enhance generalization with various types of distribution shifts and yield up to 27.4\\% accuracy improvement over state-of-the-arts on graph OOD generalization benchmarks. Source codes are available at https://github.com/fannie1208/CaNet."}, "https://arxiv.org/abs/2408.08880": {"title": "The Distribution of Vaccine among Countries in the Case of a Pandemic", "link": "https://arxiv.org/abs/2408.08880", "description": "arXiv:2408.08880v1 Announce Type: new \nAbstract: Purpose: Only few companies were able to produce vaccine again COVID-19. Thus, one producer supplied it to many countries. The distribution was not effective. Some countries overstocked the vaccine while other countries were not able to buy enough. The purpose of the present paper is to provide with a frame such that one producer distributes the vaccine to a set of countries in a way that the shortage is minimized. Methodology: The consumption of the countries are approximated by regression functions taking into account the saturation of the process. The distribution of the vaccine is determined by MIP models of operations research. Findings: Effective distribution of vaccine can be obtained for even a large number of countries. Both the level of the shortage and the number of the consecutive shortage days in a country can be controlled. Practical implications: A group of countries can act as a single partner of a pharmaceutical company. They can get a steady supply. The company gets a well-organized delivery plan. Social implications: More people can be saved because of the steady supply of the vaccine. Originality: The paper develops a new concept for the fair distribution of vaccines between countries. This concept and the method derived from it can be applied in the event of future pandemics on a global scale."}, "https://arxiv.org/abs/2408.08885": {"title": "Physical and geographic analysis of the urban cooling potential", "link": "https://arxiv.org/abs/2408.08885", "description": "arXiv:2408.08885v1 Announce Type: new \nAbstract: The performance of a number of urban cooling techniques has been thoroughly studied by the scientific community. However, decision-makers lack the tools to spatialize their deployment as part of their urban cooling and climate change adaptation strategies. Among other indicators, a spatial assessment of the cooling potential for a technique in a given area is lacking. To this end, we analyze the physical mechanisms on which these techniques are based and identify corresponding geographical indicators that influence their cooling performance. Solar irradiance, existing material properties and underground infrastructure stand out as essential indicators for this purpose."}, "https://arxiv.org/abs/2408.08889": {"title": "Aircrew rostering workload patterns and associated fatigue and sleepiness scores in short/medium haul flights under RBAC 117 rules in Brazil", "link": "https://arxiv.org/abs/2408.08889", "description": "arXiv:2408.08889v1 Announce Type: new \nAbstract: The relationships between workload and fatigue or sleepiness are investigated through the analysis of rosters and responses to questionnaires from Brazilian aircrews, taken from Fadig\\^ometro database. The approach includes temporal markers - coinciding with Samn-Perelli (SP) and Karolinska Sleepiness Scale (KSS) responses - where SAFTE-FAST model outcomes are calculated. The model results follow the increase of fatigue and sleepiness perceptions during the dawn (0h00 to 05h59), but underestimate the self-rated scores during the evening (18h00 to 23h59). On the other hand, the KSS scores fit the relative risk of pilot errors, representing a reasonable proxy for risk assessment. Linear relationships obtained between workload metrics, computed within 168-hours prior to the responses, and self-rated SP and KSS scores provide a consistent method to estimate accumulated fatigue and sleepiness. Considering 7149 rosters of 2023, the duty time ($DT$), the number of flight sectors ($N_{CREW}$) and the sum of flight sectors with sit periods longer than one hour ($N_{CREW}+N_{SIT}$) are associated with 70.1%/60.6% of the highest predicted scores of SP/KSS. Applying the mitigations $DT\\leq44h$, $N_{CREW}\\leq15$ and $N_{CREW}+N_{SIT}\\leq19$ for every 168-hour interval yields a significant decrease in the higher values of SP/KSS with minimal impact on aircrew productivity."}, "https://arxiv.org/abs/2408.08890": {"title": "Pedestrian Safety and Traffic Operations Around Near-Side Versus Far-Side Transit Stops: Emerging Observational Evidence from Utah", "link": "https://arxiv.org/abs/2408.08890", "description": "arXiv:2408.08890v1 Announce Type: new \nAbstract: This research project's objective was to investigate the impacts of transit stop location (near-side versus far-side) on pedestrian safety and traffic operations. Three different video-based behavioral observation data collections at signalized intersections in Utah were utilized, studying: (1) transit vehicle stop events and transit rider crossing behaviors and vehicle conflicts; (2) pedestrian conflicts with right-turning vehicles (driver/pedestrian reactions, conflict severity); and (3) pedestrian crossing behaviors (crossing location, crossing behaviors). These outcomes were statistically compared for near-side versus far-side transit stop locations. Far-side transit stops appear better for general traffic operations. Although transit departure delays are more likely and impactful at far-side stops, actions can be taken to improve transit operations there. On the other hand, far-side transit stops appear to be worse for pedestrian safety, corroborating prior crash-based research findings. Specifically, conflicts at far-side stops were more severe, and drivers were less likely to slow or stop for pedestrians. Reconciling these differing findings likely requires improving pedestrian safety at some far-side transit stops, and prioritizing safety over operational efficiency at other near-side transit stops."}, "https://arxiv.org/abs/2408.08891": {"title": "Application of Information Theory in Rumor Spreading Modeling Considering Polarization in Complex Networks", "link": "https://arxiv.org/abs/2408.08891", "description": "arXiv:2408.08891v1 Announce Type: new \nAbstract: It is proposed a model that makes use of the Entropy concept and the Jensen-Shannon information divergence function to simulate computationally the dissemination of opinions on a Barab\\'asi-Albert (BA) scale-free network. The simulation considers individual memory, ideological proximity between individuals, and distortion influenced by polarization."}, "https://arxiv.org/abs/2408.08893": {"title": "Why Honor Heroes? Praise as a Social Signal", "link": "https://arxiv.org/abs/2408.08893", "description": "arXiv:2408.08893v1 Announce Type: new \nAbstract: Heroes are people who perform costly altruistic acts. Few people turn out to be heroes, but most people spontaneously honor heroes overtly by commenting, applauding, or enthusiastically celebrating their deeds. This behavior seems odd from an individual fitness optimization perspective. The best strategy should be to rely on others to invest time and effort in celebrations. To explain the universal propensity to pay tribute, we propose that public admiration is a way for admirers to signal that they are committed to the same values as the hero. We show that the emergence of heroic acts is an expected side-effect of this propensity."}, "https://arxiv.org/abs/2408.08898": {"title": "Estimation of marine fishing capacity of China", "link": "https://arxiv.org/abs/2408.08898", "description": "arXiv:2408.08898v1 Announce Type: new \nAbstract: By using PTP and DEA methods, the study of marine fishing capacity in China revealed that it's more accurate to consider the input index as total power rather than the number of ships. The analysis clarified that the main issue with marine fishing capacity in China is the excess total power of fishing ships. The study suggested reducing the number of ships by 35.2%, the gross tonnage by 29.8%, and the total power by 37.3% to align with the catch levels of 1999. It also proposed the idea of supplementing peak years. The PTP methodology was found to be suitable for longitudinal analysis over time, while the DEA approach is better for comparing fishing capacity across different regions at the same time. Additionally, the study indicated that practical fishing capacity tends to be underestimated, so actual reductions in capacity are usually larger than calculated values."}, "https://arxiv.org/abs/2408.08910": {"title": "Why Do Experts Favor Solar and Wind as Renewable Energies Despite their Intermittency?", "link": "https://arxiv.org/abs/2408.08910", "description": "arXiv:2408.08910v1 Announce Type: new \nAbstract: As humanity accelerates its shift to renewable energy generation, people who are not experts in renewable energy are learning about energy technologies and the energy market, which are complex. The answers to some questions will be obvious to expert practitioners but not to non-experts. One such question is Why solar and wind generation are expected to supply the bulk of future energy when they are intermittent. We learn here that once the baseline hurdles of scalability to utility scale and the underlying resources being widely available globally are satisfied, the forecasted cost of solar and wind is 2-4X lower than competing technologies, even those that are not as scalable and available. The market views intermittency as surmountable."}, "https://arxiv.org/abs/2408.08932": {"title": "Key motifs searching in complex dynamical systems", "link": "https://arxiv.org/abs/2408.08932", "description": "arXiv:2408.08932v1 Announce Type: new \nAbstract: Key network motifs searching in complex networks is one of the crucial aspects of network analysis. There has been a series of insightful findings and valuable applications for various scenarios through the analysis of network structures. However, in dynamic systems, slight changes in the choice of dynamic equations and parameters can alter the significance of motifs. The known methods are insufficient to address this issue effectively. In this paper, we introduce a concept of perturbation energy based on the system's Jacobian matrix, and define motif centrality for dynamic systems by seamlessly integrating network topology with dynamic equations. Through simulations, we observe that the key motifs obtained by the proposed energy method present better effective and accurate than them by integrating network topology methods, without significantly increasing algorithm complexity. The finding of key motifs can be used to apply for system control, such as formulating containment policies for the spread of epidemics and protecting fragile ecosystems. Additionally, it makes substantial contribution to a deeper understanding of concepts in physics, such as signal propagation and system's stability."}, "https://arxiv.org/abs/2408.09054": {"title": "From Urban Clusters to Megaregions: Mapping Australia's Evolving Urban Regions", "link": "https://arxiv.org/abs/2408.09054", "description": "arXiv:2408.09054v1 Announce Type: new \nAbstract: This study employs percolation theory to investigate the hierarchical organisation of Australian urban centres through the connectivity of their road networks. The analysis demonstrates how discrete urban clusters have developed into integrated regional entities, delineating the pivotal distance thresholds that regulate these urban transitions. The study reveals the interconnections between disparate urban clusters, shaped by their functional differentiation and historical development. Furthermore, the study identifies a dichotomy of urban agglomeration forces and a persistent spatial disconnection between Australia's wider urban landscape. This highlights the interplay between urban densification and peripheral growth. It suggests the need for new thinking on potential integrated governance structures that bridge urban development with broader social and economic policies across regional and national scales. Additionally, the study emphasises the growing importance of national coordination in Australian urban development planning to ensure regional consistency, equity, and productivity."}, "https://arxiv.org/abs/2408.09072": {"title": "Enhancing Community Detection in Networks: A Comparative Analysis of Local Metrics and Hierarchical Algorithms", "link": "https://arxiv.org/abs/2408.09072", "description": "arXiv:2408.09072v1 Announce Type: new \nAbstract: The analysis and detection of communities in network structures are becoming increasingly relevant for understanding social behavior. One of the principal challenges in this field is the complexity of existing algorithms. The Girvan-Newman algorithm, which uses the betweenness metric as a measure of node similarity, is one of the most representative algorithms in this area. This study employs the same method to evaluate the relevance of using local similarity metrics for community detection. A series of local metrics were tested on a set of networks constructed using the Girvan-Newman basic algorithm. The efficacy of these metrics was evaluated by applying the base algorithm to several real networks with varying community sizes, using modularity and NMI. The results indicate that approaches based on local similarity metrics have significant potential for community detection."}, "https://arxiv.org/abs/2408.09149": {"title": "Uncovering key predictors of high-growth firms via explainable machine learning", "link": "https://arxiv.org/abs/2408.09149", "description": "arXiv:2408.09149v1 Announce Type: new \nAbstract: Predicting high-growth firms has attracted increasing interest from the technological forecasting and machine learning communities. Most existing studies primarily utilize financial data for these predictions. However, research suggests that a firm's research and development activities and its network position within technological ecosystems may also serve as valuable predictors. To unpack the relative importance of diverse features, this paper analyzes financial and patent data from 5,071 firms, extracting three categories of features: financial features, technological features of granted patents, and network-based features derived from firms' connections to their primary technologies. By utilizing ensemble learning algorithms, we demonstrate that incorporating financial features with either technological, network-based features, or both, leads to more accurate high-growth firm predictions compared to using financial features alone. To delve deeper into the matter, we evaluate the predictive power of each individual feature within their respective categories using explainable artificial intelligence methods. Among non-financial features, the maximum economic value of a firm's granted patents and the number of patents related to a firms' primary technologies stand out for their importance. Furthermore, firm size is positively associated with high-growth probability up to a certain threshold size, after which the association plateaus. Conversely, the maximum economic value of a firm's granted patents is positively linked to high-growth probability only after a threshold value is exceeded. These findings elucidate the complex predictive role of various features in forecasting high-growth firms and could inform technological resource allocation as well as investment decisions."}, "https://arxiv.org/abs/2408.09175": {"title": "Generative Agent-Based Models for Complex Systems Research: a review", "link": "https://arxiv.org/abs/2408.09175", "description": "arXiv:2408.09175v1 Announce Type: new \nAbstract: The advent of Large Language Models (LLMs) has significantly transformed the fields of natural and social sciences. Generative Agent-Based Models (GABMs), which utilize large language models in place of real subjects, are gaining increasing public attention. Far from aiming for comprehensiveness, this paper aims to offer readers an opportunity to understand how large language models are disrupting complex systems research and behavioral sciences. In particular, we evaluate recent advancements in various domains within complex systems, encompassing network science, evolutionary game theory, social dynamics, and epidemic propagation. Additionally, we propose possible directions for future research to further advance these fields."}, "https://arxiv.org/abs/2408.09309": {"title": "Bringing Leaders of Network Sub-Groups Closer Together Does Not Facilitate Consensus", "link": "https://arxiv.org/abs/2408.09309", "description": "arXiv:2408.09309v1 Announce Type: new \nAbstract: Consensus formation is a complex process, particularly in networked groups. When individuals are incentivized to dig in and refuse to compromise, leaders may be essential to guiding the group to consensus. Specifically, the relative geodesic position of leaders (which we use as a proxy for ease of communication between leaders) could be important for reaching consensus. Additionally, groups searching for consensus can be confounded by noisy signals in which individuals are given false information about the actions of their fellow group members. We tested the effects of the geodesic distance between leaders (geodesic distance ranging from 1-4) and of noise (noise levels at 0%, 5%, and 10%) by recruiting participants (N=3,456) for a set of experiments (n=216 groups). We find that noise makes groups less likely to reach consensus, and the groups that do reach consensus take longer to find it. We find that leadership changes the behavior of both leaders and followers in important ways (for instance, being labeled a leader makes people more likely to 'go with the flow'). However, we find no evidence that the distance between leaders is a significant factor in the probability of reaching consensus. While other network properties of leaders undoubtedly impact consensus formation, the distance between leaders in network sub-groups appears not to matter."}, "https://arxiv.org/abs/2408.09406": {"title": "Uncovering multi-order Popularity and Similarity Mechanisms in Link Prediction by graphlet predictors", "link": "https://arxiv.org/abs/2408.09406", "description": "arXiv:2408.09406v1 Announce Type: new \nAbstract: Link prediction has become a critical problem in network science and has thus attracted increasing research interest. Popularity and similarity are two primary mechanisms in the formation of real networks. However, the roles of popularity and similarity mechanisms in link prediction across various domain networks remain poorly understood. Accordingly, this study used orbit degrees of graphlets to construct multi-order popularity- and similarity-based network link predictors, demonstrating that traditional popularity- and similarity-based indices can be efficiently represented in terms of orbit degrees. Moreover, we designed a supervised learning model that fuses multiple orbit-degree-based features and validated its link prediction performance. We also evaluated the mean absolute Shapley additive explanations of each feature within this model across 550 real-world networks from six domains. We observed that the homophily mechanism, which is a similarity-based feature, dominated social networks, with its win rate being 91\\%. Moreover, a different similarity-based feature was prominent in economic, technological, and information networks. Finally, no single feature dominated the biological and transportation networks. The proposed approach improves the accuracy and interpretability of link prediction, thus facilitating the analysis of complex networks."}, "https://arxiv.org/abs/2408.09613": {"title": "How Do Social Bots Participate in Misinformation Spread? A Comprehensive Dataset and Analysis", "link": "https://arxiv.org/abs/2408.09613", "description": "arXiv:2408.09613v1 Announce Type: new \nAbstract: Information spreads faster through social media platforms than traditional media, thus becoming an ideal medium to spread misinformation. Meanwhile, automated accounts, known as social bots, contribute more to the misinformation dissemination. In this paper, we explore the interplay between social bots and misinformation on the Sina Weibo platform. We propose a comprehensive and large-scale misinformation dataset, containing 11,393 misinformation and 16,416 unbiased real information with multiple modality information, with 952,955 related users. We propose a scalable weak-surprised method to annotate social bots, obtaining 68,040 social bots and 411,635 genuine accounts. To the best of our knowledge, this dataset is the largest dataset containing misinformation and social bots. We conduct comprehensive experiments and analysis on this dataset. Results show that social bots play a central role in misinformation dissemination, participating in news discussions to amplify echo chambers, manipulate public sentiment, and reverse public stances."}, "https://arxiv.org/abs/2408.09845": {"title": "Predicting Long-term Dynamics of Complex Networks via Identifying Skeleton in Hyperbolic Space", "link": "https://arxiv.org/abs/2408.09845", "description": "arXiv:2408.09845v1 Announce Type: new \nAbstract: Learning complex network dynamics is fundamental for understanding, modeling, and controlling real-world complex systems. Though great efforts have been made to predict the future states of nodes on networks, the capability of capturing long-term dynamics remains largely limited. This is because they overlook the fact that long-term dynamics in complex network are predominantly governed by their inherent low-dimensional manifolds, i.e., skeletons. Therefore, we propose the Dynamics-Invariant Skeleton Neural Net}work (DiskNet), which identifies skeletons of complex networks based on the renormalization group structure in hyperbolic space to preserve both topological and dynamics properties. Specifically, we first condense complex networks with various dynamics into simple skeletons through physics-informed hyperbolic embeddings. Further, we design graph neural ordinary differential equations to capture the condensed dynamics on the skeletons. Finally, we recover the skeleton networks and dynamics to the original ones using a degree-based super-resolution module. Extensive experiments across three representative dynamics as well as five real-world and two synthetic networks demonstrate the superior performances of the proposed DiskNet, which outperforms the state-of-the-art baselines by an average of 10.18\\% in terms of long-term prediction accuracy. Code for reproduction is available at: https://github.com/tsinghua-fib-lab/DiskNet."}, "https://arxiv.org/abs/2408.10018": {"title": "\"EBK\" : Leveraging Crowd-Sourced Social Media Data to Quantify How Hyperlocal Gang Affiliations Shape Personal Networks and Violence in Chicago's Contemporary Southside", "link": "https://arxiv.org/abs/2408.10018", "description": "arXiv:2408.10018v1 Announce Type: new \nAbstract: Recent ethnographic research reveals that gang dynamics in Chicago's Southside have evolved with decentralized micro-gang \"set\" factions and cross-gang interpersonal networks marking the contemporary landscape. However, standard police datasets lack the depth to analyze gang violence with such granularity. To address this, we employed a natural language processing strategy to analyze text from a Chicago gangs message board. By identifying proper nouns, probabilistically linking them to gang sets, and assuming social connections among names mentioned together, we created a social network dataset of 271 individuals across 11 gang sets. Using Louvain community detection, we found that these individuals often connect with gang-affiliated peers from various gang sets that are physically proximal. Hierarchical logistic regression revealed that individuals with ties to homicide victims and central positions in the overall gang network were at increased risk of victimization, regardless of gang affiliation. This research demonstrates that utilizing crowd-sourced information online can enable the study of otherwise inaccessible topics and populations."}, "https://arxiv.org/abs/2408.10088": {"title": "Recent Surge in Public Interest in Transportation: Sentiment Analysis of Baidu Apollo Go Using Weibo Data", "link": "https://arxiv.org/abs/2408.10088", "description": "arXiv:2408.10088v1 Announce Type: new \nAbstract: Urban mobility and transportation systems have been profoundly transformed by the advancement of autonomous vehicle technologies. Baidu Apollo Go, a pioneer robotaxi service from the Chinese tech giant Baidu, has recently been widely deployed in major cities like Beijing and Wuhan, sparking increased conversation and offering a glimpse into the future of urban mobility.\n  This study investigates public attitudes towards Apollo Go across China using Sentiment Analysis with a hybrid BERT model on 36,096 Weibo posts from January to July 2024. The analysis shows that 89.56\\% of posts related to Apollo Go are clustered in July. From January to July, public sentiment was mostly positive, but negative comments began to rise after it became a hot topic on July 21.\n  Spatial analysis indicates a strong correlation between provinces with high discussion intensity and those where Apollo Go operates. Initially, Hubei and Guangdong dominated online posting volume, but by July, Guangdong, Beijing, and international regions had overtaken Hubei. Attitudes varied significantly among provinces, with Xinjiang and Qinghai showing optimism and Tibet and Gansu expressing concerns about the impact on traditional taxi services.\n  Sentiment analysis revealed that positive comments focused on technology applications and personal experiences, while negative comments centered on job displacement and safety concerns. In summary, this study highlights the divergence in public perceptions of autonomous ride-hailing services, providing valuable insights for planners, policymakers, and service providers. The model is published on Hugging Face at https://huggingface.co/wsqstar/bert-finetuned-weibo-luobokuaipao and the repository on GitHub at https://github.com/GIStudio/trb2024."}, "https://arxiv.org/abs/2408.08912": {"title": "GeneticPrism: Multifaceted Visualization of Scientific Impact Evolutions", "link": "https://arxiv.org/abs/2408.08912", "description": "arXiv:2408.08912v1 Announce Type: cross \nAbstract: Understanding the evolution of scholarly impact is essential for many real-life decision-making processes in academia, such as research planning, frontier exploration, and award selection. Popular platforms like Google Scholar and Web of Science rely on numerical indicators that are too abstract to convey the context and content of scientific impact, while most existing visualization approaches on mapping science do not consider the presentation of individual scholars' impact evolution using curated self-citation data. This paper builds on our previous work and proposes an integrated pipeline to visualize a scholar's impact evolution from multiple topic facets. A novel 3D prism-shaped visual metaphor is introduced as the overview of a scholar's impact, whilst their scientific evolution on each topic is displayed in a more structured manner. Additional designs by topic chord diagram, streamgraph visualization, and inter-topic flow map, optimized by an elaborate layout algorithm, assist in perceiving the scholar's scientific evolution across topics. A new six-degree-impact glyph metaphor highlights key interdisciplinary works driving the evolution. The proposed visualization methods are evaluated through case studies analyzing the careers of prestigious Turing award laureates and a major visualization venue."}, "https://arxiv.org/abs/2408.09366": {"title": "Improving and Assessing the Fidelity of Large Language Models Alignment to Online Communities", "link": "https://arxiv.org/abs/2408.09366", "description": "arXiv:2408.09366v1 Announce Type: cross \nAbstract: Large language models (LLMs) have shown promise in representing individuals and communities, offering new ways to study complex social dynamics. However, effectively aligning LLMs with specific human groups and systematically assessing the fidelity of the alignment remains a challenge. This paper presents a robust framework for aligning LLMs with online communities via instruction-tuning and comprehensively evaluating alignment across various aspects of language, including authenticity, emotional tone, toxicity, and harm. We demonstrate the utility of our approach by applying it to online communities centered on dieting and body image. We administer an eating disorder psychometric test to the aligned LLMs to reveal unhealthy beliefs and successfully differentiate communities with varying levels of eating disorder risk. Our results highlight the potential of LLMs in automated moderation and broader applications in public health and social science research."}, "https://arxiv.org/abs/2408.09378": {"title": "Gender Dynamics in Russian Online Political Discourse", "link": "https://arxiv.org/abs/2408.09378", "description": "arXiv:2408.09378v1 Announce Type: cross \nAbstract: The digital landscape provides a dynamic platform for political discourse crucial for understanding shifts in public opinion and engagement especially under authoritarian governments This study examines YouTube user behavior during the Russian-Ukrainian war analyzing 2168 videos with over 36000 comments from January 2022 to February 2024 We observe distinct patterns of participation and gender dynamics that correlate with major political and military events Notably females were more active in antigovernment channels especially during peak conflict periods Contrary to assumptions about online engagement in authoritarian contexts our findings suggest a complex interplay where women emerge as pivotal digital communicators This highlights online platforms role in facilitating political expression under authoritarian regimes demonstrating its potential as a barometer for public sentiment."}, "https://arxiv.org/abs/2408.09705": {"title": "Community-Centric Graph Unlearning", "link": "https://arxiv.org/abs/2408.09705", "description": "arXiv:2408.09705v1 Announce Type: cross \nAbstract: Graph unlearning technology has become increasingly important since the advent of the `right to be forgotten' and the growing concerns about the privacy and security of artificial intelligence. Graph unlearning aims to quickly eliminate the effects of specific data on graph neural networks (GNNs). However, most existing deterministic graph unlearning frameworks follow a balanced partition-submodel training-aggregation paradigm, resulting in a lack of structural information between subgraph neighborhoods and redundant unlearning parameter calculations. To address this issue, we propose a novel Graph Structure Mapping Unlearning paradigm (GSMU) and a novel method based on it named Community-centric Graph Eraser (CGE). CGE maps community subgraphs to nodes, thereby enabling the reconstruction of a node-level unlearning operation within a reduced mapped graph. CGE makes the exponential reduction of both the amount of training data and the number of unlearning parameters. Extensive experiments conducted on five real-world datasets and three widely used GNN backbones have verified the high performance and efficiency of our CGE method, highlighting its potential in the field of graph unlearning."}, "https://arxiv.org/abs/2408.09760": {"title": "Regional and spatial dependence of poverty factors in Thailand, and its use into Bayesian hierarchical regression analysis", "link": "https://arxiv.org/abs/2408.09760", "description": "arXiv:2408.09760v1 Announce Type: cross \nAbstract: Poverty is a serious issue that harms humanity progression. The simplest solution is to use one-shirt-size policy to alleviate it. Nevertheless, each region has its unique issues, which require a unique solution to solve them. In the aspect of spatial analysis, neighbor regions can provide useful information to analyze issues of a given region. In this work, we proposed inferred boundaries of regions of Thailand that can explain better the poverty dynamics, instead of the usual government administrative regions. The proposed regions maximize a trade-off between poverty-related features and geographical coherence. We use a spatial analysis together with Moran's cluster algorithms and Bayesian hierarchical regression models, with the potential of assist the implementation of the right policy to alleviate the poverty phenomenon. We found that all variables considered show a positive spatial autocorrelation. The results of analysis illustrate that 1) Northern, Northeastern Thailand, and in less extend Northcentral Thailand are the regions that require more attention in the aspect of poverty issues, 2) Northcentral, Northeastern, Northern and Southern Thailand present dramatically low levels of education, income and amount of savings contrasted with large cities such as Bangkok-Pattaya and Central Thailand, and 3) Bangkok-Pattaya is the only region whose average years of education is above 12 years, which corresponds (approx.) with a complete senior high school."}, "https://arxiv.org/abs/2311.14386": {"title": "Collective Memory, Consensus, and Learning Explained by Network Connectivity", "link": "https://arxiv.org/abs/2311.14386", "description": "arXiv:2311.14386v3 Announce Type: replace \nAbstract: Humans cluster in social groups where they discuss their shared past, problems, and potential solutions; they learn collectively when they repeat activities; they synchronize when they sing or dance together; and they bond through social cohesion. Intuitively, a group is more cohesive if people are closer together in their network and are bonded by multiple connections. This intuition is made precise by the second smallest eigenvalue of the Laplacian matrix of the group network, called the \\textit{algebraic connectivity}. This measure is key to explaining and predicting the outcomes of said activities."}, "https://arxiv.org/abs/2401.11782": {"title": "Temporal Interaction and its Role in the Evolution of Cooperation", "link": "https://arxiv.org/abs/2401.11782", "description": "arXiv:2401.11782v3 Announce Type: replace \nAbstract: This research investigates the impact of dynamic, time-varying interactions on cooperative behaviour in social dilemmas. Traditional research has focused on deterministic rules governing pairwise interactions, yet the impact of interaction frequency and synchronization in groups on cooperation remains underexplored. Addressing this gap, our work introduces two temporal interaction mechanisms to model the stochastic or periodic participation of individuals in public goods games, acknowledging real-life variances due to exogenous temporal factors and geographical time differences. We consider that the interaction state significantly influences both game payoff calculations and the strategy updating process, offering new insights into the emergence and sustainability of cooperation. Our results indicate that maximum game participation frequency is suboptimal under a stochastic interaction mechanism. Instead, an intermediate activation probability maximizes cooperation, suggesting a vital balance between interaction frequency and inactivity security. Furthermore, local synchronization of interactions within specific areas is shown to be beneficial, as time differences hinder the spread of cross-structures but promote the formation of dense cooperative clusters with smoother boundaries. We also note that stronger clustering in networks, larger group sizes and lower noise increase cooperation. This research contributes to understanding the role of node-based temporality and probabilistic interactions in social dilemmas, offering insights into fostering cooperation."}, "https://arxiv.org/abs/2402.10230": {"title": "Temporal Analysis of Drifting Hashtags in Textual Data Streams: A Graph-Based Application", "link": "https://arxiv.org/abs/2402.10230", "description": "arXiv:2402.10230v2 Announce Type: replace \nAbstract: Initially supported by Twitter, hashtags are now used on several social media platforms. Hashtags are helpful for tagging, tracking, and grouping posts on similar topics. In this paper, based on a hashtag stream regarding the hashtag #mybodymychoice, we analyze hashtag drifts over time using concepts from graph analysis and textual data streams using the Girvan-Newman method to uncover hashtag communities in annual snapshots between 2018 and 2022. In addition, we offer insights about some correlated hashtags found in the study. Our approach can be useful for monitoring changes over time in opinions and sentiment patterns about an entity on social media. Even though the hashtag #mybodymychoice was initially coupled with women's rights, abortion, and bodily autonomy, we observe that it suffered drifts during the studied period across topics such as drug legalization, vaccination, political protests, war, and civil rights. The year 2021 was the most significant drifting year, in which the communities detected and their respective sizes suggest that #mybodymychoice had a significant drift to vaccination and Covid-19-related topics."}, "https://arxiv.org/abs/2403.17440": {"title": "$(\\omega_1, \\omega_2)$-temporal random hyperbolic graphs", "link": "https://arxiv.org/abs/2403.17440", "description": "arXiv:2403.17440v2 Announce Type: replace \nAbstract: We extend a recent model of temporal random hyperbolic graphs by allowing connections and disconnections to persist across network snapshots with different probabilities, $\\omega_1$ and $\\omega_2$. This extension, while conceptually simple, poses analytical challenges involving the Appell $F_1$ series. Despite these challenges, we are able to analyze key properties of the model, which include the distributions of contact and intercontact durations, as well as the expected time-aggregated degree. The incorporation of $\\omega_1$ and $\\omega_2$ enables more flexible tuning of the average contact and intercontact durations, and of the average time-aggregated degree, providing a finer control for exploring the effect of temporal network dynamics on dynamical processes. Overall, our results provide new insights into the analysis of temporal networks and contribute to a more general representation of real-world scenarios."}, "https://arxiv.org/abs/2408.10351": {"title": "The Psychological Impacts of Algorithmic and AI-Driven Social Media on Teenagers: A Call to Action", "link": "https://arxiv.org/abs/2408.10351", "description": "arXiv:2408.10351v1 Announce Type: new \nAbstract: This study investigates the meta-issues surrounding social media, which, while theoretically designed to enhance social interactions and improve our social lives by facilitating the sharing of personal experiences and life events, often results in adverse psychological impacts. Our investigation reveals a paradoxical outcome: rather than fostering closer relationships and improving social lives, the algorithms and structures that underlie social media platforms inadvertently contribute to a profound psychological impact on individuals, influencing them in unforeseen ways. This phenomenon is particularly pronounced among teenagers, who are disproportionately affected by curated online personas, peer pressure to present a perfect digital image, and the constant bombardment of notifications and updates that characterize their social media experience. As such, we issue a call to action for policymakers, platform developers, and educators to prioritize the well-being of teenagers in the digital age and work towards creating secure and safe social media platforms that protect the young from harm, online harassment, and exploitation."}, "https://arxiv.org/abs/2408.10373": {"title": "Competing Social Contagions with Opinion Dependent Infectivity", "link": "https://arxiv.org/abs/2408.10373", "description": "arXiv:2408.10373v1 Announce Type: new \nAbstract: The spread of disinformation (maliciously spread false information) in online social networks has become an important problem in today's society. Disinformation's spread is facilitated by the fact that individuals often accept false information based on cognitive biases which predispose them to believe information that they have heard repeatedly or that aligns with their beliefs. Moreover, disinformation often spreads in direct competition with a corresponding true information. To model these phenomena, we develop a model for two competing beliefs spreading on a social network, where individuals have an internal opinion that models their cognitive biases and modulates their likelihood of adopting one of the competing beliefs. By numerical simulations of an agent-based model and a mean-field description of the dynamics, we study how the long-term dynamics of the spreading process depends on the initial conditions for the number of spreaders and the initial opinion of the population. We find that the addition of cognitive biases enriches the transient dynamics of the spreading process, facilitating behavior such as the revival of a dying belief and the overturning of an initially widespread opinion. Finally, we study how external recruitment of spreaders can lead to the eventual dominance of one of the two beliefs."}, "https://arxiv.org/abs/2408.10464": {"title": "Improved Community Detection using Stochastic Block Models", "link": "https://arxiv.org/abs/2408.10464", "description": "arXiv:2408.10464v1 Announce Type: new \nAbstract: Community detection approaches resolve complex networks into smaller groups (communities) that are expected to be relatively edge-dense and well-connected. The stochastic block model (SBM) is one of several approaches used to uncover community structure in graphs. In this study, we demonstrate that SBM software applied to various real-world and synthetic networks produces poorly-connected to disconnected clusters. We present simple modifications to improve the connectivity of SBM clusters, and show that the modifications improve accuracy using simulated networks."}, "https://arxiv.org/abs/2408.10356": {"title": "Diversity and stylization of the contemporary user-generated visual arts in the complexity-entropy plane", "link": "https://arxiv.org/abs/2408.10356", "description": "arXiv:2408.10356v1 Announce Type: cross \nAbstract: The advent of computational and numerical methods in recent times has provided new avenues for analyzing art historiographical narratives and tracing the evolution of art styles therein. Here, we investigate an evolutionary process underpinning the emergence and stylization of contemporary user-generated visual art styles using the complexity-entropy (C-H) plane, which quantifies local structures in paintings. Informatizing 149,780 images curated in DeviantArt and Behance platforms from 2010 to 2020, we analyze the relationship between local information of the C-H space and multi-level image features generated by a deep neural network and a feature extraction algorithm. The results reveal significant statistical relationships between the C-H information of visual artistic styles and the dissimilarities of the multi-level image features over time within groups of artworks. By disclosing a particular C-H region where the diversity of image representations is noticeably manifested, our analyses reveal an empirical condition of emerging styles that are both novel in the C-H plane and characterized by greater stylistic diversity. Our research shows that visual art analyses combined with physics-inspired methodologies and machine learning, can provide macroscopic insights into quantitatively mapping relevant characteristics of an evolutionary process underpinning the creative stylization of uncharted visual arts of given groups and time."}, "https://arxiv.org/abs/2402.09076": {"title": "Preserving system activity while controlling epidemic spreading in adaptive temporal networks", "link": "https://arxiv.org/abs/2402.09076", "description": "arXiv:2402.09076v2 Announce Type: replace \nAbstract: Human behaviour strongly influences the spread of infectious diseases: understanding the interplay between epidemic dynamics and adaptive behaviours is essential to improve response strategies to epidemics, with the goal of containing the epidemic while preserving a sufficient level of operativeness in the population. Through activity-driven temporal networks, we formulate a general framework which models a wide range of adaptive behaviours and mitigation strategies, observed in real populations. We analytically derive the conditions for a widespread diffusion of epidemics in the presence of arbitrary adaptive behaviours, highlighting the crucial role of correlations between agents behaviour in the infected and in the susceptible state. We focus on the effects of sick-leave, comparing the effectiveness of different strategies in reducing the impact of the epidemic and preserving the system operativeness. We show the critical relevance of heterogeneity in individual behavior: in homogeneous networks, all sick-leave strategies are equivalent and poorly effective, while in heterogeneous networks, strategies targeting the most vulnerable nodes are able to effectively mitigate the epidemic, also avoiding a deterioration in system activity and maintaining a low level of absenteeism. Interestingly, with targeted strategies both the minimum of population activity and the maximum of absenteeism anticipate the infection peak, which is effectively flattened and delayed, so that full operativeness is almost restored when the infection peak arrives. We also provide realistic estimates of the model parameters for influenza-like illness, thereby suggesting strategies for managing epidemics and absenteeism in realistic populations."}, "https://arxiv.org/abs/2302.11988": {"title": "Time Complexity of Broadcast and Consensus for Randomized Oblivious Message Adversaries", "link": "https://arxiv.org/abs/2302.11988", "description": "arXiv:2302.11988v2 Announce Type: replace-cross \nAbstract: Broadcast and consensus are most fundamental tasks in distributed computing. These tasks are particularly challenging in dynamic networks where communication across the network links may be unreliable, e.g., due to mobility or failures. Indeed, over the last years, researchers have derived several impossibility results and high time complexity lower bounds (i.e., linear in the number of nodes $n$) for these tasks, even for oblivious message adversaries where communication networks are rooted trees. However, such deterministic adversarial models may be overly conservative, as many processes in real-world settings are stochastic in nature rather than worst case.\n  This paper initiates the study of broadcast and consensus on stochastic dynamic networks, introducing a randomized oblivious message adversary. Our model is reminiscent of the SI model in epidemics, however, revolving around trees (which renders the analysis harder due to the apparent lack of independence). In particular, we show that if information dissemination occurs along random rooted trees, broadcast and consensus complete fast with high probability, namely in logarithmic time. Our analysis proves the independence of a key variable, which enables a formal understanding of the dissemination process.\n  More formally, for a network with $n$ nodes, we first consider the completely random case where in each round the communication network is chosen uniformly at random among rooted trees. We then introduce the notion of randomized oblivious message adversary, where in each round, an adversary can choose $k$ edges to appear in the communication network, and then a rooted tree is chosen uniformly at random among the set of all rooted trees that include these edges. We show that broadcast completes in $O(k+\\log n)$ rounds, and that this it is also the case for consensus as long as $k \\le 0.1n$."}, "https://arxiv.org/abs/2308.05680": {"title": "Breaking Language Barriers with MMTweets: Advancing Cross-Lingual Debunked Narrative Retrieval for Fact-Checking", "link": "https://arxiv.org/abs/2308.05680", "description": "arXiv:2308.05680v2 Announce Type: replace-cross \nAbstract: Finding previously debunked narratives involves identifying claims that have already undergone fact-checking. The issue intensifies when similar false claims persist in multiple languages, despite the availability of debunks for several months in another language. Hence, automatically finding debunks (or fact-checks) in multiple languages is crucial to make the best use of scarce fact-checkers' resources. Mainly due to the lack of readily available data, this is an understudied problem, particularly when considering the cross-lingual scenario, i.e. the retrieval of debunks in a language different from the language of the online post being checked. This study introduces cross-lingual debunked narrative retrieval and addresses this research gap by: (i) creating Multilingual Misinformation Tweets (MMTweets): a dataset that stands out, featuring cross-lingual pairs, images, human annotations, and fine-grained labels, making it a comprehensive resource compared to its counterparts; (ii) conducting an extensive experiment to benchmark state-of-the-art cross-lingual retrieval models and introducing multistage retrieval methods tailored for the task; and (iii) comprehensively evaluating retrieval models for their cross-lingual and cross-dataset transfer capabilities within MMTweets, and conducting a retrieval latency analysis. We find that MMTweets presents challenges for cross-lingual debunked narrative retrieval, highlighting areas for improvement in retrieval models. Nonetheless, the study provides valuable insights for creating MMTweets datasets and optimising debunked narrative retrieval models to empower fact-checking endeavours. The dataset and annotation codebook are publicly available at https://doi.org/10.5281/zenodo.10637161."}, "https://arxiv.org/abs/2402.18697": {"title": "Inferring Dynamic Networks from Marginals with Iterative Proportional Fitting", "link": "https://arxiv.org/abs/2402.18697", "description": "arXiv:2402.18697v2 Announce Type: replace-cross \nAbstract: A common network inference problem, arising from real-world data constraints, is how to infer a dynamic network from its time-aggregated adjacency matrix and time-varying marginals (i.e., row and column sums). Prior approaches to this problem have repurposed the classic iterative proportional fitting (IPF) procedure, also known as Sinkhorn's algorithm, with promising empirical results. However, the statistical foundation for using IPF has not been well understood: under what settings does IPF provide principled estimation of a dynamic network from its marginals, and how well does it estimate the network? In this work, we establish such a setting, by identifying a generative network model whose maximum likelihood estimates are recovered by IPF. Our model both reveals implicit assumptions on the use of IPF in such settings and enables new analyses, such as structure-dependent error bounds on IPF's parameter estimates. When IPF fails to converge on sparse network data, we introduce a principled algorithm that guarantees IPF converges under minimal changes to the network structure. Finally, we conduct experiments with synthetic and real-world data, which demonstrate the practical value of our theoretical and algorithmic contributions."}, "https://arxiv.org/abs/2408.11065": {"title": "Statistical Patterns in the Equations of Physics and the Emergence of a Meta-Law of Nature", "link": "https://arxiv.org/abs/2408.11065", "description": "arXiv:2408.11065v1 Announce Type: new \nAbstract: Physics, as a fundamental science, aims to understand the laws of Nature and describe them in mathematical equations. While the physical reality manifests itself in a wide range of phenomena with varying levels of complexity, the equations that describe them display certain statistical regularities and patterns, which we begin to explore here. By drawing inspiration from linguistics, where Zipf's law states that the frequency of any word in a large corpus of text is roughly inversely proportional to its rank in the frequency table, we investigate whether similar patterns for the distribution of operators emerge in the equations of physics. We analyse three corpora of formulae and find, using sophisticated implicit-likelihood methods, that the frequency of operators as a function of their rank in the frequency table is best described by an exponential law with a stable exponent, in contrast with Zipf's inverse power-law. Understanding the underlying reasons behind this statistical pattern may shed light on Nature's modus operandi or reveal recurrent patterns in physicists' attempts to formalise the laws of Nature. It may also provide crucial input for symbolic regression, potentially augmenting language models to generate symbolic models for physical phenomena. By pioneering the study of statistical regularities in the equations of physics, our results open the door for a meta-law of Nature, a (probabilistic) law that all physical laws obey."}, "https://arxiv.org/abs/2408.11461": {"title": "Polyrhythmic Harmonies from the Sky: Transforming Satellite Images of Clouds into Musical Compositions through Algorithms", "link": "https://arxiv.org/abs/2408.11461", "description": "arXiv:2408.11461v1 Announce Type: new \nAbstract: In a context of increasing scientific specialization and deficiencies in the scientific literacy of the population, there arises a need to broaden the methods of scientific dissemination. This study proposes an approach that combines music with scientific concepts, focusing on the sonification of satellite images as the core. A generative musical composition system is developed that uses visual data to create accessible and emotional auditory experiences, thus enriching the fields of scientific dissemination and artistic expression. It concludes with an example of the algorithm's use in a musical composition."}, "https://arxiv.org/abs/2408.11470": {"title": "A Thorough Comparison Between Independent Cascade and Susceptible-Infected-Recovered Models", "link": "https://arxiv.org/abs/2408.11470", "description": "arXiv:2408.11470v1 Announce Type: new \nAbstract: We study cascades in social networks with the independent cascade (IC) model and the Susceptible-Infected-recovered (SIR) model. The well-studied IC model fails to capture the feature of node recovery, and the SIR model is a variant of the IC model with the node recovery feature. In the SIR model, by computing the probability that a node successfully infects another before its recovery and viewing this probability as the corresponding IC parameter, the SIR model becomes an \"out-going-edge-correlated\" version of the IC model: the events of the infections along different out-going edges of a node become dependent in the SIR model, whereas these events are independent in the IC model. In this paper, we thoroughly compare the two models and examine the effect of this extra dependency in the SIR model. By a carefully designed coupling argument, we show that the seeds in the IC model have a stronger influence spread than their counterparts in the SIR model, and sometimes it can be significantly stronger. Specifically, we prove that, given the same network, the same seed sets, and the parameters of the two models being set based on the above-mentioned equivalence, the expected number of infected nodes at the end of the cascade for the IC model is weakly larger than that for the SIR model, and there are instances where this dominance is significant. We also study the influence maximization problem with the SIR model. We show that the above-mentioned difference in the two models yields different seed-selection strategies, which motivates the design of influence maximization algorithms specifically for the SIR model. We design efficient approximation algorithms with theoretical guarantees by adapting the reverse-reachable-set-based algorithms, commonly used for the IC model, to the SIR model."}, "https://arxiv.org/abs/2408.11806": {"title": "Counting simplicial pairs in hypergraphs", "link": "https://arxiv.org/abs/2408.11806", "description": "arXiv:2408.11806v1 Announce Type: new \nAbstract: We present two ways to measure the simplicial nature of a hypergraph: the simplicial ratio and the simplicial matrix. We show that the simplicial ratio captures the frequency, as well as the rarity, of simplicial interactions in a hypergraph while the simplicial matrix provides more fine-grained details. We then compute the simplicial ratio, as well as the simplicial matrix, for 10 real-world hypergraphs and, from the data collected, hypothesize that simplicial interactions are more and more deliberate as edge size increases. We then present a new Chung-Lu model that includes a parameter controlling (in expectation) the frequency of simplicial interactions. We use this new model, as well as the real-world hypergraphs, to show that multiple stochastic processes exhibit different behaviour when performed on simplicial hypergraphs vs. non-simplicial hypergraphs."}, "https://arxiv.org/abs/2408.11174": {"title": "Combining Objective and Subjective Perspectives for Political News Understanding", "link": "https://arxiv.org/abs/2408.11174", "description": "arXiv:2408.11174v1 Announce Type: cross \nAbstract: Researchers and practitioners interested in computational politics rely on automatic content analysis tools to make sense of the large amount of political texts available on the Web. Such tools should provide objective and subjective aspects at different granularity levels to make the analyses useful in practice. Existing methods produce interesting insights for objective aspects, but are limited for subjective ones, are often limited to national contexts, and have limited explainability. We introduce a text analysis framework which integrates both perspectives and provides a fine-grained processing of subjective aspects. Information retrieval techniques and knowledge bases complement powerful natural language processing components to allow a flexible aggregation of results at different granularity levels. Importantly, the proposed bottom-up approach facilitates the explainability of the obtained results. We illustrate its functioning with insights on news outlets, political orientations, topics, individual entities, and demographic segments. The approach is instantiated on a large corpus of French news, but is designed to work seamlessly for other languages and countries."}, "https://arxiv.org/abs/2408.11331": {"title": "Parallel Algorithms for Median Consensus Clustering in Complex Networks", "link": "https://arxiv.org/abs/2408.11331", "description": "arXiv:2408.11331v1 Announce Type: cross \nAbstract: We develop an algorithm that finds the consensus of many different clustering solutions of a graph. We formulate the problem as a median set partitioning problem and propose a greedy optimization technique. Unlike other approaches that find median set partitions, our algorithm takes graph structure into account and finds a comparable quality solution much faster than the other approaches. For graphs with known communities, our consensus partition captures the actual community structure more accurately than alternative approaches. To make it applicable to large graphs, we remove sequential dependencies from our algorithm and design a parallel algorithm. Our parallel algorithm achieves 35x speedup when utilizing 64 processing cores for large real-world graphs from single-cell experiments."}, "https://arxiv.org/abs/2408.11460": {"title": "Impact of changing the wet deposition schemes in ldx on 137-cs atmosperic deposits after the fukushima accident", "link": "https://arxiv.org/abs/2408.11460", "description": "arXiv:2408.11460v1 Announce Type: cross \nAbstract: The Fukushima-Daiichi release of radioactivity is a relevant event to study the atmospheric dispersion modelling of radionuclides. Actually, the atmospheric deposition onto the ground may be studied through the map of measured Cs-137 established consecutively to the accident. The limits of detection were low enough to make the measurements possible as far as 250km from the nuclear power plant. This large scale deposition has been modelled with the Eulerian model ldX. However, several weeks of emissions in multiple weather conditions make it a real challenge. Besides, these measurements are accumulated deposition of Cs-137 over the whole period and do not inform of deposition mechanisms involved: in-cloud, below-cloud, dry deposition. In a previous study (Qu{\\'e}rel et al., 2016), a comprehensive sensitivity analysis was performed in order to understand wet deposition mechanisms. It has been shown that the choice of the wet deposition scheme has a strong impact on assessment of deposition patterns. Nevertheless, a ``best'' scheme could not be highlighted as it depends on the selected criteria: the ranking differs according to the statistical indicators considered (correlation, figure of merit in space and factor 2). A possibility to explain the difficulty to discriminate between several schemes was the uncertainties in the modelling, resulting from the meteorological data for instance. Since the move of the plume is not properly modelled, the deposition processes are applied with an inaccurate activity concentration in the air. In the framework of the SAKURA project, an MRI-IRSN collaboration, new meteorological fields at higher resolution (Sekiyama et al., 2013) were provided and allow to reconsider the previous study. An update including these new meteorology data is presented. In addition, the focus is put on the deposition schemes commonly used in nuclear emergency context."}, "https://arxiv.org/abs/2408.11673": {"title": "Improved Visual Saliency of Graph Clusters with Orderable Node-Link Layouts", "link": "https://arxiv.org/abs/2408.11673", "description": "arXiv:2408.11673v1 Announce Type: cross \nAbstract: Graphs are often used to model relationships between entities. The identification and visualization of clusters in graphs enable insight discovery in many application areas, such as life sciences and social sciences. Force-directed graph layouts promote the visual saliency of clusters, as they bring adjacent nodes closer together, and push non-adjacent nodes apart. At the same time, matrices can effectively show clusters when a suitable row/column ordering is applied, but are less appealing to untrained users not providing an intuitive node-link metaphor. It is thus worth exploring layouts combining the strengths of the node-link metaphor and node ordering. In this work, we study the impact of node ordering on the visual saliency of clusters in orderable node-link diagrams, namely radial diagrams, arc diagrams and symmetric arc diagrams. Through a crowdsourced controlled experiment, we show that users can count clusters consistently more accurately, and to a large extent faster, with orderable node-link diagrams than with three state-of-the art force-directed layout algorithms, i.e., `Linlog', `Backbone' and `sfdp'. The measured advantage is greater in case of low cluster separability and/or low compactness. A free copy of this paper and all supplemental materials are available at https://osf.io/kc3dg/."}, "https://arxiv.org/abs/2408.11759": {"title": "Dynamical analysis of financial stocks network: improving forecasting using network properties", "link": "https://arxiv.org/abs/2408.11759", "description": "arXiv:2408.11759v1 Announce Type: cross \nAbstract: Applying a network analysis to stock return correlations, we study the dynamical properties of the network and how they correlate with the market return, finding meaningful variables that partially capture the complex dynamical processes of stock interactions and the market structure. We then use the individual properties of stocks within the network along with the global ones, to find correlations with the future returns of individual S&amp;P 500 stocks. Applying these properties as input variables for forecasting, we find a 50% improvement on the R2score in the prediction of stock returns on long time scales (per year), and 3% on short time scales (2 days), relative to baseline models without network variables."}, "https://arxiv.org/abs/2408.11772": {"title": "VIRIS: Simulating indoor airborne transmission combining architectural design and people movement", "link": "https://arxiv.org/abs/2408.11772", "description": "arXiv:2408.11772v1 Announce Type: cross \nAbstract: A Viral Infection Risk Indoor Simulator (VIRIS) has been developed to quickly assess and compare mitigations for airborne disease spread. This agent-based simulator combines people movement in an indoor space, viral transmission modelling and detailed architectural design, and it is powered by topologicpy, an open-source Python library. VIRIS generates very fast predictions of the viral concentration and the spatiotemporal infection risk for individuals as they move through a given space. The simulator is validated with data from a courtroom superspreader event. A sensitivity study for unknown parameter values is also performed. We compare several non-pharmaceutical interventions (NPIs) issued in UK government guidance, for two indoor settings: a care home and a supermarket. Additionally, we have developed the user-friendly VIRIS web app that allows quick exploration of diverse scenarios of interest and visualisation, allowing policymakers, architects and space managers to easily design or assess infection risk in an indoor space."}, "https://arxiv.org/abs/2008.13078": {"title": "Probability-turbulence divergence: A tunable allotaxonometric instrument for comparing heavy-tailed categorical distributions", "link": "https://arxiv.org/abs/2008.13078", "description": "arXiv:2008.13078v2 Announce Type: replace \nAbstract: Real-world complex systems often comprise many distinct types of elements as well as many more types of networked interactions between elements. When the relative abundances of types can be measured well, we further observe heavy-tailed categorical distributions for type frequencies. For the comparison of type frequency distributions of two systems or a system with itself at different time points in time -- a facet of allotaxonometry -- a great range of probability divergences are available. Here, we introduce and explore `probability-turbulence divergence', a tunable, straightforward, and interpretable instrument for comparing normalizable categorical frequency distributions. We model probability-turbulence divergence (PTD) after rank-turbulence divergence (RTD). While probability-turbulence divergence is more limited in application than rank-turbulence divergence, it is more sensitive to changes in type frequency. We build allotaxonographs to display probability turbulence, incorporating a way to visually accommodate zero probabilities for `exclusive types' which are types that appear in only one system. We explore comparisons of example distributions taken from literature, social media, and ecology. We show how probability-turbulence divergence either explicitly or functionally generalizes many existing kinds of distances and measures, including, as special cases, $L^{(p)}$ norms, the S{\\o}rensen-Dice coefficient (the $F_1$ statistic), and the Hellinger distance. We discuss similarities with the generalized entropies of R{\\'e}nyi and Tsallis, and the diversity indices (or Hill numbers) from ecology. We close with thoughts on open problems concerning the optimization of the tuning of rank- and probability-turbulence divergence."}, "https://arxiv.org/abs/2403.14584": {"title": "Dynamical importance and network perturbations", "link": "https://arxiv.org/abs/2403.14584", "description": "arXiv:2403.14584v2 Announce Type: replace \nAbstract: The leading eigenvalue $\\lambda$ of the adjacency matrix of a graph exerts much influence on the behavior of dynamical processes on that graph. It is thus relevant to relate notions of the importance (specifically, centrality measures) of network structures to $\\lambda$ and its associated eigenvector. We study a previously derived measure of edge importance known as ``dynamical importance'', which estimates how much $\\lambda$ changes when one removes an edge from a graph or adds an edge to it. We examine the accuracy of this estimate for different network structures and compare it to the true change in $\\lambda$ after an edge removal or edge addition. We then derive a first-order approximation of the change in the leading eigenvector. We also consider the effects of edge additions on Kuramoto dynamics on networks, and we express the Kuramoto order parameter in terms of dynamical importance. Through our analysis and computational experiments, we find that studying dynamical importance can improve understanding of the relationship between network perturbations and dynamical processes on networks."}, "https://arxiv.org/abs/2302.05762": {"title": "Interpretable Deep Learning for Forecasting Online Advertising Costs: Insights from the Competitive Bidding Landscape", "link": "https://arxiv.org/abs/2302.05762", "description": "arXiv:2302.05762v2 Announce Type: replace-cross \nAbstract: As advertisers increasingly shift their budgets toward digital advertising, accurately forecasting advertising costs becomes essential for optimizing marketing campaign returns. This paper presents a comprehensive study that employs various time-series forecasting methods to predict daily average CPC in the online advertising market. We evaluate the performance of statistical models, machine learning techniques, and deep learning approaches, including the Temporal Fusion Transformer (TFT). Our findings reveal that incorporating multivariate models, enriched with covariates derived from competitors' CPC patterns through time-series clustering, significantly improves forecasting accuracy. We interpret the results by analyzing feature importance and temporal attention, demonstrating how the models leverage both the advertiser's data and insights from the competitive landscape. Additionally, our method proves robust during major market shifts, such as the COVID-19 pandemic, consistently outperforming models that rely solely on individual advertisers' data. This study introduces a scalable technique for selecting relevant covariates from a broad pool of advertisers, offering more accurate long-term forecasts and strategic insights into budget allocation and competitive dynamics in digital advertising."}, "https://arxiv.org/abs/2306.13532": {"title": "PathMLP: Smooth Path Towards High-order Homophily", "link": "https://arxiv.org/abs/2306.13532", "description": "arXiv:2306.13532v2 Announce Type: replace-cross \nAbstract: Real-world graphs exhibit increasing heterophily, where nodes no longer tend to be connected to nodes with the same label, challenging the homophily assumption of classical graph neural networks (GNNs) and impeding their performance. Intriguingly, from the observation of heterophilous data, we notice that certain high-order information exhibits higher homophily, which motivates us to involve high-order information in node representation learning. However, common practices in GNNs to acquire high-order information mainly through increasing model depth and altering message-passing mechanisms, which, albeit effective to a certain extent, suffer from three shortcomings: 1) over-smoothing due to excessive model depth and propagation times; 2) high-order information is not fully utilized; 3) low computational efficiency. In this regard, we design a similarity-based path sampling strategy to capture smooth paths containing high-order homophily. Then we propose a lightweight model based on multi-layer perceptrons (MLP), named PathMLP, which can encode messages carried by paths via simple transformation and concatenation operations, and effectively learn node representations in heterophilous graphs through adaptive path aggregation. Extensive experiments demonstrate that our method outperforms baselines on 16 out of 20 datasets, underlining its effectiveness and superiority in alleviating the heterophily problem. In addition, our method is immune to over-smoothing and has high computational efficiency. The source code will be available in https://github.com/Graph4Sec-Team/PathMLP."}, "https://arxiv.org/abs/2404.12540": {"title": "State Discretization for Continuous-State MDPs in Infectious Disease Control", "link": "https://arxiv.org/abs/2404.12540", "description": "arXiv:2404.12540v2 Announce Type: replace-cross \nAbstract: Repeated decision-making problems under uncertainty may arise in the health policy context, such as infectious disease control for COVID-19 and other epidemics. These problems may sometimes be effectively solved using Markov decision processes (MDPs). However, the continuous or large state space of such problems for capturing infectious disease prevalence renders it difficult to implement tractable MDPs to identify the optimal disease control policy over time. We therefore develop an algorithm for discretizing continuous states for approximate MDP solutions in this context. We benchmark performance against a uniform discretization using both a synthetic example and an example of COVID-19 in Los Angeles County."}, "https://arxiv.org/abs/2405.02760": {"title": "GTFS2STN: Analyzing GTFS Transit Data by Generating Spatiotemporal Transit Network", "link": "https://arxiv.org/abs/2405.02760", "description": "arXiv:2405.02760v2 Announce Type: replace-cross \nAbstract: The General Transit Feed Specification (GTFS) is an open standard format for recording transit information, utilized by thousands of transit agencies worldwide. This study introduces GTFS2STN, a novel tool that converts static GTFS transit networks into spatiotemporal networks, connecting bus stops across space and time. This transformation enables comprehensive analysis of transit system accessibility. Additionally, we present a web-based application version of the GTFS2STN tool that allows users to generate spatiotemporal networks online and perform basic analyses, including the creation of isochrone maps from a given origin and the calculation of travel time variability between origin-destination pairs over time. Comparative analysis demonstrates that GTFS2STN produces results similar to those of Mapnificent, an existing open-source tool for generating isochrone maps from GTFS inputs. Compared with Mapnificent, GTFS2STN offers enhanced flexibility for researchers and planners to evaluate transit plans, as it allows users to upload and analyze historical or suggested GTFS feeds from any transit agency. This feature facilitates the assessment of accessibility and travel time variability in transit networks over extended periods, making GTFS2STN a valuable tool for the planning and research for the transit systems."}, "https://arxiv.org/abs/2408.11833": {"title": "The Impact of the Virtualization of Scholarly Conferences on the Gender Structure of Conference Contributors", "link": "https://arxiv.org/abs/2408.11833", "description": "arXiv:2408.11833v1 Announce Type: new \nAbstract: This study investigates whether the virtualization of academic conferences in the wake of the COVID-19 pandemic changed the gender structure of conference participants. We explored this question utilizing authorship data from the Web of Science Conference Proceedings Citation Index for 180 conferences in 30 conference series held between 2017 and 2023. At least one edition of each analyzed conference series was launched in a virtual or hybrid form. This sample enables a comparison of differences in the gender participation of conference authors while controlling for heterogeneity among conference series. Using linear and logistic regression models, we identified a positive difference in women's involvement in virtual and hybrid conferences compared to onsite events. However, this effect was due less to the increased participation of women in virtual and hybrid conferences than to the decreased participation of women in the onsite editions of the analyzed conference series."}, "https://arxiv.org/abs/2408.11864": {"title": "Automated and Connected Driving: State-of-the-Art and Implications for Future Scenario Analysis", "link": "https://arxiv.org/abs/2408.11864", "description": "arXiv:2408.11864v1 Announce Type: new \nAbstract: Automated driving can have a huge impact on the transport system in passenger, as well as freight applications; however, market and technological development are difficult to foresee. Therefore, a systems analysis is called for to answer the question: What is the impact of automated driving on the techno-economic performance of transport systems? It is important to quantify the potential impacts not only on a local scale and for specific use cases but for entire transport systems at large. Here, we provide an overview of the current state of automated driving, including academic research in addition to industrial development. For industrial development, we find that it will take at least until 2030-2040 for automated vehicles to be widely available for passenger transport. For freight transport on the other hand, automated vehicles might already be used within the next years at least on motorways. For academic research, we find that most studies on passenger transport consider shared automated vehicles separated from other transport modes and consider specific regions only. For freight transport we find that operational strategies and usage potentials for level 4 and 5 trucks lack alignment with real-life use cases and driving profiles. Based on this, we develop an analytical framework for future research. This includes a mode choice model for passenger transport demand calculations, a total cost of ownership model for freight trucks, transport statistics for freight flows, a microscopic traffic simulation to assess the impact of automated vehicles on traffic flow, and a road network analysis."}, "https://arxiv.org/abs/2408.11867": {"title": "Application of dual-sensitive decision mechanism based on queuing theory in airport taxi management", "link": "https://arxiv.org/abs/2408.11867", "description": "arXiv:2408.11867v1 Announce Type: new \nAbstract: With the rapid development of economy and global trade, flying has increasingly become the main way for people to travel, and taxi is the main transfer tool of the airport, especially in the airport with wide passenger sources and large passenger flow. However, at present, many large airports have the phenomenon of low taxi load rate, unbalanced income and long waiting time for passengers. Therefore, how can drivers make decisions to maximize their benefits, and how can management departments allocate resources and formulate management systems to improve ride efficiency and balance drivers' benefits have become urgent problems to be solved. This paper solves the above problems by establishing the taxi driver dual-sensitive decision model, the longitudinal taxi queuing model and the short-distance passenger re-return forced M/M/1 queuing model."}, "https://arxiv.org/abs/2408.11913": {"title": "Spline tie-decay temporal networks", "link": "https://arxiv.org/abs/2408.11913", "description": "arXiv:2408.11913v1 Announce Type: new \nAbstract: Increasing amounts of data are available on temporal, or time-varying, networks. There have been various representations of temporal network data each of which has different advantages for downstream tasks such as mathematical analysis, visualizations, agent-based and other dynamical simulations on the temporal network, and discovery of useful structure. The tie-decay network is a representation of temporal networks whose advantages include the capability of generating continuous-time networks from discrete time-stamped contact event data with mathematical tractability and a low computational cost. However, the current framework of tie-decay networks is limited in terms of how each discrete contact event can affect the time-dependent tie strength (which we call the kernel). Here we extend the tie-decay network model in terms of the kernel. Specifically, we use a cubic spline function for modeling short-term behavior of the kernel and an exponential decay function for long-term behavior, and graft them together. This spline version of tie-decay network enables delayed and $C^1$-continuous interaction rates between two nodes while it only marginally increases the computational and memory burden relative to the conventional tie-decay network. We show mathematical properties of the spline tie-decay network and numerically showcase it with three tasks: network embedding, a deterministic opinion dynamics model, and a stochastic epidemic spreading model."}, "https://arxiv.org/abs/2408.11962": {"title": "Characterizing Online Toxicity During the 2022 Mpox Outbreak: A Computational Analysis of Topical and Network Dynamics", "link": "https://arxiv.org/abs/2408.11962", "description": "arXiv:2408.11962v1 Announce Type: new \nAbstract: Background: Online toxicity, encompassing behaviors such as harassment, bullying, hate speech, and the dissemination of misinformation, has become a pressing social concern in the digital age. The 2022 Mpox outbreak, initially termed \"Monkeypox\" but subsequently renamed to mitigate associated stigmas and societal concerns, serves as a poignant backdrop to this issue. Objective: In this research, we undertake a comprehensive analysis of the toxic online discourse surrounding the 2022 Mpox outbreak. Our objective is to dissect its origins, characterize its nature and content, trace its dissemination patterns, and assess its broader societal implications, with the goal of providing insights that can inform strategies to mitigate such toxicity in future crises. Methods: We collected more than 1.6 million unique tweets and analyzed them from five dimensions, including context, extent, content, speaker, and intent. Utilizing BERT-based topic modeling and social network community clustering, we delineated the toxic dynamics on Twitter. Results: We identified five high-level topic categories in the toxic online discourse on Twitter, including disease (46.6%), health policy and healthcare (19.3%), homophobia (23.9%), politics (6.0%), and racism (4.1%). Through the toxicity diffusion networks of mentions, retweets, and the top users, we found that retweets of toxic content were widespread, while influential users rarely engaged with or countered this toxicity through retweets. Conclusions: By tracking topical dynamics, we can track the changing popularity of toxic content online, providing a better understanding of societal challenges. Network dynamics spotlight key social media influencers and their intents, indicating that addressing these central figures in toxic discourse can enhance crisis communication and inform policy-making."}, "https://arxiv.org/abs/2408.12035": {"title": "Let Community Rules Be Reflected in Online Content Moderation", "link": "https://arxiv.org/abs/2408.12035", "description": "arXiv:2408.12035v1 Announce Type: new \nAbstract: Content moderation is a widely used strategy to prevent the dissemination of irregular information on social media platforms. Despite extensive research on developing automated models to support decision-making in content moderation, there remains a notable scarcity of studies that integrate the rules of online communities into content moderation. This study addresses this gap by proposing a community rule-based content moderation framework that directly integrates community rules into the moderation of user-generated content. Our experiment results with datasets collected from two domains demonstrate the superior performance of models based on the framework to baseline models across all evaluation metrics. In particular, incorporating community rules substantially enhances model performance in content moderation. The findings of this research have significant research and practical implications for improving the effectiveness and generalizability of content moderation models in online communities."}, "https://arxiv.org/abs/2408.12127": {"title": "An evidence-accumulating drift-diffusion model of competing information spread on networks", "link": "https://arxiv.org/abs/2408.12127", "description": "arXiv:2408.12127v1 Announce Type: new \nAbstract: In this paper, we propose an agent-based model of information spread, grounded on psychological insights on the formation and spread of beliefs. In our model, we consider a network of individuals who share two opposing types of information on a specific topic (e.g., pro- vs. anti-vaccine stances), and the accumulation of evidence supporting either type of information is modelled by means of a drift-diffusion process. After formalising the model, we put forward a campaign of Monte Carlo simulations to identify population-wide behaviours emerging from agents' exposure to different sources of information, investigating the impact of the number and persistence of such sources, and the role of the network structure through which the individuals interact. We find similar emergent behaviours for all network structures considered. When there is a single type of information, the main observed emergent behaviour is consensus. When there are opposing information sources, both consensus or polarisation can result; the latter occurs if the number and persistence of the sources exceeds some threshold values. Importantly, we find the emergent behaviour is mainly influenced by how long the information sources are present for, as opposed to how many sources there are."}, "https://arxiv.org/abs/2408.12311": {"title": "Decoding Decentralized Finance Transactions through Ego Network Motif Mining", "link": "https://arxiv.org/abs/2408.12311", "description": "arXiv:2408.12311v1 Announce Type: new \nAbstract: Decentralized Finance (DeFi) is increasingly studied and adopted for its potential to provide accessible and transparent financial services. Analyzing how investors use DeFi is important for reaching a better understanding of their usage and for regulation purposes. However, analyzing DeFi transactions is challenging due to often incomplete or inaccurate labeled data. This paper presents a method to extract ego network motifs from the token transfer network, capturing the transfer of tokens between users and smart contracts. Our results demonstrate that smart contract methods performing specific DeFi operations can be efficiently identified by analyzing these motifs while providing insights into account activities."}, "https://arxiv.org/abs/2408.12438": {"title": "The impact of climate and wealth on energy consumption in small tropical islands", "link": "https://arxiv.org/abs/2408.12438", "description": "arXiv:2408.12438v1 Announce Type: new \nAbstract: Anthropic activities have a significant causal effect on climatic change but climate has also major impact on human societies. Population vulnerability to natural hazards and limited natural resources are deemed problematic, particularly on small tropical islands. Lifestyles and activities are heavily reliant on energy consumption. The relationship between climatic variations and energy consumption must be clearly understood. We demonstrate that it is possible to determine the impact of climate change on energy consumption. In small tropical islands, the relationship between climate and energy consumption is primarily driven by air conditioner electricity consumption during hotter months. Temperatures above 26{\\deg}C correlate with increased electricity consumption. Energy consumption is sensitive to: (1) climatic seasonal fluctuations, (2) cyclonic activity, (3) temperature warming over the last 20 years. On small tropical islands, demographic and wealth variations also have a significant impact on energy consumption. The relationship between climate and energy consumption suggests reconsidering the production and consumption of carbon-based energy."}, "https://arxiv.org/abs/2408.12085": {"title": "Controllability and Observability of Temporal Hypergraphs", "link": "https://arxiv.org/abs/2408.12085", "description": "arXiv:2408.12085v1 Announce Type: cross \nAbstract: Numerous complex systems, such as those arisen in ecological networks, genomic contact networks, and social networks, exhibit higher-order and time-varying characteristics, which can be effectively modeled using temporal hypergraphs. However, analyzing and controlling temporal hypergraphs poses significant challenges due to their inherent time-varying and nonlinear nature, while most existing methods predominantly target static hypergraphs. In this article, we generalize the notions of controllability and observability to temporal hypergraphs by leveraging tensor and nonlinear systems theory. Specifically, we establish tensor-based rank conditions to determine the weak controllability and observability of temporal hypergraphs. The proposed framework is further demonstrated with synthetic and real-world examples."}, "https://arxiv.org/abs/2408.12210": {"title": "Enhancing Causal Discovery in Financial Networks with Piecewise Quantile Regression", "link": "https://arxiv.org/abs/2408.12210", "description": "arXiv:2408.12210v1 Announce Type: cross \nAbstract: Financial networks can be constructed using statistical dependencies found within the price series of speculative assets. Across the various methods used to infer these networks, there is a general reliance on predictive modelling to capture cross-correlation effects. These methods usually model the flow of mean-response information, or the propagation of volatility and risk within the market. Such techniques, though insightful, don't fully capture the broader distribution-level causality that is possible within speculative markets. This paper introduces a novel approach, combining quantile regression with a piecewise linear embedding scheme - allowing us to construct causality networks that identify the complex tail interactions inherent to financial markets. Applying this method to 260 cryptocurrency return series, we uncover significant tail-tail causal effects and substantial causal asymmetry. We identify a propensity for coins to be self-influencing, with comparatively sparse cross variable effects. Assessing all link types in conjunction, Bitcoin stands out as the primary influencer - a nuance that is missed in conventional linear mean-response analyses. Our findings introduce a comprehensive framework for modelling distributional causality, paving the way towards more holistic representations of causality in financial markets."}, "https://arxiv.org/abs/2408.12449": {"title": "Looking AT the Blue Skies of Bluesky", "link": "https://arxiv.org/abs/2408.12449", "description": "arXiv:2408.12449v1 Announce Type: cross \nAbstract: The pitfalls of centralized social networks, such as Facebook and Twitter/X, have led to concerns about control, transparency, and accountability. Decentralized social networks have emerged as a result with the goal of empowering users. These decentralized approaches come with their own tradeoffs, and therefore multiple architectures exist. In this paper, we conduct the first large-scale analysis of Bluesky, a prominent decentralized microblogging platform. In contrast to alternative approaches (e.g. Mastodon), Bluesky decomposes and opens the key functions of the platform into subcomponents that can be provided by third party stakeholders. We collect a comprehensive dataset covering all the key elements of Bluesky, study user activity and assess the diversity of providers for each sub-components."}, "https://arxiv.org/abs/2310.10114": {"title": "Node classification in networks via simplicial interactions", "link": "https://arxiv.org/abs/2310.10114", "description": "arXiv:2310.10114v2 Announce Type: replace \nAbstract: In the node classification task, it is natural to presume that densely connected nodes tend to exhibit similar attributes. Given this, it is crucial to first define what constitutes a dense connection and to develop a reliable mathematical tool for assessing node cohesiveness. In this paper, we propose a probability-based objective function for semi-supervised node classification that takes advantage of higher-order networks' capabilities. The proposed function reflects the philosophy aligned with the intuition behind classifying within higher order networks, as it is designed to reduce the likelihood of nodes interconnected through higher-order networks bearing different labels. Additionally, we propose the Stochastic Block Tensor Model (SBTM) as a graph generation model designed specifically to address a significant limitation of the traditional stochastic block model, which does not adequately represent the distribution of higher-order structures in real networks. We evaluate the objective function using networks generated by the SBTM, which include both balanced and imbalanced scenarios. Furthermore, we present an approach that integrates the objective function with graph neural network (GNN)-based semi-supervised node classification methodologies, aiming for additional performance gains. Our results demonstrate that in challenging classification scenarios-characterized by a low probability of homo-connections, a high probability of hetero-connections, and limited prior node information-models based on the higher-order network outperform pairwise interaction-based models. Furthermore, experimental results suggest that integrating our proposed objective function with existing GNN-based node classification approaches enhances classification performance by efficiently learning higher-order structures distributed in the network."}, "https://arxiv.org/abs/2402.19017": {"title": "A stochastic model of discussion", "link": "https://arxiv.org/abs/2402.19017", "description": "arXiv:2402.19017v2 Announce Type: replace \nAbstract: We consider the duration of discussions in face-to-face contacts and propose a stochastic model to describe it. It is based on the points of a Levy flight where the duration of each contact corresponds to the size of the clusters produced during the walk. When confronting it to the data measured from proximity sensors, we show that several datasets obtained in different environments, are precisely reproduced by the model fixing a single parameter, the Levy index, to 1.15. We analyze the dynamics of the cluster formation during the walk and compute analytically the cluster size distribution. We find that discussions are first driven by a maximum-entropy geometric distribution and then by a rich-get-richer mechanism reminiscent of preferential-attachment (the more a discussion lasts, the more it is likely to continue). In this model, conversations may be viewed as an aggregation process with a characteristic scale fixed by the mean interaction time between the two individuals."}, "https://arxiv.org/abs/2408.12795": {"title": "From Mobilisation to Radicalisation: Probing the Persistence and Radicalisation of Social Movements Using an Agent-Based Model", "link": "https://arxiv.org/abs/2408.12795", "description": "arXiv:2408.12795v1 Announce Type: new \nAbstract: We are living in an age of protest. Although we have an excellent understanding of the factors that predict participation in protest, we understand little about the conditions that foster a sustained (versus transient) movement. How do interactions between supporters and authorities combine to influence whether and how people engage (i.e., using conventional or radical tactics)? This paper introduces a novel, theoretically-founded and empirically-informed agent-based model (DIMESim) to address these questions. We model the complex interactions between the psychological attributes of the protester (agents), the authority to whom the protests are targeted, and the environment that allows protesters to coordinate with each other -- over time, and at a population scale. Where an authority is responsive and failure is contested, a modest sized conventional movement endured. Where authorities repeatedly and incontrovertibly fail the movement, the population disengaged from action but evidenced an ongoing commitment to radicalism (latent radicalism)."}, "https://arxiv.org/abs/2408.12877": {"title": "The impact of social media on polarization in the society", "link": "https://arxiv.org/abs/2408.12877", "description": "arXiv:2408.12877v1 Announce Type: new \nAbstract: The advent of social media platforms has revolutionized information consumption patterns, with individuals frequently engaging in these platforms for social interactions. This trend has fostered an environment where people gravitate towards information that aligns with their preconceived notions, leading to the formation of echo chambers and polarization within the society. Recently introduced activity-driven models have been successful in capturing the dynamics of information propagation and polarization. The present study uses this model to explore the impact of social media on a polarized society. By considering the varying influence of media, ranging from exposing individuals to contradictory views to reinforcing existing opinions, a supercritical pitchfork bifurcation is observed, triggering a transition from consensus to polarization. The transition points from polarization to consensus are derived analytically and is validated through numerical simulations. This research sheds light on the complex interplay between social media dynamics and societal polarization."}, "https://arxiv.org/abs/2408.13236": {"title": "Large-scale Collective Dynamics in the Three Iterations of the Reddit r/place Experiment", "link": "https://arxiv.org/abs/2408.13236", "description": "arXiv:2408.13236v1 Announce Type: new \nAbstract: The Reddit r/place experiments were a series of online social experiments hosted by Reddit in 2017, 2022, and 2023, where users were allowed to update the colors of pixels in a large shared canvas. The largest of these experiments (in 2022) has attracted over 100 million users who collaborated and competed to produce elaborate artworks that together provide a unique view of the shared interests connecting the diverse communities on Reddit. The user activity traces resulting from these experiments enable us to analyze how online users engage, collaborate, and compete online at an unprecedented scale. However, this requires labeling millions of updates made during the experiments according to their intended artwork.\n  This paper characterizes large-scale activity traces from r/place with a focus on dynamics around successful and failed artworks. To achieve this goal, we propose a dynamic graph clustering algorithm to label artworks by leveraging visual and user-level features. %In the first phase of the algorithm, updates within a snapshot of the experiment are grouped based on proximity, color, and user embeddings. In the second phase, clusters across snapshots are merged via an efficient approximation for the set cover problem. We apply the proposed algorithm to the 2017 edition of r/place and show that it outperforms an existing baseline in terms of accuracy and running time. Moreover, we use our algorithm to identify key factors that distinguish successful from failed artworks in terms of user engagement, collaboration, and competition."}, "https://arxiv.org/abs/2408.12617": {"title": "Can GPT-4 Models Detect Misleading Visualizations?", "link": "https://arxiv.org/abs/2408.12617", "description": "arXiv:2408.12617v1 Announce Type: cross \nAbstract: The proliferation of misleading visualizations online, particularly during critical events like public health crises and elections, poses a significant risk. This study investigates the capability of GPT-4 models (4V, 4o, and 4o mini) to detect misleading visualizations. Utilizing a dataset of tweet-visualization pairs containing various visual misleaders, we test these models under four experimental conditions with different levels of guidance. We show that GPT-4 models can detect misleading visualizations with moderate accuracy without prior training (naive zero-shot) and that performance notably improves when provided with definitions of misleaders (guided zero-shot). However, a single prompt engineering technique does not yield the best results for all misleader types. Specifically, providing the models with misleader definitions and examples (guided few-shot) proves more effective for reasoning misleaders, while guided zero-shot performs better for design misleaders. This study underscores the feasibility of using large vision-language models to detect visual misinformation and the importance of prompt engineering for optimized detection accuracy."}, "https://arxiv.org/abs/2408.12633": {"title": "Melody predominates over harmony in the evolution of musical scales across 96 countries", "link": "https://arxiv.org/abs/2408.12633", "description": "arXiv:2408.12633v1 Announce Type: cross \nAbstract: The standard theory of musical scales since antiquity has been based on harmony, rather than melody. Some recent analyses support either view, and we lack a comparative test on cross-cultural data. We address this longstanding problem through a rigorous, computational comparison of the main theories against 1,314 scales from 96 countries. There is near-universal support for melodic theories, which predict step-sizes of 1-3 semitones. Harmony accounts for the prevalence of some simple-integer-ratio intervals, particularly for music-theoretic scales from Eurasian societies, which may explain their dominance amongst Western scholars. However, harmony poorly predicts scales measured from ethnographic recordings, particularly outside of Eurasia. Overall, we show that the historical emphasis on harmony is misguided and that melody is the primary determinant of the world's musical scales."}, "https://arxiv.org/abs/2408.12635": {"title": "Information and motor constraints shape melodic diversity across cultures", "link": "https://arxiv.org/abs/2408.12635", "description": "arXiv:2408.12635v1 Announce Type: cross \nAbstract: The number of possible melodies is unfathomably large, yet despite this virtually unlimited potential for melodic variation, melodies from different societies can be surprisingly similar. The motor constraint hypothesis accounts for certain similarities, such as scalar motion and contour shape, but not for other major common features, such as repetition, song length, and scale size. Here we investigate the role of information constraints arising from limitations on human memory in shaping these hallmarks of melodies. We measure determinants of information rate in 62 corpora of Folk melodies spanning several continents, finding multiple trade-offs that all act to constrain the information rate across societies. By contrast, 39 corpora of Art music from Europe (including Turkey) show longer, more complex melodies, and increased complexity over time, suggesting different cultural-evolutionary selection pressures in Art and Folk music, possibly due to the use of written versus oral transmission. Our parameter-free model predicts the empirical scale degree distribution using information constraints on scalar motion, melody length, and, most importantly, information rate. This provides strong evidence that information constraints during cultural transmission of music limit the number of notes in a scale, and proposes that preference for intermediate melodic complexity is a fundamental constraint on the cultural evolution of melody."}, "https://arxiv.org/abs/2408.12875": {"title": "Disentangling, Amplifying, and Debiasing: Learning Disentangled Representations for Fair Graph Neural Networks", "link": "https://arxiv.org/abs/2408.12875", "description": "arXiv:2408.12875v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs) have become essential tools for graph representation learning in various domains, such as social media and healthcare. However, they often suffer from fairness issues due to inherent biases in node attributes and graph structure, leading to unfair predictions. To address these challenges, we propose a novel GNN framework, DAB-GNN, that Disentangles, Amplifies, and deBiases attribute, structure, and potential biases in the GNN mechanism. DAB-GNN employs a disentanglement and amplification module that isolates and amplifies each type of bias through specialized disentanglers, followed by a debiasing module that minimizes the distance between subgroup distributions to ensure fairness. Extensive experiments on five datasets demonstrate that DAB-GNN significantly outperforms ten state-of-the-art competitors in terms of achieving an optimal balance between accuracy and fairness."}, "https://arxiv.org/abs/2408.13034": {"title": "Fair Pairs: Fairness-Aware Ranking Recovery from Pairwise Comparisons", "link": "https://arxiv.org/abs/2408.13034", "description": "arXiv:2408.13034v1 Announce Type: cross \nAbstract: Pairwise comparisons based on human judgements are an effective method for determining rankings of items or individuals. However, as human biases perpetuate from pairwise comparisons to recovered rankings, they affect algorithmic decision making. In this paper, we introduce the problem of fairness-aware ranking recovery from pairwise comparisons. We propose a group-conditioned accuracy measure which quantifies fairness of rankings recovered from pairwise comparisons. We evaluate the impact of state-of-the-art ranking recovery algorithms and sampling approaches on accuracy and fairness of the recovered rankings, using synthetic and empirical data. Our results show that Fairness-Aware PageRank and GNNRank with FA*IR post-processing effectively mitigate existing biases in pairwise comparisons and improve the overall accuracy of recovered rankings. We highlight limitations and strengths of different approaches, and provide a Python package to facilitate replication and future work on fair ranking recovery from pairwise comparisons."}, "https://arxiv.org/abs/2408.13075": {"title": "Spectral Recovery in the Labeled SBM", "link": "https://arxiv.org/abs/2408.13075", "description": "arXiv:2408.13075v1 Announce Type: cross \nAbstract: We consider the problem of exact community recovery in the Labeled Stochastic Block Model (LSBM) with $k$ communities, where each pair of vertices is associated with a label from the set $\\{0,1, \\dots, L\\}$. A pair of vertices from communities $i,j$ is given label $\\ell$ with probability $p_{ij}^{(\\ell)}$, and the goal is to recover the community partition. We propose a simple spectral algorithm for exact community recovery, and show that it achieves the information-theoretic threshold in the logarithmic-degree regime, under the assumption that the eigenvalues of certain parameter matrices are distinct and nonzero. Our results generalize recent work of Dhara, Gaudio, Mossel, and Sandon (2023), who showed that a spectral algorithm achieves the information-theoretic threshold in the Censored SBM, which is equivalent to the LSBM with $L = 2$. Interestingly, their algorithm uses eigenvectors from two matrix representations of the graph, while our algorithm uses eigenvectors from $L$ matrices."}, "https://arxiv.org/abs/2307.07960": {"title": "Did the Roll-Out of Community Notes Reduce Engagement With Misinformation on X/Twitter?", "link": "https://arxiv.org/abs/2307.07960", "description": "arXiv:2307.07960v2 Announce Type: replace \nAbstract: Developing interventions that successfully reduce engagement with misinformation on social media is challenging. One intervention that has recently gained great attention is X/Twitter's Community Notes (previously known as \"Birdwatch\"). Community Notes is a crowdsourced fact-checking approach that allows users to write textual notes to inform others about potentially misleading posts on X/Twitter. Yet, empirical evidence regarding its effectiveness in reducing engagement with misinformation on social media is missing. In this paper, we perform a large-scale empirical study to analyze whether the introduction of the Community Notes feature and its roll-out to users in the U.S. and around the world have reduced engagement with misinformation on X/Twitter in terms of retweet volume and likes. We employ Difference-in-Differences (DiD) models and Regression Discontinuity Design (RDD) to analyze a comprehensive dataset consisting of all fact-checking notes and corresponding source tweets since the launch of Community Notes in early 2021. Although we observe a significant increase in the volume of fact-checks carried out via Community Notes, particularly for tweets from verified users with many followers, we find no evidence that the introduction of Community Notes significantly reduced engagement with misleading tweets on X/Twitter. Rather, our findings suggest that Community Notes might be too slow to effectively reduce engagement with misinformation in the early (and most viral) stage of diffusion. Our work emphasizes the importance of evaluating fact-checking interventions in the field and offers important implications to enhance crowdsourced fact-checking strategies on social media."}, "https://arxiv.org/abs/2404.08372": {"title": "Opinion dynamics on signed graphs and graphons: Beyond the piece-wise constant case", "link": "https://arxiv.org/abs/2404.08372", "description": "arXiv:2404.08372v2 Announce Type: replace \nAbstract: In this paper we make use of graphon theory to study opinion dynamics on large undirected networks. The opinion dynamics models that we take into consideration allow for negative interactions between the individuals, i.e. competing entities whose opinions can grow apart. We consider both the repelling model and the opposing model that are studied in the literature. We define the repelling and the opposing dynamics on graphons and we show that their initial value problem's solutions exist and are unique. We then show that the graphon dynamics well approximate the dynamics on large graphs that converge to a graphon. This result applies to large random graphs that are sampled according to a graphon. All these facts are illustrated in an extended numerical example."}, "https://arxiv.org/abs/2405.00129": {"title": "Reconstructing networks from simple and complex contagions", "link": "https://arxiv.org/abs/2405.00129", "description": "arXiv:2405.00129v2 Announce Type: replace \nAbstract: Network scientists often use complex dynamic processes to describe network contagions, but tools for fitting contagion models typically assume simple dynamics. Here, we address this gap by developing a nonparametric method to reconstruct a network and dynamics from a series of node states, using a model that breaks the dichotomy between simple pairwise and complex neighborhood-based contagions. We then show that a network is more easily reconstructed when observed through the lens of complex contagions if it is dense or the dynamic saturates, and that simple contagions are better otherwise."}, "https://arxiv.org/abs/2107.06476": {"title": "Adaptability and the Pivot Penalty in Science and Technology", "link": "https://arxiv.org/abs/2107.06476", "description": "arXiv:2107.06476v2 Announce Type: replace-cross \nAbstract: Scientists and inventors set the direction of their work amidst an evolving landscape of questions, opportunities, and challenges. This paper introduces a measurement framework to quantify how far researchers move from their existing research when producing new works. We apply this framework to millions of scientific publications and patents and uncover a pervasive \"pivot penalty\", where the impact of new research steeply declines the further a researcher moves from their prior work. The pivot penalty applies nearly universally across scientific publishing and patenting and has been growing in magnitude over the past five decades. While creativity frameworks suggest a benefit to exploratory search by researchers and often emphasize outsider advantages in driving breakthroughs, we find little evidence for such an advantage. The pivot penalty is consistent with increasingly narrow specializations of researchers, and when researchers undertake large pivots, a signature of their work is weak engagement with established mixtures of prior knowledge. Unexpected shocks to the research landscape, which may push researchers away from existing areas or pull them into new ones, further demonstrate substantial pivot penalties. COVID-19 provides a high-scale case study, where many researchers engaged the pandemic, yet the pivot penalty remains severe. The pivot penalty generalizes across fields, career stage, productivity, collaboration, and funding contexts, highlighting both the breadth and depth of the adaptive challenge. Overall, the findings point to large and increasing challenges in adapting to new opportunities and threats. The results have implications for individual researchers, research organizations, science policy, and the capacity of science and society as a whole to confront emergent demands."}, "https://arxiv.org/abs/2408.13336": {"title": "Oscillatory and Excitable Dynamics in an Opinion Model with Group Opinions", "link": "https://arxiv.org/abs/2408.13336", "description": "arXiv:2408.13336v1 Announce Type: new \nAbstract: In traditional models of opinion dynamics, each agent in a network has an opinion and changes in opinions arise from pairwise (i.e., dyadic) interactions between agents. However, in many situations, groups of individuals can possess a collective opinion that may differ from the opinions of the individuals. In this paper, we study the effects of group opinions on opinion dynamics. We formulate a hypergraph model in which both individual agents and groups of 3 agents have opinions, and we examine how opinions evolve through both dyadic interactions and group memberships. In some parameter regimes, we find that the presence of group opinions can lead to oscillatory and excitable opinion dynamics. In the oscillatory regime, the mean opinion of the agents in a network has self-sustained oscillations. In the excitable regime, finite-size effects create large but short-lived opinion swings (as in social fads). We develop a mean-field approximation of our model and obtain good agreement with direct numerical simulations. We also show, both numerically and via our mean-field description, that oscillatory dynamics occur only when the number of dyadic and polyadic interactions per agent are not completely correlated. Our results illustrate how polyadic structures, such as groups of agents, can have important effects on collective opinion dynamics."}, "https://arxiv.org/abs/2408.13538": {"title": "Fast Query of Biharmonic Distance in Networks", "link": "https://arxiv.org/abs/2408.13538", "description": "arXiv:2408.13538v1 Announce Type: new \nAbstract: The \\textit{biharmonic distance} (BD) is a fundamental metric that measures the distance of two nodes in a graph. It has found applications in network coherence, machine learning, and computational graphics, among others. In spite of BD's importance, efficient algorithms for the exact computation or approximation of this metric on large graphs remain notably absent. In this work, we provide several algorithms to estimate BD, building on a novel formulation of this metric. These algorithms enjoy locality property (that is, they only read a small portion of the input graph) and at the same time possess provable performance guarantees. In particular, our main algorithms approximate the BD between any node pair with an arbitrarily small additive error $\\eps$ in time $O(\\frac{1}{\\eps^2}\\text{poly}(\\log\\frac{n}{\\eps} ))$. Furthermore, we perform an extensive empirical study on several benchmark networks, validating the performance and accuracy of our algorithms."}, "https://arxiv.org/abs/2408.13647": {"title": "Synthetic Networks That Preserve Edge Connectivity", "link": "https://arxiv.org/abs/2408.13647", "description": "arXiv:2408.13647v1 Announce Type: new \nAbstract: Since true communities within real-world networks are rarely known, synthetic networks with planted ground truths are valuable for evaluating the performance of community detection methods. Of the synthetic network generation tools available, Stochastic Block Models (SBMs) produce networks with ground truth clusters that well approximate input parameters from real-world networks and clusterings. However, we show that SBMs can produce disconnected ground truth clusters, even when given parameters from clusterings where all clusters are connected. Here we describe the REalistic Cluster Connectivity Simulator (RECCS), a technique that modifies an SBM synthetic network to improve the fit to a given clustered real-world network with respect to edge connectivity within clusters, while maintaining the good fit with respect to other network and cluster statistics. Using real-world networks up to 13.9 million nodes in size, we show that RECCS, applied to stochastic block models, results in synthetic networks that have a better fit to cluster edge connectivity than unmodified SBMs, while providing roughly the same quality fit for other network and clustering parameters as unmodified SBMs."}, "https://arxiv.org/abs/2408.14147": {"title": "ORBITAAL: A Temporal Graph Dataset of Bitcoin Entity-Entity Transactions", "link": "https://arxiv.org/abs/2408.14147", "description": "arXiv:2408.14147v1 Announce Type: new \nAbstract: Research on Bitcoin (BTC) transactions is a matter of interest for both economic and network science fields. Although this cryptocurrency is based on a decentralized system, making transaction details freely accessible, making raw blockchain data analyzable is not straightforward due to the Bitcoin protocol specificity and data richness. To address the need for an accessible dataset, we present ORBITAAL, the first comprehensive dataset based on temporal graph formalism. The dataset covers all Bitcoin transactions from January 2009 to January 2021. ORBITAAL provides temporal graph representations of entity-entity transaction networks, snapshots and stream graph. Each transaction value is given in Bitcoin and US dollar regarding daily-based conversion rate. This dataset also provides details on entities such as their global BTC balance and associated public addresses."}, "https://arxiv.org/abs/2408.13521": {"title": "HRGraph: Leveraging LLMs for HR Data Knowledge Graphs with Information Propagation-based Job Recommendation", "link": "https://arxiv.org/abs/2408.13521", "description": "arXiv:2408.13521v1 Announce Type: cross \nAbstract: Knowledge Graphs (KGs) serving as semantic networks, prove highly effective in managing complex interconnected data in different domains, by offering a unified, contextualized, and structured representation with flexibility that allows for easy adaptation to evolving knowledge. Processing complex Human Resources (HR) data, KGs can help in different HR functions like recruitment, job matching, identifying learning gaps, and enhancing employee retention. Despite their potential, limited efforts have been made to implement practical HR knowledge graphs. This study addresses this gap by presenting a framework for effectively developing HR knowledge graphs from documents using Large Language Models. The resulting KG can be used for a variety of downstream tasks, including job matching, identifying employee skill gaps, and many more. In this work, we showcase instances where HR KGs prove instrumental in precise job matching, yielding advantages for both employers and employees. Empirical evidence from experiments with information propagation in KGs and Graph Neural Nets, along with case studies underscores the effectiveness of KGs in tasks such as job and employee recommendations and job area classification. Code and data are available at : https://github.com/azminewasi/HRGraph"}, "https://arxiv.org/abs/2408.13681": {"title": "Smart Home Cyber Insurance Pricing", "link": "https://arxiv.org/abs/2408.13681", "description": "arXiv:2408.13681v1 Announce Type: cross \nAbstract: Our homes are increasingly employing various kinds of Internet of Things (IoT) devices, leading to the notion of smart homes. While this trend brings convenience to our daily life, it also introduces cyber risks. To mitigate such risks, the demand for smart home cyber insurance has been growing rapidly. However, there are no studies on analyzing the competency of smart home cyber insurance policies offered by cyber insurance vendors (i.e., insurers), where `competency' means the insurer is profitable and smart home owners are not overly charged with premiums and/or deductibles. In this paper, we propose a novel framework for pricing smart home cyber insurance, which can be adopted by insurers in practice. Our case studies show, among other things, that insurers are over charging smart home owners in terms of premiums and deductibles."}, "https://arxiv.org/abs/2408.13688": {"title": "Finding the Center and Centroid of a Graph with Multiple Sources", "link": "https://arxiv.org/abs/2408.13688", "description": "arXiv:2408.13688v1 Announce Type: cross \nAbstract: We consider the problem of finding a \"fair\" meeting place when S people want to get together. Specifically, we will consider the cases where a \"fair\" meeting place is defined to be either 1) a node on a graph that minimizes the maximum time/distance to each person or 2) a node on a graph that minimizes the sum of times/distances to each of the sources. In graph theory, these nodes are denoted as the center and centroid of a graph respectively. In this paper, we propose a novel solution for finding the center and centroid of a graph by using a multiple source alternating Dijkstra's Algorithm. Additionally, we introduce a stopping condition that significantly saves on time complexity without compromising the accuracy of the solution. The results of this paper are a low complexity algorithm that is optimal in computing the center of S sources among N nodes and a low complexity algorithm that is close to optimal for computing the centroid of S sources among N nodes."}, "https://arxiv.org/abs/2408.13961": {"title": "Optimizing Luxury Vehicle Dealership Networks: A Graph Neural Network Approach to Site Selection", "link": "https://arxiv.org/abs/2408.13961", "description": "arXiv:2408.13961v1 Announce Type: cross \nAbstract: This study presents a novel application of Graph Neural Networks (GNNs) to optimize dealership network planning for a luxury car manufacturer in the U.S. By conducting a comprehensive literature review on dealership location determinants, the study identifies 65 county-level explanatory variables, augmented by two additional measures of regional interconnectedness derived from social and mobility data. An ablation study involving 34 variable combinations and ten state-of-the-art GNN operators reveals key insights into the predictive power of various variables, particularly highlighting the significance of competition, demographic factors, and mobility patterns in influencing dealership location decisions. The analysis pinpoints seven specific counties as promising targets for network expansion. This research not only illustrates the effectiveness of GNNs in solving complex geospatial decision-making problems but also provides actionable recommendations and valuable methodological insights for industry practitioners."}, "https://arxiv.org/abs/2312.12881": {"title": "Big Tech influence over AI research revisited: memetic analysis of attribution of ideas to affiliation", "link": "https://arxiv.org/abs/2312.12881", "description": "arXiv:2312.12881v2 Announce Type: replace \nAbstract: There exists a growing discourse around the domination of Big Tech on the landscape of artificial intelligence (AI) research, yet our comprehension of this phenomenon remains cursory. This paper aims to broaden and deepen our understanding of Big Tech's reach and power within AI research. It highlights the dominance not merely in terms of sheer publication volume but rather in the propagation of new ideas or memes. Current studies often oversimplify the concept of influence to the share of affiliations in academic papers, typically sourced from limited databases such as arXiv or specific academic conferences.\n  The main goal of this paper is to unravel the specific nuances of such influence, determining which AI ideas are predominantly driven by Big Tech entities. By employing network and memetic analysis on AI-oriented paper abstracts and their citation network, we are able to grasp a deeper insight into this phenomenon. By utilizing two databases: OpenAlex and S2ORC, we are able to perform such analysis on a much bigger scale than previous attempts.\n  Our findings suggest that while Big Tech-affiliated papers are disproportionately more cited in some areas, the most cited papers are those affiliated with both Big Tech and Academia. Focusing on the most contagious memes, their attribution to specific affiliation groups (Big Tech, Academia, mixed affiliation) seems equally distributed between those three groups. This suggests that the notion of Big Tech domination over AI research is oversimplified in the discourse."}, "https://arxiv.org/abs/2211.11673": {"title": "Asymptotically Normal Estimation of Local Latent Network Curvature", "link": "https://arxiv.org/abs/2211.11673", "description": "arXiv:2211.11673v4 Announce Type: replace-cross \nAbstract: Network data, commonly used throughout the physical, social, and biological sciences, consist of nodes (individuals) and the edges (interactions) between them. One way to represent network data's complex, high-dimensional structure is to embed the graph into a low-dimensional geometric space. The curvature of this space, in particular, provides insights about the structure in the graph, such as the propensity to form triangles or present tree-like structures. We derive an estimating function for curvature based on triangle side lengths and the length of the midpoint of a side to the opposing corner. We construct an estimator where the only input is a distance matrix and also establish asymptotic normality. We next introduce a novel latent distance matrix estimator for networks and an efficient algorithm to compute the estimate via solving iterative quadratic programs. We apply this method to the Los Alamos National Laboratory Unified Network and Host dataset and show how curvature estimates can be used to detect a red-team attack faster than naive methods, as well as discover non-constant latent curvature in co-authorship networks in physics. The code for this paper is available at https://github.com/SteveJWR/netcurve, and the methods are implemented in the R package https://github.com/SteveJWR/lolaR."}, "https://arxiv.org/abs/2305.10668": {"title": "MetaGAD: Meta Representation Adaptation for Few-Shot Graph Anomaly Detection", "link": "https://arxiv.org/abs/2305.10668", "description": "arXiv:2305.10668v2 Announce Type: replace-cross \nAbstract: Graph anomaly detection has long been an important problem in various domains pertaining to information security such as financial fraud, social spam and network intrusion. The majority of existing methods are performed in an unsupervised manner, as labeled anomalies in a large scale are often too expensive to acquire. However, the identified anomalies may turn out to be uninteresting data instances due to the lack of prior knowledge. In real-world scenarios, it is often feasible to obtain limited labeled anomalies, which have great potential to advance graph anomaly detection. However, the work exploring limited labeled anomalies and a large amount of unlabeled nodes in graphs to detect anomalies is relatively limited. Therefore, in this paper, we study an important problem of few-shot graph anomaly detection. Nonetheless, it is challenging to fully leverage the information of few-shot anomalous nodes due to the irregularity of anomalies and the overfitting issue in the few-shot learning. To tackle the above challenges, we propose a novel meta-learning based framework, MetaGAD, that learns to adapt the knowledge from self-supervised learning to few-shot supervised learning for graph anomaly detection. In specific, we formulate the problem as a bi-level optimization, ensuring MetaGAD converging to minimizing the validation loss, thus enhancing the generalization capacity. The comprehensive experiments on six real-world datasets with synthetic anomalies and \"organic\" anomalies (available in the datasets) demonstrate the effectiveness of MetaGAD in detecting anomalies with few-shot anomalies. The code is available at https://github.com/XiongxiaoXu/MetaGAD."}, "https://arxiv.org/abs/2308.11076": {"title": "Constraints on Interstellar Sovereignty", "link": "https://arxiv.org/abs/2308.11076", "description": "arXiv:2308.11076v2 Announce Type: replace-cross \nAbstract: Human space exploration and settlement of other planets is becoming increasingly technologically feasible, while mission concepts for remote and crewed missions to nearby star systems continue to be developed. But the long-term success of space settlement also requires extensions and advances in models of governance. This paper provides a synthesis of the physical factors that will constrain the application of sovereignty in space as well as legal precedent on Earth that likely applies to any crewed or uncrewed missions to other stellar systems. The Outer Space Treaty limits the territorial expansion of states into space, but the requirements for oversight of nongovernmental agencies and retention of property ownership enable the extension of state jurisdiction into space. Pragmatic constraints from historical precedent on Earth suggest that new space treaties will be unlikely to succeed and new global space agencies may have limited jurisdiction over states, while hard constraints of the space environment require adherence to technical capabilities, political feasibility, and long-term sustainability. These factors form a three-prong test for assessing the viability of interstellar governance models. This discussion of interstellar governance is intended to further the conversation about sovereignty in space prior to the first intentional launch of any interstellar spacecraft."}, "https://arxiv.org/abs/2404.04300": {"title": "Quantifying age-specific household contacts in Aotearoa New Zealand for infectious disease modelling", "link": "https://arxiv.org/abs/2404.04300", "description": "arXiv:2404.04300v2 Announce Type: replace-cross \nAbstract: Accounting for population age structure and age-specific contact patterns is crucial for accurate modelling of human infectious disease dynamics and impact. A common approach is to use contact matrices, which estimate the number of contacts between individuals of different ages. These contact matrices are frequently based on data collected from populations with very different demographic and socioeconomic characteristics from the population of interest. Here we use a comprehensive household composition dataset based on Aotearoa New Zealand census and administrative data to construct a household contact matrix and a synthetic population that can be used for modelling. We investigate the behaviour of a compartment-based and an agent-based epidemic model parameterised using this data, compared to a commonly used contact matrix that was constructed by projecting international data onto New Zealand's population. We find that using the New Zealand household data, either in a compartment-based model or in an agent-based model, leads to lower attack rates in older age groups compared to using the projected contact matrix. This difference becomes larger when household transmission is more dominant relative to non-household transmission. We provide electronic versions of the synthetic population and household contact matrix for other researchers to use in infectious disease models."}, "https://arxiv.org/abs/2405.01171": {"title": "Modeling pedestrian fundamental diagram based on Directional Statistics", "link": "https://arxiv.org/abs/2405.01171", "description": "arXiv:2405.01171v2 Announce Type: replace-cross \nAbstract: Understanding pedestrian dynamics is crucial for appropriately designing pedestrian spaces. The pedestrian fundamental diagram (FD), which describes the relationship between pedestrian flow and density within a given space, characterizes these dynamics. Pedestrian FDs are significantly influenced by the flow type, such as uni-directional, bi-directional, and crossing flows. However, to the authors' knowledge, generalized pedestrian FDs that are applicable to various flow types have not been proposed. This may be due to the difficulty of using statistical methods to characterize the flow types. The flow types significantly depend on the angles of pedestrian movement; however, these angles cannot be processed by standard statistics due to their periodicity. In this study, we propose a comprehensive model for pedestrian FDs that can describe the pedestrian dynamics for various flow types by applying Directional Statistics. First, we develop a novel statistic describing the pedestrian flow type solely from pedestrian trajectory data using Directional Statistics. Then, we formulate a comprehensive pedestrian FD model that can be applied to various flow types by incorporating the proposed statistics into a traditional pedestrian FD model. The proposed model was validated using actual pedestrian trajectory data. The results confirmed that the model effectively represents the essential nature of pedestrian dynamics, such as the capacity reduction due to conflict of crossing flows and the capacity improvement due to the lane formation in bi-directional flows."}, "https://arxiv.org/abs/2408.14722": {"title": "Pervasive impact of spatial dependence on predictability", "link": "https://arxiv.org/abs/2408.14722", "description": "arXiv:2408.14722v1 Announce Type: new \nAbstract: Understanding the complex nature of spatial information is crucial for problem solving in social and environmental sciences. This study investigates how the underlying patterns of spatial data can significantly influence the outcomes of spatial predictions. Recognizing unique characteristics of spatial data, such as spatial dependence and spatial heterogeneity, we delve into the fundamental differences and similarities between spatial and non-geospatial prediction models. Through the analysis of six different datasets of environment and socio-economic variables, comparing geospatial models with non-geospatial models, our research highlights the pervasive nature of spatial dependence beyond geographical boundaries. This innovative approach not only recognizes spatial dependence in geographic spaces defined by latitude and longitude but also identifies its presence in non-geographic, attribute-based dimensions. Our findings reveal the pervasive influence of spatial dependence on prediction outcomes across various domains, and spatial dependence significantly influences prediction performance across all spaces. Our findings suggest that the strongest spatial dependence is typically found in geographic space for environment variables, a trend that does not uniformly apply to socio-economic variables. This investigation not only advances the theoretical framework for spatial data analysis, but also proposes new methodologies for accurately capturing and expressing spatial dependence under complex conditions. Our research extends spatial analysis to non-geographic dimensions such as social networks and gene expression patterns, emphasizing the role of spatial dependence in improving prediction accuracy, thereby supporting interdisciplinary applications across fields such as geographic information science, environmental science, economics, sociology, and bioinformatics."}, "https://arxiv.org/abs/2408.15117": {"title": "Inferring ghost cities on the globe in newly developed urban areas based on urban vitality with multi-source data", "link": "https://arxiv.org/abs/2408.15117", "description": "arXiv:2408.15117v1 Announce Type: new \nAbstract: Due to rapid urbanization over the past 20 years, many newly developed areas have lagged in socio-economic maturity, creating an imbalance with older cities and leading to the rise of \"ghost cities.\" However, due to the complexity of socio-economic factors, no global studies have measured this phenomenon. We propose a unified framework based on urban vitality theory and multi-source data, validated by various data sources. We derived 8841 natural cities globally with an area over 5 square kiloxmeters and divided each into new urban areas (developed after 2005) and old urban areas (developed before 2005). Urban vitality was gauged using the density of road networks, points of interest (POIs), and population density with 1 km resolution across morphological, functional, and social dimensions. By comparing urban vitality in new and old urban areas, we quantify the ghost cities index (GCI) globally using the theory of urban vitality for the first time. The results reveal that the vitality of new urban areas is 7.69% that of old ones. The top 5% (442) of cities were designated as ghost cities, a finding mirrored by news media and other research. This study sheds light on strategies for sustainable global urbanization, crucial for the United Nations' Sustainable Development Goals."}, "https://arxiv.org/abs/2408.15162": {"title": "The networks of ingredient combination in cuisines around the world", "link": "https://arxiv.org/abs/2408.15162", "description": "arXiv:2408.15162v1 Announce Type: new \nAbstract: Investigating how different ingredients are combined in popular dishes is crucial to reveal the fundamental principles behind the formation of food preferences. Here, we use data from food repositories and network analysis to characterize worldwide cuisines. In our framework, each cuisine is represented as a network, where nodes correspond to ingredient types and weighted links describe how frequently pairs of ingredient types appear together in recipes. The networks of ingredient combinations reveal cuisine-specific patterns, highlighting similarities and differences in gastronomic preferences across different world regions. We find that popular ingredients, recurrent combinations, and the way they are organized within the backbone of the network provide a unique fingerprint for each cuisine. Hence, we demonstrate that networks of ingredient combinations are able to cluster global cuisines into meaningful geo-cultural groups, and can also be used to train models to uniquely identify a cuisine from a subset of its recipes. Our study advances our understanding of food combinations and helps uncover the geography of taste, paving the way for the creation of new and innovative recipes."}, "https://arxiv.org/abs/2408.15186": {"title": "Easy-access online social media metrics can effectively identify misinformation sharing users", "link": "https://arxiv.org/abs/2408.15186", "description": "arXiv:2408.15186v1 Announce Type: new \nAbstract: Misinformation poses a significant challenge studied extensively by researchers, yet acquiring data to identify primary sharers is costly and challenging. To address this, we propose a low-barrier approach to differentiate social media users who are more likely to share misinformation from those who are less likely. Leveraging insights from previous studies, we demonstrate that easy-access online social network metrics -- average daily tweet count, and account age -- can be leveraged to help identify potential low factuality content spreaders on X (previously known as Twitter). We find that higher tweet frequency is positively associated with low factuality in shared content, while account age is negatively associated with it. We also find that some of the effects, namely the effect of the number of accounts followed and the number of tweets produced, differ depending on the number of followers a user has. Our findings show that relying on these easy-access social network metrics could serve as a low-barrier approach for initial identification of users who are more likely to spread misinformation, and therefore contribute to combating misinformation effectively on social media platforms."}, "https://arxiv.org/abs/2408.15211": {"title": "Boosting the clean energy transition through data science", "link": "https://arxiv.org/abs/2408.15211", "description": "arXiv:2408.15211v1 Announce Type: new \nAbstract: The demand for research supporting the development of new policy frameworks for energy saving and conservation has never been more critical. As climate change accelerates and its impacts become increasingly severe, the need for sustainable and resilient socioeconomic systems is increasingly pressing. In response to this global challenge, the ten articles of this special issue seek to explore how advances in Artificial Intelligence and Data Science can drive the energy transition and enhance environmental sustainability."}, "https://arxiv.org/abs/2205.03639": {"title": "Multiplex mobility network and metapopulation epidemic simulations of Italy based on Open Data", "link": "https://arxiv.org/abs/2205.03639", "description": "arXiv:2205.03639v2 Announce Type: cross \nAbstract: The patterns of human mobility play a key role in the spreading of infectious diseases and thus represent a key ingredient of epidemic modeling and forecasting. Unfortunately, as the Covid-19 pandemic has dramatically highlighted, for the vast majority of countries there is no availability of granular mobility data. This hinders the possibility of developing computational frameworks to monitor the evolution of the disease and to adopt timely and adequate prevention policies. Here we show how this problem can be addressed in the case study of Italy. We build a multiplex mobility network based solely on open data, and implement a SIR metapopulation model that allows scenario analysis through data-driven stochastic simulations. The mobility flows that we estimate are in agreement with real-time proprietary data from smartphones. Our modeling approach can thus be useful in contexts where high-resolution mobility data is not available."}, "https://arxiv.org/abs/2408.14520": {"title": "Towards Graph Prompt Learning: A Survey and Beyond", "link": "https://arxiv.org/abs/2408.14520", "description": "arXiv:2408.14520v1 Announce Type: cross \nAbstract: Large-scale \"pre-train and prompt learning\" paradigms have demonstrated remarkable adaptability, enabling broad applications across diverse domains such as question answering, image recognition, and multimodal retrieval. This approach fully leverages the potential of large-scale pre-trained models, reducing downstream data requirements and computational costs while enhancing model applicability across various tasks. Graphs, as versatile data structures that capture relationships between entities, play pivotal roles in fields such as social network analysis, recommender systems, and biological graphs. Despite the success of pre-train and prompt learning paradigms in Natural Language Processing (NLP) and Computer Vision (CV), their application in graph domains remains nascent. In graph-structured data, not only do the node and edge features often have disparate distributions, but the topological structures also differ significantly. This diversity in graph data can lead to incompatible patterns or gaps between pre-training and fine-tuning on downstream graphs. We aim to bridge this gap by summarizing methods for alleviating these disparities. This includes exploring prompt design methodologies, comparing related techniques, assessing application scenarios and datasets, and identifying unresolved problems and challenges. This survey categorizes over 100 relevant works in this field, summarizing general design principles and the latest applications, including text-attributed graphs, molecules, proteins, and recommendation systems. Through this extensive review, we provide a foundational understanding of graph prompt learning, aiming to impact not only the graph mining community but also the broader Artificial General Intelligence (AGI) community."}, "https://arxiv.org/abs/2408.14762": {"title": "Explainable Hierarchical Urban Representation Learning for Commuting Flow Prediction", "link": "https://arxiv.org/abs/2408.14762", "description": "arXiv:2408.14762v1 Announce Type: cross \nAbstract: Commuting flow prediction is an essential task for municipal operations in the real world. Previous studies have revealed that it is feasible to estimate the commuting origin-destination (OD) demand within a city using multiple auxiliary data. However, most existing methods are not suitable to deal with a similar task at a large scale, namely within a prefecture or the whole nation, owing to the increased number of geographical units that need to be maintained. In addition, region representation learning is a universal approach for gaining urban knowledge for diverse metropolitan downstream tasks. Although many researchers have developed comprehensive frameworks to describe urban units from multi-source data, they have not clarified the relationship between the selected geographical elements. Furthermore, metropolitan areas naturally preserve ranked structures, like cities and their inclusive districts, which makes elucidating relations between cross-level urban units necessary. Therefore, we develop a heterogeneous graph-based model to generate meaningful region embeddings at multiple spatial resolutions for predicting different types of inter-level OD flows. To demonstrate the effectiveness of the proposed method, extensive experiments were conducted using real-world aggregated mobile phone datasets collected from Shizuoka Prefecture, Japan. The results indicate that our proposed model outperforms existing models in terms of a uniform urban structure. We extend the understanding of predicted results using reasonable explanations to enhance the credibility of the model."}, "https://arxiv.org/abs/2408.15003": {"title": "Gradient flow-based modularity maximization for community detection in multiplex networks", "link": "https://arxiv.org/abs/2408.15003", "description": "arXiv:2408.15003v1 Announce Type: cross \nAbstract: We propose two methods for the unsupervised detection of communities in undirected multiplex networks. These networks consist of multiple layers that record different relationships between the same entities or incorporate data from different sources. Both methods are formulated as gradient flows of suitable energy functionals: the first (MPBTV) builds on the minimization of a balanced total variation functional, which we show to be equivalent to multiplex modularity maximization, while the second (DGFM3) directly maximizes multiplex modularity. The resulting non-linear matrix-valued ordinary differential equations (ODEs) are solved efficiently by a graph Merriman--Bence--Osher (MBO) scheme. Key to the efficiency is the approximate integration of the discrete linear differential operators by truncated eigendecompositions in the matrix exponential function. Numerical experiments on several real-world multiplex networks show that our methods are competitive with the state of the art with respect to various metrics. Their major benefit is a significant reduction of computational complexity leading to runtimes that are orders of magnitude faster for large multiplex networks."}, "https://arxiv.org/abs/2408.15004": {"title": "Measuring publication relatedness using controlled vocabularies", "link": "https://arxiv.org/abs/2408.15004", "description": "arXiv:2408.15004v1 Announce Type: cross \nAbstract: Measuring the relatedness between scientific publications has important applications in many areas of bibliometrics and science policy. Controlled vocabularies provide a promising basis for measuring relatedness because they address issues that arise when using citation or textual similarity to measure relatedness. While several controlled-vocabulary-based relatedness measures have been developed, there exists no comprehensive and direct test of their accuracy and suitability for different types of research questions. This paper reviews existing measures, develops a new measure, and benchmarks the measures using TREC Genomics data as a ground truth of topics. The benchmark test show that the new measure and the measure proposed by Ahlgren et al. (2020) have differing strengths and weaknesses. These results inform a discussion of which method to choose when studying interdisciplinarity, information retrieval, clustering of science, and researcher topic switching."}, "https://arxiv.org/abs/2408.15222": {"title": "SatHub Panel: Satellite Interference in Observatories Around the World", "link": "https://arxiv.org/abs/2408.15222", "description": "arXiv:2408.15222v1 Announce Type: cross \nAbstract: Satellite constellation interference occurs across astronomical disciplines. We present examples of interference from radio and $\\gamma$-Ray astronomy to optical and spectroscopic interference in ground-based and space-borne facilities. In particular, we discuss the impact of artificial satellites on the Hubble Space Telescope (HST), the High Energy Stereoscopic System (H.E.S.S.), an Imaging Atmospheric Cherenkov Telescope, as well as possible mitigation strategies for the European Southern Observatory 4-metre Multi-Object Spectrograph Telescope (ESO 4MOST). Furthermore, we shed light on how ground-based optical telescopes such as the Oukaimeden Observatory contribute to IAU Centre for the Protection of the Dark and Quiet Sky from Satellite Constellation Interference (IAU CPS) efforts that quantify satellite brightness."}, "https://arxiv.org/abs/2408.15223": {"title": "Quantifying & Mitigating Satellite Constellation Interference with SatHub", "link": "https://arxiv.org/abs/2408.15223", "description": "arXiv:2408.15223v1 Announce Type: cross \nAbstract: This Birds-of-a-Feather (BOF) session on 6 November 2023 was organized by leaders and members of SatHub at the International Astronomical Union Centre for the Protection of the Dark and Quiet Sky from Satellite Constellation Interference (IAU CPS). SatHub is dedicated to observations, data analysis, software, and related activities. The session opened with a talk on the current state of affairs with regards to satellite constellation mitigation, with a focus on optical astronomy, and moved to focused discussion around the top-voted topics. These included tools and techniques for forecasting satellite positions and brightnesses as well as streak detection and masking."}, "https://arxiv.org/abs/2311.07596": {"title": "Graph GOSPA metric: a metric to measure the discrepancy between graphs of different sizes", "link": "https://arxiv.org/abs/2311.07596", "description": "arXiv:2311.07596v2 Announce Type: replace \nAbstract: This paper proposes a metric to measure the dissimilarity between graphs that may have a different number of nodes. The proposed metric extends the generalised optimal subpattern assignment (GOSPA) metric, which is a metric for sets, to graphs. The proposed graph GOSPA metric includes costs associated with node attribute errors for properly assigned nodes, missed and false nodes and edge mismatches between graphs. The computation of this metric is based on finding the optimal assignments between nodes in the two graphs, with the possibility of leaving some of the nodes unassigned. We also propose a lower bound for the metric, which is also a metric for graphs and is computable in polynomial time using linear programming. The metric is first derived for undirected unweighted graphs and it is then extended to directed and weighted graphs. The properties of the metric are demonstrated via simulated and empirical datasets."}, "https://arxiv.org/abs/2305.19233": {"title": "q-neighbor Ising model on a polarized network", "link": "https://arxiv.org/abs/2305.19233", "description": "arXiv:2305.19233v2 Announce Type: replace-cross \nAbstract: In this paper, we examine the interplay between the lobby size $q$ in the $q$-neighbor Ising model of opinion formation [Phys. Rev. E {\\bf 92}, 052105] and the level of overlap $v$ of two fully connected graphs. Results suggest that for each lobby size $q \\ge 3$, a specific level of overlap $v^*$ exists, which destroys initially polarized clusters of opinions. By performing Monte-Carlo simulations, backed by an analytical approach, we show that the dependence of the $v^*$ on the lobby size $q$ is far from trivial in the absence of temperature, showing consecutive maximum and minimum, that additionally depends on the parity of $q$. The temperature is, in general, a destructive factor; its increase leads to the collapse of polarized clusters for smaller values of $v$ and additionally brings a substantial decrease in the level of polarization. However, we show that this behavior is counter-intuitively inverted for specific lobby sizes and temperature ranges."}, "https://arxiv.org/abs/2402.06264": {"title": "LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education", "link": "https://arxiv.org/abs/2402.06264", "description": "arXiv:2402.06264v2 Announce Type: replace-cross \nAbstract: Art appreciation is vital in nurturing critical thinking and emotional intelligence among learners. However, traditional art appreciation education has often been hindered by limited access to art resources, especially for disadvantaged students, and an imbalanced emphasis on STEM subjects in mainstream education. In response to these challenges, recent technological advancements have paved the way for innovative solutions. This study explores the application of multi-modal large language models (MLLMs) in art appreciation education, focusing on developing LLaVA-Docent, a model that leverages these advancements. Our approach involved a comprehensive literature review and consultations with experts in the field, leading to developing a robust data framework. Utilizing this framework, we generated a virtual dialogue dataset that was leveraged by GPT-4. This dataset was instrumental in training the MLLM, named LLaVA-Docent. Six researchers conducted quantitative and qualitative evaluations of LLaVA-Docent to assess its effectiveness, benchmarking it against the GPT-4 model in a few-shot setting. The evaluation process revealed distinct strengths and weaknesses of the LLaVA-Docent model. Our findings highlight the efficacy of LLaVA-Docent in enhancing the accessibility and engagement of art appreciation education. By harnessing the potential of MLLMs, this study makes a significant contribution to the field of art education, proposing a novel methodology that reimagines the way art appreciation is taught and experienced."}, "https://arxiv.org/abs/2404.03718": {"title": "The rules of multiplayer cooperation in networks of communities", "link": "https://arxiv.org/abs/2404.03718", "description": "arXiv:2404.03718v2 Announce Type: replace-cross \nAbstract: Community organization permeates both social and biological complex systems. To study its interplay with behavior emergence, we model mobile structured populations with multiplayer interactions. We derive general analytical methods for evolutionary dynamics under high home fidelity when populations self-organize into networks of asymptotically isolated communities. In this limit, community organization dominates over the network structure and emerging behavior is independent of network topology. We obtain the rules of multiplayer cooperation in networks of communities for different types of social dilemmas. The success of cooperation is a result of the benefits shared amongst communal cooperators outperforming the benefits reaped by defectors in mixed communities. Under weak selection, cooperation can evolve and be stable for any size (Q) and number (M) of communities if the reward-to-cost ratio (V/K) of public goods is higher than a critical value. Community organization is a solid mechanism for sustaining the evolution of cooperation under public goods dilemmas, particularly when populations are organized into a higher number of smaller communities. Contrary to public goods dilemmas relating to production, the multiplayer Hawk-Dove (HD) dilemma is a commons dilemma focusing on the fair consumption of preexisting resources. This game holds mixed results but tends to favour cooperation under larger communities, highlighting that the two types of social dilemmas might lead to solid differences in the behaviour adopted under community structure."}, "https://arxiv.org/abs/2408.15353": {"title": "Connecting Mass-action Models and Network Models for Infectious Diseases", "link": "https://arxiv.org/abs/2408.15353", "description": "arXiv:2408.15353v1 Announce Type: new \nAbstract: Infectious disease modeling is used to forecast epidemics and assess the effectiveness of intervention strategies. Although the core assumption of mass-action models of homogeneously mixed population is often implausible, they are nevertheless routinely used in studying epidemics and provide useful insights. Network models can account for the heterogeneous mixing of populations, which is especially important for studying sexually transmitted diseases. Despite the abundance of research on mass-action and network models, the relationship between them is not well understood. Here, we attempt to bridge the gap by first identifying a spreading rule that results in an exact match between disease spreading on a fully connected network and the classic mass-action models. We then propose a method for mapping epidemic spread on arbitrary networks to a form similar to that of mass-action models. We also provide a theoretical justification for the procedure. Finally, we show the advantages of the proposed methods using synthetic data that is based on an empirical network. These findings help us understand when mass-action models and network models are expected to provide similar results and identify reasons when they do not."}, "https://arxiv.org/abs/2408.15406": {"title": "Intertwined Biases Across Social Media Spheres: Unpacking Correlations in Media Bias Dimensions", "link": "https://arxiv.org/abs/2408.15406", "description": "arXiv:2408.15406v1 Announce Type: new \nAbstract: Media bias significantly shapes public perception by reinforcing stereotypes and exacerbating societal divisions. Prior research has often focused on isolated media bias dimensions such as \\textit{political bias} or \\textit{racial bias}, neglecting the complex interrelationships among various bias dimensions across different topic domains. Moreover, we observe that models trained on existing media bias benchmarks fail to generalize effectively on recent social media posts, particularly in certain bias identification tasks. This shortfall primarily arises because these benchmarks do not adequately reflect the rapidly evolving nature of social media content, which is characterized by shifting user behaviors and emerging trends. In response to these limitations, our research introduces a novel dataset collected from YouTube and Reddit over the past five years. Our dataset includes automated annotations for YouTube content across a broad spectrum of bias dimensions, such as gender, racial, and political biases, as well as hate speech, among others. It spans diverse domains including politics, sports, healthcare, education, and entertainment, reflecting the complex interplay of biases across different societal sectors. Through comprehensive statistical analysis, we identify significant differences in bias expression patterns and intra-domain bias correlations across these domains. By utilizing our understanding of the correlations among various bias dimensions, we lay the groundwork for creating advanced systems capable of detecting multiple biases simultaneously. Overall, our dataset advances the field of media bias identification, contributing to the development of tools that promote fairer media consumption. The comprehensive awareness of existing media bias fosters more ethical journalism, promotes cultural sensitivity, and supports a more informed and equitable public discourse."}, "https://arxiv.org/abs/2408.15873": {"title": "Constructing a Common Ground: Analyzing the quality and usage of International Auxiliary Languages in Wikipedia", "link": "https://arxiv.org/abs/2408.15873", "description": "arXiv:2408.15873v1 Announce Type: new \nAbstract: International Auxiliary Languages (IALs) are constructed languages designed to facilitate communication among speakers of different native languages while fostering equality, efficiency, and cross-cultural understanding. This study focuses on analyzing the editions of IALs on Wikipedia, including Simple English, Esperanto, Ido, Interlingua, Volapuk, Interlingue, and Novial. We compare them with three natural languages: English, Spanish, and Catalan. Our aim is to establish a basis for the use of IALs in Wikipedia as well as showcase a new methodology for categorizing wikis. We found in total there are 1.3 million articles written in these languages and they gather 15.6 million monthly views. Although this is not a negligible amount of content, in comparison with large natural language projects there is still a big room for improvement. We concluded that IAL editions on Wikipedia are similar to other projects, behaving proportionally to their communities' size. Therefore, the key to their growth is augmenting the amount and quality of the content offered in these languages. To that end, we offer a set of statistics to understand and improve these projects, and we developed a webpage that displays our findings to foster knowledge sharing and facilitate the expansion of the IAL communities."}, "https://arxiv.org/abs/2408.15882": {"title": "Recent Decade's Power Outage Data Reveals the Increasing Vulnerability of U", "link": "https://arxiv.org/abs/2408.15882", "description": "arXiv:2408.15882v1 Announce Type: new \nAbstract: Despite significant anecdotal evidence regarding the vulnerability of the U.S. power infrastructure, there is a dearth of longitudinal and nation-level characterization of the spatial and temporal patterns in the frequency and extent of power outages. A data-driven national-level characterization of power outage vulnerability is particularly essential for understanding the urgency and formulating policies to promote the resilience of power infrastructure systems. Recognizing this, we retrieved 179,053,397 county-level power outage records with a 15-minute interval across 3,022 US counties during 2014-2023 to capture power outage characteristics. We focus on three dimensions--power outage intensity, frequency, and duration--and develop multiple metrics to quantify each dimension of power outage vulnerability. The results show that in the past ten years, the vulnerability of U.S. power system has consistently been increasing. Counties experienced an average of 999.4 outages over the decade, affecting an average of more than 540,000 customers per county, with disruptions occurring approximately every week. Coastal areas, particularly in California, Florida and New Jersey, faced more frequent and prolonged outages, while inland regions showed higher outage rates. A concerning increase in outage frequency and intensity was noted, especially after 2017, with a sharp rise in prolonged outages since 2019. The research also found positive association between social vulnerability and outage metrics, with the association becoming stronger over the years under study. Areas with higher social vulnerability experienced more severe and frequent outages, exacerbating challenges in these regions. These findings reveal the much-needed empirical evidence for stakeholders to inform policy formulation and program development for enhancing the resilience of the U.S. power infrastructure."}, "https://arxiv.org/abs/2408.15308": {"title": "Antivax and off-label medication communities on brazilian Telegram: between esotericism as a gateway and the monetization of false miraculous cures", "link": "https://arxiv.org/abs/2408.15308", "description": "arXiv:2408.15308v1 Announce Type: cross \nAbstract: Conspiracy theories, particularly those focused on anti-vaccine narratives and the promotion of off-label medications such as MMS and CDS, have proliferated on Telegram, including in Brazil, finding fertile ground among communities that share esoteric beliefs and distrust towards scientific institutions. In this context, this study seeks to answer how Brazilian conspiracy theory communities on Telegram are characterized and articulated concerning anti-vaccine themes and off-label medications? It is important to highlight that this study is part of a series of seven studies aimed at understanding and characterizing Brazilian conspiracy theory communities on Telegram. This series of seven studies is openly and originally available on the arXiv of Cornell University, applying a mirrored method across all studies, changing only the thematic object of analysis and providing replicable research, including proprietary and original codes developed, contributing to the culture of free and open-source software. Regarding the main findings of this study, it was observed: Themes such as the New World Order and Apocalypse and Survivalism act as significant gateways to anti-vaccine narratives, connecting them to theories of global control; Globalism and New World Order stand out as the main communities receiving invitations from anti-vaccine communities; Occultism and Esotericism emerge as the largest sources of invitations to off-label medication communities, creating a strong connection between esoteric beliefs and the promotion of non-scientific treatments; Anti-vaccine narratives experienced a 290% increase during the COVID-19 pandemic, evidencing a growing interconnectedness with other conspiracy theories; The overlap of themes between anti-vaccine and other conspiracy theories creates an interdependent disinformation network, where different narratives mutually reinforce each other."}, "https://arxiv.org/abs/2408.15311": {"title": "Climate change denial and anti-science communities on brazilian Telegram: climate disinformation as a gateway to broader conspiracy networks", "link": "https://arxiv.org/abs/2408.15311", "description": "arXiv:2408.15311v1 Announce Type: cross \nAbstract: Conspiracy theories related to climate change denial and anti-science have found fertile ground on Telegram, particularly among Brazilian communities that distrust scientific institutions and oppose global environmental policies. This study seeks to answer the research question: how are Brazilian conspiracy theory communities on climate change and anti-science themes characterized and articulated on Telegram? It is worth noting that this study is part of a series of seven studies aimed at understanding and characterizing Brazilian conspiracy theory communities on Telegram. This series of studies is openly and originally available on arXiv from Cornell University, applying a mirrored method across all seven studies, changing only the thematic focus of analysis, and providing replicable investigation methods, including custom-developed and proprietary codes, contributing to the culture of open-source software. Regarding the main findings of this study, the following observations were made: Climate change denial and anti-science communities interact synergistically, creating a complex network that mutually reinforces disinformation narratives; Apocalyptic themes, such as Apocalypse and Survivalism, act as gateways to climate denial, with 5,057 links directed to these communities; Anti-science communities function as gatekeepers, distributing links evenly to theories such as the New World Order and Globalism, among others; During the COVID-19 pandemic, anti-science discussions experienced a significant peak, driven by vaccine disinformation; The intersection between anti-science narratives and esoteric beliefs reinforces the idea of a supposed alternative truth that challenges science; Since 2022, discussions on climate change have evolved to align with global domination theories; Additionally, the UN's 2030 Agenda is portrayed as part of a global conspiracy."}, "https://arxiv.org/abs/2408.15438": {"title": "Regional emission dynamics across phases of the EU ETS", "link": "https://arxiv.org/abs/2408.15438", "description": "arXiv:2408.15438v1 Announce Type: cross \nAbstract: This paper explores the relationship between economic growth and CO$_2$ emissions across European regions from 1990 to 2022, specifically concerning the dynamics of emissions growth rates through different phases of the European Union Emissions Trading System (EU ETS). We find that emissions dynamics exhibit significant volatility influenced by changing policy frameworks. Furthermore, the distribution of emissions growth rates is asymmetric and displays fat tails, suggesting the potential for extreme emissions events. We identify marked disparities across regions: less developed regions experience higher emissions growth rates and greater volatility compared to many developed areas, which show a trend of declining emissions and reduced volatility. Our findings highlight the sensitivity of emissions to policy changes and emphasise the need for clear and effective governance in emissions trading schemes."}, "https://arxiv.org/abs/2305.17479": {"title": "Inferring Individual Direct Causal Effects Under Heterogeneous Peer Influence", "link": "https://arxiv.org/abs/2305.17479", "description": "arXiv:2305.17479v3 Announce Type: replace \nAbstract: Causal inference in networks should account for interference, which occurs when a unit's outcome is influenced by treatments or outcomes of peers. Heterogeneous peer influence (HPI) occurs when a unit's outcome is influenced differently by different peers based on their attributes and relationships, or when each unit has a different susceptibility to peer influence. Existing solutions to estimating direct causal effects under interference consider either homogeneous influence from peers or specific heterogeneous influence mechanisms (e.g., based on local neighborhood structure). This paper presents a methodology for estimating individual direct causal effects in the presence of HPI where the mechanism of influence is not known a priori. We propose a structural causal model for networks that can capture different possible assumptions about network structure, interference conditions, and causal dependence and enables reasoning about identifiability in the presence of HPI. We find potential heterogeneous contexts using the causal model and propose a novel graph neural network-based estimator to estimate individual direct causal effects. We show that state-of-the-art methods for individual direct effect estimation produce biased results in the presence of HPI, and that our proposed estimator is robust."}, "https://arxiv.org/abs/2306.07478": {"title": "BotSCL: Heterophily-aware Social Bot Detection with Supervised Contrastive Learning", "link": "https://arxiv.org/abs/2306.07478", "description": "arXiv:2306.07478v3 Announce Type: replace \nAbstract: Detecting ever-evolving social bots has become increasingly challenging. Advanced bots tend to interact more with humans as a camouflage to evade detection. While graph-based detection methods can exploit various relations in social networks to model node behaviors, the aggregated information from neighbors largely ignore the inherent heterophily, i.e., the connections between different classes of accounts. Message passing mechanism on heterophilic edges can lead to feature mixture between bots and normal users, resulting in more false negatives. In this paper, we present BotSCL, a heterophily-aware contrastive learning framework that can adaptively differentiate neighbor representations of heterophilic relations while assimilating the representations of homophilic neighbors. Specifically, we employ two graph augmentation methods to generate different graph views and design a channel-wise and attention-free encoder to overcome the limitation of neighbor information summing. Supervised contrastive learning is used to guide the encoder to aggregate class-specific information. Extensive experiments on two social bot detection benchmarks demonstrate that BotSCL outperforms baseline approaches including the state-of-the-art bot detection approaches, partially heterophilic GNNs and self-supervised contrast learning methods."}, "https://arxiv.org/abs/2312.11343": {"title": "Quantifying Barriers of Urban Mobility", "link": "https://arxiv.org/abs/2312.11343", "description": "arXiv:2312.11343v2 Announce Type: replace \nAbstract: Barriers in cities, such as administrative boundaries, natural obstacles, railways or major roads are thought to induce segregation. However, the empirical knowledge about this phenomenon is limited. Here, we present a network science framework to assess barriers to urban mobility along their hierarchy, across residential areas and visited amenities. Using GPS mobility data, we construct a network of blocks from the sequence of individual stays in a major European city. A community detection algorithm allows us to partition this network into non-overlapping areas of dense mobility clusters, in which the effect of transportation hubs can be tuned with a parameter. We apply the Symmetric Area Difference index to quantify the overlap between these mobility clusters and the polygons of urban area separated by barriers. Reducing the effect of transportation hubs results in smaller scale mobility clusters that fit better to lower rank administrative or road barriers compared to their higher rank pairs. We find that characteristic urban barriers can replace each other in dividing mobility clusters of different scales. Next, we define the Barrier Crossing Ratio, the fraction of barrier crossings that bridge mobility clusters. The decomposition of this indicator by origins and destinations suggests a significantly higher impact of barriers on those who live closer to the city center and smaller impact on visits to complex amenities. These results contribute to the ongoing discourse on urban segregation, emphasizing the importance of barriers to urban mobility in shaping interactions and mixing."}, "https://arxiv.org/abs/2408.16022": {"title": "Characterizing Physician Referral Networks with Ricci Curvature", "link": "https://arxiv.org/abs/2408.16022", "description": "arXiv:2408.16022v1 Announce Type: new \nAbstract: Identifying (a) systemic barriers to quality healthcare access and (b) key indicators of care efficacy in the United States remains a significant challenge. To improve our understanding of regional disparities in care delivery, we introduce a novel application of curvature, a geometrical-topological property of networks, to Physician Referral Networks. Our initial findings reveal that Forman-Ricci and Ollivier-Ricci curvature measures, which are known for their expressive power in characterizing network structure, offer promising indicators for detecting variations in healthcare efficacy while capturing a range of significant regional demographic features. We also present APPARENT, an open-source tool that leverages Ricci curvature and other network features to examine correlations between regional Physician Referral Networks structure, local census data, healthcare effectiveness, and patient outcomes."}, "https://arxiv.org/abs/2408.16270": {"title": "Autocorrelation properties of temporal networks governed by dynamic node variables", "link": "https://arxiv.org/abs/2408.16270", "description": "arXiv:2408.16270v1 Announce Type: new \nAbstract: We study synthetic temporal networks whose evolution is determined by stochastically evolving node variables - synthetic analogues of, e.g., temporal proximity networks of mobile agents. We quantify the long-timescale correlations of these evolving networks by an autocorrelative measure of edge persistence. Several distinct patterns of autocorrelation arise, including power-law decay and exponential decay, depending on the choice of node-variable dynamics and connection probability function. Our methods are also applicable in wider contexts; our temporal network models are tractable mathematically and in simulation, and our long-term memory quantification is analytically tractable and straightforwardly computable from temporal network data."}, "https://arxiv.org/abs/2408.16295": {"title": "IC always bad? : Information Cocooning as a Group Emotional Stabilization Role in Social Networks", "link": "https://arxiv.org/abs/2408.16295", "description": "arXiv:2408.16295v1 Announce Type: new \nAbstract: This research aims to investigate the effects of information cocooning on group mood changes caused by information spreading. The simulation of the realistic network evolution process is realized at the structural level by building a network evolution model based on individual viewpoints. Abstracting the accuracy of the real intelligent recommendation process by setting RA (Recommended Accuracy). By analyzing the information cocoon effect due to the recommendation in the comment section, we provide the structural basis of spreading for the dynamics model. A dynamics model of emotion spreading is developed to explore the trend of individual emotion spreading and to quantify the change of group emotion. Through experiments and analysis, this paper concludes that the information cocoon has a positive effect on the stability of group emotions, and that the H-CAC (Hidden Comment Area Cocoon) structure exists widely in real online social networks, and can produce a protective \"harbor\" effect in the competition of public opinion and cognitive games. The validity of the model is verified by comparison with real cases and generalization ability experiments. This work provides a multi-perspective analysis and visualization, providing more quantitative results. The research is expected to provide new perspectives and tools for understanding the reality of information cocooning and expanding the scenarios of its use."}, "https://arxiv.org/abs/2408.16308": {"title": "AdaMotif: Graph Simplification via Adaptive Motif Design", "link": "https://arxiv.org/abs/2408.16308", "description": "arXiv:2408.16308v1 Announce Type: new \nAbstract: With the increase of graph size, it becomes difficult or even impossible to visualize graph structures clearly within the limited screen space. Consequently, it is crucial to design effective visual representations for large graphs. In this paper, we propose AdaMotif, a novel approach that can capture the essential structure patterns of large graphs and effectively reveal the overall structures via adaptive motif designs. Specifically, our approach involves partitioning a given large graph into multiple subgraphs, then clustering similar subgraphs and extracting similar structural information within each cluster. Subsequently, adaptive motifs representing each cluster are generated and utilized to replace the corresponding subgraphs, leading to a simplified visualization. Our approach aims to preserve as much information as possible from the subgraphs while simplifying the graph efficiently. Notably, our approach successfully visualizes crucial community information within a large graph. We conduct case studies and a user study using real-world graphs to validate the effectiveness of our proposed approach. The results demonstrate the capability of our approach in simplifying graphs while retaining important structural and community information."}, "https://arxiv.org/abs/2408.16013": {"title": "On the practicalities of producing a nuclear weapon using high-assay low-enriched uranium", "link": "https://arxiv.org/abs/2408.16013", "description": "arXiv:2408.16013v1 Announce Type: cross \nAbstract: It was recently argued by Kemp et al. that HALEU (high-assay low-enriched uranium, or uranium enriched up to 19.75\\%) can conceivably be used to produce a nuclear weapon and on this basis civilian enrichment limits should be lowered to 10% or 12%. We find their argument unconvincing in several respects."}, "https://arxiv.org/abs/2408.16288": {"title": "OpenFGL: A Comprehensive Benchmarks for Federated Graph Learning", "link": "https://arxiv.org/abs/2408.16288", "description": "arXiv:2408.16288v1 Announce Type: cross \nAbstract: Federated graph learning (FGL) has emerged as a promising distributed training paradigm for graph neural networks across multiple local systems without direct data sharing. This approach is particularly beneficial in privacy-sensitive scenarios and offers a new perspective on addressing scalability challenges in large-scale graph learning. Despite the proliferation of FGL, the diverse motivations from practical applications, spanning various research backgrounds and experimental settings, pose a significant challenge to fair evaluation. To fill this gap, we propose OpenFGL, a unified benchmark designed for the primary FGL scenarios: Graph-FL and Subgraph-FL. Specifically, OpenFGL includes 38 graph datasets from 16 application domains, 8 federated data simulation strategies that emphasize graph properties, and 5 graph-based downstream tasks. Additionally, it offers 18 recently proposed SOTA FGL algorithms through a user-friendly API, enabling a thorough comparison and comprehensive evaluation of their effectiveness, robustness, and efficiency. Empirical results demonstrate the ability of FGL while also revealing its potential limitations, offering valuable insights for future exploration in this thriving field."}, "https://arxiv.org/abs/2408.16317": {"title": "Healthcare Utilization Patterns Among Migrant Populations: Increased Readmissions Suggest Poorer Access", "link": "https://arxiv.org/abs/2408.16317", "description": "arXiv:2408.16317v1 Announce Type: cross \nAbstract: Equal access to health ensures that all citizens, regardless of socio-economic status, can achieve optimal health, leading to a more productive, equitable, and resilient society. Yet, migrant populations were frequently observed to have lower access to health. The reasons for this are not entirely clear and may include language barriers, a lack of knowledge of the healthcare system, and selective migration (a \"healthy migrant\" effect). We use extensive medical claims data from Austria (13 million hospital stays of approximately 4 million individuals) to compare the healthcare utilization patterns between Austrians and non-Austrians. We looked at the differences in primary diagnoses and hospital sections of initial hospital admission across different nationalities. We hypothesize that cohorts experiencing the healthy migrant effect show lower readmission rates after hospitalization compared to migrant populations that are in poorer health but show lower hospitalization rates due to barriers in access. We indeed find that all nationalities showed lower hospitalization rates than Austrians, except for Germans, who exhibit a similar healthcare usage to Austrians. Although around 20\\% of the population has a migration background, non-Austrian citizens account for only 9.4% of the hospital patients and 9.79% of hospital nights. However, results for readmission rates are much more divergent. Nationalities like Hungary, Romania, and Turkey (females) show decreased readmission rates in line with the healthy migrant effect. Patients from Russia, Serbia, and Turkey (males) show increased readmissions, suggesting that their lower hospitalization rates are more likely due to access barriers. Considering the surge in migration, our findings shed light on healthcare access and usage behaviours across patients with different nationalities, offering new insights and perspectives."}, "https://arxiv.org/abs/2408.16508": {"title": "Branch-and-cut algorithms for colorful components problems", "link": "https://arxiv.org/abs/2408.16508", "description": "arXiv:2408.16508v1 Announce Type: cross \nAbstract: We tackle three optimization problems in which a colored graph, where each node is assigned a color, must be partitioned into colorful connected components. A component is defined as colorful if each color appears at most once. The problems differ in the objective function, which determines which partition is the best one. These problems have applications in community detection, cybersecurity, and bioinformatics. We present integer non-linear formulations, which are then linearized using standard techniques. To solve these formulations, we develop exact branch-and-cut algorithms, embedding various improving techniques, such as valid inequalities, bounds limiting the number of variables, and warm-start and preprocessing techniques. Extensive computational tests on benchmark instances demonstrate the effectiveness of the proposed procedures. The branch-and-cut algorithms can solve reasonably sized instances efficiently. To the best of our knowledge, we are the first to propose an exact algorithm for solving these problems."}, "https://arxiv.org/abs/2408.16570": {"title": "Predictability maximization and the origins of word order harmony", "link": "https://arxiv.org/abs/2408.16570", "description": "arXiv:2408.16570v1 Announce Type: cross \nAbstract: We address the linguistic problem of the sequential arrangement of a head and its dependents from an information theoretic perspective. In particular, we consider the optimal placement of a head that maximizes the predictability of the sequence. We assume that dependents are statistically independent given a head, in line with the open-choice principle and the core assumptions of dependency grammar. We demonstrate the optimality of harmonic order, i.e., placing the head last maximizes the predictability of the head whereas placing the head first maximizes the predictability of dependents. We also show that postponing the head is the optimal strategy to maximize its predictability while bringing it forward is the optimal strategy to maximize the predictability of dependents. We unravel the advantages of the strategy of maximizing the predictability of the head over maximizing the predictability of dependents. Our findings shed light on the placements of the head adopted by real languages or emerging in different kinds of experiments."}, "https://arxiv.org/abs/2408.16629": {"title": "LLMs generate structurally realistic social networks but overestimate political homophily", "link": "https://arxiv.org/abs/2408.16629", "description": "arXiv:2408.16629v1 Announce Type: cross \nAbstract: Generating social networks is essential for many applications, such as epidemic modeling and social simulations. Prior approaches either involve deep learning models, which require many observed networks for training, or stylized models, which are limited in their realism and flexibility. In contrast, LLMs offer the potential for zero-shot and flexible network generation. However, two key questions are: (1) are LLM's generated networks realistic, and (2) what are risks of bias, given the importance of demographics in forming social ties? To answer these questions, we develop three prompting methods for network generation and compare the generated networks to real social networks. We find that more realistic networks are generated with \"local\" methods, where the LLM constructs relations for one persona at a time, compared to \"global\" methods that construct the entire network at once. We also find that the generated networks match real networks on many characteristics, including density, clustering, community structure, and degree. However, we find that LLMs emphasize political homophily over all other types of homophily and overestimate political homophily relative to real-world measures."}, "https://arxiv.org/abs/2307.01282": {"title": "Normalized mutual information is a biased measure for classification and community detection", "link": "https://arxiv.org/abs/2307.01282", "description": "arXiv:2307.01282v2 Announce Type: replace \nAbstract: Normalized mutual information is widely used as a similarity measure for evaluating the performance of clustering and classification algorithms. In this paper, we argue that results returned by the normalized mutual information are biased for two reasons: first, because they ignore the information content of the contingency table and, second, because their symmetric normalization introduces spurious dependence on algorithm output. We introduce a modified version of the mutual information that remedies both of these shortcomings. As a practical demonstration of the importance of using an unbiased measure, we perform extensive numerical tests on a basket of popular algorithms for network community detection and show that one's conclusions about which algorithm is best are significantly affected by the biases in the traditional mutual information."}, "https://arxiv.org/abs/2403.17208": {"title": "Constant-selection evolutionary dynamics on weighted networks", "link": "https://arxiv.org/abs/2403.17208", "description": "arXiv:2403.17208v2 Announce Type: replace \nAbstract: The population structure often impacts evolutionary dynamics. In constant-selection evolutionary dynamics between two types, amplifiers of selection are networks that promote the fitter mutant to take over the entire population, and suppressors of selection do the opposite. It has been shown that most undirected and unweighted networks are amplifiers of selection under a common updating rule and initial condition. Here, we extensively investigate how edge weights influence selection on undirected networks. We show that random edge weights make small networks less amplifying than the corresponding unweighted networks in a majority of cases and also make them suppressors of selection (i.e., less amplifying than the complete graph, or equivalently, the Moran process) in many cases. Qualitatively, the same result holds true for larger empirical networks. These results suggest that amplifiers of selection are not as common for weighted networks as for unweighted counterparts."}, "https://arxiv.org/abs/2402.07113": {"title": "Statistical properties of probabilistic context-sensitive grammars", "link": "https://arxiv.org/abs/2402.07113", "description": "arXiv:2402.07113v3 Announce Type: replace-cross \nAbstract: Probabilistic context-free grammars (PCFGs), which are commonly used to generate trees randomly, have been well analyzed theoretically, leading to applications in various domains. Despite their utility, the distributions that the grammar can express are limited to those in which the distribution of a subtree depends only on its root and not on its context. This limitation presents a challenge for modeling various real-world phenomena, such as natural languages. To overcome this limitation, a probabilistic context-sensitive grammar (PCSG) is introduced, where the distribution of a subtree depends on its context. Numerical analysis of a PCSG reveals that the distribution of a symbol does not constitute a qualitative difference from that in the context-free case, but mutual information does. Furthermore, a novel metric introduced to directly quantify the breaking of this limitation detects a distinct difference between PCFGs and PCSGs. This metric, applicable to an arbitrary distribution of a tree, allows for further investigation and characterization of various tree structures that PCFGs cannot express."}, "https://arxiv.org/abs/2402.10335": {"title": "Correlation Clustering with Vertex Splitting", "link": "https://arxiv.org/abs/2402.10335", "description": "arXiv:2402.10335v2 Announce Type: replace-cross \nAbstract: We explore Cluster Editing and its generalization Correlation Clustering with a new operation called permissive vertex splitting which addresses finding overlapping clusters in the face of uncertain information. We determine that both problems are NP-hard, yet they exhibit significant differences in parameterized complexity and approximability. For Cluster Editing with Permissive Vertex Splitting, we show a polynomial kernel when parameterized by the solution size and develop a polynomial-time algorithm with approximation factor 7. In the case of Correlation Clustering, we establish para-NP-hardness when parameterized by solution size and demonstrate that computing an $n^{1-\\epsilon}$-approximation is NP-hard for any constant $\\epsilon > 0$. Additionally, we extend the established link between Correlation Clustering and Multicut to the setting with permissive vertex splitting."}, "https://arxiv.org/abs/2408.16877": {"title": "Longitudinal Modularity, a Modularity for Link Streams", "link": "https://arxiv.org/abs/2408.16877", "description": "arXiv:2408.16877v1 Announce Type: new \nAbstract: Temporal networks are commonly used to model real-life phenomena. When these phenomena represent interactions and are captured at a fine-grained temporal resolution, they are modeled as link streams. Community detection is an essential network analysis task. Although many methods exist for static networks, and some methods have been developed for temporal networks represented as sequences of snapshots, few works can handle link streams. This article introduces the first adaptation of the well-known Modularity quality function to link streams. Unlike existing methods, it is independent of the time scale of analysis. After introducing the quality function, and its relation to existing static and dynamic definitions of Modularity, we show experimentally its relevance for dynamic community evaluation."}, "https://arxiv.org/abs/2408.16933": {"title": "Network Inference in Public Administration: Questions, Challenges, and Models of Causality", "link": "https://arxiv.org/abs/2408.16933", "description": "arXiv:2408.16933v1 Announce Type: new \nAbstract: Descriptive and inferential social network analysis has become common in public administration studies of network governance and management. A large literature has developed in two broad categories: antecedents of network structure, and network effects and outcomes. A new topic is emerging on network interventions that applies knowledge of network formation and effects to actively intervene in the social context of interaction. Yet, the question remains how might scholars deploy and determine the impact of network interventions. Inferential network analysis has primarily focused on statistical simulations of network distributions to produce probability estimates on parameters of interest in observed networks, e.g. ERGMs. There is less attention to design elements for causal inference in the network context, such as experimental interventions, randomization, control and comparison networks, and spillovers. We advance a number of important questions for network research, examine important inferential challenges and other issues related to inference in networks, and focus on a set of possible network inference models. We categorize models of network inference into (i) observational studies of networks, using descriptive and stochastic methods that lack intervention, randomization, or comparison networks; (ii) simulation studies that leverage computational resources for generating inference; (iii) natural network experiments, with unintentional network-based interventions; (iv) network field experiments, with designed interventions accompanied by comparison networks; and (v) laboratory experiments that design and implement randomization to treatment and control networks. The article offers a guide to network researchers interested in questions, challenges, and models of inference for network analysis in public administration."}, "https://arxiv.org/abs/2408.17051": {"title": "Service-Oriented AoI Modeling and Analysis for Non-Terrestrial Networks", "link": "https://arxiv.org/abs/2408.17051", "description": "arXiv:2408.17051v1 Announce Type: new \nAbstract: To achieve truly seamless global intelligent connectivity, non-terrestrial networks (NTN) mainly composed of low earth orbit (LEO) satellites and drones are recognized as important components of the future 6G network architecture. Meanwhile, the rapid advancement of the Internet of Things (IoT) has led to the proliferation of numerous applications with stringent requirements for timely information delivery. The Age of Information (AoI), a critical performance metric for assessing the freshness of data in information update systems, has gained significant importance in this context. However, existing modeling and analysis work on AoI mainly focuses on terrestrial networks, and the distribution characteristics of ground nodes and the high dynamics of satellites have not been fully considered, which poses challenges for more accurate evaluation. Against this background, we model the ground nodes as a hybrid distribution of Poisson point process (PPP) and Poisson cluster process (PCP) to capture the impact of ground node distribution on the AoI of status update packet transmission supported by UAVs and satellites in NTN, and the visibility and cross-traffic characteristics of satellites are additionally considered. We derived the average AoI for the system in these two different situations and examined the impact of various network parameters on AoI performance."}, "https://arxiv.org/abs/2408.17107": {"title": "How Many Lines to Paint the City: Exact Edge-Cover in Temporal Graphs", "link": "https://arxiv.org/abs/2408.17107", "description": "arXiv:2408.17107v1 Announce Type: new \nAbstract: Logistics and transportation networks require a large amount of resources to realize necessary connections between locations and minimizing these resources is a vital aspect of planning research. Since such networks have dynamic connections that are only available at specific times, intricate models are needed to portray them accurately. In this paper, we study the problem of minimizing the number of resources needed to realize a dynamic network, using the temporal graphs model. In a temporal graph, edges appear at specific points in time. Given a temporal graph and a natural number k, we ask whether we can cover every temporal edge exactly once using at most k temporal journeys; in a temporal journey consecutive edges have to adhere to the order of time. We conduct a thorough investigation of the complexity of the problem with respect to four dimensions: (a) whether the type of the temporal journey is a walk, a trail, or a path; (b) whether the chronological order of edges in the journey is strict or non-strict; (c) whether the temporal graph is directed or undirected; (d) whether the start and end points of each journey are given or not. We almost completely resolve the complexity of all these problems and provide dichotomies for each one of them with respect to k."}, "https://arxiv.org/abs/2408.17178": {"title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond", "link": "https://arxiv.org/abs/2408.17178", "description": "arXiv:2408.17178v1 Announce Type: new \nAbstract: This paper provides the background, methodology and validation for constructing a representation of the European high-voltage grid, including and above 200 kV, based on public data provided by OpenStreetMap. The model-independent grid dataset is published under the Open Data Commons Open Database (ODbL 1.0) licence and can be used for large-scale electricity as well as energy system modelling. The dataset and workflow are provided as part of PyPSA-Eur -- an open-source, sector-coupled optimisation model of the European energy system. By integrating with the codebase for initiatives such as PyPSA-Earth, the value of open and maintainable high-voltage grid data extends to the global context. By accessing the latest data through the the Overpass turbo API, the dataset can be easily reconstructed and updated within minutes. To assess the data quality, this paper further compares the dataset with official statistics and representative model runs using PyPSA-Eur based on different electricity grid representations."}, "https://arxiv.org/abs/2408.16992": {"title": "Exaptation: Academic mentees' career pathway to be independent and impactful", "link": "https://arxiv.org/abs/2408.16992", "description": "arXiv:2408.16992v1 Announce Type: cross \nAbstract: In science, mentees often follow their mentors' career paths, but exceptional mentees frequently break from this routine, sometimes even outperforming their mentors. However, the pathways to independence for these excellent mentees and their interactions with mentors remain unclear. We analyzed the careers of over 500,000 mentees in Chemistry, Neuroscience, and Physics over the past 60 years to examine the strategies mentees employ in selecting research topics relative to their mentors, how these strategies evolve, and their resulting impact. Utilizing co-citation network analysis and a topic-specific impact allocation algorithm, we mapped the topic territory for each mentor-mentee pair and quantified their academic impact accrued within the topic. Our findings reveal mentees tend to engage with their mentors' less-dominated topics and explore new topics at the same time, and through this exaptive process, they begin to progressively establish their own research territories. This trend is particularly pronounced among those who outperform their mentors. Moreover, we identified an inverted U-shaped curve between the extent of topic divergence and the mentees' long-term impact, suggesting a moderate divergence from the mentors' research focus optimizes the mentees' academic impact. Finally, along the path to independence, increased coauthorship with mentors impedes the mentees' impact, whereas extending their collaboration networks with the mentors' former collaborators proves beneficial. These findings fill a crucial gap in understanding how mentees' research topic selection strategies affect academic success and offer valuable guidance for early-career researchers on pursuing independent research paths."}, "https://arxiv.org/abs/2312.00910": {"title": "Effectiveness of probabilistic contact tracing in epidemic containment: the role of super-spreaders and transmission path reconstruction", "link": "https://arxiv.org/abs/2312.00910", "description": "arXiv:2312.00910v2 Announce Type: replace-cross \nAbstract: The recent COVID-19 pandemic underscores the significance of early-stage non-pharmacological intervention strategies. The widespread use of masks and the systematic implementation of contact tracing strategies provide a potentially equally effective and socially less impactful alternative to more conventional approaches, such as large-scale mobility restrictions. However, manual contact tracing faces strong limitations in accessing the network of contacts, and the scalability of currently implemented protocols for smartphone-based digital contact tracing becomes impractical during the rapid expansion phases of the outbreaks, due to the surge in exposure notifications and associated tests. A substantial improvement in digital contact tracing can be obtained through the integration of probabilistic techniques for risk assessment that can more effectively guide the allocation of new diagnostic tests. In this study, we first quantitatively analyze the diagnostic and social costs associated with these containment measures based on contact tracing, employing three state-of-the-art models of SARS-CoV-2 spreading. Our results suggest that probabilistic techniques allow for more effective mitigation at a lower cost. Secondly, our findings reveal a remarkable efficacy of probabilistic contact-tracing techniques in performing backward and multi-step tracing and capturing super-spreading events."}, "https://arxiv.org/abs/2409.00059": {"title": "Quantum Ecosystem Research and Analysis in Colombia", "link": "https://arxiv.org/abs/2409.00059", "description": "arXiv:2409.00059v1 Announce Type: new \nAbstract: The rapid growth of quantum computing in the last few years has led many countries to make relevant public investments in that field. However, Colombia lacks any investment or legislation in this area. Most previous studies have focused on other countries, contributing to the region's significant lag. In this paper, we propose efforts to include quantum computing as a fundamental pillar of Colombia's development plan. In this research we involved three stakeholders: academia (Universidad Nacional de Colombia), industry (Alianza Team), and government (Vice Ministry of Digital Transformation). We anticipate that our work will provide a connection between all stakeholders involved in public investments in quantum technology in the country and will facilitate its legislation."}, "https://arxiv.org/abs/2409.00067": {"title": "Projections of Earth's technosphere", "link": "https://arxiv.org/abs/2409.00067", "description": "arXiv:2409.00067v1 Announce Type: new \nAbstract: This study uses methods from futures studies to develop a set of ten self-consistent scenarios for Earth's 1000-year future, which can serve as examples for defining technosignature search strategies. We apply a novel worldbuilding pipeline that evaluates the dimensions of human needs in each scenario as a basis for defining the observable properties of the technosphere. Our scenarios include three with zero-growth stability, two that have collapsed into a stable state, one that oscillates between growth and collapse, and four that continue to grow. Only one scenario includes rapid growth that could lead to interstellar expansion. We examine absorption spectral features for a few scenarios to illustrate that nitrogen dioxide can serve as a technosignature to distinguish between present-day Earth, pre-agricultural Earth, and an industrial 1000-year future Earth. Three of our scenarios are spectrally indistinguishable from pre-agricultural Earth, even though these scenarios include expansive technospheres. Up to nine of these scenarios could represent steady-state examples that could persist for much longer timescales, and it remains possible that short-duration technospheres could be the most abundant. Our scenario set provides the basis for further systematic thinking about technosignature detection as well as for imagining a broad range of possibilities for Earth's future."}, "https://arxiv.org/abs/2409.00100": {"title": "Modelisation a base d'Agent Augmentes par LLM pour les Simulations Sociales: Defis et Opportunites", "link": "https://arxiv.org/abs/2409.00100", "description": "arXiv:2409.00100v1 Announce Type: new \nAbstract: As large language models (LLMs) continue to make significant strides, their better integration into agent-based simulations offers a transformational potential for understanding complex social systems. However, such integration is not trivial and poses numerous challenges. Based on this observation, in this paper, we explore architectures and methods to systematically develop LLM-augmented social simulations and discuss potential research directions in this field. We conclude that integrating LLMs with agent-based simulations offers a powerful toolset for researchers and scientists, allowing for more nuanced, realistic, and comprehensive models of complex systems and human behaviours."}, "https://arxiv.org/abs/2409.00102": {"title": "Collective Predictive Coding as Model of Science: Formalizing Scientific Activities Towards Generative Science", "link": "https://arxiv.org/abs/2409.00102", "description": "arXiv:2409.00102v1 Announce Type: new \nAbstract: This paper proposes a new conceptual framework called Collective Predictive Coding as a Model of Science (CPC-MS) to formalize and understand scientific activities. Building on the idea of collective predictive coding originally developed to explain symbol emergence, CPC-MS models science as a decentralized Bayesian inference process carried out by a community of agents. The framework describes how individual scientists' partial observations and internal representations are integrated through communication and peer review to produce shared external scientific knowledge. Key aspects of scientific practice like experimentation, hypothesis formation, theory development, and paradigm shifts are mapped onto components of the probabilistic graphical model. This paper discusses how CPC-MS provides insights into issues like social objectivity in science, scientific progress, and the potential impacts of AI on research. The generative view of science offers a unified way to analyze scientific activities and could inform efforts to automate aspects of the scientific process. Overall, CPC-MS aims to provide an intuitive yet formal model of science as a collective cognitive activity."}, "https://arxiv.org/abs/2409.00104": {"title": "Widespread misidentification of SEM instruments in the peer-reviewed materials science and engineering literature", "link": "https://arxiv.org/abs/2409.00104", "description": "arXiv:2409.00104v1 Announce Type: new \nAbstract: Materials science and engineering (MSE) research has, for the most part, escaped the doubts raised about the reliability of the scientific literature by recent large-scale replication studies in psychology and cancer biology. However, users on post-publication peer review sites have recently identified dozens of articles where the make and model of the scanning electron microscope (SEM) listed in the text of the paper does not match the instrument's metadata visible in the images in the published article. In order to systematically investigate this potential risk to the MSE literature, we develop a semi-automated approach to scan published figures for this metadata and check it against the SEM instrument identified in the text. Starting from an exhaustive set of 1,067,102 articles published since 2010 in 50 journals with impact factors ranging from 2 to 24, we identify 11,314 articles for which SEM make and model can be identified in an image's metadata. For 21.2% of those articles, the image metadata does not match the SEM manufacturer or model listed in the text and, for another 24.7%, at least some of the instruments used in the study are not reported. Unexplained patterns common to many of these articles suggest the involvement of paper mills, organizations that mass-produce, sell authorship on, and publish fraudulent scientific manuscripts at scale."}, "https://arxiv.org/abs/2409.00108": {"title": "Professionalising Community Management Roles in Interdisciplinary Research Projects", "link": "https://arxiv.org/abs/2409.00108", "description": "arXiv:2409.00108v1 Announce Type: new \nAbstract: This article discusses the professionalisation of community management roles in data science and AI research, referred to here as Research Community Managers (RCMs)."}, "https://arxiv.org/abs/2409.00110": {"title": "Deduction of the Bromilow's time-cost model from the fractal nature of activity networks", "link": "https://arxiv.org/abs/2409.00110", "description": "arXiv:2409.00110v1 Announce Type: new \nAbstract: In 1969 Bromilow observed that the time $T$ to execute a construction project follows a power law scaling with the project cost $C$, $T\\sim C^B$ [Bromilow 1969]. While the Bromilow's time-cost model has been extensively tested using data for different countries and project types, there is no theoretical explanation for the algebraic scaling. Here I mathematically deduce the Bromilow's time-cost model from the fractal nature of activity networks. The Bromislow's exponent is $B=1-\\alpha$, where $1-\\alpha$ is the scaling exponent between the number of activities in the critical path $L$ and the number of activities $N$, $L\\sim N^{1-\\alpha}$ with $0\\leq\\alpha<1$ [Vazquez et al 2023]. I provide empirical data showing that projects with low serial/parallel (SP)% have lower $B$ values than those with higher SP%. I conclude that the Bromilow's time-cost model is a law of activity networks, the Bromilow's exponent is a network property and forecasting project duration from cost should be limited to projects with high SP%."}, "https://arxiv.org/abs/2409.00111": {"title": "Inequality and Concentration in Farmland Production and Size: Regional Analysis for the European Union from 2010 to 2020", "link": "https://arxiv.org/abs/2409.00111", "description": "arXiv:2409.00111v1 Announce Type: new \nAbstract: According to Eurostat estimates, the overall number of farms in Europe declined of about 3 million units between 2010 and 2020. Parallel, the agricultural standard output increased from 304 billion to nearly 360 billion over the same period. Such evidence, legitimately leads to questions about how the structure (e.g., type of production and average size) of farms has changed and whether this change has been uniform or heterogeneous within Europe. In this paper, we aim at investigating the phenomenon of market concentration in the European agricultural and livestock farming industry from 2010 to 2020 at the regional level by exploiting the spatio-temporal dynamics of the Gini concentration index for the land owned by the European farmers and for their standard output. In particular, we are interested in exploring the variability within-and-between regions with regard to land and production size to assess if the European agricultural market suffered from an increasingly concentration of power in fewer but larger farm holding. The extensive mapping provided by this study may allow a fine spatial-scale socio-economic and political assessment of the European agricultural market integration process, its recent and future trends in the complex and uncertain post-COVID context and the restructuring of international relations due to crises and the green energy transition."}, "https://arxiv.org/abs/2409.00148": {"title": "Cartographie du confort thermique au sein d'une cours d'{\\'e}cole parisienne : couplage de mesures microclimatiques fixes et mobiles", "link": "https://arxiv.org/abs/2409.00148", "description": "arXiv:2409.00148v1 Announce Type: new \nAbstract: Climate change will result in more frequent, more intense and longer-lasting heat waves by 2050. As part of its Climate Plan and its resilience strategy, the City of Paris is deploying, through its Oasis program, a network of urban cool islands to mitigate the urban heat island phenomena: schoolyards are renovated in order to reduce the heat stress of users. We establish a methodology aiming to quantify the microclimatic impact of the transformation. Mobile measurements are carried out within a case courtyard under hot conditions and coupled with fixed weather station data to evaluate heat stress using UTCI. The heat stress mapping thus obtained allows a first microclimatic diagnosis of the schoolyard."}, "https://arxiv.org/abs/2409.00326": {"title": "Scalable analysis of stop-and-go waves", "link": "https://arxiv.org/abs/2409.00326", "description": "arXiv:2409.00326v1 Announce Type: new \nAbstract: Analyzing stop-and-go waves at the scale of miles and hours of data is an emerging challenge in traffic research. In the past, datasets were of limited scale and could be easily analyzed by hand or with rudimentary methods to identify a very limited set of traffic waves present within the data. This paper introduces an automatic and scalable stop-and-go wave identification method capable of capturing wave generation, propagation, dissipation, as well as bifurcation and merging, which have previously been observed only very rarely. Using a concise and simple critical-speed based definition of a stop-and-go wave, the proposed method identifies all wave boundaries that encompass spatio-temporal points where vehicle speed is below a chosen critical speed. The method is built upon a graph-based representation of the spatio-temporal points associated with stop-and-go waves, specifically wave front (start) points and wave tail (end) points, and approaches the solution as a graph component identification problem. The method is implemented in Python and demonstrated on a large-scale dataset, I-24 MOTION INCEPTION. New insights revealed from this demonstration with emerging phenomena include: (a) we demonstrate that waves do generate, propagate, and dissipate at a scale (miles and hours) and ubiquity never observed before; (b) wave fronts and tails travels at a consistent speed for a critical speed between 10-20 mph, with propagation variation across lanes, where wave speed on the outer lane are less consistent compared to those on the inner lane; (c) wave fronts and tails propagate at different speeds; (d) wave boundaries capture rich and non-trivial wave topologies, highlighting the complexity of waves."}, "https://arxiv.org/abs/2409.00483": {"title": "Statistics of punctuation in experimental literature -- the remarkable case of \"Finnegans Wake\" by James Joyce", "link": "https://arxiv.org/abs/2409.00483", "description": "arXiv:2409.00483v1 Announce Type: new \nAbstract: As the recent studies indicate, the structure imposed onto written texts by the presence of punctuation develops patterns which reveal certain characteristics of universality. In particular, based on a large collection of classic literary works, it has been evidenced that the distances between consecutive punctuation marks, measured in terms of the number of words, obey the discrete Weibull distribution - a discrete variant of a distribution often used in survival analysis. The present work extends the analysis of punctuation usage patterns to more experimental pieces of world literature. It turns out that the compliance of the the distances between punctuation marks with the discrete Weibull distribution typically applies here as well. However, some of the works by James Joyce are distinct in this regard - in the sense that the tails of the relevant distributions are significantly thicker and, consequently, the corresponding hazard functions are decreasing functions not observed in typical literary texts in prose. \"Finnegans Wake\" - the same one to which science owes the word \"quarks\" for the most fundamental constituents of matter - is particularly striking in this context. At the same time, in all the studied texts, the sentence lengths - representing the distances between sentence-ending punctuation marks - reveal more freedom and are not constrained by the discrete Weibull distribution. This freedom in some cases translates into long-range nonlinear correlations, which manifest themselves in multifractality. Again, a text particularly spectacular in terms of multifractality is \"Finnegans Wake\"."}, "https://arxiv.org/abs/2409.00823": {"title": "Enhancing Anti-Money Laundering Efforts with Network-Based Algorithms", "link": "https://arxiv.org/abs/2409.00823", "description": "arXiv:2409.00823v1 Announce Type: new \nAbstract: The global banking system has faced increasing challenges in combating money laundering, necessitating advanced methods for detecting suspicious transactions. Anti-money laundering (or AML) approaches have often relied on predefined thresholds and machine learning algorithms using flagged transaction data, which are limited by the availability and accuracy of existing datasets. In this paper, we introduce a novel algorithm that leverages network analysis to detect potential money laundering activities within large-scale transaction data. Utilizing an anonymized transactional dataset from Co\\\"operatieve Rabobank U.A., our method combines community detection via the Louvain algorithm and small cycle detection to identify suspicious transaction patterns below the regulatory reporting thresholds. Our approach successfully identifies cycles of transactions that may indicate layering steps in money laundering, providing a valuable tool for financial institutions to enhance their AML efforts. The results suggest the efficacy of our algorithm in pinpointing potentially illicit activities that evade current detection methods."}, "https://arxiv.org/abs/2409.01052": {"title": "A dataset of Open Source Intelligence (OSINT) Tweets about the Russo-Ukrainian war", "link": "https://arxiv.org/abs/2409.01052", "description": "arXiv:2409.01052v1 Announce Type: new \nAbstract: Open Source Intelligence (OSINT) refers to intelligence efforts based on freely available data. It has become a frequent topic of conversation on social media, where private users or networks can share their findings. Such data is highly valuable in conflicts, both for gaining a new understanding of the situation as well as for tracking the spread of misinformation. In this paper, we present a method for collecting such data as well as a novel OSINT dataset for the Russo-Ukrainian war drawn from Twitter between January 2022 and July 2023. It is based on an initial search of users posting OSINT and a subsequent snowballing approach to detect more. The final dataset contains almost 2 million Tweets posted by 1040 users. We also provide some first analyses and experiments on the data, and make suggestions for its future usage."}, "https://arxiv.org/abs/2409.01145": {"title": "LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning", "link": "https://arxiv.org/abs/2409.01145", "description": "arXiv:2409.01145v1 Announce Type: new \nAbstract: Graph Contrastive Learning (GCL) is a potent paradigm for self-supervised graph learning that has attracted attention across various application scenarios. However, GCL for learning on Text-Attributed Graphs (TAGs) has yet to be explored. Because conventional augmentation techniques like feature embedding masking cannot directly process textual attributes on TAGs. A naive strategy for applying GCL to TAGs is to encode the textual attributes into feature embeddings via a language model and then feed the embeddings into the following GCL module for processing. Such a strategy faces three key challenges: I) failure to avoid information loss, II) semantic loss during the text encoding phase, and III) implicit augmentation constraints that lead to uncontrollable and incomprehensible results. In this paper, we propose a novel GCL framework named LATEX-GCL to utilize Large Language Models (LLMs) to produce textual augmentations and LLMs' powerful natural language processing (NLP) abilities to address the three limitations aforementioned to pave the way for applying GCL to TAG tasks. Extensive experiments on four high-quality TAG datasets illustrate the superiority of the proposed LATEX-GCL method. The source codes and datasets are released to ease the reproducibility, which can be accessed via this link: https://anonymous.4open.science/r/LATEX-GCL-0712."}, "https://arxiv.org/abs/2409.01225": {"title": "Analysis of flows in social media uncovers a new multi-step model of information spread", "link": "https://arxiv.org/abs/2409.01225", "description": "arXiv:2409.01225v1 Announce Type: new \nAbstract: Since the advent of the internet, communication paradigms have continuously evolved, resulting in a present-day landscape where the dynamics of information dissemination have undergone a complete transformation compared to the past. In this study, we challenge the conventional two-step flow model of communication, a long-standing paradigm in the field. Our approach introduces a more intricate multi-step and multi-actor model that effectively captures the complexities of modern information spread. We test our hypothesis by examining the spread of information on the Twitter platform. Our findings support the multi-step and multi-actor model hypothesis. In this framework, influencers (individuals with a significant presence in social media) emerges as new central figures and partially take on the role previously attributed to opinion leaders. However, this does not apply to opinion leaders who adapt and reaffirm their influential position on social media, here defined as opinion-leading influencers. Additionally, we note a substantial number of adopters directly accessing information sources, suggesting a potentialdecline if influence in both opinion leaders and influencers. Finally, we found distinctions in the diffusion patterns of left- and right-leaning groups, indicating variations in the underlying structure of information dissemination across different ideologies."}, "https://arxiv.org/abs/2409.01336": {"title": "Finding Large Independent Sets in Networks Using Competitive Dynamics", "link": "https://arxiv.org/abs/2409.01336", "description": "arXiv:2409.01336v1 Announce Type: new \nAbstract: Many decision-making algorithms draw inspiration from the inner workings of individual biological systems. However, it remains unclear whether collective behavior among biological species can also lead to solutions for computational tasks. By studying the coexistence of species that interact through simple rules on a network, we demonstrate that the underlying dynamical system can recover near-optimal solutions to the maximum independent set problem -- a fundamental, computationally hard problem in graph theory. Furthermore, we observe that the optimality of these solutions is improved when the competitive pressure in the system is gradually increased. We explain this phenomenon by showing that the cascade of bifurcation points, which occurs with rising competitive pressure in our dynamical system, naturally gives rise to Katz centrality-based node removal in the network. By formalizing this connection, we propose a biologically inspired discrete algorithm for approximating the maximum independent set problem on a graph. Our results indicate that complex systems may collectively possess the capacity to perform non-trivial computations, with implications spanning biology, economics, and other fields."}, "https://arxiv.org/abs/2409.01355": {"title": "Detection of anomalous spatio-temporal patterns of app traffic in response to catastrophic events", "link": "https://arxiv.org/abs/2409.01355", "description": "arXiv:2409.01355v1 Announce Type: new \nAbstract: In this work, we uncover patterns of usage mobile phone applications and information spread in response to perturbations caused by unprecedented events. We focus on categorizing patterns of response in both space and time and tracking their relaxation over time. To this end, we use the NetMob2023 Data Challenge dataset, which provides mobile phone applications traffic volume data for several cities in France at a spatial resolution of 100$m^2$ and a time resolution of 15 minutes for a time period ranging from March to May 2019. We analyze the spread of information before, during, and after the catastrophic Notre-Dame fire on April 15th and a bombing that took place in the city centre of Lyon on May 24th using volume of data uploaded and downloaded to different mobile applications as a proxy of information transfer dynamics. We identify different clusters of information transfer dynamics in response to the Notre-Dame fire within the city of Paris as well as in other major French cities. We find a clear pattern of significantly above-baseline usage of the application Twitter (currently known as X) in Paris that radially spreads from the area surrounding the Notre-Dame cathedral to the rest of the city. We detect a similar pattern in the city of Lyon in response to the bombing. Further, we present a null model of radial information spread and develop methods of tracking radial patterns over time. Overall, we illustrate novel analytical methods we devise, showing how they enable a new perspective on mobile phone user response to unplanned catastrophic events, giving insight into how information spreads during a catastrophe in both time and space."}, "https://arxiv.org/abs/2409.01363": {"title": "Polaris: Sampling from the Multigraph Configuration Model with Prescribed Color Assortativity", "link": "https://arxiv.org/abs/2409.01363", "description": "arXiv:2409.01363v1 Announce Type: new \nAbstract: We introduce Polaris, a network null model for colored multi-graphs that preserves the Joint Color Matrix. Polaris is specifically designed for studying network polarization, where vertices belong to a side in a debate or a partisan group, represented by a vertex color, and relations have different strengths, represented by an integer-valued edge multiplicity. The key feature of Polaris is preserving the Joint Color Matrix (JCM) of the multigraph, which specifies the number of edges connecting vertices of any two given colors. The JCM is the basic property that determines color assortativity, a fundamental aspect in studying homophily and segregation in polarized networks. By using Polaris, network scientists can test whether a phenomenon is entirely explained by the JCM of the observed network or whether other phenomena might be at play. Technically, our null model is an extension of the configuration model: an ensemble of colored multigraphs characterized by the same degree sequence and the same JCM. To sample from this ensemble, we develop a suite of Markov Chain Monte Carlo algorithms, collectively named Polaris-*. It includes Polaris-B, an adaptation of a generic Metropolis-Hastings algorithm, and Polaris-C, a faster, specialized algorithm with higher acceptance probabilities. This new null model and the associated algorithms provide a more nuanced toolset for examining polarization in social networks, thus enabling statistically sound conclusions."}, "https://arxiv.org/abs/2409.01384": {"title": "Epidemic paradox induced by awareness driven network dynamics", "link": "https://arxiv.org/abs/2409.01384", "description": "arXiv:2409.01384v1 Announce Type: new \nAbstract: We study stationary epidemic processes in scale-free networks with local awareness behavior adopted by only susceptible, only infected, or all nodes. We find that while the epidemic size in the susceptible-aware and the all-aware scenarios scales linearly with the network size, the scaling becomes sublinear in the infected-aware scenario, suggesting that fewer aware nodes may reduce the epidemic size more effectively. We explain this paradox via numerical and theoretical analysis, and highlight the role of influential nodes and their disassortativity to raise awareness in epidemic scenarios."}, "https://arxiv.org/abs/2409.01454": {"title": "Healthcare system resilience and adaptability to pandemic disruptions in the United States", "link": "https://arxiv.org/abs/2409.01454", "description": "arXiv:2409.01454v1 Announce Type: new \nAbstract: Understanding healthcare system resilience has become paramount, particularly in the wake of the COVID-19 pandemic, which imposed unprecedented burdens on healthcare services and severely impacted public health. Resilience is defined as the system's ability to absorb, recover from, and adapt to disruptions; however, despite extensive studies on this subject, we still lack empirical evidence and mathematical tools to quantify its adaptability (the ability of the system to adjust to and learn from disruptions). By analyzing millions of patients' electronic medical records across US states, we find that the COVID-19 pandemic caused two successive waves of disruptions within the healthcare systems, enabling natural experiment analysis of the adaptive capacity for each system to adapt to past disruptions. We generalize the quantification framework and find that the US healthcare systems exhibit substantial adaptability but only a moderate level of resilience. When considering system responses across racial groups, Black and Hispanic groups were more severely impacted by pandemic disruptions than White and Asian groups. Physician abundance is the key characteristic for determining healthcare system resilience. Our results offer vital guidance in designing resilient and sustainable healthcare systems to prepare for future waves of disruptions akin to COVID-19 pandemics."}, "https://arxiv.org/abs/2409.01817": {"title": "Compact 15-minute cities are greener", "link": "https://arxiv.org/abs/2409.01817", "description": "arXiv:2409.01817v1 Announce Type: new \nAbstract: The 15-minute city concept, advocating for cities where essential services are accessible within 15 minutes on foot and by bike, has gained significant attention in recent years. However, despite being celebrated for promoting sustainability, there is an ongoing debate regarding its effectiveness in reducing car usage and, subsequently, emissions in cities. In particular, large-scale evaluations of the effectiveness of the 15-minute concept in reducing emissions are lacking. To address this gap, we investigate whether cities with better walking accessibility, like 15-minute cities, are associated with lower transportation emissions. Comparing 700 cities worldwide, we find that cities with better walking accessibility to services emit less CO$_2$ per capita for transport. Moreover, we observe that among cities with similar average accessibility, cities spreading over larger areas tend to emit more. Our findings highlight the effectiveness of decentralised urban planning, especially the proximity-based 15-minute city, in promoting sustainable mobility. However, they also emphasise the need to integrate local accessibility with urban compactness and efficient public transit, which are vital in large cities."}, "https://arxiv.org/abs/2409.01880": {"title": "Preserving the Ephemeral: Instagram Story Archiving with the Tidal Tales Plugin", "link": "https://arxiv.org/abs/2409.01880", "description": "arXiv:2409.01880v1 Announce Type: new \nAbstract: We introduce the Tidal Tales Plugin, a Firefox extension for efficiently collecting and archiving of Instagram stories, addressing the challenges of ephemeral data in social media research. It enables an automated collection of story metadata and media files without risking account bans. It contributes to Web Science by facilitating expansive, long-term studies with enhanced data access and integrity."}, "https://arxiv.org/abs/2409.02002": {"title": "The overlooked need for Ethics in Complexity Science: Why it matters", "link": "https://arxiv.org/abs/2409.02002", "description": "arXiv:2409.02002v1 Announce Type: new \nAbstract: Complexity science, despite its broad scope and potential impact, has not kept pace with fields like artificial intelligence, biotechnology and social sciences in addressing ethical concerns. The field lacks a comprehensive ethical framework, leaving us, as a community, vulnerable to ethical challenges and dilemmas. Other areas have gone through similar experiences and created, with discussions and working groups, their guides, policies and recommendations. Therefore, here we highlight the critical absence of formal guidelines, dedicated ethical committees, and widespread discussions on ethics within the complexity science community. Drawing on insights from the disciplines mentioned earlier, we propose a roadmap to enhance ethical awareness and action. Our recommendations include (i) initiating supportive mechanisms to develop ethical guidelines specific to complex systems research, (ii) creating open-access resources, and (iii) fostering inclusive dialogues to ensure that complexity science can responsibly tackle societal challenges and achieve a more inclusive environment. By initiating this dialogue, we aim to encourage a necessary shift in how ethics is integrated into complexity research, positioning the field to address contemporary challenges more effectively."}, "https://arxiv.org/abs/2409.02077": {"title": "FastEnsemble: A new scalable ensemble clustering method", "link": "https://arxiv.org/abs/2409.02077", "description": "arXiv:2409.02077v1 Announce Type: new \nAbstract: Many community detection algorithms are stochastic in nature, and their output can vary based on different input parameters and random seeds. Consensus clustering methods, such as FastConsensus and ECG, combine clusterings from multiple runs of the same clustering algorithm, in order to improve stability and accuracy. In this study we present a new consensus clustering method, FastEnsemble, and show that it provides advantages over both FastConsensus and ECG. Furthermore, FastEnsemble is designed for use with any clustering method, and we show results using \\ourmethod with Leiden optimizing modularity or the Constant Potts model. FastEnsemble is available in Github at https://github.com/ytabatabaee/fast-ensemble"}, "https://arxiv.org/abs/2409.00004": {"title": "Enhancing Trustworthiness and Minimising Bias Issues in Leveraging Social Media Data for Disaster Management Response", "link": "https://arxiv.org/abs/2409.00004", "description": "arXiv:2409.00004v1 Announce Type: cross \nAbstract: Disaster events often unfold rapidly, necessitating a swift and effective response. Developing action plans, resource allocation, and resolution of help requests in disaster scenarios is time-consuming and complex since disaster-relevant information is often uncertain. Leveraging real-time data can significantly deal with data uncertainty and enhance disaster response efforts. To deal with real-time data uncertainty, social media appeared as an alternative effective source of real-time data as there has been extensive use of social media during and after the disasters. However, it also brings forth challenges regarding trustworthiness and bias in these data. To fully leverage social media data for disaster management, it becomes crucial to mitigate biases that may arise due to specific disaster types or regional contexts. Additionally, the presence of misinformation within social media data raises concerns about the reliability of data sources, potentially impeding actionable insights and leading to improper resource utilization. To overcome these challenges, our research aimed to investigate how to ensure trustworthiness and address biases in social media data. We aim to investigate and identify the factors that can be used to enhance trustworthiness and minimize bias to make an efficient and scalable disaster management system utilizing real-time social media posts, identify disaster-related keywords, and assess the severity of the disaster. By doing so, the integration of real-time social data can improve the speed and accuracy of disaster management systems"}, "https://arxiv.org/abs/2409.00159": {"title": "LLMs hallucinate graphs too: a structural perspective", "link": "https://arxiv.org/abs/2409.00159", "description": "arXiv:2409.00159v1 Announce Type: cross \nAbstract: It is known that LLMs do hallucinate, that is, they return incorrect information as facts. In this paper, we introduce the possibility to study these hallucinations under a structured form: graphs. Hallucinations in this context are incorrect outputs when prompted for well known graphs from the literature (e.g. Karate club, Les Mis\\'erables, graph atlas). These hallucinated graphs have the advantage of being much richer than the factual accuracy -- or not -- of a fact; this paper thus argues that such rich hallucinations can be used to characterize the outputs of LLMs. Our first contribution observes the diversity of topological hallucinations from major modern LLMs. Our second contribution is the proposal of a metric for the amplitude of such hallucinations: the Graph Atlas Distance, that is the average graph edit distance from several graphs in the graph atlas set. We compare this metric to the Hallucination Leaderboard, a hallucination rank that leverages 10,000 times more prompts to obtain its ranking."}, "https://arxiv.org/abs/2409.00211": {"title": "Social MediARverse Investigating Users Social Media Content Sharing and Consuming Intentions with Location-Based AR", "link": "https://arxiv.org/abs/2409.00211", "description": "arXiv:2409.00211v1 Announce Type: cross \nAbstract: Augmented Reality (AR) is evolving to become the next frontier in social media, merging physical and virtual reality into a living metaverse, a Social MediARverse. With this transition, we must understand how different contexts (public, semi-public, and private) affect user engagement with AR content. We address this gap in current research by conducting an online survey with 110 participants, showcasing 36 AR videos, and polling them about the content's fit and appropriateness. Specifically, we manipulated these three spaces, two forms of dynamism (dynamic vs. static), and two dimensionalities (2D vs. 3D). Our findings reveal that dynamic AR content is generally more favorably received than static content. Additionally, users find sharing and engaging with AR content in private settings more comfortable than in others. By this, the study offers valuable insights for designing and implementing future Social MediARverses and guides industry and academia on content visualization and contextual considerations."}, "https://arxiv.org/abs/2409.00338": {"title": "GSpect: Spectral Filtering for Cross-Scale Graph Classification", "link": "https://arxiv.org/abs/2409.00338", "description": "arXiv:2409.00338v1 Announce Type: cross \nAbstract: Identifying structures in common forms the basis for networked systems design and optimization. However, real structures represented by graphs are often of varying sizes, leading to the low accuracy of traditional graph classification methods. These graphs are called cross-scale graphs. To overcome this limitation, in this study, we propose GSpect, an advanced spectral graph filtering model for cross-scale graph classification tasks. Compared with other methods, we use graph wavelet neural networks for the convolution layer of the model, which aggregates multi-scale messages to generate graph representations. We design a spectral-pooling layer which aggregates nodes to one node to reduce the cross-scale graphs to the same size. We collect and construct the cross-scale benchmark data set, MSG (Multi Scale Graphs). Experiments reveal that, on open data sets, GSpect improves the performance of classification accuracy by 1.62% on average, and for a maximum of 3.33% on PROTEINS. On MSG, GSpect improves the performance of classification accuracy by 15.55% on average. GSpect fills the gap in cross-scale graph classification studies and has potential to provide assistance in application research like diagnosis of brain disease by predicting the brain network's label and developing new drugs with molecular structures learned from their counterparts in other systems."}, "https://arxiv.org/abs/2409.00670": {"title": "Towards Faster Graph Partitioning via Pre-training and Inductive Inference", "link": "https://arxiv.org/abs/2409.00670", "description": "arXiv:2409.00670v1 Announce Type: cross \nAbstract: Graph partitioning (GP) is a classic problem that divides the node set of a graph into densely-connected blocks. Following the IEEE HPEC Graph Challenge and recent advances in pre-training techniques (e.g., large-language models), we propose PR-GPT (Pre-trained & Refined Graph ParTitioning) based on a novel pre-training & refinement paradigm. We first conduct the offline pre-training of a deep graph learning (DGL) model on small synthetic graphs with various topology properties. By using the inductive inference of DGL, one can directly generalize the pre-trained model (with frozen model parameters) to large graphs and derive feasible GP results. We also use the derived partition as a good initialization of an efficient GP method (e.g., InfoMap) to further refine the quality of partitioning. In this setting, the online generalization and refinement of PR-GPT can not only benefit from the transfer ability regarding quality but also ensure high inference efficiency without re-training. Based on a mechanism of reducing the scale of a graph to be processed by the refinement method, PR-GPT also has the potential to support streaming GP. Experiments on the Graph Challenge benchmark demonstrate that PR-GPT can ensure faster GP on large-scale graphs without significant quality degradation, compared with running a refinement method from scratch. We will make our code public at https://github.com/KuroginQin/PRGPT."}, "https://arxiv.org/abs/2409.00687": {"title": "When Heterophily Meets Heterogeneous Graphs: Latent Graphs Guided Unsupervised Representation Learning", "link": "https://arxiv.org/abs/2409.00687", "description": "arXiv:2409.00687v1 Announce Type: cross \nAbstract: Unsupervised heterogeneous graph representation learning (UHGRL) has gained increasing attention due to its significance in handling practical graphs without labels. However, heterophily has been largely ignored, despite its ubiquitous presence in real-world heterogeneous graphs. In this paper, we define semantic heterophily and propose an innovative framework called Latent Graphs Guided Unsupervised Representation Learning (LatGRL) to handle this problem. First, we develop a similarity mining method that couples global structures and attributes, enabling the construction of fine-grained homophilic and heterophilic latent graphs to guide the representation learning. Moreover, we propose an adaptive dual-frequency semantic fusion mechanism to address the problem of node-level semantic heterophily. To cope with the massive scale of real-world data, we further design a scalable implementation. Extensive experiments on benchmark datasets validate the effectiveness and efficiency of our proposed framework. The source code and datasets have been made available at https://github.com/zxlearningdeep/LatGRL."}, "https://arxiv.org/abs/2409.00955": {"title": "A Comprehensive Analysis of the Future of Atomically Precise Manufacturing", "link": "https://arxiv.org/abs/2409.00955", "description": "arXiv:2409.00955v1 Announce Type: cross \nAbstract: Atomically Precise Manufacturing (APM) refers to the assembly of materials with atomic precision, representing a highly advanced technology with significant potential. However, the development of APM remains in its early stages, with applications largely confined to specialized fields and lacking cohesion within a unified discipline. The current literature on APM is often dominated by older, speculative papers that discuss its immense potential risks and benefits without sufficient grounding in the latest advancements or practical limitations that exist today. This paper aims to bridge this gap by providing a comprehensive assessment of current APM and near-APM technologies, as well as using the barriers to further progress to predict future developments. Through this analysis, we seek to establish a clearer understanding of the present state of the technology and then use these insights to predict the future trajectory of APM. By doing so, we aim to create a more grounded discourse on APM and its potential risks and benefits, while also guiding future research on the necessary regulations and safety considerations for this emerging field."}, "https://arxiv.org/abs/2409.01103": {"title": "Modeling contagious disease spreading", "link": "https://arxiv.org/abs/2409.01103", "description": "arXiv:2409.01103v1 Announce Type: cross \nAbstract: An understanding of the disease spreading phenomenon based on a mathematical model is extremely needed for the implication of the correct policy measures to contain the disease propagation. Here, we report a new model namely the Ising-SIR model describing contagious disease spreading phenomena including both airborne and direct contact disease transformations. In the airborne case, a susceptible agent can catch the disease either from the environment or its infected neighbors whereas in the second case, the agent can be infected only through close contact with its infected neighbors. We have performed Monte Carlo simulations on a square lattice using periodic boundary conditions to investigate the dynamics of disease spread. The simulations demonstrate that the mechanism of disease spreading plays a significant role in the growth dynamics and leads to different growth exponent. In the direct contact disease spreading mechanism, the growth exponent is nearly equal to two for some model parameters which agrees with earlier empirical observations. In addition, the model predicts various types of spatiotemporal patterns that can be observed in nature."}, "https://arxiv.org/abs/2409.01213": {"title": "Supervised Pattern Recognition Involving Skewed Feature Densities", "link": "https://arxiv.org/abs/2409.01213", "description": "arXiv:2409.01213v1 Announce Type: cross \nAbstract: Pattern recognition constitutes a particularly important task underlying a great deal of scientific and technologica activities. At the same time, pattern recognition involves several challenges, including the choice of features to represent the data elements, as well as possible respective transformations. In the present work, the classification potential of the Euclidean distance and a dissimilarity index based on the coincidence similarity index are compared by using the k-neighbors supervised classification method respectively to features resulting from several types of transformations of one- and two-dimensional symmetric densities. Given two groups characterized by respective densities without or with overlap, different types of respective transformations are obtained and employed to quantitatively evaluate the performance of k-neighbors methodologies based on the Euclidean distance an coincidence similarity index. More specifically, the accuracy of classifying the intersection point between the densities of two adjacent groups is taken into account for the comparison. Several interesting results are described and discussed, including the enhanced potential of the dissimilarity index for classifying datasets with right skewed feature densities, as well as the identification that the sharpness of the comparison between data elements can be independent of the respective supervised classification performance."}, "https://arxiv.org/abs/2409.01262": {"title": "Random matrix ensemble for the covariance matrix of Ornstein-Uhlenbeck processes with heterogeneous temperatures", "link": "https://arxiv.org/abs/2409.01262", "description": "arXiv:2409.01262v1 Announce Type: cross \nAbstract: We introduce a random matrix model for the stationary covariance of multivariate Ornstein-Uhlenbeck processes with heterogeneous temperatures, where the covariance is constrained by the Sylvester-Lyapunov equation. Using the replica method, we compute the spectral density of the equal-time covariance matrix characterizing the stationary states, demonstrating that this model undergoes a transition between stable and unstable states. In the stable regime, the spectral density has a finite and positive support, whereas negative eigenvalues emerge in the unstable regime. We determine the critical line separating these regimes and show that the spectral density exhibits a power-law tail at marginal stability, with an exponent independent of the temperature distribution. Additionally, we compute the spectral density of the lagged covariance matrix characterizing the stationary states of linear transformations of the original dynamical variables. Our random-matrix model is potentially interesting to understand the spectral properties of empirical correlation matrices appearing in the study of complex systems."}, "https://arxiv.org/abs/2409.01823": {"title": "DAOs of Collective Intelligence? Unraveling the Complexity of Blockchain Governance in Decentralized Autonomous Organizations", "link": "https://arxiv.org/abs/2409.01823", "description": "arXiv:2409.01823v1 Announce Type: cross \nAbstract: Decentralized autonomous organizations (DAOs) have transformed organizational structures by shifting from traditional hierarchical control to decentralized approaches, leveraging blockchain and cryptoeconomics. Despite managing significant funds and building global networks, DAOs face challenges like declining participation, increasing centralization, and inabilities to adapt to changing environments, which stifle innovation. This paper explores DAOs as complex systems and applies complexity science to explain their inefficiencies. In particular, we discuss DAO challenges, their complex nature, and introduce the self-organization mechanisms of collective intelligence, digital democracy, and adaptation. By applying these mechansims to improve DAO design and construction, a practical design framework for DAOs is created. This contribution lays a foundation for future research at the intersection of complexity science and DAOs."}, "https://arxiv.org/abs/2212.01329": {"title": "Structural properties in the diffusion of the solar photovoltaic in Italy: individual people/householder vs firms", "link": "https://arxiv.org/abs/2212.01329", "description": "arXiv:2212.01329v2 Announce Type: replace \nAbstract: This paper develops two mathematical models to understand subjects' behavior in response to the urgency of a change and inputs from governments e.g., (subsides) in the context of the diffusion of the solar photovoltaic in Italy. The first model is a Markovian model of interacting particle systems. The second one, instead, is a Mean Field Game model. In both cases, we derive the scaling limit deterministic dynamics, and we compare the latter to the Italian solar photovoltaic data. We identify periods where the first model describes the behavior of domestic data well and a period where the second model captures a particular feature of data corresponding to companies. The comprehensive analysis, integrated with a philosophical inquiry focusing on the conceptual vocabulary and correlative implications, leads to the formulation of hypotheses about the efficacy of different forms of governmental subsidies."}, "https://arxiv.org/abs/2401.12843": {"title": "An embedding-based distance for temporal graphs", "link": "https://arxiv.org/abs/2401.12843", "description": "arXiv:2401.12843v2 Announce Type: replace \nAbstract: Temporal graphs are commonly used to represent time-resolved relations between entities in many natural and artificial systems. Many techniques were devised to investigate the evolution of temporal graphs by comparing their state at different time points. However, quantifying the similarity between temporal graphs as a whole is an open problem. Here, we use embeddings based on time-respecting random walks to introduce a new notion of distance between temporal graphs. This distance is well-defined for pairs of temporal graphs with different numbers of nodes and different time spans. We study the case of a matched pair of graphs, when a known relation exists between their nodes, and the case of unmatched graphs, when such a relation is unavailable and the graphs may be of different sizes. We use empirical and synthetic temporal network data to show that the distance we introduce discriminates graphs with different topological and temporal properties. We provide an efficient implementation of the distance computation suitable for large-scale temporal graphs."}, "https://arxiv.org/abs/2102.04368": {"title": "Coherence resonance in influencer networks", "link": "https://arxiv.org/abs/2102.04368", "description": "arXiv:2102.04368v2 Announce Type: replace-cross \nAbstract: Complex networks are abundant in nature and many share an important structural property: they contain a few nodes that are abnormally highly connected (hubs). Some of these hubs are called influencers because they couple strongly to the network and play fundamental dynamical and structural roles. Strikingly, despite the abundance of networks with influencers, little is known about their response to stochastic forcing. Here, for oscillatory dynamics on influencer networks, we show that subjecting influencers to an optimal intensity of noise can result in enhanced network synchronization. This new network dynamical effect, which we call coherence resonance in influencer networks, emerges from a synergy between network structure and stochasticity and is highly nonlinear, vanishing when the noise is too weak or too strong. Our results reveal that the influencer backbone can sharply increase the dynamical response in complex systems of coupled oscillators."}, "https://arxiv.org/abs/2310.14481": {"title": "Efficient Heterogeneous Graph Learning via Random Projection", "link": "https://arxiv.org/abs/2310.14481", "description": "arXiv:2310.14481v2 Announce Type: replace-cross \nAbstract: Heterogeneous Graph Neural Networks (HGNNs) are powerful tools for deep learning on heterogeneous graphs. Typical HGNNs require repetitive message passing during training, limiting efficiency for large-scale real-world graphs. Recent pre-computation-based HGNNs use one-time message passing to transform a heterogeneous graph into regular-shaped tensors, enabling efficient mini-batch training. Existing pre-computation-based HGNNs can be mainly categorized into two styles, which differ in how much information loss is allowed and efficiency. We propose a hybrid pre-computation-based HGNN, named Random Projection Heterogeneous Graph Neural Network (RpHGNN), which combines the benefits of one style's efficiency with the low information loss of the other style. To achieve efficiency, the main framework of RpHGNN consists of propagate-then-update iterations, where we introduce a Random Projection Squashing step to ensure that complexity increases only linearly. To achieve low information loss, we introduce a Relation-wise Neighbor Collection component with an Even-odd Propagation Scheme, which aims to collect information from neighbors in a finer-grained way. Experimental results indicate that our approach achieves state-of-the-art results on seven small and large benchmark datasets while also being 230% faster compared to the most effective baseline. Surprisingly, our approach not only surpasses pre-processing-based baselines but also outperforms end-to-end methods."}, "https://arxiv.org/abs/2409.02152": {"title": "Fair Railway Network Design", "link": "https://arxiv.org/abs/2409.02152", "description": "arXiv:2409.02152v1 Announce Type: new \nAbstract: When designing a public transportation network in a country, one may want to minimise the sum of travel duration of all inhabitants. This corresponds to a purely utilitarian view and does not involve any fairness consideration, as the resulting network will typically benefit the capital city and/or large central cities while leaving some peripheral cities behind. On the other hand, a more egalitarian view will allow some people to travel between peripheral cities without having to go through a central city. We define a model, propose algorithms for computing solution networks, and report on experiments based on real data."}, "https://arxiv.org/abs/2409.02304": {"title": "Wikipedia in Wartime: Experiences of Wikipedians Maintaining Articles About the Russia-Ukraine War", "link": "https://arxiv.org/abs/2409.02304", "description": "arXiv:2409.02304v1 Announce Type: new \nAbstract: How do Wikipedians maintain an accurate encyclopedia during an ongoing geopolitical conflict where state actors might seek to spread disinformation or conduct an information operation? In the context of the Russia-Ukraine War, this question becomes more pressing, given the Russian government's extensive history of orchestrating information campaigns. We conducted an interview study with 13 expert Wikipedians involved in the Russo-Ukrainian War topic area on the English-language edition of Wikipedia. While our participants did not perceive there to be clear evidence of a state-backed information operation, they agreed that war-related articles experienced high levels of disruptive editing from both Russia-aligned and Ukraine-aligned accounts. The English-language edition of Wikipedia had existing policies and processes at its disposal to counter such disruption. State-backed or not, the disruptive activity created time-intensive maintenance work for our participants. Finally, participants considered English-language Wikipedia to be more resilient than social media in preventing the spread of false information online. We conclude by discussing sociotechnical implications for Wikipedia and social platforms."}, "https://arxiv.org/abs/2409.02317": {"title": "Topological communities in complex networks", "link": "https://arxiv.org/abs/2409.02317", "description": "arXiv:2409.02317v1 Announce Type: new \nAbstract: Most complex systems can be captured by graphs or networks. Networks connect nodes (e.g.\\ neurons) through edges (synapses), thus summarizing the system's structure. A popular way of interrogating graphs is community detection, which uncovers sets of geometrically related nodes. {\\em Geometric communities} consist of nodes ``closer'' to each other than to others in the graph. Some network features do not depend on node proximity -- rather, on them playing similar roles (e.g.\\ building bridges) even if located far apart. These features can thus escape proximity-based analyses. We lack a general framework to uncover such features. We introduce {\\em topological communities}, an alternative perspective to decomposing graphs. We find clusters that describe a network as much as classical communities, yet are missed by current techniques. In our framework, each graph guides our attention to its relevant features, whether geometric or topological. Our analysis complements existing ones, and could be a default method to study networks confronted without prior knowledge. Classical community detection has bolstered our understanding of biological, neural, or social systems; yet it is only half the story. Topological communities promise deep insights on a wealth of available data. We illustrate this for the global airport network, human connectomes, and others."}, "https://arxiv.org/abs/2409.02524": {"title": "Enforcing Katz and PageRank Centrality Measures in Complex Networks", "link": "https://arxiv.org/abs/2409.02524", "description": "arXiv:2409.02524v1 Announce Type: new \nAbstract: We investigate the problem of enforcing a desired centrality measure in complex networks, while still keeping the original pattern of the network. Specifically, by representing the network as a graph with suitable nodes and weighted edges, we focus on computing the smallest perturbation on the weights required to obtain a prescribed PageRank or Katz centrality index for the nodes. Our approach relies on optimization procedures that scale with the number of modified edges, enabling the exploration of different scenarios and altering network structure and dynamics."}, "https://arxiv.org/abs/2409.02525": {"title": "A Topic-wise Exploration of the Telegram Group-verse", "link": "https://arxiv.org/abs/2409.02525", "description": "arXiv:2409.02525v1 Announce Type: new \nAbstract: Although currently one of the most popular instant messaging apps worldwide, Telegram has been largely understudied in the past years. In this paper, we aim to address this gap by presenting an analysis of publicly accessible groups covering discussions encompassing different topics, as diverse as Education, Erotic, Politics, and Cryptocurrencies. We engineer and offer an open-source tool to automate the collection of messages from Telegram groups, a non-straightforward problem. We use it to collect more than 50 million messages from 669 groups. Here, we present a first-of-its-kind, per-topic analysis, contrasting the characteristics of the messages sent on the platform from different angles -- the language, the presence of bots, the type and volume of shared media content. Our results confirm some anecdotal evidence, e.g., clues that Telegram is used to share possibly illicit content, and unveil some unexpected findings, e.g., the different sharing patterns of video and stickers in groups of different topics. While preliminary, we hope that our work paves the road for several avenues of future research on the understudied Telegram platform."}, "https://arxiv.org/abs/2409.02592": {"title": "Exploring Citation Diversity in Scholarly Literature: An Entropy-Based Approach", "link": "https://arxiv.org/abs/2409.02592", "description": "arXiv:2409.02592v1 Announce Type: new \nAbstract: This study explores global citation diversity,examining its various patterns across countries and academic disciplines.We analyzed citation distributions in top institutes worldwide,revealing that the higher citation end of the distribution follow Power law or Pareto law pattern and the Pareto law's scaling exponent changes with the number of institutes considered.An entropy based novel citation inequality measure has been introduced, enhancing the precision of our analysis. Our findings show that countries with small and large economies often group similarly based on citation diversity, with shifting the groupings as the number of institutes considered changes.Moreover,we analyzed citation diversity among award-winning scientists across six scientific disciplines,finding significant variations.We also explored the evolution of citation diversity over the past century across multiple fields.A gender-based study in various disciplines highlights citation inequalities among male and female scientists.Our innovative citation diversity measure stands out as a vital tool for evaluating citation inequality,providing insights beyond what traditional citation counts can offer.This thorough analysis deepens our understanding of global scientific contributions and promotes a more equitable view of academic accomplishments."}, "https://arxiv.org/abs/2409.02659": {"title": "Dynamics of drug trafficking: Results from a simple compartmental model", "link": "https://arxiv.org/abs/2409.02659", "description": "arXiv:2409.02659v1 Announce Type: new \nAbstract: In this work we propose a simple model for the emergence of drug dealers. For this purpose, we built a compartmental model considering four subpopulations, namely susceptibles, passive supporters, drug dealers and arrested drug dealers. The target is to study the influence of the passive supporters on the long-time prevalence of drug dealers. Passive supporters are people who are passively consenting to the drug trafficking cause. First we consider the model on a fully-connected newtork, in such a way that we can write a rate equation for each subpopulation. Our analytical and numerical results show that the emergence of drug dealers is a consequence of the rapid increase number of passive supporters. Such increase is associated with a nonequilibrium active-absorbing phase transition. After that, we consider the model on a two-dimensional square lattice, in order to compare the results in the presence of a simple social network with the previous results. The Monte Carlo simulation results suggest a similar behavior in comparison with the fully-connected network case, but the location of the critical point of the transition is distinct, due to the neighbors' correlations introduced by the presence of the lattice."}, "https://arxiv.org/abs/2409.02690": {"title": "Detecting Calls to Action in Multimodal Content: Analysis of the 2021 German Federal Election Campaign on Instagram", "link": "https://arxiv.org/abs/2409.02690", "description": "arXiv:2409.02690v1 Announce Type: new \nAbstract: This study investigates the automated classification of Calls to Action (CTAs) within the 2021 German Instagram election campaign to advance the understanding of mobilization in social media contexts. We analyzed over 2,208 Instagram stories and 712 posts using fine-tuned BERT models and OpenAI's GPT-4 models. The fine-tuned BERT model incorporating synthetic training data achieved a macro F1 score of 0.93, demonstrating a robust classification performance. Our analysis revealed that 49.58% of Instagram posts and 10.64% of stories contained CTAs, highlighting significant differences in mobilization strategies between these content types. Additionally, we found that FDP and the Greens had the highest prevalence of CTAs in posts, whereas CDU and CSU led in story CTAs."}, "https://arxiv.org/abs/2409.02702": {"title": "Incorporating Like-Minded Peers to Overcome Friend Data Sparsity in Session-Based Social Recommendations", "link": "https://arxiv.org/abs/2409.02702", "description": "arXiv:2409.02702v1 Announce Type: new \nAbstract: Session-based Social Recommendation (SSR) leverages social relationships within online networks to enhance the performance of Session-based Recommendation (SR). However, existing SSR algorithms often encounter the challenge of ``friend data sparsity''. Moreover, significant discrepancies can exist between the purchase preferences of social network friends and those of the target user, reducing the influence of friends relative to the target user's own preferences. To address these challenges, this paper introduces the concept of ``Like-minded Peers'' (LMP), representing users whose preferences align with the target user's current session based on their historical sessions. This is the first work, to our knowledge, that uses LMP to enhance the modeling of social influence in SSR. This approach not only alleviates the problem of friend data sparsity but also effectively incorporates users with similar preferences to the target user. We propose a novel model named Transformer Encoder with Graph Attention Aggregator Recommendation (TEGAARec), which includes the TEGAA module and the GAT-based social aggregation module. The TEGAA module captures and merges both long-term and short-term interests for target users and LMP users. Concurrently, the GAT-based social aggregation module is designed to aggregate the target users' dynamic interests and social influence in a weighted manner. Extensive experiments on four real-world datasets demonstrate the efficacy and superiority of our proposed model and ablation studies are done to illustrate the contributions of each component in TEGAARec."}, "https://arxiv.org/abs/2409.02816": {"title": "Simple fusion-fission quantifies Israel-Palestine violence and suggests multi-adversary solution", "link": "https://arxiv.org/abs/2409.02816", "description": "arXiv:2409.02816v1 Announce Type: new \nAbstract: Why humans fight has no easy answer. However, understanding better how humans fight could inform future interventions, hidden shifts and casualty risk. Fusion-fission describes the well-known grouping behavior of fish etc. fighting for survival in the face of strong opponents: they form clusters ('fusion') which provide collective benefits and a cluster scatters when it senses danger ('fission'). Here we show how similar clustering (fusion-fission) of human fighters provides a unified quantitative explanation for complex casualty patterns across decades of Israel-Palestine region violence, as well as the October 7 surprise attack -- and uncovers a hidden post-October 7 shift. State-of-the-art data shows this fighter fusion-fission in action. It also predicts future 'super-shock' attacks that will be more lethal than October 7 and will arrive earlier. It offers a multi-adversary solution. Our results -- which include testable formulae and a plug-and-play simulation -- enable concrete risk assessments of future casualties and policy-making grounded by fighter behavior."}, "https://arxiv.org/abs/2409.02822": {"title": "Language Understanding as a Constraint on Consensus Size in LLM Societies", "link": "https://arxiv.org/abs/2409.02822", "description": "arXiv:2409.02822v1 Announce Type: new \nAbstract: The applications of Large Language Models (LLMs) are going towards collaborative tasks where several agents interact with each other like in an LLM society. In such a setting, large groups of LLMs could reach consensus about arbitrary norms for which there is no information supporting one option over another, regulating their own behavior in a self-organized way. In human societies, the ability to reach consensus without institutions has a limit in the cognitive capacities of humans. To understand if a similar phenomenon characterizes also LLMs, we apply methods from complexity science and principles from behavioral sciences in a new approach of AI anthropology. We find that LLMs are able to reach consensus in groups and that the opinion dynamics of LLMs can be understood with a function parametrized by a majority force coefficient that determines whether consensus is possible. This majority force is stronger for models with higher language understanding capabilities and decreases for larger groups, leading to a critical group size beyond which, for a given LLM, consensus is unfeasible. This critical group size grows exponentially with the language understanding capabilities of models and for the most advanced models, it can reach an order of magnitude beyond the typical size of informal human groups."}, "https://arxiv.org/abs/2409.02519": {"title": "Language is Scary when Over-Analyzed: Unpacking Implied Misogynistic Reasoning with Argumentation Theory-Driven Prompts", "link": "https://arxiv.org/abs/2409.02519", "description": "arXiv:2409.02519v1 Announce Type: cross \nAbstract: We propose misogyny detection as an Argumentative Reasoning task and we investigate the capacity of large language models (LLMs) to understand the implicit reasoning used to convey misogyny in both Italian and English. The central aim is to generate the missing reasoning link between a message and the implied meanings encoding the misogyny. Our study uses argumentation theory as a foundation to form a collection of prompts in both zero-shot and few-shot settings. These prompts integrate different techniques, including chain-of-thought reasoning and augmented knowledge. Our findings show that LLMs fall short on reasoning capabilities about misogynistic comments and that they mostly rely on their implicit knowledge derived from internalized common stereotypes about women to generate implied assumptions, rather than on inductive reasoning."}, "https://arxiv.org/abs/2409.02728": {"title": "Task-Oriented Communication for Graph Data: A Graph Information Bottleneck Approach", "link": "https://arxiv.org/abs/2409.02728", "description": "arXiv:2409.02728v1 Announce Type: cross \nAbstract: Graph data, essential in fields like knowledge representation and social networks, often involves large networks with many nodes and edges. Transmitting these graphs can be highly inefficient due to their size and redundancy for specific tasks. This paper introduces a method to extract a smaller, task-focused subgraph that maintains key information while reducing communication overhead. Our approach utilizes graph neural networks (GNNs) and the graph information bottleneck (GIB) principle to create a compact, informative, and robust graph representation suitable for transmission. The challenge lies in the irregular structure of graph data, making GIB optimization complex. We address this by deriving a tractable variational upper bound for the objective function. Additionally, we propose the VQ-GIB mechanism, integrating vector quantization (VQ) to convert subgraph representations into a discrete codebook sequence, compatible with existing digital communication systems. Our experiments show that this GIB-based method significantly lowers communication costs while preserving essential task-related information. The approach demonstrates robust performance across various communication channels, suitable for both continuous and discrete systems."}, "https://arxiv.org/abs/2409.02758": {"title": "Power-grid modelling via gradual improvement of parameters", "link": "https://arxiv.org/abs/2409.02758", "description": "arXiv:2409.02758v1 Announce Type: cross \nAbstract: The dynamics of electric power systems are widely studied through the phase synchronization of oscillators, typically with the use of the Kuramoto equation. While there are numerous well-known order parameters to characterize these dynamics, shortcoming of these metrics are also recognized. To capture all transitions from phase disordered states over phase locking to fully synchronized systems, new metrics were proposed and demonstrated on homogeneous models. In this paper we aim to address a gap in the literature, namely, to examine how gradual improvement of power grid models affect the goodness of certain metrics. To study how the details of models are perceived by the different metrics, 12 variations of a power grid model were created, introducing varying level of heterogeneity through the coupling strength, the nodal powers and the moment of inertia. The grid models were compared using a second-order Kuramoto equation and adaptive Runge-Kutta solver, measuring the values of the phase, the frequency and the universal order parameters. Finally, frequency results of the models were compared to grid measurements. We found that the universal order parameter was able to capture more details of the grid models, especially in cases of decreasing moment of inertia. The most heterogeneous models showed very low synchronization and thus suggest a limitation of the second-order Kuramoto equation. Finally, we show local frequency results related to the multi-peaks of static models, which implies that spatial heterogeneity can also induce such multi-peak behavior."}, "https://arxiv.org/abs/2409.02884": {"title": "Regulatory Functions from Cells to Society", "link": "https://arxiv.org/abs/2409.02884", "description": "arXiv:2409.02884v1 Announce Type: cross \nAbstract: Regulatory functions are essential in both socioeconomic and biological systems, from corporate managers to regulatory genes in genomes. Regulatory functions come with substantial costs, but are often taken for granted. Here, we empirically examine regulatory costs across diverse systems -- biological organisms (bacteria and eukaryotic genomes), human organizations (companies, federal agencies, universities), and decentralized entities (Wikipedia, cities) -- using scaling analysis. We guide the empirical analysis with a conceptual model, which anticipates the scaling of regulatory costs to shift with the system's internal interaction structure -- well-mixed or modular. We find diverse systems exhibit consistent scaling patterns -- well-mixed systems exhibit superlinear scaling, while modular ones show sublinear or linear scaling. Further, we find that the socioeconomic systems containing more diverse occupational functions tend to have more regulatory costs than expected from their size, confirming the type of interactions also plays a role in regulatory costs. While many socioeconomic systems exhibit efficiencies of scale, regulatory costs in many social systems have grown disproportionally over time. Our finding suggests that the increasing complexity of functions may contribute to this trend. This cross-system comparison offers a framework for understanding regulatory costs and could guide future efforts to identify and mitigate regulatory inefficiencies."}, "https://arxiv.org/abs/2303.12660": {"title": "Structural Measures of Resilience for Supply Chains", "link": "https://arxiv.org/abs/2303.12660", "description": "arXiv:2303.12660v3 Announce Type: replace \nAbstract: We investigate the structural factors that drive cascading failures in production networks, focusing on quantifying these risks with a topological resilience metric corresponding to the largest exogenous systemic shock that the production network can withstand, such that almost all of the network survives with high probability. We model failures using a node percolation process where systemic shocks cause suppliers to fail, leading to further breakdowns. We classify networks into two categories -- resilient and fragile -- based on their ability to handle shocks as the network grows large, and give bounds on their resilience. We show that the main factors affecting resilience are the number of raw products (primary sector), the number of final goods (final sector), and the source and supply dependencies. Further, we give methods to lower bound resilience based on bounding the cascade size with a linear program that can be efficiently calculated. We establish connections between our model, the independent cascade model, the Risk Exposure Index, and the Eisenberg-Noe contagion model. We give an almost linear-time deterministic algorithm to approximate the cascade size, which matches known lower bounds up to logarithmic factors. Finally, we design intervention algorithms and show that under reasonable assumptions, targeting nodes based on Katz centrality in the edge-reversed network is optimal. Finally, we account for network heterogeneities and validate our findings with real-world data."}, "https://arxiv.org/abs/2310.09098": {"title": "Growth, Poverty Trap and Escape", "link": "https://arxiv.org/abs/2310.09098", "description": "arXiv:2310.09098v3 Announce Type: replace \nAbstract: The well-known Solow growth model is the workhorse model of the theory of economic growth, which studies capital accumulation in a model economy as a function of time with capital stock, labour and technology-based production as the basic ingredients. The capital is assumed to be in the form of manufacturing equipment and materials. Two important parameters of the model are: the saving fraction $s$ of the output of a production function and the technology efficiency parameter $A$, appearing in the production function. The saved fraction of the output is fully invested in the generation of new capital and the rest is consumed. The capital stock also depreciates as a function of time due to the wearing out of old capital and the increase in the size of the labour population. We propose a stochastic Solow growth model assuming the saving fraction to be a sigmoidal function of the per capita capital $k_p$. We derive analytically the steady state probability distribution $P(k_p)$ and demonstrate the existence of a poverty trap, of central concern in development economics. In a parameter regime, $P(k_p)$ is bimodal with the twin peaks corresponding to states of poverty and well-being respectively. The associated potential landscape has two valleys with fluctuation-driven transitions between them. The mean exit times from the valleys are computed and one finds that the escape from a poverty trap is more favourable at higher values of $A$. We identify a critical value of $A_c$ below (above) which the state of poverty (well-being) dominates and propose two early signatures of the regime shift occurring at $A_c$. The economic model, with conceptual foundation in nonlinear dynamics and statistical mechanics, shares universal features with dynamical models from diverse disciplines like ecology and cell biology."}, "https://arxiv.org/abs/2401.15031": {"title": "Tensor product algorithms for inference of contact network from epidemiological data", "link": "https://arxiv.org/abs/2401.15031", "description": "arXiv:2401.15031v2 Announce Type: replace-cross \nAbstract: We consider a problem of inferring contact network from nodal states observed during an epidemiological process. In a black--box Bayesian optimisation framework this problem reduces to a discrete likelihood optimisation over the set of possible networks. The cardinality of this set grows combinatorially with the number of network nodes, which makes this optimisation computationally challenging. For each network, its likelihood is the probability for the observed data to appear during the evolution of the epidemiological process on this network. This probability can be very small, particularly if the network is significantly different from the ground truth network, from which the observed data actually appear. A commonly used stochastic simulation algorithm struggles to recover rare events and hence to estimate small probabilities and likelihoods. In this paper we replace the stochastic simulation with solving the chemical master equation for the probabilities of all network states. Since this equation also suffers from the curse of dimensionality, we apply tensor train approximations to overcome it and enable fast and accurate computations. Numerical simulations demonstrate efficient black--box Bayesian inference of the network."}, "https://arxiv.org/abs/2409.02965": {"title": "Do We Trust What They Say or What They Do? A Multimodal User Embedding Provides Personalized Explanations", "link": "https://arxiv.org/abs/2409.02965", "description": "arXiv:2409.02965v1 Announce Type: new \nAbstract: With the rapid development of social media, the importance of analyzing social network user data has also been put on the agenda. User representation learning in social media is a critical area of research, based on which we can conduct personalized content delivery, or detect malicious actors. Being more complicated than many other types of data, social network user data has inherent multimodal nature. Various multimodal approaches have been proposed to harness both text (i.e. post content) and relation (i.e. inter-user interaction) information to learn user embeddings of higher quality. The advent of Graph Neural Network models enables more end-to-end integration of user text embeddings and user interaction graphs in social networks. However, most of those approaches do not adequately elucidate which aspects of the data - text or graph structure information - are more helpful for predicting each specific user under a particular task, putting some burden on personalized downstream analysis and untrustworthy information filtering. We propose a simple yet effective framework called Contribution-Aware Multimodal User Embedding (CAMUE) for social networks. We have demonstrated with empirical evidence, that our approach can provide personalized explainable predictions, automatically mitigating the impact of unreliable information. We also conducted case studies to show how reasonable our results are. We observe that for most users, graph structure information is more trustworthy than text information, but there are some reasonable cases where text helps more. Our work paves the way for more explainable, reliable, and effective social media user embedding which allows for better personalized content delivery."}, "https://arxiv.org/abs/2409.03067": {"title": "A Comparative Study of Offline Models and Online LLMs in Fake News Detection", "link": "https://arxiv.org/abs/2409.03067", "description": "arXiv:2409.03067v1 Announce Type: new \nAbstract: Fake news detection remains a critical challenge in today's rapidly evolving digital landscape, where misinformation can spread faster than ever before. Traditional fake news detection models often rely on static datasets and auxiliary information, such as metadata or social media interactions, which limits their adaptability to real-time scenarios. Recent advancements in Large Language Models (LLMs) have demonstrated significant potential in addressing these challenges due to their extensive pre-trained knowledge and ability to analyze textual content without relying on auxiliary data. However, many of these LLM-based approaches are still rooted in static datasets, with limited exploration into their real-time processing capabilities. This paper presents a systematic evaluation of both traditional offline models and state-of-the-art LLMs for real-time fake news detection. We demonstrate the limitations of existing offline models, including their inability to adapt to dynamic misinformation patterns. Furthermore, we show that newer LLM models with online capabilities, such as GPT-4, Claude, and Gemini, are better suited for detecting emerging fake news in real-time contexts. Our findings emphasize the importance of transitioning from offline to online LLM models for real-time fake news detection. Additionally, the public accessibility of LLMs enhances their scalability and democratizes the tools needed to combat misinformation. By leveraging real-time data, our work marks a significant step toward more adaptive, effective, and scalable fake news detection systems."}, "https://arxiv.org/abs/2409.03127": {"title": "Fast algorithms to improve fair information access in networks", "link": "https://arxiv.org/abs/2409.03127", "description": "arXiv:2409.03127v1 Announce Type: new \nAbstract: When information spreads across a network via pairwise sharing, large disparities in information access can arise from the network's structural heterogeneity. Algorithms to improve the fairness of information access seek to maximize the minimum access of a node to information by sequentially selecting new nodes to seed with the spreading information. However, existing algorithms are computationally expensive. Here, we develop and evaluate a set of 10 new scalable algorithms to improve information access in social networks; in order to compare them to the existing state-of-the-art, we introduce both a new performance metric and a new benchmark corpus of networks. Additionally, we investigate the degree to which algorithm performance on minimizing information access gaps can be predicted ahead of time from features of a network's structure. We find that while no algorithm is strictly superior to all others across networks, our new scalable algorithms are competitive with the state-of-the-art and orders of magnitude faster. We introduce a meta-learner approach that learns which of the fast algorithms is best for a specific network and is on average only 20% less effective than the state-of-the-art performance on held-out data, while about 75-130 times faster. Furthermore, on about 20% of networks the meta-learner's performance exceeds the state-of-the-art."}, "https://arxiv.org/abs/2409.03263": {"title": "A typology of activities over a century of urban growth", "link": "https://arxiv.org/abs/2409.03263", "description": "arXiv:2409.03263v1 Announce Type: new \nAbstract: Contemporary literature on the dynamics of economic activities in growing cities mainly focused on a few years or decades time frames. Using a new geo-historical database constructed from historical directories with about 1 million entries, we present a comprehensive analysis of the dynamics of activities in a major city, Paris, over almost a century (1829-1907). Our analysis suggests that activities that accompany city growth can be classified in different categories according to their dynamics and their scaling with population: (i) linear for everyday needs of residents (food stores, clothing retailers, health care practitioners), (ii) sublinear for public services (legal, administrative, educational), (iii) superlinear for the city's specific features (passing fads, specialization, timely needs). The dynamics of these activities is in addition very sensitive to historical perturbations such as large scale public works or political conflicts. These results shed light on the evolution of activities, a crucial component of growing cities."}, "https://arxiv.org/abs/2409.03349": {"title": "Spectral signatures of structural change in financial networks", "link": "https://arxiv.org/abs/2409.03349", "description": "arXiv:2409.03349v1 Announce Type: new \nAbstract: The level of systemic risk in economic and financial systems is strongly determined by the structure of the underlying networks of interdependent entities that can propagate shocks and stresses. Since changes in network structure imply changes in risk levels, it is important to identify structural transitions potentially leading to system-wide crises. Methods have been proposed to assess whether a real-world network is in a (quasi-)stationary state by checking the consistency of its structural evolution with appropriate maximum-entropy ensembles of graphs. While previous analyses of this kind have focused on dyadic and triadic motifs, hence disregarding higher-order structures, here we consider closed walks of any length. Specifically, we study the ensemble properties of the spectral radius of random graph models calibrated on real-world evolving networks. Our approach is shown to work remarkably well for directed networks, both binary and weighted. As illustrative examples, we consider the Electronic Market for Interbank Deposit (e-MID), the Dutch Interbank Network (DIN) and the International Trade Network (ITN) in their evolution across the 2008 crisis. By monitoring the deviation of the spectral radius from its ensemble expectation, we find that the ITN remains in a (quasi-)equilibrium state throughout the period considered, while both the DIN and e-MID exhibit a clear out-of-equilibrium behaviour. The spectral deviation therefore captures ongoing topological changes, extending over all length scales, to provide a compact proxy of the resilience of economic and financial networks."}, "https://arxiv.org/abs/2409.03593": {"title": "Ensuring resilience to extreme weather events increases the ambition of mitigation scenarios on solar power and storage uptake: a study on the Italian power system", "link": "https://arxiv.org/abs/2409.03593", "description": "arXiv:2409.03593v1 Announce Type: new \nAbstract: This study explores compounding impacts of climate change on power system's load and generation, emphasising the need to integrate adaptation and mitigation strategies into investment planning. We combine existing and novel empirical evidence to model impacts on: i) air-conditioning demand; ii) thermal power outages; iii) hydro-power generation shortages. Using a power dispatch and capacity expansion model, we analyse the Italian power system's response to these climate impacts in 2030, integrating mitigation targets and optimising for cost-efficiency at an hourly resolution. We outline different meteorological scenarios to explore the impacts of both average climatic changes and the intensification of extreme weather events. We find that addressing extreme weather in power system planning will require an extra 5-8 GW of photovoltaic (PV) capacity, on top of the 50 GW of the additional solar PV capacity required by the mitigation target alone. Despite the higher initial investments, we find that the adoption of renewable technologies, especially PV, alleviates the power system's vulnerability to climate change and extreme weather events. Furthermore, enhancing short-term storage with lithium-ion batteries is crucial to counterbalance the reduced availability of dispatchable hydro generation."}, "https://arxiv.org/abs/2409.03676": {"title": "Signature of maturity in cryptocurrency volatility", "link": "https://arxiv.org/abs/2409.03676", "description": "arXiv:2409.03676v1 Announce Type: new \nAbstract: We study the fluctuations, particularly the inequality of fluctuations, in cryptocurrency prices over the last ten years. We calculate the inequality in the price fluctuations through different measures, such as the Gini and Kolkata indices, and also the $Q$ factor (given by the ratio between the highest value and the average value) of these fluctuations. We compare the results with the equivalent quantities in some of the more prominent national currencies and see that while the fluctuations (or inequalities in such fluctuations) for cryptocurrencies were initially significantly higher than national currencies, over time the fluctuation levels of cryptocurrencies tend towards the levels characteristic of national currencies. We also compare similar quantities for a few prominent stock prices."}, "https://arxiv.org/abs/2409.03111": {"title": "What is Normal? A Big Data Observational Science Model of Anonymized Internet Traffic", "link": "https://arxiv.org/abs/2409.03111", "description": "arXiv:2409.03111v1 Announce Type: cross \nAbstract: Understanding what is normal is a key aspect of protecting a domain. Other domains invest heavily in observational science to develop models of normal behavior to better detect anomalies. Recent advances in high performance graph libraries, such as the GraphBLAS, coupled with supercomputers enables processing of the trillions of observations required. We leverage this approach to synthesize low-parameter observational models of anonymized Internet traffic with a high regard for privacy."}, "https://arxiv.org/abs/2310.13845": {"title": "Spectral-Aware Augmentation for Enhanced Graph Representation Learning", "link": "https://arxiv.org/abs/2310.13845", "description": "arXiv:2310.13845v2 Announce Type: replace-cross \nAbstract: Graph Contrastive Learning (GCL) has demonstrated remarkable effectiveness in learning representations on graphs in recent years. To generate ideal augmentation views, the augmentation generation methods should preserve essential information while discarding less relevant details for downstream tasks. However, current augmentation methods usually involve random topology corruption in the spatial domain, which fails to adequately address information spread across different frequencies in the spectral domain. Our preliminary study highlights this issue, demonstrating that spatial random perturbations impact all frequency bands almost uniformly. Given that task-relevant information typically resides in specific spectral regions that vary across graphs, this one-size-fits-all approach can pose challenges. We argue that indiscriminate spatial random perturbation might unintentionally weaken task-relevant information, reducing its effectiveness.\n  To tackle this challenge, we propose applying perturbations selectively, focusing on information specific to different frequencies across diverse graphs. In this paper, we present GASSER, a model that applies tailored perturbations to specific frequencies of graph structures in the spectral domain, guided by spectral hints. Through extensive experimentation and theoretical analysis, we demonstrate that the augmentation views generated by GASSER are adaptive, controllable, and intuitively aligned with the homophily ratios and spectrum of graph structures."}, "https://arxiv.org/abs/2409.03916": {"title": "A Survey on Signed Graph Embedding: Methods and Applications", "link": "https://arxiv.org/abs/2409.03916", "description": "arXiv:2409.03916v1 Announce Type: new \nAbstract: A signed graph (SG) is a graph where edges carry sign information attached to it. The sign of a network can be positive, negative, or neutral. A signed network is ubiquitous in a real-world network like social networks, citation networks, and various technical networks. There are many network embedding models have been proposed and developed for signed networks for both homogeneous and heterogeneous types. SG embedding learns low-dimensional vector representations for nodes of a network, which helps to do many network analysis tasks such as link prediction, node classification, and community detection. In this survey, we perform a comprehensive study of SG embedding methods and applications. We introduce here the basic theories and methods of SGs and survey the current state of the art of signed graph embedding methods. In addition, we explore the applications of different types of SG embedding methods in real-world scenarios. As an application, we have explored the citation network to analyze authorship networks. We also provide source code and datasets to give future direction. Lastly, we explore the challenges of SG embedding and forecast various future research directions in this field."}, "https://arxiv.org/abs/2409.03948": {"title": "The Veracity Problem: Detecting False Information and its Propagation on Online Social Media Networks", "link": "https://arxiv.org/abs/2409.03948", "description": "arXiv:2409.03948v1 Announce Type: new \nAbstract: Detecting false information on social media is critical in mitigating its negative societal impacts. To reduce the propagation of false information, automated detection provide scalable, unbiased, and cost-effective methods. However, there are three potential research areas identified which once solved improve detection. First, current AI-based solutions often provide a uni-dimensional analysis on a complex, multi-dimensional issue, with solutions differing based on the features used. Furthermore, these methods do not account for the temporal and dynamic changes observed within the document's life cycle. Second, there has been little research on the detection of coordinated information campaigns and in understanding the intent of the actors and the campaign. Thirdly, there is a lack of consideration of cross-platform analysis, with existing datasets focusing on a single platform, such as X, and detection models designed for specific platform.\n  This work aims to develop methods for effective detection of false information and its propagation. To this end, firstly we aim to propose the creation of an ensemble multi-faceted framework that leverages multiple aspects of false information. Secondly, we propose a method to identify actors and their intent when working in coordination to manipulate a narrative. Thirdly, we aim to analyse the impact of cross-platform interactions on the propagation of false information via the creation of a new dataset."}, "https://arxiv.org/abs/2409.03989": {"title": "Understanding Online Discussion Across Difference: Insights from Gun Discourse on Reddit", "link": "https://arxiv.org/abs/2409.03989", "description": "arXiv:2409.03989v1 Announce Type: new \nAbstract: When discussing difficult topics online, is it common to meaningfully engage with people from diverse perspectives? Why or why not? Could features of the online environment be redesigned to encourage civil conversation across difference? In this paper, we study discussions of gun policy on Reddit, with the overarching goal of developing insights into the potential of the internet to support understanding across difference. We use two methods: a clustering analysis of Reddit posts to contribute insights about what people discuss, and an interview study of twenty Reddit users to help us understand why certain kinds of conversation take place and others don't. We find that the discussion of gun politics falls into three groups: conservative pro-gun, liberal pro-gun, and liberal anti-gun. Each type of group has its own characteristic topics. While our subjects state that they would be willing to engage with others across the ideological divide, in practice they rarely do. Subjects are siloed into like-minded subreddits through a two-pronged effect, where they are simultaneously pushed away from opposing-view communities while actively seeking belonging in like-minded ones. Another contributing factor is Reddit's \"karma\" mechanism: fear of being downvoted and losing karma points and social approval of peers causes our subjects to hesitate to say anything in conflict with group norms. The pseudonymous nature of discussion on Reddit plays a complex role, with some subjects finding it freeing and others fearing reprisal from others not bound by face-to-face norms of politeness. Our subjects believe that content moderation can help ameliorate these issues; however, our findings suggest that moderators need different tools to do so effectively. We conclude by suggesting platform design changes that might increase discussion across difference."}, "https://arxiv.org/abs/2409.04085": {"title": "Structure and dynamics of growing networks of Reddit threads", "link": "https://arxiv.org/abs/2409.04085", "description": "arXiv:2409.04085v1 Announce Type: new \nAbstract: Millions of people use online social networks to reinforce their sense of belonging, for example by giving and asking for feedback as a form of social validation and self-recognition. It is common to observe disagreement among people beliefs and points of view when expressing this feedback. Modeling and analyzing such interactions is crucial to understand social phenomena that happen when people face different opinions while expressing and discussing their values. In this work, we study a Reddit community in which people participate to judge or be judged with respect to some behavior, as it represents a valuable source to study how users express judgments online. We model threads of this community as complex networks of user interactions growing in time, and we analyze the evolution of their structural properties. We show that the evolution of Reddit networks differ from other real social networks, despite falling in the same category. This happens because their global clustering coefficient is extremely small and the average shortest path length increases over time. Such properties reveal how users discuss in threads, i.e. with mostly one other user and often by a single message. We strengthen such result by analyzing the role that disagreement and reciprocity play in such conversations. We also show that Reddit thread's evolution over time is governed by two subgraphs growing at different speeds. We discover that, in the studied community, the difference of such speed is higher than in other communities because of the user guidelines enforcing specific user interactions. Finally, we interpret the obtained results on user behavior drawing back to Social Judgment Theory."}, "https://arxiv.org/abs/2409.04299": {"title": "Multilayer networks describing interactions in urban systems: a digital twin of five cities in Spain", "link": "https://arxiv.org/abs/2409.04299", "description": "arXiv:2409.04299v1 Announce Type: new \nAbstract: Networks specifying who interacts with whom are crucial for mathematical models of epidemic spreading. In the context of emerging diseases, these networks have the potential to encode multiple interaction contexts where non-pharmaceutical interventions can be introduced, allowing for proper comparisons among different intervention strategies in a plethora of contexts. Consequently, a multilayer network describing interactions in a population and detailing their contexts in different layers constitutes an appropriate tool for such descriptions. These approaches however become challenging in large-scale systems such as cities, particularly in a framework where data protection policies are enhanced. In this work, we present a methodology to build such multilayer networks and make those corresponding to five Spanish cities available. Our work uses approaches informed by multiple available datasets to create realistic digital twins of the citizens and their interactions and provides a playground to explore different pandemic scenario in realistic settings for better preparedness."}, "https://arxiv.org/abs/2409.03968": {"title": "Jam-absorption driving with data assimilation", "link": "https://arxiv.org/abs/2409.03968", "description": "arXiv:2409.03968v1 Announce Type: cross \nAbstract: This paper introduces a data assimilation (DA) framework based on the extended Kalman filter-cell transmission model, designed to assist jam-absorption driving (JAD) operation to alleviate sag traffic congestion. To ascertain and demonstrate the effectiveness of the DA framework for JAD operation, in this paper, we initially investigated its impact on the motion and control performance of a single absorbing vehicle. Numerical results show that the DA framework effectively mitigated underestimated or overestimated control failures of JAD caused by misestimation of key parameters (e.g., free flow speed and critical density) of the traffic flow fundamental diagram. The findings suggest that the proposed DA framework can reduce control failures and prevent significant declines and deteriorations in JAD performance caused by changes in traffic characteristics, e.g., weather conditions or traffic composition."}, "https://arxiv.org/abs/2207.05046": {"title": "Dynamic random graphs with vertex removal", "link": "https://arxiv.org/abs/2207.05046", "description": "arXiv:2207.05046v2 Announce Type: replace-cross \nAbstract: We introduce and analyse a Dynamic Random Graph with Vertex Removal (DRGVR) defined as follows. At every step, with probability $p > 1/2$ a new vertex is introduced, and with probability $1-p$ a vertex, chosen uniformly at random among the present ones (if any), is removed from the graph together with all edges adjacent to it. In the former case, the new vertex connects by an edge to every other vertex with probability inversely proportional to the number of vertices already present.\n  We prove that the DRGVR converges to a local limit and determine this limit. Moreover, we analyse its component structure and distinguish a subcritical and a supercritical regime with respect to the existence of a giant component. As a byproduct of this analysis, we obtain upper and lower bounds for the critical parameter. Furthermore, we provide precise expression of the maximum degree (as well as in- and out-degree for a natural orientation of the DRGVR). Several concentration and stability results complete the study."}, "https://arxiv.org/abs/2402.16990": {"title": "inGRASS: Incremental Graph Spectral Sparsification via Low-Resistance-Diameter Decomposition", "link": "https://arxiv.org/abs/2402.16990", "description": "arXiv:2402.16990v2 Announce Type: replace-cross \nAbstract: This work presents inGRASS, a novel algorithm designed for incremental spectral sparsification of large undirected graphs. The proposed inGRASS algorithm is highly scalable and parallel-friendly, having a nearly-linear time complexity for the setup phase and the ability to update the spectral sparsifier in $O(\\log N)$ time for each incremental change made to the original graph with $N$ nodes. A key component in the setup phase of inGRASS is a multilevel resistance embedding framework introduced for efficiently identifying spectrally-critical edges and effectively detecting redundant ones, which is achieved by decomposing the initial sparsifier into many node clusters with bounded effective-resistance diameters leveraging a low-resistance-diameter decomposition (LRD) scheme. The update phase of inGRASS exploits low-dimensional node embedding vectors for efficiently estimating the importance and uniqueness of each newly added edge. As demonstrated through extensive experiments, inGRASS achieves up to over $200 \\times$ speedups while retaining comparable solution quality in incremental spectral sparsification of graphs obtained from various datasets, such as circuit simulations, finite element analysis, and social networks."}, "https://arxiv.org/abs/2405.03486": {"title": "UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images", "link": "https://arxiv.org/abs/2405.03486", "description": "arXiv:2405.03486v2 Announce Type: replace-cross \nAbstract: With the advent of text-to-image models and concerns about their misuse, developers are increasingly relying on image safety classifiers to moderate their generated unsafe images. Yet, the performance of current image safety classifiers remains unknown for both real-world and AI-generated images. In this work, we propose UnsafeBench, a benchmarking framework that evaluates the effectiveness and robustness of image safety classifiers, with a particular focus on the impact of AI-generated images on their performance. First, we curate a large dataset of 10K real-world and AI-generated images that are annotated as safe or unsafe based on a set of 11 unsafe categories of images (sexual, violent, hateful, etc.). Then, we evaluate the effectiveness and robustness of five popular image safety classifiers, as well as three classifiers that are powered by general-purpose visual language models. Our assessment indicates that existing image safety classifiers are not comprehensive and effective enough to mitigate the multifaceted problem of unsafe images. Also, there exists a distribution shift between real-world and AI-generated images in image qualities, styles, and layouts, leading to degraded effectiveness and robustness. Motivated by these findings, we build a comprehensive image moderation tool called PerspectiveVision, which addresses the main drawbacks of existing classifiers with improved effectiveness and robustness, especially on AI-generated images. UnsafeBench and PerspectiveVision can aid the research community in better understanding the landscape of image safety classification in the era of generative AI."}, "https://arxiv.org/abs/2405.03507": {"title": "Emergence of condensation patterns in kinetic equations for opinion dynamics", "link": "https://arxiv.org/abs/2405.03507", "description": "arXiv:2405.03507v2 Announce Type: replace-cross \nAbstract: In this work, we define a class of models to understand the impact of population size on opinion formation dynamics, a phenomenon usually related to group conformity. To this end, we introduce a new kinetic model in which the interaction frequency is weighted by the kinetic density. In the quasi-invariant regime, this model reduces to a Kaniadakis-Quarati-type equation with nonlinear drift, originally introduced for the dynamics of bosons in a spatially homogeneous setting. From the obtained PDE for the evolution of the opinion density, we determine the regime of parameters for which a critical mass exists and triggers blow-up of the solution. Therefore, the model is capable of describing strong conformity phenomena in cases where the total density of individuals holding a given opinion exceeds a fixed critical size. In the final part, several numerical experiments demonstrate the features of the introduced class of models and the related consensus effects."}, "https://arxiv.org/abs/2409.04649": {"title": "Preserving Individuality while Following the Crowd: Understanding the Role of User Taste and Crowd Wisdom in Online Product Rating Prediction", "link": "https://arxiv.org/abs/2409.04649", "description": "arXiv:2409.04649v1 Announce Type: new \nAbstract: Numerous algorithms have been developed for online product rating prediction, but the specific influence of user and product information in determining the final prediction score remains largely unexplored. Existing research often relies on narrowly defined data settings, which overlooks real-world challenges such as the cold-start problem, cross-category information utilization, and scalability and deployment issues. To delve deeper into these aspects, and particularly to uncover the roles of individual user taste and collective wisdom, we propose a unique and practical approach that emphasizes historical ratings at both the user and product levels, encapsulated using a continuously updated dynamic tree representation. This representation effectively captures the temporal dynamics of users and products, leverages user information across product categories, and provides a natural solution to the cold-start problem. Furthermore, we have developed an efficient data processing strategy that makes this approach highly scalable and easily deployable. Comprehensive experiments in real industry settings demonstrate the effectiveness of our approach. Notably, our findings reveal that individual taste dominates over collective wisdom in online product rating prediction, a perspective that contrasts with the commonly observed wisdom of the crowd phenomenon in other domains. This dominance of individual user taste is consistent across various model types, including the boosting tree model, recurrent neural network (RNN), and transformer-based architectures. This observation holds true across the overall population, within individual product categories, and in cold-start scenarios. Our findings underscore the significance of individual user tastes in the context of online product rating prediction and the robustness of our approach across different model architectures."}, "https://arxiv.org/abs/2409.04696": {"title": "Dominant strategy in repeated games on networks", "link": "https://arxiv.org/abs/2409.04696", "description": "arXiv:2409.04696v1 Announce Type: new \nAbstract: Direct reciprocity, stemming from repeated interactions among players, is one of the fundamental mechanisms for understanding the evolution of cooperation. However, canonical strategies for the repeated prisoner's dilemma, such as Win-Stay-Lose-Shift and Tit-for-Tat, fail to consistently dominate alternative strategies during evolution. This complexity intensifies with the introduction of spatial structure or network behind individual interactions, where nodes represent players and edges represent their interactions. Here, we propose a new strategy, ``Cooperate-Stay-Defect-Tolerate\" (CSDT), which can dominate other strategies within networked populations by adhering to three essential characteristics. This strategy maintains current behaviour when the opponent cooperates and tolerates defection to a limited extent when the opponent defects. We demonstrate that the limit of tolerance of CSDT can vary with the network structure, evolutionary dynamics, and game payoffs. Furthermore, we find that incorporating the Always Defect strategy (ALLD) can enhance the evolution of CSDT and eliminate strategies that are vulnerable to defection in the population, providing a new interpretation of the role of ALLD in direct reciprocity. Our findings offer a novel perspective on how cooperative strategy evolves on networked populations."}, "https://arxiv.org/abs/2409.05296": {"title": "Understanding changes in traffic demand during the Tokyo 2020 Olympic and Paralympic Games", "link": "https://arxiv.org/abs/2409.05296", "description": "arXiv:2409.05296v1 Announce Type: new \nAbstract: This paper evaluated the effects of the Tokyo 2020 Olympic and Paralympic Games on traffic demand on the Metropolitan expressway. We constructed panel data for both passenger and freight vehicles' demand using longitudinal disaggregated trip records from the Metropolitan expressway. Subsequently, we established a demand function and used a difference-in-differences method to individually estimate the impacts of toll surcharges and other Olympics-related factors by leveraging the fact that the toll surcharges were not applied to freight vehicles.\n  The results indicate that toll surcharges resulted in a decrease of 25.0 % for weekdays and 36.8 % for weekends/holidays in passenger vehicle demand on the Metropolitan expressway. The estimated toll elasticities are 0.345 for weekdays and 0.615 for weekends/holidays, respectively. Notably, analysis of the Olympics-related factor demonstrated that travel demand management (TDM) strategies effectively curbed demand on weekends/holidays with a reduction of 2.9 % in traffic demand. However, on weekdays, induced demand surpassed the reduction of demand by other TDM strategies than tolling, resulting in a 4.6 % increase in traffic demand.\n  Additionally, We developed a zone-based demand function and investigate the spatial heterogeneity in toll elasticity. Our findings revealed small heterogeneity for weekdays (0.283 to 0.509) and large heterogeneity for weekends/holidays (0.484 to 0.935)."}, "https://arxiv.org/abs/2409.05471": {"title": "Fast Computation of Kemeny's Constant for Directed Graphs", "link": "https://arxiv.org/abs/2409.05471", "description": "arXiv:2409.05471v1 Announce Type: new \nAbstract: Kemeny's constant for random walks on a graph is defined as the mean hitting time from one node to another selected randomly according to the stationary distribution. It has found numerous applications and attracted considerable research interest. However, exact computation of Kemeny's constant requires matrix inversion, which scales poorly for large networks with millions of nodes. Existing approximation algorithms either leverage properties exclusive to undirected graphs or involve inefficient simulation, leaving room for further optimization. To address these limitations for directed graphs, we propose two novel approximation algorithms for estimating Kemeny's constant on directed graphs with theoretical error guarantees. Extensive numerical experiments on real-world networks validate the superiority of our algorithms over baseline methods in terms of efficiency and accuracy."}, "https://arxiv.org/abs/2409.05503": {"title": "Fast Computation for the Forest Matrix of an Evolving Graph", "link": "https://arxiv.org/abs/2409.05503", "description": "arXiv:2409.05503v1 Announce Type: new \nAbstract: The forest matrix plays a crucial role in network science, opinion dynamics, and machine learning, offering deep insights into the structure of and dynamics on networks. In this paper, we study the problem of querying entries of the forest matrix in evolving graphs, which more accurately represent the dynamic nature of real-world networks compared to static graphs. To address the unique challenges posed by evolving graphs, we first introduce two approximation algorithms, \\textsc{SFQ} and \\textsc{SFQPlus}, for static graphs. \\textsc{SFQ} employs a probabilistic interpretation of the forest matrix, while \\textsc{SFQPlus} incorporates a novel variance reduction technique and is theoretically proven to offer enhanced accuracy. Based on these two algorithms, we further devise two dynamic algorithms centered around efficiently maintaining a list of spanning converging forests. This approach ensures $O(1)$ runtime complexity for updates, including edge additions and deletions, as well as for querying matrix elements, and provides an unbiased estimation of forest matrix entries. Finally, through extensive experiments on various real-world networks, we demonstrate the efficiency and effectiveness of our algorithms. Particularly, our algorithms are scalable to massive graphs with more than forty million nodes."}, "https://arxiv.org/abs/2409.05551": {"title": "Indirect reciprocity under opinion synchronization", "link": "https://arxiv.org/abs/2409.05551", "description": "arXiv:2409.05551v1 Announce Type: new \nAbstract: Indirect reciprocity is a key explanation for the exceptional magnitude of cooperation among humans. This literature suggests that a large proportion of human cooperation is driven by social norms and individuals' incentives to maintain a good reputation. This intuition has been formalized with two types of models. In public assessment models, all community members are assumed to agree on each others' reputations; in private assessment models, people may have disagreements. Both types of models aim to understand the interplay of social norms and cooperation. Yet their results can be vastly different. Public assessment models argue that cooperation can evolve easily, and that the most effective norms tend to be stern. Private assessment models often find cooperation to be unstable, and successful norms show some leniency. Here, we propose a model that can organize these differing results within a single framework. We show that the stability of cooperation depends on a single quantity: the extent to which individual opinions turn out to be correlated. This correlation is determined by a group's norms and the structure of social interactions. In particular, we prove that no cooperative norm is evolutionarily stable when individual opinions are statistically independent. These results have important implications for our understanding of cooperation, conformity, and polarization."}, "https://arxiv.org/abs/2409.05692": {"title": "Extracting the U", "link": "https://arxiv.org/abs/2409.05692", "description": "arXiv:2409.05692v1 Announce Type: new \nAbstract: Building type information is crucial for population estimation, traffic planning, urban planning, and emergency response applications. Although essential, such data is often not readily available. To alleviate this problem, this work creates a comprehensive dataset by providing residential/non-residential building classification covering the entire United States. We propose and utilize an unsupervised machine learning method to classify building types based on building footprints and available OpenStreetMap information. The classification result is validated using authoritative ground truth data for select counties in the U.S. The validation shows a high precision for non-residential building classification and a high recall for residential buildings. We identified various approaches to improving the quality of the classification, such as removing sheds and garages from the dataset. Furthermore, analyzing the misclassifications revealed that they are mainly due to missing and scarce metadata in OSM. A major result of this work is the resulting dataset of classifying 67,705,475 buildings. We hope that this data is of value to the scientific community, including urban and transportation planners."}, "https://arxiv.org/abs/2409.04860": {"title": "Sequential Classification of Misinformation", "link": "https://arxiv.org/abs/2409.04860", "description": "arXiv:2409.04860v1 Announce Type: cross \nAbstract: In recent years there have been a growing interest in online auditing of information flow over social networks with the goal of monitoring undesirable effects, such as, misinformation and fake news. Most previous work on the subject, focus on the binary classification problem of classifying information as fake or genuine. Nonetheless, in many practical scenarios, the multi-class/label setting is of particular importance. For example, it could be the case that a social media platform may want to distinguish between ``true\", ``partly-true\", and ``false\" information. Accordingly, in this paper, we consider the problem of online multiclass classification of information flow. To that end, driven by empirical studies on information flow over real-world social media networks, we propose a probabilistic information flow model over graphs. Then, the learning task is to detect the label of the information flow, with the goal of minimizing a combination of the classification error and the detection time. For this problem, we propose two detection algorithms; the first is based on the well-known multiple sequential probability ratio test, while the second is a novel graph neural network based sequential decision algorithm. For both algorithms, we prove several strong statistical guarantees. We also construct a data driven algorithm for learning the proposed probabilistic model. Finally, we test our algorithms over two real-world datasets, and show that they outperform other state-of-the-art misinformation detection algorithms, in terms of detection time and classification error."}, "https://arxiv.org/abs/2409.04936": {"title": "A Hetero-functional Graph Resilience Analysis for Convergent Systems-of-Systems", "link": "https://arxiv.org/abs/2409.04936", "description": "arXiv:2409.04936v1 Announce Type: cross \nAbstract: Our modern life has grown to depend on many and nearly ubiquitous large complex engineering systems. Many disciplines now seemingly ask the same question: ``In the face of assumed disruption, to what degree will these systems continue to perform and when will they be able to bounce back to normal operation\"? Furthermore, there is a growing recognition that the greatest societal challenges of the Anthropocene era are intertwined, necessitating a convergent systems-of-systems modeling and analysis framework based upon reconciled ontologies, data, and theoretical methods. Consequently, this paper develops a methodology for hetero-functional graph resilience analysis and demonstrates it on a convergent system-of-systems. It uses the Systems Modeling Language, model-based systems engineering and Hetero-Functional Graph Theory (HFGT) to overcome the convergence research challenges when constructing models and measures from multiple disciplines for systems resilience. The paper includes both the ``survival\" as well as ``recovery\" components of resilience. It also strikes a middle ground between two disparate approaches to resilience measurement: structural measurement of formal graphs and detailed behavioral simulation. This paper also generalizes a previous resilience measure based on HFGT and benefits from recent theoretical and computational developments in HFGT. To demonstrate the methodological developments, the resilience analysis is conducted on a hypothetical energy-water nexus system of moderate size as a type of system-of-systems."}, "https://arxiv.org/abs/2409.05292": {"title": "Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis", "link": "https://arxiv.org/abs/2409.05292", "description": "arXiv:2409.05292v1 Announce Type: cross \nAbstract: The world is currently experiencing an outbreak of mpox, which has been declared a Public Health Emergency of International Concern by WHO. No prior work related to social media mining has focused on the development of a dataset of Instagram posts about the mpox outbreak. The work presented in this paper aims to address this research gap and makes two scientific contributions to this field. First, it presents a multilingual dataset of 60,127 Instagram posts about mpox, published between July 23, 2022, and September 5, 2024. The dataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagram posts about mpox in 52 languages. For each of these posts, the Post ID, Post Description, Date of publication, language, and translated version of the post (translation to English was performed using the Google Translate API) are presented as separate attributes in the dataset. After developing this dataset, sentiment analysis, hate speech detection, and anxiety or stress detection were performed. This process included classifying each post into (i) one of the sentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or neutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no anxiety/stress detected. These results are presented as separate attributes in the dataset. Second, this paper presents the results of performing sentiment analysis, hate speech analysis, and anxiety or stress analysis. The variation of the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and neutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and 50.64%, respectively. In terms of hate speech detection, 95.75% of the posts did not contain hate and the remaining 4.25% of the posts contained hate. Finally, 72.05% of the posts did not indicate any anxiety/stress, and the remaining 27.95% of the posts represented some form of anxiety/stress."}, "https://arxiv.org/abs/2402.16782": {"title": "Multiplex measures for higher-order networks", "link": "https://arxiv.org/abs/2402.16782", "description": "arXiv:2402.16782v3 Announce Type: replace \nAbstract: A wide variety of complex systems are characterized by interactions of different types involving varying numbers of units. Multiplex hypergraphs serve as a tool to describe such structures, capturing distinct types of higher-order interactions among a collection of units. In this work, we introduce a comprehensive set of measures to describe structural connectivity patterns in multiplex hypergraphs, considering scales from node and hyperedge levels to the system's mesoscale. We validate our measures with three real-world datasets: scientific co-authorship in physics, movie collaborations, and high school interactions. This validation reveals new collaboration patterns, identifies trends within and across movie subfields, and provides insights into daily interaction dynamics. Our framework aims to offer a more nuanced characterization of real-world systems marked by both multiplex and higher-order interactions."}, "https://arxiv.org/abs/2403.19852": {"title": "A Review of Graph Neural Networks in Epidemic Modeling", "link": "https://arxiv.org/abs/2403.19852", "description": "arXiv:2403.19852v4 Announce Type: replace-cross \nAbstract: Since the onset of the COVID-19 pandemic, there has been a growing interest in studying epidemiological models. Traditional mechanistic models mathematically describe the transmission mechanisms of infectious diseases. However, they often suffer from limitations of oversimplified or fixed assumptions, which could cause sub-optimal predictive power and inefficiency in capturing complex relation information. Consequently, Graph Neural Networks(GNNs) have emerged as a progressively popular tool in epidemic research. In this paper, we endeavor to furnish a comprehensive review of GNNs in epidemic tasks and highlight potential future directions. To accomplish this objective, we introduce hierarchical taxonomies for both epidemic tasks and methodologies, offering a trajectory of development within this domain. For epidemic tasks, we establish a taxonomy akin to those typically employed within the epidemic domain. For methodology, we categorize existing work into Neural Models and Hybrid Models. Following this, we perform an exhaustive and systematic examination of the methodologies, encompassing both the tasks and their technical details. Furthermore, we discuss the limitations of existing methods from diverse perspectives and systematically propose future research directions. This survey aims to bridge literature gaps and promote the progression of this promising field, with a list of relevant papers at https://github.com/Emory-Melody/awesome-epidemic-modeling-papers. We hope that it will facilitate synergies between the communities of GNNs and epidemiology, and contribute to their collective progress."}, "https://arxiv.org/abs/2404.18942": {"title": "GuideWalk: A Novel Graph-Based Word Embedding for Enhanced Text Classification", "link": "https://arxiv.org/abs/2404.18942", "description": "arXiv:2404.18942v2 Announce Type: replace-cross \nAbstract: One of the prime problems of computer science and machine learning is to extract information efficiently from large-scale, heterogeneous data. Text data, with its syntax, semantics, and even hidden information content, possesses an exceptional place among the data types in concern. The processing of the text data requires embedding, a method of translating the content of the text to numeric vectors. A correct embedding algorithm is the starting point for obtaining the full information content of the text data. In this work, a new text embedding approach, namely the Guided Transition Probability Matrix (GTPM) model is proposed. The model uses the graph structure of sentences to capture different types of information from text data, such as syntactic, semantic, and hidden content. Using random walks on a weighted word graph, GTPM calculates transition probabilities to derive text embedding vectors. The proposed method is tested with real-world data sets and eight well-known and successful embedding algorithms. GTPM shows significantly better classification performance for binary and multi-class datasets than well-known algorithms. Additionally, the proposed method demonstrates superior robustness, maintaining performance with limited (only $10\\%$) training data, showing an $8\\%$ decline compared to $15-20\\%$ for baseline methods."}, "https://arxiv.org/abs/2409.06099": {"title": "Emerging properties of the degree distribution in large non-growing networks", "link": "https://arxiv.org/abs/2409.06099", "description": "arXiv:2409.06099v1 Announce Type: new \nAbstract: The degree distribution is a key statistical indicator in network theory, often used to understand how information spreads across connected nodes. In this paper, we focus on non-growing networks formed through a rewiring algorithm and develop kinetic Boltzmann-type models to capture the emergence of degree distributions that characterize both preferential attachment networks and random networks. Under a suitable mean-field scaling, these models reduce to a Fokker-Planck-type partial differential equation with an affine diffusion coefficient, that is consistent with a well-established master equation for discrete rewiring processes. We further analyze the convergence to equilibrium for this class of Fokker-Planck equations, demonstrating how different regimes -- ranging from exponential to algebraic rates -- depend on network parameters. Our results provide a unified framework for modeling degree distributions in non-growing networks and offer insights into the long-time behavior of such systems."}, "https://arxiv.org/abs/2409.06194": {"title": "Uncovering the inherited vulnerability of electric distribution networks", "link": "https://arxiv.org/abs/2409.06194", "description": "arXiv:2409.06194v1 Announce Type: new \nAbstract: Research on the vulnerability of electric networks with a complex network approach has produced significant results in the last decade, especially for transmission networks. These studies have shown that there are causal relations between certain structural properties of networks and their vulnerabilities, leading to an inherent weakness. The purpose of present work was twofold: to test the hypotheses already examined on evolving transmission networks and to gain a deeper understanding on the nature of these inherent weaknesses. For this, historical models of a medium-voltage distribution network supply area were reconstructed and analysed. Topological efficiency of the networks was calculated against node and edge removals of different proportions. We found that the tolerance of the evolving grid remained practically unchanged during the examined period, implying that the increase in size is dominantly caused by the connection of geographically and spatially constrained supply areas and not by an evolutionary process. We also show that probability density functions of centrality metrics, typically connected to vulnerability, show only minor variation during the early evolution of the examined distribution network, and in many cases resemble the properties of the modern days."}, "https://arxiv.org/abs/2409.06255": {"title": "Market Reaction to News Flows in Supply Chain Networks", "link": "https://arxiv.org/abs/2409.06255", "description": "arXiv:2409.06255v1 Announce Type: new \nAbstract: This study examines whether positive news about firms increases their stock prices and, moreover, whether it increases stock prices of the firms' suppliers and customers, using a large sample of publicly listed firms across the world and another of Japanese listed firms. The level of positiveness of each news article is determined by FinBERT, a natural language processing model fine-tuned specifically for financial information. Supply chains of firms across the world are identified mostly by financial statements, while those of Japanese firms are taken from large-scale firm-level surveys. We find that positive news increases the change rate of stock prices of firms mentioned in the news before its disclosure, most likely because of diffusion of information through informal channels. Positive news also raises stock prices of the firms' suppliers and customers before its disclosure, confirming propagation of market values through supply chains. In addition, we generally find a larger post-news effect on stock prices of the mentioned firms and their suppliers and customers than the pre-news effect. The positive difference between the post- and pre-news effects can be considered as the net effect of the disclosure of positive news, controlling for informal information diffusion. However, the post-news effect on suppliers and customers in Japan is smaller than the pre-news effect, a result opposite to those from firms across the world. This notable result is possibly because supply chain links of Japanese firms are stronger than global supply chains while such knowledge is restricted to selected investors."}, "https://arxiv.org/abs/2409.06417": {"title": "Fast nonparametric inference of network backbones for graph sparsification", "link": "https://arxiv.org/abs/2409.06417", "description": "arXiv:2409.06417v1 Announce Type: new \nAbstract: A network backbone provides a useful sparse representation of a weighted network by keeping only its most important links, permitting a range of computational speedups and simplifying complex network visualizations. There are many possible criteria for a link to be considered important, and hence many methods have been developed for the task of network backboning for graph sparsification. These methods can be classified as global or local in nature depending on whether they evaluate the importance of an edge in the context of the whole network or an individual node neighborhood. A key limitation of existing network backboning methods is that they either artificially restrict the topology of the backbone to take a specific form (e.g. a tree) or they require the specification of a free parameter (e.g. a significance level) that determines the number of edges to keep in the backbone. Here we develop a completely nonparametric framework for inferring the backbone of a weighted network that overcomes these limitations by automatically selecting the optimal number of edges to retain in the backbone using the Minimum Description Length (MDL) principle from information theory. We develop two encoding schemes that serve as objective functions for global and local network backbones, as well as efficient optimization algorithms to identify the optimal backbones according to these objectives with runtime complexity log-linear in the number of edges. We show that the proposed framework is generalizable to any discrete weight distribution on the edges using a maximum a posteriori (MAP) estimation procedure with an asymptotically equivalent Bayesian generative model of the backbone. We compare the proposed method with existing methods in a range of tasks on real and synthetic networks."}, "https://arxiv.org/abs/2409.06438": {"title": "Quantum-like approaches unveil the intrinsic limits of predictability in compartmental models", "link": "https://arxiv.org/abs/2409.06438", "description": "arXiv:2409.06438v1 Announce Type: new \nAbstract: Obtaining accurate forecasts for the evolution of epidemic outbreaks from deterministic compartmental models represents a major theoretical challenge. Recently, it has been shown that these models typically exhibit trajectories' degeneracy, as different sets of epidemiological parameters yield comparable predictions at early stages of the outbreak but disparate future epidemic scenarios. Here we use the Doi-Peliti approach and extend the classical deterministic SIS and SIR models to a quantum-like formalism to explore whether the uncertainty of epidemic forecasts is also shaped by the stochastic nature of epidemic processes. This approach allows getting a probabilistic ensemble of trajectories, revealing that epidemic uncertainty is not uniform across time, being maximal around the epidemic peak and vanishing at both early and very late stages of the outbreak. Our results therefore show that, independently of the models' complexity, the stochasticity of contagion and recover processes poses a natural constraint for the uncertainty of epidemic forecasts."}, "https://arxiv.org/abs/2409.06660": {"title": "Memory and Personality in Ideological Polarization: The Politico-physics of Mnemomatter", "link": "https://arxiv.org/abs/2409.06660", "description": "arXiv:2409.06660v1 Announce Type: new \nAbstract: We used physical agents with deep memories of past events and left/right ideologies but different fixed personalities to study what drives the polarization of the dynamic population ideology. We find that agents have a critical memory depth below which complete ideology polarization of the collective cannot occur and above which it is inevitable. However, depending on the details of the personalities, the ideologies polarization can be static or dynamic in time, even chaotic. Thus, agents with different personalities and levels of memory (mnemomatter) can serve as a physics analogue of the ideology dynamics among ideological beings, illuminating how decisions influenced by individual memories of past interactions can shape and influence subsequent ideology polarization. Each constituent agent harbors a private stack memory and an onboard microcomputer/controller which both measures and controls its physical spin handedness, which is a proxy for ideology. The agent's decision to change or retain its current spin is determined by each agent's private algorithm for decisions (the personality) and the time-weighted stack history of present and previous interactions. Depending on a given agent's personality for evaluating its memory and experiences, an agent can act as a curmudgeon who never changes its ideology, a pushover who always accepts change, a contrarian who always does the opposite of what is expected, an opportunist who weighs recent events more heavily than past events in making decisions, and a traditionalist who weighs past events more heavily than recent events in decision making. We develop a field theory which maps agent ideological polarization over into a dynamic potential landscape. Perhaps such applications of physics-based systems to political systems will help us to understand the ideological instability observed in the world today."}, "https://arxiv.org/abs/2409.05890": {"title": "Automating the Practice of Science -- Opportunities, Challenges, and Implications", "link": "https://arxiv.org/abs/2409.05890", "description": "arXiv:2409.05890v1 Announce Type: cross \nAbstract: Automation transformed various aspects of our human civilization, revolutionizing industries and streamlining processes. In the domain of scientific inquiry, automated approaches emerged as powerful tools, holding promise for accelerating discovery, enhancing reproducibility, and overcoming the traditional impediments to scientific progress. This article evaluates the scope of automation within scientific practice and assesses recent approaches. Furthermore, it discusses different perspectives to the following questions: Where do the greatest opportunities lie for automation in scientific practice?; What are the current bottlenecks of automating scientific practice?; and What are significant ethical and practical consequences of automating scientific practice? By discussing the motivations behind automated science, analyzing the hurdles encountered, and examining its implications, this article invites researchers, policymakers, and stakeholders to navigate the rapidly evolving frontier of automated scientific practice."}, "https://arxiv.org/abs/2409.05899": {"title": "Universal Workflow Language and Software Enables Geometric Learning and FAIR Scientific Protocol Reporting", "link": "https://arxiv.org/abs/2409.05899", "description": "arXiv:2409.05899v1 Announce Type: cross \nAbstract: The modern technological landscape has trended towards increased precision and greater digitization of information. However, the methods used to record and communicate scientific procedures have remained largely unchanged over the last century. Written text as the primary means for communicating scientific protocols poses notable limitations in human and machine information transfer. In this work, we present the Universal Workflow Language (UWL) and the open-source Universal Workflow Language interface (UWLi). UWL is a graph-based data architecture that can capture arbitrary scientific procedures through workflow representation of protocol steps and embedded procedure metadata. It is machine readable, discipline agnostic, and compatible with FAIR reporting standards. UWLi is an accompanying software package for building and manipulating UWL files into tabular and plain text representations in a controlled, detailed, and multilingual format. UWL transcription of protocols from three high-impact publications resulted in the identification of substantial deficiencies in the detail of the reported procedures. UWL transcription of these publications identified seventeen procedural ambiguities and thirty missing parameters for every one hundred words in published procedures. In addition to preventing and identifying procedural omission, UWL files were found to be compatible with geometric learning techniques for representing scientific protocols. In a surrogate function designed to represent an arbitrary multi-step experimental process, graph transformer networks were able to predict outcomes in approximately 6,000 fewer experiments than equivalent linear models. Implementation of UWL and UWLi into the scientific reporting process will result in higher reproducibility between both experimentalists and machines, thus proving an avenue to more effective modeling and control of complex systems."}, "https://arxiv.org/abs/2409.06091": {"title": "Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity", "link": "https://arxiv.org/abs/2409.06091", "description": "arXiv:2409.06091v1 Announce Type: cross \nAbstract: Multitask learning is a widely used paradigm for training models on diverse tasks, with applications ranging from graph neural networks to language model fine-tuning. Since tasks may interfere with each other, a key notion for modeling their relationships is task affinity. This includes pairwise task affinity, computed among pairs of tasks, and higher-order affinity, computed among subsets of tasks. Naively computing either of them requires repeatedly training on data from various task combinations, which is computationally intensive. We present a new algorithm Grad-TAG that can estimate task affinities without this repeated training.\n  The key idea of Grad-TAG is to train a \"base\" model for all tasks and then use a linearization technique to estimate the loss of the model for a specific task combination. The linearization works by computing a gradient-based approximation of the loss, using low-dimensional projections of gradients as features in a logistic regression to predict labels for the task combination. We show that the linearized model can provably approximate the loss when the gradient-based approximation is accurate, and also empirically verify that on several large models. Then, given the estimated task affinity, we design a semi-definite program for clustering similar tasks by maximizing the average density of clusters.\n  We evaluate Grad-TAG's performance across seven datasets, including multi-label classification on graphs, and instruction fine-tuning of language models. Our task affinity estimates are within 2.7% distance to the true affinities while needing only 3% of FLOPs in full training. On our largest graph with 21M edges and 500 labeling tasks, our algorithm delivers estimates within 5% distance to the true affinities, using only 112 GPU hours. Our results show that Grad-TAG achieves excellent performance and runtime tradeoffs compared to existing approaches."}, "https://arxiv.org/abs/2409.06212": {"title": "Is methane the 'climate culprit'? Fixing the 'Broken Record' while unmasking the dangers of using imprecise, long-term GWP for methane to address the climate emergency", "link": "https://arxiv.org/abs/2409.06212", "description": "arXiv:2409.06212v1 Announce Type: cross \nAbstract: Methane (CH4) is a potent greenhouse gas (GHG) with a short atmospheric half-life (~8.4 years) and a high short-term impact on global warming, significantly higher than CO2 (Kleinberg, 2020; Balcombe et al., 2018). Traditional metrics such as the 100-year Global Warming Potential (GWP100) obscure methane's short-term, negative climatic effects, potentially leading to inadequate policy responses (Kleinberg, 2020). This letter examines the limitations of GWP100 in capturing methane's true climate impact, explores alternative metrics, and discusses the implications of underreporting methane emissions. We highlight the necessity of adopting a more immediate perspective on methane to accelerate climate emergency action, while noting the adverse effects of the rapid growth rate of methane emissions on reduction efforts. Additionally, we hope that in the immediate future, during COP29, policymakers will adopt actions that give appropriate attention to methane's short-term warming potential to dramatically reduce emissions and address the immediate climate crisis."}, "https://arxiv.org/abs/2307.08131": {"title": "INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks", "link": "https://arxiv.org/abs/2307.08131", "description": "arXiv:2307.08131v4 Announce Type: replace \nAbstract: Leveraging network information for predictive modeling has become widespread in many domains. Within the realm of referral and targeted marketing, influencer detection stands out as an area that could greatly benefit from the incorporation of dynamic network representation due to the continuous evolution of customer-brand relationships. In this paper, we present INFLECT-DGNN, a new method for profit-driven INFLuencer prEdiCTion with Dynamic Graph Neural Networks that innovatively combines Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) with weighted loss functions, synthetic minority oversampling adapted to graph data, and a carefully crafted rolling-window strategy. We introduce a novel profit-driven framework that supports decision-making based on model predictions. To test the framework, we use a unique corporate dataset with diverse networks, capturing the customer interactions across three cities with different socioeconomic and demographic characteristics. Our results show how using RNNs to encode temporal attributes alongside GNNs significantly improves predictive performance, while the profit-driven framework determines the optimal classification threshold for profit maximization. We compare the results of different models to demonstrate the importance of capturing network representation, temporal dependencies, and using a profit-driven evaluation. Our research has significant implications for the fields of referral and targeted marketing, expanding the technical use of deep graph learning within corporate environments."}, "https://arxiv.org/abs/2308.10838": {"title": "An impossibility result for Markov Chain Monte Carlo sampling from micro-canonical bipartite graph ensembles", "link": "https://arxiv.org/abs/2308.10838", "description": "arXiv:2308.10838v3 Announce Type: replace \nAbstract: Markov Chain Monte Carlo (MCMC) algorithms are commonly used to sample from graph ensembles. Two graphs are neighbors in the state space if one can be obtained from the other with only a few modifications, e.g., edge rewirings. For many common ensembles, e.g., those preserving the degree sequences of bipartite graphs, rewiring operations involving two edges are sufficient to create a fully-connected state space, and they can be performed efficiently. We show that, for ensembles of bipartite graphs with fixed degree sequences and number of butterflies (k2,2 bi-cliques), there is no universal constant c such that a rewiring of at most c edges at every step is sufficient for any such ensemble to be fully connected. Our proof relies on an explicit construction of a family of pairs of graphs with the same degree sequences and number of butterflies, with each pair indexed by a natural c, and such that any sequence of rewiring operations transforming one graph into the other must include at least one rewiring operation involving at least c edges. Whether rewiring these many edges is sufficient to guarantee the full connectivity of the state space of any such ensemble remains an open question. Our result implies the impossibility of developing efficient, graph-agnostic, MCMC algorithms for these ensembles, as the necessity to rewire an impractically large number of edges may hinder taking a step on the state space."}, "https://arxiv.org/abs/2404.08263": {"title": "Relational Prompt-based Pre-trained Language Models for Social Event Detection", "link": "https://arxiv.org/abs/2404.08263", "description": "arXiv:2404.08263v2 Announce Type: replace-cross \nAbstract: Social Event Detection (SED) aims to identify significant events from social streams, and has a wide application ranging from public opinion analysis to risk management. In recent years, Graph Neural Network (GNN) based solutions have achieved state-of-the-art performance. However, GNN-based methods often struggle with missing and noisy edges between messages, affecting the quality of learned message embedding. Moreover, these methods statically initialize node embedding before training, which, in turn, limits the ability to learn from message texts and relations simultaneously. In this paper, we approach social event detection from a new perspective based on Pre-trained Language Models (PLMs), and present RPLM_SED (Relational prompt-based Pre-trained Language Models for Social Event Detection). We first propose a new pairwise message modeling strategy to construct social messages into message pairs with multi-relational sequences. Secondly, a new multi-relational prompt-based pairwise message learning mechanism is proposed to learn more comprehensive message representation from message pairs with multi-relational prompts using PLMs. Thirdly, we design a new clustering constraint to optimize the encoding process by enhancing intra-cluster compactness and inter-cluster dispersion, making the message representation more distinguishable. We evaluate the RPLM_SED on three real-world datasets, demonstrating that the RPLM_SED model achieves state-of-the-art performance in offline, online, low-resource, and long-tail distribution scenarios for social event detection tasks."}, "https://arxiv.org/abs/2409.06994": {"title": "Graph sub-sampling for divide-and-conquer algorithms in large networks", "link": "https://arxiv.org/abs/2409.06994", "description": "arXiv:2409.06994v1 Announce Type: new \nAbstract: As networks continue to increase in size, current methods must be capable of handling large numbers of nodes and edges in order to be practically relevant. Instead of working directly with the entire (large) network, analyzing sub-networks has become a popular approach. Due to a network's inherent inter-connectedness, sub-sampling is not a trivial task. While this problem has gained attention in recent years, it has not received sufficient attention from the statistics community. In this work, we provide a thorough comparison of seven graph sub-sampling algorithms by applying them to divide-and-conquer algorithms for community structure and core-periphery (CP) structure. After discussing the various algorithms and sub-sampling routines, we derive theoretical results for the mis-classification rate of the divide-and-conquer algorithm for CP structure under various sub-sampling schemes. We then perform extensive experiments on both simulated and real-world data to compare the various methods. For the community detection task, we found that sampling nodes uniformly at random yields the best performance. For CP structure on the other hand, there was no single winner, but algorithms which sampled core nodes at a higher rate consistently outperformed other sampling routines, e.g., random edge sampling and random walk sampling. The varying performance of the sampling algorithms on different tasks demonstrates the importance of carefully selecting a sub-sampling routine for the specific application."}, "https://arxiv.org/abs/2409.07057": {"title": "A Novel Voting System for Medical Catalogues in National Health Insurance", "link": "https://arxiv.org/abs/2409.07057", "description": "arXiv:2409.07057v1 Announce Type: new \nAbstract: This study explores the conceptual development of a medical insurance catalogue voting system. The methodology is centred on creating a model where doctors would vote on treatment inclusions, aiming to demonstrate transparency and integrity. The results from Monte Carlo simulations suggest a robust consensus on the selection of medicines and treatments. Further theoretical investigations propose incorporating a patient outcome-based incentive mechanism. This conceptual approach could enhance decision-making in healthcare by aligning stakeholder interests with patient outcomes, aiming for an optimised, equitable insurance catalogue with potential blockchain-based smart-contracts to ensure transparency and integrity."}, "https://arxiv.org/abs/2409.07102": {"title": "DisasterNeedFinder: Understanding the Information Needs in the 2024 Noto Earthquake (Comprehensive Explanation)", "link": "https://arxiv.org/abs/2409.07102", "description": "arXiv:2409.07102v1 Announce Type: new \nAbstract: We propose and demonstrate the DisasterNeedFinder framework in order to provide appropriate information support for the Noto Peninsula Earthquake. In the event of a large-scale disaster, it is essential to accurately capture the ever-changing information needs. However, it is difficult to obtain appropriate information from the chaotic situation on the ground. Therefore, as a data-driven approach, we aim to pick up precise information needs at the site by integrally analyzing the location information of disaster victims and search information. It is difficult to make a clear estimation of information needs by just analyzing search history information in disaster areas, due to the large amount of noise and the small number of users. Therefore, the idea of assuming that the magnitude of information needs is not the volume of searches, but the degree of abnormalities in searches, enables an appropriate understanding of the information needs of the disaster victims. DNF has been continuously clarifying the information needs of disaster areas since the disaster strike, and has been recognized as a new approach to support disaster areas by being featured in the major Japanese media on several occasions."}, "https://arxiv.org/abs/2409.06737": {"title": "Waste Heat and Habitability: Constraints from Technological Energy Consumption", "link": "https://arxiv.org/abs/2409.06737", "description": "arXiv:2409.06737v1 Announce Type: cross \nAbstract: Waste heat production represents an inevitable consequence of energy conversion as per the laws of thermodynamics. Based on this fact, by using simple theoretical models, we analyze constraints on the habitability of Earth-like terrestrial planets hosting putative technological species and technospheres characterized by persistent exponential growth of energy consumption and waste heat generation: in particular, we quantify the deleterious effects of rising surface temperature on biospheric processes and the eventual loss of liquid water. Irrespective of whether these sources of energy are ultimately stellar or planetary (e.g., nuclear, fossil fuels) in nature, we demonstrate that the loss of habitable conditions on such terrestrial planets may be expected to occur on timescales of $\\lesssim 1000$ years, as measured from the start of the exponential phase, provided that the annual growth rate of energy consumption is of order $1\\%$. We conclude by discussing the types of evolutionary trajectories that might be feasible for industrialized technological species, and sketch the ensuing implications for technosignature searches."}, "https://arxiv.org/abs/2409.06998": {"title": "Learning Personalized Scoping for Graph Neural Networks under Heterophily", "link": "https://arxiv.org/abs/2409.06998", "description": "arXiv:2409.06998v1 Announce Type: cross \nAbstract: Heterophilous graphs, where dissimilar nodes tend to connect, pose a challenge for graph neural networks (GNNs) as their superior performance typically comes from aggregating homophilous information. Increasing the GNN depth can expand the scope (i.e., receptive field), potentially finding homophily from the higher-order neighborhoods. However, uniformly expanding the scope results in subpar performance since real-world graphs often exhibit homophily disparity between nodes. An ideal way is personalized scopes, allowing nodes to have varying scope sizes. Existing methods typically add node-adaptive weights for each hop. Although expressive, they inevitably suffer from severe overfitting. To address this issue, we formalize personalized scoping as a separate scope classification problem that overcomes GNN overfitting in node classification. Specifically, we predict the optimal GNN depth for each node. Our theoretical and empirical analysis suggests that accurately predicting the depth can significantly enhance generalization. We further propose Adaptive Scope (AS), a lightweight MLP-based approach that only participates in GNN inference. AS encodes structural patterns and predicts the depth to select the best model for each node's prediction. Experimental results show that AS is highly flexible with various GNN architectures across a wide range of datasets while significantly improving accuracy."}, "https://arxiv.org/abs/2303.09274": {"title": "Random matching in balanced bipartite graphs: The (un)fairness of draw mechanisms used in sports", "link": "https://arxiv.org/abs/2303.09274", "description": "arXiv:2303.09274v4 Announce Type: replace \nAbstract: The draw of some knockout tournaments requires finding a perfect matching in a balanced bipartite graph. The problem becomes challenging with draw constraints: the two field-proven procedures used in sports are known to be non-uniformly distributed (the feasible matchings are not equally likely), which may threaten fairness. We compare the biases of both mechanisms, each of them having two forms, for reasonable subsets of balanced bipartite graphs up to 16 nodes. A mechanism is found to dominate all others in the draw of quarterfinals under reasonable restrictions. The UEFA Champions League Round of 16 draw is verified to apply the best design among the four available options between the 2003/04 and 2023/24 seasons. However, considerable scope remains to improve the performance of these randomisation procedures, especially because they tend to distort the probabilities in the same direction and roughly with the same magnitude."}, "https://arxiv.org/abs/2309.16455": {"title": "Signatures of criticality in turning avalanches of schooling fish", "link": "https://arxiv.org/abs/2309.16455", "description": "arXiv:2309.16455v3 Announce Type: replace-cross \nAbstract: Moving animal groups transmit information through propagating waves or behavioral cascades, exhibiting characteristics akin to systems near a critical point from statistical physics. Using data from freely swimming schooling fish in an experimental tank, we investigate spontaneous behavioral cascades involving turning avalanches, where large directional shifts propagate across the group. We analyze several avalanche metrics and provide a detailed picture of the dynamics associated to turning avalanches, employing tools from avalanche behavior in condensed matter physics and seismology. Our results identify power-law distributions and robust scale-free behaviour through data collapses and scaling relationships, confirming a necessary condition for criticality in fish schools. We explore the biological function of turning avalanches and link them to collective decision-making processes in selecting a new movement direction for the school. We report relevant boundary effects arising from interactions with the tank walls and influential roles of boundary individuals. Finally, spatial and temporal correlations in avalanches are explored using the concept of aftershocks from seismology, revealing clustering of avalanche events below a designated timescale and an Omori law with a faster decay rate than observed in earthquakes."}, "https://arxiv.org/abs/2311.04916": {"title": "Explainable Identification of Hate Speech towards Islam using Graph Neural Networks", "link": "https://arxiv.org/abs/2311.04916", "description": "arXiv:2311.04916v3 Announce Type: replace-cross \nAbstract: Islamophobic language on online platforms fosters intolerance, making detection and elimination crucial for promoting harmony. Traditional hate speech detection models rely on NLP techniques like tokenization, part-of-speech tagging, and encoder-decoder models. However, Graph Neural Networks (GNNs), with their ability to utilize relationships between data points, offer more effective detection and greater explainability. In this work, we represent speeches as nodes and connect them with edges based on their context and similarity to develop the graph. This study introduces a novel paradigm using GNNs to identify and explain hate speech towards Islam. Our model leverages GNNs to understand the context and patterns of hate speech by connecting texts via pretrained NLP-generated word embeddings, achieving state-of-the-art performance and enhancing detection accuracy while providing valuable explanations. This highlights the potential of GNNs in combating online hate speech and fostering a safer, more inclusive online environment."}, "https://arxiv.org/abs/2409.07492": {"title": "Analyzing fisher effort -- Gender differences and the impact of Covid-19", "link": "https://arxiv.org/abs/2409.07492", "description": "arXiv:2409.07492v1 Announce Type: new \nAbstract: Fishing is a valuable recreational activity in our society. To assess future fishing activity, identifying variables related to differences in fishing activity, such as gender or Covid-19, is helpful. We conducted a Canada-wide email survey of users of an online fishing platform and analyzed responses with a focus on gender, the impact of Covid-19, and variables directly related to fisher effort. Genders (90.1% male and 9.9% female respondents) significantly differed in demographics, socioeconomic status, and fishing skills but were similar in fishing preferences, fisher effort in terms of trip frequency, and travel distance. For almost half of the fishers, Covid-19 caused a change in trip frequency, determined by the activity level and gender of the fisher. A Bayesian network revealed that travel distance was the main determinant of trip frequency and negatively impacted the fishing activity of 61% of the fishers. Fisher effort was also directly related to fishing expertise. The study shows how online surveys and Bayesian networks can help understand the relationship between fishers' characteristics and activity and predict future fishing trends."}, "https://arxiv.org/abs/2409.07496": {"title": "The Status Quo of Architecture and Its Impact on Urban Management: Christopher Alexander's Insights", "link": "https://arxiv.org/abs/2409.07496", "description": "arXiv:2409.07496v1 Announce Type: new \nAbstract: Christopher Alexander offers a critical perspective on the modernist approach to architecture, which he argues has prioritized innovation, abstraction, and mechanistic efficiency at the expense of human-centered and organic values. This shift has led to the proliferation of buildings that, while visually striking, often feel cold, impersonal, and disconnected from the deeper needs of the people who inhabit them. Alexander advocates for a return to timeless architectural principles such as harmony, balance, and a deep connection to the natural and cultural context. He introduces the concept of living structure, which emphasizes creating spaces that resonate with the intrinsic order found in nature and human life, fostering environments that are not only functional and beautiful but also profoundly life-affirming. By challenging the dominance of \"iconic\" but alienating designs, Alexander calls for a holistic, human-centered approach to architecture-one that prioritizes the well-being of individuals and communities, creating spaces that nurture a sense of place, belonging, and harmony with the world around them."}, "https://arxiv.org/abs/2409.07498": {"title": "Structural Robustness and Vulnerability of Networks", "link": "https://arxiv.org/abs/2409.07498", "description": "arXiv:2409.07498v1 Announce Type: new \nAbstract: Networks are useful descriptions of the structure of many complex systems. Unsurprisingly, it is thus important to analyze the robustness of networks in many scientific disciplines. In applications in communication, logistics, finance, ecology, biomedicine, and many other fields, researchers have studied the robustness of networks to the removal of nodes, edges, or other subnetworks to identify and characterize robust network structures. A major challenge in the study of network robustness is that researchers have reported that different and seemingly contradictory network properties are correlated with a network's robustness. Using a framework by Alderson and Doyle~\\cite{Alderson2010}, we categorize several notions of network robustness and we examine these ostensible contradictions. We survey studies of network robustness with a focus on (1)~identifying robustness specifications in common use, (2)~understanding when these specifications are appropriate, and (3)~understanding the conditions under which one can expect different notions of robustness to yield similar results. With this review, we aim to give researchers an overview of the large, interdisciplinary body of work on network robustness and develop practical guidance for the design of computational experiments to study a network's robustness."}, "https://arxiv.org/abs/2409.07684": {"title": "Modeling Information Narrative Detection and Evolution on Telegram during the Russia-Ukraine War", "link": "https://arxiv.org/abs/2409.07684", "description": "arXiv:2409.07684v1 Announce Type: new \nAbstract: Following the Russian Federation's full-scale invasion of Ukraine in February 2022, a multitude of information narratives emerged within both pro-Russian and pro-Ukrainian communities online. As the conflict progresses, so too do the information narratives, constantly adapting and influencing local and global community perceptions and attitudes. This dynamic nature of the evolving information environment (IE) underscores a critical need to fully discern how narratives evolve and affect online communities. Existing research, however, often fails to capture information narrative evolution, overlooking both the fluid nature of narratives and the internal mechanisms that drive their evolution. Recognizing this, we introduce a novel approach designed to both model narrative evolution and uncover the underlying mechanisms driving them. In this work we perform a comparative discourse analysis across communities on Telegram covering the initial three months following the invasion. First, we uncover substantial disparities in narratives and perceptions between pro-Russian and pro-Ukrainian communities. Then, we probe deeper into prevalent narratives of each group, identifying key themes and examining the underlying mechanisms fueling their evolution. Finally, we explore influences and factors that may shape the development and spread of narratives."}, "https://arxiv.org/abs/2409.07710": {"title": "Surprising Resilience of Science During a Global Pandemic: A Large-Scale Descriptive Analysis", "link": "https://arxiv.org/abs/2409.07710", "description": "arXiv:2409.07710v1 Announce Type: new \nAbstract: The COVID-19 pandemic profoundly impacted people globally, yet its effect on scientists and research institutions has yet to be fully examined. To address this knowledge gap, we use a newly available bibliographic dataset covering tens of millions of papers and authors to investigate changes in research activity and collaboration during this period. Employing statistical methods, we analyze the pandemic's disruptions on the participation, productivity, and collaborations of researchers at the top 1,000 institutions worldwide based on historical productivity, taking into account variables such as geography, researcher seniority and gender, and field of study. Our findings reveal an unexpected trend: research activity and output significantly increased in the early stages of the pandemic, indicating a surprising resilience in the scientific community. However, by the end of 2022, there was a notable reversion to historical trends in research participation and productivity. This reversion suggests that the initial spike in research activity was a short-lived disruption rather than a permanent shift. As such, monitoring scientific outputs in 2023 and beyond becomes crucial. There may be a delayed negative effect of the pandemic on research, given the long time horizon for many research fields and the temporary closure of wet labs. Further analysis is needed to fully comprehend the factors that underpin the resilience of scientific innovation in the face of global crises. Our study provides an initial comprehensive exploration up to the end of 2022, offering valuable insights into how the scientific community has adapted and responded over the course of the pandemic."}, "https://arxiv.org/abs/2409.07712": {"title": "Virtual Node Generation for Node Classification in Sparsely-Labeled Graphs", "link": "https://arxiv.org/abs/2409.07712", "description": "arXiv:2409.07712v1 Announce Type: new \nAbstract: In the broader machine learning literature, data-generation methods demonstrate promising results by generating additional informative training examples via augmenting sparse labels. Such methods are less studied in graphs due to the intricate dependencies among nodes in complex topology structures. This paper presents a novel node generation method that infuses a small set of high-quality synthesized nodes into the graph as additional labeled nodes to optimally expand the propagation of labeled information. By simply infusing additional nodes, the framework is orthogonal to the graph learning and downstream classification techniques, and thus is compatible with most popular graph pre-training (self-supervised learning), semi-supervised learning, and meta-learning methods. The contribution lies in designing the generated node set by solving a novel optimization problem. The optimization places the generated nodes in a manner that: (1) minimizes the classification loss to guarantee training accuracy and (2) maximizes label propagation to low-confidence nodes in the downstream task to ensure high-quality propagation. Theoretically, we show that the above dual optimization maximizes the global confidence of node classification. Our Experiments demonstrate statistically significant performance improvements over 14 baselines on 10 publicly available datasets."}, "https://arxiv.org/abs/2409.07716": {"title": "Polarization Detection on Social Networks: dual contrastive objectives for Self-supervision", "link": "https://arxiv.org/abs/2409.07716", "description": "arXiv:2409.07716v1 Announce Type: new \nAbstract: Echo chambers and online discourses have become prevalent social phenomena where communities engage in dramatic intra-group confirmations and inter-group hostility. Polarization detection is a rising research topic for detecting and identifying such polarized groups. Previous works on polarization detection primarily focus on hand-crafted features derived from dataset-specific characteristics and prior knowledge, which fail to generalize to other datasets. This paper proposes a unified self-supervised polarization detection framework, outperforming previous methods in unsupervised and semi-supervised polarization detection tasks on various publicly available datasets. Our framework utilizes a dual contrastive objective (DocTra): (1) interaction-level: to contrast between node interactions to extract critical features on interaction patterns, and (2) feature-level: to contrast extracted polarized and invariant features to encourage feature decoupling. Our experiments extensively evaluate our methods again 7 baselines on 7 public datasets, demonstrating significant performance improvements."}, "https://arxiv.org/abs/2409.07718": {"title": "Unsupervised node clustering via contrastive hard sampling", "link": "https://arxiv.org/abs/2409.07718", "description": "arXiv:2409.07718v1 Announce Type: new \nAbstract: This paper introduces a fine-grained contrastive learning scheme for unsupervised node clustering. Previous clustering methods only focus on a small feature set (class-dependent features), which demonstrates explicit clustering characteristics, ignoring the rest of the feature spaces (class-invariant features). This paper exploits class-invariant features via graph contrastive learning to discover additional high-quality features for unsupervised clustering. We formulate a novel node-level fine-grained augmentation framework for self-supervised learning, which iteratively identifies competitive contrastive samples from the whole feature spaces, in the form of positive and negative examples of node relations. While positive examples of node relations are usually expressed as edges in graph homophily, negative examples are implicit without a direct edge. We show, however, that simply sampling nodes beyond the local neighborhood results in less competitive negative pairs, that are less effective for contrastive learning. Inspired by counterfactual augmentation, we instead sample competitive negative node relations by creating virtual nodes that inherit (in a self-supervised fashion) class-invariant features, while altering class-dependent features, creating contrasting pairs that lie closer to the boundary and offering better contrast. Consequently, our experiments demonstrate significant improvements in supervised node clustering tasks on six baselines and six real-world social network datasets."}, "https://arxiv.org/abs/2409.07720": {"title": "Keeping it Authentic: The Social Footprint of the Trolls Network", "link": "https://arxiv.org/abs/2409.07720", "description": "arXiv:2409.07720v1 Announce Type: new \nAbstract: In 2016, a network of social media accounts animated by Russian operatives attempted to divert political discourse within the American public around the presidential elections. This was a coordinated effort, part of a Russian-led complex information operation. Utilizing the anonymity and outreach of social media platforms Russian operatives created an online astroturf that is in direct contact with regular Americans, promoting Russian agenda and goals. The elusiveness of this type of adversarial approach rendered security agencies helpless, stressing the unique challenges this type of intervention presents. Building on existing scholarship on the functions within influence networks on social media, we suggest a new approach to map those types of operations. We argue that pretending to be legitimate social actors obliges the network to adhere to social expectations, leaving a social footprint. To test the robustness of this social footprint we train artificial intelligence to identify it and create a predictive model. We use Twitter data identified as part of the Russian influence network for training the artificial intelligence and to test the prediction. Our model attains 88% prediction accuracy for the test set. Testing our prediction on two additional models results in 90.7% and 90.5% accuracy, validating our model. The predictive and validation results suggest that building a machine learning model around social functions within the Russian influence network can be used to map its actors and functions."}, "https://arxiv.org/abs/2409.07733": {"title": "Self-similarity of temporal interaction networks arises from hyperbolic geometry with time-varying curvature", "link": "https://arxiv.org/abs/2409.07733", "description": "arXiv:2409.07733v1 Announce Type: new \nAbstract: The self-similarity of complex systems has been studied intensely across different domains due to its potential applications in system modeling, complexity analysis, etc., as well as for deep theoretical interest. Existing studies rely on scale transformations conceptualized over either a definite geometric structure of the system (very often realized as length-scale transformations) or purely temporal scale transformations. However, many physical and social systems are observed as temporal interactions among agents without any definitive geometry. Yet, one can imagine the existence of an underlying notion of distance as the interactions are mostly localized. Analysing only the time-scale transformations over such systems would uncover only a limited aspect of the complexity. In this work, we propose a novel technique of scale transformation that dissects temporal interaction networks under spatio-temporal scales, namely, flow scales. Upon experimenting with multiple social and biological interaction networks, we find that many of them possess a finite fractal dimension under flow-scale transformation. Finally, we relate the emergence of flow-scale self-similarity to the latent geometry of such networks. We observe strong evidence that justifies the assumption of an underlying, variable-curvature hyperbolic geometry that induces self-similarity of temporal interaction networks. Our work bears implications for modeling temporal interaction networks at different scales and uncovering their latent geometric structures."}, "https://arxiv.org/abs/2409.08016": {"title": "A review of the structure of street networks", "link": "https://arxiv.org/abs/2409.08016", "description": "arXiv:2409.08016v1 Announce Type: new \nAbstract: We review measures of street network structure proposed in the recent literature, establish their relevance to practice, and identify open challenges facing researchers. These measures' empirical values vary substantially across world regions and development eras, indicating street networks' geometric and topological heterogeneity."}, "https://arxiv.org/abs/2409.08106": {"title": "Hypergraph Change Point Detection using Adapted Cardinality-Based Gadgets: Applications in Dynamic Legal Structures", "link": "https://arxiv.org/abs/2409.08106", "description": "arXiv:2409.08106v1 Announce Type: new \nAbstract: Hypergraphs provide a robust framework for modeling complex systems with higher-order interactions. However, analyzing them in dynamic settings presents significant computational challenges. To address this, we introduce a novel method that adapts the cardinality-based gadget to convert hypergraphs into strongly connected weighted directed graphs, complemented by a symmetrized combinatorial Laplacian. We demonstrate that the harmonic mean of the conductance and edge expansion of the original hypergraph can be upper-bounded by the conductance of the transformed directed graph, effectively preserving crucial cut information. Additionally, we analyze how the resulting Laplacian relates to that derived from the star expansion. Our approach was validated through change point detection experiments on both synthetic and real datasets, showing superior performance over clique and star expansions in maintaining spectral information in dynamic settings. Finally, we applied our method to analyze a dynamic legal hypergraph constructed from extensive United States court opinion data."}, "https://arxiv.org/abs/2409.08135": {"title": "Reducing Population-level Inequality Can Improve Demographic Group Fairness: a Twitter Case Study", "link": "https://arxiv.org/abs/2409.08135", "description": "arXiv:2409.08135v1 Announce Type: new \nAbstract: Many existing fairness metrics measure group-wise demographic disparities in system behavior or model performance. Calculating these metrics requires access to demographic information, which, in industrial settings, is often unavailable. By contrast, economic inequality metrics, such as the Gini coefficient, require no demographic data to measure. However, reductions in economic inequality do not necessarily correspond to reductions in demographic disparities. In this paper, we empirically explore the relationship between demographic-free inequality metrics -- such as the Gini coefficient -- and standard demographic bias metrics that measure group-wise model performance disparities specifically in the case of engagement inequality on Twitter. We analyze tweets from 174K users over the duration of 2021 and find that demographic-free impression inequality metrics are positively correlated with gender, race, and age disparities in the average case, and weakly (but still positively) correlated with demographic bias in the worst case. We therefore recommend inequality metrics as a potentially useful proxy measure of average group-wise disparities, especially in cases where such disparities cannot be measured directly. Based on these results, we believe they can be used as part of broader efforts to improve fairness between demographic groups in scenarios like content recommendation on social media."}, "https://arxiv.org/abs/2409.07932": {"title": "Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies", "link": "https://arxiv.org/abs/2409.07932", "description": "arXiv:2409.07932v1 Announce Type: cross \nAbstract: Graph path search is a classic computer science problem that has been recently approached with Reinforcement Learning (RL) due to its potential to outperform prior methods. Existing RL techniques typically assume a global view of the network, which is not suitable for large-scale, dynamic, and privacy-sensitive settings. An area of particular interest is search in social networks due to its numerous applications. Inspired by seminal work in experimental sociology, which showed that decentralized yet efficient search is possible in social networks, we frame the problem as a collaborative task between multiple agents equipped with a limited local view of the network. We propose a multi-agent approach for graph path search that successfully leverages both homophily and structural heterogeneity. Our experiments, carried out over synthetic and real-world social networks, demonstrate that our model significantly outperforms learned and heuristic baselines. Furthermore, our results show that meaningful embeddings for graph navigation can be constructed using reward-driven learning."}, "https://arxiv.org/abs/2409.07956": {"title": "Community detection in multi-layer networks by regularized debiased spectral clustering", "link": "https://arxiv.org/abs/2409.07956", "description": "arXiv:2409.07956v1 Announce Type: cross \nAbstract: Community detection is a crucial problem in the analysis of multi-layer networks. In this work, we introduce a new method, called regularized debiased sum of squared adjacency matrices (RDSoS), to detect latent communities in multi-layer networks. RDSoS is developed based on a novel regularized Laplacian matrix that regularizes the debiased sum of squared adjacency matrices. In contrast, the classical regularized Laplacian matrix typically regularizes the adjacency matrix of a single-layer network. Therefore, at a high level, our regularized Laplacian matrix extends the classical regularized Laplacian matrix to multi-layer networks. We establish the consistency property of RDSoS under the multi-layer stochastic block model (MLSBM) and further extend RDSoS and its theoretical results to the degree-corrected version of the MLSBM model. The effectiveness of the proposed methods is evaluated and demonstrated through synthetic and real datasets."}, "https://arxiv.org/abs/2409.08104": {"title": "Designing a Collaborative Platform for Advancing Supply Chain Transparency", "link": "https://arxiv.org/abs/2409.08104", "description": "arXiv:2409.08104v1 Announce Type: cross \nAbstract: Enabling supply chain transparency (SCT) is essential for regulatory compliance and meeting sustainability standards. Multi-tier SCT plays a pivotal role in identifying and mitigating an organization's operational, environmental, and social (ESG) risks. While research observes increasing efforts towards SCT, a minority of companies are currently publishing supply chain information. Using the Design Science Research approach, we develop a collaborative platform for supply chain transparency. We derive design requirements, formulate design principles, and evaluate the artefact with industry experts. Our artefact is initialized with publicly available supply chain data through an automated pipeline designed to onboard future participants to our platform. This work contributes to SCT research by providing insights into the challenges and opportunities of implementing multi-tier SCT and offers a practical solution that encourages organizations to participate in a transparent ecosystem."}, "https://arxiv.org/abs/2404.03916": {"title": "Estimating mixed memberships in multi-layer networks", "link": "https://arxiv.org/abs/2404.03916", "description": "arXiv:2404.03916v2 Announce Type: replace \nAbstract: Community detection in multi-layer networks has emerged as a crucial area of modern network analysis. However, conventional approaches often assume that nodes belong exclusively to a single community, which fails to capture the complex structure of real-world networks where nodes may belong to multiple communities simultaneously. To address this limitation, we propose novel spectral methods to estimate the common mixed memberships in the multi-layer mixed membership stochastic block model. The proposed methods leverage the eigen-decomposition of three aggregate matrices: the sum of adjacency matrices, the debiased sum of squared adjacency matrices, and the sum of squared adjacency matrices. We establish rigorous theoretical guarantees for the consistency of our methods. Specifically, we derive per-node error rates under mild conditions on network sparsity, demonstrating their consistency as the number of nodes and/or layers increases under the multi-layer mixed membership stochastic block model. Our theoretical results reveal that the method leveraging the sum of adjacency matrices generally performs poorer than the other two methods for mixed membership estimation in multi-layer networks. We conduct extensive numerical experiments to empirically validate our theoretical findings. For real-world multi-layer networks with unknown community information, we introduce two novel modularity metrics to quantify the quality of mixed membership community detection. Finally, we demonstrate the practical applications of our algorithms and modularity metrics by applying them to real-world multi-layer networks, demonstrating their effectiveness in extracting meaningful community structures."}, "https://arxiv.org/abs/2312.17679": {"title": "Data Augmentation for Supervised Graph Outlier Detection with Latent Diffusion Models", "link": "https://arxiv.org/abs/2312.17679", "description": "arXiv:2312.17679v2 Announce Type: replace-cross \nAbstract: Graph outlier detection is a prominent task of research and application in the realm of graph neural networks. It identifies the outlier nodes that exhibit deviation from the majority in the graph. One of the fundamental challenges confronting supervised graph outlier detection algorithms is the prevalent issue of class imbalance, where the scarcity of outlier instances compared to normal instances often results in suboptimal performance. Conventional methods mitigate the imbalance by reweighting instances in the estimation of the loss function, assigning higher weights to outliers and lower weights to inliers. Nonetheless, these strategies are prone to overfitting and underfitting, respectively. Recently, generative models, especially diffusion models, have demonstrated their efficacy in synthesizing high-fidelity images. Despite their extraordinary generation quality, their potential in data augmentation for supervised graph outlier detection remains largely underexplored.\n  To bridge this gap, we introduce GODM, a novel data augmentation for mitigating class imbalance in supervised Graph Outlier detection with latent Diffusion Models. Specifically, our proposed method consists of three key components: (1) Variantioanl Encoder maps the heterogeneous information inherent within the graph data into a unified latent space. (2) Graph Generator synthesizes graph data that are statistically similar to real outliers from latent space, and (3) Latent Diffusion Model learns the latent space distribution of real organic data by iterative denoising. Extensive experiments conducted on multiple datasets substantiate the effectiveness and efficiency of GODM. The case study further demonstrated the generation quality of our synthetic data. To foster accessibility and reproducibility, we encapsulate GODM into a plug-and-play package and release it at the Python Package Index (PyPI)."}, "https://arxiv.org/abs/2409.08287": {"title": "Integrating En Route and Home Proximity in EV Charging Accessibility: A Spatial Analysis in the Washington Metropolitan Area", "link": "https://arxiv.org/abs/2409.08287", "description": "arXiv:2409.08287v1 Announce Type: new \nAbstract: This study evaluates the accessibility of public EV charging stations in the Washington metropolitan area using a comprehensive measure that accounts for both destination-based and en route charging opportunities. By incorporating the full spectrum of daily travel patterns into the accessibility evaluation, our methodology offers a more realistic measure of charging opportunities than destination-based methods that prioritize proximity to residential locations. Results from spatial autocorrelation analysis indicate that conventional accessibility assessments often overestimate the availability of infrastructure in central urban areas and underestimate it in peripheral commuting zones, potentially leading to misallocated resources. By highlighting significant clusters of high-access and low-access areas, our approach identifies spatial inequalities in infrastructure distribution and provides insights into areas requiring targeted interventions. This study underscores the importance of incorporating daily mobility patterns into urban planning to ensure equitable access to EV charging infrastructure and suggests a framework that other regions could adopt to enhance sustainable transportation networks and support equitable urban development."}, "https://arxiv.org/abs/2409.08291": {"title": "Typical daily profiles of PM concentrations in parisian underground railway stations", "link": "https://arxiv.org/abs/2409.08291", "description": "arXiv:2409.08291v1 Announce Type: new \nAbstract: To enhance the understanding of air quality within underground railway stations (URS), a methodology has been developed to establish a baseline profile of particle concentrations (PM10 and PM2.5). This approach incorporates an extensive data cleaning process based on the identification of URS operation periods, physically inconsistent or mathematically aberrant data, and comparing the profile of each day to an average profile. The versatility of this methodology allows its application to different particle classes within various URS. The results obtained from the three studied URS indicate the possibility of obtaining reliable daily typical profiles even over short measurement periods (up to one or two weeks)."}, "https://arxiv.org/abs/2409.08298": {"title": "Sustainability of Scale-Free Properties in Synchronizations of Dynamic Scale-Free Networks", "link": "https://arxiv.org/abs/2409.08298", "description": "arXiv:2409.08298v1 Announce Type: new \nAbstract: Scale-free networks are ubiquitous in social, biological and technological networked systems. Dynamic Scale-free networks and their synchronizations are important to understand and predict the behavior of social, biological and technological networked systems. In this research, computational experiments have been conducted to understand the sustainability of scale-free properties during the time of synchronizations in dynamic scale-free networks. Two synchronization phenomena which are synchronization based on states of nodes with coupling configuration matrix and synchronization based on states of nodes with network centralities have been implemented for the synchronization in dynamic scale-free networks. In experiments, dynamic scale-free networks have been generated with a network generation algorithm and analyzed to understand the fluctuation from the scale-free properties in their phases during the time of synchronizations."}, "https://arxiv.org/abs/2409.08304": {"title": "Resilient Infrastructure Network: Sparse Edge Change Identification via L1-Regularized Least Squares", "link": "https://arxiv.org/abs/2409.08304", "description": "arXiv:2409.08304v1 Announce Type: new \nAbstract: Adversarial actions and a rapid climate change are disrupting operations of infrastructure networks (e.g., energy, water, and transportation systems). Unaddressed disruptions lead to system-wide shutdowns, emphasizing the need for quick and robust identification methods. One significant disruption arises from edge changes (addition or deletion) in networks. We present an $\\ell_1$-norm regularized least-squares framework to identify multiple but sparse edge changes using noisy data. We focus only on networks that obey equilibrium equations, as commonly observed in the above sectors. The presence or lack of edges in these networks is captured by the sparsity pattern of the weighted, symmetric Laplacian matrix, while noisy data are node injections and potentials. Our proposed framework systematically leverages the inherent structure within the Laplacian matrix, effectively avoiding overparameterization. We demonstrate the robustness and efficacy of the proposed approach through a series of representative examples, with a primary emphasis on power networks."}, "https://arxiv.org/abs/2409.08305": {"title": "Mapping the Russian Internet Troll Network on Twitter using a Predictive Model", "link": "https://arxiv.org/abs/2409.08305", "description": "arXiv:2409.08305v1 Announce Type: new \nAbstract: Russian Internet Trolls use fake personas to spread disinformation through multiple social media streams. Given the increased frequency of this threat across social media platforms, understanding those operations is paramount in combating their influence. Using Twitter content identified as part of the Russian influence network, we created a predictive model to map the network operations. We classify accounts type based on their authenticity function for a sub-sample of accounts by introducing logical categories and training a predictive model to identify similar behavior patterns across the network. Our model attains 88% prediction accuracy for the test set. Validation is done by comparing the similarities with the 3 million Russian troll tweets dataset. The result indicates a 90.7% similarity between the two datasets. Furthermore, we compare our model predictions on a Russian tweets dataset, and the results state that there is 90.5% correspondence between the predictions and the actual categories. The prediction and validation results suggest that our predictive model can assist with mapping the actors in such networks."}, "https://arxiv.org/abs/2409.08349": {"title": "Scientific and technological knowledge grows linearly over time", "link": "https://arxiv.org/abs/2409.08349", "description": "arXiv:2409.08349v1 Announce Type: new \nAbstract: The past few centuries have witnessed a dramatic growth in scientific and technological knowledge. However, the nature of that growth - whether exponential or otherwise - remains controversial, perhaps partly due to the lack of quantitative characterizations. We evaluated knowledge as a collective thinking structure, using citation networks as a representation, by examining extensive datasets that include 213 million publications (1800-2020) and 7.6 million patents (1976-2020). We found that knowledge - which we conceptualize as the reduction of uncertainty in a knowledge network - grew linearly over time in naturally formed citation networks that themselves expanded exponentially. Moreover, our results revealed inflection points in the growth of knowledge that often corresponded to important developments within fields, such as major breakthroughs, new paradigms, or the emergence of entirely new areas of study. Around these inflection points, knowledge may grow rapidly or exponentially on a local scale, although the overall growth rate remains linear when viewed globally. Previous studies concluding an exponential growth of knowledge may have focused primarily on these local bursts of rapid growth around key developments, leading to the misconception of a global exponential trend. Our findings help to reconcile the discrepancy between the perceived exponential growth and the actual linear growth of knowledge by highlighting the distinction between local and global growth patterns. Overall, our findings reveal major science development trends for policymaking, showing that producing knowledge is far more challenging than producing papers."}, "https://arxiv.org/abs/2409.08360": {"title": "The Informal Labor of Content Creators: Situating Xiaohongshu's Key Opinion Consumers in Relationships to Marketers, Consumer Brands, and the Platform", "link": "https://arxiv.org/abs/2409.08360", "description": "arXiv:2409.08360v1 Announce Type: new \nAbstract: This paper critically examines flexible content creation conducted by Key Opinion Consumers (KOCs) on a prominent social media and e-commerce platform in China, Xiaohongshu (RED). Drawing on nine-month ethnographic work conducted online, we find that the production of the KOC role on RED is predicated on the interactions and negotiations among multiple stakeholders -- content creators, marketers, consumer brands (corporations), and the platform. KOCs are instrumental in RED influencer marketing tactics and amplify the mundane and daily life content popular on the platform. They navigate the dynamics in the triangulated relations with other stakeholders in order to secure economic opportunities for producing advertorial content, and yet, the labor involved in producing such content is deliberately obscured to make it appear as spontaneous, ordinary user posts for the sake of marketing campaigns. Meanwhile, the commercial value of their work is often underestimated and overshadowed in corporate paperwork, platform technological mechanisms, and business models, resulting in and reinforcing inadequate recognition and compensation of KOCs. We propose the concept of ``informal labor'' to offer a new lens to understand content creation labor that is indispensable yet unrecognized by the social media industry. We advocate for a contextualized and nuanced examination of how labor is valued and compensated and urge for better protections and working conditions for informal laborers like KOCs."}, "https://arxiv.org/abs/2409.08405": {"title": "Consistent Strong Triadic Closure in Multilayer Networks", "link": "https://arxiv.org/abs/2409.08405", "description": "arXiv:2409.08405v1 Announce Type: new \nAbstract: Social network users are commonly connected to hundreds or even thousands of other users. However, these ties are not all of equal strength; for example, we often are connected to good friends or family members as well as acquaintances. Inferring the tie strengths is an essential task in social network analysis. Common approaches classify the ties into strong and weak edges based on the network topology using the strong triadic closure (STC). The STC states that if for three nodes, $\\textit{A}$, $\\textit{B}$, and $\\textit{C}$, there are strong ties between $\\textit{A}$ and $\\textit{B}$, as well as $\\textit{A}$ and $\\textit{C}$, there has to be a (weak or strong) tie between $\\textit{B}$ and $\\textit{C}$. Moreover, a variant of the STC called STC+ allows adding new weak edges to obtain improved solutions. Recently, the focus of social network analysis has been shifting from single-layer to multilayer networks due to their ability to represent complex systems with multiple types of interactions or relationships in multiple social network platforms like Facebook, LinkedIn, or X (formerly Twitter). However, straightforwardly applying the STC separately to each layer of multilayer networks usually leads to inconsistent labelings between layers. Avoiding such inconsistencies is essential as they contradict the idea that tie strengths represent underlying, consistent truths about the relationships between users. Therefore, we adapt the definitions of the STC and STC+ for multilayer networks and provide ILP formulations to solve the problems exactly. Solving the ILPs is computationally costly; hence, we additionally provide an efficient 2-approximation for the STC and a 6-approximation for the STC+ minimization variants. The experiments show that, unlike standard approaches, our new highly efficient algorithms lead to consistent strong/weak labelings of the multilayer network edges."}, "https://arxiv.org/abs/2409.08459": {"title": "Toward satisfactory public accessibility: A crowdsourcing approach through online reviews to inclusive urban design", "link": "https://arxiv.org/abs/2409.08459", "description": "arXiv:2409.08459v1 Announce Type: new \nAbstract: As urban populations grow, the need for accessible urban design has become urgent. Traditional survey methods for assessing public perceptions of accessibility are often limited in scope. Crowdsourcing via online reviews offers a valuable alternative to understanding public perceptions, and advancements in large language models can facilitate their use. This study uses Google Maps reviews across the United States and fine-tunes Llama 3 model with the Low-Rank Adaptation technique to analyze public sentiment on accessibility. At the POI level, most categories -- restaurants, retail, hotels, and healthcare -- show negative sentiments. Socio-spatial analysis reveals that areas with higher proportions of white residents and greater socioeconomic status report more positive sentiment, while areas with more elderly, highly-educated residents exhibit more negative sentiment. Interestingly, no clear link is found between the presence of disabilities and public sentiments. Overall, this study highlights the potential of crowdsourcing for identifying accessibility challenges and providing insights for urban planners."}, "https://arxiv.org/abs/2409.08522": {"title": "MAPX: An explainable model-agnostic framework for the detection of false information on social media networks", "link": "https://arxiv.org/abs/2409.08522", "description": "arXiv:2409.08522v1 Announce Type: new \nAbstract: The automated detection of false information has become a fundamental task in combating the spread of \"fake news\" on online social media networks (OSMN) as it reduces the need for manual discernment by individuals. In the literature, leveraging various content or context features of OSMN documents have been found useful. However, most of the existing detection models often utilise these features in isolation without regard to the temporal and dynamic changes oft-seen in reality, thus, limiting the robustness of the models. Furthermore, there has been little to no consideration of the impact of the quality of documents' features on the trustworthiness of the final prediction. In this paper, we introduce a novel model-agnostic framework, called MAPX, which allows evidence based aggregation of predictions from existing models in an explainable manner. Indeed, the developed aggregation method is adaptive, dynamic and considers the quality of OSMN document features. Further, we perform extensive experiments on benchmarked fake news datasets to demonstrate the effectiveness of MAPX using various real-world data quality scenarios. Our empirical results show that the proposed framework consistently outperforms all state-of-the-art models evaluated. For reproducibility, a demo of MAPX is available at \\href{https://github.com/SCondran/MAPX_framework}{this link}"}, "https://arxiv.org/abs/2409.08599": {"title": "Estimation of Graph Features Based on Random Walks Using Neighbors' Properties", "link": "https://arxiv.org/abs/2409.08599", "description": "arXiv:2409.08599v1 Announce Type: new \nAbstract: Using random walks for sampling has proven advantageous in assessing the characteristics of large and unknown social networks. Several algorithms based on random walks have been introduced in recent years. In the practical application of social network sampling, there is a recurrent reliance on an application programming interface (API) for obtaining adjacent nodes. However, owing to constraints related to query frequency and associated API expenses, it is preferable to minimize API calls during the feature estimation process. In this study, considering the acquisition of neighboring nodes as a cost factor, we introduce a feature estimation algorithm that outperforms existing algorithms in terms of accuracy. Through experiments that simulate sampling on known graphs, we demonstrate the superior accuracy of our proposed algorithm when compared to existing alternatives."}, "https://arxiv.org/abs/2409.08631": {"title": "Sybil Detection using Graph Neural Networks", "link": "https://arxiv.org/abs/2409.08631", "description": "arXiv:2409.08631v1 Announce Type: new \nAbstract: This paper presents SYBILGAT, a novel approach to Sybil detection in social networks using Graph Attention Networks (GATs). Traditional methods for Sybil detection primarily leverage structural properties of networks; however, they tend to struggle with a large number of attack edges and are often unable to simultaneously utilize both known Sybil and honest nodes. Our proposed method addresses these limitations by dynamically assigning attention weights to different nodes during aggregations, enhancing detection performance. We conducted extensive experiments in various scenarios, including pretraining in sampled subgraphs, synthetic networks, and networks under targeted attacks. The results show that SYBILGAT significantly outperforms the state-of-the-art algorithms, particularly in scenarios with high attack complexity and when the number of attack edges increases. Our approach shows robust performance across different network models and sizes, even as the detection task becomes more challenging. We successfully applied the model to a real-world Twitter graph with more than 269k nodes and 6.8M edges. The flexibility and generalizability of SYBILGAT make it a promising tool to defend against Sybil attacks in online social networks with only structural information."}, "https://arxiv.org/abs/2409.08690": {"title": "Generating Temporal Contact Graphs Using Random Walkers", "link": "https://arxiv.org/abs/2409.08690", "description": "arXiv:2409.08690v1 Announce Type: new \nAbstract: We study human mobility networks through timeseries of contacts between individuals. Our proposed Random Walkers Induced temporal Graph (RWIG) model generates temporal graph sequences based on independent random walkers that traverse an underlying graph in discrete time steps. Co-location of walkers at a given node and time defines an individual-level contact. RWIG is shown to be a realistic model for temporal human contact graphs, which may place RWIG on a same footing as the Erdos-Renyi (ER) and Barabasi-Albert (BA) models for fixed graphs. Moreover, RWIG is analytically feasible: we derive closed form solutions for the probability distribution of contact graphs."}, "https://arxiv.org/abs/2409.08717": {"title": "Fusing Dynamics Equation: A Social Opinions Prediction Algorithm with LLM-based Agents", "link": "https://arxiv.org/abs/2409.08717", "description": "arXiv:2409.08717v1 Announce Type: new \nAbstract: In the context where social media is increasingly becoming a significant platform for social movements and the formation of public opinion, accurately simulating and predicting the dynamics of user opinions is of great importance for understanding social phenomena, policy making, and guiding public opinion. However, existing simulation methods face challenges in capturing the complexity and dynamics of user behavior. Addressing this issue, this paper proposes an innovative simulation method for the dynamics of social media user opinions, the FDE-LLM algorithm, which incorporates opinion dynamics and epidemic model. This effectively constrains the actions and opinion evolution process of large language models (LLM), making them more aligned with the real cyber world. In particular, the FDE-LLM categorizes users into opinion leaders and followers. Opinion leaders are based on LLM role-playing and are constrained by the CA model, while opinion followers are integrated into a dynamic system that combines the CA model with the SIR model. This innovative design significantly improves the accuracy and efficiency of the simulation. Experiments were conducted on four real Weibo datasets and validated using the open-source model ChatGLM. The results show that, compared to traditional agent-based modeling (ABM) opinion dynamics algorithms and LLM-based opinion diffusion algorithms, our FDE-LLM algorithm demonstrates higher accuracy and interpretability."}, "https://arxiv.org/abs/2409.08781": {"title": "Community-based fact-checking reduces the spread of misleading posts on social media", "link": "https://arxiv.org/abs/2409.08781", "description": "arXiv:2409.08781v1 Announce Type: new \nAbstract: Community-based fact-checking is a promising approach to verify social media content and correct misleading posts at scale. Yet, causal evidence regarding its effectiveness in reducing the spread of misinformation on social media is missing. Here, we performed a large-scale empirical study to analyze whether community notes reduce the spread of misleading posts on X. Using a Difference-in-Differences design and repost time series data for N=237,677 (community fact-checked) cascades that had been reposted more than 431 million times, we found that exposing users to community notes reduced the spread of misleading posts by, on average, 62.0%. Furthermore, community notes increased the odds that users delete their misleading posts by 103.4%. However, our findings also suggest that community notes might be too slow to intervene in the early (and most viral) stage of the diffusion. Our work offers important implications to enhance the effectiveness of community-based fact-checking approaches on social media."}, "https://arxiv.org/abs/2409.08803": {"title": "A Review on Flood Risk Conceptual Frameworks and Development of Hierarchical Structures for Assessment Criteria", "link": "https://arxiv.org/abs/2409.08803", "description": "arXiv:2409.08803v1 Announce Type: new \nAbstract: Climate change and rapid urbanization have led to more frequent and severe flooding, causing significant damage. The existing literature on flood risk encompasses a variety of dimensions, such as physical, economic, social, political, environmental, infrastructural, and managerial aspects. This paper aims to provide an extensive review of proposed conceptual frameworks and their components used in flood risk assessment. For this purpose, Initially, conceptual frameworks were extracted to configure the components of flood risk including hazard, vulnerability, exposure, resilience, and susceptibility. Subsequently, a comprehensive set of criteria from the literature were identified, addressing risk components. In this paper, the risk conceptual framework is defined by the intersection of vulnerability and hazard. Vulnerability, shaped by exposure and susceptibility, can be reduced by enhancing resiliency, which includes coping and adaptive capacities. In total, 102 criteria/subcriteria were identified and classified into three hierarchical structures of hazard, susceptibility, and resilience. Finally, flood risk assessment methods were reviewed, with an emphasis on their applicability and characteristics. The review highlighted the strengths and limitations of various methods, providing a comprehensive overview of their suitability for different scenarios. The outcomes of this review could serve as a valuable reference for professionals involved in flood risk assessment, aiding in the identification of the most appropriate risk concepts, assessment criteria, and suitable methods for quantification based on the specific study area and data availability."}, "https://arxiv.org/abs/2409.08829": {"title": "Community Fact-Checks Trigger Moral Outrage in Replies to Misleading Posts on Social Media", "link": "https://arxiv.org/abs/2409.08829", "description": "arXiv:2409.08829v1 Announce Type: new \nAbstract: Displaying community fact-checks is a promising approach to reduce engagement with misinformation on social media. However, how users respond to misleading content emotionally after community fact-checks are displayed on posts is unclear. Here, we employ quasi-experimental methods to causally analyze changes in sentiments and (moral) emotions in replies to misleading posts following the display of community fact-checks. Our evaluation is based on a large-scale panel dataset comprising N=2,225,260 replies across 1841 source posts from X's Community Notes platform. We find that informing users about falsehoods through community fact-checks significantly increases negativity (by 7.3%), anger (by 13.2%), disgust (by 4.7%), and moral outrage (by 16.0%) in the corresponding replies. These results indicate that users perceive spreading misinformation as a violation of social norms and that those who spread misinformation should expect negative reactions once their content is debunked. We derive important implications for the design of community-based fact-checking systems."}, "https://arxiv.org/abs/2409.08944": {"title": "Unveiling User Engagement Patterns on Stack Exchange Through Network Analysis", "link": "https://arxiv.org/abs/2409.08944", "description": "arXiv:2409.08944v1 Announce Type: new \nAbstract: Stack Exchange, a question-and-answer(Q&amp;A) platform, has exhibited signs of a declining user engagement. This paper investigates user engagement dynamics across various Stack Exchange communities including Data science, AI, software engineering, project management, and GenAI. We propose a network graph representing users as nodes and their interactions as edges. We explore engagement patterns through key network metrics including Degree Centerality, Betweenness Centrality, and PageRank. The study findings reveal distinct community dynamics across these platforms, with smaller communities demonstrating more concentrated user influence, while larger platforms showcase more distributed engagement. Besides, the results showed insights into user roles, influence, and potential strategies for enhancing engagement. This research contributes to understanding of online community behavior and provides a framework for future studies to improve the Stack Exchange user experience."}, "https://arxiv.org/abs/2409.08966": {"title": "User Identity Linkage on Social Networks: A Review of Modern Techniques and Applications", "link": "https://arxiv.org/abs/2409.08966", "description": "arXiv:2409.08966v1 Announce Type: new \nAbstract: In an Online Social Network (OSN), users can create a unique public persona by crafting a user identity that may encompass profile details, content, and network-related information. As a result, a relevant task of interest is related to the ability to link identities across different OSNs. Linking users across social networks can have multiple implications in several contexts both at the individual level and at the group level. At the individual level, the main interest in linking the same identity across social networks is to enable a better knowledge of each user. At the group level, linking user identities through different OSNs helps in predicting user behaviors, network dynamics, information diffusion, and migration phenomena across social media. The process of tying together user accounts on different OSNs is challenging and has attracted more and more research attention in the last fifteen years. The purpose of this work is to provide a comprehensive review of recent studies (from 2016 to the present) on User Identity Linkage (UIL) methods across online social networks. This review aims to offer guidance for other researchers in the field by outlining the main problem formulations, the different feature extraction strategies, algorithms, machine learning models, datasets, and evaluation metrics proposed by researchers working in this area. The proposed overview takes a pragmatic perspective to highlight the concrete possibilities for accomplishing this task depending on the type of available data."}, "https://arxiv.org/abs/2409.08975": {"title": "Accurate and Fast Estimation of Temporal Motifs using Path Sampling", "link": "https://arxiv.org/abs/2409.08975", "description": "arXiv:2409.08975v1 Announce Type: new \nAbstract: Counting the number of small subgraphs, called motifs, is a fundamental problem in social network analysis and graph mining. Many real-world networks are directed and temporal, where edges have timestamps. Motif counting in directed, temporal graphs is especially challenging because there are a plethora of different kinds of patterns. Temporal motif counts reveal much richer information and there is a need for scalable algorithms for motif counting.\n  A major challenge in counting is that there can be trillions of temporal motif matches even with a graph with only millions of vertices. Both the motifs and the input graphs can have multiple edges between two vertices, leading to a combinatorial explosion problem. Counting temporal motifs involving just four vertices is not feasible with current state-of-the-art algorithms.\n  We design an algorithm, TEACUPS, that addresses this problem using a novel technique of temporal path sampling. We combine a path sampling method with carefully designed temporal data structures, to propose an efficient approximate algorithm for temporal motif counting. TEACUPS is an unbiased estimator with provable concentration behavior, which can be used to bound the estimation error. For a Bitcoin graph with hundreds of millions of edges, TEACUPS runs in less than 1 minute, while the exact counting algorithm takes more than a day. We empirically demonstrate the accuracy of TEACUPS on large datasets, showing an average of 30$\\times$ speedup (up to 2000$\\times$ speedup) compared to existing GPU-based exact counting methods while preserving high count estimation accuracy."}, "https://arxiv.org/abs/2409.08473": {"title": "Stark Decline in Journalists' Use of Preprints Post-pandemic", "link": "https://arxiv.org/abs/2409.08473", "description": "arXiv:2409.08473v1 Announce Type: cross \nAbstract: The COVID-19 pandemic accelerated the use of preprints, aiding rapid research dissemination but also facilitating the spread of misinformation. This study analyzes media coverage of preprints from 2014 to 2023, revealing a significant post-pandemic decline. Our findings suggest that heightened awareness of the risks associated with preprints has led to more cautious media practices. While the decline in preprint coverage may mitigate concerns about premature media exposure, it also raises questions about the future role of preprints in science communication, especially during emergencies. Balanced policies based on up-to-date evidence are needed to address this shift."}, "https://arxiv.org/abs/2409.08736": {"title": "Bifurcations in the Kuramoto model with external forcing and higher-order interactions", "link": "https://arxiv.org/abs/2409.08736", "description": "arXiv:2409.08736v1 Announce Type: cross \nAbstract: Synchronization is an important phenomenon in a wide variety of systems comprising interacting oscillatory units, whether natural (like neurons, biochemical reactions, cardiac cells) or artificial (like metronomes, power grids, Josephson junctions). The Kuramoto model provides a simple description of these systems and has been useful in their mathematical exploration. Here we investigate this model in the presence of two characteristics that may be important in applications: an external periodic influence and higher-order interactions among the units. The combination of these ingredients leads to a very rich bifurcation scenario in the dynamics of the order parameter that describes phase transitions. Our theoretical calculations are validated by numerical simulations."}, "https://arxiv.org/abs/2409.08946": {"title": "DELTA: Dual Consistency Delving with Topological Uncertainty for Active Graph Domain Adaptation", "link": "https://arxiv.org/abs/2409.08946", "description": "arXiv:2409.08946v1 Announce Type: cross \nAbstract: Graph domain adaptation has recently enabled knowledge transfer across different graphs. However, without the semantic information on target graphs, the performance on target graphs is still far from satisfactory. To address the issue, we study the problem of active graph domain adaptation, which selects a small quantitative of informative nodes on the target graph for extra annotation. This problem is highly challenging due to the complicated topological relationships and the distribution discrepancy across graphs. In this paper, we propose a novel approach named Dual Consistency Delving with Topological Uncertainty (DELTA) for active graph domain adaptation. Our DELTA consists of an edge-oriented graph subnetwork and a path-oriented graph subnetwork, which can explore topological semantics from complementary perspectives. In particular, our edge-oriented graph subnetwork utilizes the message passing mechanism to learn neighborhood information, while our path-oriented graph subnetwork explores high-order relationships from substructures. To jointly learn from two subnetworks, we roughly select informative candidate nodes with the consideration of consistency across two subnetworks. Then, we aggregate local semantics from its K-hop subgraph based on node degrees for topological uncertainty estimation. To overcome potential distribution shifts, we compare target nodes and their corresponding source nodes for discrepancy scores as an additional component for fine selection. Extensive experiments on benchmark datasets demonstrate that DELTA outperforms various state-of-the-art approaches."}, "https://arxiv.org/abs/2409.08963": {"title": "Safeguarding Decentralized Social Media: LLM Agents for Automating Community Rule Compliance", "link": "https://arxiv.org/abs/2409.08963", "description": "arXiv:2409.08963v1 Announce Type: cross \nAbstract: Ensuring content compliance with community guidelines is crucial for maintaining healthy online social environments. However, traditional human-based compliance checking struggles with scaling due to the increasing volume of user-generated content and a limited number of moderators. Recent advancements in Natural Language Understanding demonstrated by Large Language Models unlock new opportunities for automated content compliance verification. This work evaluates six AI-agents built on Open-LLMs for automated rule compliance checking in Decentralized Social Networks, a challenging environment due to heterogeneous community scopes and rules. Analyzing over 50,000 posts from hundreds of Mastodon servers, we find that AI-agents effectively detect non-compliant content, grasp linguistic subtleties, and adapt to diverse community contexts. Most agents also show high inter-rater reliability and consistency in score justification and suggestions for compliance. Human-based evaluation with domain experts confirmed the agents' reliability and usefulness, rendering them promising tools for semi-automated or human-in-the-loop content moderation systems."}, "https://arxiv.org/abs/2409.09001": {"title": "E2MoCase: A Dataset for Emotional, Event and Moral Observations in News Articles on High-impact Legal Cases", "link": "https://arxiv.org/abs/2409.09001", "description": "arXiv:2409.09001v1 Announce Type: cross \nAbstract: The way media reports on legal cases can significantly shape public opinion, often embedding subtle biases that influence societal views on justice and morality. Analyzing these biases requires a holistic approach that captures the emotional tone, moral framing, and specific events within the narratives. In this work we introduce E2MoCase, a novel dataset designed to facilitate the integrated analysis of emotions, moral values, and events within legal narratives and media coverage. By leveraging advanced models for emotion detection, moral value identification, and event extraction, E2MoCase offers a multi-dimensional perspective on how legal cases are portrayed in news articles."}, "https://arxiv.org/abs/2205.12719": {"title": "Social network heterogeneity benefits individuals at the expense of groups in the creation of innovation", "link": "https://arxiv.org/abs/2205.12719", "description": "arXiv:2205.12719v2 Announce Type: replace \nAbstract: Innovation is fundamental for development and provides a competitive advantage for societies. It is the process of creating more complex technologies, ideas, or protocols from existing ones. While innovation may be created by single agents (i.e. individuals or organisations), it is often a result of social interactions between agents exchanging and combining complementary expertise and perspectives. The structure of social networks impacts this knowledge exchange process. To study the role of social network structures on the creation of new technologies, we design an evolutionary mechanistic model combining self-creation and social learning. We find that social heterogeneity allows agents to leverage the benefits of diversity and to develop technologies of higher complexity. Social heterogeneity, however, reduces the group ability to innovate. Not only the social structure but also the openness of agents to collaborate affect innovation. We find that interdisciplinary interactions lead to more complex technologies benefiting the entire group but also increase the inequality in the innovation output. Lower openness to interdisciplinary collaborations may be compensated by a higher ability to collaborate with multiple peers, but low openness also neutralises the intrinsic benefits of network heterogeneity. Our findings indicate that social network heterogeneity has contrasting effects on microscopic (local) and macroscopic (group) levels, suggesting that the emergence of innovation leaders may suppress the overall group performance."}, "https://arxiv.org/abs/2310.08821": {"title": "Is Fact-Checking Politically Neutral? Asymmetries in How U", "link": "https://arxiv.org/abs/2310.08821", "description": "arXiv:2310.08821v2 Announce Type: replace \nAbstract: Political elites play an important role in the proliferation of online misinformation. However, an understanding of how fact-checking platforms pick up politicized misinformation for fact-checking is still in its infancy. Here, we conduct an empirical analysis of mentions of U.S. political elites within fact-checked statements. For this purpose, we collect a comprehensive dataset consisting of 35,014 true and false statements that have been fact-checked by two major fact-checking organizations (Snopes, PolitiFact) in the U.S. between 2008 and 2023, i.e., within an observation period of 15 years. Subsequently, we perform content analysis and explanatory regression modeling to analyze how veracity is linked to mentions of U.S. political elites in fact-checked statements. Our analysis yields the following main findings: (i) Fact-checked false statements are, on average, 20% more likely to mention political elites than true fact-checked statements. (ii) There is a partisan asymmetry such that fact-checked false statements are 88.1% more likely to mention Democrats, but 26.5% less likely to mention Republicans, compared to fact-checked true statements. (iii) Mentions of political elites in fact-checked false statements reach the highest level during the months preceding elections. (iv) Fact-checked false statements that mention political elites carry stronger other-condemning emotions and are more likely to be pro-Republican, compared to fact-checked true statements. In sum, our study offers new insights into understanding mentions of political elites in false statements on U.S. fact-checking platforms, and bridges important findings at the intersection between misinformation and politicization."}, "https://arxiv.org/abs/2311.07127": {"title": "Multi-agent Attacks for Black-box Social Recommendations", "link": "https://arxiv.org/abs/2311.07127", "description": "arXiv:2311.07127v3 Announce Type: replace \nAbstract: The rise of online social networks has facilitated the evolution of social recommender systems, which incorporate social relations to enhance users' decision-making process. With the great success of Graph Neural Networks (GNNs) in learning node representations, GNN-based social recommendations have been widely studied to model user-item interactions and user-user social relations simultaneously. Despite their great successes, recent studies have shown that these advanced recommender systems are highly vulnerable to adversarial attacks, in which attackers can inject well-designed fake user profiles to disrupt recommendation performances. While most existing studies mainly focus on argeted attacks to promote target items on vanilla recommender systems, untargeted attacks to degrade the overall prediction performance are less explored on social recommendations under a black-box scenario. To perform untargeted attacks on social recommender systems, attackers can construct malicious social relationships for fake users to enhance the attack performance. However, the coordination of social relations and item profiles is challenging for attacking black-box social recommendations. To address this limitation, we first conduct several preliminary studies to demonstrate the effectiveness of cross-community connections and cold-start items in degrading recommendations performance. Specifically, we propose a novel framework MultiAttack based on multi-agent reinforcement learning to coordinate the generation of cold-start item profiles and cross-community social relations for conducting untargeted attacks on black-box social recommendations. Comprehensive experiments on various real-world datasets demonstrate the effectiveness of our proposed attacking framework under the black-box setting."}, "https://arxiv.org/abs/2403.18868": {"title": "A recommender network perspective on the informational value of critics and crowds", "link": "https://arxiv.org/abs/2403.18868", "description": "arXiv:2403.18868v2 Announce Type: replace \nAbstract: How do the ratings of critics and amateurs compare and how should they be combined? Previous research has produced mixed results about the first question, while the second remains unanswered. We have created a new, unique dataset, with wine ratings from critics and amateurs, and simulated a recommender system using the k-nearest-neighbor algorithm. We then formalized the advice seeking network spanned by that algorithm and studied people's relative influence. We find that critics are more consistent than amateurs, and thus their advice is more predictive than advice from amateurs. Getting advice from both groups can further boost performance. Our network theoretic approach allows us to identify influential critics, talented amateurs, and the information flow between groups. Our results provide evidence about the informational function of critics, while our framework is broadly applicable and can be leveraged to devise good decision strategies and more transparent recommender systems."}, "https://arxiv.org/abs/2409.09338": {"title": "What you say or how you say it? Predicting Conflict Outcomes in Real and LLM-Generated Conversations", "link": "https://arxiv.org/abs/2409.09338", "description": "arXiv:2409.09338v1 Announce Type: new \nAbstract: When conflicts escalate, is it due to what is said or how it is said? In the conflict literature, two theoretical approaches take opposing views: one focuses on the content of the disagreement, while the other focuses on how it is expressed. This paper aims to integrate these two perspectives through a computational analysis of 191 communication features -- 128 related to expression and 63 to content. We analyze 1,200 GPT-4 simulated conversations and 12,630 real-world discussions from Reddit. We find that expression features more reliably predict destructive conflict outcomes across both settings, although the most important features differ. In the Reddit data, conversational dynamics such as turn-taking and conversational equality are highly predictive, but they are not predictive in simulated conversations. These results may suggest a possible limitation in simulating social interactions with language models, and we discuss the implications for our findings on building social computing systems."}, "https://arxiv.org/abs/2409.09701": {"title": "Costly punishment sustains indirect reciprocity under low defection detectability", "link": "https://arxiv.org/abs/2409.09701", "description": "arXiv:2409.09701v1 Announce Type: new \nAbstract: Cooperation is fundamental to human societies, and indirect reciprocity, where individuals cooperate to build a positive reputation for future benefits, plays a key role in promoting it. Previous theoretical and experimental studies have explored both the effectiveness and limitations of costly punishment in sustaining cooperation. While empirical observations show that costly punishment by third parties is common, some theoretical models suggest it may not be effective in the context of indirect reciprocity, raising doubts about its potential to enhance cooperation. In this study, we theoretically investigate the conditions under which costly punishment is effective. Building on a previous model, we introduce a new type of error in perceiving actions, where defection may be mistakenly perceived as cooperation. This extension models a realistic scenario where defectors have a strong incentive to disguise their defection as cooperation. Our analysis reveals that when defection is difficult to detect, norms involving costly punishment can emerge as the most efficient evolutionarily stable strategies. These findings demonstrate that costly punishment can play a crucial role in promoting cooperation within indirect reciprocity."}, "https://arxiv.org/abs/2409.09817": {"title": "Social Influence and Consensus Building: Introducing a q-Voter Model with Weighted Influence", "link": "https://arxiv.org/abs/2409.09817", "description": "arXiv:2409.09817v1 Announce Type: new \nAbstract: We investigate a dynamical model of opinion formation in which an individual's opinion is influenced by interactions with a group of other agents. We introduce a bias towards one of the opinions in a manner not considered earlier to the best of our knowledge. When the bias is neutral, the model is reduced to a mean-field voter model. We analyze the behavior and steady states of the system, identifying three distinct regimes based on the bias level: one favoring negative opinions, one favoring positive opinions, and a neutral case. In large systems, the equilibrium properties become independent of the size of the group, indicating that only the bias influences the final outcome. However, for small groups, the time to reach equilibrium depends on the size of the group. Our results show that even a small initial bias leads to a consensus where all agents eventually share the same opinion when the bias is not neutral. The system exhibits universal behavior, with critical slowing down occurring near the neutral bias point, marking it as a critical dynamical threshold. The time required to reach consensus scales logarithmically when the bias is non-neutral and linearly when it is neutral. Although short-term dynamics depends on group size for small groups, long-term behavior is governed solely by the bias."}, "https://arxiv.org/abs/2409.10160": {"title": "Efficient Network Embedding by Approximate Equitable Partitions", "link": "https://arxiv.org/abs/2409.10160", "description": "arXiv:2409.10160v1 Announce Type: new \nAbstract: Structural network embedding is a crucial step in enabling effective downstream tasks for complex systems that aims to project a network into a lower-dimensional space while preserving similarities among nodes. We introduce a simple and efficient embedding technique based on approximate variants of equitable partitions. The approximation consists in introducing a user-tunable tolerance parameter relaxing the otherwise strict condition for exact equitable partitions that can be hardly found in real-world networks. We exploit a relationship between equitable partitions and equivalence relations for Markov chains and ordinary differential equations to develop a partition refinement algorithm for computing an approximate equitable partition in polynomial time. We compare our method against state-of-the-art embedding techniques on benchmark networks. We report comparable -- when not superior -- performance for visualization, classification, and regression tasks at a cost between one and three orders of magnitude smaller using a prototype implementation, enabling the embedding of large-scale networks which could not be efficiently handled by most of the competing techniques."}, "https://arxiv.org/abs/2409.10244": {"title": "ES-KT-24: A Multimodal Knowledge Tracing Benchmark Dataset with Educational Game Playing Video and Synthetic Text Generation", "link": "https://arxiv.org/abs/2409.10244", "description": "arXiv:2409.10244v1 Announce Type: new \nAbstract: This paper introduces ES-KT-24, a novel multimodal Knowledge Tracing (KT) dataset for intelligent tutoring systems in educational game contexts. Although KT is crucial in adaptive learning, existing datasets often lack game-based and multimodal elements. ES-KT-24 addresses these limitations by incorporating educational game-playing videos, synthetically generated question text, and detailed game logs. The dataset covers Mathematics, English, Indonesian, and Malaysian subjects, emphasizing diversity and including non-English content. The synthetic text component, generated using a large language model, encompasses 28 distinct knowledge concepts and 182 questions, featuring 15,032 users and 7,782,928 interactions. Our benchmark experiments demonstrate the dataset's utility for KT research by comparing Deep learning-based KT models with Language Model-based Knowledge Tracing (LKT) approaches. Notably, LKT models showed slightly higher performance than traditional DKT models, highlighting the potential of language model-based approaches in this field. Furthermore, ES-KT-24 has the potential to significantly advance research in multimodal KT models and learning analytics. By integrating game-playing videos and detailed game logs, this dataset offers a unique approach to dissecting student learning patterns through advanced data analysis and machine-learning techniques. It has the potential to unearth new insights into the learning process and inspire further exploration in the field."}, "https://arxiv.org/abs/2409.09157": {"title": "Exact solution for a discrete-time SIR model", "link": "https://arxiv.org/abs/2409.09157", "description": "arXiv:2409.09157v1 Announce Type: cross \nAbstract: We propose a nonstandard finite difference scheme for the Susceptible-Infected-Removed (SIR) continuous model. We prove that our discretized system is dynamically consistent with its continuous counterpart and we derive its exact solution. We end with the analysis of the long-term behavior of susceptible, infected and removed individuals, illustrating our results with examples. In contrast with the SIR discrete-time model available in the literature, our new model is simultaneously mathematically and biologically sound."}, "https://arxiv.org/abs/2409.09888": {"title": "Flexible Diffusion Scopes with Parameterized Laplacian for Heterophilic Graph Learning", "link": "https://arxiv.org/abs/2409.09888", "description": "arXiv:2409.09888v1 Announce Type: cross \nAbstract: The ability of Graph Neural Networks (GNNs) to capture long-range and global topology information is limited by the scope of conventional graph Laplacian, leading to unsatisfactory performance on some datasets, particularly on heterophilic graphs. To address this limitation, we propose a new class of parameterized Laplacian matrices, which provably offers more flexibility in controlling the diffusion distance between nodes than the conventional graph Laplacian, allowing long-range information to be adaptively captured through diffusion on graph. Specifically, we first prove that the diffusion distance and spectral distance on graph have an order-preserving relationship. With this result, we demonstrate that the parameterized Laplacian can accelerate the diffusion of long-range information, and the parameters in the Laplacian enable flexibility of the diffusion scopes. Based on the theoretical results, we propose topology-guided rewiring mechanism to capture helpful long-range neighborhood information for heterophilic graphs. With this mechanism and the new Laplacian, we propose two GNNs with flexible diffusion scopes: namely the Parameterized Diffusion based Graph Convolutional Networks (PD-GCN) and Graph Attention Networks (PD-GAT). Synthetic experiments reveal the high correlations between the parameters of the new Laplacian and the performance of parameterized GNNs under various graph homophily levels, which verifies that our new proposed GNNs indeed have the ability to adjust the parameters to adaptively capture the global information for different levels of heterophilic graphs. They also outperform the state-of-the-art (SOTA) models on 6 out of 7 real-world benchmark datasets, which further confirms their superiority."}, "https://arxiv.org/abs/2409.09892": {"title": "Dynamic Fraud Detection: Integrating Reinforcement Learning into Graph Neural Networks", "link": "https://arxiv.org/abs/2409.09892", "description": "arXiv:2409.09892v1 Announce Type: cross \nAbstract: Financial fraud refers to the act of obtaining financial benefits through dishonest means. Such behavior not only disrupts the order of the financial market but also harms economic and social development and breeds other illegal and criminal activities. With the popularization of the internet and online payment methods, many fraudulent activities and money laundering behaviors in life have shifted from offline to online, posing a great challenge to regulatory authorities. How to efficiently detect these financial fraud activities has become an urgent issue that needs to be resolved. Graph neural networks are a type of deep learning model that can utilize the interactive relationships within graph structures, and they have been widely applied in the field of fraud detection. However, there are still some issues. First, fraudulent activities only account for a very small part of transaction transfers, leading to an inevitable problem of label imbalance in fraud detection. At the same time, fraudsters often disguise their behavior, which can have a negative impact on the final prediction results. In addition, existing research has overlooked the importance of balancing neighbor information and central node information. For example, when the central node has too many neighbors, the features of the central node itself are often neglected. Finally, fraud activities and patterns are constantly changing over time, so considering the dynamic evolution of graph edge relationships is also very important."}, "https://arxiv.org/abs/2409.09945": {"title": "Tracking the spatial dynamics of the synthetic opioid crisis in the USA, 2013-2020 using human mobility-based graph neural network", "link": "https://arxiv.org/abs/2409.09945", "description": "arXiv:2409.09945v1 Announce Type: cross \nAbstract: Synthetic opioids are the most common drugs involved in drug-involved overdose mortalities in the U.S. The Center for Disease Control and Prevention reported that in 2018, about 70% of all drug overdose deaths involved opioids and 67% of all opioid-involved deaths were accounted for by synthetic opioids. In this study, we investigated the spread of synthetic opioids between 2013 and 2020 in the U.S., and analyzed the relationship between the spatiotemporal pattern of synthetic opioid-involved deaths and another key opioid, heroin, and compared patterns of deaths involving these two types of drugs during this time period. Spatial connections between counties were incorporated into a graph convolutional neural network model to represent and analyze the spread of synthetic opioid-involved deaths, and in the context of heroin-involved deaths."}, "https://arxiv.org/abs/2409.10263": {"title": "Hierarchical Graph Pooling Based on Minimum Description Length", "link": "https://arxiv.org/abs/2409.10263", "description": "arXiv:2409.10263v1 Announce Type: cross \nAbstract: Graph pooling is an essential part of deep graph representation learning. We introduce MapEqPool, a principled pooling operator that takes the inherent hierarchical structure of real-world graphs into account. MapEqPool builds on the map equation, an information-theoretic objective function for community detection based on the minimum description length principle which naturally implements Occam's razor and balances between model complexity and fit. We demonstrate MapEqPool's competitive performance with an empirical comparison against various baselines across standard graph classification datasets."}, "https://arxiv.org/abs/2409.10340": {"title": "Hyperedge Modeling in Hypergraph Neural Networks by using Densest Overlapping Subgraphs", "link": "https://arxiv.org/abs/2409.10340", "description": "arXiv:2409.10340v1 Announce Type: cross \nAbstract: Hypergraphs tackle the limitations of traditional graphs by introducing {\\em hyperedges}. While graph edges connect only two nodes, hyperedges connect an arbitrary number of nodes along their edges. Also, the underlying message-passing mechanisms in Hypergraph Neural Networks (HGNNs) are in the form of vertex-hyperedge-vertex, which let HGNNs capture and utilize richer and more complex structural information than traditional Graph Neural Networks (GNNs). More recently, the idea of overlapping subgraphs has emerged. These subgraphs can capture more information about subgroups of vertices without limiting one vertex belonging to just one group, allowing vertices to belong to multiple groups or subgraphs. In addition, one of the most important problems in graph clustering is to find densest overlapping subgraphs (DOS). In this paper, we propose a solution to the DOS problem via Agglomerative Greedy Enumeration (DOSAGE) algorithm as a novel approach to enhance the process of generating the densest overlapping subgraphs and, hence, a robust construction of the hypergraphs. Experiments on standard benchmarks show that the DOSAGE algorithm significantly outperforms the HGNNs and six other methods on the node classification task."}, "https://arxiv.org/abs/2409.10402": {"title": "A Statistical Equilibrium Approach to Adam Smith's Labor Theory of Value", "link": "https://arxiv.org/abs/2409.10402", "description": "arXiv:2409.10402v1 Announce Type: cross \nAbstract: Adam Smith's inquiry into the emergence and stability of the self-organization of the division of labor in commodity production and exchange is considered using statistical equilibrium methods from statistical physics. We develop a statistical equilibrium model of the distribution of independent direct producers in a hub-and-spoke framework that predicts both the center of gravity of producers across lines of production as well as the endogenous fluctuations between lines of production that arise from Smith's concept of \"perfect liberty\". The ergodic distribution of producers implies a long-run balancing of \"advantages to disadvantages\" across lines of employment and gravitation of market prices around Smith's natural prices."}, "https://arxiv.org/abs/2409.10452": {"title": "Signed Graph Autoencoder for Explainable and Polarization-Aware Network Embeddings", "link": "https://arxiv.org/abs/2409.10452", "description": "arXiv:2409.10452v1 Announce Type: cross \nAbstract: Autoencoders based on Graph Neural Networks (GNNs) have garnered significant attention in recent years for their ability to extract informative latent representations, characterizing the structure of complex topologies, such as graphs. Despite the prevalence of Graph Autoencoders, there has been limited focus on developing and evaluating explainable neural-based graph generative models specifically designed for signed networks. To address this gap, we propose the Signed Graph Archetypal Autoencoder (SGAAE) framework. SGAAE extracts node-level representations that express node memberships over distinct extreme profiles, referred to as archetypes, within the network. This is achieved by projecting the graph onto a learned polytope, which governs its polarization. The framework employs a recently proposed likelihood for analyzing signed networks based on the Skellam distribution, combined with relational archetypal analysis and GNNs. Our experimental evaluation demonstrates the SGAAEs' capability to successfully infer node memberships over the different underlying latent structures while extracting competing communities formed through the participation of the opposing views in the network. Additionally, we introduce the 2-level network polarization problem and show how SGAAE is able to characterize such a setting. The proposed model achieves high performance in different tasks of signed link prediction across four real-world datasets, outperforming several baseline models."}, "https://arxiv.org/abs/2409.10456": {"title": "Mean Residual Life Ageing Intensity Function", "link": "https://arxiv.org/abs/2409.10456", "description": "arXiv:2409.10456v1 Announce Type: cross \nAbstract: The ageing intensity function is a powerful analytical tool that provides valuable insights into the ageing process across diverse domains such as reliability engineering, actuarial science, and healthcare. Its applications continue to expand as researchers delve deeper into understanding the complex dynamics of ageing and its implications for society. One common approach to defining the ageing intensity function is through the hazard rate or failure rate function, extensively explored in scholarly literature. Equally significant to the hazard rate function is the mean residual life function, which plays a crucial role in analyzing the ageing patterns exhibited by units or components. This article introduces the mean residual life ageing intensity (MRLAI) function to delve into component ageing behaviours across various distributions. Additionally, we scrutinize the closure properties of the MRLAI function across different reliability operations. Furthermore, a new order termed the mean residual life ageing intensity order is defined to analyze the ageing behaviour of a system, and the closure property of this order under various reliability operations is discussed."}, "https://arxiv.org/abs/2409.10475": {"title": "Leadership and Engagement Dynamics in Legislative Twitter Networks: Statistical Analysis and Modeling", "link": "https://arxiv.org/abs/2409.10475", "description": "arXiv:2409.10475v1 Announce Type: cross \nAbstract: In this manuscript, we analyze the interaction network on Twitter among members of the 117th U.S. Congress to assess the visibility of political leaders and explore how systemic properties and node attributes influence the formation of legislative connections. We employ descriptive social network statistical methods, the exponential random graph model (ERGM), and the stochastic block model (SBM) to evaluate the relative impact of network systemic properties, as well as institutional and personal traits, on the generation of online relationships among legislators. Our findings reveal that legislative networks on social media platforms like Twitter tend to reinforce the leadership of dominant political actors rather than diminishing their influence. However, we identify that these leadership roles can manifest in various forms. Additionally, we highlight that online connections within legislative networks are influenced by both the systemic properties of the network and institutional characteristics."}, "https://arxiv.org/abs/1901.05149": {"title": "Beyond Uniform Reverse Sampling: A Hybrid Sampling Technique for Misinformation Prevention", "link": "https://arxiv.org/abs/1901.05149", "description": "arXiv:1901.05149v3 Announce Type: replace \nAbstract: Online misinformation has been considered as one of the top global risks as it may cause serious consequences such as economic damages and public panic. The misinformation prevention problem aims at generating a positive cascade with appropriate seed nodes in order to compete against the misinformation. In this paper, we study the misinformation prevention problem under the prominent independent cascade model. Due to the #P-hardness in computing influence, the core problem is to design effective sampling methods to estimate the function value. The main contribution of this paper is a novel sampling method. Different from the classic reverse sampling technique which treats all nodes equally and samples the node uniformly, the proposed method proceeds with a hybrid sampling process which is able to attach high weights to the users who are prone to be affected by the misinformation. Consequently, the new sampling method is more powerful in generating effective samples used for computing seed nodes for the positive cascade. Based on the new hybrid sample technique, we design an algorithm offering a $(1-1/e-\\epsilon)$-approximation. We experimentally evaluate the proposed method on extensive datasets and show that it significantly outperforms the state-of-the-art solutions."}, "https://arxiv.org/abs/2211.16229": {"title": "Triadic Temporal Exponential Random Graph Models (TTERGM)", "link": "https://arxiv.org/abs/2211.16229", "description": "arXiv:2211.16229v2 Announce Type: replace \nAbstract: Temporal exponential random graph models (TERGM) are powerful statistical models that can be used to infer the temporal pattern of edge formation and elimination in complex networks (e.g., social networks). TERGMs can also be used in a generative capacity to predict longitudinal time series data in these evolving graphs. However, parameter estimation within this framework fails to capture many real-world properties of social networks, including: triadic relationships, small world characteristics, and social learning theories which could be used to constrain the probabilistic estimation of dyadic covariates. Here, we propose triadic temporal exponential random graph models (TTERGM) to fill this void, which includes these hierarchical network relationships within the graph model. We represent social network learning theory as an additional probability distribution that optimizes Markov chains in the graph vector space. The new parameters are then approximated via Monte Carlo maximum likelihood estimation. We show that our TTERGM model achieves improved fidelity and more accurate predictions compared to several benchmark methods on GitHub network data."}, "https://arxiv.org/abs/2308.15556": {"title": "Polarized Speech on Online Platforms in the American Political Context", "link": "https://arxiv.org/abs/2308.15556", "description": "arXiv:2308.15556v3 Announce Type: replace \nAbstract: Using language models, we analyze approximately 2.5 billion comments from Reddit and Twitter across 1.7 million accounts spanning 2007 to 2023 to study the prevalence and evolution of polarized rhetoric in American political discourse. Our findings show rising outgroup polarization on both platforms, with each new cohort of users displaying higher polarization than the last, and a small fraction of users contributing disproportionately to negative polarization. Polarization varies consistently by topic: right-leaning users are twice as likely to use polarized rhetoric around immigration, while left-leaning users are more polarized on healthcare. On Twitter, U.S. politicians on the left have been consistently more polarized than everyday users, but in the past four years, politicians on the right have experienced the sharpest rise in polarization, surpassing journalists, media, and everyday users. Today, politicians, the group listened to the most for their political rhetoric, are far more polarized than everyday users. On Reddit, a few communities dominate polarized discussions, with the rise and eventual ban of r/TheDonald significantly shaping polarization trends on the right. Our large-scale analysis reveals previously unknown patterns of polarization across groups and topics on two major social media platforms."}, "https://arxiv.org/abs/2311.17912": {"title": "Beyond Signal and Noise: Unraveling Scale Invariance in Neuroscience and Financial Networks with Topological Data Analysis", "link": "https://arxiv.org/abs/2311.17912", "description": "arXiv:2311.17912v2 Announce Type: replace \nAbstract: Topological Data Analysis (TDA) is increasingly crucial in investigating the shape of complex data structures across scientific fields, particularly in neuroscience and finance. This study delves into persistent homology, a TDA component initially aimed at differentiating between signal and noise. We explore two methodologies: the conventional cycle length approach and the novel death-birth ratio method proposed by Bobrowski and Skraba. Analyzing rs-fMRI data from the Human Connectome Project and daily $S\\&amp;P 500$ financial networks, our study compares these methods in identifying significant cycles. A key discovery is a robust relationship between z-score thresholds applied to bar lengths or ratios and behavioural traits in brain networks and market volatility in financial networks. In the brain, this is evident in the strong correlation between significant 1-cycles, brain volumes, and sex-based differences. In financial markets, a fractal pattern emerges, where market volatility negatively correlates with the number of significant cycles, indicating that more complex market topologies are associated with increased stability. Our findings also imply a fractal nature of 1-cycles at both population levels and across multiple days in the stock market. The distribution of significant loops, marked by high z-scores, remains consistent across various z-score thresholds, revealing a scale-invariant, fractal structure in both data sets. Given the scale invariance in these fractal structures, the traditional TDA distinction between signal and noise becomes less meaningful. This suggests that all scales of cycle length are relevant, challenging the conventional approach of segregating signal from noise and broadening the scope of TDA to reveal intricate, scale-invariant relationships in complex systems."}, "https://arxiv.org/abs/2402.06451": {"title": "Competitive and Weighted Evolving Simplicial Complexes", "link": "https://arxiv.org/abs/2402.06451", "description": "arXiv:2402.06451v2 Announce Type: replace \nAbstract: A simplex-based network is referred to as a higher-order network, in which describe that the interactions can include more than two nodes. Many multicomponent interactions can be grasped through simplicial complexes, which have recently found applications in social, technological, and biological contexts. The paper first proposes a competitive evolving model of higher-order networks. We introduce the difference equation analysis approach in the high-order network to make the analysis network more rigorous. It avoids the assumption that the degrees of nodes are continuous in the traditional analysis network. We obtain an analytical expression for the distribution of higher-order degrees by employing the theory of Poisson processes. The established results indicate that in a d-order network the scale-free behavior for the (d-1)-dim simplex with respect to the d-order degree is controlled by the competitiveness factor. As the competitiveness increases, the d-order degree of the (d-1)-dim simplex is bent under the logarithmic coordinates. While the e(<d-1)-dim simplex with respect to the d-order degree exhibits scale-free behavior. Second, by considering the weight changes of the neighboring simplices, as triggered by the selected simplex, a new weighted evolving model in higher-order networks is proposed. The results of the competitive evolving model of higher-order networks are used to analyze the weighted evolving model so that obtained are the analytical expressions of the higher-order degree distribution and higher-order strength density function of weighted higher-order networks. The outcomes of the simulation experiments are consistent with the theoretical analysis."}, "https://arxiv.org/abs/2402.19042": {"title": "H$_2$ and CO$_2$ Network Strategies for the European Energy System", "link": "https://arxiv.org/abs/2402.19042", "description": "arXiv:2402.19042v2 Announce Type: replace \nAbstract: Hydrogen and carbon dioxide transport can both play an essential role in climate-neutral energy systems. Hydrogen networks help serve regions with high energy demand, while excess emissions are transported away in carbon dioxide networks. For the synthesis of carbonaceous fuels, it is less clear which input should be transported: hydrogen to carbon point sources or carbon to low-cost hydrogen. We explore both networks' potential synergies and competition in a cost-optimal carbon-neutral European energy system. In a direct comparison, a hydrogen network is more cost-effective than a carbon network, as it serves to transport hydrogen to demand and to point source of carbon for utilization. However, in a hybrid scenario where both networks are present, the carbon network effectively complements the hydrogen network, promoting carbon capture from distributed biomass and reducing reliance on direct air capture. The layouts of the hydrogen and carbon dioxide networks are robust if the climate target is tightened to be net-negative."}, "https://arxiv.org/abs/2312.14668": {"title": "Emergence of vertical diversity under disturbance", "link": "https://arxiv.org/abs/2312.14668", "description": "arXiv:2312.14668v4 Announce Type: replace-cross \nAbstract: We propose a statistical physics model of a neutral community, where each agent can represent identical plant species growing in the vertical direction with sunlight in the form of rich-get-richer competition. Disturbance added to this ecosystem, which makes an agent restart from the lowest growth level, is realized as a stochastic resetting. We show that in this model for sufficiently strong competition, vertical diversity characterized by a family of Hill numbers robustly emerges as a local maximum at intermediate disturbance."}, "https://arxiv.org/abs/2409.10754": {"title": "Impact Of Emotions on Information Seeking And Sharing Behaviors During Pandemic", "link": "https://arxiv.org/abs/2409.10754", "description": "arXiv:2409.10754v1 Announce Type: new \nAbstract: We propose a novel approach to assess the public's coping behavior during the COVID-19 outbreak by examining the emotions. Specifically, we explore (1) changes in the public's emotions with the COVID-19 crisis progression and (2) the impacts of the public's emotions on their information-seeking, information-sharing behaviors, and compliance with stay-at-home policies. We base the study on the appraisal tendency framework, detect the public's emotions by fine-tuning a pre-trained RoBERTa model, and cross-analyze third-party behavioral data. We demonstrate the feasibility and reliability of our proposed approach in providing a large-scale examination of the publi's emotions and coping behaviors in a real-world crisis: COVID-19. The approach complements prior crisis communication research, mainly based on self-reported, small-scale experiments and survey data. Our results show that anger and fear are more prominent than other emotions experienced by the public at the pandemic's outbreak stage. Results also show that the extent of low certainty and passive emotions (e.g., sadness, fear) was related to increased information-seeking and information-sharing behaviors. Additionally, high-certainty (e.g., anger) and low-certainty (e.g., sadness, fear) emotions during the outbreak correlated to the public's compliance with stay-at-home orders."}, "https://arxiv.org/abs/2409.10809": {"title": "Consensus in Models for Opinion Dynamics with Generalized-Bias", "link": "https://arxiv.org/abs/2409.10809", "description": "arXiv:2409.10809v1 Announce Type: new \nAbstract: Interest is growing in social learning models where users share opinions and adjust their beliefs in response to others. This paper introduces generalized-bias opinion models, an extension of the DeGroot model, that captures a broader range of cognitive biases. These models can capture, among others, dynamic (changing) influences as well as ingroup favoritism and out-group hostility, a bias where agents may react differently to opinions from members of their own group compared to those from outside. The reactions are formalized as arbitrary functions that depend, not only on opinion difference, but also on the particular opinions of the individuals interacting. Under certain reasonable conditions, all agents (despite their biases) will converge to a consensus if the influence graph is strongly connected, as in the original DeGroot model. The proposed approach combines different biases, providing deeper insights into the mechanics of opinion dynamics and influence within social networks."}, "https://arxiv.org/abs/2409.10949": {"title": "Inside Alameda Research: A Multi-Token Network Analysis", "link": "https://arxiv.org/abs/2409.10949", "description": "arXiv:2409.10949v1 Announce Type: new \nAbstract: We analyze the token transfer network on Ethereum, focusing on accounts associated with Alameda Research, a cryptocurrency trading firm implicated in the misuse of FTX customer funds. Using a multi-token network representation, we examine node centralities and the network backbone to identify critical accounts, tokens, and activity groups. The temporal evolution of Alameda accounts reveals shifts in token accumulation and distribution patterns leading up to its bankruptcy in November 2022. Through network analysis, our work offers insights into the activities and dynamics that shape the DeFi ecosystem."}, "https://arxiv.org/abs/2409.11099": {"title": "Unveiling the Social Fabric: A Temporal, Nation-Scale Social Network and its Characteristics", "link": "https://arxiv.org/abs/2409.11099", "description": "arXiv:2409.11099v1 Announce Type: new \nAbstract: Social networks shape individuals' lives, influencing everything from career paths to health. This paper presents a registry-based, multi-layer and temporal network of the entire Danish population in the years 2008-2021 (roughly 7.2 mill. individuals). Our network maps the relationships formed through family, households, neighborhoods, colleagues and classmates. We outline key properties of this multiplex network, introducing both an individual-focused perspective as well as a bipartite representation. We show how to aggregate and combine the layers, and how to efficiently compute network measures such as shortest paths in large administrative networks. Our analysis reveals how past connections reappear later in other layers, that the number of relationships aggregated over time reflects the position in the income distribution, and that we can recover canonical shortest path length distributions when appropriately weighting connections. Along with the network data, we release a Python package that uses the bipartite network representation for efficient analysis."}, "https://arxiv.org/abs/2409.10670": {"title": "Let's Influence Algorithms Together: How Millions of Fans Build Collective Understanding of Algorithms and Organize Coordinated Algorithmic Actions", "link": "https://arxiv.org/abs/2409.10670", "description": "arXiv:2409.10670v1 Announce Type: cross \nAbstract: Previous research pays attention to how users strategically understand and consciously interact with algorithms but mainly focuses on an individual level, making it difficult to explore how users within communities could develop a collective understanding of algorithms and organize collective algorithmic actions. Through a two-year ethnography of online fan activities, this study investigates 43 core fans who always organize large-scale fans' collective actions and their corresponding general fan groups. This study aims to reveal how these core fans mobilize millions of general fans through collective algorithmic actions. These core fans reported the rhetorical strategies used to persuade general fans, the steps taken to build a collective understanding of algorithms, and the collaborative processes that adapt collective actions across platforms and cultures. Our findings highlight the key factors that enable computer-supported collective algorithmic actions and extend collective action research into large-scale domain targeting algorithms."}, "https://arxiv.org/abs/2409.10760": {"title": "Semantics Preserving Emoji Recommendation with Large Language Models", "link": "https://arxiv.org/abs/2409.10760", "description": "arXiv:2409.10760v1 Announce Type: cross \nAbstract: Emojis have become an integral part of digital communication, enriching text by conveying emotions, tone, and intent. Existing emoji recommendation methods are primarily evaluated based on their ability to match the exact emoji a user chooses in the original text. However, they ignore the essence of users' behavior on social media in that each text can correspond to multiple reasonable emojis. To better assess a model's ability to align with such real-world emoji usage, we propose a new semantics preserving evaluation framework for emoji recommendation, which measures a model's ability to recommend emojis that maintain the semantic consistency with the user's text. To evaluate how well a model preserves semantics, we assess whether the predicted affective state, demographic profile, and attitudinal stance of the user remain unchanged. If these attributes are preserved, we consider the recommended emojis to have maintained the original semantics. The advanced abilities of Large Language Models (LLMs) in understanding and generating nuanced, contextually relevant output make them well-suited for handling the complexities of semantics preserving emoji recommendation. To this end, we construct a comprehensive benchmark to systematically assess the performance of six proprietary and open-source LLMs using different prompting techniques on our task. Our experiments demonstrate that GPT-4o outperforms other LLMs, achieving a semantics preservation score of 79.23%. Additionally, we conduct case studies to analyze model biases in downstream classification tasks and evaluate the diversity of the recommended emojis."}, "https://arxiv.org/abs/2409.11170": {"title": "Capturing Differences in Character Representations Between Communities: An Initial Study with Fandom", "link": "https://arxiv.org/abs/2409.11170", "description": "arXiv:2409.11170v1 Announce Type: cross \nAbstract: Sociolinguistic theories have highlighted how narratives are often retold, co-constructed and reconceptualized in collaborative settings. This working paper focuses on the re-interpretation of characters, an integral part of the narrative story-world, and attempts to study how this may be computationally compared between online communities. Using online fandom - a highly communal phenomenon that has been largely studied qualitatively - as data, computational methods were applied to explore shifts in character representations between two communities and the original text. Specifically, text from the Harry Potter novels, r/HarryPotter subreddit, and fanfiction on Archive of Our Own were analyzed for changes in character mentions, centrality measures from co-occurrence networks, and semantic associations. While fandom elevates secondary characters as found in past work, the two fan communities prioritize different subsets of characters. Word embedding tests reveal starkly different associations of the same characters between communities on the gendered concepts of femininity/masculinity, cruelty, and beauty. Furthermore, fanfiction descriptions of a male character analyzed between romance pairings scored higher for feminine-coded characteristics in male-male romance, matching past qualitative theorizing. The results high-light the potential for computational methods to assist in capturing the re-conceptualization of narrative elements across communities and in supporting qualitative research on fandom."}, "https://arxiv.org/abs/2409.11389": {"title": "Normalization in Proportional Feature Spaces", "link": "https://arxiv.org/abs/2409.11389", "description": "arXiv:2409.11389v1 Announce Type: cross \nAbstract: The subject of features normalization plays an important central role in data representation, characterization, visualization, analysis, comparison, classification, and modeling, as it can substantially influence and be influenced by all of these activities and respective aspects. The selection of an appropriate normalization method needs to take into account the type and characteristics of the involved features, the methods to be used subsequently for the just mentioned data processing, as well as the specific questions being considered. After briefly considering how normalization constitutes one of the many interrelated parts typically involved in data analysis and modeling, the present work addressed the important issue of feature normalization from the perspective of uniform and proportional (right skewed) features and comparison operations. More general right skewed features are also considered in an approximated manner. Several concepts, properties, and results are described and discussed, including the description of a duality relationship between uniform and proportional feature spaces and respective comparisons, specifying conditions for consistency between comparisons in each of the two domains. Two normalization possibilities based on non-centralized dispersion of features are also presented, and also described is a modified version of the Jaccard similarity index which incorporates intrinsically normalization. Preliminary experiments are presented in order to illustrate the developed concepts and methods."}, "https://arxiv.org/abs/2304.14971": {"title": "Popularity Ratio Maximization: Surpassing Competitors through Influence Propagation", "link": "https://arxiv.org/abs/2304.14971", "description": "arXiv:2304.14971v2 Announce Type: replace \nAbstract: In this paper, we present an algorithmic study on how to surpass competitors in popularity by strategic promotions in social networks. We first propose a novel model, in which we integrate the Preferential Attachment (PA) model for popularity growth with the Independent Cascade (IC) model for influence propagation in social networks called PA-IC model. In PA-IC, a popular item and a novice item grab shares of popularity from the natural popularity growth via the PA model, while the novice item tries to gain extra popularity via influence cascade in a social network. The popularity ratio is defined as the ratio of the popularity measure between the novice item and the popular item. We formulate Popularity Ratio Maximization (PRM) as the problem of selecting seeds in multiple rounds to maximize the popularity ratio in the end. We analyze the popularity ratio and show that it is monotone but not submodular. To provide an effective solution, we devise a surrogate objective function and show that empirically it is very close to the original objective function while theoretically, it is monotone and submodular. We design two efficient algorithms, one for the overlapping influence and non-overlapping seeds (across rounds) setting and the other for the non-overlapping influence and overlapping seed setting, and further discuss how to deal with other models and problem variants. Our empirical evaluation further demonstrates that the proposed PRM-IMM method consistently achieves the best popularity promotion compared to other methods. Our theoretical and empirical analyses shed light on the interplay between influence maximization and preferential attachment in social networks."}, "https://arxiv.org/abs/2307.09819": {"title": "Analyzing large scale political discussions on Twitter: the use case of the Greek wiretapping scandal (#ypoklopes)", "link": "https://arxiv.org/abs/2307.09819", "description": "arXiv:2307.09819v2 Announce Type: replace \nAbstract: In this paper, we study the Greek wiretappings scandal, which has been revealed in 2022 and attracted a lot of attention by press and citizens. Specifically, we propose a methodology for collecting data and analyzing patterns of online public discussions on Twitter. We apply our methodology to the Greek wiretappings use case, and present findings related to the evolution of the discussion over time, its polarization, and the role of the media. The methodology can be of wider use and replicated to other topics. Finally, we provide publicly an open dataset, and online resources with the results."}, "https://arxiv.org/abs/2310.20354": {"title": "Statistical Complexity of Heterogeneous Geometric Networks", "link": "https://arxiv.org/abs/2310.20354", "description": "arXiv:2310.20354v3 Announce Type: replace \nAbstract: Degree heterogeneity and latent geometry, also referred to as popularity and similarity, are key explanatory components underlying the structure of real-world networks. The relationship between these components and the statistical complexity of networks is not well understood. We introduce a parsimonious normalised measure of statistical complexity for networks. The measure is trivially 0 in regular graphs and we prove that this measure tends to 0 in Erd\\\"os-R\\'enyi random graphs in the thermodynamic limit. We go on to demonstrate that greater complexity arises from the combination of heterogeneous and geometric components to the network structure than either on their own. Further, the levels of complexity achieved are similar to those found in many real-world networks. However, we also find that real-world networks establish connections in a way which increases complexity and which our null models fail to explain. We study this using ten link growth mechanisms and find that only one mechanism successfully and consistently replicates this phenomenon -- probabilities proportional to the exponential of the number of common neighbours between two nodes. Common neighbours is a mechanism which implicitly accounts for degree heterogeneity and latent geometry. This explains how a simple mechanism facilitates the growth of statistical complexity in real-world networks."}, "https://arxiv.org/abs/2402.00078": {"title": "Hypergraph reconstruction from dynamics", "link": "https://arxiv.org/abs/2402.00078", "description": "arXiv:2402.00078v2 Announce Type: replace \nAbstract: A plethora of methods have been developed in the past two decades to infer the underlying network structure of an interconnected system from its collective dynamics. However, methods capable of inferring nonpairwise interactions are only starting to appear. Here, we develop an inference algorithm based on sparse identification of nonlinear dynamics (SINDy) to reconstruct hypergraphs and simplicial complexes from time-series data. Our model-free method does not require information about node dynamics or coupling functions, making it applicable to complex systems that do not have a reliable mathematical description. We first benchmark the new method on synthetic data generated from Kuramoto and Lorenz dynamics. We then use it to infer the effective connectivity in the brain from resting-state EEG data, which reveals significant contributions from non-pairwise interactions in shaping the macroscopic brain dynamics."}, "https://arxiv.org/abs/2308.00014": {"title": "A new mapping of technological interdependence", "link": "https://arxiv.org/abs/2308.00014", "description": "arXiv:2308.00014v3 Announce Type: replace-cross \nAbstract: How does technological interdependence affect innovation? We address this question by examining the influence of neighbors' innovativeness and the structure of the innovators' network on a sector's capacity to develop new technologies. We study these two dimensions of technological interdependence by applying novel methods of text mining and network analysis to the documents of 6.5 million patents granted by the United States Patent and Trademark Office (USPTO) between 1976 and 2021. We find that, in the long run, the influence of network linkages is as important as that of neighbor innovativeness. In the short run, however, positive shocks to neighbor innovativeness yield relatively rapid effects, while the impact of shocks strengthening network linkages manifests with delay, even though lasts longer. Our analysis also highlights that patent text contains a wealth of information often not captured by traditional innovation metrics, such as patent citations."}, "https://arxiv.org/abs/2312.09384": {"title": "Modeling Epidemic Spread: A Gaussian Process Regression Approach", "link": "https://arxiv.org/abs/2312.09384", "description": "arXiv:2312.09384v2 Announce Type: replace-cross \nAbstract: Modeling epidemic spread is critical for informing policy decisions aimed at mitigation. Accordingly, in this work we present a new data-driven method based on Gaussian process regression (GPR) to model epidemic spread. We bound the variance of the predictions made by GPR, which quantifies the impact of epidemic data on the proposed model. Next, we derive a high-probability error bound on the prediction error in terms of the distance between the training points and a testing point, the posterior variance, and the level of change in the spreading process, and we assess how the characteristics of the epidemic spread and infection data influence this error bound. We present examples that use GPR to model and predict epidemic spread by using real-world infection data gathered in the UK during the COVID-19 epidemic. These examples illustrate that, under typical conditions, the prediction for the next twenty days has 94.29% of the noisy data located within the 95% confidence interval, validating these predictions."}, "https://arxiv.org/abs/2403.09755": {"title": "Estimating the history of a random recursive tree", "link": "https://arxiv.org/abs/2403.09755", "description": "arXiv:2403.09755v2 Announce Type: replace-cross \nAbstract: This paper studies the problem of estimating the order of arrival of the vertices in a random recursive tree. Specifically, we study two fundamental models: the uniform attachment model and the linear preferential attachment model. We propose an order estimator based on the Jordan centrality measure and define a family of risk measures to quantify the quality of the ordering procedure. Moreover, we establish a minimax lower bound for this problem, and prove that the proposed estimator is nearly optimal. Finally, we numerically demonstrate that the proposed estimator outperforms degree-based and spectral ordering procedures."}, "https://arxiv.org/abs/2409.11426": {"title": "Towards Opinion Shaping: A Deep Reinforcement Learning Approach in Bot-User Interactions", "link": "https://arxiv.org/abs/2409.11426", "description": "arXiv:2409.11426v1 Announce Type: new \nAbstract: This paper aims to investigate the impact of interference in social network algorithms via user-bot interactions, focusing on the Stochastic Bounded Confidence Model (SBCM). This paper explores two approaches: positioning bots controlled by agents into the network and targeted advertising under various circumstances, operating with an advertising budget. This study integrates the Deep Deterministic Policy Gradient (DDPG) algorithm and its variants to experiment with different Deep Reinforcement Learning (DRL). Finally, experimental results demonstrate that this approach can result in efficient opinion shaping, indicating its potential in deploying advertising resources on social platforms."}, "https://arxiv.org/abs/2409.11482": {"title": "Female representation across mythologies", "link": "https://arxiv.org/abs/2409.11482", "description": "arXiv:2409.11482v1 Announce Type: new \nAbstract: Social groups have been studied throughout history to understand how different configurations impact those within them. Along with this came the interest in investigating social groups of both fictional and mythological works. Over the last decade these social groups have been studied through the lens of network science allowing for a new level of comparison between these stories. We use this approach to focus on the attributes of the characters within these networks, specifically looking at their gender. With this we review how the female populations within various narratives and to some extent the societies they are based in are portrayed. Through this we find that although there is not a trend of all narratives of the same origin having similar levels of representation some are noticeably better than others. We also observe which narratives overall prioritise important female characters and which do not."}, "https://arxiv.org/abs/2409.11665": {"title": "Community Shaping in the Digital Age: A Temporal Fusion Framework for Analyzing Discourse Fragmentation in Online Social Networks", "link": "https://arxiv.org/abs/2409.11665", "description": "arXiv:2409.11665v1 Announce Type: new \nAbstract: This research presents a framework for analyzing the dynamics of online communities in social media platforms, utilizing a temporal fusion of text and network data. By combining text classification and dynamic social network analysis, we uncover mechanisms driving community formation and evolution, revealing the influence of real-world events. We introduced fourteen key elements based on social science theories to evaluate social media dynamics, validating our framework through a case study of Twitter data during major U.S. events in 2020. Our analysis centers on discrimination discourse, identifying sexism, racism, xenophobia, ableism, homophobia, and religious intolerance as main fragments. Results demonstrate rapid community emergence and dissolution cycles representative of discourse fragments. We reveal how real-world circumstances impact discourse dominance and how social media contributes to echo chamber formation and societal polarization. Our comprehensive approach provides insights into discourse fragmentation, opinion dynamics, and structural aspects of online communities, offering a methodology for understanding the complex interplay between online interactions and societal trends."}, "https://arxiv.org/abs/2409.11687": {"title": "A novel DFS/BFS approach towards link prediction", "link": "https://arxiv.org/abs/2409.11687", "description": "arXiv:2409.11687v1 Announce Type: new \nAbstract: Knowledge graphs have been shown to play a significant role in current knowledge mining fields, including life sciences, bioinformatics, computational social sciences, and social network analysis. The problem of link prediction bears many applications and has been extensively studied. However, most methods are restricted to dimension reduction, probabilistic model, or similarity-based approaches and are inherently biased. In this paper, we provide a definition of graph prediction for link prediction and outline related work to support our novel approach, which integrates centrality measures with classical machine learning methods. We examine our experimental results in detail and identify areas for potential further research. Our method shows promise, particularly when utilizing randomly selected nodes and degree centrality."}, "https://arxiv.org/abs/2409.11759": {"title": "My Views Do Not Reflect Those of My Employer: Differences in Behavior of Organizations' Official and Personal Social Media Accounts", "link": "https://arxiv.org/abs/2409.11759", "description": "arXiv:2409.11759v1 Announce Type: new \nAbstract: On social media, the boundaries between people's private and public lives often blur. The need to navigate both roles, which are governed by distinct norms, impacts how individuals conduct themselves online, and presents methodological challenges for researchers. We conduct a systematic exploration on how an organization's official Twitter accounts and its members' personal accounts differ. Using a climate change Twitter data set as our case, we find substantial differences in activity and connectivity across the organizational levels we examined. The levels differed considerably in their overall retweet network structures, and accounts within each level were more likely to have similar connections than accounts at different levels. We illustrate the implications of these differences for applied research by showing that the levels closer to the core of the organization display more sectoral homophily but less triadic closure, and how each level consists of very different group structures. Our results show that the common practice of solely analyzing accounts from a single organizational level, grouping together all levels, or excluding certain levels can lead to a skewed understanding of how organizations are represented on social media."}, "https://arxiv.org/abs/2409.11857": {"title": "Continuity equation and fundamental diagram of pedestrians", "link": "https://arxiv.org/abs/2409.11857", "description": "arXiv:2409.11857v1 Announce Type: new \nAbstract: Since the beginning of the century, capturing trajectories of pedestrian streams precisely from video recordings has been possible. To enable measurements at high density, the heads of the pedestrians are marked and tracked, thus providing a complete representation of the phase space. However, classical definitions and local measurements of flow, density, and velocity of pedestrian streams using trajectories are based on different segments in phase space (Lagrangian representation). The flow is defined as an average value over time, while the density is defined as the average value of an area. This leads to inconsistencies in central relations, such as the flow equation or the fundamental diagram. These have a particular effect in inhomogeneous states, such as the stop-and-go waves, where, in addition, the pedestrians do not change their position in the stop phase, but the head of the body moves. In order to obtain a local and spatio-temporally consistent measurement of the quantities flow, density, and velocity while ensuring particle number conservation fields (Euler representation) and the continuity equation could be used. To map trajectories of pedestrians heads parameter free and unambiguously to fields, this article introduces a method based on the Voronoi decomposition. These new definitions of flow, density, speed, and the particle number conserving flow equation are consistent with classical measurements. They are able to scrutinise inconsistencies in the state of the art of pedestrian fundamental diagrams."}, "https://arxiv.org/abs/2409.11858": {"title": "Crossing the disciplines -- a starter toolkit for researchers who wish to explore early Irish literature", "link": "https://arxiv.org/abs/2409.11858", "description": "arXiv:2409.11858v1 Announce Type: new \nAbstract: The inspiration behind this paper came from both authors' long-term collaboration with our friend and colleague, Professor Ralph Kenna. This connection emerged initially through his interest in Rathcroghan and in our paper, `Exploring the Nature of the Fr\\'aoch Saga', which we concluded with the statement that we believed it `presents a case that will hopefully ignite conversation between disciplines'. This led us to consider the potential value for researchers of compiling a template list of useful and reliable sources and resources to consult, in other words a type of starter toolkit or guide for any individual from an alternative discipline or background, who might possess, or, in time, develop a personal or professional interest in Early Ireland and Early Irish literature. In doing this, we decided for ease of illustration, to take the example of the location name Rathcroghan/Cruachan A\\'i, (the prehistoric Royal Site of Connacht in the west of Ireland and the place that we both work in and interact with on a daily basis), as a case study in order to demonstrate an initial methodological approach to not only the types of resources and information available, but also to highlight some potential pitfalls that may arise in the course of an investigation."}, "https://arxiv.org/abs/2409.11955": {"title": "Anomalous behavior of Replicator dynamics for the Prisoner's Dilemma on diluted lattices", "link": "https://arxiv.org/abs/2409.11955", "description": "arXiv:2409.11955v1 Announce Type: new \nAbstract: In diluted lattices, cooperation is often enhanced at specific densities, particularly near the percolation threshold for stochastic updating rules. However, the Replicator rule, despite being probabilistic, does not follow this trend. We find that this anomalous behavior is caused by structures formed by holes and defectors, which prevent some agents from experiencing fluctuations, thereby restricting the free flow of information across the network. As a result, the system becomes trapped in a frozen state, though this can be disrupted by introducing perturbations. Finally, we provide a more quantitative analysis of the relationship between the percolation threshold and cooperation, tracking its development within clusters of varying sizes and demonstrating how the percolation threshold shapes the fundamental structures of the lattice."}, "https://arxiv.org/abs/2409.11984": {"title": "Spectral clustering of time-evolving networks using the inflated dynamic Laplacian for graphs", "link": "https://arxiv.org/abs/2409.11984", "description": "arXiv:2409.11984v1 Announce Type: new \nAbstract: Complex time-varying networks are prominent models for a wide variety of spatiotemporal phenomena. The functioning of networks depends crucially on their connectivity, yet reliable techniques for determining communities in spacetime networks remain elusive. We adapt successful spectral techniques from continuous-time dynamics on manifolds to the graph setting to fill this gap. We formulate an {\\it inflated dynamic Laplacian} for graphs and develop a spectral theory to underpin the corresponding algorithmic realisations. We develop spectral clustering approaches for both multiplex and non-multiplex networks, based on the eigenvectors of the inflated dynamic Laplacian and specialised Sparse EigenBasis Approximation (SEBA) post-processing of these eigenvectors. We demonstrate that our approach can outperform the Leiden algorithm applied both in spacetime and layer-by-layer, and we analyse voting data from the US senate (where senators come and go as congresses evolve) to quantify increasing polarisation in time."}, "https://arxiv.org/abs/2409.12071": {"title": "Quantifying the role of supernatural entities and the effect of missing data in Irish sagas", "link": "https://arxiv.org/abs/2409.12071", "description": "arXiv:2409.12071v1 Announce Type: new \nAbstract: For over a decade, complex networks have been applied to mythological texts in order to quantitatively compare them. This has allowed us to identify similarities between texts in different cultures, as well as to quantify the significance of some heroic characters. Analysing a full mythology of a culture requires gathering data from many individual myths which is time consuming and often impractical. In this work, we attempt to bypass this by analysing the network of characters in a dictionary of mythological characters. We show that the top characters identified by different centrality measures are consistent with central figures in the Irish sagas. Although much of Irish mythology has been lost, we demonstrate that these most central characters are highly robust to a large random removal of edges."}, "https://arxiv.org/abs/2409.12177": {"title": "LitFM: A Retrieval Augmented Structure-aware Foundation Model For Citation Graphs", "link": "https://arxiv.org/abs/2409.12177", "description": "arXiv:2409.12177v1 Announce Type: new \nAbstract: With the advent of large language models (LLMs), managing scientific literature via LLMs has become a promising direction of research. However, existing approaches often overlook the rich structural and semantic relevance among scientific literature, limiting their ability to discern the relationships between pieces of scientific knowledge, and suffer from various types of hallucinations. These methods also focus narrowly on individual downstream tasks, limiting their applicability across use cases. Here we propose LitFM, the first literature foundation model designed for a wide variety of practical downstream tasks on domain-specific literature, with a focus on citation information. At its core, LitFM contains a novel graph retriever to integrate graph structure by navigating citation graphs and extracting relevant literature, thereby enhancing model reliability. LitFM also leverages a knowledge-infused LLM, fine-tuned through a well-developed instruction paradigm. It enables LitFM to extract domain-specific knowledge from literature and reason relationships among them. By integrating citation graphs during both training and inference, LitFM can generalize to unseen papers and accurately assess their relevance within existing literature. Additionally, we introduce new large-scale literature citation benchmark datasets on three academic fields, featuring sentence-level citation information and local context. Extensive experiments validate the superiority of LitFM, achieving 28.1% improvement on retrieval task in precision, and an average improvement of 7.52% over state-of-the-art across six downstream literature-related tasks"}, "https://arxiv.org/abs/2409.11554": {"title": "A Property Encoder for Graph Neural Networks", "link": "https://arxiv.org/abs/2409.11554", "description": "arXiv:2409.11554v1 Announce Type: cross \nAbstract: Graph machine learning, particularly using graph neural networks, fundamentally relies on node features. Nevertheless, numerous real-world systems, such as social and biological networks, often lack node features due to various reasons, including privacy concerns, incomplete or missing data, and limitations in data collection. In such scenarios, researchers typically resort to methods like structural and positional encoding to construct node features. However, the length of such features is contingent on the maximum value within the property being encoded, for example, the highest node degree, which can be exceedingly large in applications like scale-free networks. Furthermore, these encoding schemes are limited to categorical data and might not be able to encode metrics returning other type of values. In this paper, we introduce a novel, universally applicable encoder, termed PropEnc, which constructs expressive node embedding from any given graph metric. PropEnc leverages histogram construction combined with reverse index encoding, offering a flexible method for node features initialization. It supports flexible encoding in terms of both dimensionality and type of input, demonstrating its effectiveness across diverse applications. PropEnc allows encoding metrics in low-dimensional space which effectively avoids the issue of sparsity and enhances the efficiency of the models. We show that \\emph{PropEnc} can construct node features that either exactly replicate one-hot encoding or closely approximate indices under various settings. Our extensive evaluations in graph classification setting across multiple social networks that lack node features support our hypothesis. The empirical results conclusively demonstrate that PropEnc is both an efficient and effective mechanism for constructing node features from diverse set of graph metrics."}, "https://arxiv.org/abs/2409.11618": {"title": "PieClam: A Universal Graph Autoencoder Based on Overlapping Inclusive and Exclusive Communities", "link": "https://arxiv.org/abs/2409.11618", "description": "arXiv:2409.11618v1 Announce Type: cross \nAbstract: We propose PieClam (Prior Inclusive Exclusive Cluster Affiliation Model): a probabilistic graph model for representing any graph as overlapping generalized communities. Our method can be interpreted as a graph autoencoder: nodes are embedded into a code space by an algorithm that maximizes the log-likelihood of the decoded graph, given the input graph. PieClam is a community affiliation model that extends well-known methods like BigClam in two main manners. First, instead of the decoder being defined via pairwise interactions between the nodes in the code space, we also incorporate a learned prior on the distribution of nodes in the code space, turning our method into a graph generative model. Secondly, we generalize the notion of communities by allowing not only sets of nodes with strong connectivity, which we call inclusive communities, but also sets of nodes with strong disconnection, which we call exclusive communities. To model both types of communities, we propose a new type of decoder based the Lorentz inner product, which we prove to be much more expressive than standard decoders based on standard inner products or norm distances. By introducing a new graph similarity measure, that we call the log cut distance, we show that PieClam is a universal autoencoder, able to uniformly approximately reconstruct any graph. Our method is shown to obtain competitive performance in graph anomaly detection benchmarks."}, "https://arxiv.org/abs/2409.12000": {"title": "\"It Might be Technically Impressive, But It's Practically Useless to Us\": Practices, Challenges, and Opportunities for Cross-Functional Collaboration around AI within the News Industry", "link": "https://arxiv.org/abs/2409.12000", "description": "arXiv:2409.12000v1 Announce Type: cross \nAbstract: Recently, an increasing number of news organizations have integrated artificial intelligence (AI) into their workflows, leading to a further influx of AI technologists and data workers into the news industry. This has initiated cross-functional collaborations between these professionals and journalists. While prior research has explored the impact of AI-related roles entering the news industry, there is a lack of studies on how cross-functional collaboration unfolds between AI professionals and journalists. Through interviews with 17 journalists, 6 AI technologists, and 3 AI workers with cross-functional experience from leading news organizations, we investigate the current practices, challenges, and opportunities for cross-functional collaboration around AI in today's news industry. We first study how journalists and AI professionals perceive existing cross-collaboration strategies. We further explore the challenges of cross-functional collaboration and provide recommendations for enhancing future cross-functional collaboration around AI in the news industry."}, "https://arxiv.org/abs/2409.12097": {"title": "Skill matching at scale: freelancer-project alignment for efficient multilingual candidate retrieval", "link": "https://arxiv.org/abs/2409.12097", "description": "arXiv:2409.12097v1 Announce Type: cross \nAbstract: Finding the perfect match between a job proposal and a set of freelancers is not an easy task to perform at scale, especially in multiple languages. In this paper, we propose a novel neural retriever architecture that tackles this problem in a multilingual setting. Our method encodes project descriptions and freelancer profiles by leveraging pre-trained multilingual language models. The latter are used as backbone for a custom transformer architecture that aims to keep the structure of the profiles and project. This model is trained with a contrastive loss on historical data. Thanks to several experiments, we show that this approach effectively captures skill matching similarity and facilitates efficient matching, outperforming traditional methods."}, "https://arxiv.org/abs/2209.04562": {"title": "The Bayan Algorithm: Detecting Communities in Networks Through Exact and Approximate Optimization of Modularity", "link": "https://arxiv.org/abs/2209.04562", "description": "arXiv:2209.04562v4 Announce Type: replace \nAbstract: Community detection is a classic network problem with extensive applications in various fields. Its most common method is using modularity maximization heuristics which rarely return an optimal partition or anything similar. Partitions with globally optimal modularity are difficult to compute, and therefore have been underexplored. Using structurally diverse networks, we compare 30 community detection methods including our proposed algorithm that offers optimality and approximation guarantees: the Bayan algorithm. Unlike existing methods, Bayan globally maximizes modularity or approximates it within a factor. Our results show the distinctive accuracy and stability of maximum-modularity partitions in retrieving planted partitions at rates higher than most alternatives for a wide range of parameter settings in two standard benchmarks. Compared to the partitions from 29 other algorithms, maximum-modularity partitions have the best medians for description length, coverage, performance, average conductance, and well clusteredness. These advantages come at the cost of additional computations which Bayan makes possible for small networks (networks that have up to 3000 edges in their largest connected component). Bayan is several times faster than using open-source and commercial solvers for modularity maximization, making it capable of finding optimal partitions for instances that cannot be optimized by any other existing method. Our results point to a few well performing algorithms, among which Bayan stands out as the most reliable method for small networks. A Python implementation of the Bayan algorithm (bayanpy) is publicly available through the package installer for Python (pip)."}, "https://arxiv.org/abs/2209.14836": {"title": "Characterizing the Structure of Online Conversations Across Reddit", "link": "https://arxiv.org/abs/2209.14836", "description": "arXiv:2209.14836v2 Announce Type: replace \nAbstract: The proliferation of social media platforms has afforded social scientists unprecedented access to vast troves of data on human interactions, facilitating the study of online behavior at an unparalleled scale. These platforms typically structure conversations as threads, forming tree-like structures known as \"discussion trees.\" This paper examines the structural properties of online discussions on Reddit by analyzing both global (community-level) and local (post-level) attributes of these discussion trees. We conduct a comprehensive statistical analysis of a year's worth of Reddit data, encompassing a quarter of a million posts and several million comments. Our primary objective is to disentangle the relative impacts of global and local properties and evaluate how specific features shape discussion tree structures. The results reveal that both local and global features contribute significantly to explaining structural variation in discussion trees. However, local features, such as post content and sentiment, collectively have a greater impact, accounting for a larger proportion of variation in the width, depth, and size of discussion trees. Our analysis also uncovers considerable heterogeneity in the impact of various features on discussion structures. Notably, certain global features play crucial roles in determining specific discussion tree properties. These features include the subreddit's topic, age, popularity, and content redundancy. For instance, posts in subreddits focused on politics, sports, and current events tend to generate deeper and wider discussion trees. This research enhances our understanding of online conversation dynamics and offers valuable insights for both content creators and platform designers. By elucidating the factors that shape online discussions, our work contributes to ongoing efforts to improve the quality and effectiveness of digital discourse."}, "https://arxiv.org/abs/2211.16054": {"title": "Mitigating the risk of tanking in multi-stage tournaments", "link": "https://arxiv.org/abs/2211.16054", "description": "arXiv:2211.16054v5 Announce Type: replace \nAbstract: Multi-stage tournaments consisting of a round-robin group stage followed by a knockout phase are ubiquitous in sports. However, this format is incentive incompatible if at least two teams from a group advance to the knockout stage where the brackets are predetermined. A model is developed to quantify the risk of tanking in these contests. The suggested approach is applied to the 2022 FIFA World Cup to uncover how its design could have been improved by changing group labelling (a reform that has received no attention before) and the schedule of group matches. Scheduling is found to be a surprisingly weak intervention compared to previous results on the risk of collusion in a group. The probability of tanking, which is disturbingly high around 25\\%, cannot be reduced by more than 3 percentage points via these policies. Tournament organisers need to consider more fundamental changes against tanking."}, "https://arxiv.org/abs/2403.01535": {"title": "Neural Graph Generator: Feature-Conditioned Graph Generation using Latent Diffusion Models", "link": "https://arxiv.org/abs/2403.01535", "description": "arXiv:2403.01535v3 Announce Type: replace-cross \nAbstract: Graph generation has emerged as a crucial task in machine learning, with significant challenges in generating graphs that accurately reflect specific properties. Existing methods often fall short in efficiently addressing this need as they struggle with the high-dimensional complexity and varied nature of graph properties. In this paper, we introduce the Neural Graph Generator (NGG), a novel approach which utilizes conditioned latent diffusion models for graph generation. NGG demonstrates a remarkable capacity to model complex graph patterns, offering control over the graph generation process. NGG employs a variational graph autoencoder for graph compression and a diffusion process in the latent vector space, guided by vectors summarizing graph statistics. We demonstrate NGG's versatility across various graph generation tasks, showing its capability to capture desired graph properties and generalize to unseen graphs. We also compare our generator to the graph generation capabilities of different LLMs. This work signifies a shift in graph generation methodologies, offering a more practical and efficient solution for generating diverse graphs with specific characteristics."}, "https://arxiv.org/abs/2409.12203": {"title": "A Simple Model to Estimate Sharing Effects in Social Networks", "link": "https://arxiv.org/abs/2409.12203", "description": "arXiv:2409.12203v1 Announce Type: new \nAbstract: Randomised Controlled Trials (RCTs) are the gold standard for estimating treatment effects across many fields of science. Technology companies have adopted A/B-testing methods as a modern RCT counterpart, where end-users are randomly assigned various system variants and user behaviour is tracked continuously. The objective is then to estimate the causal effect that the treatment variant would have on certain metrics of interest to the business.\n  When the outcomes for randomisation units -- end-users in this case -- are not statistically independent, this obfuscates identifiability of treatment effects, and harms decision-makers' observability of the system. Social networks exemplify this, as they are designed to promote inter-user interactions. This interference by design notoriously complicates measurement of, e.g., the effects of sharing. In this work, we propose a simple Markov Decision Process (MDP)-based model describing user sharing behaviour in social networks. We derive an unbiased estimator for treatment effects under this model, and demonstrate through reproducible synthetic experiments that it outperforms existing methods by a significant margin."}, "https://arxiv.org/abs/2409.12242": {"title": "Cool Pavements", "link": "https://arxiv.org/abs/2409.12242", "description": "arXiv:2409.12242v1 Announce Type: new \nAbstract: Cool pavements designate alternative pavements designed to reduce their contribution to urban heating. Urban heating generally refers to the sensible heat exchanged with the atmosphere by urban materials but can also include the radiative load they impose on pedestrians. In either case, pavement surface temperature is a very important parameter, which cool pavements seek to decrease compared with standard pavement designs. The energy balance of a pavement surface or very thin pavement slab helps identify the outbound flows which cool pavements attempt to promote and fundamental physical principles which govern them. On this basis, cool pavements can be classified as reflective pavements, green and evaporative pavements, high inertia or phase changing pavements as well as conductive or heatharvesting pavements. This chapter presents the urban heat island and the urban heating phenomena and provides an overview of cool pavement technologies, detailing areas which require further scientific investigation."}, "https://arxiv.org/abs/2409.12357": {"title": "Dynamics of Post-disaster Recovery in Behavior-dependent Business Networks", "link": "https://arxiv.org/abs/2409.12357", "description": "arXiv:2409.12357v1 Announce Type: new \nAbstract: The recovery of businesses after a disaster is vital to community economic resilience, yet the network dynamics influencing the speed and spillover effects of recovery remain poorly understood. Understanding these dynamics is essential for characterizing economic resilience and informing more effective recovery policies. This study explores the extent to which post-disaster business recovery is shaped by network diffusion processes within pre-disaster business dependency networks, driven by visitation behaviors among business points of interest (POIs). We developed a network diffusion model to simulate recovery across businesses in the Louisiana Gulf Coast following Hurricane Ida (2021) and assessed its performance using empirical data. Our analysis focuses on four key areas: (1) the presence of a diffusion process influencing recovery across the business network; (2) variations in how different business types depend on others for recovery; (3) identification of recovery multiplier businesses that accelerate regional recovery; and (4) differences in recovery multipliers across high- and low-income areas. The findings reveal that business recovery is governed by diffusion dynamics in these behavior-based networks, with recovery speed closely linked to pre-disaster visitation patterns. Retail and service businesses are identified as key recovery multipliers whose rapid recovery accelerates the broader business network's recovery, enhancing economic resilience. Additionally, recovery multipliers vary between high- and low-income areas. This study enhances our understanding of network mechanisms in post-disaster recovery and offers valuable insights for improving recovery policies."}, "https://arxiv.org/abs/2409.12460": {"title": "Robustness of the public transport network against attacks on its routes", "link": "https://arxiv.org/abs/2409.12460", "description": "arXiv:2409.12460v1 Announce Type: new \nAbstract: We investigate the robustness of Public Transport Networks (PTNs) when subjected to route attacks, focusing specifically on public bus lines. Such attacks, mirroring real-world scenarios, offer insight into the multifaceted dynamics of cities. Our study delves into the consequences of systematically removing entire routes based on strategies that use centrality measures. We evaluate the network's robustness by analyzing the sizes of fragmented networks, focusing on the largest components and derived metrics. To assess the efficacy of various attack strategies, we employ them on both a synthetic PTN model and a real-world example, specifically the Buenos Aires Metropolitan Area in Argentina. We examine these strategies and contrast them with random, and one-step most and least harmful procedures. Our findings indicate that \\textit{betweenness}-based attacks and the one-step most (\\textit{maximal}) harmful procedure emerge as the most effective attack strategies. Remarkably, the \\textit{betweenness} strategy partitions the network into components of similar sizes, whereas alternative approaches yield one dominant and several minor components."}, "https://arxiv.org/abs/2409.12686": {"title": "Trust in society: A stochastic compartmental model", "link": "https://arxiv.org/abs/2409.12686", "description": "arXiv:2409.12686v1 Announce Type: new \nAbstract: This paper studies a novel stochastic compartmental model that describes the dynamics of trust in society. The population is split into three compartments representing levels of trust in society: trusters, skeptics and doubters. The focus lies on assessing the long-term dynamics, under `bounded confidence' i.e., trusters and doubters do not communicate). We state and classify the stationary points of the system's mean behavior. We find that an increase in life-expectancy, and a greater population may increase the proportion of individuals who lose their trust completely. In addition, the relationship between the rate at which doubters convince skeptics to join their cause and the expected number of doubters is not monotonic -- it does not always help to be more convincing to ensure the survival of your group. We numerically illustrate the workings of our analysis. Because the study of stochastic compartmental models for social dynamics is not common, we in particular shed light on the limitations of deterministic compartmental models.\n  In our experiments we make use of fluid and diffusion approximation techniques as well as Gillespie simulation."}, "https://arxiv.org/abs/2409.12852": {"title": "Characterizing variability in complex network community structure with a recursive significance clustering scheme", "link": "https://arxiv.org/abs/2409.12852", "description": "arXiv:2409.12852v1 Announce Type: new \nAbstract: Network science has presented community detection as a valuable tool for revealing the functional modules in complex systems as rooted in the wiring architectures of complex networks. The varying procedures of community detection can produce, however, divisions of a network into communities that are prone to degeneracy given small changes to the network's configuration. This yields network partitions of similar merit that are structurally dissimilar, which is especially problematic when the network is constructed on uncertain data. To reconcile with the ambiguity in interpreting degenerate network partitions as representations of underlying system function, we introduce a recursive significance clustering scheme that identifies the subsets of nodes that have stable joint community assignments under network perturbation. These robust node groups are referred to here as cores, and represent well-supported features of the network. We show that cores characterize the variability inherent to non-overlapping community structure in weighted and directed networks and indicate robust community assignments that are cohesive under temporal evolution of the network."}, "https://arxiv.org/abs/2409.12853": {"title": "A New Perspective on ADHD Research: Knowledge Graph Construction with LLMs and Network Based Insights", "link": "https://arxiv.org/abs/2409.12853", "description": "arXiv:2409.12853v1 Announce Type: new \nAbstract: Attention-Deficit/Hyperactivity Disorder (ADHD) is a challenging disorder to study due to its complex symptomatology and diverse contributing factors. To explore how we can gain deeper insights on this topic, we performed a network analysis on a comprehensive knowledge graph (KG) of ADHD, constructed by integrating scientific literature and clinical data with the help of cutting-edge large language models. The analysis, including k-core techniques, identified critical nodes and relationships that are central to understanding the disorder. Building on these findings, we developed a context-aware chatbot using Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG), enabling accurate and informed interactions. Our knowledge graph not only advances the understanding of ADHD but also provides a powerful tool for research and clinical applications."}, "https://arxiv.org/abs/2409.12933": {"title": "Emergence of echo chambers in a noisy adaptive voter model", "link": "https://arxiv.org/abs/2409.12933", "description": "arXiv:2409.12933v1 Announce Type: new \nAbstract: Belief perseverance is the widely documented tendency of holding to a belief, even in the presence of contradicting evidence. In online environments, this tendency leads to heated arguments with users ``blocking'' each other. Introducing this element to opinion modelling in a social network, leads to an adaptive network where agents tend to connect preferentially to like-minded peers. In this work we study how this type of dynamics behaves in the voter model with the addition of a noise that makes agents change opinion at random. As the intensity of the noise and the propensity of users blocking each other is changed, we observe a transition between 2 phases. One in which there is only one community in the whole network and another where communities arise and in each of then there is a very clear majority opinion, mimicking the phenomenon of echo chambers. These results are obtained with simulations and with a mean-field theory."}, "https://arxiv.org/abs/2409.12263": {"title": "Detecting LGBTQ+ Instances of Cyberbullying", "link": "https://arxiv.org/abs/2409.12263", "description": "arXiv:2409.12263v1 Announce Type: cross \nAbstract: Social media continues to have an impact on the trajectory of humanity. However, its introduction has also weaponized keyboards, allowing the abusive language normally reserved for in-person bullying to jump onto the screen, i.e., cyberbullying. Cyberbullying poses a significant threat to adolescents globally, affecting the mental health and well-being of many. A group that is particularly at risk is the LGBTQ+ community, as researchers have uncovered a strong correlation between identifying as LGBTQ+ and suffering from greater online harassment. Therefore, it is critical to develop machine learning models that can accurately discern cyberbullying incidents as they happen to LGBTQ+ members. The aim of this study is to compare the efficacy of several transformer models in identifying cyberbullying targeting LGBTQ+ individuals. We seek to determine the relative merits and demerits of these existing methods in addressing complex and subtle kinds of cyberbullying by assessing their effectiveness with real social media data."}, "https://arxiv.org/abs/2409.12420": {"title": "Spatially-invariant opinion dynamics on the circle", "link": "https://arxiv.org/abs/2409.12420", "description": "arXiv:2409.12420v1 Announce Type: cross \nAbstract: We propose a nonlinear opinion dynamics model for an agent making decisions about a continuous distribution of options in the presence of distributed input. Inspired by perceptual decision-making, we develop the theory for options distributed on the circle, representing, e.g., the set of possible heading directions in planar robotic navigation problems. Interactions among options are encoded through a spatially invariant kernel. We design the kernel to ensure that decision-making is robust, in the sense that only a finite subset of options can be favored over the continuum. We prove the spatial invariance of the model linearization and use this result to prove an opinion-forming bifurcation in the model with zero input. We then use space and time frequency domain analysis of the model linearization to infer the ultra-sensitive input-output behavior of the nonlinear dynamics with input. We illustrate our model's versatility with a robotic navigation example in crowded spaces."}, "https://arxiv.org/abs/2106.13672": {"title": "Exploring individual differences through network topology", "link": "https://arxiv.org/abs/2106.13672", "description": "arXiv:2106.13672v3 Announce Type: replace \nAbstract: Social animals, including humans, have a broad range of personality traits, which can be used to predict individual behavioral responses and decisions. Current methods to quantify individual personality traits in humans rely on self-report questionnaires, which require time and effort to collect, and rely on active cooperation. However, personality differences naturally manifest in social interactions such as online social networks. Here, we explored this option and found that the topology of an online social network can be used to characterize the personality traits of its members. We analyzed the directed social graph formed by the users of the LiveJournal (LJ) blogging platform. Individual user personality traits, inferred from their self-reported domains of interest (DOIs), were associated with their network measures. Empirical clustering of DOIs by topological similarity exposed two main self-emergent DOI groups that were in alignment with the personality meta-traits plasticity and stability. Closeness, a global topological measure of network centrality, was higher for bloggers associated with plasticity (vs. stability). A local network motif (a triad of 3 connected bloggers) also separated the personality meta-traits. Finally, topology-based classification of DOIs (without analyzing the blog content) attained > 70% accuracy (average AUC of the test-set). These results indicate that personality traits can be detected in social network topology. This has serious implications for user privacy. But, if used responsibly, network identification of personality traits could aid in early identification of health-related risks, at the population level."}, "https://arxiv.org/abs/2305.18385": {"title": "Self-attention Dual Embedding for Graphs with Heterophily", "link": "https://arxiv.org/abs/2305.18385", "description": "arXiv:2305.18385v2 Announce Type: replace-cross \nAbstract: Graph Neural Networks (GNNs) have been highly successful for the node classification task. GNNs typically assume graphs are homophilic, i.e. neighboring nodes are likely to belong to the same class. However, a number of real-world graphs are heterophilic, and this leads to much lower classification accuracy using standard GNNs. In this work, we design a novel GNN which is effective for both heterophilic and homophilic graphs. Our work is based on three main observations. First, we show that node features and graph topology provide different amounts of informativeness in different graphs, and therefore they should be encoded independently and prioritized in an adaptive manner. Second, we show that allowing negative attention weights when propagating graph topology information improves accuracy. Finally, we show that asymmetric attention weights between nodes are helpful. We design a GNN which makes use of these observations through a novel self-attention mechanism. We evaluate our algorithm on real-world graphs containing thousands to millions of nodes and show that we achieve state-of-the-art results compared to existing GNNs. We also analyze the effectiveness of the main components of our design on different graphs."}, "https://arxiv.org/abs/2401.10268": {"title": "The complementary contributions of academia and industry to AI research", "link": "https://arxiv.org/abs/2401.10268", "description": "arXiv:2401.10268v2 Announce Type: replace-cross \nAbstract: Artificial intelligence (AI) has seen fast paced development in industry and academia. However, striking recent advances by industry have stunned the field, inviting a fresh perspective on the role of academic research on this progress. Here, we characterize the impact and type of AI produced by both environments over the last 25 years and establish several patterns. We find that articles published by teams consisting exclusively of industry researchers tend to get greater attention, with a higher chance of being highly cited and citation-disruptive, and several times more likely to produce state-of-the-art models. In contrast, we find that exclusively academic teams publish the bulk of AI research and tend to produce higher novelty work, with single papers having several times higher likelihood of being unconventional and atypical. The respective impact-novelty advantages of industry and academia are robust to controls for subfield, team size, seniority, and prestige. We find that academic-industry collaborations produce the most impactful work overall but do not have the novelty level of academic teams. Together, our findings identify the unique and nearly irreplaceable contributions that both academia and industry make toward the progress of AI."}, "https://arxiv.org/abs/2409.13064": {"title": "Fear and Loathing on the Frontline: Decoding the Language of Othering by Russia-Ukraine War Bloggers", "link": "https://arxiv.org/abs/2409.13064", "description": "arXiv:2409.13064v1 Announce Type: new \nAbstract: Othering, the act of portraying outgroups as fundamentally different from the ingroup, often escalates into framing them as existential threats--fueling intergroup conflict and justifying exclusion and violence. These dynamics are alarmingly pervasive, spanning from the extreme historical examples of genocides against minorities in Germany and Rwanda to the ongoing violence and rhetoric targeting migrants in the US and Europe. While concepts like hate speech and fear speech have been explored in existing literature, they capture only part of this broader and more nuanced dynamic which can often be harder to detect, particularly in online speech and propaganda. To address this challenge, we introduce a novel computational framework that leverages large language models (LLMs) to quantify othering across diverse contexts, extending beyond traditional linguistic indicators of hostility. Applying the model to real-world data from Telegram war bloggers and political discussions on Gab reveals how othering escalates during conflicts, interacts with moral language, and garners significant attention, particularly during periods of crisis. Our framework, designed to offer deeper insights into othering dynamics, combines with a rapid adaptation process to provide essential tools for mitigating othering's adverse impacts on social cohesion."}, "https://arxiv.org/abs/2409.13098": {"title": "Predicting soccer matches with complex networks and machine learning", "link": "https://arxiv.org/abs/2409.13098", "description": "arXiv:2409.13098v1 Announce Type: new \nAbstract: Soccer attracts the attention of many researchers and professionals in the sports industry. Therefore, the incorporation of science into the sport is constantly growing, with increasing investments in performance analysis and sports prediction industries. This study aims to (i) highlight the use of complex networks as an alternative tool for predicting soccer match outcomes, and (ii) show how the combination of structural analysis of passing networks with match statistical data can provide deeper insights into the game patterns and strategies used by teams. In order to do so, complex network metrics and match statistics were used to build machine learning models that predict the wins and losses of soccer teams in different leagues. The results showed that models based on passing networks were as effective as ``traditional'' models, which use general match statistics. Another finding was that by combining both approaches, more accurate models were obtained than when they were used separately, demonstrating that the fusion of such approaches can offer a deeper understanding of game patterns, allowing the comprehension of tactics employed by teams relationships between players, their positions, and interactions during matches. It is worth mentioning that both network metrics and match statistics were important and impactful for the mixed model. Furthermore, the use of networks with a lower granularity of temporal evolution (such as creating a network for each half of the match) performed better than a single network for the entire game."}, "https://arxiv.org/abs/2409.13237": {"title": "RTs != Endorsements: Rethinking Exposure Fairness on Social Media Platforms", "link": "https://arxiv.org/abs/2409.13237", "description": "arXiv:2409.13237v1 Announce Type: new \nAbstract: Recommender systems underpin many of the personalized services in the online information & social media ecosystem. However, the assumptions in the research on content recommendations in domains like search, video, and music are often applied wholesale to domains that require a better understanding of why and how users interact with the systems. In this position paper we focus on social media and argue that personalized timelines have an added layer of complexity that is derived from the social nature of the platform itself. In particular, definitions of exposure fairness should be expanded to consider the social environment each user is situated in: how often a user is exposed to others is as important as who they get exposed to."}, "https://arxiv.org/abs/2409.13362": {"title": "Spatiotemporal variability and prediction of e-bike battery levels in bike-sharing systems", "link": "https://arxiv.org/abs/2409.13362", "description": "arXiv:2409.13362v1 Announce Type: new \nAbstract: Bike Sharing Systems (BSSs) play a crucial role in promoting sustainable urban mobility by facilitating short-range trips and connecting with other transport modes. Traditionally, most BSS fleets have consisted of mechanical bikes (m-bikes), but electric bikes (e-bikes) are being progressively introduced due to their ability to cover longer distances and appeal to a wider range of users. However, the charging requirements of e-bikes often hinder their deployment and optimal functioning. This study examines the spatiotemporal variations in battery levels of Barcelona's BSS, revealing that bikes stationed near the city centre tend to have shorter rest periods and lower average battery levels. Additionally, to improve the management of e-bike fleets, a Markov-chain approach is developed to predict both bike availability and battery levels. This research offers a unique perspective on the dynamics of e-bike battery levels and provides a practical tool to overcome the main operational challenges in their implementation."}, "https://arxiv.org/abs/2409.13461": {"title": "Engagement, Content Quality and Ideology over Time on the Facebook URL Dataset", "link": "https://arxiv.org/abs/2409.13461", "description": "arXiv:2409.13461v1 Announce Type: cross \nAbstract: Unpacking the relationship between the ideology of social media users and their online news consumption offers critical insight into the feedback loop between users' engagement behavior and the recommender systems' content provision. However, disentangling inherent user behavior from platform-induced influences poses significant challenges, particularly when working with datasets covering limited time periods. In this study, we conduct both aggregate and longitudinal analyses using the Facebook Privacy-Protected Full URLs Dataset, examining user engagement metrics related to news URLs in the U.S. from January 2017 to December 2020. By incorporating the ideological alignment and quality of news sources, along with users' political preferences, we construct weighted averages of ideology and quality of news consumption for liberal, conservative, and moderate audiences. This allows us to track the evolution of (i) the ideological gap between liberals and conservatives and (ii) the average quality of each group's news consumption. These metrics are linked to broader phenomena such as polarization and misinformation. We identify two significant shifts in trends for both metrics, each coinciding with changes in user engagement. Interestingly, during both inflection points, the ideological gap widens and news quality declines; however, engagement increases after the first one and decreases after the second. Finally, we contextualize these changes by discussing their potential relation to two major updates to Facebook's News Feed algorithm."}, "https://arxiv.org/abs/2409.13542": {"title": "Optimal control of a kinetic model describing social interactions on a graph", "link": "https://arxiv.org/abs/2409.13542", "description": "arXiv:2409.13542v1 Announce Type: cross \nAbstract: In this paper we introduce the optimal control of a kinetic model describing agents who migrate on a graph and interact within its nodes exchanging a physical quantity. As a prototype model, we consider the spread of an infectious disease on a graph, so that the exchanged quantity is the viral-load. The control, exerted on both the mobility and on the interactions separately, aims at minimising the average macroscopic viral-load.\n  We prove that minimising the average viral-load weighted by the mass in each node is the most effective and convenient strategy. We consider two different interactions: in the first one the infection (gain) and the healing (loss) processes happen within the same interaction, while in the second case the infection and healing result from two different processes. With the appropriate controls, we prove that in the first case it is possible to stop the increase of the disease, but paying a very high cost in terms of control, while in the second case it is possible to eradicate the disease. We test numerically the role of each intervention and the interplay between the mobility and the interaction control strategies in each model."}, "https://arxiv.org/abs/2409.13594": {"title": "Cross-Target Stance Detection: A Survey of Techniques, Datasets, and Challenges", "link": "https://arxiv.org/abs/2409.13594", "description": "arXiv:2409.13594v1 Announce Type: cross \nAbstract: Stance detection is the task of determining the viewpoint expressed in a text towards a given target. A specific direction within the task focuses on cross-target stance detection, where a model trained on samples pertaining to certain targets is then applied to a new, unseen target. With the increasing need to analyze and mining viewpoints and opinions online, the task has recently seen a significant surge in interest. This review paper examines the advancements in cross-target stance detection over the last decade, highlighting the evolution from basic statistical methods to contemporary neural and LLM-based models. These advancements have led to notable improvements in accuracy and adaptability. Innovative approaches include the use of topic-grouped attention and adversarial learning for zero-shot detection, as well as fine-tuning techniques that enhance model robustness. Additionally, prompt-tuning methods and the integration of external knowledge have further refined model performance. A comprehensive overview of the datasets used for evaluating these models is also provided, offering valuable insights into the progress and challenges in the field. We conclude by highlighting emerging directions of research and by suggesting avenues for future work in the task."}, "https://arxiv.org/abs/2409.13664": {"title": "Analysis of Gene Regulatory Networks from Gene Expression Using Graph Neural Networks", "link": "https://arxiv.org/abs/2409.13664", "description": "arXiv:2409.13664v1 Announce Type: cross \nAbstract: Unraveling the complexities of Gene Regulatory Networks (GRNs) is crucial for understanding cellular processes and disease mechanisms. Traditional computational methods often struggle with the dynamic nature of these networks. This study explores the use of Graph Neural Networks (GNNs), a powerful approach for modeling graph-structured data like GRNs. Utilizing a Graph Attention Network v2 (GATv2), our study presents a novel approach to the construction and interrogation of GRNs, informed by gene expression data and Boolean models derived from literature. The model's adeptness in accurately predicting regulatory interactions and pinpointing key regulators is attributed to advanced attention mechanisms, a hallmark of the GNN framework. These insights suggest that GNNs are primed to revolutionize GRN analysis, addressing traditional limitations and offering richer biological insights. The success of GNNs, as highlighted by our model's reliance on high-quality data, calls for enhanced data collection methods to sustain progress. The integration of GNNs in GRN research is set to pioneer developments in personalized medicine, drug discovery, and our grasp of biological systems, bolstered by the structural analysis of networks for improved node and edge prediction."}, "https://arxiv.org/abs/2409.13674": {"title": "Topological Components in a Community Currency Network", "link": "https://arxiv.org/abs/2409.13674", "description": "arXiv:2409.13674v1 Announce Type: cross \nAbstract: Transaction data from digital payment systems can be used to study economic processes at such a detail that was not possible previously. Here, we analyse the data from Sarafu token network, a community inclusion currency in Kenya. During the COVID-19 emergency, the Sarafu was disbursed as part of a humanitarian aid project. In this work, the transactions are analysed using network science. A topological categorisation is defined to identify cyclic and acyclic components. Furthermore, temporal aspects of circulation taking place within these components are considered. The significant presence of different types of strongly connected components as compared to randomized null models shows the importance of cycles in this economic network. Especially, indicating their key role in currency recirculation. In some acyclic components, the most significant triad suggests the presence of a group of users collecting currency from accounts active only once, hinting at a misuse of the system. In some other acyclic components, small isolated groups of users were active only once, suggesting the presence of users only interested in trying out the system. The methods used in this paper can answer specific questions related to user activities, currency design, and assessment of monetary interventions. Our methodology provides a general quantitative tool for analysing the behaviour of users in a currency network."}, "https://arxiv.org/abs/2402.03583": {"title": "MQuinE: a cure for \"Z-paradox\" in knowledge graph embedding models", "link": "https://arxiv.org/abs/2402.03583", "description": "arXiv:2402.03583v3 Announce Type: replace \nAbstract: Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called \\emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE model called \\emph{MQuinE} that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20\\% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction tasks."}, "https://arxiv.org/abs/2402.01440": {"title": "A Survey of Few-Shot Learning on Graphs: from Meta-Learning to Pre-Training and Prompt Learning", "link": "https://arxiv.org/abs/2402.01440", "description": "arXiv:2402.01440v4 Announce Type: replace-cross \nAbstract: Graph representation learning, a critical step in graph-centric tasks, has seen significant advancements. Earlier techniques often operate in an end-to-end setting, which heavily rely on the availability of ample labeled data. This constraint has spurred the emergence of few-shot learning on graphs, where only a few labels are available for each task. Given the extensive literature in this field, this survey endeavors to synthesize recent developments, provide comparative insights, and identify future directions. We systematically categorize existing studies based on two major taxonomies: (1) Problem taxonomy, which explores different types of data scarcity problems and their applications, and (2) Technique taxonomy, which details key strategies for addressing these data-scarce few-shot problems. The techniques can be broadly categorized into meta-learning, pre-training, and hybrid approaches, with a finer-grained classification in each category to aid readers in their method selection process. Within each category, we analyze the relationships among these methods and compare their strengths and limitations. Finally, we outline prospective directions for few-shot learning on graphs to catalyze continued innovation in this field. The website for this survey can be accessed by \\url{https://github.com/smufang/fewshotgraph}."}, "https://arxiv.org/abs/2409.14034": {"title": "Cost-Effective Community-Hierarchy-Based Mutual Voting Approach for Influence Maximization in Complex Networks", "link": "https://arxiv.org/abs/2409.14034", "description": "arXiv:2409.14034v1 Announce Type: new \nAbstract: Various types of promising techniques have come into being for influence maximization whose aim is to identify influential nodes in complex networks. In essence, real-world applications usually have high requirements on the balance between time complexity and accuracy of influential nodes identification. To address the challenges of imperfect node influence measurement and inefficient seed nodes selection mechanism in such class of foregoing techniques, this article proposes a novel approach called Cost-Effective Community-Hierarchy-Based Mutual Voting for influence maximization in complex networks. First, we develop a method for measuring the importance of different nodes in networks based on an original concept of Dual-Scale Community-Hierarchy Information that synthesizes both hierarchy structural information and community structural information of nodes. The community structural information contained in the nodes is measured by a new notion of Hierarchical-Community Entropy. Second, we develop a method named Cost-Effective Mutual-Influence-based Voting for seed nodes selection. Hereinto, a low-computational-cost mutual voting mechanism and an updating strategy called Lazy Score Updating Strategy are newly constructed for optimizing the selecting of seed nodes. Third, we develop a balance index to evaluate the performance of different methods in striking the tradeoff between time complexity and the accuracy of influential nodes identification. Finally, we demonstrate the approach performance over ten public datasets. The extensive experiments show that the proposed approach outperforms 16 state-of-the-art techniques on the balance between time complexity and accuracy of influential nodes identification. Compared with the method with the second highest value of the balance index, our approach can be improved by at most 9.29%."}, "https://arxiv.org/abs/2409.14073": {"title": "Digital Advertising in a Post-Cookie World: Charting the Impact of Google's Topics API", "link": "https://arxiv.org/abs/2409.14073", "description": "arXiv:2409.14073v1 Announce Type: new \nAbstract: Integrating Google's Topics API into the digital advertising ecosystem represents a significant shift toward privacy-conscious advertising practices. This article analyses the implications of implementing Topics API on ad networks, focusing on competition dynamics and ad space accessibility. Through simulations based on extensive datasets capturing user behavior and market share data for ad networks, we evaluate metrics such as Ad Placement Eligibility, Low Competition Rate, and solo competitor. The findings reveal a noticeable impact on ad networks, with larger players strengthening their dominance and smaller networks facing challenges securing ad spaces and competing effectively. Moreover, the study explores the potential environmental implications of Google's actions, highlighting the need to carefully consider policy and regulatory measures to ensure fair competition and privacy protection. Overall, this research contributes valuable insights into the evolving dynamics of digital advertising and highlights the importance of balancing privacy with competition and innovation in the online advertising landscape."}, "https://arxiv.org/abs/2409.14311": {"title": "Interplay of Reward and Size of Groups in the Optional Public Goods Game", "link": "https://arxiv.org/abs/2409.14311", "description": "arXiv:2409.14311v1 Announce Type: new \nAbstract: The Optional Public Goods Game is a three-strategy game in which an individual can play as a cooperator or defector or decide not to participate. Despite its simplicity, this model can effectively represent many human social dilemmas, such as those found in the use of public services, environmental concerns, or other activities related to society. In this contribution, we present a comprehensive analysis of the conditions under which spontaneous, sustained cooperation emerges and the characteristics of these cooperative states. Through simulations, we demonstrate the conditions leading to the coexistence of the three strategies in a steady equilibrium or the alternate dominance of each strategy in a rock-paper-scissors fashion. The results identify each of the possible scenarios in terms of two key parameters: the multiplication rate of the public good game (reward) and the size of the group of potential players. We also discuss other details of the game that may influence the appearance of cycles, along with relevant characteristics of these cycles, such as the prevalence of cooperation."}, "https://arxiv.org/abs/2409.14975": {"title": "Unbiased third-party bots lead to a tradeoff between cooperation and social payoffs", "link": "https://arxiv.org/abs/2409.14975", "description": "arXiv:2409.14975v1 Announce Type: new \nAbstract: The rise of artificial intelligence (AI) offers new opportunities to influence cooperative dynamics with greater applicability and control. In this paper, we examine the impact of third-party bots--agents that do not directly participate in games but unbiasedly modify the payoffs of normal players engaged in prisoner's dilemma interactions--on the emergence of cooperation. Using an evolutionary simulation model, we demonstrate that unbiased bots are unable to shift the defective equilibrium among normal players in well-mixed populations. However, in structured populations, despite their unbiased actions, the bots spontaneously generate distinct impacts on cooperators and defectors, leading to enhanced cooperation. Notably, bots that apply negative influences are more effective at promoting cooperation than those applying positive ones, as fewer bots are needed to catalyze cooperative behavior among normal players. However, as the number of bots increases, a trade-off emerges: while cooperation is maintained, overall social payoffs decline. These findings highlight the need for careful management of AI's role in social systems, as even well-intentioned bots can have unintended consequences on collective outcomes."}, "https://arxiv.org/abs/2409.15122": {"title": "Promoting collective intelligence: The advantage of temporal star-structures", "link": "https://arxiv.org/abs/2409.15122", "description": "arXiv:2409.15122v1 Announce Type: new \nAbstract: System structures play an essential role in the emergence of collective intelligence in many natural and engineering systems. In empirical systems, interactions among multiple agents may change over time, forming a temporal network structure, where nodes represent the system's components and links capture who interacts with whom. Recent studies report that temporal networks are more conducive to the emergence of collective cooperation compared to their aggregated static structures. However, the question of which kind of structural characteristics of temporal networks promote collective cooperation still remains elusive. Here we systematically investigate the evolution of cooperation on temporal networks with diverse structural characteristics, such as random, star, and cluster structures. We uncover that temporal networks with single-star structures which lack network clusters are more conducive to collective cooperation than other structures. This counterintuitive result cautions against the common belief that network clusters normally facilitate collective cooperation, revealing the unique advantages of temporal networks over static networks. We further propose an index to quantify the capacity of structural characteristics of temporal networks in promoting collective cooperation. Our findings pave the way for designing the optimal structure of temporal networks to favour collective cooperation."}, "https://arxiv.org/abs/2409.15142": {"title": "Critical Node Detection in Temporal Social Networks, Based on Global and Semi-local Centrality Measures", "link": "https://arxiv.org/abs/2409.15142", "description": "arXiv:2409.15142v1 Announce Type: new \nAbstract: Nodes that play strategic roles in networks are called critical or influential nodes. For example, in an epidemic, we can control the infection spread by isolating critical nodes; in marketing, we can use certain nodes as the initial spreaders aiming to reach the largest part of the network, or they can be selected for removal in targeted attacks to maximise the fragmentation of the network. In this study, we focus on critical node detection in temporal networks. We propose three new measures to identify the critical nodes in temporal networks: the temporal supracycle ratio, temporal semi-local integration, and temporal semi-local centrality. We analyse the performance of these measures based on their effect on the SIR epidemic model in three scenarios: isolating the influential nodes when an epidemic happens, using the influential nodes as seeds of the epidemic, or removing them to analyse the robustness of the network. We compare the results with existing centrality measures, particularly temporal betweenness, temporal centrality, and temporal degree deviation. The results show that the introduced measures help identify influential nodes more accurately. The proposed methods can be used to detect nodes that need to be isolated to reduce the spread of an epidemic or as initial nodes to speedup dissemination of information."}, "https://arxiv.org/abs/2409.15148": {"title": "Bounded-confidence opinion models with random-time interactions", "link": "https://arxiv.org/abs/2409.15148", "description": "arXiv:2409.15148v1 Announce Type: new \nAbstract: In models of opinion dynamics, the opinions of individual agents evolve with time. One type of opinion model is a bounded-confidence model (BCM), in which opinions take continuous values and interacting agents compromise their opinions with each other if those opinions are sufficiently similar. In studies of BCMs, it is typically assumed that interactions between agents occur at deterministic times. This assumption neglects an inherent element of randomness in social systems. In this paper, we study BCMs on networks and allow agents to interact at random times. To incorporate random-time interactions, we use renewal processes to determine social interactions, which can follow arbitrary waiting-time distributions (WTDs). We establish connections between these random-time-interaction BCMs and deterministic-time-interaction BCMs. We find that BCMs with Markovian WTDs have consistent statistical properties on different networks but that the statistical properties of BCMs with non-Markovian WTDs depend on network structure."}, "https://arxiv.org/abs/2409.13700": {"title": "MAS4POI: a Multi-Agents Collaboration System for Next POI Recommendation", "link": "https://arxiv.org/abs/2409.13700", "description": "arXiv:2409.13700v1 Announce Type: cross \nAbstract: LLM-based Multi-Agent Systems have potential benefits of complex decision-making tasks management across various domains but their applications in the next Point-of-Interest (POI) recommendation remain underexplored. This paper proposes a novel MAS4POI system designed to enhance next POI recommendations through multi-agent interactions. MAS4POI supports Large Language Models (LLMs) specializing in distinct agents such as DataAgent, Manager, Analyst, and Navigator with each contributes to a collaborative process of generating the next POI recommendations.The system is examined by integrating six distinct LLMs and evaluated by two real-world datasets for recommendation accuracy improvement in real-world scenarios. Our code is available at https://github.com/yuqian2003/MAS4POI."}, "https://arxiv.org/abs/2409.13740": {"title": "Language agents achieve superhuman synthesis of scientific knowledge", "link": "https://arxiv.org/abs/2409.13740", "description": "arXiv:2409.13740v1 Announce Type: cross \nAbstract: Language models are known to produce incorrect information, and their accuracy and reliability for scientific research are still in question. We developed a detailed human-AI comparison method to evaluate language models on real-world literature search tasks, including information retrieval, summarization, and contradiction detection. Our findings show that PaperQA2, an advanced language model focused on improving factual accuracy, matches or outperforms subject matter experts on three realistic literature search tasks, with no restrictions on human participants (full internet access, search tools, and time). PaperQA2 generates cited, Wikipedia-style summaries of scientific topics that are significantly more accurate than current human-written Wikipedia entries. We also present LitQA2, a new benchmark for scientific literature research, which shaped the development of PaperQA2 and contributed to its superior performance. Additionally, PaperQA2 identifies contradictions in scientific literature, a challenging task for humans. It finds an average of 2.34 +/- 1.99 contradictions per paper in a random sample of biology papers, with 70% of these contradictions validated by human experts. These results show that language models can now surpass domain experts in important scientific literature tasks."}, "https://arxiv.org/abs/2409.14464": {"title": "AggregHate: An Efficient Aggregative Approach for the Detection of Hatemongers on Social Platforms", "link": "https://arxiv.org/abs/2409.14464", "description": "arXiv:2409.14464v1 Announce Type: cross \nAbstract: Automatic detection of online hate speech serves as a crucial step in the detoxification of the online discourse. Moreover, accurate classification can promote a better understanding of the proliferation of hate as a social phenomenon. While most prior work focus on the detection of hateful utterances, we argue that focusing on the user level is as important, albeit challenging. In this paper we consider a multimodal aggregative approach for the detection of hate-mongers, taking into account the potentially hateful texts, user activity, and the user network. We evaluate our methods on three unique datasets X (Twitter), Gab, and Parler showing that a processing a user's texts in her social context significantly improves the detection of hate mongers, compared to previously used text and graph-based methods. Our method can be then used to improve the classification of coded messages, dog-whistling, and racial gas-lighting, as well as inform intervention measures. Moreover, our approach is highly efficient even for very large datasets and networks."}, "https://arxiv.org/abs/2409.14596": {"title": "DarkGram: Exploring and Mitigating Cybercriminal content shared in Telegram channels", "link": "https://arxiv.org/abs/2409.14596", "description": "arXiv:2409.14596v1 Announce Type: cross \nAbstract: We present the first large scale analysis of 339 cybercriminal activity channels (CACs) on Telegram from February to May 2024. Collectively followed by over 23.8 million users, these channels shared a wide array of illicit content, including compromised credentials, pirated software and media, tools for blackhat hacking resources such as malware, social engineering scams, and exploit kits. We developed DarkGram, a BERT based framework that identifies malicious posts from the CACs with an accuracy of 96%, using which we conducted a quantitative analysis of 53,605 posts from these channels, revealing key characteristics of shared content. While much of this content is distributed for free, channel administrators frequently employ promotions and giveaways to engage users and boost the sales of premium cybercriminal content. These channels also pose significant risks to their own subscribers. Notably, 28.1% of shared links contained phishing attacks, and 38% of executable files were bundled with malware. Moreover, our qualitative analysis of replies in CACs shows how subscribers cultivate a dangerous sense of community through requests for illegal content, illicit knowledge sharing, and collaborative hacking efforts, while their reactions to posts, including emoji responses, further underscore their appreciation for such content. We also find that the CACs can evade scrutiny by quickly migrating to new channels with minimal subscriber loss, highlighting the resilience of this ecosystem. To counteract this, we further utilized DarkGram to detect new channels, reporting malicious content to Telegram and the affected organizations which resulted in the takedown of 196 such channels over three months. To aid further collaborative efforts in taking down these channels, we open source our dataset and the DarkGram framework."}, "https://arxiv.org/abs/2409.14659": {"title": "Image memorability enhances social media virality", "link": "https://arxiv.org/abs/2409.14659", "description": "arXiv:2409.14659v1 Announce Type: cross \nAbstract: Certain social media contents can achieve widespread virality. Prior research has identified that emotion and morality may play a role in this phenomenon. Yet, due to the variability in subjective perception of these factors, they may not consistently predict virality. Recent work in vision and memory has identified a property intrinsic to images - memorability - that can automatically drive human memory. Here, we present evidence that memorability can enhance social media virality by analyzing a naturalistic dataset from Reddit, a widely used social media platform. Specifically, we discover that more memorable images (as judged automatically by neural network ResMem) cause more comments and higher upvotes, and this effect replicates across three different timepoints. To uncover the mechanism of this effect, we employ natural language processing techniques finding that memorable images tend to evoke abstract and less emotional comments. Leveraging an object recognition neural network, we discover that memorable images result in comments directed to information external to the image, which causes them to be more abstract. Further analysis quantifying the representations within the ResMem neural network reveals that images with more semantically distinct features are more likely to be memorable, and consequently, more likely to go viral. These findings reveal that images that are easier to remember become more viral, offering new future directions such as the creation of predictive models of content virality or the application of these insights to enhance the design of impactful visual content."}, "https://arxiv.org/abs/2309.11188": {"title": "Rebellions and Impeachments in a Neural Network Society", "link": "https://arxiv.org/abs/2309.11188", "description": "arXiv:2309.11188v2 Announce Type: replace \nAbstract: Basede on a study of the modern presidencial democracies in South America, we present a statistical mechanics exploration of the collective, coordinated action of political actors in the legislative chamber that may result on the impeachment of the executive. By representing the legislative political actors with neurla networks, we observed that the larger the effective number of presidential-agenda items are treated, the smaller the chances for a cross-party dialogue, which, if combined with a decrement in the president's public approval rating, could trigger an impeachment process."}, "https://arxiv.org/abs/2312.14983": {"title": "Energy Justice and Equity: A Review of Definitions, Measures, and Practice in Policy, Planning, and Operations", "link": "https://arxiv.org/abs/2312.14983", "description": "arXiv:2312.14983v2 Announce Type: replace \nAbstract: Energy justice, at the intersection of energy and societal ethics, studies the origins, quantification, and resolution of persistent and potential inequities within the energy sector, serving as a foundational pillar for societal harmony. In this review, we overview the historical and modern definitions of energy equity and frameworks of energy justice. We highlight the tools adopted to measure equity in the energy context, unveiling multifaceted inequities that permeate global energy landscapes. We discuss the limitations of prevalent metrics such as the Gini coefficient and Generalized Entropy Indices in the evaluation of energy justice concerns. Finally, we analyze publications that examined current practices and proposed improving methods towards a more equitable energy market for the society from policy, planning, and operation perspectives."}, "https://arxiv.org/abs/2103.10952": {"title": "Asymmetry underlies stability in power grids", "link": "https://arxiv.org/abs/2103.10952", "description": "arXiv:2103.10952v2 Announce Type: replace-cross \nAbstract: Behavioral homogeneity is often critical for the functioning of network systems of interacting entities. In power grids, whose stable operation requires generator frequencies to be synchronized--and thus homogeneous--across the network, previous work suggests that the stability of synchronous states can be improved by making the generators homogeneous. Here, we show that a substantial additional improvement is possible by instead making the generators suitably heterogeneous. We develop a general method for attributing this counterintuitive effect to converse symmetry breaking, a recently established phenomenon in which the system must be asymmetric to maintain a stable symmetric state. These findings constitute the first demonstration of converse symmetry breaking in real-world systems, and our method promises to enable identification of this phenomenon in other networks whose functions rely on behavioral homogeneity."}, "https://arxiv.org/abs/2404.03116": {"title": "ALAAMEE: Open-source software for fitting autologistic actor attribute models", "link": "https://arxiv.org/abs/2404.03116", "description": "arXiv:2404.03116v2 Announce Type: replace-cross \nAbstract: The autologistic actor attribute model (ALAAM) is a model for social influence, derived from the more widely known exponential-family random graph model (ERGM). ALAAMs can be used to estimate parameters corresponding to multiple forms of social contagion associated with network structure and actor covariates. This work introduces ALAAMEE, open-source Python software for estimation, simulation, and goodness-of-fit testing for ALAAM models. ALAAMEE implements both the stochastic approximation and equilibrium expectation (EE) algorithms for ALAAM parameter estimation, including estimation from snowball sampled network data. It implements data structures and statistics for undirected, directed, and bipartite networks. We use a simulation study to assess the accuracy of the EE algorithm for ALAAM parameter estimation and statistical inference, and demonstrate the use of ALAAMEE with empirical examples using both small (fewer than 100 nodes) and large (more than 10 000 nodes) networks."}, "https://arxiv.org/abs/2409.15402": {"title": "Uncovering Coordinated Cross-Platform Information Operations Threatening the Integrity of the 2024 U", "link": "https://arxiv.org/abs/2409.15402", "description": "arXiv:2409.15402v1 Announce Type: new \nAbstract: Information Operations (IOs) pose a significant threat to the integrity of democratic processes, with the potential to influence election-related online discourse. In anticipation of the 2024 U.S. presidential election, we present a study aimed at uncovering the digital traces of coordinated IOs on $\\mathbb{X}$ (formerly Twitter). Using our machine learning framework for detecting online coordination, we analyze a dataset comprising election-related conversations on $\\mathbb{X}$ from May 2024. This reveals a network of coordinated inauthentic actors, displaying notable similarities in their link-sharing behaviors. Our analysis shows concerted efforts by these accounts to disseminate misleading, redundant, and biased information across the Web through a coordinated cross-platform information operation: The links shared by this network frequently direct users to other social media platforms or suspicious websites featuring low-quality political content and, in turn, promoting the same $\\mathbb{X}$ and YouTube accounts. Members of this network also shared deceptive images generated by AI, accompanied by language attacking political figures and symbolic imagery intended to convey power and dominance. While $\\mathbb{X}$ has suspended a subset of these accounts, more than 75% of the coordinated network remains active. Our findings underscore the critical role of developing computational models to scale up the detection of threats on large social media platforms, and emphasize the broader implications of these techniques to detect IOs across the wider Web."}, "https://arxiv.org/abs/2409.15605": {"title": "Coexistence of positive and negative information in information-epidemic dynamics on multiplex networks", "link": "https://arxiv.org/abs/2409.15605", "description": "arXiv:2409.15605v1 Announce Type: new \nAbstract: This paper investigates the coexistence of positive and negative information in the context of information-epidemic dynamics on multiplex networks. In accordance with the tenets of mean field theory, we present not only the analytic solution of the prevalence threshold, but also the coexistence conditions of two distinct forms of information (i.e., the two phase transition points at which a single form of information becomes extinct). In regions where multiple forms of information coexist, two completely distinct patterns emerge: monotonic and non-monotonic. The physical mechanisms that give rise to these different patterns have also been elucidated. The theoretical results are robust with regard to the network structure and show a high degree of agreement with the findings of the Monte Carlo simulation."}, "https://arxiv.org/abs/2409.16163": {"title": "The anonymization problem in social networks", "link": "https://arxiv.org/abs/2409.16163", "description": "arXiv:2409.16163v1 Announce Type: new \nAbstract: In this paper we introduce a general version of the anonymization problem in social networks, in which the goal is to maximize the number of anonymous nodes by altering a given graph. We define three variants of this optimization problem, being full, partial and budgeted anonymization. In each, the objective is to maximize the number of k-anonymous nodes, i.e., nodes for which there are at least k-1 equivalent nodes, according to a particular anonymity measure of structural node equivalence. We propose six new heuristic algorithms for solving the anonymization problem which we implement into the reusable ANO-NET computational framework. As a baseline, we use an edge sampling method introduced in previous work. Experiments on both graph models and 17 real-world network datasets result in three empirical findings. First, we demonstrate that edge deletion is the most effective graph alteration operation. Second, we compare four commonly used anonymity measures from the literature and highlight how the choice of anonymity measure has a tremendous effect on both the achieved anonymity as well as the difficulty of solving the anonymization problem. Third, we find that the proposed algorithms that preferentially delete edges with a larger effect on nodes at a structurally unique position consistently outperform heuristics solely based on network structure. With similar runtimes, our algorithms retain on average 17 times more edges, ensuring higher data utility after full anonymization. In the budgeted variant, they achieve 4.4 times more anonymous nodes than the baseline. This work lays important foundations for future development of algorithms for anonymizing social networks."}, "https://arxiv.org/abs/2409.15333": {"title": "Fractional and fractal extensions of epidemiological models", "link": "https://arxiv.org/abs/2409.15333", "description": "arXiv:2409.15333v1 Announce Type: cross \nAbstract: One way to study the spread of disease is through mathematical models. The most successful models compartmentalize the host population according to their infectious stage, e.g., susceptible (S), infected (I), exposed (E), and recovered (R). The composition of these compartments leads to the SI, SIS, SIR, and SEIR models. In this Chapter, we present and compare three formulations of SI, SIS, SIR, and SEIR models in the framework of standard (integer operators), fractional (Caputo sense), and fractal derivatives (Hausdorff sense). As an application of the SI model, we study the evolution of AIDS cases in Bangladesh from 2001 to 2021. For this case, our simulations suggest that fractal formulation describes the data well. For the SIS model, we consider syphilis data from Brazil from 2006 to 2017. In this case, the three frameworks describe the data with good accuracy. We used data from Influenza A to adjust the SIR model in previous approaches and observed that the fractional formulation was better. The last application considers the COVID-19 data from India in the range 2020-04-10 to 2020-12-31 to adjust the parameters of the SEIR model. The standard formulation fits the data better than the other approaches. As a common result, all models exhibit steady solutions in the different formulations. The time to reach a steady solution is correlated to the considered approach. The standard and fractal formulations reach the steady state earlier when compared with the fractional formulation."}, "https://arxiv.org/abs/2409.15369": {"title": "Geometric Relational Embeddings", "link": "https://arxiv.org/abs/2409.15369", "description": "arXiv:2409.15369v1 Announce Type: cross \nAbstract: Relational representation learning transforms relational data into continuous and low-dimensional vector representations. However, vector-based representations fall short in capturing crucial properties of relational data that are complex and symbolic. We propose geometric relational embeddings, a paradigm of relational embeddings that respect the underlying symbolic structures. Specifically, this dissertation introduces various geometric relational embedding models capable of capturing: 1) complex structured patterns like hierarchies and cycles in networks and knowledge graphs; 2) logical structures in ontologies and logical constraints applicable for constraining machine learning model outputs; and 3) high-order structures between entities and relations. Our results obtained from benchmark and real-world datasets demonstrate the efficacy of geometric relational embeddings in adeptly capturing these discrete, symbolic, and structured properties inherent in relational data."}, "https://arxiv.org/abs/2409.15652": {"title": "English offensive text detection using CNN based Bi-GRU model", "link": "https://arxiv.org/abs/2409.15652", "description": "arXiv:2409.15652v1 Announce Type: cross \nAbstract: Over the years, the number of users of social media has increased drastically. People frequently share their thoughts through social platforms, and this leads to an increase in hate content. In this virtual community, individuals share their views, express their feelings, and post photos, videos, blogs, and more. Social networking sites like Facebook and Twitter provide platforms to share vast amounts of content with a single click. However, these platforms do not impose restrictions on the uploaded content, which may include abusive language and explicit images unsuitable for social media. To resolve this issue, a new idea must be implemented to divide the inappropriate content. Numerous studies have been done to automate the process. In this paper, we propose a new Bi-GRU-CNN model to classify whether the text is offensive or not. The combination of the Bi-GRU and CNN models outperforms the existing model"}, "https://arxiv.org/abs/2409.15698": {"title": "GraphGI:A GNN Explanation Method using Game Interaction", "link": "https://arxiv.org/abs/2409.15698", "description": "arXiv:2409.15698v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs) have garnered significant attention and have been extensively utilized across various domains. However, similar to other deep learning models, GNNs are often viewed as black-box models, making it challenging to interpret their prediction mechanisms. Current graph explanation techniques focus on identifying key nodes or edges, attributing the critical data features that drive model predictions. Nevertheless, these features do not independently influence the model's outcomes; rather, they interact with one another to collectively affect predictions. In this work, we propose a novel explanatory method GraphGI, which identifies the coalition with the highest interaction strength and presents it as an explanatory subgraph. Given a trained model and an input graph, our method explains predictions by gradually incorporating significant edges into the selected subgraph. We utilize game-theoretic interaction values to assess the interaction strength after edge additions, ensuring that the newly added edges confer maximum interaction strength to the explanatory subgraph. To enhance computational efficiency, we adopt effective approximation techniques for calculating Shapley values and game-theoretic interaction values. Empirical evaluations demonstrate that our method achieves superior fidelity and sparsity, maintaining the interpretability of the results at a comprehensible level."}, "https://arxiv.org/abs/2409.15754": {"title": "NFTracer: Tracing NFT Impact Dynamics in Transaction-flow Substitutive Systems with Visual Analytics", "link": "https://arxiv.org/abs/2409.15754", "description": "arXiv:2409.15754v1 Announce Type: cross \nAbstract: Impact dynamics are crucial for estimating the growth patterns of NFT projects by tracking the diffusion and decay of their relative appeal among stakeholders. Machine learning methods for impact dynamics analysis are incomprehensible and rigid in terms of their interpretability and transparency, whilst stakeholders require interactive tools for informed decision-making. Nevertheless, developing such a tool is challenging due to the substantial, heterogeneous NFT transaction data and the requirements for flexible, customized interactions. To this end, we integrate intuitive visualizations to unveil the impact dynamics of NFT projects. We first conduct a formative study and summarize analysis criteria, including substitution mechanisms, impact attributes, and design requirements from stakeholders. Next, we propose the Minimal Substitution Model to simulate substitutive systems of NFT projects that can be feasibly represented as node-link graphs. Particularly, we utilize attribute-aware techniques to embed the project status and stakeholder behaviors in the layout design. Accordingly, we develop a multi-view visual analytics system, namely NFTracer, allowing interactive analysis of impact dynamics in NFT transactions. We demonstrate the informativeness, effectiveness, and usability of NFTracer by performing two case studies with domain experts and one user study with stakeholders. The studies suggest that NFT projects featuring a higher degree of similarity are more likely to substitute each other. The impact of NFT projects within substitutive systems is contingent upon the degree of stakeholders' influx and projects' freshness."}, "https://arxiv.org/abs/2404.05034": {"title": "Tournament design: A review from an operational research perspective", "link": "https://arxiv.org/abs/2404.05034", "description": "arXiv:2404.05034v2 Announce Type: replace \nAbstract: Every sport needs rules. Tournament design refers to the rules that determine how a tournament, a series of games between a number of competitors, is organized. This study aims to provide an overview of the tournament design literature from the perspective of operational research. Three important design criteria are discussed: efficacy, fairness, and attractiveness. Our survey classifies the papers discussing these properties according to the main components of tournament design: format, seeding, draw, scheduling, and ranking. We also outline several open questions and promising directions for future research."}, "https://arxiv.org/abs/2211.04352": {"title": "Emergent Strategies for Shepherding a Flock", "link": "https://arxiv.org/abs/2211.04352", "description": "arXiv:2211.04352v3 Announce Type: replace-cross \nAbstract: We investigate how a shepherd should move to effectively herd a flock towards a target. Using an agent-based (ABM) and a coarse-grained (ODE) model for the flock, we pose and solve for the optimal strategy of a shepherd that must keep the flock cohesive and coerce it towards a target. Three distinct strategies emerge naturally as a function of the scaled herd size {and} the scaled shepherd speed: (i) mustering, where the shepherd circles the herd to ensure compactness, (ii) droving, where the shepherd chases the herd in a desired direction while sweeping back and forth, and (iii) driving, where the flock surrounds a shepherd that drives it from within. A minimal dynamical model for the size, shape, and position of the herd captures the effective behavior of the ABM and further allows us to characterize the different herding strategies in terms of the behavior of the shepherd that librates (mustering), oscillates (droving), or moves steadily (driving)."}, "https://arxiv.org/abs/2403.01121": {"title": "OpenGraph: Towards Open Graph Foundation Models", "link": "https://arxiv.org/abs/2403.01121", "description": "arXiv:2403.01121v3 Announce Type: replace-cross \nAbstract: Graph learning has become indispensable for interpreting and harnessing relational data in diverse fields, ranging from recommendation systems to social network analysis. In this context, a variety of GNNs have emerged as promising methodologies for encoding the structural information of graphs. By effectively capturing the graph's underlying structure, these GNNs have shown great potential in enhancing performance in graph learning tasks, such as link prediction and node classification. However, despite their successes, a significant challenge persists: these advanced methods often face difficulties in generalizing to unseen graph data that significantly differs from the training instances. In this work, our aim is to advance the graph learning paradigm by developing a general graph foundation model. This model is designed to understand the complex topological patterns present in diverse graph data, enabling it to excel in zero-shot graph learning tasks across different downstream datasets. To achieve this goal, we address several key technical challenges in our OpenGraph model. Firstly, we propose a unified graph tokenizer to adapt our graph model to generalize well on unseen graph data, even when the underlying graph properties differ significantly from those encountered during training. Secondly, we develop a scalable graph transformer as the foundational encoder, which effectively captures node-wise dependencies within the global topological context. Thirdly, we introduce a data augmentation mechanism enhanced by a LLM to alleviate the limitations of data scarcity in real-world scenarios. Extensive experiments validate the effectiveness of our framework. By adapting our OpenGraph to new graph characteristics and comprehending the nuances of diverse graphs, our approach achieves remarkable zero-shot graph learning performance across various settings and domains."}, "https://arxiv.org/abs/2403.15291": {"title": "Wastewater-based Epidemiology for COVID-19 Surveillance and Beyond: A Survey", "link": "https://arxiv.org/abs/2403.15291", "description": "arXiv:2403.15291v2 Announce Type: replace-cross \nAbstract: The pandemic of COVID-19 has imposed tremendous pressure on public health systems and social economic ecosystems over the past years. To alleviate its social impact, it is important to proactively track the prevalence of COVID-19 within communities. The traditional way to estimate the disease prevalence is to estimate from reported clinical test data or surveys. However, the coverage of clinical tests is often limited and the tests can be labor-intensive, requires reliable and timely results, and consistent diagnostic and reporting criteria. Recent studies revealed that patients who are diagnosed with COVID-19 often undergo fecal shedding of SARS-CoV-2 virus into wastewater, which makes wastewater-based epidemiology for COVID-19 surveillance a promising approach to complement traditional clinical testing. In this paper, we survey the existing literature regarding wastewater-based epidemiology for COVID-19 surveillance and summarize the current advances in the area. Specifically, we have covered the key aspects of wastewater sampling, sample testing, and presented a comprehensive and organized summary of wastewater data analytical methods. Finally, we provide the open challenges on current wastewater-based COVID-19 surveillance studies, aiming to encourage new ideas to advance the development of effective wastewater-based surveillance systems for general infectious diseases."}, "https://arxiv.org/abs/2409.16343": {"title": "Opinion polarisation in social networks driven by cognitive dissonance avoidance", "link": "https://arxiv.org/abs/2409.16343", "description": "arXiv:2409.16343v1 Announce Type: new \nAbstract: As the consequences of opinion polarization effect our everyday life in more and more aspect, the understanding of its origins and driving forces becomes increasingly important. Here we develop an agent-based network model with realistic human traits: individuals in our simulations are endowed with an internal belief system which they attempt to keep as coherent as possible. This desire -- to reassure existing attitudes while avoiding cognitive dissonance -- is one of the most influential and widely accepted theories in social psychology by now. Our model shows that even in networks that start out completely uniform (from a society of clones), this effort leads to fragmentation and polarization, reflected both by the individual beliefs (attitudes) and the emerging communities in the social network. By fine-tuning two parameters: (i) \"dissonance penalty\", measuring the strength with which agents attempt to avoid cognitive dissonance, and (ii) \"triadic closure affinity\", the parameter reflecting agents' likelihood to connect with friends of friends, a wide range of possible community structures are observed."}, "https://arxiv.org/abs/2409.16558": {"title": "Bias Reduction in Social Networks through Agent-Based Simulations", "link": "https://arxiv.org/abs/2409.16558", "description": "arXiv:2409.16558v1 Announce Type: new \nAbstract: Online social networks use recommender systems to suggest relevant information to their users in the form of personalized timelines. Studying how these systems expose people to information at scale is difficult to do as one cannot assume each user is subject to the same timeline condition and building appropriate evaluation infrastructure is costly. We show that a simple agent-based model where users have fixed preferences affords us the ability to compare different recommender systems (and thus different personalized timelines) in their ability to skew users' perception of their network. Importantly, we show that a simple greedy algorithm that constructs a feed based on network properties reduces such perception biases comparable to a random feed. This underscores the influence network structure has in determining the effectiveness of recommender systems in the social network context and offers a tool for mitigating perception biases through algorithmic feed construction."}, "https://arxiv.org/abs/2409.16671": {"title": "Wildlife Product Trading in Online Social Networks: A Case Study on Ivory-Related Product Sales Promotion Posts", "link": "https://arxiv.org/abs/2409.16671", "description": "arXiv:2409.16671v1 Announce Type: new \nAbstract: Wildlife trafficking (WLT) has emerged as a global issue, with traffickers expanding their operations from offline to online platforms, utilizing e-commerce websites and social networks to enhance their illicit trade. This paper addresses the challenge of detecting and recognizing wildlife product sales promotion behaviors in online social networks, a crucial aspect in combating these environmentally harmful activities. To counter these environmentally damaging illegal operations, in this research, we focus on wildlife product sales promotion behaviors in online social networks. Specifically, 1) A scalable dataset related to wildlife product trading is collected using a network-based approach. This dataset is labeled through a human-in-the-loop machine learning process, distinguishing positive class samples containing wildlife product selling posts and hard-negatives representing normal posts misclassified as potential WLT posts, subsequently corrected by human annotators. 2) We benchmark the machine learning results on the proposed dataset and build a practical framework that automatically identifies suspicious wildlife selling posts and accounts, sufficiently leveraging the multi-modal nature of online social networks. 3) This research delves into an in-depth analysis of trading posts, shedding light on the systematic and organized selling behaviors prevalent in the current landscape. We provide detailed insights into the nature of these behaviors, contributing valuable information for understanding and countering illegal wildlife product trading."}, "https://arxiv.org/abs/2409.16478": {"title": "Algorithmic Drift: A Simulation Framework to Study the Effects of Recommender Systems on User Preferences", "link": "https://arxiv.org/abs/2409.16478", "description": "arXiv:2409.16478v1 Announce Type: cross \nAbstract: Digital platforms such as social media and e-commerce websites adopt Recommender Systems to provide value to the user. However, the social consequences deriving from their adoption are still unclear. Many scholars argue that recommenders may lead to detrimental effects, such as bias-amplification deriving from the feedback loop between algorithmic suggestions and users' choices. Nonetheless, the extent to which recommenders influence changes in users leaning remains uncertain. In this context, it is important to provide a controlled environment for evaluating the recommendation algorithm before deployment. To address this, we propose a stochastic simulation framework that mimics user-recommender system interactions in a long-term scenario. In particular, we simulate the user choices by formalizing a user model, which comprises behavioral aspects, such as the user resistance towards the recommendation algorithm and their inertia in relying on the received suggestions. Additionally, we introduce two novel metrics for quantifying the algorithm's impact on user preferences, specifically in terms of drift over time. We conduct an extensive evaluation on multiple synthetic datasets, aiming at testing the robustness of our framework when considering different scenarios and hyper-parameters setting. The experimental results prove that the proposed methodology is effective in detecting and quantifying the drift over the users preferences by means of the simulation. All the code and data used to perform the experiments are publicly available."}, "https://arxiv.org/abs/2409.16943": {"title": "Divergence asymmetry and connected components in a general duplication-divergence graph model", "link": "https://arxiv.org/abs/2409.16943", "description": "arXiv:2409.16943v1 Announce Type: cross \nAbstract: This Letter introduces a generalization of known duplication-divergence models for growing random graphs. This general model includes a coupled divergence asymmetry rate, which allows to obtain, for the first time, the structure of random growing networks by duplication and divergence in a continuous range of configurations between complete asymmetric divergence -- divergence rates affect only edges emanating from one of the duplicate vertices -- and symmetric divergence -- divergence rates affect equiprobably both the original and the copy vertex. Multiple connected sub-graphs (of order greater than one) emerge as the divergence asymmetry rate slightly moves from the complete asymmetric divergence case. Mean-field results of priorly published models are nicely reproduced by this general model. Moreover, in special cases, the connected sub-graph size distribution $C_s$ of networks grown by this model suggests a power-law scaling of the form $C_s \\sim s^{-\\lambda}$ for $s>1$, e.g., with $\\lambda \\approx 5/3$ for divergence rate $\\delta \\approx 0.7$."}, "https://arxiv.org/abs/2108.02091": {"title": "Capturing Tie Strength with Algebraic Topology", "link": "https://arxiv.org/abs/2108.02091", "description": "arXiv:2108.02091v3 Announce Type: replace \nAbstract: The association between tie strength and social structure is a fundamental topic in the social sciences. We study this association by analyzing tie strength in higher-order networks, an increasingly relevant model which can encode group interactions between three or more individuals. First, we introduce three measures based on algebraic topology which characterize the network context and influence of an edge. Our experimental results across 15 datasets indicate that these measures outperform standard network proxies in estimating tie strength. We further find that these measures can replicate and explain a puzzle wherein certain bridging ties are surprisingly strong. We then consider a single centrality measure which combines the three initial measures, is highly inversely related to tie strength, and can be interpreted through an information exchange process which highlights ties that have access to useful information. In this sense, we are able to illuminate the information advantages of weak ties due to their network position."}, "https://arxiv.org/abs/2409.17186": {"title": "Don't Trust A Single Gerrymandering Metric", "link": "https://arxiv.org/abs/2409.17186", "description": "arXiv:2409.17186v1 Announce Type: new \nAbstract: In recent years, in an effort to promote fairness in the election process, a wide variety of techniques and metrics have been proposed to determine whether a map is a partisan gerrymander. The most accessible measures, requiring easily obtained data, are metrics such as the Mean-Median Difference, Efficiency Gap, Declination, and GEO metric. But for most of these metrics, researchers have struggled to describe, given no additional information, how a value of that metric on a single map indicates the presence or absence of gerrymandering.\n  Our main result is that each of these metrics is gameable when used as a single, isolated quantity to detect gerrymandering (or the lack thereof). That is, for each of the four metrics, we can find district plans for a given state with an extremely large number of Democratic-won (or Republican-won) districts while the metric value of that plan falls within a reasonable, predetermined bound. We do this by using a hill-climbing method to generate district plans that are constrained by the bounds on the metric but also maximize or nearly maximize the number of districts won by a party.\n  In addition, extreme values of the Mean-Median Difference do not necessarily correspond to maps with an extreme number of districts won. Thus, the Mean- Median Difference metric is particularly misleading, as it cannot distinguish more extreme maps from less extreme maps. The other metrics are more nuanced, but when assessed on an ensemble, none perform substantially differently from simply measuring number of districts won by a fixed party.\n  One clear consequence of these results is that they demonstrate the folly of specifying a priori bounds on a metric that a redistricting commission must meet in order to avoid gerrymandering."}, "https://arxiv.org/abs/2409.17352": {"title": "On the Interplay of Clustering and Evolution in the Emergence of Epidemic Outbreaks", "link": "https://arxiv.org/abs/2409.17352", "description": "arXiv:2409.17352v1 Announce Type: new \nAbstract: In an increasingly interconnected world, a key scientific challenge is to examine mechanisms that lead to the widespread propagation of contagions, such as misinformation and pathogens, and identify risk factors that can trigger large-scale outbreaks. Underlying both the spread of disease and misinformation epidemics is the evolution of the contagion as it propagates, leading to the emergence of different strains, e.g., through genetic mutations in pathogens and alterations in the information content. Recent studies have revealed that models that do not account for heterogeneity in transmission risks associated with different strains of the circulating contagion can lead to inaccurate predictions. However, existing results on multi-strain spreading assume that the network has a vanishingly small clustering coefficient, whereas clustering is widely known to be a fundamental property of real-world social networks. In this work, we investigate spreading processes that entail evolutionary adaptations on random graphs with tunable clustering and arbitrary degree distributions. We derive a mathematical framework to quantify the epidemic characteristics of a contagion that evolves as it spreads, with the structure of the underlying network as given via arbitrary {\\em joint} degree distributions of single-edges and triangles. To the best of our knowledge, our work is the first to jointly analyze the impact of clustering and evolution on the emergence of epidemic outbreaks. We supplement our theoretical finding with numerical simulations and case studies, shedding light on the impact of clustering on contagion spread."}, "https://arxiv.org/abs/2409.17425": {"title": "Website visits can predict angler presence using machine learning", "link": "https://arxiv.org/abs/2409.17425", "description": "arXiv:2409.17425v1 Announce Type: new \nAbstract: Understanding and predicting recreational fishing activity is important for sustainable fisheries management. However, traditional methods of measuring fishing pressure, such as surveys, can be costly and limited in both time and spatial extent. Predictive models that relate fishing activity to environmental or economic factors typically rely on historical data, which often restricts their spatial applicability due to data scarcity. In this study, high-resolution angler-generated data from an online platform and easily accessible auxiliary data were tested to predict daily boat presence and aerial counts of boats at almost 200 lakes over five years in Ontario, Canada. Lake-information website visits alone enabled predicting daily angler boat presence with 78% accuracy. While incorporating additional environmental, socio-ecological, weather and angler-generated features into machine learning models did not remarkably improve prediction performance of boat presence, they were substantial for the prediction of boat counts. Models achieved an R2 of up to 0.77 at known lakes included in the model training, but they performed poorly for unknown lakes (R2 = 0.21). The results demonstrate the value of integrating angler-generated data from online platforms into predictive models and highlight the potential of machine learning models to enhance fisheries management."}, "https://arxiv.org/abs/2409.17573": {"title": "Planned behavior, perceptual biases, and the dynamics of collective action", "link": "https://arxiv.org/abs/2409.17573", "description": "arXiv:2409.17573v1 Announce Type: new \nAbstract: Many classical models of collective behavior assume that emergent dynamics result from external and observable interactions among individuals. However, how collective dynamics in human populations depend on the internal psychological processes of individuals remains underexplored. Here, we develop a mathematical model to investigate the effects of internal psychology on the dynamics of collective action. Our model is grounded in the theory of planned behavior -- a well-established conceptual framework in social psychology that links intrinsic beliefs to behavior. By incorporating temporal biases in social perception and individual differences in decision-making processes into our model, we find that the interplay between internal and external drivers of behavior can produce diverse outcomes, ranging from partial participation in collective action to rapid or delayed cascades of action-taking. These distinct outcomes are preceded by transient dynamics that are qualitatively similar to one another, which, just as in real-world scenarios, makes it difficult to predict long-term collective dynamics from early observations. Our model thus provides a useful test bed for methods that aim to predict the emergence of collective action, and it lays the groundwork for studying the nuanced dynamics of collective human behavior arising from the interaction between psychological processes and observable actions."}, "https://arxiv.org/abs/2409.17637": {"title": "Intervention strategies for misinformation sharing on social media: A bibliometric analysis", "link": "https://arxiv.org/abs/2409.17637", "description": "arXiv:2409.17637v1 Announce Type: new \nAbstract: Widely distributed misinformation shared across social media channels is a pressing issue that poses a significant threat to many aspects of society's well-being. Inaccurate shared information causes confusion, can adversely affect mental health, and can lead to mis-informed decision-making. Therefore, it is important to implement proactive measures to intervene and curb the spread of misinformation where possible. This has prompted scholars to investigate a variety of intervention strategies for misinformation sharing on social media. This study explores the typology of intervention strategies for addressing misinformation sharing on social media, identifying 4 important clusters - cognition-based, automated-based, information-based, and hybrid-based. The literature selection process utilized the PRISMA method to ensure a systematic and comprehensive analysis of relevant literature while maintaining transparency and reproducibility. A total of 139 articles published from 2013-2023 were then analyzed. Meanwhile, bibliometric analyses were conducted using performance analysis and science mapping techniques for the typology development. A comparative analysis of the typology was conducted to reveal patterns and evolution in the field. This provides valuable insights for both theory and practical applications. Overall, the study concludes that scholarly contributions to scientific research and publication help to address research gaps and expand knowledge in this field. Understanding the evolution of intervention strategies for misinformation sharing on social media can support future research that contributes to the development of more effective and sustainable solutions to this persistent problem."}, "https://arxiv.org/abs/2409.17669": {"title": "Impact of opinion formation phenomena in epidemic dynamics: kinetic modeling on networks", "link": "https://arxiv.org/abs/2409.17669", "description": "arXiv:2409.17669v1 Announce Type: new \nAbstract: After the recent COVID-19 outbreaks, it became increasingly evident that individuals' thoughts and beliefs can have a strong impact on disease transmission. It becomes therefore important to understand how information and opinions on protective measures evolve during epidemics. To this end, incorporating the impact of social media is essential to take into account the hierarchical structure of these platforms. In this context, we present a novel approach to take into account the interplay between infectious disease dynamics and socially-structured opinion dynamics. Our work extends a conventional compartmental framework including behavioral attitudes in shaping public opinion and promoting the adoption of protective measures under the influence of different degrees of connectivity. The proposed approach is capable to reproduce the emergence of epidemic waves. Specifically, it provides a clear link between the social influence of highly connected individuals and the epidemic dynamics. Through a heterogeneity of numerical tests we show how this comprehensive framework offers a more nuanced understanding of epidemic dynamics in the context of modern information dissemination and social behavior."}, "https://arxiv.org/abs/2409.17156": {"title": "An Art-centric perspective on AI-based content moderation of nudity", "link": "https://arxiv.org/abs/2409.17156", "description": "arXiv:2409.17156v1 Announce Type: cross \nAbstract: At a time when the influence of generative Artificial Intelligence on visual arts is a highly debated topic, we raise the attention towards a more subtle phenomenon: the algorithmic censorship of artistic nudity online. We analyze the performance of three \"Not-Safe-For-Work'' image classifiers on artistic nudity, and empirically uncover the existence of a gender and a stylistic bias, as well as evident technical limitations, especially when only considering visual information. Hence, we propose a multi-modal zero-shot classification approach that improves artistic nudity classification. From our research, we draw several implications that we hope will inform future research on this topic."}, "https://arxiv.org/abs/2409.17386": {"title": "Beyond Redundancy: Information-aware Unsupervised Multiplex Graph Structure Learning", "link": "https://arxiv.org/abs/2409.17386", "description": "arXiv:2409.17386v1 Announce Type: cross \nAbstract: Unsupervised Multiplex Graph Learning (UMGL) aims to learn node representations on various edge types without manual labeling. However, existing research overlooks a key factor: the reliability of the graph structure. Real-world data often exhibit a complex nature and contain abundant task-irrelevant noise, severely compromising UMGL's performance. Moreover, existing methods primarily rely on contrastive learning to maximize mutual information across different graphs, limiting them to multiplex graph redundant scenarios and failing to capture view-unique task-relevant information. In this paper, we focus on a more realistic and challenging task: to unsupervisedly learn a fused graph from multiple graphs that preserve sufficient task-relevant information while removing task-irrelevant noise. Specifically, our proposed Information-aware Unsupervised Multiplex Graph Fusion framework (InfoMGF) uses graph structure refinement to eliminate irrelevant noise and simultaneously maximizes view-shared and view-unique task-relevant information, thereby tackling the frontier of non-redundant multiplex graph. Theoretical analyses further guarantee the effectiveness of InfoMGF. Comprehensive experiments against various baselines on different downstream tasks demonstrate its superior performance and robustness. Surprisingly, our unsupervised method even beats the sophisticated supervised approaches. The source code and datasets are available at https://github.com/zxlearningdeep/InfoMGF."}, "https://arxiv.org/abs/2409.17495": {"title": "Human Mobility Modeling with Limited Information via Large Language Models", "link": "https://arxiv.org/abs/2409.17495", "description": "arXiv:2409.17495v1 Announce Type: cross \nAbstract: Understanding human mobility patterns has traditionally been a complex challenge in transportation modeling. Due to the difficulties in obtaining high-quality training datasets across diverse locations, conventional activity-based models and learning-based human mobility modeling algorithms are particularly limited by the availability and quality of datasets. Furthermore, current research mainly focuses on the spatial-temporal travel pattern but lacks an understanding of the semantic information between activities, which is crucial for modeling the interdependence between activities. In this paper, we propose an innovative Large Language Model (LLM) empowered human mobility modeling framework. Our proposed approach significantly reduces the reliance on detailed human mobility statistical data, utilizing basic socio-demographic information of individuals to generate their daily mobility patterns. We have validated our results using the NHTS and SCAG-ABM datasets, demonstrating the effective modeling of mobility patterns and the strong adaptability of our framework across various geographic locations."}, "https://arxiv.org/abs/2409.17952": {"title": "Participatory design: A systematic review and insights for future practice", "link": "https://arxiv.org/abs/2409.17952", "description": "arXiv:2409.17952v1 Announce Type: cross \nAbstract: Participatory Design -- an iterative, flexible design process that uses the close involvement of stakeholders, most often end users -- is growing in use across design disciplines. As an increasing number of practitioners turn to Participatory Design (PD), it has become less rigidly defined, with stakeholders engaged to varying degrees through the use of disjointed techniques. This ambiguous understanding can be counterproductive when discussing PD processes. Our findings synthesize key decisions and approaches from design peers that can support others in engaging in PD practice. We investigated how scholars report the use of Participatory Design in the field through a systematic literature review. We found that a majority of PD literature examined specific case studies of PD (53 of 88 articles), with the design of intangible systems representing the most common design context (61 of 88 articles). Stakeholders most often participated throughout multiple stages of a design process (65 of 88 articles), recruited in a variety of ways and engaged in several of the 14 specific participatory techniques identified. This systematic review provides today's practitioners synthesized learnings from past Participatory Design processes to inform and improve future use of PD, attempting to remedy inequitable design by engaging directly with stakeholders and users."}, "https://arxiv.org/abs/2409.18094": {"title": "Mobility in Age-Based Gossip Networks", "link": "https://arxiv.org/abs/2409.18094", "description": "arXiv:2409.18094v1 Announce Type: cross \nAbstract: We consider a gossiping network where a source forwards updates to a set of $n$ gossiping nodes that are placed in an arbitrary graph structure and gossip with their neighbors. In this paper, we analyze how mobility of nodes affects the freshness of nodes in the gossiping network. To model mobility, we let nodes randomly exchange positions with other nodes in the network. The position of the node determines how the node interacts with the rest of the network. In order to quantify information freshness, we use the version age of information metric. We use the stochastic hybrid system (SHS) framework to derive recursive equations to find the version age for a set of positions in the network in terms of the version ages of sets of positions that are one larger or of the same size. We use these recursive equations to find an upper bound for the average version age of a node in two example networks. We show that mobility can decrease the version age of nodes in a disconnected network from linear scaling in $n$ to at most square root scaling and even to constant scaling in some cases. We perform numerical simulations to analyze how mobility affects the version age of different positions in the network and also show that the upper bounds obtained for the example networks are tight."}, "https://arxiv.org/abs/2205.15541": {"title": "Disruption and recovery of the US domestic airline networks during the COVID-19 pandemic", "link": "https://arxiv.org/abs/2205.15541", "description": "arXiv:2205.15541v3 Announce Type: replace \nAbstract: The COVID-19 pandemic has had serious impacts on the airline industry. Ensuring that aviation policies in emergent situations both guarantee network connectivity and maintain competition among airlines is crucial in these circumstances. To this end, we aimed to understand the network dynamics of individual airlines. In this study, we quantitatively reveal the day-to-day dynamics of these US domestic airline networks, comprising 17 airlines, from January 2019 to December 2021. Specifically, we applied a framework for analyzing temporal networks, in which the network structure changes over time. We found that, first, even though the number of nodes and edges returned to pre-pandemic levels around July 2021, the structure of the entire US domestic airline network remained altered. We also found that the network dynamics varied significantly from airline to airline. Full-service carriers were less flexible in changing their network structure and suffered higher revenue losses. On the contrary, most regional carriers completely shifted to a new structure, which may have contributed to reducing their revenue losses. Low-cost carriers were characterized by more pronounced differences between airlines and drastically changed their network structure immediately after the declaration of a national emergency. Finally, we also examined the recovery process and found that the flights connecting airports that are more central and share more common neighbors, and those connecting airports with larger numbers of connections tend to recover earlier for most of the airlines."}, "https://arxiv.org/abs/2211.01179": {"title": "Solidago: A Modular Collaborative Scoring Pipeline", "link": "https://arxiv.org/abs/2211.01179", "description": "arXiv:2211.01179v3 Announce Type: replace \nAbstract: This paper presents Solidago, an end-to-end modular pipeline to allow any community of users to collaboratively score any number of entities. Solidago proposes a six-module decomposition. First, it uses pretrust and peer-to-peer vouches to assign trust scores to users. Second, based on participation, trust scores are turned into voting rights per user per entity. Third, for each user, a preference model is learned from the user's evaluation data. Fourth, users' models are put on a similar scale. Fifth, these models are securely aggregated. Sixth, models are post-processed to yield human-readable global scores. We also propose default implementations of the six modules, including a novel trust propagation algorithm, and adaptations of state-of-the-art scaling and aggregation solutions. Our pipeline has been successfully deployed on the open-source platform tournesol.app. We thereby lay an appealing foundation for the collaborative, effective, scalable, fair, interpretable and secure scoring of any set of entities."}, "https://arxiv.org/abs/2309.06228": {"title": "Delay propagation patterns in Japan's domestic air transport network", "link": "https://arxiv.org/abs/2309.06228", "description": "arXiv:2309.06228v2 Announce Type: replace \nAbstract: We experience air traffic delays every day, but are there any recurrent patterns in these delays? In this study, we investigate the recurrence of delay propagation patterns in Japan's domestic air transport network in 2019 by integrating delay causality networks and temporal network analysis. Additionally, we examine characteristics unique to delay propagation by comparing delay causality networks with corresponding randomized networks generated by a directed configuration model. As a result, we found that the structure of the delay propagation patterns can be classified into several groups. The identified groups exhibit statistically significant differences in total delay time and average out-degree, with different airports playing central roles in spreading delays. The results also suggest that some delay propagation patterns are particularly prominent during specific times of the year, which could be influenced by Japan's seasonal and geographical factors. Moreover, we discovered that specific network motifs appear significantly more (or less) frequently in delay causality networks than their corresponding randomized counterparts. This characteristic is particularly pronounced in groups with more significant delays. These results suggest that delays propagate following specific directional patterns, which could significantly contribute to predicting air traffic delays. We expect the present study to trigger further research on recurrent and non-recurrent natures of air traffic delay propagation."}, "https://arxiv.org/abs/2409.18388": {"title": "Scale Free Projections Arise from Bipartite Random Networks", "link": "https://arxiv.org/abs/2409.18388", "description": "arXiv:2409.18388v1 Announce Type: new \nAbstract: The degree distribution of a real world network -- the number of links per node -- often follows a power law, with some hubs having many more links than traditional graph generation methods predict. For years, preferential attachment and growth have been the proposed mechanisms that lead to these scale free networks. However, the two sides of bipartite graphs like collaboration networks are usually not scale free, and are therefore not well-explained by these processes. Here we develop a bipartite extension to the Randomly Stopped Linking Model and show that mixtures of geometric distributions lead to power laws according to a Central Limit Theorem for distributions with high variance. The two halves of the actor-movie network are not scale free and can be represented by just 5 geometric distributions, but they combine to form a scale free actor-actor unipartite projection without preferential attachment or growth. This result supports our claim that scale free networks are the natural result of many Bernoulli trials with high variance of which preferential attachment and growth are only one example."}, "https://arxiv.org/abs/2409.18393": {"title": "Social media algorithms can curb misinformation, but do they?", "link": "https://arxiv.org/abs/2409.18393", "description": "arXiv:2409.18393v1 Announce Type: new \nAbstract: A recent article in $\\textit{Science}$ by Guess et al. estimated the effect of Facebook's news feed algorithm on exposure to misinformation and political information among Facebook users. However, its reporting and conclusions did not account for a series of temporary emergency changes to Facebook's news feed algorithm in the wake of the 2020 U.S. presidential election that were designed to diminish the spread of voter-fraud misinformation. Here, we demonstrate that these emergency measures systematically reduced the amount of misinformation in the control group of the study, which was using the news feed algorithm. This issue may have led readers to misinterpret the results of the study and to conclude that the Facebook news feed algorithm used outside of the study period mitigates political misinformation as compared to reverse chronological feed."}, "https://arxiv.org/abs/2409.18665": {"title": "Kaleidoscopic reorganization of network communities across different scales", "link": "https://arxiv.org/abs/2409.18665", "description": "arXiv:2409.18665v1 Announce Type: new \nAbstract: The notion of structural heterogeneity is pervasive in real networks, and their community organization is no exception. Still, a vast majority of community detection methods assume neatly hierarchically organized communities of a characteristic scale for a given hierarchical level. In this work, we demonstrate that the reality of scale-dependent community reorganization is convoluted with simultaneous processes of community splitting and merging, challenging the conventional understanding of community-scale adjustment. We provide the mathematical argument on the modularity function, the results from the real-network analysis, and a simple network model for a comprehensive understanding of the nontrivial community reorganization process characterized by a local dip in the number of communities as the resolution parameter varies. This study suggests a need for a paradigm shift in the study of network communities, which emphasizes the importance of considering scale-dependent reorganization to better understand the genuine structural organization of networks."}, "https://arxiv.org/abs/2409.18931": {"title": "Social Media Bot Policies: Evaluating Passive and Active Enforcement", "link": "https://arxiv.org/abs/2409.18931", "description": "arXiv:2409.18931v1 Announce Type: new \nAbstract: The emergence of Multimodal Foundation Models (MFMs) holds significant promise for transforming social media platforms. However, this advancement also introduces substantial security and ethical concerns, as it may facilitate malicious actors in the exploitation of online users. We aim to evaluate the strength of security protocols on prominent social media platforms in mitigating the deployment of MFM bots. We examined the bot and content policies of eight popular social media platforms: X (formerly Twitter), Instagram, Facebook, Threads, TikTok, Mastodon, Reddit, and LinkedIn. Using Selenium, we developed a web bot to test bot deployment and AI-generated content policies and their enforcement mechanisms. Our findings indicate significant vulnerabilities within the current enforcement mechanisms of these platforms. Despite having explicit policies against bot activity, all platforms failed to detect and prevent the operation of our MFM bots. This finding reveals a critical gap in the security measures employed by these social media platforms, underscoring the potential for malicious actors to exploit these weaknesses to disseminate misinformation, commit fraud, or manipulate users."}, "https://arxiv.org/abs/2409.18162": {"title": "The Nexus of AR/VR, Large Language Models, UI/UX, and Robotics Technologies in Enhancing Learning and Social Interaction for Children: A Systematic Review", "link": "https://arxiv.org/abs/2409.18162", "description": "arXiv:2409.18162v1 Announce Type: cross \nAbstract: The combination of large language models (LLMs), augmented reality (AR), and user interface/user experience (UI/UX) design in therapies for children, especially with disorders like autism spectrum disorder (ASD), is examined in this review study. 150 publications were found by a thorough literature search throughout PubMed, ACM, IEEE Xplore, Elsevier, and Google Scholar; 42 of them were chosen for in-depth study due to their methodological rigor and relevance. Three primary areas are covered in this review: how AR can improve social and learning results; how LLMs can help with communication; and how UI/UX design affects how effective these technologies are. Results reveal that while LLMs can provide individualized learning and communication support, AR has demonstrated promise in enhancing social skills, motivation, and attention. For children with ASD, accessible and interesting interventions depend heavily on effective UI/UX design. To optimize the benefits of these technologies in ASD therapies, the study emphasizes the need for additional research to address difficulties related to customization, accessibility, and integration."}, "https://arxiv.org/abs/2409.18240": {"title": "Measuring Research Interest Similarity with Transition Probabilities", "link": "https://arxiv.org/abs/2409.18240", "description": "arXiv:2409.18240v1 Announce Type: cross \nAbstract: We propose a method to measure the similarity of papers and authors by simulating a literature search procedure on citation networks, which is an information retrieval inspired conceptualization of similarity. This transition probability (TP) based approach does not require a curated classification system, avoids clustering complications, and provides a continuous measure of similarity. We perform testing scenarios to explore several versions of the general TP concept and the Node2vec machine-learning technique. We found that TP measures outperform Node2vec in mapping the macroscopic structure of fields. The paper provides a general discussion of how to implement TP similarity measurement, with a particular focus on how to utilize publication-level information to approximate the research interest similarity of individual scientists. This paper is accompanied by a Python package capable of calculating all the tested metrics."}, "https://arxiv.org/abs/2409.18280": {"title": "easylayout: an R package for interactive force-directed layouts within RStudio", "link": "https://arxiv.org/abs/2409.18280", "description": "arXiv:2409.18280v1 Announce Type: cross \nAbstract: Motivation\n  Network visualization is critical for effective communication in various fields of knowledge. Currently, a gap separates network manipulation from network visualization in programming environments. Users often export network data to be laid out in external interactive software, like Cytoscape and Gephi. We argue the current R package ecosystem lacks an interactive layout engine well integrated with common data analysis workflows.\n  Results\n  We present easylayout, an R package that bridges network manipulation and visualization by leveraging interactive force simulations within the IDE itself (e.g., RStudio, VSCode). It is not yet another visualization library, but instead aims to interconnect existing libraries and streamline their usage into the R ecosystem. easylayout takes an igraph object and serializes it into a web application integrated with the IDE's interface through a Shiny server. The app lays out the network by simulating attraction and repulsion forces. Simulation parameters can be adjusted in real-time. An editing mode allows moving and rotating nodes. The implementation aims for performance, so that even lower-end devices are able to work with relatively large networks. Once the user finishes tweaking the layout, it is sent back to the R session to be plotted through popular libraries like ggraph, igraph or even the base package itself. The current implementation focuses on the R ecosystem, but using web technologies makes it easily portable to similar environments, like Python/Jupyter Notebooks. We expect this tool to reduce the time spent searching for suitable network layouts, ultimately allowing researchers to generate more compelling figures.\n  Availability and implementation\n  easylayout is freely available under an MIT license on GitHub (https://github.com/dalmolingroup/easylayout). The package is implemented in R/Shiny and JavaScript/Svelte."}, "https://arxiv.org/abs/2409.18427": {"title": "Neural Collaborative Filtering to Detect Anomalies in Human Semantic Trajectories", "link": "https://arxiv.org/abs/2409.18427", "description": "arXiv:2409.18427v1 Announce Type: cross \nAbstract: Human trajectory anomaly detection has become increasingly important across a wide range of applications, including security surveillance and public health. However, existing trajectory anomaly detection methods are primarily focused on vehicle-level traffic, while human-level trajectory anomaly detection remains under-explored. Since human trajectory data is often very sparse, machine learning methods have become the preferred approach for identifying complex patterns. However, concerns regarding potential biases and the robustness of these models have intensified the demand for more transparent and explainable alternatives. In response to these challenges, our research focuses on developing a lightweight anomaly detection model specifically designed to detect anomalies in human trajectories. We propose a Neural Collaborative Filtering approach to model and predict normal mobility. Our method is designed to model users' daily patterns of life without requiring prior knowledge, thereby enhancing performance in scenarios where data is sparse or incomplete, such as in cold start situations. Our algorithm consists of two main modules. The first is the collaborative filtering module, which applies collaborative filtering to model normal mobility of individual humans to places of interest. The second is the neural module, responsible for interpreting the complex spatio-temporal relationships inherent in human trajectory data. To validate our approach, we conducted extensive experiments using simulated and real-world datasets comparing to numerous state-of-the-art trajectory anomaly detection approaches."}, "https://arxiv.org/abs/2409.18865": {"title": "Positional Encoder Graph Quantile Neural Networks for Geographic Data", "link": "https://arxiv.org/abs/2409.18865", "description": "arXiv:2409.18865v1 Announce Type: cross \nAbstract: Positional Encoder Graph Neural Networks (PE-GNNs) are a leading approach for modeling continuous spatial data. However, they often fail to produce calibrated predictive distributions, limiting their effectiveness for uncertainty quantification. We introduce the Positional Encoder Graph Quantile Neural Network (PE-GQNN), a novel method that integrates PE-GNNs, Quantile Neural Networks, and recalibration techniques in a fully nonparametric framework, requiring minimal assumptions about the predictive distributions. We propose a new network architecture that, when combined with a quantile-based loss function, yields accurate and reliable probabilistic models without increasing computational complexity. Our approach provides a flexible, robust framework for conditional density estimation, applicable beyond spatial data contexts. We further introduce a structured method for incorporating a KNN predictor into the model while avoiding data leakage through the GNN layer operation. Experiments on benchmark datasets demonstrate that PE-GQNN significantly outperforms existing state-of-the-art methods in both predictive accuracy and uncertainty quantification."}, "https://arxiv.org/abs/2409.18911": {"title": "Soft Measures for Extracting Causal Collective Intelligence", "link": "https://arxiv.org/abs/2409.18911", "description": "arXiv:2409.18911v1 Announce Type: cross \nAbstract: Understanding and modeling collective intelligence is essential for addressing complex social systems. Directed graphs called fuzzy cognitive maps (FCMs) offer a powerful tool for encoding causal mental models, but extracting high-integrity FCMs from text is challenging. This study presents an approach using large language models (LLMs) to automate FCM extraction. We introduce novel graph-based similarity measures and evaluate them by correlating their outputs with human judgments through the Elo rating system. Results show positive correlations with human evaluations, but even the best-performing measure exhibits limitations in capturing FCM nuances. Fine-tuning LLMs improves performance, but existing measures still fall short. This study highlights the need for soft similarity measures tailored to FCM extraction, advancing collective intelligence modeling with NLP."}, "https://arxiv.org/abs/2409.18981": {"title": "The role of coal plant retrofitting strategies in developing India's net-zero power system: a data-driven sub-national analysis", "link": "https://arxiv.org/abs/2409.18981", "description": "arXiv:2409.18981v1 Announce Type: new \nAbstract: India set two Nationally Determined Contribution targets to achieve the net zero carbon emission goal by 2070, which requires deep decarbonization of India's power generation sector. Yet, coal power generation contributes to more than 60\\% of its total power generation, and policies still permit further coal fleet expansion and lifetime extensions. In this paper, we investigate the role of retrofitting India's coal plants for carbon capture and storage and biomass co-firing in developing the net-zero power system. We model the power generation and transmission network expansions across 30 Indian states in four representative grid evolution scenarios under progressively tighter carbon emission caps, taking into account sub-national coal price variation and thermal efficiency of individual coal plants. We find that coal plant retrofitting could happen by 2035 when an annual carbon cap for the power generation sector is less than 1,000 million tons CO$_2$. This reduces the unabated coal plant capacity, electricity generation, and carbon abatement costs. Exploiting renewable energy potentials solely, such as wind resources, could reduce carbon abatement costs significantly but will result in low coal plant utilization and uneven renewable generation deployment between Southern and Central regions concerning energy justice."}, "https://arxiv.org/abs/2409.18985": {"title": "Quantum-like perceptual entanglement in collective motion", "link": "https://arxiv.org/abs/2409.18985", "description": "arXiv:2409.18985v1 Announce Type: new \nAbstract: We explore the collective motion of self-propelled agents through the lens of quantum-like entanglement in visual perception. This approach makes the non-linear dynamics in ordinary space a derivative of dynamics in the Hilbert space of perception. By appropriately selecting operators, we demonstrate how this framework can give rise to the well-known Vicsek model. Additionally, we investigate the impact of different types of entangled states, such as GHZ states, W states, and cluster states, on collective behavior."}, "https://arxiv.org/abs/2409.18994": {"title": "Closing the reproducibility gap: 2D materials research", "link": "https://arxiv.org/abs/2409.18994", "description": "arXiv:2409.18994v1 Announce Type: new \nAbstract: 2D materials research has reached significant scientific milestones, accompanied by a rapidly growing industrial sector in the two decades since the field's inception. Such rapid progress requires pushing past the boundary of what is technically and scientifically feasible and carries the risk of disseminating irreproducible research results. This Expert Recommendation addresses the need for enhanced reproducibility in 2D materials science and physics. Through a comprehensive examination of the factors that affect reproducibility the authors present a set of concrete guidelines designed to improve the reliability of research results. The introduction of a Standardised Template for Experimental Procedures (STEP) offers a novel approach to documenting experimental details that are crucial for replication and troubleshooting. We emphasise the importance of involving stakeholders from research, industry, publishing, funding agencies, and policymaking to foster a culture of transparency, reliability, and trust without blind angles and critical oversights. By addressing systemic issues that hinder reproducibility and presenting actionable steps for improvement, we aim to pave the way for more robust research practices in 2D materials science, contributing to the field's scientific maturation and the subsequent development of beneficial technologies."}, "https://arxiv.org/abs/2409.19000": {"title": "Pickleball Flight Dynamics", "link": "https://arxiv.org/abs/2409.19000", "description": "arXiv:2409.19000v1 Announce Type: new \nAbstract: This paper considers the flight dynamics of the ball in the sport of pickleball. Various simplifications are introduced according to the features of the game. These simplifications and some approximations enable straightforward coding to study aspects of the game such as the trajectory of the ball and its velocity. In turn, strategic questions may be addressed that have not been previously considered. In particular, our primary research question involves the preference between playing with the wind versus against the wind. It is demonstrated that playing against the wind is often preferable than playing with the wind."}, "https://arxiv.org/abs/2409.19003": {"title": "Unravelling compound risks of hydrological extremes in a changing climate: Typology, methods and futures", "link": "https://arxiv.org/abs/2409.19003", "description": "arXiv:2409.19003v1 Announce Type: new \nAbstract: We have witnessed and experienced increasing compound extreme events resulting from simultaneous or sequential occurrence of multiple events in a changing climate. In addition to a growing demand for a clearer explanation of compound risks from a hydrological perspective, there has been a lack of attention paid to socioeconomic factors driving and impacted by these risks. Through a critical review and co-production approaches, we identified four types of compound hydrological events based on autocorrelated, multivariate, and spatiotemporal patterns. A framework to quantify compound risks based on conditional probability is offered, including an argument on the potential use of generative Artificial Intelligence (AI) algorithms for identifying emerging trends and patterns for climate change. Insights for practices are discussed, highlighting the implications for disaster risk reduction and knowledge co-production. Our argument centres on the importance of meaningfully considering the socioeconomic contexts in which compound risks may have impacts, and the need for interdisciplinary collaboration to effectively translate climate science to climate actions."}, "https://arxiv.org/abs/2409.19008": {"title": "Forecasting Energy Needs with Logistics", "link": "https://arxiv.org/abs/2409.19008", "description": "arXiv:2409.19008v1 Announce Type: new \nAbstract: The logistic function is used to forecast energy consumed worldwide and oil production in the U.S. The logistic substitution model is used to describe the energy mix since 1965 presenting a picture significantly different from the one covering the previous 100 years. In the new picture coal gently gains on oil and hydroelectric gains on natural gas even if it is three times smaller. Finally, renewables (wind, geothermal, solar, biomass, and waste) grow exclusively on the expense of nuclear and are poised to overtake it by the late 2030s. By mid-21st century, coal, oil, and natural gas still remain the main players of comparable size. Hydroelectric has almost doubled in size. The only significant substitution is that of renewables having replaced nuclear albeit remaining at less than one-fourth the size of the other three energy sources. U.S. oil produced by fracking is forecasted to cease by mid-21st century, while oil produced by traditional methods should continue on its slowly declining trend. US oil production is likely to represent less than 1% of the oil consumed worldwide by mid-21st century."}, "https://arxiv.org/abs/2409.19018": {"title": "A Generalized Framework for Assessing Equity in Ground Transportation Infrastructure: An Exploratory Study", "link": "https://arxiv.org/abs/2409.19018", "description": "arXiv:2409.19018v1 Announce Type: new \nAbstract: Ground transportation infrastructure significantly impacts community connectivity, economic growth, and access to essential services such as jobs, education, and healthcare. However, in practice, these infrastructures do not provide equitable services for all, and historical disparities have led to inequitable conditions for many individuals. This paper explores diverse definitions of equity relevant to ground infrastructures, examines the consequences of inequitable transportation systems throughout the history of the U.S. highway system, and explores various approaches for conducting equity analysis. Based on the collected information, a generalized framework is introduced to analyze the impact of these infrastructures on transportation equity. The study also provides a novel equity index that can be used to assess equity considering accessibility and affordability. Finally, the framework is applied to a case study in Baltimore City, Maryland, examining the equitable distribution of electric vehicle (EV) chargers. By demonstrating its practical application, the paper offers managers and policymakers a concise step-by-step approach to analyze transportation equity. This method assesses both the socioeconomic features of affected populations and the distribution of services, contributing to the development of a more sustainable ground transport system."}, "https://arxiv.org/abs/2409.19041": {"title": "Precipitation analysis in Tlaquepaque: a look at the past and the present", "link": "https://arxiv.org/abs/2409.19041", "description": "arXiv:2409.19041v1 Announce Type: new \nAbstract: Precipitation events in large cities can be a challenge and a problem for continuing the regular dynamics of people flows and economic activities. It is enough to remember what happens at overpasses, on main avenues, as well as in the lower and marginalized parts of cities when we have intense rainfall. Unfortunately, the accelerated and disorganized growth of metropolises in Mexico has not allowed for the fulfillment of urban planning (S\\'anchez-Rodr\\'iguez et al., 2013), a situation that is especially evident during conditions of abundant rainfall. In this work, the behavior of rainfall in the municipality of Tlaquepaque, Jalisco, since 1951 and until 2022, is specifically explained and with a certain level of detail."}, "https://arxiv.org/abs/2409.19166": {"title": "Understanding #vent Channels on Discord", "link": "https://arxiv.org/abs/2409.19166", "description": "arXiv:2409.19166v1 Announce Type: new \nAbstract: Vent channels on Discord, which are chat channels developed for people to express frustrations, can become an informal type of peer support system. This paper is a qualitative study of experiences with vent channels on Discord, examining the experiences of 13 participants through semi-structured interviews. We find that participants are able to meet their needs for social support via vent channels by receiving commiseration, advice, and validation from the responses of others. At the same time, vent channels can lead to frustration when participants have conflicting expectations for their interactions. We suggest ways that Discord or Discord server moderators can provide enhanced structure, clarity, and transparency in order to enable participants to have better experiences in vent channels."}, "https://arxiv.org/abs/2409.19243": {"title": "Jointly modelling the evolution of community structure and language in online extremist groups", "link": "https://arxiv.org/abs/2409.19243", "description": "arXiv:2409.19243v1 Announce Type: new \nAbstract: Group interactions take place within a particular socio-temporal context, which should be taken into account when modelling communities. We propose a method for jointly modelling community structure and language over time, and apply it in the context of extremist anti-women online groups (collectively known as the manosphere). Our model derives temporally grounded embeddings for words and users, which evolve over the training window. We show that this approach outperforms prior models which lacked one of these components (i.e. not incorporating social structure, or using static word embeddings). Using these embeddings, we investigate the evolution of users and words within these communities in three ways: (i) we model a user as a sequence of embeddings and forecast their affinity groups beyond the training window, (ii) we illustrate how word evolution is useful in the context of temporal events, and (iii) we characterise the propensity for violent language within subgroups of the manosphere."}, "https://arxiv.org/abs/2409.19338": {"title": "Decoding Echo Chambers: LLM-Powered Simulations Revealing Polarization in Social Networks", "link": "https://arxiv.org/abs/2409.19338", "description": "arXiv:2409.19338v1 Announce Type: new \nAbstract: The impact of social media on critical issues such as echo chambers needs to be addressed, as these phenomena can have disruptive consequences for our society. Traditional research often oversimplifies emotional tendencies and opinion evolution into numbers and formulas, neglecting that news and communication are conveyed through text, which limits these approaches. Hence, in this work, we propose an LLM-based simulation for the social opinion network to evaluate and counter polarization phenomena. We first construct three typical network structures to simulate different characteristics of social interactions. Then, agents interact based on recommendation algorithms and update their strategies through reasoning and analysis. By comparing these interactions with the classic Bounded Confidence Model (BCM), the Friedkin Johnsen (FJ) model, and using echo chamber-related indices, we demonstrate the effectiveness of our framework in simulating opinion dynamics and reproducing phenomena such as opinion polarization and echo chambers. We propose two mitigation methods, active and passive nudges, that can help reduce echo chambers, specifically within language-based simulations. We hope our work will offer valuable insights and guidance for social polarization mitigation."}, "https://arxiv.org/abs/2409.19524": {"title": "Understanding everyday public transit travel habits: a measurement framework for the peakedness of departure time distributions", "link": "https://arxiv.org/abs/2409.19524", "description": "arXiv:2409.19524v1 Announce Type: new \nAbstract: Persuasive scholarship presents how individual daily travel habits implicate congestion, environmental pollution, and the travel experience. However, the empirical characteristics and dynamics of travel habits remain poorly understood. Quantifying both our individual travel habits and how these habits aggregate to form system-wide dynamics is of critical importance to enable the smart design of public transit systems that are better tailored to our daily mobility needs. We contribute to this need through the development and implementation of a new measurement framework capturing the 'peakedness' of users' departure time distributions. Departure time 'peakedness' reflects a user's tendency to repeatedly choose the same departure time for a given origin-destination trip, offering a clearer and more intuitive representation of regularity and habitual patterns compared to traditional metrics like standard deviation or entropy. Our framework demonstrates that system-wide departure time peakedness can be decomposed into individual users' departure time peakedness and the alignment of their peak times. This allows for a systematic analysis of both individual and collective behaviours. We apply our framework to departure time data from a 12-month period, encompassing 5,947,907 bus journeys made by 29,640 individuals across three urban networks within a large regional metropolis. Our findings reveal that departure time peakedness is more deeply tied to inherent, passenger-specific characteristics, such as passenger type, rather than external factors like weather or holidays. Additionally, individual-level departure time peakedness shows notable dynamics over time, indicating that habitual routines can evolve in the long term, while system-level peakedness exhibits remarkable long-term stability."}, "https://arxiv.org/abs/2409.19880": {"title": "Indirect rewards outperform direct punishment in promoting cooperation", "link": "https://arxiv.org/abs/2409.19880", "description": "arXiv:2409.19880v1 Announce Type: new \nAbstract: Existing research demonstrates that direct punishment, which targets defectors who harm the individual directly, can address social dilemmas and sustain cooperation in social networks. This study introduces a novel form of punishment that targets second-order defectors--those who harm an individual's neighbors--rather than directly punishing one's own defecting neighbors. This mechanism acts as an indirect reward to immediate neighbors by following the principle of ``I help you by punishing those who defect against you.\" Using evolutionary simulations, we find that in a narrow region of the cost-fine space, when punishment is less costly and fines on defectors are low, indirect rewards are more effective at promoting cooperation than direct punishment. In other cases, direct punishment proves more effective. These results are robust regardless of whether network reciprocity alone can support cooperation. Indirect reward excel under these conditions, because it imposes multiple sources of punishment on defectors without substantially raising the costs for the punishers, thereby amplifying the overall punitive effect. In contrast, direct punishment targets defectors within a limited range, reducing its impact. Taken together, these findings provide novel insights into the comparative effectiveness of different punishment forms, and highlight the importance of indirect rewards in understanding the evolution of cooperation."}, "https://arxiv.org/abs/2409.20078": {"title": "Quantifying discriminability of evaluation metrics in link prediction for real networks", "link": "https://arxiv.org/abs/2409.20078", "description": "arXiv:2409.20078v1 Announce Type: new \nAbstract: Link prediction is one of the most productive branches in network science, aiming to predict links that would have existed but have not yet been observed, or links that will appear during the evolution of the network. Over nearly two decades, the field of link prediction has amassed a substantial body of research, encompassing a plethora of algorithms and diverse applications. For any algorithm, one or more evaluation metrics are required to assess its performance. Because using different evaluation metrics can provide different assessments of the algorithm performance, how to select appropriate evaluation metrics is a fundamental issue in link prediction. To address this issue, we propose a novel measure that quantifiers the discriminability of any evaluation metric given a real network and an algorithm. Based on 131 real networks and 20 representative algorithms, we systematically compare the discriminabilities of eight evaluation metrics, and demonstrate that H-measure and Area Under the ROC Curve (AUC) exhibit the strongest discriminabilities, followed by Normalized Discounted Cumulative Gain (NDCG). Our finding is robust for networks in different domains and algorithms of different types. This study provides insights into the selection of evaluation metrics, which may further contribute to standardizing the evaluating process of link prediction algorithms."}, "https://arxiv.org/abs/2409.20079": {"title": "Online Influence Maximization with Semi-Bandit Feedback under Corruptions", "link": "https://arxiv.org/abs/2409.20079", "description": "arXiv:2409.20079v1 Announce Type: new \nAbstract: In this work, we investigate the online influence maximization in social networks. Most prior research studies on online influence maximization assume that the nodes are fully cooperative and act according to their stochastically generated influence probabilities on others. In contrast, we study the online influence maximization problem in the presence of some corrupted nodes whose damaging effects diffuse throughout the network. We propose a novel bandit algorithm, CW-IMLinUCB, which robustly learns and finds the optimal seed set in the presence of corrupted users. Theoretical analyses establish that the regret performance of our proposed algorithm is better than the state-of-the-art online influence maximization algorithms. Extensive empirical evaluations on synthetic and real-world datasets also show the superior performance of our proposed algorithm."}, "https://arxiv.org/abs/2409.20437": {"title": "Extreme heat is associated with reductions in human activity", "link": "https://arxiv.org/abs/2409.20437", "description": "arXiv:2409.20437v1 Announce Type: new \nAbstract: Extreme heat is a growing threat to both individual livelihoods and broader economies, killing an increasing number of people each year as temperatures increase and limiting productivity - often in the countries that would benefit most from economic growth. Many studies document the link between heat waves and mortality or morbidity, and others explore the economic consequences of them, but few are able to identify the ways in which populations respond to the shock of extreme heat. Toward this end, we investigate the link between human mobility and fluctuations in temperature. Using additive models fitting data in Indonesia, India and Mexico, we show that extreme heat reduces mobility by up to 10% in rural areas and 5% in urban settings. Effects are stronger in poorer areas. Twinning these models with climate projections, we show that without adaptation mobility will fall 1-2% per year on aggregate, with certain seasons and places seeing activity fall by as much as 10%. According to our estimates, rural areas will face the highest relative losses and urban megacities will experience the greatest absolute impacts."}, "https://arxiv.org/abs/2409.18976": {"title": "Prioritizing Risk Factors in Media Entrepreneurship on Social Networks: Hybrid Fuzzy Z-Number Approaches for Strategic Budget Allocation and Risk Management in Advertising Construction Campaigns", "link": "https://arxiv.org/abs/2409.18976", "description": "arXiv:2409.18976v1 Announce Type: cross \nAbstract: The proliferation of complex online media has accelerated the process of ideology formation, influenced by stakeholders through advertising channels. The media channels, which vary in cost and effectiveness, present a dilemma in prioritizing optimal fund allocation. There are technical challenges in describing the optimal budget allocation between channels over time, which involves defining the finite vector structure of controls on the chart. To enhance marketing productivity, it's crucial to determine how to distribute a budget across all channels to maximize business outcomes like revenue and ROI. Therefore, the strategy for media budget allocation is primarily an exercise focused on cost and achieving goals, by identifying a specific framework for a media program. Numerous researchers optimize the achievement and frequency of media selection models to aid superior planning decisions amid complexity and vast information availability. In this study, we present a planning model using the media mix model for advertising construction campaigns. Additionally, a decision-making strategy centered on FMEA identifies and prioritizes financial risk factors of the media system in companies. Despite some limitations, this research proposes a decision-making approach based on Z-number theory. To address the drawbacks of the RPN score, the suggested decision-making methodology integrates Z-SWARA and Z-WASPAS techniques with the FMEA method."}, "https://arxiv.org/abs/2409.18997": {"title": "PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent", "link": "https://arxiv.org/abs/2409.18997", "description": "arXiv:2409.18997v1 Announce Type: cross \nAbstract: Propaganda plays a critical role in shaping public opinion and fueling disinformation. While existing research primarily focuses on identifying propaganda techniques, it lacks the ability to capture the broader motives and the impacts of such content. To address these challenges, we introduce propainsight, a conceptual framework grounded in foundational social science research, which systematically dissects propaganda into techniques, arousal appeals, and underlying intent. propainsight offers a more granular understanding of how propaganda operates across different contexts. Additionally, we present propagaze, a novel dataset that combines human-annotated data with high-quality synthetic data generated through a meticulously designed pipeline. Our experiments show that off-the-shelf LLMs struggle with propaganda analysis, but training with propagaze significantly improves performance. Fine-tuned Llama-7B-Chat achieves 203.4% higher text span IoU in technique identification and 66.2% higher BertScore in appeal analysis compared to 1-shot GPT-4-Turbo. Moreover, propagaze complements limited human-annotated data in data-sparse and cross-domain scenarios, showing its potential for comprehensive and generalizable propaganda analysis."}, "https://arxiv.org/abs/2409.19158": {"title": "bnRep: A repository of Bayesian networks from the academic literature", "link": "https://arxiv.org/abs/2409.19158", "description": "arXiv:2409.19158v1 Announce Type: cross \nAbstract: Bayesian networks (BNs) are widely used for modeling complex systems with uncertainty, yet repositories of pre-built BNs remain limited. This paper introduces bnRep, an open-source R package offering a comprehensive collection of documented BNs, facilitating benchmarking, replicability, and education. With over 200 networks from academic publications, bnRep integrates seamlessly with bnlearn and other R packages, providing users with interactive tools for network exploration."}, "https://arxiv.org/abs/2409.19257": {"title": "LISTN: Lexicon induction with socio-temporal nuance", "link": "https://arxiv.org/abs/2409.19257", "description": "arXiv:2409.19257v1 Announce Type: cross \nAbstract: Research on extremist online communities frequently utilizes linguistic analysis to explore group dynamics and behaviour. Existing studies often rely on outdated lexicons that do not capture the evolving nature of in-group language, nor the social structure of the community. This paper proposes a novel method for inducing in-group lexicons which incorporates its socio-temporal context. Using dynamic word and user embeddings trained on conversations from online anti-women communities, our approach outperforms prior methods for lexicon induction. We provide a new lexicon of manosphere terms, validated by human experts, which quantifies the relevance of each term to a specific sub-community. We present novel insights on in-group language which illustrate the utility of this approach."}, "https://arxiv.org/abs/2409.19616": {"title": "DuoGNN: Topology-aware Graph Neural Network with Homophily and Heterophily Interaction-Decoupling", "link": "https://arxiv.org/abs/2409.19616", "description": "arXiv:2409.19616v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs) have proven effective in various medical imaging applications, such as automated disease diagnosis. However, due to the local neighborhood aggregation paradigm in message passing which characterizes these models, they inherently suffer from two fundamental limitations: first, indistinguishable node embeddings due to heterophilic node aggregation (known as over-smoothing), and second, impaired message passing due to aggregation through graph bottlenecks (known as over-squashing). These challenges hinder the model expressiveness and prevent us from using deeper models to capture long-range node dependencies within the graph. Popular solutions in the literature are either too expensive to process large graphs due to high time complexity or do not generalize across all graph topologies. To address these limitations, we propose DuoGNN, a scalable and generalizable architecture which leverages topology to decouple homophilic and heterophilic edges and capture both short-range and long-range interactions. Our three core contributions introduce (i) a topological edge-filtering algorithm which extracts homophilic interactions and enables the model to generalize well for any graph topology, (ii) a heterophilic graph condensation technique which extracts heterophilic interactions and ensures scalability, and (iii) a dual homophilic and heterophilic aggregation pipeline which prevents over-smoothing and over-squashing during the message passing. We benchmark our model on medical and non-medical node classification datasets and compare it with its variants, showing consistent improvements across all tasks. Our DuoGNN code is available at https://github.com/basiralab/DuoGNN."}, "https://arxiv.org/abs/2409.20073": {"title": "Whole-Graph Representation Learning For the Classification of Signed Networks", "link": "https://arxiv.org/abs/2409.20073", "description": "arXiv:2409.20073v1 Announce Type: cross \nAbstract: Graphs are ubiquitous for modeling complex systems involving structured data and relationships. Consequently, graph representation learning, which aims to automatically learn low-dimensional representations of graphs, has drawn a lot of attention in recent years. The overwhelming majority of existing methods handle unsigned graphs. However, signed graphs appear in an increasing number of application domains to model systems involving two types of opposed relationships. Several authors took an interest in signed graphs and proposed methods for providing vertex-level representations, but only one exists for whole-graph representations, and it can handle only fully connected graphs. In this article, we tackle this issue by proposing two approaches to learning whole-graph representations of general signed graphs. The first is a SG2V, a signed generalization of the whole-graph embedding method Graph2vec that relies on a modification of the Weisfeiler--Lehman relabelling procedure. The second one is WSGCN, a whole-graph generalization of the signed vertex embedding method SGCN that relies on the introduction of master nodes into the GCN. We propose several variants of both these approaches. A bottleneck in the development of whole-graph-oriented methods is the lack of data. We constitute a benchmark composed of three collections of signed graphs with corresponding ground truths. We assess our methods on this benchmark, and our results show that the signed whole-graph methods learn better representations for this task. Overall, the baseline obtains an F-measure score of 58.57, when SG2V and WSGCN reach 73.01 and 81.20, respectively. Our source code and benchmark dataset are both publicly available online."}, "https://arxiv.org/abs/2306.01766": {"title": "An interpretable wildfire spreading model for real-time predictions", "link": "https://arxiv.org/abs/2306.01766", "description": "arXiv:2306.01766v2 Announce Type: replace \nAbstract: Forest fires pose a natural threat with devastating social, environmental, and economic implications. The rapid and highly uncertain rate of spread of wildfires necessitates a trustworthy digital tool capable of providing real-time estimates of fire evolution and human interventions, while receiving continuous input from remote sensing. The current work aims at developing an interpretable, physics-based model that will serve as the core of such a tool. This model is constructed using easily understandable equations, incorporating a limited set of parameters that capture essential quantities and heat transport mechanisms. The simplicity of the model allows for effective utilization of data from sensory input, enabling optimal estimation of these parameters. In particular, simplified versions of combustion kinetics and mass/energy balances lead to a computationally inexpensive system of differential equations that provide the spatio-temporal evolution of temperature and flammables over a two-dimensional region. The model is validated by comparing its predictions and the effect of parameters such as flammable bulk density, moisture content, and wind speed, with benchmark results. Additionally, the model successfully captures the evolution of the firefront shape and its rate of spread in multiple directions."}, "https://arxiv.org/abs/2401.13576": {"title": "Influence of initiators on the tipping point in the extended Watts model", "link": "https://arxiv.org/abs/2401.13576", "description": "arXiv:2401.13576v2 Announce Type: replace \nAbstract: In this paper, we study how the influence of initiators (seeds) affects the tipping point of information cascades in networks. We consider an extended version of the Watts model, in which each node is either active (i.e., having adopted an innovation) or inactive. In this extended model, the adoption threshold, defined as the fraction of active neighbors required for an inactive node to become active, depends on whether the node is a seed neighbor (i.e., connected to one or more initiators) or an ordinary node (i.e., not connected to any initiators). Using the tree approximation on random graphs, we determine the tipping point, at which the fraction of active nodes in the final state increases discontinuously with an increasing seed fraction. The occurrence of a tipping point and the scale of cascades depend on two factors: whether a giant component of seed neighbors is formed when the seed fraction is large enough to trigger cascades among seed neighbors, and whether the giant component of ordinary nodes is maintained when newly activated nodes trigger further activations among ordinary nodes. The coexistence of two giant components suggests that a tipping point can appear twice. We present an example demonstrating the existence of two tipping points when there is a gap between the adoption thresholds of seed neighbors and ordinary nodes. Monte Carlo simulations clearly show that the first cascade, occurring at a small tipping point, occurs in the giant component of seed neighbors, while the second cascade, occurring at a larger tipping point, extends into the giant component of ordinary nodes."}, "https://arxiv.org/abs/2110.11856": {"title": "L-2 Regularized maximum likelihood for $\\beta$-model in large and sparse networks", "link": "https://arxiv.org/abs/2110.11856", "description": "arXiv:2110.11856v4 Announce Type: replace-cross \nAbstract: The $\\beta$-model is a powerful tool for modeling large and sparse networks driven by degree heterogeneity, where many network models become infeasible due to computational challenge and network sparsity. However, existing estimation algorithms for $\\beta$-model do not scale up. Also, theoretical understandings remain limited to dense networks. This paper brings several significant improvements over existing results to address the urgent needs of practice. We propose a new $\\ell_2$-penalized MLE algorithm that can comfortably handle sparse networks of millions of nodes with much-improved memory parsimony. We establish the first rate-optimal error bounds and high-dimensional asymptotic normality results for $\\beta$-models, under much weaker network sparsity assumptions than best existing results.\n  Application of our method to large COVID-19 network data sets and discover meaningful results."}, "https://arxiv.org/abs/2308.09275": {"title": "Stochastic Opinion Dynamics under Social Pressure in Arbitrary Networks", "link": "https://arxiv.org/abs/2308.09275", "description": "arXiv:2308.09275v3 Announce Type: replace-cross \nAbstract: Social pressure is a key factor affecting the evolution of opinions on networks in many types of settings, pushing people to conform to their neighbors' opinions. To study this, the interacting Polya urn model was introduced by Jadbabaie et al., in which each agent has two kinds of opinion: inherent beliefs, which are hidden from the other agents and fixed; and declared opinions, which are randomly sampled at each step from a distribution which depends on the agent's inherent belief and her neighbors' past declared opinions (the social pressure component), and which is then communicated to her neighbors. Each agent also has a bias parameter denoting her level of resistance to social pressure. At every step, each agent updates her declared opinion (simultaneously with all other agents) according to her neighbors' aggregate past declared opinions, her inherent belief, and her bias parameter. We study the asymptotic behavior of this opinion dynamics model and show that the agents' declaration probabilities approaches a set of equilibrium points of the expected dynamics using Lyapunov theory and stochastic approximation techniques. We also derive necessary and sufficient conditions for the agents to approach consensus on their declared opinions. Our work provides further insight into the difficulty of inferring the inherent beliefs of agents when they are under social pressure."}, "https://arxiv.org/abs/2402.05024": {"title": "Does the Use of Unusual Combinations of Datasets Contribute to Greater Scientific Impact?", "link": "https://arxiv.org/abs/2402.05024", "description": "arXiv:2402.05024v3 Announce Type: replace-cross \nAbstract: Scientific datasets play a crucial role in contemporary data-driven research, as they allow for the progress of science by facilitating the discovery of new patterns and phenomena. This mounting demand for empirical research raises important questions on how strategic data utilization in research projects can stimulate scientific advancement. In this study, we examine the hypothesis inspired by the recombination theory, which suggests that innovative combinations of existing knowledge, including the use of unusual combinations of datasets, can lead to high-impact discoveries. We investigate the scientific outcomes of such atypical data combinations in more than 30,000 publications that leverage over 6,000 datasets curated within one of the largest social science databases, ICPSR. This study offers four important insights. First, combining datasets, particularly those infrequently paired, significantly contributes to both scientific and broader impacts (e.g., dissemination to the general public). Second, the combination of datasets with atypically combined topics has the opposite effect -- the use of such data is associated with fewer citations. Third, younger and less experienced research teams tend to use atypical combinations of datasets in research at a higher frequency than their older and more experienced counterparts. Lastly, despite the benefits of data combination, papers that amalgamate data remain infrequent. This finding suggests that the unconventional combination of datasets is an under-utilized but powerful strategy correlated with the scientific and broader impact of scientific discoveries."}, "https://arxiv.org/abs/2410.00075": {"title": "Optimizing Treatment Allocation in the Presence of Interference", "link": "https://arxiv.org/abs/2410.00075", "description": "arXiv:2410.00075v1 Announce Type: new \nAbstract: In Influence Maximization (IM), the objective is to -- given a budget -- select the optimal set of entities in a network to target with a treatment so as to maximize the total effect. For instance, in marketing, the objective is to target the set of customers that maximizes the total response rate, resulting from both direct treatment effects on targeted customers and indirect, spillover, effects that follow from targeting these customers. Recently, new methods to estimate treatment effects in the presence of network interference have been proposed. However, the issue of how to leverage these models to make better treatment allocation decisions has been largely overlooked. Traditionally, in Uplift Modeling (UM), entities are ranked according to estimated treatment effect, and the top entities are allocated treatment. Since, in a network context, entities influence each other, the UM ranking approach will be suboptimal. The problem of finding the optimal treatment allocation in a network setting is combinatorial and generally has to be solved heuristically. To fill the gap between IM and UM, we propose OTAPI: Optimizing Treatment Allocation in the Presence of Interference to find solutions to the IM problem using treatment effect estimates. OTAPI consists of two steps. First, a causal estimator is trained to predict treatment effects in a network setting. Second, this estimator is leveraged to identify an optimal treatment allocation by integrating it into classic IM algorithms. We demonstrate that this novel method outperforms classic IM and UM approaches on both synthetic and semi-synthetic datasets."}, "https://arxiv.org/abs/2410.00082": {"title": "Graph Residual Noise Learner Network for Brain Connectivity Graph Prediction", "link": "https://arxiv.org/abs/2410.00082", "description": "arXiv:2410.00082v1 Announce Type: new \nAbstract: A morphological brain graph depicting a connectional fingerprint is of paramount importance for charting brain dysconnectivity patterns. Such data often has missing observations due to various reasons such as time-consuming and incomplete neuroimage processing pipelines. Thus, predicting a target brain graph from a source graph is crucial for better diagnosing neurological disorders with minimal data acquisition resources. Many brain graph generative models were proposed for promising results, yet they are mostly based on generative adversarial networks (GAN), which could suffer from mode collapse and require large training datasets. Recent developments in diffusion models address these problems by offering essential properties such as a stable training objective and easy scalability. However, applying a diffusion process to graph edges fails to maintain the topological symmetry of the brain connectivity matrices. To meet these challenges, we propose the Graph Residual Noise Learner Network (Grenol-Net), the first graph diffusion model for predicting a target graph from a source graph."}, "https://arxiv.org/abs/2410.00126": {"title": "Resonance Reduction Against Adversarial Attacks in Dynamic Networks via Eigenspectrum Optimization", "link": "https://arxiv.org/abs/2410.00126", "description": "arXiv:2410.00126v1 Announce Type: new \nAbstract: Resonance is a well-known phenomenon that happens in systems with second order dynamics. In this paper we address the fundamental question of making a network robust to signal being periodically pumped into it at or near a resonant frequency by an adversarial agent with the aim of saturating the network with the signal. Towards this goal, we develop the notion of network vulnerability, which is measured by the expected resonance amplitude on the network under a stochastically modeled adversarial attack. Assuming a second order dynamics model based on the network graph Laplacian matrix and a known stochastic model for the adversarial attack, we propose two methods for minimizing the network vulnerability that leverage the principle of eigenspectrum optimization. We provide extensive numerical results analyzing the effects of both methods."}, "https://arxiv.org/abs/2410.00301": {"title": "Network Science in Psychology", "link": "https://arxiv.org/abs/2410.00301", "description": "arXiv:2410.00301v1 Announce Type: new \nAbstract: Social network analysis can answer research questions such as why or how individuals interact or form relationships and how those relationships impact other outcomes. Despite the breadth of methods available to address psychological research questions, social network analysis is not yet a standard practice in psychological research. To promote the use of social network analysis in psychological research, we present an overview of network methods, situating each method within the context of research studies and questions in psychology."}, "https://arxiv.org/abs/2410.00446": {"title": "Probabilistic modelling of car traffic accidents", "link": "https://arxiv.org/abs/2410.00446", "description": "arXiv:2410.00446v1 Announce Type: new \nAbstract: We introduce a counting process to model the random occurrence in time of car traffic accidents, taking into account some aspects of the self-excitation typical of this phenomenon. By combining methods from probability and differential equations, we study this stochastic process in terms of its statistical moments and large-time trend. Moreover, we derive analytically the probability density functions of the times of occurrence of traffic accidents and of the time elapsing between two consecutive accidents. Finally, we demonstrate the suitability of our modelling approach by means of numerical simulations, which address also a comparison with real data of weekly trends of traffic accidents."}, "https://arxiv.org/abs/2410.00668": {"title": "Unifying a Public Software Ecosystem: How Omaolo Responded to the COVID-19 Challenge", "link": "https://arxiv.org/abs/2410.00668", "description": "arXiv:2410.00668v1 Announce Type: new \nAbstract: Public actors are often seen as slow, especially in renewing information systems, due to complex tendering and competition regulations, which delay decisions. This challenge is even greater in multi-company ecosystems. However, when faced with a common threat, the ecosystem needs to unite to face the challenge. This study explores how the Omaolo ecosystem in Finland evolved from traditional public-private cooperation to an alliance model during the COVID-19 pandemic from 2020 to 2022. It highlights how the crisis accelerated changes in operations and collaboration between public and private participants, identifying key shifts, benefits, and challenges. Key findings include the removal of traditional barriers and the creation of an alliance approach that sped up the development of Omaolo's symptom assessment tool. This improved collaboration, service scalability, and responsiveness to healthcare needs despite the initial regulatory and stakeholder alignment challenges. The study concludes that crises can drive agile responses in public ecosystems. The new collaboration model helped Omaolo to adapt quickly to changing service demands, managing healthcare patient loads more effectively. These findings highlight the value of flexible, collaborative strategies for responding to emergencies in public software ecosystems."}, "https://arxiv.org/abs/2410.00697": {"title": "The Sensitivity of the U", "link": "https://arxiv.org/abs/2410.00697", "description": "arXiv:2410.00697v1 Announce Type: new \nAbstract: U.S. presidential elections are decided by the Electoral College, established in 1789, and designed to mitigate potential risks arising from the collusion of large groups of citizens. A statewide winner-take-all popular voting system for electors is implemented in all but two states, which has led to instances where narrow victories in key states were decisive in several recent elections. Small groups of voters can significantly impact the election, for example, through voter turnout. However, another dynamic can also influence this: a surprisingly small number of dedicated voters moving short distances across state lines. The extent to which the election's outcome is sensitive to small and well-coordinated movements of people has not been investigated in detail. Using a combination of forecasting, simulation, and optimization, we show that a candidate's probability of winning can be increased by 1% through the strategic relocation of approximately 10,000 people no farther than 100 miles from their current county of residence, less than 0.006% of the eligible voting population. Moreover, an 8% probability increase can be realized by a mere 50,000 voters relocating across state lines, or 0.03% of the voting population. Given the remarkably small number of people involved and the fact that establishing electoral residence in many states takes about a month, this coordinated relocation of voters is not nearly as challenging as previously thought. As it stands, U.S. presidential elections may be vulnerable to the exploitation of the aforementioned loophole. Therefore, we anticipate our findings will have direct consequences on policymaking and campaign strategy, as well as motivate new operations research methods within the political sciences."}, "https://arxiv.org/abs/2410.00724": {"title": "Discriminative community detection for multiplex networks", "link": "https://arxiv.org/abs/2410.00724", "description": "arXiv:2410.00724v1 Announce Type: new \nAbstract: Multiplex networks have emerged as a promising approach for modeling complex systems, where each layer represents a different mode of interaction among entities of the same type. A core task in analyzing these networks is to identify the community structure for a better understanding of the overall functioning of the network. While different methods have been proposed to detect the community structure of multiplex networks, the majority deal with extracting the consensus community structure across layers. In this paper, we address the community detection problem across two closely related multiplex networks. For example in neuroimaging studies, it is common to have multiple multiplex brain networks where each layer corresponds to an individual and each group to different experimental conditions. In this setting, one may be interested in both learning the community structure representing each experimental condition and the discriminative community structure between two groups. In this paper, we introduce two discriminative community detection algorithms based on spectral clustering. The first approach aims to identify the discriminative subgraph structure between the groups, while the second one learns the discriminative and the consensus community structures, simultaneously. The proposed approaches are evaluated on both simulated and real world multiplex networks."}, "https://arxiv.org/abs/2410.00725": {"title": "Early Career Citations Capture Judicial Idiosyncrasies and Predict Judgments", "link": "https://arxiv.org/abs/2410.00725", "description": "arXiv:2410.00725v1 Announce Type: new \nAbstract: Judicial impartiality is a cornerstone of well-functioning legal systems. We assemble a dataset of 112,312 civil lawsuits in U.S. District Courts to study the effect of extraneous factors on judicial decision making. We show that cases are randomly assigned to judges and that biographical judge features are predictive of judicial decisions. We use low-dimensional representations of judges' early-career citation records as generic representations of judicial idiosyncrasies. These predict future judgments with accuracies exceeding 65% for high-confidence predictions on balanced out-of-sample test cases. For 6-8% of judges, these representations are significant predictors across all judgments. These findings indicate that a small but significant group of judges routinely relies on extraneous factors and careful vetting of judges prior to appointment may partially address this issue. Our use of low-dimensional representations of citation records may also be generalized to other jurisdictions or to study other aspects of judicial decision making."}, "https://arxiv.org/abs/2410.00780": {"title": "Mutual benefits of social learning and algorithmic mediation for cumulative culture", "link": "https://arxiv.org/abs/2410.00780", "description": "arXiv:2410.00780v1 Announce Type: new \nAbstract: The remarkable ecological success of humans is often attributed to our ability to develop complex cultural artefacts that enable us to cope with environmental challenges. The evolution of complex culture (cumulative cultural evolution) is usually modeled as a collective process in which individuals invent new artefacts (innovation) and copy information from others (social learning).\n  This classic picture overlooks the fact that in the digital age, intelligent algorithms are increasingly mediating information between humans, with potential consequences for cumulative cultural evolution. Building on an established model of cultural evolution, we investigate the combined effects of network-based social learning and a simplistic version of algorithmic mediation on cultural accumulation. We find that algorithmic mediation has a strong impact on cultural accumulation and that this impact generally increases as social networks become less densely connected. Moreover, cultural accumulation tends to be optimal when social learning and algorithmic mediation are combined, and the optimal ratio depends on the network's density. Our modeling results are a first step towards formalising the impact of intelligent algorithms on cumulative cultural evolution within an established framework. Models of this kind will also help to uncover mechanisms of human-machine interaction in cultural contexts, guiding hypotheses for future experimental testing."}, "https://arxiv.org/abs/2410.00049": {"title": "Epidemiology-Aware Neural ODE with Continuous Disease Transmission Graph", "link": "https://arxiv.org/abs/2410.00049", "description": "arXiv:2410.00049v1 Announce Type: cross \nAbstract: Effective epidemic forecasting is critical for public health strategies and efficient medical resource allocation, especially in the face of rapidly spreading infectious diseases. However, existing deep-learning methods often overlook the dynamic nature of epidemics and fail to account for the specific mechanisms of disease transmission. In response to these challenges, we introduce an innovative end-to-end framework called Epidemiology-Aware Neural ODE with Continuous Disease Transmission Graph (EARTH) in this paper. To learn continuous and regional disease transmission patterns, we first propose EANO, which seamlessly integrates the neural ODE approach with the epidemic mechanism, considering the complex spatial spread process during epidemic evolution. Additionally, we introduce GLTG to model global infection trends and leverage these signals to guide local transmission dynamically. To accommodate both the global coherence of epidemic trends and the local nuances of epidemic transmission patterns, we build a cross-attention approach to fuse the most meaningful information for forecasting. Through the smooth synergy of both components, EARTH offers a more robust and flexible approach to understanding and predicting the spread of infectious diseases. Extensive experiments show EARTH superior performance in forecasting real-world epidemics compared to state-of-the-art methods. The code will be available at https://github.com/Emory-Melody/EpiLearn."}, "https://arxiv.org/abs/2410.00289": {"title": "Delving Deep into Engagement Prediction of Short Videos", "link": "https://arxiv.org/abs/2410.00289", "description": "arXiv:2410.00289v1 Announce Type: cross \nAbstract: Understanding and modeling the popularity of User Generated Content (UGC) short videos on social media platforms presents a critical challenge with broad implications for content creators and recommendation systems. This study delves deep into the intricacies of predicting engagement for newly published videos with limited user interactions. Surprisingly, our findings reveal that Mean Opinion Scores from previous video quality assessment datasets do not strongly correlate with video engagement levels. To address this, we introduce a substantial dataset comprising 90,000 real-world UGC short videos from Snapchat. Rather than relying on view count, average watch time, or rate of likes, we propose two metrics: normalized average watch percentage (NAWP) and engagement continuation rate (ECR) to describe the engagement levels of short videos. Comprehensive multi-modal features, including visual content, background music, and text data, are investigated to enhance engagement prediction. With the proposed dataset and two key metrics, our method demonstrates its ability to predict engagements of short videos purely from video content."}, "https://arxiv.org/abs/2410.00453": {"title": "The NetMob23 Dataset: Population Density and OD Matrices from Four LMIC Countries", "link": "https://arxiv.org/abs/2410.00453", "description": "arXiv:2410.00453v1 Announce Type: cross \nAbstract: The NetMob24 dataset offers a unique opportunity for researchers from a range of academic fields to access comprehensive spatiotemporal data sets spanning four countries (India, Mexico, Indonesia, and Colombia) over the course of two years (2019 and 2020). This dataset, developed in collaboration with Cuebiq (Also referred to as Spectus), comprises privacy-preserving aggregated data sets derived from mobile application (app) data collected from users who have voluntarily consented to anonymous data collection for research purposes. It is our hope that this reference dataset will foster the production of new research methods and the reproducibility of research outcomes."}, "https://arxiv.org/abs/2410.00601": {"title": "$k$-local Graphs", "link": "https://arxiv.org/abs/2410.00601", "description": "arXiv:2410.00601v1 Announce Type: cross \nAbstract: In 2017 Day et al. introduced the notion of locality as a structural complexity-measure for patterns in the field of pattern matching established by Angluin in 1980. In 2019 Casel et al. showed that determining the locality of an arbitrary pattern is NP-complete. Inspired by hierarchical clustering, we extend the notion to coloured graphs, i.e., given a coloured graph determine an enumeration of the colours such that colouring the graph stepwise according to the enumeration leads to as few clusters as possible. Next to first theoretical results on graph classes, we propose a priority search algorithm to compute the $k$-locality of a graph. The algorithm is optimal in the number of marking prefix expansions, and is faster by orders of magnitude than an exhaustive search. Finally, we perform a case study on a DBLP subgraph to demonstrate the potential of $k$-locality for knowledge discovery."}, "https://arxiv.org/abs/2410.00788": {"title": "The Risks of Scientific Gerontocracy", "link": "https://arxiv.org/abs/2410.00788", "description": "arXiv:2410.00788v1 Announce Type: cross \nAbstract: While much has been written about the problem of information overload in news and social media, little attention has been paid to its consequence in science. Scientific literature, however, has witnessed decades of exponential growth, to the point that the publications of the last twenty years now constitute 60% of all academic literature. This information overload is not without consequence. Our analysis reveals that, unlike other cultural products, scientific publications face unique challenges: the decreasing proportion of papers capturing large shares of researchers' attention and the slow turnover of influential papers lead to a disproportionate prominence of established works, resulting in stagnation and aging of scientific canons. To determine whether scientific hypergrowth is responsible for such ``gerontocratization of science'', we propose a generative model of paper citations based on random discovery and cumulative advantage, with a varying number of new papers each year. Our findings show that, as exponential growth intensifies, gerontocratization appears and becomes increasingly pronounced. Recognizing and understanding this mechanism is hence essential for developing targeted strategies to counteract this trend and promote a balanced and healthy renewal of scientific canons."}, "https://arxiv.org/abs/2410.00860": {"title": "Enhancing Web Spam Detection through a Blockchain-Enabled Crowdsourcing Mechanism", "link": "https://arxiv.org/abs/2410.00860", "description": "arXiv:2410.00860v1 Announce Type: cross \nAbstract: The proliferation of spam on the Web has necessitated the development of machine learning models to automate their detection. However, the dynamic nature of spam and the sophisticated evasion techniques employed by spammers often lead to low accuracy in these models. Traditional machine-learning approaches struggle to keep pace with spammers' constantly evolving tactics, resulting in a persistent challenge to maintain high detection rates. To address this, we propose blockchain-enabled incentivized crowdsourcing as a novel solution to enhance spam detection systems. We create an incentive mechanism for data collection and labeling by leveraging blockchain's decentralized and transparent framework. Contributors are rewarded for accurate labels and penalized for inaccuracies, ensuring high-quality data. A smart contract governs the submission and evaluation process, with participants staking cryptocurrency as collateral to guarantee integrity. Simulations show that incentivized crowdsourcing improves data quality, leading to more effective machine-learning models for spam detection. This approach offers a scalable and adaptable solution to the challenges of traditional methods."}, "https://arxiv.org/abs/2410.00866": {"title": "\"I don't trust them\": Exploring Perceptions of Fact-checking Entities for Flagging Online Misinformation", "link": "https://arxiv.org/abs/2410.00866", "description": "arXiv:2410.00866v1 Announce Type: cross \nAbstract: The spread of misinformation through online social media platforms has had substantial societal consequences. As a result, platforms have introduced measures to alert users of news content that may be misleading or contain inaccuracies as a means to discourage them from sharing it. These interventions sometimes cite external sources, such as fact-checking organizations and news outlets, for providing assessments related to the accuracy of the content. However, it is unclear whether users trust the assessments provided by these entities and whether perceptions vary across different topics of news. We conducted an online study with 655 US participants to explore user perceptions of eight categories of fact-checking entities across two misinformation topics, as well as factors that may impact users' perceptions. We found that participants' opinions regarding the trustworthiness and bias of the entities varied greatly, aligning largely with their political preference. However, just the presence of a fact-checking label appeared to discourage participants from sharing the headlines studied. Our results hint at the need for further exploring fact-checking entities that may be perceived as neutral, as well as the potential for incorporating multiple assessments in such labels."}, "https://arxiv.org/abs/2307.10151": {"title": "A pair-based approximation for simplicial contagion", "link": "https://arxiv.org/abs/2307.10151", "description": "arXiv:2307.10151v2 Announce Type: replace \nAbstract: Higher-order interactions play an important role in complex contagion processes. Mean-field approximations have been used to characterize the onset of spreading in the presence of group interactions. However, individual-based mean-field models are unable to capture correlations between different subsets of nodes, which can significantly influence the dynamics of a contagion process. In this paper, we introduce a pair-based mean-field approximation that allows to study the dynamics of a SIS model on simplicial complexes by taking into account correlations at the level of pairs of nodes. %by taking into account dynamical correlations emerging in groups of nodes. Compared to individual-based mean-field approaches, the proposed approximation yields more accurate predictions of the dynamics of contagion processes on simplicial complexes. Specifically, the pair-based mean-field approximation provides higher accuracy in predicting the extent of the region of bistability, the type of transition from disease-free to endemic state, and the average time evolution of the fraction of infected individuals. Crucially, the pair-based approximation correctly predicts that the onset of the epidemic outbreak in simplicial complexes depends on the strength of higher-order interactions. Overall, our findings highlight the importance of accounting for pair correlations when investigating contagion processes in the presence of higher-order interactions."}, "https://arxiv.org/abs/2311.10270": {"title": "Multiscale Hodge Scattering Networks for Data Analysis", "link": "https://arxiv.org/abs/2311.10270", "description": "arXiv:2311.10270v3 Announce Type: replace-cross \nAbstract: We propose new scattering networks for signals measured on simplicial complexes, which we call \\emph{Multiscale Hodge Scattering Networks} (MHSNs). Our construction is based on multiscale basis dictionaries on simplicial complexes, i.e., the $\\kappa$-GHWT and $\\kappa$-HGLET, which we recently developed for simplices of dimension $\\kappa \\in \\mathbb{N}$ in a given simplicial complex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT) and Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\\kappa$-GHWT and the $\\kappa$-HGLET both form redundant sets (i.e., dictionaries) of multiscale basis vectors and the corresponding expansion coefficients of a given signal. Our MHSNs use a layered structure analogous to a convolutional neural network (CNN) to cascade the moments of the modulus of the dictionary coefficients. The resulting features are invariant to reordering of the simplices (i.e., node permutation of the underlying graphs). Importantly, the use of multiscale basis dictionaries in our MHSNs admits a natural pooling operation that is akin to local pooling in CNNs, and which may be performed either locally or per-scale. These pooling operations are harder to define in both traditional scattering networks based on Morlet wavelets, and geometric scattering networks based on Diffusion Wavelets. As a result, we are able to extract a rich set of descriptive yet robust features that can be used along with very simple machine learning methods (i.e., logistic regression or support vector machines) to achieve high-accuracy classification systems with far fewer parameters to train than most modern graph neural networks. Finally, we demonstrate the usefulness of our MHSNs in three distinct types of problems: signal classification, domain (i.e., graph/simplex) classification, and molecular dynamics prediction."}, "https://arxiv.org/abs/2410.01155": {"title": "Consequences of non-Markovian healing processes on epidemic models with recurrent infections on networks", "link": "https://arxiv.org/abs/2410.01155", "description": "arXiv:2410.01155v1 Announce Type: new \nAbstract: Infections diseases are marked by recovering time distributions which can be far from the exponential one associated with Markovian/Poisson processes, broadly applied in epidemic compartmental models. In the present work, we tackled this problem by investigating a susceptible-infected-recovered-susceptible model on networks with $\\eta$ independent infectious compartments (SI$_{\\eta}$RS), each one with a Markovian dynamics, that leads to a Gamma-distributed recovering times. We analytically develop a theory for the epidemic lifespan on star graphs with a center and $K$ leaves showing that the epidemic lifespan scales with a non-universal power-law $\\tau_{K}\\sim K^{\\alpha/\\mu\\eta}$ plus logarithm corrections, where $\\alpha^{-1}$ and $\\mu^{-1}$ are the mean waning immunity and recovering times, respectively. Compared with standard SIRS dynamics with $\\eta=1$ and the same mean recovering time, the epidemic lifespan on star graphs is severely reduced as the number of stages increases. In particular, the case $\\eta\\rightarrow\\infty$ leads to a finite lifespan. Numerical simulations support the approximated analytical calculations. For the SIS dynamics, numerical simulations show that the lifespan increases exponentially with the number of leaves, with a nonuniversal rate that decays with the number of infectious compartments. We investigated the SI$_{\\eta}$RS dynamics on power-law networks with degree distribution $P(K)\\sim k^{-\\gamma}$. When $\\gamma<5/2$, the epidemic spreading is ruled by a maximum $k$-core activation, the alteration of the hub activity time does not alter either the epidemic threshold or the localization pattern. For $\\gamma>3$, where hub mutual activation is at work, the localization is reduced but not sufficiently to alter the threshold scaling with the network size. Therefore, the activation mechanisms remain the same as in the case of Markovian healing."}, "https://arxiv.org/abs/2410.01320": {"title": "Detecting Viral Social Events through Censored Observation with Deep Survival Analysis", "link": "https://arxiv.org/abs/2410.01320", "description": "arXiv:2410.01320v1 Announce Type: new \nAbstract: Users increasing activity across various social networks made it the most widely used platform for exchanging and propagating information among individuals. To spread information within a network, a user initially shared information on a social network, and then other users in direct contact with him might have shared that information. Information expanded throughout the network by repeatedly following this process. A set of information that became popular and was repeatedly shared by different individuals was called viral events. Identifying and analyzing viral social events led to valuable insights into the dynamics of information dissemination within a network. However, more importantly, proactive approaches emerged. In other words, by observing the dissemination pattern of a piece of information in the early stages of expansion, it became possible to determine whether this cascade would become viral in the future. This research aimed to predict and detect viral events in social networks by observing granular information and using a deep survival analysis-based method. This model could play a significant role in identifying rumors, predicting the impact of information, and assisting in optimal decision-making in information management and marketing. Ultimately, the proposed method was tested on various real-world datasets from Twitter, Weibo, and Digg."}, "https://arxiv.org/abs/2410.01402": {"title": "CompLex: legal systems through the lens of complexity science", "link": "https://arxiv.org/abs/2410.01402", "description": "arXiv:2410.01402v1 Announce Type: new \nAbstract: While \"complexity science\" has achieved significant successes in several interdisciplinary fields such as economics and biology, it is only a very recent observation that legal systems -- from the way legal texts are drafted and connected to the rest of the corpus, up to the level of how judges and courts reach decisions under a variety of conflicting inputs -- share several features with standard Complex Adaptive Systems. This review is meant as a gentle introduction to the use of quantitative tools and techniques of complexity science to describe, analyse, and tame the complex web of human interactions that the Law is supposed to regulate. We offer an overview of the main directions of research undertaken so far as well as an outlook for future research, and we argue that statistical physicists and complexity scientists should not ignore the opportunities offered by the cross-fertilisation between legal scholarship and complex-systems modelling."}, "https://arxiv.org/abs/2410.01510": {"title": "Measuring Global Urban Complexity from the Perspective of Living Structure", "link": "https://arxiv.org/abs/2410.01510", "description": "arXiv:2410.01510v1 Announce Type: new \nAbstract: As urban critic Jane Jacobs conceived, a city is essentially the problem of organized complexity. What underlies the complexity refers to a structural factor, called living structure, which is defined as a mathematical structure composed of hierarchically organized substructures. Through these substructures, the complexity of cities, or equivalent to the livingness of urban space (L), can be measured by the multiplication the number of cities or substructures (S) and their scaling hierarchy (H), indicating that complexity is about both quantity of cities and how well the city is organized hierarchically. In other words, complexity emerges from a hierarchical structure where there are far more small cities or substructures than large ones across all scales, and cities are more or less similar within each individual hierarchical level. In this paper, we conduct comprehensive case studies to investigate urban complexity on a global scale using multisource geospatial data. We develop an efficient approach to recursively identifying all natural cities with their inner hotspots worldwide through connected component analysis. To characterize urban complexity, urban space is initially represented as a hierarchy of recursively defined natural cities, and all the cities are then represented as a network for measuring the degree of complexity or livingness of the urban space. The results show the Earth's surface is growing more complex from an economic perspective, and the dynamics of urban complexity are more explicit from nighttime light imagery than from population data. We further discuss the implications in city science, aiming to help create and recreate urban environments that are more resilient and livable by fostering organized complexity from the perspective of living structure."}, "https://arxiv.org/abs/2410.01607": {"title": "Spread of Crime Dynamics: A mathematical approach", "link": "https://arxiv.org/abs/2410.01607", "description": "arXiv:2410.01607v1 Announce Type: new \nAbstract: In this work, the spread of crime dynamics in the US is analyzed from a mathematical scope, an epidemiological model is established, including five compartments: Susceptible (S), Latent 1 (E1), Latent 2 (E2), Incarcerated (I), and Recovered (R). A system of differential equations is used to model the spread of crime. A result to show the positivity of the solutions for the system is included. The basic reproduction number and the stability for the disease-free equilibrium results are calculated following epidemiological theories. Numerical simulations are performed with US parameter values. Understanding the dynamics of the spread of crime helps to determine what factors may work best together to reduce violent crime."}, "https://arxiv.org/abs/2410.01635": {"title": "Does Graph Prompt Work? A Data Operation Perspective with Theoretical Analysis", "link": "https://arxiv.org/abs/2410.01635", "description": "arXiv:2410.01635v1 Announce Type: cross \nAbstract: In recent years, graph prompting has emerged as a promising research direction, enabling the learning of additional tokens or subgraphs appended to the original graphs without requiring retraining of pre-trained graph models across various applications. This novel paradigm, shifting from the traditional pretraining and finetuning to pretraining and prompting has shown significant empirical success in simulating graph data operations, with applications ranging from recommendation systems to biological networks and graph transferring. However, despite its potential, the theoretical underpinnings of graph prompting remain underexplored, raising critical questions about its fundamental effectiveness. The lack of rigorous theoretical proof of why and how much it works is more like a dark cloud over the graph prompt area to go further. To fill this gap, this paper introduces a theoretical framework that rigorously analyzes graph prompting from a data operation perspective. Our contributions are threefold: First, we provide a formal guarantee theorem, demonstrating graph prompts capacity to approximate graph transformation operators, effectively linking upstream and downstream tasks. Second, we derive upper bounds on the error of these data operations by graph prompts for a single graph and extend this discussion to batches of graphs, which are common in graph model training. Third, we analyze the distribution of data operation errors, extending our theoretical findings from linear graph models (e.g., GCN) to non-linear graph models (e.g., GAT). Extensive experiments support our theoretical results and confirm the practical implications of these guarantees."}, "https://arxiv.org/abs/2410.01708": {"title": "Examining the Role of Relationship Alignment in Large Language Models", "link": "https://arxiv.org/abs/2410.01708", "description": "arXiv:2410.01708v1 Announce Type: cross \nAbstract: The rapid development and deployment of Generative AI in social settings raise important questions about how to optimally personalize them for users while maintaining accuracy and realism. Based on a Facebook public post-comment dataset, this study evaluates the ability of Llama 3.0 (70B) to predict the semantic tones across different combinations of a commenter's and poster's gender, age, and friendship closeness and to replicate these differences in LLM-generated comments.\n  The study consists of two parts: Part I assesses differences in semantic tones across social relationship categories, and Part II examines the similarity between comments generated by Llama 3.0 (70B) and human comments from Part I given public Facebook posts as input. Part I results show that including social relationship information improves the ability of a model to predict the semantic tone of human comments. However, Part II results show that even without including social context information in the prompt, LLM-generated comments and human comments are equally sensitive to social context, suggesting that LLMs can comprehend semantics from the original post alone. When we include all social relationship information in the prompt, the similarity between human comments and LLM-generated comments decreases. This inconsistency may occur because LLMs did not include social context information as part of their training data. Together these results demonstrate the ability of LLMs to comprehend semantics from the original post and respond similarly to human comments, but also highlights their limitations in generalizing personalized comments through prompting alone."}, "https://arxiv.org/abs/2305.06888": {"title": "Socioeconomic disparities in mobility behavior during the COVID-19 pandemic in developing countries", "link": "https://arxiv.org/abs/2305.06888", "description": "arXiv:2305.06888v3 Announce Type: replace \nAbstract: Mobile phone data have played a key role in quantifying human mobility during the COVID-19 pandemic. Existing studies on mobility patterns have primarily focused on regional aggregates in high-income countries, obfuscating the accentuated impact of the pandemic on the most vulnerable populations. Leveraging geolocation data from mobile-phone users and population census for 6 middle-income countries across 3 continents between March and December 2020, we uncovered common disparities in the behavioral response to the pandemic across socioeconomic groups. Users living in low-wealth neighborhoods were less likely to respond by self-isolating, relocating to rural areas, or refraining from commuting to work. The gap in the behavioral responses between socioeconomic groups persisted during the entire observation period. Among users living in low-wealth neighborhoods, those who commute to work in high-wealth neighborhoods pre-pandemic were particularly at risk of experiencing economic stress, facing both the reduction in economic activity in the high-wealth neighborhood and being more likely to be affected by public transport closures due to their longer commute distances. While confinement policies were predominantly country-wide, these results suggest that, when data to identify vulnerable individuals are not readily available, GPS-based analytics could help design targeted place-based policies to aid the most vulnerable."}, "https://arxiv.org/abs/1803.05136": {"title": "A primer on the use of probability generating functions in infectious disease modeling", "link": "https://arxiv.org/abs/1803.05136", "description": "arXiv:1803.05136v4 Announce Type: replace-cross \nAbstract: We explore the application of probability generating functions (PGFs) to invasive processes, focusing on infectious disease introduced into large populations. Our goal is to acquaint the reader with applications of PGFs, moreso than to derive new results. PGFs help predict a number of properties about early outbreak behavior while the population is still effectively infinite, including the probability of an epidemic, the size distribution after some number of generations, and the cumulative size distribution of non-epidemic outbreaks. We show how PGFs can be used in both discrete-time and continuous-time settings, and discuss how to use these results to infer disease parameters from observed outbreaks. In the large population limit for susceptible-infected-recovered (SIR) epidemics PGFs lead to survival-function based models that are equivalent the the usual mass-action SIR models but with fewer ODEs. We use these to explore properties such as the final size of epidemics or even the dynamics once stochastic effects are negligible. We target this tutorial to biologists and public health researchers who want to learn how to apply PGFs to invasive diseases, but it could also be used in an introductory mathematics course on PGFs. We include many exercises to help demonstrate concepts and to give practice applying the results. We summarize our main results in a few tables. Additionally we provide a small python package which performs many of the relevant calculations."}, "https://arxiv.org/abs/2105.00110": {"title": "Triangle Centrality", "link": "https://arxiv.org/abs/2105.00110", "description": "arXiv:2105.00110v3 Announce Type: replace-cross \nAbstract: Triangle centrality is introduced for finding important vertices in a graph based on the concentration of triangles surrounding each vertex. It has the distinct feature of allowing a vertex to be central if it is in many triangles or none at all.\n  We show experimentally that triangle centrality is broadly applicable to many different types of networks. Our empirical results demonstrate that 30% of the time triangle centrality identified central vertices that differed with those found by five well-known centrality measures, which suggests novelty without being overly specialized. It is also asymptotically faster to compute on sparse graphs than all but the most trivial of these other measures.\n  We introduce optimal algorithms that compute triangle centrality in $O(m\\bar\\delta)$ time and $O(m+n)$ space, where $\\bar\\delta\\le O(\\sqrt{m})$ is the $\\textit{average degeneracy}$ introduced by Burkhardt, Faber, and Harris (2020). In practical applications, $\\bar\\delta$ is much smaller than $\\sqrt{m}$ so triangle centrality can be computed in nearly linear time. On a Concurrent Read Exclusive Write (CREW) Parallel Random Access Machine (PRAM), we give a near work-optimal parallel algorithm that takes $O(\\log n)$ time using $O(m\\sqrt{m})$ CREW PRAM processors. In MapReduce, we show it takes four rounds using $O(m\\sqrt{m})$ communication bits and is therefore optimal. We also derive a linear algebraic formulation of triangle centrality which can be computed in $O(m\\bar\\delta)$ time on sparse graphs."}, "https://arxiv.org/abs/2309.11880": {"title": "Polynomial growth in degree-dependent first passage percolation on spatial random graphs", "link": "https://arxiv.org/abs/2309.11880", "description": "arXiv:2309.11880v3 Announce Type: replace-cross \nAbstract: In this paper we study a version of (non-Markovian) first passage percolation on graphs, where the transmission time between two connected vertices is non-iid, but increases by a penalty factor polynomial in their expected degrees. Based on the exponent of the penalty-polynomial, this makes it increasingly harder to transmit to and from high-degree vertices. This choice is motivated by awareness or time-limitations. For the iid part of the transmission times we allow any nonnegative distribution with regularly varying behaviour at $0$. For the underlying graph models we choose spatial random graphs that have power-law degree distributions, so that the effect of the penalisation becomes visible: (finite and infinite) Geometric Inhomogeneous Random Graphs, and Scale-Free Percolation. In these spatial models, the connection probability between two vertices depends on their spatial distance and on their expected degrees. We prove that upon increasing the penalty exponent, the transmission time between two far away vertices $x,y$ sweeps through four universal phases even for a single underlying graph: explosive (tight transmission times), polylogarithmic, polynomial but sublinear ($|x-y|^{\\eta_0+o(1)}$ for an explicit $\\eta_0<1$), and linear ($\\Theta(|x-y|)$) in their Euclidean distance. Further, none of these phases are restricted to phase boundaries, and those are non-trivial in the main model parameters: the tail of the degree-distribution, a long-range parameter, and the exponent of regular variation of the iid part of the transmission times. In this paper we present proofs of lower bounds for the latter two phases and the upper bound for the linear phase. These complement the matching upper bounds for the polynomial regime in our companion paper."}, "https://arxiv.org/abs/2410.01838": {"title": "What we should learn from pandemic publishing", "link": "https://arxiv.org/abs/2410.01838", "description": "arXiv:2410.01838v1 Announce Type: new \nAbstract: Authors of COVID-19 papers produced during the pandemic were overwhelmingly not subject matter experts. Such a massive inflow of scholars from different expertise areas is both an asset and a potential problem. Domain-informed scientific collaboration is the key to preparing for future crises."}, "https://arxiv.org/abs/2410.01842": {"title": "Public interest in science or bots? Selective amplification of scientific articles on Twitter", "link": "https://arxiv.org/abs/2410.01842", "description": "arXiv:2410.01842v1 Announce Type: new \nAbstract: With the remarkable capability to reach the public instantly, social media has become integral in sharing scholarly articles to measure public response. Since spamming by bots on social media can steer the conversation and present a false public interest in given research, affecting policies impacting the public's lives in the real world, this topic warrants critical study and attention. We used the Altmetric dataset in combination with data collected through the Twitter Application Programming Interface (API) and the Botometer API. We combined the data into an extensive dataset with academic articles, several features from the article and a label indicating whether the article had excessive bot activity on Twitter or not. We analyzed the data to see the possibility of bot activity based on different characteristics of the article. We also trained machine-learning models using this dataset to identify possible bot activity in any given article. Our machine-learning models were capable of identifying possible bot activity in any academic article with an accuracy of 0.70. We also found that articles related to \"Health and Human Science\" are more prone to bot activity compared to other research areas. Without arguing the maliciousness of the bot activity, our work presents a tool to identify the presence of bot activity in the dissemination of an academic article and creates a baseline for future research in this direction."}, "https://arxiv.org/abs/2410.01844": {"title": "Urban Anomalies: A Simulated Human Mobility Dataset with Injected Anomalies", "link": "https://arxiv.org/abs/2410.01844", "description": "arXiv:2410.01844v1 Announce Type: new \nAbstract: Human mobility anomaly detection based on location is essential in areas such as public health, safety, welfare, and urban planning. Developing models and approaches for location-based anomaly detection requires a comprehensive dataset. However, privacy concerns and the absence of ground truth hinder the availability of publicly available datasets. With this paper, we provide extensive simulated human mobility datasets featuring various anomaly types created using an existing Urban Patterns of Life Simulation. To create these datasets, we inject changes in the logic of individual agents to change their behavior. Specifically, we create four of anomalous agent behavior by (1) changing the agents' appetite (causing agents to have meals more frequently), (2) changing their group of interest (causing agents to interact with different agents from another group). (3) changing their social place selection (causing agents to visit different recreational places) and (4) changing their work schedule (causing agents to skip work), For each type of anomaly, we use three degrees of behavioral change to tune the difficulty of detecting the anomalous agents. To select agents to inject anomalous behavior into, we employ three methods: (1) Random selection using a centralized manipulation mechanism, (2) Spread based selection using an infectious disease model, and (3) through exposure of agents to a specific location. All datasets are split into normal and anomalous phases. The normal phase, which can be used for training models of normalcy, exhibits no anomalous behavior. The anomalous phase, which can be used for testing for anomalous detection algorithm, includes ground truth labels that indicate, for each five-minute simulation step, which agents are anomalous at that time. Datasets are generated using the maps (roads and buildings) for Atlanta and Berlin, having 1k agents in each simulation."}, "https://arxiv.org/abs/2410.01865": {"title": "Simplifying complex machine learning by linearly separable network embedding spaces", "link": "https://arxiv.org/abs/2410.01865", "description": "arXiv:2410.01865v1 Announce Type: new \nAbstract: Low-dimensional embeddings are a cornerstone in the modelling and analysis of complex networks. However, most existing approaches for mining network embedding spaces rely on computationally intensive machine learning systems to facilitate downstream tasks. In the field of NLP, word embedding spaces capture semantic relationships \\textit{linearly}, allowing for information retrieval using \\textit{simple linear operations} on word embedding vectors. Here, we demonstrate that there are structural properties of network data that yields this linearity. We show that the more homophilic the network representation, the more linearly separable the corresponding network embedding space, yielding better downstream analysis results. Hence, we introduce novel graphlet-based methods enabling embedding of networks into more linearly separable spaces, allowing for their better mining. Our fundamental insights into the structure of network data that enable their \\textit{\\textbf{linear}} mining and exploitation enable the ML community to build upon, towards efficiently and explainably mining of the complex network data."}, "https://arxiv.org/abs/2410.02071": {"title": "Estimating Disaster Resilience of Hurricane Helene on Florida Counties", "link": "https://arxiv.org/abs/2410.02071", "description": "arXiv:2410.02071v1 Announce Type: new \nAbstract: This paper presents a rapid approach to assessing disaster resilience in Florida, particularly regarding Hurricane Helene (2024). This category four storm made landfall on Florida's Gulf Coast in September 2024. Using the Disaster Resilience Index (DRI) developed in this paper, the preparedness and adaptive capacities of communities across counties in Florida are evaluated, identifying the most resilient areas based on three key variables: population size, average per-person income, and the Social Vulnerability Index (SVI). While the Social Vulnerability Index (SVI) accounts for factors like socioeconomic status, household composition, minority status, and housing conditions-key elements in determining a community's resilience to disasters-incorporating a county's population and per person income provides additional insight. A county's total population is directly linked to the number of individuals impacted by a disaster, while personal income reflects a household's capacity to recover. Spatial analysis was performed on the index to compare the vulnerability and resilience levels across thirty-four counties vulnerable to Hurricane Helene's projected path. The results highlight that counties with high income and lower population densities, such as Monroe and Collier, exhibit greater resilience. In contrast, areas with larger populations and higher social vulnerabilities are at greater risk of damage. This study contributes to disaster management planning by providing a rapid yet comprehensive and reassuring socioeconomic impact assessment, offering actionable insights for anticipatory measures and resource allocation."}, "https://arxiv.org/abs/2410.02118": {"title": "A Comprehensive Review of Propagation Models in Complex Networks: From Deterministic to Deep Learning Approaches", "link": "https://arxiv.org/abs/2410.02118", "description": "arXiv:2410.02118v1 Announce Type: new \nAbstract: Understanding propagation mechanisms in complex networks is essential for fields like epidemiology and multi-robot networks. This paper reviews various propagation models, from traditional deterministic frameworks to advanced data-driven and deep learning approaches. We differentiate between static and dynamic networks, noting that static models provide foundational insights, while dynamic models capture real-world temporal changes. Deterministic models like the SIR framework offer clear mathematical insights but often lack adaptability to randomness, whereas stochastic models enhance realism at the cost of interpretability. Behavior-based models focus on individual decision-making, demanding more computational resources. Data-driven approaches improve accuracy in nonlinear scenarios by adapting to evolving networks, using either traditional models or model-free machine learning techniques. We explore supervised and unsupervised learning methods, as well as reinforcement learning, which operates without predefined datasets. The application of graph neural networks (GNNs) is also discussed, highlighting their effectiveness in modeling propagation in complex networks. The paper underscores key applications and challenges associated with each model type, emphasizing the increasing importance of hybrid and machine learning-based solutions in contemporary network propagation issues."}, "https://arxiv.org/abs/2410.02333": {"title": "Fragility of Chess positions: measure, universality and tipping points", "link": "https://arxiv.org/abs/2410.02333", "description": "arXiv:2410.02333v1 Announce Type: new \nAbstract: We introduce a novel metric to quantify the fragility of chess positions using the interaction graph of pieces. This fragility score $F$ captures the tension within a position and serves as a strong indicator of tipping points in a game. In well-known games, maximum fragility often aligns with decisive moments marked by brilliant moves. Analyzing a large dataset of games, we find that fragility typically peaks around move $15$, with pawns ($\\approx 60\\%$) and knights ($\\approx 20\\%$) frequently involved in high-tension positions. Remarkably, average fragility curves show a universal pattern across a wide range of players, games, and openings, with a subtle deviation observed in games played by the engine Stockfish. Our analysis reveals a gradual buildup of fragility starting around $8$ moves before the peak, followed by a prolonged fragile state lasting up to $15$ moves. This suggests a gradual intensification of positional tension leading to decisive moments in the game. These insights offer a valuable tool for both players and engines to assess critical moments in chess."}, "https://arxiv.org/abs/2410.02366": {"title": "Estimating the population-level effects of non-pharmaceutical interventions when transmission rates of COVID-19 vary by orders of magnitude from one contact to another", "link": "https://arxiv.org/abs/2410.02366", "description": "arXiv:2410.02366v1 Announce Type: new \nAbstract: Statistical physicists have long studied systems where the variable of interest spans many orders of magnitude, the classic example is the relaxation times of glassy materials, which are often found to follow power laws. A power-law dependence has been found for the probability of transmission of COVID-19, as a function of length of time a susceptible person is in contact with an infected person. This is in data from the United Kingdom's COVID-19 app. The amount of virus in infected people spans many orders of magnitude. Inspired by this I assume that the power-law behaviour found in COVID-19 transmission, is due to the effective transmission rate varying over orders of magnitude from one contact to another. I then use a model from statistical physics to estimate that if a population all wear FFP2/N95 masks, this reduces the effective reproduction number for COVID-19 transmission by a factor of approximately nine."}, "https://arxiv.org/abs/2410.02582": {"title": "Spontaneous Symmetry Breaking, Group Decision Making and Beyond 1", "link": "https://arxiv.org/abs/2410.02582", "description": "arXiv:2410.02582v1 Announce Type: new \nAbstract: Starting from a symmetrical multiple choice individual I build a sociophysics model of decision making. Reducing the choices to two and interactions to pairs recover the Ising model from physics at zero temperature. The associated equilibrium state is a spontaneous symmetry breaking with the whole group sharing a unique choice selected at random. However, my focus departs from physics, which aims at identifying the true equilibrium state discarding any possible impact of the initial conditions, the size of the sample and the used update algorithm. Any memory of the past history is erased. In contrast, I claim that dealing with a social system, the history of the system must be taken into account in identifying the relevant social equilibrium state. Using Monte Carlo simulations I explore the spectrum of non universal equilibrium states of the Ising model at zero temperature. I show that different initial conditions with the same value of the order parameter lead to different equilibrium states. The same applies for different sizes and different update algorithms. The results indicate that in the presence of a social network composed of agents sharing different initial opinions, it is their interactions that lead them to share a unique choice and not their mere membership in the network. This finding sheds a new light on the emergence of echo chambers, which appear to be the end of a dynamical process of opinion update and not its beginning with a preferential attachment. Furthermore, polarization is obtained as a side effect of the random selection of the respective unanimous choices of the various echo chambers within a social community. The study points to social media exchange algorithms, which are purely technical levers independent of the issue and opinions at stake, to tackle polarization by either hindering or accelerating the completion of symmetry breaking between agents."}, "https://arxiv.org/abs/2410.01987": {"title": "Financial Sentiment Analysis on News and Reports Using Large Language Models and FinBERT", "link": "https://arxiv.org/abs/2410.01987", "description": "arXiv:2410.01987v1 Announce Type: cross \nAbstract: Financial sentiment analysis (FSA) is crucial for evaluating market sentiment and making well-informed financial decisions. The advent of large language models (LLMs) such as BERT and its financial variant, FinBERT, has notably enhanced sentiment analysis capabilities. This paper investigates the application of LLMs and FinBERT for FSA, comparing their performance on news articles, financial reports and company announcements. The study emphasizes the advantages of prompt engineering with zero-shot and few-shot strategy to improve sentiment classification accuracy. Experimental results indicate that GPT-4o, with few-shot examples of financial texts, can be as competent as a well fine-tuned FinBERT in this specialized field."}, "https://arxiv.org/abs/2410.02011": {"title": "A Census-Based Genetic Algorithm for Target Set Selection Problem in Social Networks", "link": "https://arxiv.org/abs/2410.02011", "description": "arXiv:2410.02011v1 Announce Type: cross \nAbstract: This paper considers the Target Set Selection (TSS) Problem in social networks, a fundamental problem in viral marketing. In the TSS problem, a graph and a threshold value for each vertex of the graph are given. We need to find a minimum size vertex subset to \"activate\" such that all graph vertices are activated at the end of the propagation process. Specifically, we propose a novel approach called \"a census-based genetic algorithm\" for the TSS problem. In our algorithm, we use the idea of a census to gather and store information about each individual in a population and collect census data from the individuals constructed during the algorithm's execution so that we can achieve greater diversity and avoid premature convergence at locally optimal solutions. We use two distinct census information: (a) for each individual, the algorithm stores how many times it has been identified during the execution (b) for each network node, the algorithm counts how many times it has been included in a solution. The proposed algorithm can also self-adjust by using a parameter specifying the aggressiveness employed in each reproduction method. Additionally, the algorithm is designed to run in a parallelized environment to minimize the computational cost and check each individual's feasibility. Moreover, our algorithm finds the optimal solution in all cases while experimenting on random graphs. Furthermore, we execute the proposed algorithm on 14 large graphs of real-life social network instances from the literature, improving around 9.57 solution size (on average) and 134 vertices (in total) compared to the best solutions obtained in previous studies."}, "https://arxiv.org/abs/2311.09536": {"title": "Correlation networks: Interdisciplinary approaches beyond thresholding", "link": "https://arxiv.org/abs/2311.09536", "description": "arXiv:2311.09536v2 Announce Type: replace \nAbstract: Many empirical networks originate from correlational data, arising in domains as diverse as psychology, neuroscience, genomics, microbiology, finance, and climate science. Specialized algorithms and theory have been developed in different application domains for working with such networks, as well as in statistics, network science, and computer science, often with limited communication between practitioners in different fields. This leaves significant room for cross-pollination across disciplines. A central challenge is that it is not always clear how to best transform correlation matrix data into networks for the application at hand, and probably the most widespread method, i.e., thresholding on the correlation value to create either unweighted or weighted networks, suffers from multiple problems. In this article, we review various methods of constructing and analyzing correlation networks, ranging from thresholding and its improvements to weighted networks, regularization, dynamic correlation networks, threshold-free approaches, comparison with null models, and more. Finally, we propose and discuss recommended practices and a variety of key open questions currently confronting this field."}, "https://arxiv.org/abs/2401.16610": {"title": "Perceptions of Moderators as a Large-Scale Measure of Online Community Governance", "link": "https://arxiv.org/abs/2401.16610", "description": "arXiv:2401.16610v2 Announce Type: replace \nAbstract: Millions of online communities are governed by volunteer moderators, who shape their communities by setting and enforcing rules, recruiting additional moderators, and participating in the community themselves. These moderators must regularly make decisions about how to govern, yet measuring the 'success' of governance is complex and nuanced, making it challenging to determine what governance strategies are most successful. Furthermore, prior work has shown that communities have differing values, suggesting that 'one-size-fits-all' approaches to governance are unlikely to serve all communities well. In this work, we assess governance practices on reddit by classifying the sentiment of community members' public discussion of their own moderators. We label 1.89 million posts and comments made on reddit over an 18 month period. We relate these perceptions to characteristics of community governance, and to different actions that community moderators take. We identify types of communities where moderators are perceived particularly positively and negatively, and highlight promising strategies for moderator teams. Amongst other findings, we show that strict rule enforcement is linked to more favorable perceptions of moderators of communities dedicated to certain topics, such as news communities, than others. We investigate what kinds of moderators are associated with improved community perceptions upon their addition to a mod team, and find that moderators who are active community members before and during their mod tenures are seen more favorably. We make all our models, datasets, and code public."}, "https://arxiv.org/abs/2404.04710": {"title": "Explaining Indian Stock Market through Geometry of Scale free Networks", "link": "https://arxiv.org/abs/2404.04710", "description": "arXiv:2404.04710v2 Announce Type: replace \nAbstract: In this study, we model the Indian stock market as heterogenous scale free network, which is then embedded in a two dimensional hyperbolic space through a machine learning based technique called as coalescent embedding. This allows us to apply the hyperbolic kmeans algorithm on the Poincare disc and the clusters so obtained resemble the original network communities more closely than the clusters obtained via Euclidean kmeans on the basis of well-known measures normalised mutual information and adjusted mutual information. Through this, we are able to clearly distinguish between periods of market stability and volatility by applying non-parametric statistical tests with a significance level of 0.05 to geometric measures namely hyperbolic distance and hyperbolic shortest path distance. After that, we are able to spot significant market change early by leveraging the Bollinger Band analysis on the time series of modularity in the embedded networks of each window. Finally, the radial distance and the Equidistance Angular coordinates help in visualizing the embedded network in the Poincare disc and it is seen that specific market sectors cluster together."}, "https://arxiv.org/abs/2403.18430": {"title": "Exploring language relations through syntactic distances and geographic proximity", "link": "https://arxiv.org/abs/2403.18430", "description": "arXiv:2403.18430v2 Announce Type: replace-cross \nAbstract: Languages are grouped into families that share common linguistic traits. While this approach has been successful in understanding genetic relations between diverse languages, more analyses are needed to accurately quantify their relatedness, especially in less studied linguistic levels such as syntax. Here, we explore linguistic distances using series of parts of speech (POS) extracted from the Universal Dependencies dataset. Within an information-theoretic framework, we show that employing POS trigrams maximizes the possibility of capturing syntactic variations while being at the same time compatible with the amount of available data. Linguistic connections are then established by assessing pairwise distances based on the POS distributions. Intriguingly, our analysis reveals definite clusters that correspond to well known language families and groups, with exceptions explained by distinct morphological typologies. Furthermore, we obtain a significant correlation between language similarity and geographic distance, which underscores the influence of spatial proximity on language kinships."}, "https://arxiv.org/abs/2404.08492": {"title": "Strategic Interactions between Large Language Models-based Agents in Beauty Contests", "link": "https://arxiv.org/abs/2404.08492", "description": "arXiv:2404.08492v2 Announce Type: replace-cross \nAbstract: The growing adoption of large language models (LLMs) presents potential for deeper understanding of human behaviours within game theory frameworks. Addressing research gap on multi-player competitive games, this paper examines the strategic interactions among multiple types of LLM-based agents in a classical beauty contest game. LLM-based agents demonstrate varying depth of reasoning that fall within a range of level-0 to 1, which are lower than experimental results conducted with human subjects, but they do display similar convergence pattern towards Nash Equilibrium (NE) choice in repeated setting. Further, through variation in group composition of agent types, I found environment with lower strategic uncertainty enhances convergence for LLM-based agents, and having a mixed environment comprises of LLM-based agents of differing strategic levels accelerates convergence for all. Higher average payoffs for the more intelligent agents are usually observed, albeit at the expense of less intelligent agents. The results from game play with simulated agents not only convey insights on potential human behaviours under specified experimental set-ups, they also offer valuable understanding of strategic interactions among algorithms."}, "https://arxiv.org/abs/2410.02765": {"title": "Forecasting and decisions in the birth-death-suppression Markov model for wildfires", "link": "https://arxiv.org/abs/2410.02765", "description": "arXiv:2410.02765v1 Announce Type: new \nAbstract: As changing climates transform the landscape of wildfire management and suppression, agencies are faced with difficult resource allocation decisions. We analyze trade-offs in temporal resource allocation using a simple but robust Markov model of a wildfire under suppression: the birth-death-suppression process. Though the model is not spatial, its stochastic nature and rich temporal structure make it broadly applicable in describing the dynamic evolution of a fire including ignition, the effect of adverse conditions, and the effect of external suppression. With strong analytical and numerical control of the probabilities of outcomes, we construct classes of processes which analogize common wildfire suppression scenarios and determine aspects of optimal suppression allocations. We model problems which include resource management in changing conditions, the effect of resource mobilization delay, and allocation under uncertainty about future events. Our results are consistent with modern resource management and suppression practices in wildland fire."}, "https://arxiv.org/abs/2410.02987": {"title": "Parrondo's effects with aperiodic protocols", "link": "https://arxiv.org/abs/2410.02987", "description": "arXiv:2410.02987v1 Announce Type: new \nAbstract: In this work, we study the effectiveness of employing archetypal aperiodic sequencing -- namely Fibonacci, Thue-Morse, and Rudin-Saphiro -- on the Parrondian effect. From a capital gain perspective, our results show that these series do yield a Parrondo's Paradox with the Thue-Morse based strategy outperforming not only the other two aperiodic strategies but benchmark Parrondian games with random and periodical ($AABBAABB\\ldots$) switching as well. The least performing of the three aperiodic strategies is the Rudin-Shapiro. To elucidate the underlying causes of these results, we analyze the cross-correlation between the capital generated by the switching protocols and that of the isolated losing games. This analysis reveals that a pronounced anti-correlation (below -0.95) with both isolated games is typically required to achieve a robust manifestation of Parrondo's effect. We also study the influence of the sequencing on the capital using the lacunarity and persistence measures. In general, we observe that the switching protocols tend to become less performing in terms of the capital as one increases the persistence and thus approaches the features of an isolated losing game. For the (log-)lacunarity, a property related to heterogeneity, we notice that for small persistence (less than 0.5) the performance increases with the lacunarity with a maximum around 0.4. In respect of this, our work shows that the optimisation of a switching protocol is strongly dependent on a fine tune between persistence and heterogeneity."}, "https://arxiv.org/abs/2410.03250": {"title": "Geospatial analysis of toponyms in geotagged social media posts", "link": "https://arxiv.org/abs/2410.03250", "description": "arXiv:2410.03250v1 Announce Type: new \nAbstract: Place names, or toponyms, play an integral role in human representation and communication of geographic space. In particular, how people relate each toponym with particular locations in geographic space should be indicative of their spatial perception. Here, we make use of an extensive dataset of georeferenced social media posts, retreived from Twitter, to perform a statistical analysis of the geographic distribution of toponyms and uncover the relationship between toponyms and geographic space. We show that the occurrence of toponyms is characterized by spatial inhomogeneity, giving rise to patterns that are distinct from the distribution of common nouns. Using simple models, we quantify the spatial specificity of toponym distributions and identify their core-periphery structures. In particular, we find that toponyms are used with a probability that decays as a power law with distance from the geographic center of their occurrence. Our findings highlight the potential of social media data to explore linguistic patterns in geographic space, paving the way for comprehensive analyses of human spatial representations."}, "https://arxiv.org/abs/2410.03252": {"title": "An egonet-based approach to effective weighted network comparison", "link": "https://arxiv.org/abs/2410.03252", "description": "arXiv:2410.03252v1 Announce Type: new \nAbstract: With the impressive growth of network models in practically every scientific and technological area, we are often faced with the need to compare graphs, i.e., to quantify their (dis)similarity using appropriate metrics. This is necessary, for example, to identify networks with comparable characteristics or to spot anomalous instants in a time sequence of graphs. While a large number of metrics are available for binary networks, the set of comparison methods capable of handling weighted graphs is much smaller. Yet, the strength of connections is often a key ingredient of the model, and ignoring this information could lead to misleading results. In this paper we introduce a family of dissimilarity measures to compare undirected weighted networks. They fall into the class of alignment-free metrics: as such, they do not require the correspondence of the nodes between the two graphs and can also compare networks of different sizes. In short, they are based on the distributions, on the graph, of a few egonet features which are easily defined and computed: the distance between two graphs is then the distance between the corresponding distributions. On a properly defined testbed with a pool of weighted network models with diversified characteristics, the proposed metrics are shown to achieve state-of-the-art performance in the model classification task. The effectiveness and applicability of the proposed metrics are then demonstrated on two examples. In the first, some ''filtering'' schemes -- designed to eliminate non-significant links while maintaining most of the total weight -- are evaluated in their ability to produce as output a graph faithful to the original, in terms of the local structure around nodes. In the second example, analyzing a timeline of stock market correlation graphs highlights anomalies associated with periods of financial instability."}, "https://arxiv.org/abs/2410.03270": {"title": "CLOVE: Travelling Salesman's approach to hyperbolic embeddings of complex networks with communities", "link": "https://arxiv.org/abs/2410.03270", "description": "arXiv:2410.03270v1 Announce Type: new \nAbstract: The embedding of complex networks into metric spaces has become a research topic of high interest with a wide variety of proposed methods. Low dimensional hyperbolic spaces offer a natural co-domain for embeddings allowing a roughly uniform spatial distribution of the nodes even for scale-free networks and the efficient navigability and estimation of linking probabilities. According to recent results, the communities of a complex network after optimization can be naturally mapped into well-defined angular sectors of the hyperbolic space. Here we introduce CLOVE, an embedding method exploiting this property based on iterative arrangement of the communities in a hierarchical manner, down to individual nodes. A crucial step in the process is finding the optimal angular order of the communities at a given level of the hierarchy, which is solved based on the Travelling Salesman Problem. Since CLOVE outperforms most of the alternative methods regarding different embedding quality measures and is computationally very efficient, it can be very useful in related down-stream machine learning tasks such as AI based pattern recognition."}, "https://arxiv.org/abs/2410.03428": {"title": "Research Landscape of the novel emerging field of Cryptoeconomics", "link": "https://arxiv.org/abs/2410.03428", "description": "arXiv:2410.03428v1 Announce Type: new \nAbstract: A bibliometric literature analysis was conducted to illuminate the evolving and rapidly expanding literature in the field of cryptoeconomics. This analysis presented the emerging field's intellectual, social, and conceptual structure. The intellectual structure, characterized by schools of thought, emerged through a common citation analysis. The social structure revealed collaborations among researchers, identified through a co-authorship analysis. Network analysis highlighted collaborative communities facilitating innovation and knowledge exchange within the field. The conceptual structure was enlightened by analyzing common terms occurring in titles, author keywords, abstracts, and the publication itself. This bibliometric analysis of the rapidly advancing field of cryptoeconomics serves as a foundational resource, providing insights into research productivity and emerging trends. It contributes to a deeper understanding of the field, offering valuable information on research patterns and trends. Furthermore, this analysis empowers researchers, policymakers, and industry sectors to make informed decisions, establish collaborations, and navigate the dynamic and evolving landscape of the cryptoeconomics field."}, "https://arxiv.org/abs/2410.03151": {"title": "Media Framing through the Lens of Event-Centric Narratives", "link": "https://arxiv.org/abs/2410.03151", "description": "arXiv:2410.03151v1 Announce Type: cross \nAbstract: From a communications perspective, a frame defines the packaging of the language used in such a way as to encourage certain interpretations and to discourage others. For example, a news article can frame immigration as either a boost or a drain on the economy, and thus communicate very different interpretations of the same phenomenon. In this work, we argue that to explain framing devices we have to look at the way narratives are constructed. As a first step in this direction, we propose a framework that extracts events and their relations to other events, and groups them into high-level narratives that help explain frames in news articles. We show that our framework can be used to analyze framing in U.S. news for two different domains: immigration and gun control."}, "https://arxiv.org/abs/2410.03265": {"title": "Multimodal Point-of-Interest Recommendation", "link": "https://arxiv.org/abs/2410.03265", "description": "arXiv:2410.03265v1 Announce Type: cross \nAbstract: Large Language Models are applied to recommendation tasks such as items to buy and news articles to read. Point of Interest is quite a new area to sequential recommendation based on language representations of multimodal datasets. As a first step to prove our concepts, we focused on restaurant recommendation based on each user's past visit history. When choosing a next restaurant to visit, a user would consider genre and location of the venue and, if available, pictures of dishes served there. We created a pseudo restaurant check-in history dataset from the Foursquare dataset and the FoodX-251 dataset by converting pictures into text descriptions with a multimodal model called LLaVA, and used a language-based sequential recommendation framework named Recformer proposed in 2023. A model trained on this semi-multimodal dataset has outperformed another model trained on the same dataset without picture descriptions. This suggests that this semi-multimodal model reflects actual human behaviours and that our path to a multimodal recommendation model is in the right direction."}, "https://arxiv.org/abs/2410.03293": {"title": "Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis", "link": "https://arxiv.org/abs/2410.03293", "description": "arXiv:2410.03293v1 Announce Type: cross \nAbstract: The work presented in this paper makes three scientific contributions with a specific focus on mining and analysis of COVID-19-related posts on Instagram. First, it presents a multilingual dataset of 500,153 Instagram posts about COVID-19 published between January 2020 and September 2024. This dataset, available at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in 161 different languages as well as 535,021 distinct hashtags. After the development of this dataset, multilingual sentiment analysis was performed, which involved classifying each post as positive, negative, or neutral. The results of sentiment analysis are presented as a separate attribute in this dataset. Second, it presents the results of performing sentiment analysis per year from 2020 to 2024. The findings revealed the trends in sentiment related to COVID-19 on Instagram since the beginning of the pandemic. For instance, between 2020 and 2024, the sentiment trends show a notable shift, with positive sentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from 44.19% to 58.34%. Finally, the paper also presents findings of language-specific sentiment analysis. This analysis highlighted similar and contrasting trends of sentiment across posts published in different languages on Instagram. For instance, out of all English posts, 49.68% were positive, 14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts, 4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting distinct differences in the sentiment distribution between these two languages."}, "https://arxiv.org/abs/2410.03342": {"title": "Impact factors of astrophysics journals revisited", "link": "https://arxiv.org/abs/2410.03342", "description": "arXiv:2410.03342v1 Announce Type: cross \nAbstract: We calculate the 2024 impact factors of 36 most widely used journals in Astrophysics, using the citations collated by NASA/ADS (Astrophysics Data System) and compare them to the official impact factors. This includes journals which publish papers outside of astrophysics such as PRD, EPJC, Nature etc. We also propose a new metric to gauge the impact factor based on the median number of citations in a journal and calculate the same for all the journals. We find that the ADS-based impact factors are mostly in agreement, albeit higher than the official impact factors for most journals. The journals with the maximum fractional difference in median-based and old impact factors are JHEAP and PTEP. We find the maximum difference between the ADS and official impact factor for Nature."}, "https://arxiv.org/abs/2410.03474": {"title": "Group Fairness in Peer Review", "link": "https://arxiv.org/abs/2410.03474", "description": "arXiv:2410.03474v1 Announce Type: cross \nAbstract: Large conferences such as NeurIPS and AAAI serve as crossroads of various AI fields, since they attract submissions from a vast number of communities. However, in some cases, this has resulted in a poor reviewing experience for some communities, whose submissions get assigned to less qualified reviewers outside of their communities. An often-advocated solution is to break up any such large conference into smaller conferences, but this can lead to isolation of communities and harm interdisciplinary research. We tackle this challenge by introducing a notion of group fairness, called the core, which requires that every possible community (subset of researchers) to be treated in a way that prevents them from unilaterally benefiting by withdrawing from a large conference.\n  We study a simple peer review model, prove that it always admits a reviewing assignment in the core, and design an efficient algorithm to find one such assignment. We use real data from CVPR and ICLR conferences to compare our algorithm to existing reviewing assignment algorithms on a number of metrics."}, "https://arxiv.org/abs/2310.01283": {"title": "The influence of coordinated behavior on toxicity", "link": "https://arxiv.org/abs/2310.01283", "description": "arXiv:2310.01283v2 Announce Type: replace \nAbstract: In the intricate landscape of social media, genuine content dissemination may be altered by a number of threats. Coordinated Behavior (CB), defined as orchestrated efforts by entities to deceive or mislead users about their identity and intentions, emerges as a tactic to exploit or manipulate online discourse. This study delves into the relationship between CB and toxic conversation on X (formerly known as Twitter). Using a dataset of 11 million tweets from 1 million users preceding the 2019 UK general election, we show that users displaying CB typically disseminate less harmful content, irrespective of political affiliation. However, distinct toxicity patterns emerge among different coordinated cohorts. Compared to their non-CB counterparts, CB participants show marginally higher toxicity levels only when considering their original posts. We further show the effects of CB-driven toxic content on non-CB users, gauging its impact based on political leanings. Our findings suggest that CB only has a limited impact on the toxicity of digital discourse."}, "https://arxiv.org/abs/2402.09272": {"title": "Insights and caveats from mining local and global temporal motifs in cryptocurrency transaction networks", "link": "https://arxiv.org/abs/2402.09272", "description": "arXiv:2402.09272v2 Announce Type: replace \nAbstract: Distributed ledger technologies have opened up a wealth of fine-grained transaction data from cryptocurrencies like Bitcoin and Ethereum. This allows research into problems like anomaly detection, anti-money laundering, pattern mining and activity clustering (where data from traditional currencies is rarely available). The formalism of temporal networks offers a natural way of representing this data and offers access to a wealth of metrics and models. However, the large scale of the data presents a challenge using standard graph analysis techniques. We use temporal motifs to analyse two Bitcoin datasets and one NFT dataset, using sequences of three transactions and up to three users. We show that the commonly used technique of simply counting temporal motifs over all users and all time can give misleading conclusions. Here we also study the motifs contributed by each user and discover that the motif distribution is heavy-tailed and that the key players have diverse motif signatures. We study the motifs that occur in different time periods and find events and anomalous activity that cannot be seen just by a count on the whole dataset. Studying motif completion time reveals dynamics driven by human behaviour as well as algorithmic behaviour."}, "https://arxiv.org/abs/2302.14123": {"title": "Private Blotto: Viewpoint Competition with Polarized Agents", "link": "https://arxiv.org/abs/2302.14123", "description": "arXiv:2302.14123v3 Announce Type: replace-cross \nAbstract: Social media platforms are responsible for collecting and disseminating vast quantities of content. Recently, however, they have also begun enlisting users in helping annotate this content - for example, to provide context or label disinformation. However, users may act strategically, sometimes reflecting biases (e.g. political) about the \"right\" label. How can social media platforms design their systems to use human time most efficiently? Historically, competition over multiple items has been explored in the Colonel Blotto game setting (Borel, 1921). However, they were originally designed to model two centrally-controlled armies competing over zero-sum \"items\", a specific scenario with limited modern-day application. In this work, we propose and study the Private Blotto game, a variant with the key difference that individual agents act independently, without being coordinated by a central \"Colonel\". We completely characterize the Nash stability of this game and how this impacts the amount of \"misallocated effort\" of users on unimportant items. We show that the outcome function (aggregating multiple labels on a single item) has a critical impact, and specifically contrast a majority rule outcome (the median) as compared to a smoother outcome function (mean). In general, for median outcomes we show that instances without stable arrangements only occur for relatively few numbers of agents, but stable arrangements may have very high levels of misallocated effort. For mean outcome functions, we show that unstable arrangements can occur even for arbitrarily large numbers of agents, but when stable arrangements exist, they always have low misallocated effort. We conclude by discussing implications our results have for motivating examples in social media platforms and political competition."}, "https://arxiv.org/abs/2410.04212": {"title": "Development of the Complex Nexus of Socio-Techno-Economic-Environmental Parametric (STEEP) Metrics for Evaluating Coal-to-Clean Energy Transitions", "link": "https://arxiv.org/abs/2410.04212", "description": "arXiv:2410.04212v1 Announce Type: new \nAbstract: Transitioning from coal to clean energy, such as nuclear and renewables, is essential for mitigating climate change, improving air quality, and ensuring sustainable energy security. Reducing reliance on coal lowers greenhouse gas emissions and pollution, which enhances public health and economic growth through renewable energy investments. Clean energy also fosters energy independence and long-term sustainability. This paper presents a Complex Nexus of Socio-Techno-Economic-Environmental Parametric (STEEP) Metrics to systematically evaluate and guide these transitions, facilitating informed decision-making and optimizing resource allocation. The methodology is classified into three approaches: optimal site selection using a multi-criteria decision-making framework that ranks coal plant sites based on societal, technical, economic, and environmental criteria; long-term planning evaluation through performance indicators comparing Greenfield, Coal-to-Nuclear (C2N), and Coal-to-Integrated Energy Systems (C2IES); and short-term operational benefits assessment using the Unit Commitment Economic Dispatch (UCED) model to optimize generator scheduling and minimize costs across various scenarios. This framework enables practical analysis of coal-to-clean transitions, identifying the best strategies for implementation."}, "https://arxiv.org/abs/2410.04301": {"title": "Coalescing Force of Group Pressure: Consensus in Nonlinear Opinion Dynamics", "link": "https://arxiv.org/abs/2410.04301", "description": "arXiv:2410.04301v1 Announce Type: new \nAbstract: This work extends the recent opinion dynamics model from Cheng et al., emphasizing the role of group pressure in consensus formation. We generalize the findings to incorporate social influence algorithms with general time-varying, opinion-dependent weights and multidimensional opinions, beyond bounded confidence dynamics. We demonstrate that, with uniformly positive conformity levels, group pressure consistently drives consensus and provide a tighter estimate for the convergence rate. Unlike previous models, the common public opinion in our framework can assume arbitrary forms within the convex hull of current opinions, offering flexibility applicable to real-world scenarios such as opinion polls with random participant selection. This analysis provides deeper insights into how group pressure mechanisms foster consensus under diverse conditions."}, "https://arxiv.org/abs/2410.04303": {"title": "Applicability of spatial early warning signals to complex network dynamics", "link": "https://arxiv.org/abs/2410.04303", "description": "arXiv:2410.04303v1 Announce Type: new \nAbstract: Early warning signals (EWSs) for complex dynamical systems aim to anticipate tipping points, or large regime shifts, before they occur. While signals computed from time series data, such as temporal variance and lagged autocorrelation functions, are useful for this task, they are costly to obtain in practice because they need many samples over time to calculate. Spatial EWSs use just a single sample per spatial location and aggregate the samples over space rather than time to try to mitigate this limitation. However, although many complex systems in nature and society form diverse networks, the performance of spatial EWSs is mostly unknown for general networks because the vast majority of studies of spatial EWSs have been on regular lattice networks. Therefore, we have carried out a comprehensive investigation of four major spatial EWSs on various networks. We find that the winning EWS depends on tipping scenarios, while spatial skewness tends to excel when tipping occurs via a saddle-node bifurcation, which is a commonly studied scenario in the literature. We also find that spatial EWSs behave in a drastically different manner between the square lattice and complex networks and tend to be more reliable for the latter than the former according to a standard performance measure. The present results encourage further studies of spatial EWSs on complex networks."}, "https://arxiv.org/abs/2410.04317": {"title": "Enabling Asymptotic Truth Learning in a Social Network", "link": "https://arxiv.org/abs/2410.04317", "description": "arXiv:2410.04317v1 Announce Type: new \nAbstract: Consider a network of agents that all want to guess the correct value of some ground truth state. In a sequential order, each agent makes its decision using a single private signal which has a constant probability of error, as well as observations of actions from its network neighbors earlier in the order. We are interested in enabling \\emph{network-wide asymptotic truth learning} -- that in a network of $n$ agents, almost all agents make a correct prediction with probability approaching one as $n$ goes to infinity. In this paper we study both random orderings and carefully crafted decision orders with respect to the graph topology as well as sufficient or necessary conditions for a graph to support such a good ordering. We first show that on a sparse graph of average constant degree with a random ordering asymptotic truth learning does not happen. We then show a rather modest sufficient condition to enable asymptotic truth learning. With the help of this condition we characterize graphs generated from the Erd\\\"os R\\'enyi model and preferential attachment model. In an Erd\\\"os R\\'enyi graph, unless the graph is super sparse (with $O(n)$ edges) or super dense (nearly a complete graph), there exists a decision ordering that supports asymptotic truth learning. Similarly, any preferential attachment network with a constant number of edges per node can achieve asymptotic truth learning under a carefully designed ordering but not under either a random ordering nor the arrival order. We also evaluated a variant of the decision ordering on different network topologies and demonstrated clear effectiveness in improving truth learning over random orderings."}, "https://arxiv.org/abs/2410.04493": {"title": "Increasing volume and decreasing disruption in US case law", "link": "https://arxiv.org/abs/2410.04493", "description": "arXiv:2410.04493v1 Announce Type: new \nAbstract: Law evolves with society. As population growth and social changes give rise to new issues and conflicts, additional laws are introduced into the existing legal system. These new laws not only expand the volume of the system but can also disrupt it by overturning or replacing older laws. In this paper, we demonstrate that these two aspects of legal evolution, i.e., growth and disruption, can be effectively described and explained through the application of two computational frameworks to US case law data. Our analysis shows that the volume of case law has been growing at a rate faster than population growth, with the scaling exponent of 1.74, while its average disruptiveness has decreased over the past two centuries. This finding implies that the increasing size and complexity of the legal system make it harder for individual cases to drive significant change. Nevertheless, we find that social structural factors such as authority and ideology can empower lawmakers to overcome this inertia and still produce disruptions under certain conditions. Specifically, lawmakers with greater authority generate more disruptive rulings, and political liberalism and ideological consensus among those with the highest authority leads to greater disruption. This result suggests that increasing ideological polarization may be contributing to the decline in disruption within US case law."}, "https://arxiv.org/abs/2410.04552": {"title": "Modeling Social Media Recommendation Impacts Using Academic Networks: A Graph Neural Network Approach", "link": "https://arxiv.org/abs/2410.04552", "description": "arXiv:2410.04552v1 Announce Type: new \nAbstract: The widespread use of social media has highlighted potential negative impacts on society and individuals, largely driven by recommendation algorithms that shape user behavior and social dynamics. Understanding these algorithms is essential but challenging due to the complex, distributed nature of social media networks as well as limited access to real-world data. This study proposes to use academic social networks as a proxy for investigating recommendation systems in social media. By employing Graph Neural Networks (GNNs), we develop a model that separates the prediction of academic infosphere from behavior prediction, allowing us to simulate recommender-generated infospheres and assess the model's performance in predicting future co-authorships. Our approach aims to improve our understanding of recommendation systems' roles and social networks modeling. To support the reproducibility of our work we publicly make available our implementations: https://github.com/DimNeuroLab/academic_network_project"}, "https://arxiv.org/abs/2410.04619": {"title": "The Role of Social Support and Influencers in Social Media Communities", "link": "https://arxiv.org/abs/2410.04619", "description": "arXiv:2410.04619v1 Announce Type: new \nAbstract: How can individual agents coordinate their actions to achieve a shared objective in distributed systems? This challenge spans economic, technical, and sociological domains, each confronting scalability, heterogeneity, and conflicts between individual and collective goals. In economic markets, a common currency facilitates coordination, raising the question of whether such mechanisms can be applied in other contexts. This paper explores this idea within social media platforms, where social support (likes, shares, comments) acts as a currency that shapes content production and sharing. We investigate two key questions: (1) Can social support serve as an effective coordination tool, and (2) What role do influencers play in content creation and dissemination? Our formal analysis shows that social support can coordinate user actions similarly to money in economic markets. Influencers serve dual roles, aggregating content and acting as information proxies, guiding content producers in large markets. While imperfections in information lead to a \"price of influence\" and suboptimal outcomes, this price diminishes as markets grow, improving social welfare. These insights provide a framework for understanding coordination in distributed environments, with applications in both sociological systems and multi-agent AI systems."}, "https://arxiv.org/abs/2410.04737": {"title": "Urbanization, economic development, and income distribution dynamics in India", "link": "https://arxiv.org/abs/2410.04737", "description": "arXiv:2410.04737v1 Announce Type: new \nAbstract: India's urbanization is often characterized as particularly challenging and very unequal but systematic empirical analyses, comparable to other nations, have largely been lacking. Here, we characterize India's economic and human development along with changes in its personal income distribution as a function of the nation's growing urbanization. On aggregate, we find that India outperforms most other nations in the growth of various indicators of development with urbanization, including income and human development. These results are due in part to India's present low levels of urbanization but also demonstrate the transformational role of its cities in driving multi-dimensional development. To test these changes at the more local level, we study the income distributions of large Indian cities to find evidence for high positive growth in the lowest decile (poorest) of the population, enabling sharp reductions in poverty over time. We also test the hypothesis that inequality-reducing cities are more attractive destinations for rural migrants. Finally, we use income distributions to characterize changes in poverty rates directly. This shows much lower levels of poverty in urban India and especially in its largest cities. The dynamics of poverty rates during the recent COVID-19 pandemic shows both a high fragility of these improvements during a crisis and their resilience over longer times. Sustaining a long-term dynamic where urbanization continues to be closely associated with human development and poverty reduction is likely India's fastest path to a more prosperous and equitable future."}, "https://arxiv.org/abs/2410.04820": {"title": "BCIM: Budget and capacity constrained influence maximization in multilayer networks", "link": "https://arxiv.org/abs/2410.04820", "description": "arXiv:2410.04820v1 Announce Type: new \nAbstract: Influence maximization (IM) seeks to identify a seed set that maximizes influence within a network, with applications in areas such as viral marketing, disease control, and political campaigns. The budgeted influence maximization (BIM) problem extends IM by incorporating cost constraints for different nodes. However, the current BIM problem, limited by budget alone, often results in the selection of numerous low-cost nodes, which may not be applicable to real-world scenarios. Moreover, considering that users can transmit information across multiple social platforms, solving the BIM problem across these platforms could lead to more optimized resource utilization. To address these challenges, we propose the Budget and Capacity Constrained Influence Maximization (BCIM) problem within multilayer networks and introduce a Multilayer Multi-population Genetic Algorithm (MMGA) to solve it. The MMGA employs modules, such as initialization, repair, and parallel evolution, designed not only to meet budget and capacity constraints but also to significantly enhance algorithmic efficiency. Extensive experiments on both synthetic and empirical multilayer networks demonstrate that MMGA improves spreading performance by at least 10% under the two constraints compared to baselines extended from classical IM problems. The BCIM framework introduces a novel direction in influence maximization, providing an effective and efficient solution to the problem."}, "https://arxiv.org/abs/2410.04923": {"title": "Integrated or Segregated? User Behavior Change after Cross-Party Interactions on Reddit", "link": "https://arxiv.org/abs/2410.04923", "description": "arXiv:2410.04923v1 Announce Type: new \nAbstract: It has been a widely shared concern that social media reinforces echo chambers of like-minded users and exacerbate political polarization. While fostering interactions across party lines is recognized as an important strategy to break echo chambers, there is a lack of empirical evidence on whether users will actually become more integrated or instead more segregated following such interactions on real social media platforms. We fill this gap by inspecting how users change their community engagement after receiving a cross-party reply in the U.S. politics discussion on Reddit. More specifically, we investigate if they increase their activity in communities of the opposing party, or in communities of their own party. We find that receiving a cross-party reply to a comment in a non-partisan discussion space is not significantly associated with increased out-party subreddit activity, unless the comment itself is already a reply to another comment. Meanwhile, receiving a cross-party reply is significantly associated with increased in-party subreddit activity, but the effect is comparable to that of receiving a same-party reply. Our results reveal a highly conditional depolarization effect following cross-party interactions in spurring activity in out-party communities, which is likely part of a more general dynamic of feedback-boosted engagement."}, "https://arxiv.org/abs/2410.04967": {"title": "Crowd-sourced particle physics stories from DESY-CMS", "link": "https://arxiv.org/abs/2410.04967", "description": "arXiv:2410.04967v1 Announce Type: new \nAbstract: The CMS at DESY outreach Instagram account (@cmsatdesy) serves as a platform for science communication and outreach for a large experimental particle physics group. The initiative aims to promote scientific research, engage young scientists in outreach activities, and showcase their contributions. Instagram was chosen for its strong alignment with the target demographic and its broad user base in Germany and internationally.\n  The account highlights the work of young scientists, providing insights into their scientific journeys and disseminating particle physics outreach content. Multiple contributors collaborate on content creation, offering early career researchers opportunities for training in science communication while maintaining a manageable time commitment. This paper presents the evolution of the project, its initial objectives, target audience, and the experiences gained in content development and public engagement on social media platforms."}, "https://arxiv.org/abs/2410.04987": {"title": "icon: Fast Simulation of Epidemics on Coevolving Networks", "link": "https://arxiv.org/abs/2410.04987", "description": "arXiv:2410.04987v1 Announce Type: new \nAbstract: We introduce a fast simulation technique for modeling epidemics on adaptive networks. Our rejection-based algorithm efficiently simulates the co-evolution of the network structure and the epidemic dynamics. We extend the classical SIS model by incorporating stochastic rules that allow for the association of susceptible nodes and the dissociation of infected nodes. The method outperforms standard baselines in terms of computational efficiency while revealing new emergent patterns in epidemic spread. Code is made available at github.com/GerritGr/icon."}, "https://arxiv.org/abs/2410.05002": {"title": "Social Network Datasets on Reddit Financial Discussion", "link": "https://arxiv.org/abs/2410.05002", "description": "arXiv:2410.05002v1 Announce Type: new \nAbstract: Stock markets are impacted by a large variety of factors including news and discussions among investors about investment opportunities. With the emergence of social media, new opportunities for having financial discussions arose. The market frenzy surrounding GameStop (GME) on the Reddit subreddit Wallstreetbets, caused financial discussion forums to receive widespread attention and it was established that Wallstreetbets played a leading role in the stock market movements of GME. Here, we present a new data set for exploring the effect of social media discussion forums on the stock market. The dataset consists of posts published on various Reddit subreddits concerning the popular meme stocks GameStop (GME), American Multi-Cinema Entertainment Holdings (AMC), and BlackBerry (BB). We document the data collection and processing steps and show that the posts and comments about these meme stocks are related to their market movements."}, "https://arxiv.org/abs/2410.05188": {"title": "Matrix-weighted networks for modeling multidimensional dynamics", "link": "https://arxiv.org/abs/2410.05188", "description": "arXiv:2410.05188v1 Announce Type: new \nAbstract: Networks are powerful tools for modeling interactions in complex systems. While traditional networks use scalar edge weights, many real-world systems involve multidimensional interactions. For example, in social networks, individuals often have multiple interconnected opinions that can affect different opinions of other individuals, which can be better characterized by matrices. We propose a novel, general framework for modeling such multidimensional interacting dynamics: matrix-weighted networks (MWNs). We present the mathematical foundations of MWNs and examine consensus dynamics and random walks within this context. Our results reveal that the coherence of MWNs gives rise to non-trivial steady states that generalize the notions of communities and structural balance in traditional networks."}, "https://arxiv.org/abs/2410.03771": {"title": "SeeSay: An Assistive Device for the Visually Impaired Using Retrieval Augmented Generation", "link": "https://arxiv.org/abs/2410.03771", "description": "arXiv:2410.03771v1 Announce Type: cross \nAbstract: In this paper, we present SeeSay, an assistive device designed for individuals with visual impairments. This system leverages large language models (LLMs) for speech recognition and visual querying. It effectively identifies, records, and responds to the user's environment by providing audio guidance using retrieval-augmented generation (RAG). Our experiments demonstrate the system's capability to recognize its surroundings and respond to queries with audio feedback in diverse settings. We hope that the SeeSay system will facilitate users' comprehension and recollection of their surroundings, thereby enhancing their environmental perception, improving navigational capabilities, and boosting overall independence."}, "https://arxiv.org/abs/2410.04054": {"title": "Large Language Models can Achieve Social Balance", "link": "https://arxiv.org/abs/2410.04054", "description": "arXiv:2410.04054v1 Announce Type: cross \nAbstract: Social balance is a concept in sociology which states that if every three individuals in a population achieve certain structures of positive or negative interactions, then the whole population ends up in one faction of positive interactions or divided between two or more antagonistic factions. In this paper, we consider a group of interacting large language models (LLMs) and study how, after continuous interactions, they can achieve social balance. Across three different LLM models, we found that social balance depends on (i) whether interactions are updated based on \"relationships\", \"appraisals\", or \"opinions\"; (ii) whether agents update their interactions based on homophily or influence from their peers; and (iii) the number of simultaneous interactions the LLMs consider. When social balance is achieved, its particular structure of positive or negative interactions depends on these three conditions and are different across LLM models and sizes. The stability of interactions and the justification for their update also vary across models. Thus, social balance is driven by the pre-training and alignment particular to each LLM model."}, "https://arxiv.org/abs/2410.04251": {"title": "Enhancing Future Link Prediction in Quantum Computing Semantic Networks through LLM-Initiated Node Features", "link": "https://arxiv.org/abs/2410.04251", "description": "arXiv:2410.04251v1 Announce Type: cross \nAbstract: Quantum computing is rapidly evolving in both physics and computer science, offering the potential to solve complex problems and accelerate computational processes. The development of quantum chips necessitates understanding the correlations among diverse experimental conditions. Semantic networks built on scientific literature, representing meaningful relationships between concepts, have been used across various domains to identify knowledge gaps and novel concept combinations. Neural network-based approaches have shown promise in link prediction within these networks. This study proposes initializing node features using LLMs to enhance node representations for link prediction tasks in graph neural networks. LLMs can provide rich descriptions, reducing the need for manual feature creation and lowering costs. Our method, evaluated using various link prediction models on a quantum computing semantic network, demonstrated efficacy compared to traditional node embedding techniques."}, "https://arxiv.org/abs/2410.04254": {"title": "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "link": "https://arxiv.org/abs/2410.04254", "description": "arXiv:2410.04254v1 Announce Type: cross \nAbstract: Links are a fundamental part of information networks, turning isolated pieces of knowledge into a network of information that is much richer than the sum of its parts. However, adding a new link to the network is not trivial: it requires not only the identification of a suitable pair of source and target entities but also the understanding of the content of the source to locate a suitable position for the link in the text. The latter problem has not been addressed effectively, particularly in the absence of text spans in the source that could serve as anchors to insert a link to the target entity. To bridge this gap, we introduce and operationalize the task of entity insertion in information networks. Focusing on the case of Wikipedia, we empirically show that this problem is, both, relevant and challenging for editors. We compile a benchmark dataset in 105 languages and develop a framework for entity insertion called LocEI (Localized Entity Insertion) and its multilingual variant XLocEI. We show that XLocEI outperforms all baseline models (including state-of-the-art prompt-based ranking with LLMs such as GPT-4) and that it can be applied in a zero-shot manner on languages not seen during training with minimal performance drop. These findings are important for applying entity insertion models in practice, e.g., to support editors in adding links across the more than 300 language versions of Wikipedia."}, "https://arxiv.org/abs/2410.04287": {"title": "Unveiling the Impact of Local Homophily on GNN Fairness: In-Depth Analysis and New Benchmarks", "link": "https://arxiv.org/abs/2410.04287", "description": "arXiv:2410.04287v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs) often struggle to generalize when graphs exhibit both homophily (same-class connections) and heterophily (different-class connections). Specifically, GNNs tend to underperform for nodes with local homophily levels that differ significantly from the global homophily level. This issue poses a risk in user-centric applications where underrepresented homophily levels are present. Concurrently, fairness within GNNs has received substantial attention due to the potential amplification of biases via message passing. However, the connection between local homophily and fairness in GNNs remains underexplored. In this work, we move beyond global homophily and explore how local homophily levels can lead to unfair predictions. We begin by formalizing the challenge of fair predictions for underrepresented homophily levels as an out-of-distribution (OOD) problem. We then conduct a theoretical analysis that demonstrates how local homophily levels can alter predictions for differing sensitive attributes. We additionally introduce three new GNN fairness benchmarks, as well as a novel semi-synthetic graph generator, to empirically study the OOD problem. Across extensive analysis we find that two factors can promote unfairness: (a) OOD distance, and (b) heterophilous nodes situated in homophilous graphs. In cases where these two conditions are met, fairness drops by up to 24% on real world datasets, and 30% in semi-synthetic datasets. Together, our theoretical insights, empirical analysis, and algorithmic contributions unveil a previously overlooked source of unfairness rooted in the graph's homophily information."}, "https://arxiv.org/abs/2410.04825": {"title": "The divide between us: Internet access among people with and without disabilities in the post-pandemic era", "link": "https://arxiv.org/abs/2410.04825", "description": "arXiv:2410.04825v1 Announce Type: cross \nAbstract: The COVID-19 pandemic highlighted the importance of internet access across various aspects of life, from remote work and online education to healthcare services and social connections. As we transition to a post-pandemic era, a pressing need arises to update our understanding of the multifaceted nature of internet access. This study is one of the first attempts to do so. Using survey data from New Zealand adult internet users (n=960), it compares internet connection types, frequency of internet use at home, social media use, and concerns about online risk between people with and without disabilities. Results show people with disabilities have restricted fibre access and higher wireless broadband (a much slower connection type). People with disabilities use social media platforms less and are more concerned about certain online risks. The findings highlight persistent disparities in internet access for people with disabilities in the post-pandemic era. Implications of the study are discussed."}, "https://arxiv.org/abs/2410.04921": {"title": "Music-triggered fashion design: from songs to the metaverse", "link": "https://arxiv.org/abs/2410.04921", "description": "arXiv:2410.04921v1 Announce Type: cross \nAbstract: The advent of increasingly-growing virtual realities poses unprecedented opportunities and challenges to different societies. Artistic collectives are not an exception, and we here aim to put special attention into musicians. Compositions, lyrics and even show-advertisements are constituents of a message that artists transmit about their reality. As such, artistic creations are ultimately linked to feelings and emotions, with aesthetics playing a crucial role when it comes to transmit artist's intentions. In this context, we here analyze how virtual realities can help to broaden the opportunities for musicians to bridge with their audiences, by devising a dynamical fashion-design recommendation system inspired by sound stimulus. We present our first steps towards re-defining musical experiences in the metaverse, opening up alternative opportunities for artists to connect both with real and virtual (\\textit{e.g.} machine-learning agents operating in the metaverse) in potentially broader ways."}, "https://arxiv.org/abs/2111.06235": {"title": "Limits of Multilayer Diffusion Network Inference in Social Media Research", "link": "https://arxiv.org/abs/2111.06235", "description": "arXiv:2111.06235v2 Announce Type: replace \nAbstract: Information on social media spreads through an underlying diffusion network that connects people of common interests and opinions. This diffusion network often comprises multiple layers, each capturing the spreading dynamics of a certain type of information characterized by, for example, topic, language, or attitude. Researchers have previously proposed methods to infer these underlying multilayer diffusion networks from observed spreading patterns, but little is known about how well these methods perform across the range of realistic spreading data. In this paper, we conduct an extensive series of synthetic data experiments to systematically analyze the performance of the multilayer diffusion network inference framework, under varied network structure (e.g. density, number of layers) and information diffusion settings (e.g. cascade size, layer mixing) that are designed to mimic real-world spreading on social media. Our results show extreme performance variation of the inference framework: notably, it achieves much higher accuracy when inferring a denser diffusion network, while it fails to decompose the diffusion network correctly when most cascades in the data reach a limited audience. In demonstrating the conditions under which the inference accuracy is extremely low, our paper highlights the need to carefully evaluate the applicability of the inference before running it on real data. Practically, our results serve as a reference for this evaluation, and our publicly available implementation, which outperforms previous implementations in accuracy, supports further testing under personalized settings."}, "https://arxiv.org/abs/2306.03940": {"title": "Orphan Articles: The Dark Matter of Wikipedia", "link": "https://arxiv.org/abs/2306.03940", "description": "arXiv:2306.03940v2 Announce Type: replace \nAbstract: With 60M articles in more than 300 language versions, Wikipedia is the largest platform for open and freely accessible knowledge. While the available content has been growing continuously at a rate of around 200K new articles each month, very little attention has been paid to the accessibility of the content. One crucial aspect of accessibility is the integration of hyperlinks into the network so the articles are visible to readers navigating Wikipedia. In order to understand this phenomenon, we conduct the first systematic study of orphan articles, which are articles without any incoming links from other Wikipedia articles, across 319 different language versions of Wikipedia. We find that a surprisingly large extent of content, roughly 15\\% (8.8M) of all articles, is de facto invisible to readers navigating Wikipedia, and thus, rightfully term orphan articles as the dark matter of Wikipedia. We also provide causal evidence through a quasi-experiment that adding new incoming links to orphans (de-orphanization) leads to a statistically significant increase of their visibility in terms of the number of pageviews. We further highlight the challenges faced by editors for de-orphanizing articles, demonstrate the need to support them in addressing this issue, and provide potential solutions for developing automated tools based on cross-lingual approaches. Overall, our work not only unravels a key limitation in the link structure of Wikipedia and quantitatively assesses its impact, but also provides a new perspective on the challenges of maintenance associated with content creation at scale in Wikipedia."}, "https://arxiv.org/abs/2311.16457": {"title": "Fixation dynamics on multilayer networks", "link": "https://arxiv.org/abs/2311.16457", "description": "arXiv:2311.16457v2 Announce Type: replace \nAbstract: Network structure has a large impact on constant-selection evolutionary dynamics, with which multiple types of fitness (i.e., strength) compete on the network. Here we study constant-selection dynamics on two-layer networks in which the fitness of a node in one layer affects that in the other layer, under birth-death processes and uniform initialization, which are commonly assumed. We show mathematically and numerically that two-layer networks are suppressors of selection, which means that they suppress the effects of the different fitness values among the different types on the final outcomes of the evolutionary dynamics (called fixation probability) relative to the constituent one-layer networks. In fact, many two-layer networks are suppressors of selection relative to the most basic baseline, the Moran process. This result is in stark contrast with the results for conventional one-layer networks for which most networks are amplifiers of selection."}, "https://arxiv.org/abs/2410.05279": {"title": "Wavelet analysis and forecast of pollutants in Puebla City, Mexico", "link": "https://arxiv.org/abs/2410.05279", "description": "arXiv:2410.05279v1 Announce Type: new \nAbstract: This article presents a detailed analysis of atmospheric pollutants in Puebla City, Mexico, based on data collected between 2016 and 2024. The research focuses on the daily variation of six major pollutants: ozone (O$_3$), particles smaller than 10 microns (PM10), particles smaller than 2.5 microns (PM2.5), sulfur dioxide (SO$_2$), and nitrogen dioxide (NO$_2$). The Mann-Kendall test, Innovate Trend, and Wavelet Transform Analysis were applied to identify significant trends and seasonal patterns. The results indicate an increase in the levels of O$_3$, SO$_2$, and NO$_2$, while the levels of PM10, and PM2.5 have shown a decrease. The study also employs the Prophet Forecasting Model to predict PM2.5 and PM10 concentrations for the year 2022, demonstrating that the model's accuracy increases as the analysis extends over longer periods."}, "https://arxiv.org/abs/2410.05283": {"title": "La dignificaci\\'on de la pepena: un an\\'alisis del reciclaje de residuos s\\'olidos urbanos en la ciudad de Chihuahua", "link": "https://arxiv.org/abs/2410.05283", "description": "arXiv:2410.05283v1 Announce Type: new \nAbstract: The pepenaactivity is part of the informal sector, characterized by precariousness and social invisibility; however, it is a key element in the recycling production chain, providing an economic income for many people. In the city of Chihuahua, this activity is carried out on a significant scale. Therefore, this study analyzes and dignifies the recycling of urban solid waste from the perspective of scavengers, also explaining their relationship with other social actors at the city's final disposal site. The study starts from the premise that waste picking is, in the current era of global environmental crisis, an effective way to conserve resources and reduce environmental impacts. Methodologically, participatory action research was used as a strategy to raise awareness among scavengers about the importance of their work and to present their community organization as an example to other urban waste picking groups in different locations. Finally, we found issues in these groups such as informality, lack of legal support,specific health risks, and, more broadly, the absence of public policies to recognize this activity as valuable in addressing the global environmental crisis"}, "https://arxiv.org/abs/2410.05285": {"title": "A Hard-Science Approach to Kondratieff's Economic Cycle", "link": "https://arxiv.org/abs/2410.05285", "description": "arXiv:2410.05285v1 Announce Type: new \nAbstract: In an effort to evidence the Kondratieff cycle more scientifically than the way economists do, physical variables are studied rather than monetary indicators. Previously published graphs are reproduced and updated here with recent data. A cyclical rather regular variation of energy consumption reveals a 56-year cycle. A dozen human endeavors/phenomena, such as bank failures, homicides, hurricanes, feminism, and sunspot activity are shown to resonate with this cycle. Possible explanations for this phenomenon may have to do with a climatic variation or with the length of time any individual actively influences the environment. There is some evidence that the cycle may be getting shorter in amplitude and duration in recent years. All quantitative confidence levels involved in these observations are poor by scientific standards and permit critics to question the very existence of this phenomenon."}, "https://arxiv.org/abs/2410.05286": {"title": "Dependent Infrastructure Service Disruption Mapping (DISruptionMap): A Method to Assess Cascading Service Disruptions in Disaster Scenarios", "link": "https://arxiv.org/abs/2410.05286", "description": "arXiv:2410.05286v1 Announce Type: new \nAbstract: Critical infrastructures provide essential services for our modern society. Large-scale natural hazards, such as floods or storms, can disrupt multiple critical infrastructures at once. In addition, a localized failure of one service can trigger a cascade of failures of other dependent services. This makes it challenging to anticipate and prepare adequately for direct and indirect consequences of such events. Existing methods that are spatially explicit and consider service dependencies currently lack practicality, as they require large amounts of data. To address this gap, we propose a novel method called DISruptionMap which analyzes complex disruptions to critical infrastructure services. The proposed method combines i) spatial service models to assess direct service disruptions with ii) a service dependency model to assess indirect (cascading) service disruptions. A fault tree-based approach is implemented, resulting in a significant decrease in the information required to set up the service dependency model. We demonstrate the effectiveness of our method in a case study examining the impact of an extreme flood on health, transport, and power services in Cologne, Germany."}, "https://arxiv.org/abs/2410.05288": {"title": "Dense Crowd Dynamics and Pedestrian Trajectories: A Multiscale Field Study at the F\\^ete des Lumi\\`eres in Lyon", "link": "https://arxiv.org/abs/2410.05288", "description": "arXiv:2410.05288v1 Announce Type: new \nAbstract: We present one of the first comprehensive field datasets capturing dense pedestrian dynamics across multiple scales, ranging from macroscopic crowd flows over distances of several hundred meters to microscopic individual trajectories, including approximately 7,000 recorded trajectories. The dataset also includes a sample of GPS traces, statistics on contact and push interactions, as well as a catalog of non-standard crowd phenomena observed in video recordings. Data were collected during the 2022 Festival of Lights in Lyon, France, within the framework of the French-German MADRAS project, covering pedestrian densities up to 4 individuals per square meter."}, "https://arxiv.org/abs/2410.05290": {"title": "Curve Segment Neighborhood-based Vector Field Exploration", "link": "https://arxiv.org/abs/2410.05290", "description": "arXiv:2410.05290v1 Announce Type: new \nAbstract: Integral curves have been widely used to represent and analyze various vector fields. In this paper, we propose a Curve Segment Neighborhood Graph (CSNG) to capture the relationships between neighboring curve segments. This graph representation enables us to adapt the fast community detection algorithm, i.e., the Louvain algorithm, to identify individual graph communities from CSNG. Our results show that these communities often correspond to the features of the flow. To achieve a multi-level interactive exploration of the detected communities, we adapt a force-directed layout that allows users to refine and re-group communities based on their domain knowledge. We incorporate the proposed techniques into an interactive system to enable effective analysis and interpretation of complex patterns in large-scale integral curve datasets."}, "https://arxiv.org/abs/2410.05291": {"title": "Liberal-Conservative Hierarchies of Intercoder Reliability Estimators", "link": "https://arxiv.org/abs/2410.05291", "description": "arXiv:2410.05291v1 Announce Type: new \nAbstract: While numerous indices of inter-coder reliability exist, Krippendorff's {\\alpha} and Cohen's \\{kappa} have long dominated in communication studies and other fields, respectively. The near consensus, however, may be near the end. Recent theoretical and mathematical analyses reveal that these indices assume intentional and maximal random coding, leading to paradoxes and inaccuracies. A controlled experiment with one-way golden standard and Monte Carlo simulations supports these findings, showing that \\{kappa} and {\\alpha} are poor predictors and approximators of true intercoder reliability. As consensus on a perfect index remains elusive, more authors recommend selecting the best available index for specific situations (BAFS). To make informed choices, researchers, reviewers, and educators need to understand the liberal-conservative hierarchy of indices, i.e., which indices produce higher or lower scores. This study extends previous efforts by expanding the math-based hierarchies to include 23 indices and constructing six additional hierarchies using Monte Carlo simulations. These simulations account for factors like the number of categories and distribution skew. The resulting eight hierarchies display a consistent pattern and reveal a previously undetected paradox in the Ir index."}, "https://arxiv.org/abs/2410.05303": {"title": "Integrating problem structuring methods with formal design theory: collective water management policy design in Tunisia", "link": "https://arxiv.org/abs/2410.05303", "description": "arXiv:2410.05303v1 Announce Type: new \nAbstract: Groundwater management, especially in regions like Tunisia, is challenging due to diverse stakeholder interests and the dry structure of climate, which is extremely challenging for the sustainability of water resources. This paper proposes an innovative approach to policy design by merging Problem Structuring Methods (PSMs) and the Policy-Knowledge, Concepts, Proposals (P-KCP) methodology. Utilizing cognitive maps and value trees, the study aims to generate new collective groundwater management practices. Bridging decision theory and design theory, the study addresses the gap in new alternative generation and highlights the P-KCP's role in innovative policy design. Integrating PSMs and C-K theory, the framework expands policy alternatives and advocates for participatory approaches. It emphasizes adaptability across contexts, provides replicable process descriptions, and encourages the creation of unconventional policy solutions. Ultimately, this comprehensive framework offers a practical guide for policy innovation and collaboration."}, "https://arxiv.org/abs/2410.05327": {"title": "Investigating the Trade-off between Infections and Social Interactions Using a Compact Model of Endemic Infections on Networks", "link": "https://arxiv.org/abs/2410.05327", "description": "arXiv:2410.05327v1 Announce Type: new \nAbstract: In many epidemiological and ecological contexts, there is a trade-off between infections and interaction. This arises because the links between individuals capable of spreading infections are also often associated with beneficial activities. Here, we consider how the presence of explicit network structure changes the optimal solution of a class of infection-interaction trade-offs. In order to do this, we develop and analyse a low-dimensional dynamical system approximating the network SIS epidemic. We find that network structure in the form of heterogeneous numbers of contacts can have a significant impact on the optimal number of contacts that comes out of a trade-off model."}, "https://arxiv.org/abs/2410.05453": {"title": "Interconnected Kingdoms: Comparing 'A Song of Ice and Fire' Adaptations Across Media Using Complex Networks", "link": "https://arxiv.org/abs/2410.05453", "description": "arXiv:2410.05453v1 Announce Type: new \nAbstract: In this article, we propose and apply a method to compare adaptations of the same story across different media. We tackle this task by modelling such adaptations through character networks. We compare them by leveraging two concepts at the core of storytelling: the characters involved, and the dynamics of the story. We propose several methods to match characters between media and compare their position in the networks; and perform narrative matching, i.e. match the sequences of narrative units that constitute the plots. We apply these methods to the novel series \\textit{A Song of Ice and Fire}, by G.R.R. Martin, and its comics and TV show adaptations. Our results show that interactions between characters are not sufficient to properly match individual characters between adaptations, but that using some additional information such as character affiliation or gender significantly improves the performance. On the contrary, character interactions convey enough information to perform narrative matching, and allow us to detect the divergence between the original novels and its TV show adaptation."}, "https://arxiv.org/abs/2410.05487": {"title": "Group Fairness Metrics for Community Detection Methods in Social Networks", "link": "https://arxiv.org/abs/2410.05487", "description": "arXiv:2410.05487v1 Announce Type: new \nAbstract: Understanding community structure has played an essential role in explaining network evolution, as nodes join communities which connect further to form large-scale complex networks. In real-world networks, nodes are often organized into communities based on ethnicity, gender, race, or wealth, leading to structural biases and inequalities. Community detection (CD) methods use network structure and nodes' attributes to identify communities, and can produce biased outcomes if they fail to account for structural inequalities, especially affecting minority groups. In this work, we propose group fairness metrics ($\\Phi^{F*}_{p}$) to evaluate CD methods from a fairness perspective. We also conduct a comparative analysis of existing CD methods, focusing on the performance-fairness trade-off, to determine whether certain methods favor specific types of communities based on their size, density, or conductance. Our findings reveal that the trade-off varies significantly across methods, with no specific type of method consistently outperforming others. The proposed metrics and insights will help develop and evaluate fair and high performing CD methods."}, "https://arxiv.org/abs/2410.05595": {"title": "Disruption Risk Evaluation on Large-scale Production Network with Establishments and Products", "link": "https://arxiv.org/abs/2410.05595", "description": "arXiv:2410.05595v1 Announce Type: new \nAbstract: We constructed an establishment-level production network where each establishment inputs and outputs multiple products, using data that includes the firm-level production network and establishments covering nearly all Japanese entities. The network represents the manufacturing sector with 183,951 establishments across 157,537 firms and 919,982 inter-establishment linkages. A probabilistic model of supply chain disruptions was applied to this network. The key findings are as follows: (1) The establishment-level network exhibits greater shock propagation compared to the firm-level network. (2) Incorporating actual product information leads to a larger impact on propagation compared to using industry-level information. (3) Regional shock simulations reveal that while the firm-level network shows greater shock propagation when the shock originates in Tokyo, no such difference is observed in the establishment-level network."}, "https://arxiv.org/abs/2410.05668": {"title": "Diversity and Inclusion Index with Networks and Similarity: Analysis and its Application", "link": "https://arxiv.org/abs/2410.05668", "description": "arXiv:2410.05668v1 Announce Type: new \nAbstract: In recent years, the concepts of ``diversity'' and ``inclusion'' have attracted considerable attention across a range of fields, encompassing both social and biological disciplines. To fully understand these concepts, it is critical to not only examine the number of categories but also the similarities and relationships among them. In this study, I introduce a novel index for diversity and inclusion that considers similarities and network connections. I analyzed the properties of these indices and investigated their mathematical relationships using established measures of diversity and networks. Moreover, I developed a methodology for estimating similarities based on the utility of diversity. I also created a method for visualizing proportions, similarities, and network connections. Finally, I evaluated the correlation with external metrics using real-world data, confirming that both the proposed indices and our index can be effectively utilized. This study contributes to a more nuanced understanding of diversity and inclusion analysis."}, "https://arxiv.org/abs/2410.05862": {"title": "Deliberation Among Informed Citizens: The Value of Exploring Alternative Thinking Frames", "link": "https://arxiv.org/abs/2410.05862", "description": "arXiv:2410.05862v1 Announce Type: new \nAbstract: We investigate the potential of deliberation to create consensus among fully-informed citizens. Our approach relies on two cognitive assumptions: i. citizens need a thinking frame (or perspective) to consider an issue; and ii. citizens cannot consider all relevant perspectives simultaneously, but only sequentially. Altogether, these assumptions imply that the citizens opinions are intrinsically contextual, and exhibit features analogous to those of quantum systems. Formally, we capture contextuality in a simple quantum-like cognitive model. We consider a binary voting problem, in which citizens with incompatible thinking frames and initially opposite voting intentions deliberate under the guidance of a benevolent facilitator. We find that offering a citizen the opportunity to probe alternative thinking frames may allow them change opinion. The probability for reaching consensus depends on the correlation between perspectives, and on their sophistication (mathematically captured by their dimensionality in the quantum model). Maximally uncorrelated sophisticated perspectives give the highest chance for opinion change, and hence for reaching consensus. With more than two citizens, multiple deliberation rounds with experts allow reaching consensus with significant probability. A first central lesson of this work is that, if one admits that opinions are contextual, the diversity of perspectives is beneficial, and even necessary, to overcome initial disagreement. One also learns that well-designed procedures managed by a facilitator are needed to increase the probability for consensus. An additional finding is that the richness of a thinking frame helps convergence towards consensus, and that the optimal facilitator's strategy entails focusing deliberation on a properly reduced problem."}, "https://arxiv.org/abs/2410.06017": {"title": "Evacuation patterns and socioeconomic stratification in the context of wildfires in Chile", "link": "https://arxiv.org/abs/2410.06017", "description": "arXiv:2410.06017v1 Announce Type: new \nAbstract: Climate change is altering the frequency and intensity of wildfires, leading to increased evacuation events that disrupt human mobility and socioeconomic structures. These disruptions affect access to resources, employment, and housing, amplifying existing vulnerabilities within communities. Understanding the interplay between climate change, wildfires, evacuation patterns, and socioeconomic factors is crucial for developing effective mitigation and adaptation strategies. To contribute to this challenge, we use high-definition mobile phone records to analyse evacuation patterns during the wildfires in Valpara\\'iso, Chile, that took place between February 2-3, 2024. This data allows us to track the movements of individuals in the disaster area, providing insight into how people respond to large-scale evacuations in the context of severe wildfires. We apply a causal inference approach that combines regression discontinuity and difference-in-differences methodologies to observe evacuation behaviours during wildfires, with a focus on socioeconomic stratification. This approach allows us to isolate the impact of the wildfires on different socioeconomic groups by comparing the evacuation patterns of affected populations before and after the event, while accounting for underlying trends and discontinuities at the threshold of the disaster. We find that many people spent nights away from home, with those in the lowest socioeconomic segment stayed away the longest. In general, people reduced their travel distance during the evacuation, and the lowest socioeconomic group moved the least. Initially, movements became more random, as people sought refuge in a rush, but eventually gravitated towards areas with similar socioeconomic status. Our results show that socioeconomic differences play a role in evacuation dynamics, providing useful insights for response planning."}, "https://arxiv.org/abs/2410.06031": {"title": "Patient flow networks absorb healthcare stress during pandemic crises", "link": "https://arxiv.org/abs/2410.06031", "description": "arXiv:2410.06031v1 Announce Type: new \nAbstract: Disasters, such as the recent COVID-19 pandemic, impose recurrent and heterogeneous stress on healthcare systems, necessitating the redistribution of stress to enhance healthcare resilience. However, existing studies have been hindered by limited datasets and approaches for assessing its absorptive capacity - defined as the system's ability to absorb stress by redistributing patient flows. This study addresses this gap by analyzing patient flow networks constructed from billions of electronic medical records and introducing an approach to quantify network absorptivity under crisis conditions. Our analysis of U.S. healthcare systems reveals that during the COVID-19 pandemic, cross-regional patient flows increased by 3.89%, a 0.90% rise from pre-pandemic levels. The networks exhibited an average absorptivity of 0.21, representing a 10% increase over pre-pandemic conditions. Flow networks with higher connectivity and heterogeneity showed a greater capacity to alleviate system burdens. These empirical and analytical insights underscore the critical role of proactive patient flow management in strengthening healthcare resilience during crises."}, "https://arxiv.org/abs/2410.06279": {"title": "Exploring Growing Complex Systems with Higher-Order Interactions", "link": "https://arxiv.org/abs/2410.06279", "description": "arXiv:2410.06279v1 Announce Type: new \nAbstract: A complex system with many interacting individuals can be represented by a network consisting of nodes and links representing individuals and pairwise interactions, respectively. However, real-world systems grow with time and include many higher-order interactions. Such systems with higher-order interactions can be well described by a simplicial complex (SC), which is a type of hypergraph, consisting of simplexes representing sets of multiple interacting nodes. Here, capturing the properties of growing real-world systems, we propose a growing random SC (GRSC) model where a node is added and a higher dimensional simplex is established among nodes at each time step. We then rigorously derive various percolation properties in the GRSC. Finally, we confirm that the system exhibits an infinite-order phase transition as higher-order interactions accelerate the growth of the system and result in the advanced emergence of a giant cluster. This work can pave the way for interpreting growing complex systems with higher-order interactions such as social, biological, brain, and technological systems."}, "https://arxiv.org/abs/2410.06457": {"title": "What can PhD students and postdocs do to counter inequalities?", "link": "https://arxiv.org/abs/2410.06457", "description": "arXiv:2410.06457v1 Announce Type: new \nAbstract: In this opinion article, we gathered some reflections and practical tips on what Early Stage Researchers do against inequalities in academia. This is the longer version of an opinion paper that was recently published on the website of the International Society for Optics and Photonics (spie.org), containing more details and suggestions."}, "https://arxiv.org/abs/2410.06663": {"title": "Data-informed modeling of the formation, persistence, and evolution of social norms and conventions", "link": "https://arxiv.org/abs/2410.06663", "description": "arXiv:2410.06663v1 Announce Type: new \nAbstract: Social norms and conventions are commonly accepted and adopted behaviors and practices within a social group that guide interactions -- e.g., how to spell a word or how to greet people -- and are central to a group's culture and identity. Understanding the key mechanisms that govern the formation, persistence, and evolution of social norms and conventions in social communities is a problem of paramount importance for a broad range of real-world applications, spanning from preparedness for future emergencies to promotion of sustainable practices. In the past decades, mathematical modeling has emerged as a powerful tool to reproduce and study the complex dynamics of norm and convention change, gaining insights into their mechanisms, and ultimately deriving tools to predict their evolution. The first goal of this chapter is to introduce some of the main mathematical approaches for modeling social norms and conventions, including population models and agent-based models relying on the theories of dynamical systems, evolutionary dynamics, and game theory. The second goal of the chapter is to illustrate how quantitative observations and empirical data can be incorporated into these mathematical models in a systematic manner, establishing a data-based approach to mathematical modeling of formation, persistence, and evolution of social norms and conventions. Finally, current challenges and future opportunities in this growing field of research are discussed."}, "https://arxiv.org/abs/2410.07156": {"title": "Towards the science of living structure: Making and remaking livable cities as part of Urban Informatics", "link": "https://arxiv.org/abs/2410.07156", "description": "arXiv:2410.07156v1 Announce Type: new \nAbstract: This chapter investigates the concept of living structure - which is defined as a structural hierarchy that has a recurring pattern of an abundance of small substructures compared to larger ones - and the application of such structures in creating livable cities within urban informatics. By integrating practical, scientific, and artistic innovations, living structures provide a theoretical framework for designing healing environments and understanding urban complexity. We conceptualize spaces through hierarchical transformations, calculating livingness (L) as L = S * H, where S is the number of substructures and H is the inherent hierarchy of those substructures. Living structure is governed by the scaling law and Tobler's law, and guided by differentiation and adaptation principles, and it fosters vibrant and engaging spaces that enhance human well-being and a sense of place. Urban informatics, urban planning, and architecture must evolve beyond just understanding and prediction to include purposeful design. This new kind of science integrates the theory of living structure and emphasizes creation and design, thus transforming those disciplines. This chapter looks at environments that have high structural beauty, as defined by the 15 properties that Christopher Alexander proposed, and discusses the application of those properties in architecture, urban informatics, and emerging technologies such as artificial intelligence, with the aim of making built environments more vibrant and conducive to human well-being.\n  Keywords: Livable cities, structural beauty, differentiation, adaptation, architectural design, urban complexity"}, "https://arxiv.org/abs/2410.05287": {"title": "Hate Speech Detection Using Cross-Platform Social Media Data In English and German Language", "link": "https://arxiv.org/abs/2410.05287", "description": "arXiv:2410.05287v1 Announce Type: cross \nAbstract: Hate speech has grown into a pervasive phenomenon, intensifying during times of crisis, elections, and social unrest. Multiple approaches have been developed to detect hate speech using artificial intelligence, but a generalized model is yet unaccomplished. The challenge for hate speech detection as text classification is the cost of obtaining high-quality training data. This study focuses on detecting bilingual hate speech in YouTube comments and measuring the impact of using additional data from other platforms in the performance of the classification model. We examine the value of additional training datasets from cross-platforms for improving the performance of classification models. We also included factors such as content similarity, definition similarity, and common hate words to measure the impact of datasets on performance. Our findings show that adding more similar datasets based on content similarity, hate words, and definitions improves the performance of classification models. The best performance was obtained by combining datasets from YouTube comments, Twitter, and Gab with an F1-score of 0.74 and 0.68 for English and German YouTube comments."}, "https://arxiv.org/abs/2410.05359": {"title": "Interactive Event Sifting using Bayesian Graph Neural Networks", "link": "https://arxiv.org/abs/2410.05359", "description": "arXiv:2410.05359v1 Announce Type: cross \nAbstract: Forensic analysts often use social media imagery and texts to understand important events. A primary challenge is the initial sifting of irrelevant posts. This work introduces an interactive process for training an event-centric, learning-based multimodal classification model that automates sanitization. We propose a method based on Bayesian Graph Neural Networks (BGNNs) and evaluate active learning and pseudo-labeling formulations to reduce the number of posts the analyst must manually annotate. Our results indicate that BGNNs are useful for social-media data sifting for forensics investigations of events of interest, the value of active learning and pseudo-labeling varies based on the setting, and incorporating unlabelled data from other events improves performance."}, "https://arxiv.org/abs/2410.05401": {"title": "Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation", "link": "https://arxiv.org/abs/2410.05401", "description": "arXiv:2410.05401v1 Announce Type: cross \nAbstract: Climate change communication on social media increasingly employs microtargeting strategies to effectively reach and influence specific demographic groups. This study presents a post-hoc analysis of microtargeting practices within climate campaigns by leveraging large language models (LLMs) to examine Facebook advertisements. Our analysis focuses on two key aspects: demographic targeting and fairness. We evaluate the ability of LLMs to accurately predict the intended demographic targets, such as gender and age group, achieving an overall accuracy of 88.55%. Furthermore, we instruct the LLMs to generate explanations for their classifications, providing transparent reasoning behind each decision. These explanations reveal the specific thematic elements used to engage different demographic segments, highlighting distinct strategies tailored to various audiences. Our findings show that young adults are primarily targeted through messages emphasizing activism and environmental consciousness, while women are engaged through themes related to caregiving roles and social advocacy. In addition to evaluating the effectiveness of LLMs in detecting microtargeted messaging, we conduct a comprehensive fairness analysis to identify potential biases in model predictions. Our findings indicate that while LLMs perform well overall, certain biases exist, particularly in the classification of senior citizens and male audiences. By showcasing the efficacy of LLMs in dissecting and explaining targeted communication strategies and by highlighting fairness concerns, this study provides a valuable framework for future research aimed at enhancing transparency, accountability, and inclusivity in social media-driven climate campaigns."}, "https://arxiv.org/abs/2410.06042": {"title": "Weighted Embeddings for Low-Dimensional Graph Representation", "link": "https://arxiv.org/abs/2410.06042", "description": "arXiv:2410.06042v1 Announce Type: cross \nAbstract: Learning low-dimensional numerical representations from symbolic data, e.g., embedding the nodes of a graph into a geometric space, is an important concept in machine learning. While embedding into Euclidean space is common, recent observations indicate that hyperbolic geometry is better suited to represent hierarchical information and heterogeneous data (e.g., graphs with a scale-free degree distribution). Despite their potential for more accurate representations, hyperbolic embeddings also have downsides like being more difficult to compute and harder to use in downstream tasks.\n  We propose embedding into a weighted space, which is closely related to hyperbolic geometry but mathematically simpler. We provide the embedding algorithm WEmbed and demonstrate, based on generated as well as over 2000 real-world graphs, that our weighted embeddings heavily outperform state-of-the-art Euclidean embeddings for heterogeneous graphs while using fewer dimensions. The running time of WEmbed and embedding quality for the remaining instances is on par with state-of-the-art Euclidean embedders."}, "https://arxiv.org/abs/2410.06370": {"title": "HumVI: A Multilingual Dataset for Detecting Violent Incidents Impacting Humanitarian Aid", "link": "https://arxiv.org/abs/2410.06370", "description": "arXiv:2410.06370v1 Announce Type: cross \nAbstract: Humanitarian organizations can enhance their effectiveness by analyzing data to discover trends, gather aggregated insights, manage their security risks, support decision-making, and inform advocacy and funding proposals. However, data about violent incidents with direct impact and relevance for humanitarian aid operations is not readily available. An automatic data collection and NLP-backed classification framework aligned with humanitarian perspectives can help bridge this gap. In this paper, we present HumVI - a dataset comprising news articles in three languages (English, French, Arabic) containing instances of different types of violent incidents categorized by the humanitarian sector they impact, e.g., aid security, education, food security, health, and protection. Reliable labels were obtained for the dataset by partnering with a data-backed humanitarian organization, Insecurity Insight. We provide multiple benchmarks for the dataset, employing various deep learning architectures and techniques, including data augmentation and mask loss, to address different task-related challenges, e.g., domain expansion. The dataset is publicly available at https://github.com/dataminr-ai/humvi-dataset."}, "https://arxiv.org/abs/2410.06398": {"title": "Public Quantum Network: The First Node", "link": "https://arxiv.org/abs/2410.06398", "description": "arXiv:2410.06398v1 Announce Type: cross \nAbstract: We present a quantum network that distributes entangled photons between the University of Illinois Urbana-Champaign and a public library in Urbana. The network allows members of the public to perform measurements on the photons. We describe its design and implementation and outreach based on the network. Over 400 instances of public interaction have been logged with the system since it was launched in November 2023."}, "https://arxiv.org/abs/2410.06549": {"title": "DiffGAD: A Diffusion-based Unsupervised Graph Anomaly Detector", "link": "https://arxiv.org/abs/2410.06549", "description": "arXiv:2410.06549v1 Announce Type: cross \nAbstract: Graph Anomaly Detection (GAD) is crucial for identifying abnormal entities within networks, garnering significant attention across various fields. Traditional unsupervised methods, which decode encoded latent representations of unlabeled data with a reconstruction focus, often fail to capture critical discriminative content, leading to suboptimal anomaly detection. To address these challenges, we present a Diffusion-based Graph Anomaly Detector (DiffGAD). At the heart of DiffGAD is a novel latent space learning paradigm, meticulously designed to enhance its proficiency by guiding it with discriminative content. This innovative approach leverages diffusion sampling to infuse the latent space with discriminative content and introduces a content-preservation mechanism that retains valuable information across different scales, significantly improving its adeptness at identifying anomalies with limited time and space complexity. Our comprehensive evaluation of DiffGAD, conducted on six real-world and large-scale datasets with various metrics, demonstrated its exceptional performance."}, "https://arxiv.org/abs/2410.06739": {"title": "The multiscale self-similarity of the weighted human brain connectome", "link": "https://arxiv.org/abs/2410.06739", "description": "arXiv:2410.06739v1 Announce Type: cross \nAbstract: Anatomical connectivity between different regions in the brain can be mapped to a network representation, the connectome, where the intensities of the links, the weights, influence its structural resilience and the functional processes it sustains. Yet, many features associated with the weights in the human brain connectome are not fully understood, particularly their multiscale organization. In this paper, we elucidate the architecture of weights, including weak ties, in multiscale hierarchical human brain connectomes reconstructed from empirical data. Our findings reveal multiscale self-similarity in the weighted statistical properties, including the ordering of weak ties, that remain consistent across the analyzed length scales of every individual and the group representatives. This phenomenon is effectively captured by a renormalization of the weighted structure applied to hyperbolic embeddings of the connectomes, based on a unique weighted geometric model that integrates links of all weights across all length scales. This eliminates the need for separate generative weighted connectivity rules for each scale or to replicate weak and strong ties at specific scales in brain connectomes. The observed symmetry represents a distinct signature of criticality in the weighted connectivity of human brain connectomes, aligning with the fractality observed in their topology, and raises important questions for future research, like the existence of a resolution threshold where the observed symmetry breaks, or whether it is preserved in cases of neurodegenerative disease or psychiatric disorder."}, "https://arxiv.org/abs/2410.07097": {"title": "A Law of Large Numbers for SIR on the Stochastic Block Model: A Proof via Herd Immunity", "link": "https://arxiv.org/abs/2410.07097", "description": "arXiv:2410.07097v1 Announce Type: cross \nAbstract: In this paper, we study the dynamics of the susceptible-infected-recovered (SIR) model on a network with community structure, namely the stochastic block model (SBM). As usual, the SIR model is a stochastic model for an epidemic where infected vertices infect susceptible neighbors at some rate $\\eta$ and recover at rate $\\gamma$, and the SBM is a random graph model where vertices are partitioned into a finite number of communities. The connection probability between two vertices depends on their community affiliation, here scaled so that the average degrees have a finite limit as the network grows. We prove laws of large numbers (LLN) for the epidemic's trajectory to a system of ordinary differential equations over any time horizon (finite or infinite), including in particular a LLN for the final size of the infection.\n  Our proofs rely on two main ingredients: (i) a new coupling of the SIR epidemic and the randomness of the SBM, revealing a vector-valued random variable that drives the epidemic (related to what is usually called the ``force of the infection'' via a linear transformation), and (ii) a novel technique for analyzing the limiting behavior of the infinite time horizon for the infection, using the fact that once the infection passes the herd immunity threshold it dies out quickly and has a negligible impact on the overall size of the infection."}, "https://arxiv.org/abs/2410.07157": {"title": "InstructG2I: Synthesizing Images from Multimodal Attributed Graphs", "link": "https://arxiv.org/abs/2410.07157", "description": "arXiv:2410.07157v1 Announce Type: cross \nAbstract: In this paper, we approach an overlooked yet critical task Graph2Image: generating images from multimodal attributed graphs (MMAGs). This task poses significant challenges due to the explosion in graph size, dependencies among graph entities, and the need for controllability in graph conditions. To address these challenges, we propose a graph context-conditioned diffusion model called InstructG2I. InstructG2I first exploits the graph structure and multimodal information to conduct informative neighbor sampling by combining personalized page rank and re-ranking based on vision-language features. Then, a Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliary set of graph prompts to guide the denoising process of diffusion. Finally, we propose graph classifier-free guidance, enabling controllable generation by varying the strength of graph guidance and multiple connected edges to a node. Extensive experiments conducted on three datasets from different domains demonstrate the effectiveness and controllability of our approach. The code is available at https://github.com/PeterGriffinJin/InstructG2I."}, "https://arxiv.org/abs/2309.12798": {"title": "Near-future projections in continuous agent-based models for crowd dynamics: mathematical structures in use and their implications", "link": "https://arxiv.org/abs/2309.12798", "description": "arXiv:2309.12798v2 Announce Type: replace \nAbstract: This paper addresses the theoretical foundations of pedestrian models for crowd dynamics. While the topic gains momentum, current models differ widely in their mathematical structure, even if we only consider continuous agent-based models. To clarify their underpinning, we first lay mathematical foundations for the common hierarchical decomposition into strategic, tactical, and operational levels and underline the practical interest in preserving the continuity between the latter two levels by working with a floor field, rather than way-points. Turning to local navigation, we clarify how three archetypical approaches, namely, purely reactive models, anticipatory models based on the idea of times to collision, and game theory, differ in the way they extrapolate trajectories in the near future. We also insist on the oft-overlooked distinction between processes pertaining to decision-making and mechanical effects in dense settings. The implications of these differences are illustrated with a comparison of the numerical predictions of these models in the simple scenario of head-on collision avoidance between agents, by varying the walking speed, the reaction times, and the degree of courtesy of the agents, notably."}, "https://arxiv.org/abs/2312.10081": {"title": "Why business adoption of quantum and AI technology must be ethical", "link": "https://arxiv.org/abs/2312.10081", "description": "arXiv:2312.10081v2 Announce Type: replace \nAbstract: Artificial intelligence (AI) recently had its 'iPhone moment' and adoption has drastically accelerated. Quantum computing appears poised to follow suit over the next years. However, while there has been discourse about how to use AI responsibly, there is still little appreciation and awareness among executives, managers, and practitioners about the broader ethical questions and implications raised by the intersection of these emerging technologies. In this article, it is highlighted why quantum computing and AI ethics must be taken seriously by businesspersons and how these technologies affect strategic decisions; moreover, recommendations and action areas are formulated."}, "https://arxiv.org/abs/2312.16179": {"title": "European Football Player Valuation: Integrating Financial Models and Network Theory", "link": "https://arxiv.org/abs/2312.16179", "description": "arXiv:2312.16179v2 Announce Type: replace \nAbstract: This paper presents a new framework for player valuation in European football by fusing principles from financial mathematics and network theory. The valuation model leverages a \"passing matrix\" to encapsulate player interactions on the field, utilizing centrality measures to quantify individual influence. Unlike traditional approaches, this model is both metric-driven and cohort-free, providing a dynamic and individualized framework for ascertaining a player's fair market value. The methodology is empirically validated through a case study in European football, employing real-world match and financial data. The paper advances the disciplines of sports analytics and financial mathematics by offering a cross-disciplinary mechanism for player valuation, and also links together two well-known econometric methods in marginal revenue product and expected present valuation."}, "https://arxiv.org/abs/2403.14890": {"title": "Unraveling the Viral Spread of Misinformation: Maximum-Likelihood Estimation and Starlike Tree Approximation in Markovian Spreading Models", "link": "https://arxiv.org/abs/2403.14890", "description": "arXiv:2403.14890v2 Announce Type: replace \nAbstract: Identifying the source of epidemic-like spread in networks is crucial for removing internet viruses or finding the source of rumors in online social networks. The challenge lies in tracing the source from a snapshot observation of infected nodes. How do we accurately pinpoint the source? Utilizing snapshot data, we apply a probabilistic approach, focusing on the graph boundary and the observed time, to detect sources via an effective maximum likelihood algorithm. A novel starlike tree approximation extends applicability to general graphs, demonstrating versatility. Unlike previous works that rely heavily on structural properties alone, our method also incorporates temporal data for more precise source detection. We highlight the utility of the Gamma function for analyzing the ratio of the likelihood being the source between nodes asymptotically. Comprehensive evaluations confirm algorithmic effectiveness in diverse network scenarios, advancing source detection in large-scale network analysis and information dissemination strategies."}, "https://arxiv.org/abs/2403.15105": {"title": "SAGraph: A Large-scale Text-Rich Social Graph Dataset for Advertising Campaigns", "link": "https://arxiv.org/abs/2403.15105", "description": "arXiv:2403.15105v2 Announce Type: replace \nAbstract: Influencer selection in marketing involves choosing users with a strong online presence to promote products or services, leveraging their credibility and audience reach. This process is vital for its direct impact on brand visibility, consumer trust, and ultimately, sales conversion. Current research simplifies complex elements like user attitudes, thought processes, and advertising content into numerical values. This kind of approach risks missing the dynamic and contextual nuances crucial for developing effective influencer marketing strategies. To bridge this gap, we introduce a text-rich large Social Advertisement Graph (SAGraph) dataset collected from Weibo, a real-world influencer advertising platform. Our dataset centers around the advertising campaign for 6 products, consisting of 317,287 users, each with their profile information, and interaction data including 891,834 comments and 441,836 reposts. By leveraging this rich interaction and textual content, one can gain deeper insights into consumer behavior, refine influencer selection criteria, and develop more targeted and effective marketing strategies. We evaluated existing influencer selection baselines and the latest LLMs on this dataset, demonstrating the importance of textual content in advertising campaigns, as well as the availability and significant potential of LLMs for enhancing advertising strategies. We hope that this dataset will inspire further research: \\url{https://github.com/xiaoqzhwhu/SAGraph/}."}, "https://arxiv.org/abs/2306.02285": {"title": "Clarify Confused Nodes via Separated Learning", "link": "https://arxiv.org/abs/2306.02285", "description": "arXiv:2306.02285v4 Announce Type: replace-cross \nAbstract: Graph neural networks (GNNs) have achieved remarkable advances in graph-oriented tasks. However, real-world graphs invariably contain a certain proportion of heterophilous nodes, challenging the homophily assumption of traditional GNNs and hindering their performance. Most existing studies continue to design generic models with shared weights between heterophilous and homophilous nodes. Despite the incorporation of high-order messages or multi-channel architectures, these efforts often fall short. A minority of studies attempt to train different node groups separately but suffer from inappropriate separation metrics and low efficiency. In this paper, we first propose a new metric, termed Neighborhood Confusion (NC), to facilitate a more reliable separation of nodes. We observe that node groups with different levels of NC values exhibit certain differences in intra-group accuracy and visualized embeddings. These pave the way for Neighborhood Confusion-guided Graph Convolutional Network (NCGCN), in which nodes are grouped by their NC values and accept intra-group weight sharing and message passing. Extensive experiments on both homophilous and heterophilous benchmarks demonstrate that our framework can effectively separate nodes and yield significant performance improvement compared to the latest methods. The source code will be available in https://github.com/Graph4Sec-Team/NCGNN."}, "https://arxiv.org/abs/2405.02522": {"title": "New contexts, old heuristics: How young people in India and the US trust online content in the age of generative AI", "link": "https://arxiv.org/abs/2405.02522", "description": "arXiv:2405.02522v2 Announce Type: replace-cross \nAbstract: We conducted in-person ethnography in India and the US to investigate how young people (18-24) trusted online content, just as generative AI (genAI) became mainstream. We found that when online, how participants determined what content to trust was shaped by emotional states, which we term \"information modes.\" Our participants reflexively shifted between modes to maintain \"emotional equilibrium,\" and eschewed engaging literacy skills in the more passive modes in which they spent the most time. We found participants imported trust heuristics from established online contexts into emerging ones (i.e., genAI). This led them to use ill-fitting trust heuristics, and exposed them to the risk of trusting false and misleading information. While many had reservations about AI, prioritizing efficiency, they used genAI and habitual heuristics to quickly achieve goals at the expense of accuracy. We conclude that literacy interventions designed to match users' distinct information modes will be most effective."}, "https://arxiv.org/abs/2410.07240": {"title": "Evaluating internal and external dissonance of belief dynamics in social systems", "link": "https://arxiv.org/abs/2410.07240", "description": "arXiv:2410.07240v1 Announce Type: new \nAbstract: Belief dynamics are fundamental to human behavior and social coordination. Individuals rely on accurate beliefs to make decisions, and shared beliefs form the basis of successful cooperation. Traditional studies often examined beliefs in isolation, but recent perspectives suggest beliefs operate as interconnected systems, both within individuals and across social networks. To better understand belief dynamics, we propose an extension of Galesic et al.'s model, which allows individuals to weigh internal and social dissonance based on belief certainty. Our model suggests that belief convergence occurs in two patterns: internal alignment, where beliefs become ideologically consistent but socially disagreeable, or social alignment, where beliefs become socially consistent but internally varied. These results highlight a competition between internal and social belief networks, with one network often dominating. Our findings suggest that belief dynamics tend to settle at extremes, indicating a need for future models to incorporate negative feedback to reflect more nuanced societal belief changes."}, "https://arxiv.org/abs/2410.07253": {"title": "Do we practice what we preach? The dissonance between resilience understanding and measurement", "link": "https://arxiv.org/abs/2410.07253", "description": "arXiv:2410.07253v1 Announce Type: new \nAbstract: Resilience is needed to make infrastructures fit for the future, but its operationalization is still lively discussed. Here, we identify three understandings of resilience from the existing literature: resilience as a process, an outcome, and a capacity. We show that all three understandings have their justification, as each plays its part in the core business of resilience, that is, dealing with disruptive events. But, we also find that the trio differs considerably in terms of the implications for the operationalization of resilience. Most importantly, only the understanding of resilience as a capacity allows for a continuous resilience monitoring and a management which is agnostic to the type of disruptive event. We therefore advocate to understand and assess resilience as a capacity. While this understanding is in line with popular opinion, it is often not reflected in the assessment approaches applied. This dissonance shows, for example, in the use of single performance curves to assess resilience. We argue that in order to assess resilience as a capacity, we need to consider multiple performance curves, otherwise we will capture the system's ability to deal with one specific event instead of its ability to deal with any surprises that come its way."}, "https://arxiv.org/abs/2410.07287": {"title": "Crafting desirable climate trajectories with RL explored socio-environmental simulations", "link": "https://arxiv.org/abs/2410.07287", "description": "arXiv:2410.07287v1 Announce Type: new \nAbstract: Climate change poses an existential threat, necessitating effective climate policies to enact impactful change. Decisions in this domain are incredibly complex, involving conflicting entities and evidence. In the last decades, policymakers increasingly use simulations and computational methods to guide some of their decisions. Integrated Assessment Models (IAMs) are one of such methods, which combine social, economic, and environmental simulations to forecast potential policy effects. For example, the UN uses outputs of IAMs for their recent Intergovernmental Panel on Climate Change (IPCC) reports. Traditionally these have been solved using recursive equation solvers, but have several shortcomings, e.g. struggling at decision making under uncertainty. Recent preliminary work using Reinforcement Learning (RL) to replace the traditional solvers shows promising results in decision making in uncertain and noisy scenarios. We extend on this work by introducing multiple interacting RL agents as a preliminary analysis on modelling the complex interplay of socio-interactions between various stakeholders or nations that drives much of the current climate crisis. Our findings show that cooperative agents in this framework can consistently chart pathways towards more desirable futures in terms of reduced carbon emissions and improved economy. However, upon introducing competition between agents, for instance by using opposing reward functions, desirable climate futures are rarely reached. Modelling competition is key to increased realism in these simulations, as such we employ policy interpretation by visualising what states lead to more uncertain behaviour, to understand algorithm failure. Finally, we highlight the current limitations and avenues for further work to ensure future technology uptake for policy derivation."}, "https://arxiv.org/abs/2410.07288": {"title": "How Social Network Structure Impacts the Ability of Zealots to Promote Weak Opinions", "link": "https://arxiv.org/abs/2410.07288", "description": "arXiv:2410.07288v1 Announce Type: new \nAbstract: Social networks are often permeated by agents who promote their opinions without allowing for their own mind to be changed: Understanding how these so-called `zealots' act to increase the prevalence of their promoted opinion over the network is important for understanding opinion dynamics. In this work, we consider these promoted opinions to be `weak' and therefore less likely to be accepted relative to the default opinion in the network. We show how the proportion of zealots in the network, the relative strength of the weak opinion, and the structure of the network impact the long-term proportion of the those in the network who subscribe to the weak opinion."}, "https://arxiv.org/abs/2410.07388": {"title": "On Densest $k$-Subgraph Mining and Diagonal Loading", "link": "https://arxiv.org/abs/2410.07388", "description": "arXiv:2410.07388v1 Announce Type: new \nAbstract: The Densest $k$-Subgraph (D$k$S) problem aims to find a subgraph comprising $k$ vertices with the maximum number of edges between them. A continuous reformulation of the binary quadratic D$k$S problem is considered, which incorporates a diagonal loading term. It is shown that this non-convex, continuous relaxation is tight for a range of diagonal loading parameters, and the impact of the diagonal loading parameter on the optimization landscape is studied. On the algorithmic side, two projection-free algorithms are proposed to tackle the relaxed problem, based on Frank-Wolfe and explicit constraint parametrization, respectively. Experiments suggest that both algorithms have merits relative to the state-of-art, while the Frank-Wolfe-based algorithm stands out in terms of subgraph density, computational complexity, and ability to scale up to very large datasets."}, "https://arxiv.org/abs/2410.07744": {"title": "Modularity maximization and community detection in complex networks through recursive and hierarchical annealing in the D-Wave Advantage quantum processing units", "link": "https://arxiv.org/abs/2410.07744", "description": "arXiv:2410.07744v1 Announce Type: new \nAbstract: Quantum adiabatic optimization has long been expected to outperform classical methods in solving NP-type problems. While this has been proven in certain experiments, its main applications still reside in academic problems where the size of the system to be solved would not represent an obstacle to any modern desktop computer. Here we develop a systematic procedure to find the global optima of the modularity function to discover community structure in complex networks solely relying on pure annealers rather than hybrid solutions. We bypass the one-hot encoding constraints by hierarchically and recursively encoding binary instances of the problem that can be solved without the need to guess the exact penalties for the Lagrange multipliers. We study the variability, and robustness of the annealer as a function of network size, directness of connections, topology, and the resolution of the communities. We show how our approach produces meaningful and at least equally optimal solutions to state-of-the-art community detection algorithms while maintaining tractable computing times. Lastly, due to its recursive nature, the annealing process returns intermediate subdivisions thus offering interpretable rather than black-box solutions. These \\textit{dendrograms} can be used to unveil normal and pathological hidden hierarchies in brain networks hence opening the door to clinical workflows. Overall, this represents a first step towards an applicable practice-oriented usage of pure quantum annealing potentially bridging two segregated communities in modern science and engineering; that of network science and quantum computing."}, "https://arxiv.org/abs/2410.07947": {"title": "Exploring the core-periphery and community structure in the financial networks through random matrix theory", "link": "https://arxiv.org/abs/2410.07947", "description": "arXiv:2410.07947v1 Announce Type: new \nAbstract: In finance, Random Matrix Theory (RMT) is an important tool for filtering out noise from large datasets, revealing true correlations among stocks, enhancing risk management and portfolio optimization. In this study, we use RMT to filter out noise from the full cross-correlation matrix of stock price returns for the NIFTY 200 and NIFTY 500 indices on the National Stock Exchange of India. In addition, we applied network theory tools to analyze market and sector modes as filtered correlation structures to study local interactions within financial networks. This allows us to study the very fundamental properties of networks, such as the core-periphery and the community structure of constructed networks over these filtered modes, and compare the results with the network constructed over the full cross-correlation matrix. The results suggest that the core-periphery structure is contained in the market mode, while the community structure is in the sector mode. Thus, both modes outperform the full cross-correlation in terms of capturing the essential respective structure of the network. Furthermore, we used these insights to build portfolios based on communities of the networks corresponding to the sector mode and the network corresponding to the full cross-correlation matrix. The results suggest that the portfolio constructed on the complete cross-correlation-based matrix performs better than the sector mode. These insights provide a greater understanding of RMT application in the financial market."}, "https://arxiv.org/abs/2410.07280": {"title": "Action Research in Astronomy and Ecology: The observatory of the night environment in Grenoble", "link": "https://arxiv.org/abs/2410.07280", "description": "arXiv:2410.07280v1 Announce Type: cross \nAbstract: We present an example of low-carbon research activity carried out by astrophysicists and focused on ecology and environmental protection, with direct impacts on territories and society. This project serves as an illustration of action research for an astrophysics lab in the context of the current ecological crisis."}, "https://arxiv.org/abs/2410.07285": {"title": "Impact of Exoplanet Science on Society: Professional Contributions, Citizen Science Engagement and Public Perception", "link": "https://arxiv.org/abs/2410.07285", "description": "arXiv:2410.07285v1 Announce Type: cross \nAbstract: The impact of exoplanet science on both the scientific community and on the general public is presented through various indicators and examples. It is estimated that about 3-4% of all refereed astronomy articles focus on exoplanets, and between 15-20% percent of current, and up to 25% of upcoming astronomy space missions are dedicated to exoplanet research. Also, about 15-20% of the science cases for large multi-purpose ground-based astronomical instruments involve exoplanet science.\n  Interactions between the scientific community and the public occur on several levels and play a crucial role in shaping the future of exoplanet science. The rise of citizen science platforms and the successes of coordinated observing projects involving amateur astronomers have engaged the public in meaningful scientific contributions, and contribute to some areas of discovery and characterization of exoplanet systems, for which several examples are given. These initiatives not only fuel public interest in the search for extraterrestrial life but also promote STEM education, broadening participation in science.\n  Lastly, the changing perception of the informed public about the existence of \"other Earths\" and life in the Universe in the light of results from exoplanet science is outlined. Media coverage of results from exoplanet science has furthered the acceptance that extraterrestrial life, be it intelligent of not, is not rare in the Universe. The shift in perception that such life might be detected in a potentially not very distant future has, in turn, promoted public support for the research infrastructure necessary to sustain the growth of exoplanetology."}, "https://arxiv.org/abs/2410.07302": {"title": "Examining the Prevalence and Dynamics of AI-Generated Media in Art Subreddits", "link": "https://arxiv.org/abs/2410.07302", "description": "arXiv:2410.07302v1 Announce Type: cross \nAbstract: Broadly accessible generative AI models like Dall-E have made it possible for anyone to create compelling visual art. In online communities, the introduction of AI-generated content (AIGC) may impact community dynamics by shifting the kinds of content being posted or the responses to content suspected of being generated by AI. We take steps towards examining the potential impact of AIGC on art-related communities on Reddit. We distinguish between communities that disallow AI content and those without a direct policy. We look at image-based posts made to these communities that are transparently created by AI, or comments in these communities that suspect authors of using generative AI. We find that AI posts (and accusations) have played a very small part in these communities through the end of 2023, accounting for fewer than 0.2% of the image-based posts. Even as the absolute number of author-labelled AI posts dwindles over time, accusations of AI use remain more persistent. We show that AI content is more readily used by newcomers and may help increase participation if it aligns with community rules. However, the tone of comments suspecting AI use by others have become more negative over time, especially in communities that do not have explicit rules about AI. Overall, the results show the changing norms and interactions around AIGC in online communities designated for creativity."}, "https://arxiv.org/abs/2410.07713": {"title": "A Hate Speech Moderated Chat Application: Use Case for GDPR and DSA Compliance", "link": "https://arxiv.org/abs/2410.07713", "description": "arXiv:2410.07713v1 Announce Type: cross \nAbstract: The detection of hate speech or toxic content online is a complex and sensitive issue. While the identification itself is highly dependent on the context of the situation, sensitive personal attributes such as age, language, and nationality are rarely available due to privacy concerns. Additionally, platforms struggle with a wide range of local jurisdictions regarding online hate speech and the evaluation of content based on their internal ethical norms. This research presents a novel approach that demonstrates a GDPR-compliant application capable of implementing legal and ethical reasoning into the content moderation process. The application increases the explainability of moderation decisions by utilizing user information. Two use cases fundamental to online communication are presented and implemented using technologies such as GPT-3.5, Solid Pods, and the rule language Prova. The first use case demonstrates the scenario of a platform aiming to protect adolescents from potentially harmful content by limiting the ability to post certain content when minors are present. The second use case aims to identify and counter problematic statements online by providing counter hate speech. The counter hate speech is generated using personal attributes to appeal to the user. This research lays the groundwork for future DSA compliance of online platforms. The work proposes a novel approach to reason within different legal and ethical definitions of hate speech and plan the fitting counter hate speech. Overall, the platform provides a fitted protection to users and a more explainable and individualized response. The hate speech detection service, the chat platform, and the reasoning in Prova are discussed, and the potential benefits for content moderation and algorithmic hate speech detection are outlined. A selection of important aspects for DSA compliance is outlined."}, "https://arxiv.org/abs/2410.08030": {"title": "Chaos in opinion-driven disease dynamics", "link": "https://arxiv.org/abs/2410.08030", "description": "arXiv:2410.08030v1 Announce Type: cross \nAbstract: During the COVID-19 pandemic, it became evident that the effectiveness of applying intervention measures is significantly influenced by societal acceptance, which, in turn, is affected by the processes of opinion formation. This article explores one among the many possibilities of a coupled opinion-epidemic system. The findings reveal either intricate periodic patterns or chaotic dynamics, leading to substantial fluctuations in opinion distribution and, consequently, significant variations in the total number of infections over time. Interestingly, the model is exhibiting the protective pattern."}, "https://arxiv.org/abs/2410.08050": {"title": "Agent-based modeling for realistic reproduction of human mobility and contact behavior to evaluate test and isolation strategies in epidemic infectious disease spread", "link": "https://arxiv.org/abs/2410.08050", "description": "arXiv:2410.08050v1 Announce Type: cross \nAbstract: Agent-based models have proven to be useful tools in supporting decision-making processes in different application domains. The advent of modern computers and supercomputers has enabled these bottom-up approaches to realistically model human mobility and contact behavior. The COVID-19 pandemic showcased the urgent need for detailed and informative models that can answer research questions on transmission dynamics. We present a sophisticated agent-based model to simulate the spread of respiratory diseases. The model is highly modularized and can be used on various scales, from a small collection of buildings up to cities or countries. Although not being the focus of this paper, the model has undergone performance engineering on a single core and provides an efficient intra- and inter-simulation parallelization for time-critical decision-making processes.\n  In order to allow answering research questions on individual level resolution, nonpharmaceutical intervention strategies such as face masks or venue closures can be implemented for particular locations or agents. In particular, we allow for sophisticated testing and isolation strategies to study the effects of minimal-invasive infectious disease mitigation. With realistic human mobility patterns for the region of Brunswick, Germany, we study the effects of different interventions between March 1st and May 30, 2021 in the SARS-CoV-2 pandemic. Our analyses suggest that symptom-independent testing has limited impact on the mitigation of disease dynamics if the dark figure in symptomatic cases is high. Furthermore, we found that quarantine length is more important than quarantine efficiency but that, with sufficient symptomatic control, also short quarantines can have a substantial effect."}, "https://arxiv.org/abs/2211.03419": {"title": "Body and mind: Decoding the dynamics of pedestrians and the effect of smartphone distraction by coupling mechanical and decisional processes", "link": "https://arxiv.org/abs/2211.03419", "description": "arXiv:2211.03419v4 Announce Type: replace-cross \nAbstract: Pedestrians are able to anticipate, which gives them an edge in avoiding collisions and navigating in cluttered spaces. However, these capabilities are impaired by digital distraction through smartphones, a growing safety concern. To capture these features, we put forward a continuous agent-based model (dubbed ANDA) hinging on a transparent delineation of a decision-making process, wherein a desired velocity is selected as the optimum of a perceived cost, and a mechanical layer that handles contacts and collisions. Altogether, the model includes less than a dozen parameters, many of which are fit using independent experimental data. The versatility of ANDA is demonstrated by numerical simulations that successfully replicate empirical observations in a very wide range of scenarios. These scenarios vary from collision avoidance involving one, two, or more agents, to collective flow properties in unidirectional and bidirectional settings, and to the dynamics of evacuation through a bottleneck, where contact forces are directly accessible. Remarkably, the model is able to replicate the enhanced chaoticity of the flow observed experimentally in 'smartphone-walking' pedestrians, by reducing the frequency of decisional updates, replicating the digital distraction effect. The conceptual transparency of the model makes it easy to pinpoint the origin of its current limitations and to clarify the singular position of pedestrian crowds amid active-matter systems."}, "https://arxiv.org/abs/2402.11454": {"title": "An Approach for Addressing Internally-Disconnected Communities in Louvain Algorithm", "link": "https://arxiv.org/abs/2402.11454", "description": "arXiv:2402.11454v5 Announce Type: replace-cross \nAbstract: Community detection is the problem of identifying densely connected clusters within a network. While the Louvain algorithm is commonly used for this task, it can produce internally-disconnected communities. To address this, the Leiden algorithm was introduced. This technical report introduces GSP-Louvain, a parallel algorithm based on Louvain, which mitigates this issue. Running on a system with two 16-core Intel Xeon Gold 6226R processors, GSP-Louvain outperforms Leiden, NetworKit Leiden, and cuGraph Leiden by 391x, 6.9x, and 2.6x respectively, processing 410M edges per second on a 3.8B edge graph. Furthermore, GSP-Louvain improves performance at a rate of 1.5x for every doubling of threads."}, "https://arxiv.org/abs/2410.08238": {"title": "NetDiff: Deep Graph Denoising Diffusion for Ad Hoc Network Topology Generation", "link": "https://arxiv.org/abs/2410.08238", "description": "arXiv:2410.08238v1 Announce Type: new \nAbstract: This work introduces NetDiff, an expressive graph denoising diffusion probabilistic architecture that generates wireless ad hoc network link topologies. Such networks, with directional antennas, can achieve unmatched performance when the communication links are designed to provide good geometric properties, notably by reducing interference between these links while respecting diverse physical constraints. How to craft such a link assignment algorithm is yet a real problem. Deep graph generation offers multiple advantages compared to traditional approaches: it allows to relieve the network nodes of the communication burden caused by the search of viable links and to avoid resorting to heavy combinatorial methods to find a good link topology. Denoising diffusion also provides a built-in method to update the network over time. Given that graph neural networks sometimes tend to struggle with global, structural properties, we augment the popular graph transformer with cross-attentive modulation tokens in order to improve global control over the predicted topology. We also incorporate simple node and edge features, as well as additional loss terms, to facilitate the compliance with the network topology physical constraints. A network evolution algorithm based on partial diffusion is also proposed to maintain a stable network topology over time when the nodes move. Our results show that the generated links are realistic, present structural properties similar to the dataset graphs', and require only minor corrections and verification steps to be operational."}, "https://arxiv.org/abs/2410.08302": {"title": "A Framework to Audit Email Address Privacy and Analyze Email Marketing Practices of Online Services and Apps", "link": "https://arxiv.org/abs/2410.08302", "description": "arXiv:2410.08302v1 Announce Type: new \nAbstract: This study explores the widespread perception that personal data, such as email addresses, may be shared or sold without informed user consent, investigating whether these concerns are reflected in actual practices of popular online services and apps. Over the course of a year, we collected and analyzed the source, volume, frequency, and content of emails received by users after signing up for the 150 most popular online services and apps across various sectors. By examining patterns in email communications, we aim to identify consistent strategies used across industries, including potential signs of third-party data sharing. This analysis provides a critical evaluation of how email marketing tactics may intersect with data-sharing practices, with important implications for consumer privacy and regulatory oversight. Our study findings, conducted post-CCPA and GDPR, indicate that while no third-party spam email was detected, internal email marketing practices were pervasive, with companies frequently sending promotional and CRM emails despite opt-out preferences. The framework established in this work is designed to be scalable, allowing for continuous monitoring, and can be extended to include a more diverse set of apps and services for broader analysis, ultimately contributing to improved user perception of data privacy practices."}, "https://arxiv.org/abs/2410.08374": {"title": "Decoding Segregation: Navigating a century of segregation research across disciplines and introducing a bottom-up ontology", "link": "https://arxiv.org/abs/2410.08374", "description": "arXiv:2410.08374v1 Announce Type: new \nAbstract: Segregation is a widely recognised phenomenon with profound implications for societies worldwide. From political science and gender studies to anthropology and urban studies, it has garnered considerable attention across numerous scientific fields due to its multifaceted nature. However, what makes segregation such a far-reaching phenomenon? In fact, how many forms of segregation exist? Have different disciplines engaged in segregation research uncovered all its facets? This article systematically explores the landscape of segregation research spanning over a century. We analyzed 10,754 documents from the Scopus database to unveil the dynamics of the discovery of segregation forms through several findings. We identify (1) the exponential growth and increasing diversification of segregation forms, driven by combinatorial and exploratory work and increasing transdisciplinarity and intersectionality in research; (2) the evolution and structure of the field in hierarchies and clusters of segregation forms, revealing trends, persistence, and shifts over time; (3) the timing and geographical distribution of first publications on segregation forms, along with contextual variations across world regions and countries; (4) path dependencies in the historical and geographical shaping of segregation research; and (5) the structure of knowledge production. Aiming to contribute semantic organization to an increasingly complex field, we explore these findings to introduce a bottom-up ontology of segregation, marking the first comprehensive effort of its kind."}, "https://arxiv.org/abs/2410.08614": {"title": "Interdependency and cascading failures in co-patenting and shareholding interfirm networks", "link": "https://arxiv.org/abs/2410.08614", "description": "arXiv:2410.08614v1 Announce Type: new \nAbstract: This work analyses the interdependent link creation of patent and shareholding links in interfirm networks, and how this dynamics affects the resilience of such networks in the face of cascading failures. Using the Orbis dataset, we construct very large co-patenting and shareholding networks, globally as well as in terms of individual countries. Besides, we construct smaller overlap networks from those firm pairs which have both types of links between them, for nine years between 2008-2016. We use information theoretic measures, such as mutual information, active information storage, and transfer entropy, to characterise the topological similarities and shared topological information between the relevant co-patenting and shareholding networks. We then construct a cascading failure model, and use it to analyse the resilience of interdependent interfirm networks in terms of multiple failure characteristics. We find that there is relatively high level of mutual information between co-patenting networks and the shareholding networks from later years, suggesting that the formation of shareholding links is influenced by the existence of patent links in previous years. We also show that this influence becomes most apparent after a delay of four years between the formation of co-patenting links and shareholding links. Analysing the resilience of shareholding networks against cascading failures, we show that in terms of both mean downtime, and failure proportion of firms, certain countries have less resilient shareholding networks compared to other countries with significant economies. Based on our results, we postulate that an interfirm network model which considers multiple types of relationships together could be a to highlight important features of economic systems around the world."}, "https://arxiv.org/abs/2410.08642": {"title": "More than Memes: A Multimodal Topic Modeling Approach to Conspiracy Theories on Telegram", "link": "https://arxiv.org/abs/2410.08642", "description": "arXiv:2410.08642v1 Announce Type: new \nAbstract: Research on conspiracy theories and related content online has traditionally focused on textual data. To address the increasing prevalence of (audio-)visual data on social media, and to capture the evolving and dynamic nature of this communication, researchers have begun to explore the potential of unsupervised approaches for analyzing multimodal online content. Our research contributes to this field by exploring the potential of multimodal topic modeling for analyzing conspiracy theories in German-language Telegram channels. Our work uses the BERTopic topic modeling approach in combination with CLIP for the analysis of textual and visual data. We analyze a corpus of ~40, 000 Telegram messages posted in October 2023 in 571 German-language Telegram channels known for disseminating conspiracy theories and other deceptive content. We explore the potentials and challenges of this approach for studying a medium-sized corpus of user-generated, text-image online content. We offer insights into the dominant topics across modalities, different text and image genres discovered during the analysis, quantitative inter-modal topic analyses, and a qualitative case study of textual, visual, and multimodal narrative strategies in the communication of conspiracy theories."}, "https://arxiv.org/abs/2410.08672": {"title": "Explorative pedestrian mobility GPS data from a citizen science experiment in a neighbourhood", "link": "https://arxiv.org/abs/2410.08672", "description": "arXiv:2410.08672v1 Announce Type: new \nAbstract: Pedestrian GPS data are key to a better understanding of micro-mobility and micro-behaviour within a neighbourhood. These data can bring new insights into walkability and livability in the context of urban sustainability. However, pedestrian open data are scarce and often lack a context for their transformation into actionable knowledge in a neighbourhood. Citizen science and public involvement practices are powerful instruments for obtaining these data and take a community-centred placemaking approach. The study shares some 3000 GPS recordings corresponding to 19 unique trajectories made and recorded by groups of participants from three distinct communities in a relatively small neighbourhood. The groups explored the neighbourhood through a number of tasks and chose different places to stop and perform various social and festive activities. The study shares not only raw data but also processed records with specific filtering and processing to facilitate and accelerate data usage. Citizen science practices and the data-collection protocols involved are reported in order to offer a complete perspective of the research undertaken jointly with an assessment of how community-centred placemaking and operative mapping are incorporated into local urban transformation actions."}, "https://arxiv.org/abs/2410.08753": {"title": "Who should fight the spread of fake news?", "link": "https://arxiv.org/abs/2410.08753", "description": "arXiv:2410.08753v1 Announce Type: new \nAbstract: This study investigates who should bear the responsibility of combating the spread of misinformation in social networks. Should that be the online platforms or their users? Should that be done by debunking the \"fake news\" already in circulation or by investing in preemptive efforts to prevent their diffusion altogether? We seek to answer such questions in a stylized opinion dynamics framework, where agents in a network aggregate the information they receive from peers and/or from influential external sources, with the aim of learning a ground truth among a set of competing hypotheses. In most cases, we find centralized sources to be more effective at combating misinformation than distributed ones, suggesting that online platforms should play an active role in the fight against fake news. In line with literature on the \"backfire effect\", we find that debunking in certain circumstances can be a counterproductive strategy, whereas some targeted strategies (akin to \"deplatforming\") and/or preemptive campaigns turn out to be quite effective. Despite its simplicity, our model provides useful guidelines that could inform the ongoing debate on online disinformation and the best ways to limit its damaging effects."}, "https://arxiv.org/abs/2410.08777": {"title": "Compressing regularised dynamics improves link prediction in sparse networks", "link": "https://arxiv.org/abs/2410.08777", "description": "arXiv:2410.08777v1 Announce Type: new \nAbstract: Predicting future interactions or novel links in networks is an indispensable tool across diverse domains, including genetic research, online social networks, and recommendation systems. Among the numerous techniques developed for link prediction, those leveraging the networks' community structure have proven highly effective. For example, the recently proposed MapSim predicts links based on a similarity measure derived from the code structure of the map equation, a community-detection objective function that operates on network flows. However, the standard map equation assumes complete observations and typically identifies many small modules in networks where the nodes connect through only a few links. This aspect can degrade MapSim's performance on sparse networks. To overcome this limitation, we incorporate a global regularisation method based on a Bayesian estimate of the transition rates along with three local regularisation methods. The regularised versions of the map equation compensate for incomplete observations and decrease the number of identified communities in sparse networks. The regularised methods outperform standard MapSim and several state-of-the-art embedding methods in highly sparse networks. This performance holds across multiple real-world networks with randomly removed links, simulating incomplete observations. Among the proposed regularisation methods, the global regularisation method provides the most reliable community detection and the highest link prediction performance across different network densities."}, "https://arxiv.org/abs/2410.08352": {"title": "Revealing COVID-19's Social Dynamics: Diachronic Semantic Analysis of Vaccine and Symptom Discourse on Twitter", "link": "https://arxiv.org/abs/2410.08352", "description": "arXiv:2410.08352v1 Announce Type: cross \nAbstract: Social media is recognized as an important source for deriving insights into public opinion dynamics and social impacts due to the vast textual data generated daily and the 'unconstrained' behavior of people interacting on these platforms. However, such analyses prove challenging due to the semantic shift phenomenon, where word meanings evolve over time. This paper proposes an unsupervised dynamic word embedding method to capture longitudinal semantic shifts in social media data without predefined anchor words. The method leverages word co-occurrence statistics and dynamic updating to adapt embeddings over time, addressing the challenges of data sparseness, imbalanced distributions, and synergistic semantic effects. Evaluated on a large COVID-19 Twitter dataset, the method reveals semantic evolution patterns of vaccine- and symptom-related entities across different pandemic stages, and their potential correlations with real-world statistics. Our key contributions include the dynamic embedding technique, empirical analysis of COVID-19 semantic shifts, and discussions on enhancing semantic shift modeling for computational social science research. This study enables capturing longitudinal semantic dynamics on social media to understand public discourse and collective phenomena."}, "https://arxiv.org/abs/2410.08610": {"title": "Derivation of macroscopic epidemic models from multi-agent systems", "link": "https://arxiv.org/abs/2410.08610", "description": "arXiv:2410.08610v1 Announce Type: cross \nAbstract: We present a systematic review of some basic results on the derivation of classical epidemiological models from simple agent-based dynamics. The evolution of large populations is coupled with the dynamics of the contact distribution, providing insights into how individual behaviors impact macroscopic epidemiological trends. The resulting set of equations incorporates local characteristics of the operator governing the emergence of a family of contact distributions. To validate the proposed approach, we provide several numerical results based on asymptotic preserving methods, demonstrating their effectiveness in capturing the multi-scale nature of the problem and ensuring robust performance across different regimes."}, "https://arxiv.org/abs/2410.08875": {"title": "Online design of dynamic networks", "link": "https://arxiv.org/abs/2410.08875", "description": "arXiv:2410.08875v1 Announce Type: cross \nAbstract: Designing a network (e.g., a telecommunication or transport network) is mainly done offline, in a planning phase, prior to the operation of the network. On the other hand, a massive effort has been devoted to characterizing dynamic networks, i.e., those that evolve over time. The novelty of this paper is that we introduce a method for the online design of dynamic networks. The need to do so emerges when a network needs to operate in a dynamic and stochastic environment. In this case, one may wish to build a network over time, on the fly, in order to react to the changes of the environment and to keep certain performance targets. We tackle this online design problem with a rolling horizon optimization based on Monte Carlo Tree Search. The potential of online network design is showcased for the design of a futuristic dynamic public transport network, where bus lines are constructed on the fly to better adapt to a stochastic user demand. In such a scenario, we compare our results with state-of-the-art dynamic vehicle routing problem (VRP) resolution methods, simulating requests from a New York City taxi dataset. Differently from classic VRP methods, that extend vehicle trajectories in isolation, our method enables us to build a structured network of line buses, where complex user journeys are possible, thus increasing system performance."}, "https://arxiv.org/abs/2410.08948": {"title": "The Dynamics of Social Conventions in LLM populations: Spontaneous Emergence, Collective Biases and Tipping Points", "link": "https://arxiv.org/abs/2410.08948", "description": "arXiv:2410.08948v1 Announce Type: cross \nAbstract: Social conventions are the foundation for social and economic life. As legions of AI agents increasingly interact with each other and with humans, their ability to form shared conventions will determine how effectively they will coordinate behaviors, integrate into society and influence it. Here, we investigate the dynamics of conventions within populations of Large Language Model (LLM) agents using simulated interactions. First, we show that globally accepted social conventions can spontaneously arise from local interactions between communicating LLMs. Second, we demonstrate how strong collective biases can emerge during this process, even when individual agents appear to be unbiased. Third, we examine how minority groups of committed LLMs can drive social change by establishing new social conventions. We show that once these minority groups reach a critical size, they can consistently overturn established behaviors. In all cases, contrasting the experimental results with predictions from a minimal multi-agent model allows us to isolate the specific role of LLM agents. Our results clarify how AI systems can autonomously develop norms without explicit programming and have implications for designing AI systems that align with human values and societal goals."}, "https://arxiv.org/abs/2410.09092": {"title": "The impact of different degrees of leadership on collective navigation in follower-leader systems", "link": "https://arxiv.org/abs/2410.09092", "description": "arXiv:2410.09092v1 Announce Type: new \nAbstract: In both animal and cell populations, the presence of leaders often underlies the success of collective migration processes, which we characterise by a group maintaining a cohesive configuration that consistently moves toward a target. We extend a recent non-local hyperbolic model for follower-leader systems to investigate different degrees of leadership. Specifically, we consider three levels of leadership: indifferent leaders, who do not alter their movement according to followers; observant leaders, who attempt to remain connected with the followers, but do not allow followers to affect their desired alignment; and persuadable leaders, who integrate their attempt to reach some target with the alignment of all neighbours, both followers and leaders. A combination of analysis and numerical simulations is used to investigate under which conditions each degree of leadership allows successful collective movement to a destination. We find that the indifferent leaders' strategy can result in a cohesive and target-directed migration only for short times. Observant and persuadable leaders instead provide robust guidance, showing that the optimal leader behavior depends on the connection between the migrating individuals: if alignment is low, greater follower influence on leaders is beneficial for successful guidance; otherwise, it can be detrimental and may generate various unsuccessful swarming dynamics."}, "https://arxiv.org/abs/2410.09098": {"title": "Making public reputation out of private assessments", "link": "https://arxiv.org/abs/2410.09098", "description": "arXiv:2410.09098v1 Announce Type: new \nAbstract: Reputation is not just a simple opinion that an individual has about another but a social construct that emerges through communication. Despite the huge importance in coordinating human behavior, such a communicative aspect has remained relatively unexplored in the field of indirect reciprocity. In this work, we bridge the gap between private assessment and public reputation: We begin by clarifying what we mean by reputation and argue that the formation of reputation can be modeled by a bi-stochastic matrix, provided that both assessment and behavior are regarded as continuous variables. By choosing bi-stochastic matrices that represent averaging processes, we show that only four norms among the leading eight, which judge a good person's cooperation toward a bad one as good, will keep cooperation asymptotically or neutrally stable against assessment error in a homogeneous society where every member has adopted the same norm. However, when one of those four norms is used by the resident population, the opinion averaging process allows neutral invasion of mutant norms with small differences in the assessment rule. Our approach provides a theoretical framework for describing the formation of reputation in mathematical terms."}, "https://arxiv.org/abs/2410.09225": {"title": "Harmonizing vs Polarizing Platform Influence Functions", "link": "https://arxiv.org/abs/2410.09225", "description": "arXiv:2410.09225v1 Announce Type: new \nAbstract: We investigate the dynamics of opinion formation on social networking platforms, focusing on how individual opinions, influenced by both social connections and platform algorithms, evolve. We model this process using a differential equation, accounting for both peer influence and the platform's content curation based on user preferences. Our primary aim is to analyze how these factors contribute to opinion polarization and identify potential strategies for its mitigation. We explore the conditions under which opinions converge to a consensus or remain polarized, emphasizing the role of the platform's influence function. Our findings in two-agent, complete graphs, and stochastic block model provide insights into the impact of social media algorithms on public discourse and offer a framework for understanding how polarization can be avoided."}, "https://arxiv.org/abs/2410.09345": {"title": "Contrastive Learning for Implicit Social Factors in Social Media Popularity Prediction", "link": "https://arxiv.org/abs/2410.09345", "description": "arXiv:2410.09345v1 Announce Type: new \nAbstract: On social media sharing platforms, some posts are inherently destined for popularity. Therefore, understanding the reasons behind this phenomenon and predicting popularity before post publication holds significant practical value. The previous work predominantly focuses on enhancing post content extraction for better prediction results. However, certain factors introduced by social platforms also impact post popularity, which has not been extensively studied. For instance, users are more likely to engage with posts from individuals they follow, potentially influencing the popularity of these posts. We term these factors, unrelated to the explicit attractiveness of content, as implicit social factors. Through the analysis of users' post browsing behavior (also validated in public datasets), we propose three implicit social factors related to popularity, including content relevance, user influence similarity, and user identity. To model the proposed social factors, we introduce three supervised contrastive learning tasks. For different task objectives and data types, we assign them to different encoders and control their gradient flows to achieve joint optimization. We also design corresponding sampling and augmentation algorithms to improve the effectiveness of contrastive learning. Extensive experiments on the Social Media Popularity Dataset validate the superiority of our proposed method and also confirm the important role of implicit social factors in popularity prediction. We open source the code at https://github.com/Daisy-zzz/PPCL.git."}, "https://arxiv.org/abs/2410.09477": {"title": "Efficient Bipartite Graph Embedding Induced by Clustering Constraints", "link": "https://arxiv.org/abs/2410.09477", "description": "arXiv:2410.09477v1 Announce Type: new \nAbstract: Bipartite graph embedding (BGE) maps nodes to compressed embedding vectors that can reflect the hidden topological features of the network, and learning high-quality BGE is crucial for facilitating downstream applications such as recommender systems. However, most existing methods either struggle to efficiently learn embeddings suitable for users and items with fewer interactions, or exhibit poor scalability to handle large-scale networks. In this paper, we propose a Clustering Constraints induced BIpartite graph Embedding (CCBIE) as an integrated solution to both problems. CCBIE facilitates automatic and dynamic soft clustering of items in a top-down manner, and capturing macro-preference information of users through clusters. Specifically, by leveraging the cluster embedding matrix of items, CCBIE calculates the cluster assignment matrix for items and also captures the extent of user preferences across different clusters, thereby elucidating the similarity between users and items on a macro-scale level. CCBIE effectively preserves the global properties of bipartite graphs, maintaining the cluster structure of isomorphic nodes and accounting for long-range dependencies among heterogeneous nodes. Our approach significantly enhances user-item collaborative relation modeling by integrating adaptive clustering for relationship learning, thereby markedly improving prediction performance, particularly benefiting cold users and items. Extensive experiments indicate that CCBIE consistently and significantly improves accuracy over baseline models, particularly on sparse graphs, while also enhancing training speed and reducing memory requirements on large-scale bipartite graphs."}, "https://arxiv.org/abs/2410.09698": {"title": "Incentivized Network Dynamics in Digital Job Recruitment", "link": "https://arxiv.org/abs/2410.09698", "description": "arXiv:2410.09698v1 Announce Type: new \nAbstract: Online platforms have transformed the formal job market but continue to struggle with effectively engaging passive candidates-individuals not actively seeking employment but open to compelling opportunities. We introduce the Independent Halting Cascade (IHC) model, a novel framework that integrates complex network diffusion dynamics with economic game theory to address this challenge. Unlike traditional models that focus solely on information propagation, the IHC model empowers network agents to either disseminate a job posting or halt its spread by applying for the position themselves. By embedding economic incentives into agent decision-making processes, the model creates a dynamic interplay between maximizing information spread and promoting application. Our analysis uncovers distinct behavioral regimes within the IHC model, characterized by critical thresholds in recommendation and application probabilities. Extensive simulations on both synthetic and real-world network topologies demonstrate that the IHC model significantly outperforms traditional direct-recommendation systems in recruiting suitable passive candidates. Specifically, the model achieves up to a 30% higher hiring success rate compared to baseline methods. These findings offer strategic insights into leveraging economic incentives and network structures to enhance recruitment efficiency. The IHC model thus provides a robust framework for modernizing recruitment strategies, particularly in engaging the vast pool of passive candidates in the job market."}, "https://arxiv.org/abs/2410.10274": {"title": "Science vs Propaganda: The case of Russia", "link": "https://arxiv.org/abs/2410.10274", "description": "arXiv:2410.10274v1 Announce Type: new \nAbstract: This note highlights how Russia uses the international academic sphere-including scientometric databases, international publishers, and international organizations-as a propaganda tool to legitimize its appropriation of Ukrainian territories."}, "https://arxiv.org/abs/2410.10472": {"title": "Data informed epidemiological-behavioural modelling", "link": "https://arxiv.org/abs/2410.10472", "description": "arXiv:2410.10472v1 Announce Type: new \nAbstract: Augmenting classical epidemiological models with information from the social sciences helps unveil the interplay between contagion dynamics and social responses. However, multidisciplinary integration of social analysis and epidemiological modelling is often challenging, due to scarcity of vast and reliable data sources and because ad hoc modelling assumptions may not reproduce empirically observed patters. Here, we test the hypothesis that awareness and information spreading straightforwardly translates into behavioural responses, analysing empirical data to generate insights about their dynamics and relationships. We employ such results to build a data-informed behavioural-epidemiological model that elucidates the impact of compliant behaviours and the role of centralised regulations in mitigating epidemics. We investigate model properties and its benefits in integrating theoretical modelling and data."}, "https://arxiv.org/abs/2410.10658": {"title": "A Personalized MOOC Learning Group and Course Recommendation Method Based on Graph Neural Network and Social Network Analysis", "link": "https://arxiv.org/abs/2410.10658", "description": "arXiv:2410.10658v1 Announce Type: new \nAbstract: In order to enhance students' initiative and participation in MOOC learning, this study constructed a multi-level network model based on Social Network Analysis (SNA). The model makes use of data pertaining to nearly 40,000 users and tens of thousands of courses from various higher education MOOC platforms. Furthermore, an AI-based assistant has been developed which utilises the collected data to provide personalised recommendations regarding courses and study groups for students. The objective is to examine the relationship between students' course selection preferences and their academic interest levels. Based on the results of the relationship analysis, the AI assistant employs technologies such as GNN to recommend suitable courses and study groups to students. This study offers new insights into the potential of personalised teaching on MOOC platforms, demonstrating the value of data-driven and AI-assisted methods in improving the quality of online learning experiences, increasing student engagement, and enhancing learning outcomes."}, "https://arxiv.org/abs/2410.10746": {"title": "CosForce: A Force-Based General Model for Simulating Pedestrian Anticipation and Reaction Mechanisms", "link": "https://arxiv.org/abs/2410.10746", "description": "arXiv:2410.10746v1 Announce Type: new \nAbstract: In this paper, a force-based general model, named the CosForce model, has been developed. To the best of our knowledge, this may represent the most simplified version of the social force model. Considering the anisotropic influence of the environment on pedestrian interaction, the model solely focuses on interactions between pedestrians and their nearest neighbors, rather than all surrounding pedestrians. Specifically, cosine functions are employed to describe the interactions between nearest neighbors, allowing the model to simulate the anticipation and reaction mechanisms. Notably, due to the simplification of binary interactions between pedestrians, the simulation efficiency of this model is significantly improved compared to the classical social force model. By applying this model to simulations across various scenarios, a wide range of pedestrian dynamics can be simulated, including pedestrian fundamental diagrams and self-organized phenomena such as lane formation, stripe formation, and cross-channel formation, among others. The open-source codes of the model can be found at: https://drive.google.com/drive/folders/1NYVnRp0z8VPuskfezMr51gB-sraOf6Iq?usp=drive_link, and a short video summary is available at: https://www.bilibili.com/video/BV17B1oYVEQm/."}, "https://arxiv.org/abs/2410.09067": {"title": "Evaluating Cooling Center Coverage Using Persistent Homology of a Filtered Witness Complex", "link": "https://arxiv.org/abs/2410.09067", "description": "arXiv:2410.09067v1 Announce Type: cross \nAbstract: In light of the increase in frequency of extreme heat events, there is a critical need to develop tools to identify geographic locations that are at risk of heat-related mortality. This paper aims to identify locations by assessing holes in cooling-center coverage using persistent homology, a method from topological data analysis. Methods involving persistent homology have shown promising results in identifying holes in coverage of specific resources. We adapt these methods using a witness complex construction to study the coverage of cooling centers. One standard technique for studying the risk of heat-related mortality for a geographic area is a heat vulnerability index (HVIs) based on demographic information. We test our topological approach and an HVI on four locations (central Boston, MA; central Austin, TX; Portland, OR; and Miami, FL) and use death times of connected components and cycles to identify most at risk regions. PH and HVI identify different locations as vulnerable, thus showing measures of coverage need to extend beyond a simple statistic. Using persistent homology along side the HVI score identifies a complementary set of regions at risk, thus the combination of the two provide a more holistic understanding of coverage."}, "https://arxiv.org/abs/2410.09073": {"title": "Level of Scientific Readiness with Ternary Data Types", "link": "https://arxiv.org/abs/2410.09073", "description": "arXiv:2410.09073v1 Announce Type: cross \nAbstract: In addition to the technology readiness level (TRL the scientific readiness level (SRL) has been introduced as a more authentic and adequate tool for determining the status quo of scientific and scientific-technical projects of fundamental or applied nature. The SRL includes 10 levels of scientific readiness, namely, the FRL (Fundamental Readiness Level), the ARL (Applied Readiness Level), and the IRL (Innovation Readiness Level). For quantitative and visual assessment of the level of scientific readiness, a system of positional ternary (three-valued, trinary) codes with integer trits (ternary digit) is introduced. The ternary code interprets the degree of project elaboration according to the fundamental, applied, and innovation vectors/trits FRL/ARL/IRL. Examples of scientific project assessments are provided. The main characteristics of the new scale and assessment of the level of readiness and the application areas of the scientific readiness level are noted. Tasks for further development of the system of scientific readiness levels are formulated."}, "https://arxiv.org/abs/2410.09348": {"title": "BANGS: Game-Theoretic Node Selection for Graph Self-Training", "link": "https://arxiv.org/abs/2410.09348", "description": "arXiv:2410.09348v1 Announce Type: cross \nAbstract: Graph self-training is a semi-supervised learning method that iteratively selects a set of unlabeled data to retrain the underlying graph neural network (GNN) model and improve its prediction performance. While selecting highly confident nodes has proven effective for self-training, this pseudo-labeling strategy ignores the combinatorial dependencies between nodes and suffers from a local view of the distribution. To overcome these issues, we propose BANGS, a novel framework that unifies the labeling strategy with conditional mutual information as the objective of node selection. Our approach -- grounded in game theory -- selects nodes in a combinatorial fashion and provides theoretical guarantees for robustness under noisy objective. More specifically, unlike traditional methods that rank and select nodes independently, BANGS considers nodes as a collective set in the self-training process. Our method demonstrates superior performance and robustness across various datasets, base models, and hyperparameter settings, outperforming existing techniques. The codebase is available on https://github.com/fangxin-wang/BANGS ."}, "https://arxiv.org/abs/2410.09965": {"title": "A Fully-dynamic Approximation Algorithm for Maximum Weight b-Matchings in Graphs", "link": "https://arxiv.org/abs/2410.09965", "description": "arXiv:2410.09965v1 Announce Type: cross \nAbstract: Matching nodes in a graph G = (V, E) is a well-studied algorithmic problem with many applications. The b-matching problem is a generalizati on that allows to match a node with up to b neighbors. This allows more flexible connectivity patterns whenever vertices may have multiple associations. The algorithm b-suitor [Khan et al., SISC2016] is able to compute a (1/2)-approximation of a maximum weight b-matching in O(|E|) time. Since real-world graphs often change over time, fast dynamic methods for b-matching optimization are desirable. In this work, we propose Dyn-b-suitor, a dynamic algorithm for the weighted b-matching problem. As a non-trivial extension to the dynamic Suitor algorithm for 1-matchings [Angriman et al., JEA 2022], our approach computes (1/2)-approximate b-matchings by identifying and updating affected vertices without static recomputation. Our proposed algorithm is fully-dynamic, i. e., it supports both edge insertions and deletions, and we prove that it computes the same solution as its static counterpart.\n  In extensive experiments on real-world benchmark graphs and generated instances, our dynamic algorithm yields significant savings compared to the sequential b-suitor, e. g., for batch updates with $10^3$ edges with an average acceleration factor of $10^3$. When comparing our sequential dynamic algorithm with the parallel (static) b-suitor on a 128-core machine, our dynamic algorithm is still $59$x to $10^4$ faster."}, "https://arxiv.org/abs/2410.10562": {"title": "Causal Modeling of Climate Activism on Reddit", "link": "https://arxiv.org/abs/2410.10562", "description": "arXiv:2410.10562v1 Announce Type: cross \nAbstract: Climate activism is crucial in stimulating collective societal and behavioral change towards sustainable practices through political pressure. Although multiple factors contribute to the participation in activism, their complex relationships and the scarcity of data on their interactions have restricted most prior research to studying them in isolation, thus preventing the development of a quantitative, causal understanding of why people approach activism. In this work, we develop a comprehensive causal model of how and why Reddit users engage with activist communities driving mass climate protests (mainly the 2019 Earth Strike, Fridays for Future, and Extinction Rebellion). Our framework, based on Stochastic Variational Inference applied to Bayesian Networks, learns the causal pathways over multiple time periods. Distinct from previous studies, our approach uses large-scale and fine-grained longitudinal data (2016 to 2022) to jointly model the roles of sociodemographic makeup, experience of extreme weather events, exposure to climate-related news, and social influence through online interactions. We find that among users interested in climate change, participation in online activist communities is indeed influenced by direct interactions with activists and largely by recent exposure to media coverage of climate protests. Among people aware of climate change, left-leaning people from lower socioeconomic backgrounds are particularly represented in online activist groups. Our findings offer empirical validation for theories of media influence and critical mass, and lay the foundations to inform interventions and future studies to foster public participation in collective action."}, "https://arxiv.org/abs/2312.04358": {"title": "Balance Correlations, Agentic Zeros, and Networks: The Structure of 192 Years of War and Peace", "link": "https://arxiv.org/abs/2312.04358", "description": "arXiv:2312.04358v2 Announce Type: replace \nAbstract: Social network extensions of Heider's balance theory have not always been consistent. Structural balance theory primarily focuses on graph partitioning, thereby assuming, homogeneity in balance-driven behavior of nodes. We present a general model and formal notation that permit testing such behavioral assumptions. Specifically, we formulate statements as a comparison of two conditional probabilities of a tie, $Ego\\stackrel{q}{\\text{-}}Alter$, first conditional on 2-paths $Ego\\, \\stackrel{r}{\\text{-}}\\,X\\,\\stackrel{s}{\\text{-}}\\,Alter$, and second conditional on all others, $\\neg (Ego\\,\\stackrel{r}{\\text{-}}\\,X\\,\\stackrel{s}{\\text{-}}\\,Alter)$. The key here is that $q$, $r$ and $s$ represent indices of relations in a set of mutually exclusive and exhaustive relations (their sum produces a complete graph). This relaxes the assumption of a signed graph dichotomy. Here we identify neutral as distinct from negative and positive ties. Descriptive statistics measuring the difference in conditional probabilities, or the prevalence for any stipulated balance configuration, are given by the point bi-serial correlations of relation $q$ with the count of $2$-paths (through relations $r$ and $s$). Two major advantages are: direct comparison, even if network sizes and densities differ, and evaluation of specific (un)balance behaviors. We apply this approach on a data set with friendly vs hostile relations between countries from 1816 to 2007. We find strong evidence for one of the four classic Heiderian balance theory predictions, and virtually no evidence in support of the unbalanced predictions. However, we do find stable and surprising evidence that the neutral ties are important in balancing the relations among nations. Results further suggest that prevalence of balance driven behavior varies over time, and that other triadic motivated behaviors prevail among countries in certain eras."}, "https://arxiv.org/abs/2312.11543": {"title": "Introduction to graph theory and basic algorithms", "link": "https://arxiv.org/abs/2312.11543", "description": "arXiv:2312.11543v2 Announce Type: replace \nAbstract: This book collects the lectures about graph theory and its applications which were given to students of mathematical departments of Moscow State University and Peking University. Graph theory is a very wide field with a lot of applications in almost every scientific area: in many branches of mathematics, computer science, physics, chemistry, biology and also in psychology, arts, philosophy and many others. Nowadays, graph theory becomes especially more important because of the rapid development of molecular biology, neural networks and AI fields. One of the aims of writing this book was to give students thorough knowledge about graphs to understand modern scientific fields more deeply. Here we tried to give classical and modern theorems and algorithms in more understandable and simple way. We spent many time to rewrite them and close the gaps in several simplest well-known proofs to provide more precise and accurate material for students."}, "https://arxiv.org/abs/2405.05177": {"title": "Network mutual information measures for graph similarity", "link": "https://arxiv.org/abs/2405.05177", "description": "arXiv:2405.05177v2 Announce Type: replace \nAbstract: A wide range of tasks in network analysis, such as clustering network populations or identifying anomalies in temporal graph streams, require a measure of the similarity between two graphs. To provide a meaningful data summary for downstream scientific analyses, the graph similarity measures used for these tasks must be principled, interpretable, and capable of distinguishing meaningful overlapping network structure from statistical noise at different scales of interest. Here we derive a family of graph mutual information measures that satisfy these criteria and are constructed using only fundamental information theoretic principles. Our measures capture the information shared among networks according to different encodings of their structural information, with our mesoscale mutual information measure allowing for network comparison under any specified network coarse-graining. We test our measures in a range of applications on real and synthetic network data, finding that they effectively highlight intuitive aspects of network similarity across scales in a variety of systems."}, "https://arxiv.org/abs/2309.00976": {"title": "Pure Message Passing Can Estimate Common Neighbor for Link Prediction", "link": "https://arxiv.org/abs/2309.00976", "description": "arXiv:2309.00976v4 Announce Type: replace-cross \nAbstract: Message Passing Neural Networks (MPNNs) have emerged as the {\\em de facto} standard in graph representation learning. However, when it comes to link prediction, they often struggle, surpassed by simple heuristics such as Common Neighbor (CN). This discrepancy stems from a fundamental limitation: while MPNNs excel in node-level representation, they stumble with encoding the joint structural features essential to link prediction, like CN. To bridge this gap, we posit that, by harnessing the orthogonality of input vectors, pure message-passing can indeed capture joint structural features. Specifically, we study the proficiency of MPNNs in approximating CN heuristics. Based on our findings, we introduce the Message Passing Link Predictor (MPLP), a novel link prediction model. MPLP taps into quasi-orthogonal vectors to estimate link-level structural features, all while preserving the node-level complexities. Moreover, our approach demonstrates that leveraging message-passing to capture structural features could offset MPNNs' expressiveness limitations at the expense of estimation variance. We conduct experiments on benchmark datasets from various domains, where our method consistently outperforms the baseline methods."}, "https://arxiv.org/abs/2311.09630": {"title": "Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach", "link": "https://arxiv.org/abs/2311.09630", "description": "arXiv:2311.09630v3 Announce Type: replace-cross \nAbstract: Susceptibility to misinformation describes the degree of belief in unverifiable claims, a latent aspect of individuals' mental processes that is not observable. Existing susceptibility studies heavily rely on self-reported beliefs, which can be subject to bias, expensive to collect, and challenging to scale for downstream applications. To address these limitations, in this work, we propose a computational approach to model users' latent susceptibility levels. As shown in previous research, susceptibility is influenced by various factors (e.g., demographic factors, political ideology), and directly influences people's reposting behavior on social media. To represent the underlying mental process, our susceptibility modeling incorporates these factors as inputs, guided by the supervision of people's sharing behavior. Using COVID-19 as a testbed domain, our experiments demonstrate a significant alignment between the susceptibility scores estimated by our computational modeling and human judgments, confirming the effectiveness of this latent modeling approach. Furthermore, we apply our model to annotate susceptibility scores on a large-scale dataset and analyze the relationships between susceptibility with various factors. Our analysis reveals that political leanings and psychological factors exhibit varying degrees of association with susceptibility to COVID-19 misinformation."}, "https://arxiv.org/abs/2311.16308": {"title": "Compression-based inference of network motif sets", "link": "https://arxiv.org/abs/2311.16308", "description": "arXiv:2311.16308v2 Announce Type: replace-cross \nAbstract: Physical and functional constraints on biological networks lead to complex topological patterns across multiple scales in their organization. A particular type of higher-order network feature that has received considerable interest is network motifs, defined as statistically regular subgraphs. These may implement fundamental logical and computational circuits and are referred as \"building blocks of complex networks\". Their well-defined structures and small sizes also enables the testing of their functions in synthetic and natural biological experiments. The statistical inference of network motifs is however fraught with difficulties, from defining and sampling the right null model to accounting for the large number of possible motifs and their potential correlations in statistical testing. Here we develop a framework for motif mining based on lossless network compression using subgraph contractions. The minimum description length principle allows us to select the most significant set of motifs as well as other prominent network features in terms of their combined compression of the network. The approach inherently accounts for multiple testing and correlations between subgraphs and does not rely on a priori specification of an appropriate null model. This provides an alternative definition of motif significance which guarantees more robust statistical inference. Our approach overcomes the common problems in classic testing-based motif analysis. We apply our methodology to perform comparative connectomics by evaluating the compressibility and the circuit motifs of a range of synaptic-resolution neural connectomes."}, "https://arxiv.org/abs/2410.10844": {"title": "Links between Entropy, Complexity, and the Technological Singularity", "link": "https://arxiv.org/abs/2410.10844", "description": "arXiv:2410.10844v1 Announce Type: new \nAbstract: Entropy always increases monotonically in a closed system but complexity increases at first and then decreases as equilibrium is approached. Commonsense information-related definitions for entropy and complexity demonstrate that complexity behaves like the time derivative of entropy, which is proposed here as a new definition for complexity. A 20-year-old study had attempted to quantify complexity (in arbitrary units) for the entire Universe in terms of 28 milestones, breaks in historical perspective, and had concluded that complexity will soon begin decreasing. That conclusion is now corroborated by other researchers. In addition, the exponential runaway technology trend advocated by supporters of the singularity hypothesis, which was in part based on the trend of the very 28 milestones mentioned above, would have anticipated five new such milestones by now, but none have been observed. The conclusions of the 20-year-old study remain valid: we are at the maximum of complexity and we should expect the next two milestones at around 2033 and 2078."}, "https://arxiv.org/abs/2410.10845": {"title": "A Quantitative Model Of Social Group Sizes From The Dynamics of Trust", "link": "https://arxiv.org/abs/2410.10845", "description": "arXiv:2410.10845v1 Announce Type: new \nAbstract: We present an argument (for a cross disciplinary audience) to explain the Dunbar scaling hierarchy for social groups. Our analysis is based on the Promise Theory of trust, and basic dimensional analysis. We derive a universal scaling relation, and pinpoint how groups form from the seeding of individuals by relative alignment, which continues until the costs associated with group contention outweigh the benefits in a detailed balance scenario. We identify an `energy' parameter for this balance that has the semantics of trust in a social setting. Subject to partial efficiency of maintaining aligned intentions, we can calculate a series of compatible rates that balance growth with entropy."}, "https://arxiv.org/abs/2410.10875": {"title": "SHyPar: A Spectral Coarsening Approach to Hypergraph Partitioning", "link": "https://arxiv.org/abs/2410.10875", "description": "arXiv:2410.10875v1 Announce Type: new \nAbstract: State-of-the-art hypergraph partitioners utilize a multilevel paradigm to construct progressively coarser hypergraphs across multiple layers, guiding cut refinements at each level of the hierarchy. Traditionally, these partitioners employ heuristic methods for coarsening and do not consider the structural features of hypergraphs. In this work, we introduce a multilevel spectral framework, SHyPar, for partitioning large-scale hypergraphs by leveraging hyperedge effective resistances and flow-based community detection techniques. Inspired by the latest theoretical spectral clustering frameworks, such as HyperEF and HyperSF, SHyPar aims to decompose large hypergraphs into multiple subgraphs with few inter-partition hyperedges (cut size). A key component of SHyPar is a flow-based local clustering scheme for hypergraph coarsening, which incorporates a max-flow-based algorithm to produce clusters with substantially improved conductance. Additionally, SHyPar utilizes an effective resistance-based rating function for merging nodes that are strongly connected (coupled). Compared with existing state-of-the-art hypergraph partitioning methods, our extensive experimental results on real-world VLSI designs demonstrate that SHyPar can more effectively partition hypergraphs, achieving state-of-the-art solution quality."}, "https://arxiv.org/abs/2410.10886": {"title": "Understanding U", "link": "https://arxiv.org/abs/2410.10886", "description": "arXiv:2410.10886v1 Announce Type: new \nAbstract: Racial segregation is a widespread social and physical phenomenon present in every city across the United States. Although prevalent nationwide, each city has a unique history of racial segregation, resulting in distinct \"shapes\" of segregation. We use persistent homology, a technique from applied algebraic topology, to investigate whether common patterns of racial segregation exist among U.S. cities. We explore two methods of constructing simplicial complexes that preserve geospatial data, applying them to White, Black, Asian, and Hispanic demographic data from the U.S. census for 112 U.S. cities. Using these methods, we cluster the cities based on their persistence to identify groups with similar segregation \"shapes\". Finally, we apply cluster analysis techniques to explore the characteristics of our clusters. This includes calculating the mean cluster statistics to gain insights into the demographics of each cluster and using the Adjusted Rand Index to compare our results with other clustering methods."}, "https://arxiv.org/abs/2410.10891": {"title": "The Scope 4 Emission: Neutralized Carbon Emissions", "link": "https://arxiv.org/abs/2410.10891", "description": "arXiv:2410.10891v1 Announce Type: new \nAbstract: Assessing carbon negative and carbon neutrality is critical for mitigating and adapting global climate change. Here we proposed a new framework to account for carbon-negative and carbon-neutral actions by introducing the definition of Carbon Negative (C0),Carbon Neutrality Stock (C1), Carbon Supply (C2) and carbon-neutral emissions or Scope 4 emissions, which refers to the avoided emission due to use of non-fossil energy or C1 products. For the first time, we calculated the global neutralized carbon emissions or Scope 4 emission by renewable electricity generation, and the results indicating the significant contributions by China, with total neutralized carbon emissions (2.15 Mt C/day ) much higher than the U.S. (0.85 Mt C/day)and EU27 & UK (1.25 Mt C/day) together. We show that China contributed to more than 36% of global neutralized CO2 emissions, and such contributions are still increasing. This new framework reflects remarkable contributions for China to the global climate mitigation through the development of carbon neutrality energy system."}, "https://arxiv.org/abs/2410.10904": {"title": "Multifractality of complex networks is also due to geometry", "link": "https://arxiv.org/abs/2410.10904", "description": "arXiv:2410.10904v1 Announce Type: new \nAbstract: Over the past three decades, describing the reality surrounding us using the language of complex networks has become very useful and therefore popular. One of the most important features, especially of real networks, is their complexity, which often manifests itself in a fractal or even multifractal structure. As a generalisation of fractal analysis, multifractal analysis of complex networks is a useful tool for the identification and quantitative description of the spatial hierarchy of both theoretical and numerical fractal patterns. Nowadays, there are many methods of multifractal analysis. However, all these methods take into account only the fact of connection between nodes (and eventually the weight of edges) and do not take into account the real positions (coordinates) of nodes in space. However, intuition suggests that the geometry of network nodes' position should have a significant impact on its true fractal structure. Many networks identified in nature (e.g. air connection networks, energy networks, social networks, mountain ridge networks, networks of neurones in the brain, street networks) have their own often unique and characteristic geometry, which is not taken into account in the identification process of multifractality in commonly used methods. In this paper, we propose a multifractal network analysis method that takes into account both connections between nodes and the location coordinates of nodes (network geometry). We show the results for different geometrical variants of the same network and reveal that this method, contrary to the commonly used method, is sensitive to changes in network geometry. We carry out tests for synthetic as well as for real-world networks."}, "https://arxiv.org/abs/2410.10921": {"title": "Cooperation in Public Goods Games: Leveraging Other-Regarding Reinforcement Learning on Hypergraphs", "link": "https://arxiv.org/abs/2410.10921", "description": "arXiv:2410.10921v1 Announce Type: new \nAbstract: Cooperation as a self-organized collective behavior plays a significant role in the evolution of ecosystems and human society. Reinforcement learning (RL) offers a new perspective, distinct from imitation learning in evolutionary games, for exploring the mechanisms underlying its emergence. However, most existing studies with the public good game (PGG) employ a self-regarding setup or are on pairwise interaction networks. Players in the real world, however, optimize their policies based not only on their histories but also on the histories of their co-players, and the game is played in a group manner. In the work, we investigate the evolution of cooperation in the PGG under the other-regarding reinforcement learning evolutionary game (OR-RLEG) on hypergraph by combining the Q-learning algorithm and evolutionary game framework, where other players' action history is incorporated and the game is played on hypergraphs. Our results show that as the synergy factor increases, the parameter interval is divided into three distinct regions, the absence of cooperation (AC), medium cooperation (MC), and high cooperation (HC), accompanied by two abrupt transitions in the cooperation level near two transition points, respectively. Interestingly, we identify regular and anti-coordinated chessboard structures in the spatial pattern that positively contribute to the first cooperation transition but adversely affect the second. Furthermore, we provide a theoretical treatment for the first transition with an approximated first transition point and reveal that players with a long-sighted perspective and low exploration rate are more likely to reciprocate kindness with each other, thus facilitating the emergence of cooperation. Our findings contribute to understanding the evolution of human cooperation, where other-regarding information and group interactions are commonplace."}, "https://arxiv.org/abs/2410.11273": {"title": "GCLS$^2$: Towards Efficient Community Detection using Graph Contrastive Learning with Structure Semantics", "link": "https://arxiv.org/abs/2410.11273", "description": "arXiv:2410.11273v1 Announce Type: new \nAbstract: Due to powerful ability to learn representations from unlabeled graphs, graph contrastive learning (GCL) has shown excellent performance in community detection tasks. Existing GCL-based methods on the community detection usually focused on learning attribute representations of individual nodes, which, however, ignores structure semantics of communities (e.g., nodes in the same community should be close to each other). Therefore, in this paper, we will consider the semantics of community structures for the community detection, and propose an effective framework of graph contrastive learning under structure semantics (GCLS$^2$) for detecting communities. To seamlessly integrate interior dense and exterior sparse characteristics of communities with our contrastive learning strategy, we employ classic community structures to extract high-level structural views and design a structure semantic expression module to augment the original structural feature representation. Moreover, we formulate the structure contrastive loss to optimize the feature representation of nodes, which can better capture the topology of communities. Extensive experiments have been conducted on various real-world graph datasets and confirmed that GCLS$^2$ outperforms eight state-of-the-art methods, in terms of the accuracy and modularity of the detected communities."}, "https://arxiv.org/abs/2410.11352": {"title": "Modelling advection on distance-weighted directed networks", "link": "https://arxiv.org/abs/2410.11352", "description": "arXiv:2410.11352v1 Announce Type: new \nAbstract: In this paper we propose a model for describing advection dynamics on distance-weighted directed graphs. To this end we establish a set of key properties, or axioms, that a discrete advection operator should satisfy, and prove that there exists an essentially unique operator satisfying all such properties. Both infinite and finite networks are considered, as well as possible variants and extensions. We illustrate the proposed model through examples, both analytical and numerical, and we describe an application to the simulation of a traffic network."}, "https://arxiv.org/abs/2410.11493": {"title": "Towards Fair Graph Representation Learning in Social Networks", "link": "https://arxiv.org/abs/2410.11493", "description": "arXiv:2410.11493v1 Announce Type: new \nAbstract: With the widespread use of Graph Neural Networks (GNNs) for representation learning from network data, the fairness of GNN models has raised great attention lately. Fair GNNs aim to ensure that node representations can be accurately classified, but not easily associated with a specific group. Existing advanced approaches essentially enhance the generalisation of node representation in combination with data augmentation strategy, and do not directly impose constraints on the fairness of GNNs. In this work, we identify that a fundamental reason for the unfairness of GNNs in social network learning is the phenomenon of social homophily, i.e., users in the same group are more inclined to congregate. The message-passing mechanism of GNNs can cause users in the same group to have similar representations due to social homophily, leading model predictions to establish spurious correlations with sensitive attributes. Inspired by this reason, we propose a method called Equity-Aware GNN (EAGNN) towards fair graph representation learning. Specifically, to ensure that model predictions are independent of sensitive attributes while maintaining prediction performance, we introduce constraints for fair representation learning based on three principles: sufficiency, independence, and separation. We theoretically demonstrate that our EAGNN method can effectively achieve group fairness. Extensive experiments on three datasets with varying levels of social homophily illustrate that our EAGNN method achieves the state-of-the-art performance across two fairness metrics and offers competitive effectiveness."}, "https://arxiv.org/abs/2410.11512": {"title": "Beyond the Median Voter: How Affective Polarization Shapes Party Polarization in Multidimensional Ideological Spaces", "link": "https://arxiv.org/abs/2410.11512", "description": "arXiv:2410.11512v1 Announce Type: new \nAbstract: Contrary to what is expected by the median voter theorem, party polarization has emerged as a global political phenomenon, particularly pronounced in the United States. Amid concerns that this phenomenon threatens democracies, political science has sought explanations for its causes. One explanation is that affective (or social) polarization simplifies the voters' political mindset, reducing the dimensionality of identity space and compressing the diversity of interests and information in the political system. To theoretically investigate such findings, we studied a mathematical model of bipartisan elections in which voters and parties share a multidimensional ideological space. We established equations that determine the critical value of the expected number of voters for a party, which delineates the point at which it is more strategically advantageous for parties to position themselves at the center or outside of it. This critical value fundamentally depends on the dimension of the ideological space, which we identify as being related to affective polarization and which considerably mitigates party polarization as the dimension increases, even with preexisting population bimodality, the latter being related to issue polarization. This result aligns with the thesis that affective polarization, more than issue polarization, drives party polarization, offering a framework for how this phenomenon occurs."}, "https://arxiv.org/abs/2410.11629": {"title": "Minimizing emissions through ride pooling incentives", "link": "https://arxiv.org/abs/2410.11629", "description": "arXiv:2410.11629v1 Announce Type: new \nAbstract: In face of the climate emergency and growing challenges ranging from pollution to traffic jams, ride pooling has been floated as a potential solution for less congested, low-carbon and more space-efficient urban transport. However, it is unclear which system configurations enable an economically viable case for shared pooled mobility. To gain a better understanding of mechanisms behind this, we develop a simplified model to analyze the switching potential and CO2 emissions of ride pooling systems for a given number of transport users, street network topology and other system parameter values with different hypothetical switch rate functions. We find that CO2 emissions of local transport can be reduced by 39 to 45 % depending on the assumed switch rate function with other system parameters only having a secondary effect of a few percentage points. We call for empirically gauged analyses that translate our model into scenarios for metropolitan low-carbon and smart para-transit."}, "https://arxiv.org/abs/2410.11635": {"title": "Evidence of equilibrium dynamics in human social networks evolving in time", "link": "https://arxiv.org/abs/2410.11635", "description": "arXiv:2410.11635v1 Announce Type: new \nAbstract: The dynamics of personal relationships remain largely unexplored due to the inherent difficulties of the longitudinal data collection process. In this paper, we analyse a dataset tracking the temporal evolution of a network of personal relationships among 900 people over the course of four years. We search for evidence that the network is in equilibrium, meaning that all macroscopic properties remain constant, fluctuating around stable values, while the internal microscopic dynamics are active. We find that the probabilities governing the network dynamics are stationary over time and that the degree distributions, as well as edge and triangle abundances match the theoretical equilibrium distributions expected under these dynamics. Furthermore, we verify that the system satisfies the detailed balance condition, with only minor point deviations, confirming that it is indeed in equilibrium. Remarkably, this equilibrium persists despite a high turnover in network composition, suggesting that it is an inherent characteristic of human social interactions rather than a trait of the individuals themselves. We argue that this equilibrium may be a general feature of human social networks arising from the competition between different dynamical mechanisms and also from the cognitive and material resources management of individuals. From a practical perspective, the fact that networks are in equilibrium could simplify data collection processes, validate the use of cross-sectional data-based methods like Exponential Random Graph Models, and inform the design of interventions. Our findings advance the understanding of collective human behaviour predictability and our ability to describe it using simple mathematical models."}, "https://arxiv.org/abs/2410.11100": {"title": "Characterizing the MrDeepFakes Sexual Deepfake Marketplace", "link": "https://arxiv.org/abs/2410.11100", "description": "arXiv:2410.11100v1 Announce Type: cross \nAbstract: The prevalence of sexual deepfake material has exploded over the past several years. Attackers create and utilize deepfakes for many reasons: to seek sexual gratification, to harass and humiliate targets, or to exert power over an intimate partner. In tandem with this growth, several markets have emerged to support the buying and selling of sexual deepfake material. In this paper, we systematically characterize the most prominent and mainstream marketplace, MrDeepFakes. We analyze the marketplace economics, the targets of created media, and user discussions of how to create deepfakes, which we use to understand the current state-of-the-art in deepfake creation. Our work uncovers little enforcement of posted rules (e.g., limiting targeting to well-established celebrities), previously undocumented attacker motivations, and unexplored attacker tactics for acquiring resources to create sexual deepfakes."}, "https://arxiv.org/abs/2410.11698": {"title": "AI Rules? Characterizing Reddit Community Policies Towards AI-Generated Content", "link": "https://arxiv.org/abs/2410.11698", "description": "arXiv:2410.11698v1 Announce Type: cross \nAbstract: How are Reddit communities responding to AI-generated content? We explored this question through a large-scale analysis of subreddit community rules and their change over time. We collected the metadata and community rules for over 300,000 public subreddits and measured the prevalence of rules governing AI. We labeled subreddits and AI rules according to existing taxonomies from the HCI literature and a new taxonomy we developed specific to AI rules. While rules about AI are still relatively uncommon, the number of subreddits with these rules almost doubled over the course of a year. AI rules are also more common in larger subreddits and communities focused on art or celebrity topics, and less common in those focused on social support. These rules often focus on AI images and evoke, as justification, concerns about quality and authenticity. Overall, our findings illustrate the emergence of varied concerns about AI, in different community contexts."}, "https://arxiv.org/abs/2310.04331": {"title": "Framing the Fray: Evaluating Conflict and Game Frames in Indian Election News Coverage", "link": "https://arxiv.org/abs/2310.04331", "description": "arXiv:2310.04331v2 Announce Type: replace \nAbstract: Journalists often use conflict frames when reporting on election news stories. A conflict frame depicts events and issues as contentious or adversarial, often highlighting confrontations between opposing parties or groups. In this work, we examine the use of conflict frames in online news articles published by seven major news outlets in the 2014 and 2019 Indian general elections. We find that the use of conflict frames in Indian online media is not linked to the media outlets' ideological biases but is associated with TV-based (rather than print-based) media. Further, the majority of news outlets do not exhibit ideological biases in portraying parties as aggressors or targets in articles with conflict frames. Finally, we find while these outlets consistently report on mentions of the opposition party, they underreport more substantive electoral issues such as farmers' issues and infrastructure."}, "https://arxiv.org/abs/2401.02130": {"title": "Spectral-Based Graph Neural Networks for Complementary Item Recommendation", "link": "https://arxiv.org/abs/2401.02130", "description": "arXiv:2401.02130v4 Announce Type: replace-cross \nAbstract: Modeling complementary relationships greatly helps recommender systems to accurately and promptly recommend the subsequent items when one item is purchased. Unlike traditional similar relationships, items with complementary relationships may be purchased successively (such as iPhone and Airpods Pro), and they not only share relevance but also exhibit dissimilarity. Since the two attributes are opposites, modeling complementary relationships is challenging. Previous attempts to exploit these relationships have either ignored or oversimplified the dissimilarity attribute, resulting in ineffective modeling and an inability to balance the two attributes. Since Graph Neural Networks (GNNs) can capture the relevance and dissimilarity between nodes in the spectral domain, we can leverage spectral-based GNNs to effectively understand and model complementary relationships. In this study, we present a novel approach called Spectral-based Complementary Graph Neural Networks (SComGNN) that utilizes the spectral properties of complementary item graphs. We make the first observation that complementary relationships consist of low-frequency and mid-frequency components, corresponding to the relevance and dissimilarity attributes, respectively. Based on this spectral observation, we design spectral graph convolutional networks with low-pass and mid-pass filters to capture the low-frequency and mid-frequency components. Additionally, we propose a two-stage attention mechanism to adaptively integrate and balance the two attributes. Experimental results on four e-commerce datasets demonstrate the effectiveness of our model, with SComGNN significantly outperforming existing baseline models."}, "https://arxiv.org/abs/2410.10464": {"title": "Information propagation dynamics in Deep Graph Networks", "link": "https://arxiv.org/abs/2410.10464", "description": "arXiv:2410.10464v2 Announce Type: replace-cross \nAbstract: Graphs are a highly expressive abstraction for modeling entities and their relations, such as molecular structures, social networks, and traffic networks. Deep Graph Networks (DGNs) have emerged as a family of deep learning models that can effectively process and learn such structured information. However, learning effective information propagation patterns within DGNs remains a critical challenge that heavily influences the model capabilities, both in the static domain and in the temporal domain (where features and/or topology evolve). Given this challenge, this thesis investigates the dynamics of information propagation within DGNs for static and dynamic graphs, focusing on their design as dynamical systems. Throughout this work, we provide theoretical and empirical evidence to demonstrate the effectiveness of our proposed architectures in propagating and preserving long-term dependencies between nodes, and in learning complex spatio-temporal patterns from irregular and sparsely sampled dynamic graphs. In summary, this thesis provides a comprehensive exploration of the intersection between graphs, deep learning, and dynamical systems, offering insights and advancements for the field of graph representation learning and paving the way for more effective and versatile graph-based learning models."}, "https://arxiv.org/abs/2410.11972": {"title": "Heterogeneous Graph Generation: A Hierarchical Approach using Node Feature Pooling", "link": "https://arxiv.org/abs/2410.11972", "description": "arXiv:2410.11972v1 Announce Type: new \nAbstract: Heterogeneous graphs are present in various domains, such as social networks, recommendation systems, and biological networks. Unlike homogeneous graphs, heterogeneous graphs consist of multiple types of nodes and edges, each representing different entities and relationships. Generating realistic heterogeneous graphs that capture the complex interactions among diverse entities is a difficult task due to several reasons. The generator has to model both the node type distribution along with the feature distribution for each node type. In this paper, we look into solving challenges in heterogeneous graph generation, by employing a two phase hierarchical structure, wherein the first phase creates a skeleton graph with node types using a prior diffusion based model and in the second phase, we use an encoder and a sampler structure as generator to assign node type specific features to the nodes. A discriminator is used to guide training of the generator and feature vectors are sampled from a node feature pool. We conduct extensive experiments with subsets of IMDB and DBLP datasets to show the effectiveness of our method and also the need for various architecture components."}, "https://arxiv.org/abs/2410.12061": {"title": "CrediRAG: Network-Augmented Credibility-Based Retrieval for Misinformation Detection in Reddit", "link": "https://arxiv.org/abs/2410.12061", "description": "arXiv:2410.12061v1 Announce Type: new \nAbstract: Fake news threatens democracy and exacerbates the polarization and divisions in society; therefore, accurately detecting online misinformation is the foundation of addressing this issue. We present CrediRAG, the first fake news detection model that combines language models with access to a rich external political knowledge base with a dense social network to detect fake news across social media at scale. CrediRAG uses a news retriever to initially assign a misinformation score to each post based on the source credibility of similar news articles to the post title content. CrediRAG then improves the initial retrieval estimations through a novel weighted post-to-post network connected based on shared commenters and weighted by the average stance of all shared commenters across every pair of posts. We achieve 11% increase in the F1-score in detecting misinformative posts over state-of-the-art methods. Extensive experiments conducted on curated real-world Reddit data of over 200,000 posts demonstrate the superior performance of CrediRAG on existing baselines. Thus, our approach offers a more accurate and scalable solution to combat the spread of fake news across social media platforms."}, "https://arxiv.org/abs/2410.12676": {"title": "Identity Emergence in the Context of Vaccine Criticism in France", "link": "https://arxiv.org/abs/2410.12676", "description": "arXiv:2410.12676v1 Announce Type: new \nAbstract: This study investigates the emergence of collective identity among individuals critical of vaccination policies in France during the COVID-19 pandemic. As concerns grew over mandated health measures, a loose collective formed on Twitter to assert autonomy over vaccination decisions. Using analyses of pronoun usage, outgroup labeling, and tweet similarity, we examine how this identity emerged. A turning point occurred following President Macron's announcement of mandatory vaccination for health workers and the health pass, sparking substantial changes in linguistic patterns. We observed a shift from first-person singular (I) to first-person plural (we) pronouns, alongside an increased focus on vaccinated individuals as a central outgroup, in addition to authority figures. This shift in language patterns was further reflected in the behavior of new users. An analysis of incoming users revealed that a core group of frequent posters played a crucial role in fostering cohesion and shaping norms. New users who joined during the week of Macron's announcement and continued posting afterward showed an increased similarity with the language of the core group, contributing to the crystallization of the emerging collective identity."}, "https://arxiv.org/abs/2410.12688": {"title": "A spatial hypergraph model where epidemic spread demonstrates clear higher-order effects", "link": "https://arxiv.org/abs/2410.12688", "description": "arXiv:2410.12688v1 Announce Type: new \nAbstract: We demonstrate a spatial hypergraph model that allows us to vary the amount of higher-order structure in the generated hypergraph. Specifically, we can vary from a model that is a pure pairwise graph into a model that is almost a pure hypergraph. We use this spatial hypergraph model to study higher-order effects in epidemic spread. We use a susceptible-infected-recovered-susceptible (SIRS) epidemic model designed to mimic the spread of an airborne pathogen. We study three types of airborne effects that emulate airborne dilution effects. For the scenario of linear dilution, which roughly correspond to constant ventilation per person as required in many building codes, we see essentially no impact from introducing small hyperedges up to size 15 whereas we do see effects when the hyperedge set is dominated by large hyperedges. Specifically, we track the mean infections after the SIRS epidemic has run for awhile so it is in a \"steady state\" and find the mean is higher in the large hyperedge regime wheras it is unchanged from pairwise to small hyperedge regime."}, "https://arxiv.org/abs/2410.11853": {"title": "GeoLife+: Large-Scale Simulated Trajectory Datasets Calibrated to the GeoLife Dataset", "link": "https://arxiv.org/abs/2410.11853", "description": "arXiv:2410.11853v1 Announce Type: cross \nAbstract: Analyzing individual human trajectory data helps our understanding of human mobility and finds many commercial and academic applications. There are two main approaches to accessing trajectory data for research: one involves using real-world datasets like GeoLife, while the other employs simulations to synthesize data. Real-world data provides insights from real human activities, but such data is generally sparse due to voluntary participation. Conversely, simulated data can be more comprehensive but may capture unrealistic human behavior. In this Data and Resource paper, we combine the benefit of both by leveraging the statistical features of real-world data and the comprehensiveness of simulated data. Specifically, we extract features from the real-world GeoLife dataset such as the average number of individual daily trips, average radius of gyration, and maximum and minimum trip distances. We calibrate the Pattern of Life Simulation, a realistic simulation of human mobility, to reproduce these features. Therefore, we use a genetic algorithm to calibrate the parameters of the simulation to mimic the GeoLife features. For this calibration, we simulated numerous random simulation settings, measured the similarity of generated trajectories to GeoLife, and iteratively (over many generations) combined parameter settings of trajectory datasets most similar to GeoLife. Using the calibrated simulation, we simulate large trajectory datasets that we call GeoLife+, where + denotes the Kleene Plus, indicating unlimited replication with at least one occurrence. We provide simulated GeoLife+ data with 182, 1k, and 5k over 5 years, 10k, and 50k over a year and 100k users over 6 months of simulation lifetime."}, "https://arxiv.org/abs/2410.11890": {"title": "Online Digital Investigative Journalism using SociaLens", "link": "https://arxiv.org/abs/2410.11890", "description": "arXiv:2410.11890v1 Announce Type: cross \nAbstract: Media companies witnessed a significant transformation with the rise of the internet, bigdata, machine learning (ML) and AI. Recent emergence of large language models (LLM) have added another aspect to this transformation. Researchers believe that with the help of these technologies, investigative digital journalism will enter a new era. Using a smart set of data gathering and analysis tools, journalists will be able to create data driven contents and insights in unprecedented ways. In this paper, we introduce a versatile and autonomous investigative journalism tool, called {\\em SociaLens}, for identifying and extracting query specific data from online sources, responding to probing queries and drawing conclusions entailed by large volumes of data using ML analytics fully autonomously. We envision its use in investigative journalism, law enforcement and social policy planning. The proposed system capitalizes on the integration of ML technology with LLMs and advanced bigdata search techniques. We illustrate the functionality of SociaLens using a focused case study on rape incidents in a developing country and demonstrate that journalists can gain nuanced insights without requiring coding expertise they might lack. SociaLens is designed as a ChatBot that is capable of contextual conversation, find and collect data relevant to queries, initiate ML tasks to respond to queries, generate textual and visual reports, all fully autonomously within the ChatBot environment."}, "https://arxiv.org/abs/2410.12045": {"title": "Differential Privacy on Trust Graphs", "link": "https://arxiv.org/abs/2410.12045", "description": "arXiv:2410.12045v1 Announce Type: cross \nAbstract: We study differential privacy (DP) in a multi-party setting where each party only trusts a (known) subset of the other parties with its data. Specifically, given a trust graph where vertices correspond to parties and neighbors are mutually trusting, we give a DP algorithm for aggregation with a much better privacy-utility trade-off than in the well-studied local model of DP (where each party trusts no other party). We further study a robust variant where each party trusts all but an unknown subset of at most $t$ of its neighbors (where $t$ is a given parameter), and give an algorithm for this setting. We complement our algorithms with lower bounds, and discuss implications of our work to other tasks in private learning and analytics."}, "https://arxiv.org/abs/2410.12126": {"title": "Parametric Graph Representations in the Era of Foundation Models: A Survey and Position", "link": "https://arxiv.org/abs/2410.12126", "description": "arXiv:2410.12126v1 Announce Type: cross \nAbstract: Graphs have been widely used in the past decades of big data and AI to model comprehensive relational data. When analyzing a graph's statistical properties, graph laws serve as essential tools for parameterizing its structure. Identifying meaningful graph laws can significantly enhance the effectiveness of various applications, such as graph generation and link prediction. Facing the large-scale foundation model developments nowadays, the study of graph laws reveals new research potential, e.g., providing multi-modal information for graph neural representation learning and breaking the domain inconsistency of different graph data. In this survey, we first review the previous study of graph laws from multiple perspectives, i.e., macroscope and microscope of graphs, low-order and high-order graphs, static and dynamic graphs, different observation spaces, and newly proposed graph parameters. After we review various real-world applications benefiting from the guidance of graph laws, we conclude the paper with current challenges and future research directions."}, "https://arxiv.org/abs/2410.12682": {"title": "Phase transitions for polyadic epidemic and voter models with multiscale groups", "link": "https://arxiv.org/abs/2410.12682", "description": "arXiv:2410.12682v1 Announce Type: cross \nAbstract: Polyadic (or higher-order) interactions can significantly impact the dynamics of interacting particle systems. However, so far, the group size interactions have been relatively small. In this work, we examine the influence of multiscale polyadic group interactions, where some groups are small and others very large. We consider two paradigmatic examples, an SIS-epidemic and the adaptive voter model. On the level of the mean field, we specifically discuss the impact of the multiscale polyadic interactions on equilibrium dynamics and phase transitions. For the SIS-epidemic model, we find a region of bistability that protects the disease-free state over a wide range beyond the classical epidemic threshold from a significant outbreak. For the adaptive voter model, we show that multiscale polyadic interactions can stabilize the network or increase the convergence rate to an unbiased equilibrium."}, "https://arxiv.org/abs/2311.12646": {"title": "Online landmark replacement for out-of-sample dimensionality reduction methods", "link": "https://arxiv.org/abs/2311.12646", "description": "arXiv:2311.12646v2 Announce Type: replace \nAbstract: A strategy to assist visualization and analysis of large and complex data sets is dimensionality reduction, with which one maps each data point into a low-dimensional manifold. However, various dimensionality reduction techniques are computationally infeasible for large data. Out-of-sample techniques aim to resolve this difficulty; they only apply the dimensionality reduction technique on a small portion of data, referred to as landmarks, and determine the embedding coordinates of the other points using landmarks as references. Out-of-sample techniques have been applied to online settings, or when data arrive as time series. However, existing online out-of-sample techniques use either all the previous data points as landmarks or the fixed set of landmarks and therefore are potentially not good at capturing the geometry of the entire data set when the time series is non-stationary. To address this problem, we propose an online landmark replacement algorithm for out-of-sample techniques using geometric graphs and the minimal dominating set on them. We mathematically analyze some properties of the proposed algorithm, particularly focusing on the case of landmark multidimensional scaling as the out-of-sample technique, and test its performance on synthetic and empirical time series data."}, "https://arxiv.org/abs/2312.15198": {"title": "Do LLM Agents Exhibit Social Behavior?", "link": "https://arxiv.org/abs/2312.15198", "description": "arXiv:2312.15198v3 Announce Type: replace-cross \nAbstract: As LLMs increasingly take on roles in human-AI interactions and autonomous AI systems, understanding their social behavior becomes important for informed use and continuous improvement. However, their behaviors in social interactions with humans and other agents, as well as the mechanisms shaping their responses, remain underexplored. To address this gap, we introduce a novel probabilistic framework, State-Understanding-Value-Action (SUVA), to systematically analyze LLM responses in social contexts based on their textual outputs (i.e., utterances). Using canonical behavioral economics games and social preference concepts relatable to LLM users, SUVA assesses LLMs' social behavior through both their final decisions and the response generation processes leading to those decisions. Our analysis of eight LLMs -- including two GPT, four LLaMA, and two Mistral models -- suggests that most models do not generate decisions aligned solely with self-interest; instead, they often produce responses that reflect social welfare considerations and display patterns consistent with direct and indirect reciprocity. Additionally, higher-capacity models more frequently display group identity effects. The SUVA framework also provides explainable tools -- including tree-based visualizations and probabilistic dependency analysis -- to elucidate how factors in LLMs' utterance-based reasoning influence their decisions. We demonstrate that utterance-based reasoning reliably predicts LLMs' final actions; references to altruism, fairness, and cooperation in the reasoning increase the likelihood of prosocial actions, while mentions of self-interest and competition reduce them. Overall, our framework enables practitioners to assess LLMs for applications involving social interactions, and provides researchers with a structured method to interpret how LLM behavior arises from utterance-based reasoning."}, "https://arxiv.org/abs/2410.13026": {"title": "Design and Feasibility of a Community Motorcycle Ambulance System in the Philippines", "link": "https://arxiv.org/abs/2410.13026", "description": "arXiv:2410.13026v1 Announce Type: new \nAbstract: This study investigates the potential for motorcycle ambulance (motorlance) deployment in Metro Manila and Iloilo City to improve emergency medical care in high-traffic, underserved regions of the Philippines. VSee, a humanitarian technology company, has organized numerous free clinics in the Philippines and identified a critical need for improved emergency services. Motorlances offer a fast, affordable alternative to traditional ambulances, particularly in congested urban settings and remote rural locations. Pilot programs in Malawi, Thailand, and Iran have demonstrated significant improvements in response times and cost-efficiency with motorlance systems. This study presents a framework for motorlance operation and identifies three potential pilot locations: Mandaluyong, Smokey Mountain, and Iloilo City. Site visits, driver interviews, and user surveys indicate public trust in the motorlance concept and positive reception to potential motorlance deployment. Cost analysis verifies the financial feasibility of motorlance systems. Future work will focus on implementing a physical pilot in Mandaluyong, with the aim of expanding service to similar regions contingent on the Mandaluyong pilot's success."}, "https://arxiv.org/abs/2410.13095": {"title": "Future of Algorithmic Organization: Large-Scale Analysis of Decentralized Autonomous Organizations (DAOs)", "link": "https://arxiv.org/abs/2410.13095", "description": "arXiv:2410.13095v1 Announce Type: new \nAbstract: Decentralized Autonomous Organizations (DAOs) resemble early online communities, particularly those centered around open-source projects, and present a potential empirical framework for complex social-computing systems by encoding governance rules within \"smart contracts\" on the blockchain. A key function of a DAO is collective decision-making, typically carried out through a series of proposals where members vote on organizational events using governance tokens, signifying relative influence within the DAO. In just a few years, the deployment of DAOs surged with a total treasury of $24.5 billion and 11.1M governance token holders collectively managing decisions across over 13,000 DAOs as of 2024. In this study, we examine the operational dynamics of 100 DAOs, like pleasrdao, lexdao, lootdao, optimism collective, uniswap, etc. With large-scale empirical analysis of a diverse set of DAO categories and smart contracts and by leveraging on-chain (e.g., voting results) and off-chain data, we examine factors such as voting power, participation, and DAO characteristics dictating the level of decentralization, thus, the efficiency of management structures. As such, our study highlights that increased grassroots participation correlates with higher decentralization in a DAO, and lower variance in voting power within a DAO correlates with a higher level of decentralization, as consistently measured by Gini metrics. These insights closely align with key topics in political science, such as the allocation of power in decision-making and the effects of various governance models. We conclude by discussing the implications for researchers, and practitioners, emphasizing how these factors can inform the design of democratic governance systems in emerging applications that require active engagement from stakeholders in decision-making."}, "https://arxiv.org/abs/2410.13105": {"title": "AgileRate: Bringing Adaptivity and Robustness to DeFi Lending Markets", "link": "https://arxiv.org/abs/2410.13105", "description": "arXiv:2410.13105v1 Announce Type: new \nAbstract: Decentralized Finance (DeFi) has revolutionized lending by replacing intermediaries with algorithm-driven liquidity pools. However, existing platforms like Aave and Compound rely on static interest rate curves and collateral requirements that struggle to adapt to rapid market changes, leading to inefficiencies in utilization and increased risks of liquidations. In this work, we propose a dynamic model of the lending market based on evolving demand and supply curves, alongside an adaptive interest rate controller that responds in real-time to shifting market conditions. Using a Recursive Least Squares algorithm, our controller estimates tracks the external market and achieves stable utilization, while also minimizing risk. We provide theoretical guarantees on the interest rate convergence and utilization stability of our algorithm. We establish bounds on the system's vulnerability to adversarial manipulation compared to static curves, while quantifying the trade-off between adaptivity and adversarial robustness. Our dynamic curve demand/supply model demonstrates a low best-fit error on Aave data, while our interest rate controller significantly outperforms static curve protocols in maintaining optimal utilization and minimizing liquidations."}, "https://arxiv.org/abs/2410.13266": {"title": "Continuous agent-based modeling of adult-child pairs based on a pseudo-energy: Relevance for public safety and egress efficiency", "link": "https://arxiv.org/abs/2410.13266", "description": "arXiv:2410.13266v1 Announce Type: new \nAbstract: Pushes, falls, stampedes, and crushes are safety hazards that emerge from the collective motion of crowds, but might be avoided by better design and guidance. While pedestrian dynamics are now getting better understood on the whole, complex heterogeneous flows involvinge.g. adult-child pairs, though widely found at e.g. crowded Chinese training schools, still defy the current understanding and capabilities of crowd simulation models. We substantially extend a recent agent-based model in which each agent's choice of motion results from the minimization ofa sum of intuitive contributions, in order to integrate adult-child pairs. This is achieved by adding a suitably defined pairing potential. The resulting model captures the relative positions of pair members in a quantitative fashion, as confirmed by small-scale controlled experiments, and alsosucceeds in describing collision avoidance between pairs. The model is used to simulate mixed adult-child flows at a T-junction and test the sensitivity to the design and pairing strategies. Simulation shows that making the post-confluence corridor wide enough is critical to avoidfriction in the flow, and that tight hand-holding is advisable for safer evacuations (whereas more loosely bound pairs get split at high density) and, more marginally, more efficient egresses in normal conditions."}, "https://arxiv.org/abs/2410.13378": {"title": "A modified Hegselmann-Krause model for interacting voters and political parties", "link": "https://arxiv.org/abs/2410.13378", "description": "arXiv:2410.13378v1 Announce Type: new \nAbstract: The Hegselmann-Krause model is a prototypical model for opinion dynamics. It models the stochastic time evolution of an agent's or voter's opinion in response to the opinion of other like-minded agents. The Hegselmann-Krause model only considers the opinions of voters; we extend it here by incorporating the dynamics of political parties which influence and are influenced by the voters. We show in numerical simulations for $1$- and $2$-dimensional opinion spaces that, as for the original Hegselmann-Krause model, the modified model exhibits opinion cluster formation as well as a phase transition from disagreement to consensus. We provide an analytical sufficient condition for the formation of unanimous consensus in which voters and parties collapse to the same point in opinion space in the deterministic case. Using mean-field theory, we further derive an approximation for the critical noise strength delineating consensus from non-consensus in the stochastically driven modified Hegselmann-Krause model. We compare our analytical findings with simulations of the modified Hegselmann-Krause model."}, "https://arxiv.org/abs/2410.13492": {"title": "Recovery of contour nodes in interdependent directed networks", "link": "https://arxiv.org/abs/2410.13492", "description": "arXiv:2410.13492v1 Announce Type: new \nAbstract: Interdependent directed networks are essential in understanding the dynamics of real-world systems such as power grids and communication networks. A minor malfunction in such systems can propagate and lead to catastrophic consequences in a process known as cascading failures, which highlights the need for effective recovery strategies that help to mitigate or avoid eventual damages. In this work, we analyze the impact of a recovery strategy on cascading failures in two interdependent directed networks, where a fraction $q$ of nodes have single dependencies. After a random removal of a fraction $1 - p$ of nodes, we repair nodes in the contour of each giant strongly connected component, with probability $\\gamma$. We find that the sustained application of the strategy leads to an abrupt transition between full collapse and complete recovery. Phase diagrams in the $(p, \\gamma)$ plane reveal three regions: one where the system collapses despite intervention, another where the strategy ensures recovery, and a third where no intervention is necessary to avoid collapse. For obtaining these results, we develop an analytical framework using node percolation and generating functions that aligns well with simulation results."}, "https://arxiv.org/abs/2410.13512": {"title": "A Proposal for Uncovering Hidden Social Bots via Genetic Similarity", "link": "https://arxiv.org/abs/2410.13512", "description": "arXiv:2410.13512v1 Announce Type: new \nAbstract: Social media platforms face an ongoing challenge in combating the proliferation of social bots, automated accounts that are also known to distort public opinion and support the spread of disinformation. Over the years, social bots have evolved greatly, often becoming indistinguishable from real users, and more recently, families of bots have been identified that are powered by Large Language Models to produce content for posting. We suggest an idea to classify social users as bots or not using genetic similarity algorithms. These algorithms provide an adaptive method for analyzing user behavior, allowing for the continuous evolution of detection criteria in response to the ever-changing tactics of social bots. Our proposal involves an initial clustering of social users into distinct macro species based on the similarities of their timelines. Macro species are then classified as either bot or genuine based on genetic characteristics. The preliminary idea we present, once fully developed, will allow existing detection applications based on timeline equality alone to be extended to detect bots. By incorporating new metrics, our approach will systematically classify non-trivial accounts into appropriate categories, effectively peeling back layers to reveal non-obvious species."}, "https://arxiv.org/abs/2410.13527": {"title": "Connect-while-in-range: modelling the impact of spatial constraints on dynamic communication network structures", "link": "https://arxiv.org/abs/2410.13527", "description": "arXiv:2410.13527v1 Announce Type: new \nAbstract: Like other social animals and biological systems, human groups constantly exchange information. Network models provide a way of quantifying this process by representing the pathways of information propagation between individuals. Existing approaches to studying these networks largely hypothesize network formation to be a result of cognitive biases and choices about who to connect to. Observational data suggests, however, that physical proximity plays a major role in shaping the formation of communication networks in human groups. Here we report results from a series of agent-based simulations in which agents move around at random in a bounded 2D space and connect while within communication range. Comparing the results to a non-spatial model, we show how including spatial constraints impacts our predictions of network structure: ranged networks are more clustered, with slightly higher degree, higher average shortest path length, a lower number of connected components and a higher small-world index. We find two important drivers of network structure in range-constrained dynamic networks: communication range relative to environment size, and population density. These results show that neglecting spatial constraints in models of network formation makes a difference for predicted network structures. Our simulation model quantifies this part of the process of network formation, realized by simply situating individuals in an environment. The model also provides a tool to include spatial constraints in other models of human communication, as well as dynamic models of network formation more generally."}, "https://arxiv.org/abs/2410.13739": {"title": "Microcanonical Monte Carlo simulation of opinion dynamics under the influence of mass media", "link": "https://arxiv.org/abs/2410.13739", "description": "arXiv:2410.13739v1 Announce Type: new \nAbstract: The formation of large social groups having uniform opinions influenced by mass media is currently an important topic in the social sciences. In this work, we explore and extend an off-lattice, two-dimensional Potts model (Eur. Phys. J. B 87, 78 [2014]) that describes the formation and dynamics of opinions in social groups according to individual consequence and agreement between neighbors. This model was originally obtained by the application of the maximum entropy principle, a general method in statistical inference, and using the same methodology we have now included the influence of mass media as a constant external field. By means of microcanonical Monte Carlo Metropolis simulations on a setup with two regions with opposing external influences, we have shown the presence of metastable states associated to the formation of clusters aligned with the locally imposed opinion. Our results suggest that, for some values of the total energy of the system, only a single cluster with a uniform opinion survives, thus the presence of two large, opposing groups is not a thermodynamically stable configuration."}, "https://arxiv.org/abs/2410.12799": {"title": "Ads Supply Personalization via Doubly Robust Learning", "link": "https://arxiv.org/abs/2410.12799", "description": "arXiv:2410.12799v1 Announce Type: cross \nAbstract: Ads supply personalization aims to balance the revenue and user engagement, two long-term objectives in social media ads, by tailoring the ad quantity and density. In the industry-scale system, the challenge for ads supply lies in modeling the counterfactual effects of a conservative supply treatment (e.g., a small density change) over an extended duration. In this paper, we present a streamlined framework for personalized ad supply. This framework optimally utilizes information from data collection policies through the doubly robust learning. Consequently, it significantly improves the accuracy of long-term treatment effect estimates. Additionally, its low-complexity design not only results in computational cost savings compared to existing methods, but also makes it scalable for billion-scale applications. Through both offline experiments and online production tests, the framework consistently demonstrated significant improvements in top-line business metrics over months. The framework has been fully deployed to live traffic in one of the world's largest social media platforms."}, "https://arxiv.org/abs/2410.12993": {"title": "Opinion-driven risk perception and reaction in SIS epidemics", "link": "https://arxiv.org/abs/2410.12993", "description": "arXiv:2410.12993v1 Announce Type: cross \nAbstract: We present and analyze a mathematical model to study the feedback between behavior and epidemic spread in a population that is actively assessing and reacting to risk of infection. In our model, a population dynamically forms an opinion that reflects its willingness to engage in risky behavior (e.g., not wearing a mask in a crowded area) or reduce it (e.g., social distancing). We consider SIS epidemic dynamics in which the contact rate within a population adapts as a function of its opinion. For the new coupled model, we prove the existence of two distinct parameter regimes. One regime corresponds to a low baseline infectiousness, and the equilibria of the epidemic spread are identical to those of the standard SIS model. The other regime corresponds to a high baseline infectiousness, and there is a bistability between two new endemic equilibria that reflect an initial preference towards either risk seeking behavior or risk aversion. We prove that risk seeking behavior increases the steady-state infection level in the population compared to the baseline SIS model, whereas risk aversion decreases it. When a population is highly reactive to extreme opinions, we show how risk aversion enables the complete eradication of infection in the population. Extensions of the model to a network of populations or individuals are explored numerically."}, "https://arxiv.org/abs/2410.13006": {"title": "LLM Chain Ensembles for Scalable and Accurate Data Annotation", "link": "https://arxiv.org/abs/2410.13006", "description": "arXiv:2410.13006v1 Announce Type: cross \nAbstract: The ability of large language models (LLMs) to perform zero-shot classification makes them viable solutions for data annotation in rapidly evolving domains where quality labeled data is often scarce and costly to obtain. However, the large-scale deployment of LLMs can be prohibitively expensive. This paper introduces an LLM chain ensemble methodology that aligns multiple LLMs in a sequence, routing data subsets to subsequent models based on classification uncertainty. This approach leverages the strengths of individual LLMs within a broader system, allowing each model to handle data points where it exhibits the highest confidence, while forwarding more complex cases to potentially more robust models. Our results show that the chain ensemble method often exceeds the performance of the best individual model in the chain and achieves substantial cost savings, making LLM chain ensembles a practical and efficient solution for large-scale data annotation challenges."}, "https://arxiv.org/abs/2410.13020": {"title": "Persistent Hierarchy in Contemporary International Collaboration", "link": "https://arxiv.org/abs/2410.13020", "description": "arXiv:2410.13020v1 Announce Type: cross \nAbstract: Science is increasingly global, with international collaboration playing a crucial role in advancing scientific development and knowledge exchange across borders. However, the processes that regulate how scientific labor is distributed among countries remain underexplored, leading to challenges in ensuring both effective collaboration and equitable participation across diverse scientific communities. Here, we leverage three million internationally coauthored publications produced by countries worldwide to examine the division of scientific labor in international collaboration, identify the factors that shape this distribution, and assess its broader consequences. Our findings uncover a persistent hierarchical structure in international collaboration, with researchers from scientifically advanced countries tend to occupy leading roles, while those from less-developed countries are often relegated to supportive roles, even after controlling for various influential factors. This hierarchy is also reflected in the research content, as countries with lower scientific capacity tend to participate in international collaborations that deviate from their domestic science. By analyzing the labor division within international collaborations, we demonstrate that researchers from less-developed countries face systematic disadvantages, which not only limit their contributions to the global scientific community but also prevent them from fully benefiting from international collaborations."}, "https://arxiv.org/abs/2410.13024": {"title": "Edge-based Modeling for Disease Transmission on Random Graphs: An Application to Mitigate a Syphilis Outbreak", "link": "https://arxiv.org/abs/2410.13024", "description": "arXiv:2410.13024v1 Announce Type: cross \nAbstract: Edge-based network models, especially those based on bond percolation methods, can be used to model disease transmission on complex networks and accommodate social heterogeneity while keeping tractability. Here we present an application of an edge-based network model to the spread of syphilis in the Kingston, Frontenac and Lennox & Addington (KFL&amp;A) region of Southeastern Ontario, Canada. We compared the results of using a network-based susceptible-infectious-recovered (SIR) model to those generated from using a traditional mass action SIR model. We found that the network model yields very different predictions, including a much lower estimate of the final epidemic size. We also used the network model to estimate the potential impact of introducing a rapid syphilis point of care test (POCT) and treatment intervention strategy that has recently been implemented by the public health unit to mitigate syphilis transmission."}, "https://arxiv.org/abs/2410.13036": {"title": "Uncovering the Internet's Hidden Values: An Empirical Study of Desirable Behavior Using Highly-Upvoted Content on Reddit", "link": "https://arxiv.org/abs/2410.13036", "description": "arXiv:2410.13036v1 Announce Type: cross \nAbstract: A major task for moderators of online spaces is norm-setting, essentially creating shared norms for user behavior in their communities. Platform design principles emphasize the importance of highlighting norm-adhering examples and explicitly stating community norms. However, norms and values vary between communities and go beyond content-level attributes, making it challenging for platforms and researchers to provide automated ways to identify desirable behavior to be highlighted. Current automated approaches of detecting desirability are limited to measures of prosocial behavior, but we do not know whether these measures fully capture the spectrum of what communities value. In this paper, we use upvotes, which express community approval, as a proxy for desirability and conduct an analysis of highly-upvoted comments across 85 popular sub-communities on Reddit. Using a large language model, we extract values from these comments and compile 97 $\\textit{macro}$, $\\textit{meso}$, and $\\textit{micro}$ values based on their frequency across communities. Furthermore, we find that existing computational models for measuring prosociality were inadequate to capture 86 of the values we extracted. Finally, we show that our approach can not only extract most of the qualitatively-identified values from prior taxonomies, but also uncover new values that are actually encouraged in practice. This work has implications for improving moderator understanding of their community values, motivates the need for nuanced models of desirability beyond prosocial measures, and provides a framework that can supplement qualitative work with larger-scale content analyses."}, "https://arxiv.org/abs/2410.13345": {"title": "Dynamical Analysis of a Predator-Prey Model with Additive Allee Effect and Prey Group Defense", "link": "https://arxiv.org/abs/2410.13345", "description": "arXiv:2410.13345v1 Announce Type: cross \nAbstract: In this article, we develop a predator-prey model with Allee effect and prey group defense. The model has three equilibrium points i.e. the trivial point, the predator extinction point, and the coexistence point. All equilibrium points are locally asymptotically stable under certain conditions. The Allee effect in this model influences the stability of the equilibrium point. If the Allee effect is weak, then the trivial equilibrium point is unstable. Meanwhile, if the Allee effect is strong, then the trivial equilibrium point is locally asymptotically stable. Those mean that a strong Allee effect can lead to the extinction of both populations. Moreover, under weak Allee condition, forward bifurcation and Hopf bifurcation occur at the predator extinction equilibrium point. Meanwhile, a strong Allee effect may induce bistability at both the trivial equilibrium point and the predator extinction equilibrium point. Those mean that prey can survive without the presence of predators, but a strong Allee effect can lead to prey extinction if the population size is very small. To support our analytical findings, we perform some numerical simulations in the final section."}, "https://arxiv.org/abs/2410.13588": {"title": "Cross-Domain Sequential Recommendation via Neural Process", "link": "https://arxiv.org/abs/2410.13588", "description": "arXiv:2410.13588v1 Announce Type: cross \nAbstract: Cross-Domain Sequential Recommendation (CDSR) is a hot topic in sequence-based user interest modeling, which aims at utilizing a single model to predict the next items for different domains. To tackle the CDSR, many methods are focused on domain overlapped users' behaviors fitting, which heavily relies on the same user's different-domain item sequences collaborating signals to capture the synergy of cross-domain item-item correlation. Indeed, these overlapped users occupy a small fraction of the entire user set only, which introduces a strong assumption that the small group of domain overlapped users is enough to represent all domain user behavior characteristics. However, intuitively, such a suggestion is biased, and the insufficient learning paradigm in non-overlapped users will inevitably limit model performance. Further, it is not trivial to model non-overlapped user behaviors in CDSR because there are no other domain behaviors to collaborate with, which causes the observed single-domain users' behavior sequences to be hard to contribute to cross-domain knowledge mining. Considering such a phenomenon, we raise a challenging and unexplored question: How to unleash the potential of non-overlapped users' behaviors to empower CDSR?"}, "https://arxiv.org/abs/2410.13784": {"title": "An Exposition of Pathfinding Strategies Within Lightning Network Clients", "link": "https://arxiv.org/abs/2410.13784", "description": "arXiv:2410.13784v1 Announce Type: cross \nAbstract: The Lightning Network is a peer-to-peer network designed to address Bitcoin's scalability challenges, facilitating rapid, cost-effective, and instantaneous transactions through bidirectional, blockchain-backed payment channels among network peers. Due to a source-based routing of payments, different pathfinding strategies are used in practice, trading off different objectives for each other such as payment reliability and routing fees. This paper explores differences within pathfinding strategies used by prominent Lightning Network node implementations, which include different underlying cost functions and different constraints, as well as different greedy algorithms of shortest path-type. Surprisingly, we observe that the pathfinding problems that most LN node implementations attempt to solve are NP-complete, and cannot be guaranteed to be optimally solved by the variants of Dijkstra's algorithm currently deployed in production. Through comparative analysis and simulations, we evaluate efficacy of different pathfinding strategies across metrics such as success rate, fees, path length, and timelock. Our experiments indicate that the strategies used by LND tend to be advantageous in terms of payment reliability, Eclair tends to result in paths with low fees, and that LDK exhibits average reliability with larger fee levels for smaller payment amounts; furthermore, CLN stands out for its minimal timelock paths. Additionally, we investigate the impact of Lightning node connectivity levels on routing efficiency. The findings of our analysis provide insights towards future improvements of pathfinding strategies and algorithms used within the Lightning Network."}, "https://arxiv.org/abs/2307.15702": {"title": "The Strong Maximum Circulation Algorithm: A New Method for Aggregating Preference Rankings", "link": "https://arxiv.org/abs/2307.15702", "description": "arXiv:2307.15702v3 Announce Type: replace \nAbstract: We present a new optimization-based method for aggregating preferences in settings where each voter expresses preferences over pairs of alternatives. Our approach to identifying a consensus partial order is motivated by the observation that collections of votes that form a cycle can be treated as collective ties. Our approach then removes unions of cycles of votes, or circulations, from the vote graph and determines aggregate preferences from the remainder. Specifically, we study the removal of maximal circulations attained by any union of cycles the removal of which leaves an acyclic graph. We introduce the strong maximum circulation, the removal of which guarantees a unique outcome in terms of the induced partial order, called the strong partial order. The strong maximum circulation also satisfies strong complementary slackness conditions, and is shown to be solved efficiently as a network flow problem. We further establish the relationship between the dual of the maximum circulation problem and Kemeny's method, a popular optimization-based approach for preference aggregation. We also show that identifying a minimum maximal circulation -- i.e., a maximal circulation containing the smallest number of votes -- is an NP-hard problem. Further an instance of the minimum maximal circulation may have multiple optimal solutions whose removal results in conflicting partial orders."}, "https://arxiv.org/abs/2310.05981": {"title": "Quantifying the visual impact of wind farm lights on the nocturnal landscape", "link": "https://arxiv.org/abs/2310.05981", "description": "arXiv:2310.05981v2 Announce Type: replace \nAbstract: Wind farm lights are a conspicuous feature in the nocturnal landscape. Their presence is a source of light pollution for residents and the environment, severely disrupting in some places the aesthetic, cultural, and scientific values of the pristine starry skies. In this work we present a simple model for quantifying the visual impact of individual wind turbine lights, based on the comparison of their brightnesses with the brightness of well-known night sky objects. The model includes atmospheric and visual variables, and for typical parameters it shows that medium-intensity turbine lights can be brighter than Venus up to ~4 km from the turbine, brighter than alpha CMa (the brightest star on the nighttime sky) until about ~10 km, and reach the standard stellar visibility limit for the unaided eye (m_v=+6.00) at ~38 km. These results suggest that the visual range of wind farms at nighttime may be significantly larger than at daytime, a factor that should be taken into account in environmental impact assessments."}, "https://arxiv.org/abs/2402.03239": {"title": "Bluesky and the AT Protocol: Usable Decentralized Social Media", "link": "https://arxiv.org/abs/2402.03239", "description": "arXiv:2402.03239v2 Announce Type: replace-cross \nAbstract: Bluesky is a new social network built upon the AT Protocol, a decentralized foundation for public social media. It was launched in private beta in February 2023, and has grown to over 10 million registered users by October 2024. In this paper we introduce the architecture of Bluesky and the AT Protocol, and explain how the technical design of Bluesky is informed by our goals: to enable decentralization by having multiple interoperable providers for every part of the system; to make it easy for users to switch providers; to give users agency over the content they see; and to provide a simple user experience that does not burden users with complexity arising from the system's decentralized nature. The system's openness allows anybody to contribute to content moderation and community management, and we invite the research community to use Bluesky as a dataset and testing ground for new approaches in social media moderation."}, "https://arxiv.org/abs/2410.13887": {"title": "Observing the Southern US Culture of Honor Using Large-Scale Social Media Analysis", "link": "https://arxiv.org/abs/2410.13887", "description": "arXiv:2410.13887v1 Announce Type: new \nAbstract: A \\textit{culture of honor} refers to a social system where individuals' status, reputation, and esteem play a central role in governing interpersonal relations. Past works have associated this concept with the United States (US) South and related with it various traits such as higher sensitivity to insult, a higher value on reputation, and a tendency to react violently to insults. In this paper, we hypothesize and confirm that internet users from the US South, where a \\textit{culture of honor} is more prevalent, are more likely to display a trait predicted by their belonging to a \\textit{culture of honor}. Specifically, we test the hypothesis that US Southerners are more likely to retaliate to personal attacks by personally attacking back. We leverage OpenAI's GPT-3.5 API to both geolocate internet users and to automatically detect whether users are insulting each other. We validate the use of GPT-3.5 by measuring its performance on manually-labeled subsets of the data. Our work demonstrates the potential of formulating a hypothesis based on a conceptual framework, operationalizing it in a way that is amenable to large-scale LLM-aided analysis, manually validating the use of the LLM, and drawing a conclusion."}, "https://arxiv.org/abs/2410.13905": {"title": "P4GCN: Vertical Federated Social Recommendation with Privacy-Preserving Two-Party Graph Convolution Networks", "link": "https://arxiv.org/abs/2410.13905", "description": "arXiv:2410.13905v1 Announce Type: new \nAbstract: In recent years, graph neural networks (GNNs) have been commonly utilized for social recommendation systems. However, real-world scenarios often present challenges related to user privacy and business constraints, inhibiting direct access to valuable social information from other platforms. While many existing methods have tackled matrix factorization-based social recommendations without direct social data access, developing GNN-based federated social recommendation models under similar conditions remains largely unexplored. To address this issue, we propose a novel vertical federated social recommendation method leveraging privacy-preserving two-party graph convolution networks (P4GCN) to enhance recommendation accuracy without requiring direct access to sensitive social information. First, we introduce a Sandwich-Encryption module to ensure comprehensive data privacy during the collaborative computing process. Second, we provide a thorough theoretical analysis of the privacy guarantees, considering the participation of both curious and honest parties. Extensive experiments on four real-world datasets demonstrate that P4GCN outperforms state-of-the-art methods in terms of recommendation accuracy. The code is available at https://github.com/WwZzz/P4GCN."}, "https://arxiv.org/abs/2410.13909": {"title": "Large Language Model-driven Multi-Agent Simulation for News Diffusion Under Different Network Structures", "link": "https://arxiv.org/abs/2410.13909", "description": "arXiv:2410.13909v1 Announce Type: new \nAbstract: The proliferation of fake news in the digital age has raised critical concerns, particularly regarding its impact on societal trust and democratic processes. Diverging from conventional agent-based simulation approaches, this work introduces an innovative approach by employing a large language model (LLM)-driven multi-agent simulation to replicate complex interactions within information ecosystems. We investigate key factors that facilitate news propagation, such as agent personalities and network structures, while also evaluating strategies to combat misinformation. Through simulations across varying network structures, we demonstrate the potential of LLM-based agents in modeling the dynamics of misinformation spread, validating the influence of agent traits on the diffusion process. Our findings emphasize the advantages of LLM-based simulations over traditional techniques, as they uncover underlying causes of information spread -- such as agents promoting discussions -- beyond the predefined rules typically employed in existing agent-based models. Additionally, we evaluate three countermeasure strategies, discovering that brute-force blocking influential agents in the network or announcing news accuracy can effectively mitigate misinformation. However, their effectiveness is influenced by the network structure, highlighting the importance of considering network structure in the development of future misinformation countermeasures."}, "https://arxiv.org/abs/2410.13912": {"title": "A spatiotemporal knowledge graph-based method for identifying individual activity locations from mobile phone data", "link": "https://arxiv.org/abs/2410.13912", "description": "arXiv:2410.13912v1 Announce Type: new \nAbstract: In recent years, mobile phone data has been widely used for human mobility analytics. Identifying individual activity locations is the fundamental step for mobile phone data processing. Current methods typically aggregate spatially adjacent location records over multiple days to identify activity locations. However, only considering spatial relationships while overlooking temporal ones may lead to inaccurate activity location identification, and also affect activity pattern analysis. In this study, we propose a spatiotemporal knowledge graph-based (STKG) method for identifying activity locations from mobile phone data. An STKG is designed and constructed to describe individual mobility characteristics. The spatial and temporal relationships of individual stays are inferred and transformed into a spatiotemporal graph. The modularity-optimization community detection algorithm is applied to identify stays with dense spatiotemporal relationships, which are considering as activity locations. A case study in Shanghai was conducted to verify the performance of the proposed method. The results show that compared with two baseline methods, the STKG-based method can limit an additional 45% of activity locations with the longest daytime stay within a reasonable spatial range; In addition, the STKG-based method exhibit lower variance in the start and end times of activities across different days, performing approximately 10% to 20% better than the two baseline methods. Moreover, the STKG-based method effectively distinguishes between locations that are geographically close but exhibit different temporal patterns. These findings demonstrate the effectiveness of STKG-based method in enhancing both spatial precision and temporal consistency."}, "https://arxiv.org/abs/2410.13915": {"title": "A Simulation System Towards Solving Societal-Scale Manipulation", "link": "https://arxiv.org/abs/2410.13915", "description": "arXiv:2410.13915v1 Announce Type: new \nAbstract: The rise of AI-driven manipulation poses significant risks to societal trust and democratic processes. Yet, studying these effects in real-world settings at scale is ethically and logistically impractical, highlighting a need for simulation tools that can model these dynamics in controlled settings to enable experimentation with possible defenses. We present a simulation environment designed to address this. We elaborate upon the Concordia framework that simulates offline, `real life' activity by adding online interactions to the simulation through social media with the integration of a Mastodon server. We improve simulation efficiency and information flow, and add a set of measurement tools, particularly longitudinal surveys. We demonstrate the simulator with a tailored example in which we track agents' political positions and show how partisan manipulation of agents can affect election results."}, "https://arxiv.org/abs/2410.14056": {"title": "Approximating Spanning Centrality with Random Bouquets", "link": "https://arxiv.org/abs/2410.14056", "description": "arXiv:2410.14056v1 Announce Type: new \nAbstract: Spanning Centrality is a measure used in network analysis to determine the importance of an edge in a graph based on its contribution to the connectivity of the entire network. Specifically, it quantifies how critical an edge is in terms of the number of spanning trees that include that edge. The current state-of-the-art for All Edges Spanning Centrality~(AESC), which computes the exact centrality values for all the edges, has a time complexity of $\\mathcal{O}(mn^{3/2})$ for $n$ vertices and $m$ edges. This makes the computation infeasible even for moderately sized graphs. Instead, there exist approximation algorithms which process a large number of random walks to estimate edge centralities. However, even the approximation algorithms can be computationally overwhelming, especially if the approximation error bound is small. In this work, we propose a novel, hash-based sampling method and a vectorized algorithm which greatly improves the execution time by clustering random walks into {\\it Bouquets}. On synthetic random walk benchmarks, {\\it Bouquets} performs $7.8\\times$ faster compared to naive, traditional random-walk generation. We also show that the proposed technique is scalable by employing it within a state-of-the-art AESC approximation algorithm, {\\sc TGT+}. The experiments show that using Bouquets yields more than $100\\times$ speed-up via parallelization with 16 threads."}, "https://arxiv.org/abs/2410.14147": {"title": "Leveraging Large Language Models for Enhancing Public Transit Services", "link": "https://arxiv.org/abs/2410.14147", "description": "arXiv:2410.14147v1 Announce Type: new \nAbstract: Public transit systems play a crucial role in providing efficient and sustainable transportation options in urban areas. However, these systems face various challenges in meeting commuters' needs. On the other hand, despite the rapid development of Large Language Models (LLMs) worldwide, their integration into transit systems remains relatively unexplored.\n  The objective of this paper is to explore the utilization of LLMs in the public transit system, with a specific focus on improving the customers' experience and transit staff performance. We present a general framework for developing LLM applications in transit systems, wherein the LLM serves as the intermediary for information communication between natural language content and the resources within the database. In this context, the LLM serves a multifaceted role, including understanding users' requirements, retrieving data from the dataset in response to user queries, and tailoring the information to align with the users' specific needs. Three transit LLM applications are presented: Tweet Writer, Trip Advisor, and Policy Navigator. Tweet Writer automates updates to the transit system alerts on social media, Trip Advisor offers customized transit trip suggestions, and Policy Navigator provides clear and personalized answers to policy queries. Leveraging LLMs in these applications enhances seamless communication with their capabilities of understanding and generating human-like languages. With the help of these three LLM transit applications, transit system media personnel can provide system updates more efficiently, and customers can access travel information and policy answers in a more user-friendly manner."}, "https://arxiv.org/abs/2410.14181": {"title": "Exploring the Role of Network Centrality in Player Selection: A Case Study of Pakistan Super League", "link": "https://arxiv.org/abs/2410.14181", "description": "arXiv:2410.14181v1 Announce Type: new \nAbstract: Cricket, a popular bat-and-ball game in South Asia, is played between two 11-player teams. The Pakistan Super League (PSL) is a commercial T20 domestic league comprised of six franchise-owned teams, where player selection is competitive. In this study, an existing role-based ranking structure is assessed that evaluates player performance in the context of team belongingness to generate optimal Pakistan cricket teams for international tournaments. The underlying assumption is that since cricket is fundamentally a team sport, the performance of players compared to their peers plays a crucial role in their selection. To accomplish this, a network is generated using ball-by-ball data from previous PSL matches (2016-2022), and social network analysis (SNA) techniques such as centrality and clustering coefficient measures, are employed to quantify the level of belongingness among Pakistani cricket players within the PSL network. Characteristic network models, such as the Erd\\\"os-R\\'enyi, Watts-Strogatz, and Barab\\'asi-Albert models are utilized to gain insights into the small-world properties of the network. By ranking players using centrality and clustering coefficient metrics, four teams are formulated, and these teams are subsequently compared to the official squad selected by the Pakistan Cricket Board (PCB) for the recent ICC Men's T20 World Cup in 2022. This evaluation sheds light on the allegations of nepotism and favoritism in team formations that have been attributed to the PCB over the years. Based on our findings, out of the 18 players in the World Cup squad, 11 were included in the teams we formed. While most of the 7 players who were not included in our teams were still selected for the ICC Men's T20 World Cup 2022, they ranked highly in our rankings, suggesting their potential and competence."}, "https://arxiv.org/abs/2410.14047": {"title": "DiFuseR: A Distributed Sketch-based Influence Maximization Algorithm for GPUs", "link": "https://arxiv.org/abs/2410.14047", "description": "arXiv:2410.14047v1 Announce Type: cross \nAbstract: Influence Maximization (IM) aims to find a given number of \"seed\" vertices that can effectively maximize the expected spread under a given diffusion model. Due to the NP-Hardness of finding an optimal seed set, approximation algorithms are often used for IM. However, these algorithms require a large number of simulations to find good seed sets. In this work, we propose DiFuseR, a blazing-fast, high-quality IM algorithm that can run on multiple GPUs in a distributed setting. DiFuseR is designed to increase GPU utilization, reduce inter-node communication, and minimize overlapping data/computation among the nodes. Based on the experiments with various graphs, containing some of the largest networks available, and diffusion settings, the proposed approach is found to be 3.2x and 12x faster on average on a single GPU and 8 GPUs, respectively. It can achieve up to 8x and 233.7x speedup on the same hardware settings. Furthermore, thanks to its smart load-balancing mechanism, on 8 GPUs, it is on average 5.6x faster compared to its single-GPU performance."}, "https://arxiv.org/abs/2410.14146": {"title": "CausalChat: Interactive Causal Model Development and Refinement Using Large Language Models", "link": "https://arxiv.org/abs/2410.14146", "description": "arXiv:2410.14146v1 Announce Type: cross \nAbstract: Causal networks are widely used in many fields to model the complex relationships between variables. A recent approach has sought to construct causal networks by leveraging the wisdom of crowds through the collective participation of humans. While this can yield detailed causal networks that model the underlying phenomena quite well, it requires a large number of individuals with domain understanding. We adopt a different approach: leveraging the causal knowledge that large language models, such as OpenAI's GPT-4, have learned by ingesting massive amounts of literature. Within a dedicated visual analytics interface, called CausalChat, users explore single variables or variable pairs recursively to identify causal relations, latent variables, confounders, and mediators, constructing detailed causal networks through conversation. Each probing interaction is translated into a tailored GPT-4 prompt and the response is conveyed through visual representations which are linked to the generated text for explanations. We demonstrate the functionality of CausalChat across diverse data contexts and conduct user studies involving both domain experts and laypersons."}, "https://arxiv.org/abs/2410.14515": {"title": "Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media", "link": "https://arxiv.org/abs/2410.14515", "description": "arXiv:2410.14515v1 Announce Type: cross \nAbstract: Misinformation spreads rapidly on social media, confusing the truth and targetting potentially vulnerable people. To effectively mitigate the negative impact of misinformation, it must first be accurately detected before applying a mitigation strategy, such as X's community notes, which is currently a manual process. This study takes a knowledge-based approach to misinformation detection, modelling the problem similarly to one of natural language inference. The EffiARA annotation framework is introduced, aiming to utilise inter- and intra-annotator agreement to understand the reliability of each annotator and influence the training of large language models for classification based on annotator reliability. In assessing the EffiARA annotation framework, the Russo-Ukrainian Conflict Knowledge-Based Misinformation Classification Dataset (RUC-MCD) was developed and made publicly available. This study finds that sample weighting using annotator reliability performs the best, utilising both inter- and intra-annotator agreement and soft-label training. The highest classification performance achieved using Llama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large."}, "https://arxiv.org/abs/2410.14617": {"title": "On the Use of Proxies in Political Ad Targeting", "link": "https://arxiv.org/abs/2410.14617", "description": "arXiv:2410.14617v1 Announce Type: cross \nAbstract: Detailed targeting of advertisements has long been one of the core offerings of online platforms. Unfortunately, malicious advertisers have frequently abused such targeting features, with results that range from violating civil rights laws to driving division, polarization, and even social unrest. Platforms have often attempted to mitigate this behavior by removing targeting attributes deemed problematic, such as inferred political leaning, religion, or ethnicity. In this work, we examine the effectiveness of these mitigations by collecting data from political ads placed on Facebook in the lead up to the 2022 U.S. midterm elections. We show that major political advertisers circumvented these mitigations by targeting proxy attributes: seemingly innocuous targeting criteria that closely correspond to political and racial divides in American society. We introduce novel methods for directly measuring the skew of various targeting criteria to quantify their effectiveness as proxies, and then examine the scale at which those attributes are used. Our findings have crucial implications for the ongoing discussion on the regulation of political advertising and emphasize the urgency for increased transparency."}, "https://arxiv.org/abs/2403.14296": {"title": "Optimal prevention strategies for chronic diseases in a compartmental disease trajectory model", "link": "https://arxiv.org/abs/2403.14296", "description": "arXiv:2403.14296v3 Announce Type: replace \nAbstract: In countries with growing elderly populations, multimorbidity poses a significant healthcare challenge. Despite its pressing development, important questions remain on how to model the development of multimorbidity. Leveraging a comprehensive dataset from approximately 45 million hospital stays spanning 17 years in Austria, we propose a compartmental model, traditionally used in infectious diseases, describing chronic disease trajectories across 132 distinct multimorbidity patterns (compartments). Our compartmental disease trajectory model (CDTM) forecasts changes in the incidence of 131 diagnostic groups and their combinations until 2030, highlighting patterns involving hypertensive diseases with cardiovascular diseases and metabolic disorders. We additionally use the model to pinpoint specific diagnoses with the greatest potential for preventive interventions to promote healthy aging. According to our model, a 5% reduction in new cases of hypertensive disease (I10-I15) leads to a 0.57 (0.06)% reduction in all-cause mortality over a 15-year period, and a 0.57 (0.07)% reduction in mortality for malignant neoplasms (C00-C97). Furthermore, we use the model to assess the long-term consequences of the Covid-19 pandemic on hospitalizations, revealing earlier and more frequent hospitalizations across multiple diagnoses. Our fully data-driven approach identifies leverage points for proactive preparation by physicians and policymakers to reduce the overall disease burden in the population, emphasizing a shift toward patient-centered care."}, "https://arxiv.org/abs/2404.02175": {"title": "Social Dynamics of Consumer Response: A Unified Framework Integrating Statistical Physics and Marketing Dynamics", "link": "https://arxiv.org/abs/2404.02175", "description": "arXiv:2404.02175v2 Announce Type: replace \nAbstract: Understanding how consumers react to advertising inputs is essential for marketers aiming to optimize advertising strategies and improve campaign effectiveness. This study examines the complex nature of consumer behaviour by applying theoretical frameworks derived from physics and social psychology. We present an innovative equation that captures the relation between spending on advertising and consumer response, using concepts such as symmetries, scaling laws, and phase transitions. By validating our equation against well-known models such as the Michaelis-Menten and Hill equations, we prove its effectiveness in accurately representing the complexity of consumer response dynamics. The analysis emphasizes the importance of key model parameters, such as marketing effectiveness, response sensitivity, and behavioural sensitivity, in influencing consumer behaviour. The work explores the practical implications for advertisers and marketers, as well as discussing the limitations and future research directions. In summary, this study provides a thorough framework for comprehending and forecasting consumer reactions to advertising, which has implications for optimizing advertising strategies and allocating resources."}, "https://arxiv.org/abs/1908.05923": {"title": "Evolution of cooperation in networks with well-connected cooperators", "link": "https://arxiv.org/abs/1908.05923", "description": "arXiv:1908.05923v4 Announce Type: replace-cross \nAbstract: Cooperative behavior constitutes a key aspect of human society and non-human animal systems, but explaining how cooperation evolves represents a major scientific challenge. It is now well established that social network structure plays a central role for the viability of cooperation. However, not much is known about the importance of the positions of cooperators in the networks for the evolution of cooperation. Here, we investigate how the spread of cooperation is affected by correlations between cooperativeness and individual social connectedness (such that cooperators occupy well-connected network positions). Using simulation models, we find that these correlations enhance cooperation in standard scale-free networks but not in standard Poisson networks. In contrast, when degree assortativity is increased such that individuals cluster with others of similar social connectedness, we find that Poisson networks can maintain high levels of cooperation, which can even exceed those of scale-free networks. We show that this is due to dynamics where bridge areas between social clusters act as barriers to the spread of defection. We also find that this positive effect on cooperation is sensitive to the presence of Trojan horses (defectors placed within cooperator clusters), which allow defection to invade. The results provide new knowledge about the conditions under which cooperation may evolve, and are also relevant to consider in regard to the design of cooperation studies."}, "https://arxiv.org/abs/2301.12562": {"title": "Simplifying Subgraph Representation Learning for Scalable Link Prediction", "link": "https://arxiv.org/abs/2301.12562", "description": "arXiv:2301.12562v4 Announce Type: replace-cross \nAbstract: Link prediction on graphs is a fundamental problem. Subgraph representation learning approaches (SGRLs), by transforming link prediction to graph classification on the subgraphs around the links, have achieved state-of-the-art performance in link prediction. However, SGRLs are computationally expensive, and not scalable to large-scale graphs due to expensive subgraph-level operations. To unlock the scalability of SGRLs, we propose a new class of SGRLs, that we call Scalable Simplified SGRL (S3GRL). Aimed at faster training and inference, S3GRL simplifies the message passing and aggregation operations in each link's subgraph. S3GRL, as a scalability framework, accommodates various subgraph sampling strategies and diffusion operators to emulate computationally-expensive SGRLs. We propose multiple instances of S3GRL and empirically study them on small to large-scale graphs. Our extensive experiments demonstrate that the proposed S3GRL models scale up SGRLs without significant performance compromise (even with considerable gains in some cases), while offering substantially lower computational footprints (e.g., multi-fold inference and training speedup)."}, "https://arxiv.org/abs/2304.10681": {"title": "A simplicity bubble problem and zemblanity in digitally intermediated societies", "link": "https://arxiv.org/abs/2304.10681", "description": "arXiv:2304.10681v3 Announce Type: replace-cross \nAbstract: In this article, we discuss the ubiquity of Big Data and machine learning in society and propose that it evinces the need of further investigation of their fundamental limitations. We extend the ``too much information tends to behave like very little information'' phenomenon to formal knowledge about lawlike universes and arbitrary collections of computably generated datasets. This gives rise to the simplicity bubble problem, which refers to a learning algorithm equipped with a formal theory that can be deceived by a dataset to find a locally optimal model which it deems to be the global one. In the context of lawlike (computable) universes and formal learning systems, we show that there is a ceiling above which formal knowledge cannot further decrease the probability of zemblanitous findings, should the randomly generated data made available to the formal learning system be sufficiently large in comparison to their joint complexity. Zemblanity, the opposite of serendipity, is defined by an undesirable but expected finding that reveals an underlying problem or negative consequence in a given model or theory, which is in principle predictable in case the formal theory contains sufficient information. We also argue that this is an epistemological limitation that may generate unpredictable problems in digitally intermediated societies."}, "https://arxiv.org/abs/2401.02290": {"title": "Path-based Explanation for Knowledge Graph Completion", "link": "https://arxiv.org/abs/2401.02290", "description": "arXiv:2401.02290v2 Announce Type: replace-cross \nAbstract: Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph Completion (KGC) by modelling how entities and relations interact in recent years. However, the explanation of the predicted facts has not caught the necessary attention. Proper explanations for the results of GNN-based KGC models increase model transparency and help researchers develop more reliable models. Existing practices for explaining KGC tasks rely on instance/subgraph-based approaches, while in some scenarios, paths can provide more user-friendly and interpretable explanations. Nonetheless, the methods for generating path-based explanations for KGs have not been well-explored. To address this gap, we propose Power-Link, the first path-based KGC explainer that explores GNN-based models. We design a novel simplified graph-powering technique, which enables the generation of path-based explanations with a fully parallelisable and memory-efficient training scheme. We further introduce three new metrics for quantitative evaluation of the explanations, together with a qualitative human evaluation. Extensive experiments demonstrate that Power-Link outperforms the SOTA baselines in interpretability, efficiency, and scalability."}, "https://arxiv.org/abs/2404.07298": {"title": "A Deep Learning Method for Predicting Mergers and Acquisitions: Temporal Dynamic Industry Networks", "link": "https://arxiv.org/abs/2404.07298", "description": "arXiv:2404.07298v3 Announce Type: replace-cross \nAbstract: Merger and Acquisition (M&amp;A) activities play a vital role in market consolidation and restructuring. For acquiring companies, M&amp;A serves as a key investment strategy, with one primary goal being to attain complementarities that enhance market power in competitive industries. In addition to intrinsic factors, a M&amp;A behavior of a firm is influenced by the M&amp;A activities of its peers, a phenomenon known as the \"peer effect.\" However, existing research often fails to capture the rich interdependencies among M&amp;A events within industry networks.\n  An effective M&amp;A predictive model should offer deal-level predictions without requiring ad-hoc feature engineering or data rebalancing. Such a model would predict the M&amp;A behaviors of rival firms and provide specific recommendations for both bidder and target firms. However, most current models only predict one side of an M&amp;A deal, lack firm-specific recommendations, and rely on arbitrary time intervals that impair predictive accuracy. Additionally, due to the sparsity of M&amp;A events, existing models require data rebalancing, which introduces bias and limits their real-world applicability.\n  To address these challenges, we propose a Temporal Dynamic Industry Network (TDIN) model, leveraging temporal point processes and deep learning to capture complex M&amp;A interdependencies without ad-hoc data adjustments. The temporal point process framework inherently models event sparsity, eliminating the need for data rebalancing. Empirical evaluations on M&amp;A data from January 1997 to December 2020 validate the effectiveness of our approach in predicting M&amp;A events and offering actionable, deal-level recommendations."}, "https://arxiv.org/abs/2410.14970": {"title": "Taming the Long Tail in Human Mobility Prediction", "link": "https://arxiv.org/abs/2410.14970", "description": "arXiv:2410.14970v1 Announce Type: new \nAbstract: With the popularity of location-based services, human mobility prediction plays a key role in enhancing personalized navigation, optimizing recommendation systems, and facilitating urban mobility and planning. This involves predicting a user's next POI (point-of-interest) visit using their past visit history. However, the uneven distribution of visitations over time and space, namely the long-tail problem in spatial distribution, makes it difficult for AI models to predict those POIs that are less visited by humans. In light of this issue, we propose the Long-Tail Adjusted Next POI Prediction (LoTNext) framework for mobility prediction, combining a Long-Tailed Graph Adjustment module to reduce the impact of the long-tailed nodes in the user-POI interaction graph and a novel Long-Tailed Loss Adjustment module to adjust loss by logit score and sample weight adjustment strategy. Also, we employ the auxiliary prediction task to enhance generalization and accuracy. Our experiments with two real-world trajectory datasets demonstrate that LoTNext significantly surpasses existing state-of-the-art works. Our code is available at https://github.com/Yukayo/LoTNext."}, "https://arxiv.org/abs/2410.15046": {"title": "Towards Truss-Based Temporal Community Search", "link": "https://arxiv.org/abs/2410.15046", "description": "arXiv:2410.15046v1 Announce Type: new \nAbstract: Identifying communities from temporal networks facilitates the understanding of potential dynamic relationships among entities, which has already received extensive applications. However, existing methods primarily rely on lower-order connectivity (e.g., temporal edges) to capture the structural and temporal cohesiveness of the community, often neglecting higher-order temporal connectivity, which leads to sub-optimal results. To overcome this dilemma, we propose a novel temporal community model named maximal-truss (MDT). This model emphasizes maximal temporal support, ensuring all edges are connected by a sequence of triangles with elegant temporal properties. To search the MDT containing the user-initiated query node q (q-MDT), we first design a powerful local search framework with some effective pruning strategies. This approach aims to identify the solution from the small temporal subgraph which is expanded from q. To further improve the performance on large graphs, we build the temporal trussness index (TT-index) for all edges. Leveraging the TT-index allows us to efficiently search high-probability target subgraphs instead of performing a full search across the entire input graph. Empirical results on nine real-world networks and seven competitors demonstrate the superiority of our solutions in terms of efficiency, effectiveness, and scalability"}, "https://arxiv.org/abs/2410.15330": {"title": "Decoding how higher-order network interactions shape complex contagion dynamics", "link": "https://arxiv.org/abs/2410.15330", "description": "arXiv:2410.15330v1 Announce Type: new \nAbstract: Complex contagion models that involve contagion along higher-order structures, such as simplicial complexes and hypergraphs, yield new classes of mean-field models. Interestingly, the differential equations arising from many such models often exhibit a similar form, resulting in qualitatively comparable global bifurcation patterns. Motivated by this observation, we investigate a generalized mean-field-type model that provides a unified framework for analysing a range of different models. In particular, we derive analytical conditions for the emergence of different bifurcation regimes exhibited by three models of increasing complexity -- ranging from three- and four-body interactions to two connected populations with both pairwise and three-body interactions. For the first two cases, we give a complete characterisation of all possible outcomes, along with the corresponding conditions on network and epidemic parameters. In the third case, we demonstrate that multistability is possible despite only three-body interactions. Our results reveal that single population models with three-body interactions can only exhibit simple transcritical transitions or bistability, whereas with four-body interactions multistability with two distinct endemic steady states is possible. Surprisingly, the two-population model exhibits multistability via symmetry breaking despite three-body interactions only. Our work sheds light on the relationship between equation structure and model behaviour and makes the first step towards elucidating mechanisms by which different system behaviours arise, and how network and dynamic properties facilitate or hinder outcomes."}, "https://arxiv.org/abs/2410.15451": {"title": "Heuristic-based Dynamic Leiden Algorithm for Efficient Tracking of Communities on Evolving Graphs", "link": "https://arxiv.org/abs/2410.15451", "description": "arXiv:2410.15451v1 Announce Type: new \nAbstract: Community detection, or clustering, identifies groups of nodes in a graph that are more densely connected to each other than to the rest of the network. Given the size and dynamic nature of real-world graphs, efficient community detection is crucial for tracking evolving communities, enhancing our understanding and management of complex systems. The Leiden algorithm, which improves upon the Louvain algorithm, efficiently detects communities in large networks, producing high-quality structures. However, existing multicore dynamic community detection algorithms based on Leiden are inefficient and lack support for tracking evolving communities. This technical report introduces the first implementations of parallel Naive-dynamic (ND), Delta-screening (DS), and Dynamic Frontier (DF) Leiden algorithms that efficiently track communities over time. Experiments on a 64-core AMD EPYC-7742 processor demonstrate that ND, DS, and DF Leiden achieve average speedups of 3.9x, 4.4x, and 6.1x, respectively, on large graphs with random batch updates compared to the Static Leiden algorithm, and these approaches scale at 1.4 - 1.5x for every thread doubling."}, "https://arxiv.org/abs/2410.15707": {"title": "Changes in Sentiments and User Engagement for 2024 U", "link": "https://arxiv.org/abs/2410.15707", "description": "arXiv:2410.15707v1 Announce Type: new \nAbstract: The 2024 U.S. presidential election has sparked widespread online discussions about the presidential candidates. Joe Biden's withdrawal from the race and Kamala Harris's subsequent entry as the Democratic candidate likely alter the dynamics of these online discussions; yet, this hypothesis requires evidence. Here, we study how sentiments and user engagement in social media posts mentioning presidential candidates change after Biden's withdrawal. Our analysis is based on N=680,609 TikTok videos that have accumulated over 4 billion views, with more than 23 million comments, 31 million shares, and 335 million likes from November 1, 2023, to October 6, 2024. We find that: (i) Before Biden's withdrawal, video posts mentioning the Republican candidate (Donald Trump) have higher positive sentiment and lower negative sentiment compared to those mentioning the Democratic candidate (Joe Biden). (ii) Following Biden's withdrawal, positive sentiment in video posts mentioning the Democratic candidate (Kamala Harris) increases by 46.8%, while negative sentiment decreases by 52.0%. (iii) Regarding user engagement, before Biden's withdrawal, video posts mentioning the Democratic candidate have 64.9% higher odds of being shared and 39.5% higher odds of receiving likes compared to posts mentioning the Republican candidate, with similar odds of receiving comments. (iv) After Biden's withdrawal, the odds of being shared increase by 53.3%, and the odds of receiving likes increase by 77.4% in both video posts mentioning the Democratic candidate and video posts mentioning the Republican candidate. Our findings offer insights into how sentiments and user engagement in online posts about the 2024 U.S. presidential candidates shift following Biden's dropping out from the presidential race."}, "https://arxiv.org/abs/2410.15938": {"title": "Quantifying world geography as seen through the lens of Soviet propaganda", "link": "https://arxiv.org/abs/2410.15938", "description": "arXiv:2410.15938v1 Announce Type: new \nAbstract: Cultural data typically contains a variety of biases. In particular, geographical locations are unequally portrayed in media, creating a distorted representation of the world. Identifying and measuring such biases is crucial to understand both the data and the socio-cultural processes that have produced them. Here we suggest to measure geographical biases in a large historical news media corpus by studying the representation of cities. Leveraging ideas of quantitative urban science, we develop a mixed quantitative-qualitative procedure, which allows us to get robust quantitative estimates of the biases. These biases can be further qualitatively interpreted resulting in a hermeneutic feedback loop. We apply this procedure to a corpus of the Soviet newsreel series 'Novosti Dnya' (News of the Day) and show that city representation grows super-linearly with city size, and is further biased by city specialization and geographical location. This allows to systematically identify geographical regions which are explicitly or sneakily emphasized by Soviet propaganda and quantify their importance."}, "https://arxiv.org/abs/2410.16258": {"title": "The microscale organization of directed hypergraphs", "link": "https://arxiv.org/abs/2410.16258", "description": "arXiv:2410.16258v1 Announce Type: new \nAbstract: Many real-world complex systems are characterized by non-pairwise -- higher-order -- interactions among system's units, and can be effectively modeled as hypergraphs. Directed hypergraphs distinguish between source and target sets within each hyperedge, and allow to account for the directional flow of information between nodes. Here, we provide a framework to characterize the structural organization of directed higher-order networks at their microscale. First, we extract the fingerprint of a directed hypergraph, capturing the frequency of hyperedges with a certain source and target sizes, and use this information to compute differences in higher-order connectivity patterns among real-world systems. Then, we formulate reciprocity in hypergraphs, including exact, strong, and weak definitions, to measure to which extent hyperedges are reciprocated. Finally, we extend motif analysis to identify recurring interaction patterns and extract the building blocks of directed hypergraphs. We validate our framework on empirical datasets, including Bitcoin transactions, metabolic networks, and citation data, revealing structural principles behind the organization of real-world systems."}, "https://arxiv.org/abs/2410.14858": {"title": "Misleading Ourselves: How Disinformation Manipulates Sensemaking", "link": "https://arxiv.org/abs/2410.14858", "description": "arXiv:2410.14858v1 Announce Type: cross \nAbstract: Informal sensemaking surrounding U.S. election processes has been fraught in recent years, due to the inherent uncertainty of elections, the complexity of election processes in the U.S., and to disinformation. Based on insights from qualitative analysis of election rumors spreading online in 2020 and 2022, we introduce the concept of manipulated sensemaking to describe how disinformation functions by disrupting online audiences ability to make sense of novel, uncertain, or ambiguous information. We describe how at the core of this disruption is the ability for disinformation to shape broad, underlying stories called deep stories which determine the frames we use to make sense of this novel information. Additionally, we explain how sensemakings orientation around plausible explanations over accurate explanations makes it vulnerable to manipulation. Lastly, we demonstrate how disinformed deep stories shape sensemaking not just for a single event, but for many events in the future."}, "https://arxiv.org/abs/2410.14961": {"title": "LangGFM: A Large Language Model Alone Can be a Powerful Graph Foundation Model", "link": "https://arxiv.org/abs/2410.14961", "description": "arXiv:2410.14961v1 Announce Type: cross \nAbstract: Graph foundation models (GFMs) have recently gained significant attention. However, the unique data processing and evaluation setups employed by different studies hinder a deeper understanding of their progress. Additionally, current research tends to focus on specific subsets of graph learning tasks, such as structural tasks, node-level tasks, or classification tasks. As a result, they often incorporate specialized modules tailored to particular task types, losing their applicability to other graph learning tasks and contradicting the original intent of foundation models to be universal. Therefore, to enhance consistency, coverage, and diversity across domains, tasks, and research interests within the graph learning community in the evaluation of GFMs, we propose GFMBench-a systematic and comprehensive benchmark comprising 26 datasets. Moreover, we introduce LangGFM, a novel GFM that relies entirely on large language models. By revisiting and exploring the effective graph textualization principles, as well as repurposing successful techniques from graph augmentation and graph self-supervised learning within the language space, LangGFM achieves performance on par with or exceeding the state of the art across GFMBench, which can offer us new perspectives, experiences, and baselines to drive forward the evolution of GFMs."}, "https://arxiv.org/abs/2410.15013": {"title": "DST-TransitNet: A Dynamic Spatio-Temporal Deep Learning Model for Scalable and Efficient Network-Wide Prediction of Station-Level Transit Ridership", "link": "https://arxiv.org/abs/2410.15013", "description": "arXiv:2410.15013v1 Announce Type: cross \nAbstract: Accurate prediction of public transit ridership is vital for efficient planning and management of transit in rapidly growing urban areas in Canada. Unexpected increases in passengers can cause overcrowded vehicles, longer boarding times, and service disruptions. Traditional time series models like ARIMA and SARIMA face limitations, particularly in short-term predictions and integration of spatial and temporal features. These models struggle with the dynamic nature of ridership patterns and often ignore spatial correlations between nearby stops. Deep Learning (DL) models present a promising alternative, demonstrating superior performance in short-term prediction tasks by effectively capturing both spatial and temporal features. However, challenges such as dynamic spatial feature extraction, balancing accuracy with computational efficiency, and ensuring scalability remain.\n  This paper introduces DST-TransitNet, a hybrid DL model for system-wide station-level ridership prediction. This proposed model uses graph neural networks (GNN) and recurrent neural networks (RNN) to dynamically integrate the changing temporal and spatial correlations within the stations. The model also employs a precise time series decomposition framework to enhance accuracy and interpretability. Tested on Bogota's BRT system data, with three distinct social scenarios, DST-TransitNet outperformed state-of-the-art models in precision, efficiency and robustness. Meanwhile, it maintains stability over long prediction intervals, demonstrating practical applicability."}, "https://arxiv.org/abs/2410.15016": {"title": "Transit Pulse: Utilizing Social Media as a Source for Customer Feedback and Information Extraction with Large Language Model", "link": "https://arxiv.org/abs/2410.15016", "description": "arXiv:2410.15016v1 Announce Type: cross \nAbstract: Users of the transit system flood social networks daily with messages that contain valuable insights crucial for improving service quality. These posts help transit agencies quickly identify emerging issues. Parsing topics and sentiments is key to gaining comprehensive insights to foster service excellence. However, the volume of messages makes manual analysis impractical, and standard NLP techniques like Term Frequency-Inverse Document Frequency (TF-IDF) fall short in nuanced interpretation. Traditional sentiment analysis separates topics and sentiments before integrating them, often missing the interaction between them. This incremental approach complicates classification and reduces analytical productivity. To address these challenges, we propose a novel approach to extracting and analyzing transit-related information, including sentiment and sarcasm detection, identification of unusual system problems, and location data from social media. Our method employs Large Language Models (LLM), specifically Llama 3, for a streamlined analysis free from pre-established topic labels. To enhance the model's domain-specific knowledge, we utilize Retrieval-Augmented Generation (RAG), integrating external knowledge sources into the information extraction pipeline. We validated our method through extensive experiments comparing its performance with traditional NLP approaches on user tweet data from the real world transit system. Our results demonstrate the potential of LLMs to transform social media data analysis in the public transit domain, providing actionable insights and enhancing transit agencies' responsiveness by extracting a broader range of information."}, "https://arxiv.org/abs/2410.15174": {"title": "Crafting Tomorrow: The Influence of Design Choices on Fresh Content in Social Media Recommendation", "link": "https://arxiv.org/abs/2410.15174", "description": "arXiv:2410.15174v1 Announce Type: cross \nAbstract: The rise in popularity of social media platforms, has resulted in millions of new, content pieces being created every day. This surge in content creation underscores the need to pay attention to our design choices as they can greatly impact how long content remains relevant. In today's landscape where regularly recommending new content is crucial, particularly in the absence of detailed information, a variety of factors such as UI features, algorithms and system settings contribute to shaping the journey of content across the platform. While previous research has focused on how new content affects users' experiences, this study takes a different approach by analyzing these decisions considering the content itself.\n  Through a series of carefully crafted experiments we explore how seemingly small decisions can influence the longevity of content, measured by metrics like Content Progression (CVP) and Content Survival (CSR). We also emphasize the importance of recognizing the stages that content goes through underscoring the need to tailor strategies for each stage as a one size fits all approach may not be effective. Additionally we argue for a departure from traditional experimental setups in the study of content lifecycles, to avoid potential misunderstandings while proposing advanced techniques, to achieve greater precision and accuracy in the evaluation process."}, "https://arxiv.org/abs/2410.15212": {"title": "The Shifting Paradigm in AI : Why Generative Artificial Intelligence is the new Economic Variable", "link": "https://arxiv.org/abs/2410.15212", "description": "arXiv:2410.15212v1 Announce Type: cross \nAbstract: The relentless pursuit of technological advancements has ushered in a new era where artificial intelligence (AI) is not only a powerful tool but also a critical economic driver. At the forefront of this transformation is Generative AI, which is catalyzing a paradigm shift across industries. Deep generative models, an integration of generative and deep learning techniques, excel in creating new data beyond analyzing existing ones, revolutionizing sectors from production and manufacturing to finance. By automating design, optimization, and innovation cycles, Generative AI is reshaping core industrial processes. In the financial sector, it is transforming risk assessment, trading strategies, and forecasting, demonstrating its profound impact. This paper explores the sweeping changes driven by deep learning models like Large Language Models (LLMs), highlighting their potential to foster innovative business models, disruptive technologies, and novel economic landscapes. As we stand at the threshold of an AI-driven economic era, Generative AI is emerging as a pivotal force, driving innovation, disruption, and economic evolution on a global scale."}, "https://arxiv.org/abs/2410.15233": {"title": "A Semidefinite Relaxation Approach for Fair Graph Clustering", "link": "https://arxiv.org/abs/2410.15233", "description": "arXiv:2410.15233v1 Announce Type: cross \nAbstract: Fair graph clustering is crucial for ensuring equitable representation and treatment of diverse communities in network analysis. Traditional methods often ignore disparities among social, economic, and demographic groups, perpetuating biased outcomes and reinforcing inequalities. This study introduces fair graph clustering within the framework of the disparate impact doctrine, treating it as a joint optimization problem integrating clustering quality and fairness constraints. Given the NP-hard nature of this problem, we employ a semidefinite relaxation approach to approximate the underlying optimization problem. For up to medium-sized graphs, we utilize a singular value decomposition-based algorithm, while for larger graphs, we propose a novel algorithm based on the alternative direction method of multipliers. Unlike existing methods, our formulation allows for tuning the trade-off between clustering quality and fairness. Experimental results on graphs generated from the standard stochastic block model demonstrate the superiority of our approach in achieving an optimal accuracy-fairness trade-off compared to state-of-the-art methods."}, "https://arxiv.org/abs/2410.15969": {"title": "Fusion divided: what prevented European collaboration on controlled thermonuclear fusion in 1958", "link": "https://arxiv.org/abs/2410.15969", "description": "arXiv:2410.15969v1 Announce Type: cross \nAbstract: The European Organization for Nuclear Research (CERN) in Geneva today operates the world`s largest particle accelerator complex and is a much-cited prototype for high-profile international collaboration. What is less well known, however, is that when its first big accelerator was still under construction, CERN went through an experimental phase in terms of scientific fields and exchanges with similar organizations. This was reflected in a controversy surrounding a CERN-sponsored study group on controlled thermonuclear fusion that met from 1958 to 1964. Invited scientists from CERN member states attended these meetings, as did delegates from the European Atomic Energy Community (Euratom), the European Atomic Energy Agency (ENEA), and the US Atomic Energy Commission (AEC). Although the CERN Study Group on Fusion Problems created an international network for the exchange of reports and for better coordination of scientific projects to avoid duplication of work, it failed to materialize joint fusion research programmes. This article explains why, thereby providing insights into intergovernmental power dynamics and how those shaped the relationships between international organizations."}, "https://arxiv.org/abs/2410.16158": {"title": "Networks: The Visual Language of Complexity", "link": "https://arxiv.org/abs/2410.16158", "description": "arXiv:2410.16158v1 Announce Type: cross \nAbstract: Understanding the origins of complexity is a fundamental challenge with implications for biological and technological systems. Network theory emerges as a powerful tool to model complex systems. Networks are an intuitive framework to represent inter-dependencies among many system components, facilitating the study of both local and global properties. However, it is unclear whether we can define a universal theoretical framework for evolving networks. While basic growth mechanisms, like preferential attachment, recapitulate common properties such as the power-law degree distribution, they fall short in capturing other system-specific properties. Tinkering, on the other hand, has shown to be very successful in generating modular or nested structures \"for-free\", highlighting the role of internal, non-adaptive mechanisms in the evolution of complexity. Different network extensions, like hypergraphs, have been recently developed to integrate exogenous factors in evolutionary models, as pairwise interactions are insufficient to capture environmentally-mediated species associations. As we confront global societal and climatic challenges, the study of network and hypergraphs provides valuable insights, emphasizing the importance of scientific exploration in understanding and managing complexity."}, "https://arxiv.org/abs/2306.04733": {"title": "Epidemic spreading in group-structured populations", "link": "https://arxiv.org/abs/2306.04733", "description": "arXiv:2306.04733v3 Announce Type: replace \nAbstract: Individuals involved in common group activities/settings -- e.g., college students that are enrolled in the same class and/or live in the same dorm -- are exposed to recurrent contacts of physical proximity. These contacts are known to mediate the spread of an infectious disease, however, it is not obvious how the properties of the spreading process are determined by the structure of and the interrelation among the group settings that are at the root of those recurrent interactions. Here, we show that reshaping the organization of groups within a population can be used as an effective strategy to decrease the severity of an epidemic. Specifically, we show that when group structures are sufficiently correlated -- e.g., the likelihood for two students living in the same dorm to attend the same class is sufficiently high -- outbreaks are longer but milder than for uncorrelated group structures. Also, we show that the effectiveness of interventions for disease containment increases as the correlation among group structures increases. We demonstrate the practical relevance of our findings by taking advantage of data about housing and attendance of students at the Indiana University campus in Bloomington. By appropriately optimizing the assignment of students to dorms based on their enrollment, we are able to observe a two- to five-fold reduction in the severity of simulated epidemic processes."}, "https://arxiv.org/abs/2306.15709": {"title": "Privacy-Preserving Community Detection for Locally Distributed Multiple Networks", "link": "https://arxiv.org/abs/2306.15709", "description": "arXiv:2306.15709v2 Announce Type: replace \nAbstract: Modern multi-layer networks are commonly stored and analyzed in a local and distributed fashion because of the privacy, ownership, and communication costs. The literature on the model-based statistical methods for community detection based on these data is still limited. This paper proposes a new method for consensus community detection and estimation in a multi-layer stochastic block model using locally stored and computed network data with privacy protection. A novel algorithm named privacy-preserving Distributed Spectral Clustering (ppDSC) is developed. To preserve the edges' privacy, we adopt the randomized response (RR) mechanism to perturb the network edges, which satisfies the strong notion of differential privacy. The ppDSC algorithm is performed on the squared RR-perturbed adjacency matrices to prevent possible cancellation of communities among different layers. To remove the bias incurred by RR and the squared network matrices, we develop a two-step bias-adjustment procedure. Then we perform eigen-decomposition on the debiased matrices, aggregation of the local eigenvectors using an orthogonal Procrustes transformation, and k-means clustering. We provide theoretical analysis on the statistical errors of ppDSC in terms of eigen-vector estimation. In addition, the blessings and curses of network heterogeneity are well-explained by our bounds."}, "https://arxiv.org/abs/2309.05854": {"title": "Exploring Information Acquisition in Social Learning", "link": "https://arxiv.org/abs/2309.05854", "description": "arXiv:2309.05854v4 Announce Type: replace \nAbstract: Social learning, a fundamental process through which individuals shape their beliefs and perspectives via observation and interaction with others, is critical for the development of our society and the functioning of social governance. Prior works on social learning usually assume that the initial beliefs are given and focus on the update rule. With the recent proliferation of online social networks, there is an avalanche amount of information, which may significantly influence users' initial beliefs. In this paper, we use the rational inattention theory to model how agents acquire information to form initial beliefs and assess its influence on their adjustments in beliefs. Furthermore, we analyze the dynamic evolution of belief distribution among agents. Simulations and social experiments are conducted to validate our proposed model and analyze the impact of model parameters on belief dynamics."}, "https://arxiv.org/abs/2310.09555": {"title": "Indirect reciprocity in the public goods game with collective reputations", "link": "https://arxiv.org/abs/2310.09555", "description": "arXiv:2310.09555v2 Announce Type: replace \nAbstract: Indirect reciprocity unveils how social cooperation is founded upon moral systems. Within the frame of dyadic games based on individual reputations, the \"leading-eight\" strategies distinguish themselves in promoting and sustaining cooperation. However, in the real-world societies, there are widespread interactions at the group level, where individuals need to make a singular action choice when facing multiple individuals with different reputations. Here, through introducing the assessment of collective reputations, we develop a framework that embeds group-level reputation structure into public goods game to study the evolution of group-level indirect reciprocity. We show that changing the criteria of group assessment destabilize the reputation dynamics of leading-eight strategies. In a particular range of social assessment criteria, all leading-eight strategies can break the social dilemma in public goods games and sustain cooperation. Specifically, there exists an optimal, moderately set assessment criterion that is most conducive to promoting cooperation. Moreover, in the evolution of assessment criteria, the preference of the leading-eight strategies for social strictness is inversely correlated with the payoff level. Our work reveals the impact of social strictness on prosocial behavior, highlighting the importance of group-level interactions in the analysis of evolutionary games and complex social dynamics."}, "https://arxiv.org/abs/2401.13618": {"title": "A call for frugal modelling: two case studies involving molecular spin dynamics", "link": "https://arxiv.org/abs/2401.13618", "description": "arXiv:2401.13618v3 Announce Type: replace \nAbstract: As scientists living through a climate emergency, we have a responsibility to lead by example, or to at least be consistent with our understanding of the problem. This common goal of reducing the carbon footprint of our work can be approached through a variety of strategies. For theoreticians, this includes not only optimizing algorithms and improving computational efficiency but also adopting a frugal approach to modeling. Here we present and critically illustrate this principle. First, we compare two models of very different level of sophistication which nevertheless yield the same qualitative agreement with an experiment involving electric manipulation of molecular spin qubits while presenting a difference in cost of $>4$ orders of magnitude. As a second stage, an already minimalistic model of the potential use of single-ion magnets to implement a network of probabilistic p-bits, programmed in two different programming languages, is shown to present a difference in cost of a factor of $\\simeq 50$. In both examples, the computationally expensive version of the model was the one that was published. As a community, we still have a lot of room for improvement in this direction."}, "https://arxiv.org/abs/2312.15113": {"title": "Identifying built environment factors influencing driver yielding behavior at unsignalized intersections: A naturalistic open-source dataset collected in Minnesota", "link": "https://arxiv.org/abs/2312.15113", "description": "arXiv:2312.15113v2 Announce Type: replace-cross \nAbstract: Many factors influence the yielding result of a driver-pedestrian interaction, including traffic volume, vehicle speed, roadway characteristics, etc. While individual aspects of these interactions have been explored, comprehensive, naturalistic studies, particularly those considering the built environment's influence on driver-yielding behavior, are lacking. To address this gap, our study introduces an extensive open-source dataset, compiled from video data at 18 unsignalized intersections across Minnesota. Documenting more than 3000 interactions, this dataset provides a detailed view of driver-pedestrian interactions and over 50 distinct contextual variables. The data, which covers individual driver-pedestrian interactions and contextual factors, is made publicly available at https://github.com/tianyi17/pedestrian_yielding_data_MN.\n  Using logistic regression, we developed a classification model that predicts driver yielding based on the identified variables. Our analysis indicates that vehicle speed, the presence of parking lots, proximity to parks or schools, and the width of major road crossings significantly influence driver yielding at unsignalized intersections. Through our findings and by publishing one of the most comprehensive driver-pedestrian datasets in the United States, our study will support communities across Minnesota and the United States in their ongoing efforts to improve road safety for pedestrians and be helpful for automated vehicle design."}, "https://arxiv.org/abs/2405.05119": {"title": "Combining Rollout Designs and Clustering for Causal Inference under Low-order Interference", "link": "https://arxiv.org/abs/2405.05119", "description": "arXiv:2405.05119v2 Announce Type: replace-cross \nAbstract: Estimating causal effects under interference is pertinent to many real-world settings. Recent work with low-order potential outcomes models uses a rollout design to obtain unbiased estimators that require no interference network information. However, the required extrapolation can lead to prohibitively high variance. To address this, we propose a two-stage experiment that selects a sub-population in the first stage and restricts treatment rollout to this sub-population in the second stage. We explore the role of clustering in the first stage by analyzing the bias and variance of a polynomial interpolation-style estimator under this experimental design. Bias increases with the number of edges cut in the clustering of the interference network, but variance depends on qualities of the clustering that relate to homophily and covariate balance. There is a tension between clustering objectives that minimize the number of cut edges versus those that maximize covariate balance across clusters. Through simulations, we explore a bias-variance trade-off and compare the performance of the estimator under different clustering strategies."}, "https://arxiv.org/abs/2408.14134": {"title": "Exploring the Potential of Large Language Models for Heterophilic Graphs", "link": "https://arxiv.org/abs/2408.14134", "description": "arXiv:2408.14134v2 Announce Type: replace-cross \nAbstract: Large language models (LLMs) have presented significant opportunities to enhance various machine learning applications, including graph neural networks (GNNs). By leveraging the vast open-world knowledge within LLMs, we can more effectively interpret and utilize textual data to better characterize heterophilic graphs, where neighboring nodes often have different labels. However, existing approaches for heterophilic graphs overlook the rich textual data associated with nodes, which could unlock deeper insights into their heterophilic contexts. In this work, we explore the potential of LLMs for modeling heterophilic graphs and propose a novel two-stage framework: LLM-enhanced edge discriminator and LLM-guided edge reweighting. In the first stage, we fine-tune the LLM to better identify homophilic and heterophilic edges based on the textual content of their nodes. In the second stage, we adaptively manage message propagation in GNNs for different edge types based on node features, structures, and heterophilic or homophilic characteristics. To cope with the computational demands when deploying LLMs in practical scenarios, we further explore model distillation techniques to fine-tune smaller, more efficient models that maintain competitive performance. Extensive experiments validate the effectiveness of our framework, demonstrating the feasibility of using LLMs to enhance node classification on heterophilic graphs."}, "https://arxiv.org/abs/2410.16306": {"title": "Neutralina: promoting science and gender-equality in Latin-America", "link": "https://arxiv.org/abs/2410.16306", "description": "arXiv:2410.16306v1 Announce Type: new \nAbstract: The Covid-19 pandemic has exposed certain societal weaknesses, including the lack of scientists in the media and the readiness of the public to believe in fake news. \"Neutralina\" is a science communication personality created on Instagram (@neutralina.lu) in response to the observed need for scientific outreach done by women in Peruvian and Latin American society. The objectives of this project include normalizing the presence of women in science, fighting against stereotypes and fake news, and disseminating scientific knowledge. Neutralina has attracted a sizable young audience and is expanding beyond Instagram into other formats such as podcasts, real-life conferences, and roundtable discussions. Statistics detailing its growth will be presented, alongside strategies employed to engage the young audience."}, "https://arxiv.org/abs/2410.16603": {"title": "Efficient and Effective Algorithms for A Family of Influence Maximization Problems with A Matroid Constraint", "link": "https://arxiv.org/abs/2410.16603", "description": "arXiv:2410.16603v1 Announce Type: new \nAbstract: Influence maximization (IM) is a classic problem that aims to identify a small group of critical individuals, known as seeds, who can influence the largest number of users in a social network through word-of-mouth. This problem finds important applications including viral marketing, infection detection, and misinformation containment. The conventional IM problem is typically studied with the oversimplified goal of selecting a single seed set. Many real-world scenarios call for multiple sets of seeds, particularly on social media platforms where various viral marketing campaigns need different sets of seeds to propagate effectively. To this end, previous works have formulated various IM variants, central to which is the requirement of multiple seed sets, naturally modeled as a matroid constraint. However, the current best-known solutions for these variants either offer a weak $(1/2-\\epsilon)$-approximation, or offer a $(1-1/e-\\epsilon)$-approximation algorithm that is very expensive. We propose an efficient seed selection method called AMP, an algorithm with a $(1-1/e-\\epsilon)$-approximation guarantee for this family of IM variants. To further improve efficiency, we also devise a fast implementation, called RAMP. We extensively evaluate the performance of our proposal against 6 competitors across 4 IM variants and on 7 real-world networks, demonstrating that our proposal outperforms all competitors in terms of result quality, running time, and memory usage. We have also deployed RAMP in a real industry strength application involving online gaming, where we show that our deployed solution significantly improves upon the baselines."}, "https://arxiv.org/abs/2410.16625": {"title": "FastGEMF: Scalable High-Speed Simulation of Stochastic Spreading Processes over Complex Multilayer Networks", "link": "https://arxiv.org/abs/2410.16625", "description": "arXiv:2410.16625v1 Announce Type: new \nAbstract: Predicting the spread of processes across complex multi-layered networks has long challenged researchers due to the intricate interplay between network structure and propagation dynamics. Each layer of these networks possesses unique characteristics, further complicating analysis. To authors' knowledge, a comprehensive framework capable of simulating various spreading processes across different layers, particularly in networks with millions of nodes and connections, has been notably absent. This study introduces a novel framework that efficiently predicts Markov Chain processes over large-scale networks, while significantly reducing time and space complexity. This approach enables exact simulation of spreading processes across extensive real-world multi-layer networks, accounting for diverse influencers on each layer. FastGEMF provides a baseline framework for exact simulating stochastic spread processes, facilitating comparative analysis of models across diverse domains, from epidemiology to social media dynamics."}, "https://arxiv.org/abs/2410.16934": {"title": "Impact of Cognitive Dissonance on Social Hysteresis: Insights fromthe Expressed and Private Opinions Model", "link": "https://arxiv.org/abs/2410.16934", "description": "arXiv:2410.16934v1 Announce Type: new \nAbstract: The growing interest in models of opinion dynamics, particularly those that distinguish between private beliefs and publicly expressed opinions, spans several academic disciplines, from the social sciences to the hard sciences. They have been developed to study decision-making mechanisms and applied to many social phenomena, such as pluralistic ignorance, the spiral of silence, or preference falsification. Within these models, however, there is a notable gap in the study of social hysteresis, a concept crucial to understanding the delayed responses of societies to the rapid changes of the modern world. This research aims to fill this gap by examining the impact of cognitive dissonance on social hysteresis through an updated model of expressed and private opinions (EPOs). We study the model both analytically and through Monte Carlo simulations. To facilitate understanding and replication of our model, and to provide a resource for further exploration to a diverse interdisciplinary audience, we have made the special NetLogo implementation of the model publicly available at GitHub. By incorporating a cognitive dissonance mitigation mechanism into an agent-based q-voter-type model of Expressed and Private Opinions (EPOs), we demonstrate that this mechanism induces social hysteresis. Consequently, we argue that refraining from rationalizing publicly expressed opinions could mitigate social hysteresis and facilitate consensus, offering insights into potential strategies for managing societal responses to change."}, "https://arxiv.org/abs/2410.17151": {"title": "Mechanistic interplay between information spreading and opinion polarization", "link": "https://arxiv.org/abs/2410.17151", "description": "arXiv:2410.17151v1 Announce Type: new \nAbstract: We investigate how information-spreading mechanisms affect opinion dynamics and vice-versa via an agent-based simulation on adaptive social networks. First, we characterize the impact of reposting on user behavior with limited memory, a feature that introduces novel system states. Then, we build an experiment mimicking information-limiting environments seen on social media platforms and study how the model parameters can determine the configuration of opinions. In this scenario, different posting behaviors may sustain polarization or revert it. We further show the adaptability of the model by calibrating it to reproduce the statistical organization of information cascades as seen empirically in a microblogging social media platform."}, "https://arxiv.org/abs/2410.16386": {"title": "LEGO-Learn: Label-Efficient Graph Open-Set Learning", "link": "https://arxiv.org/abs/2410.16386", "description": "arXiv:2410.16386v1 Announce Type: cross \nAbstract: How can we train graph-based models to recognize unseen classes while keeping labeling costs low? Graph open-set learning (GOL) and out-of-distribution (OOD) detection aim to address this challenge by training models that can accurately classify known, in-distribution (ID) classes while identifying and handling previously unseen classes during inference. It is critical for high-stakes, real-world applications where models frequently encounter unexpected data, including finance, security, and healthcare. However, current GOL methods assume access to many labeled ID samples, which is unrealistic for large-scale graphs due to high annotation costs.\n  In this paper, we propose LEGO-Learn (Label-Efficient Graph Open-set Learning), a novel framework that tackles open-set node classification on graphs within a given label budget by selecting the most informative ID nodes. LEGO-Learn employs a GNN-based filter to identify and exclude potential OOD nodes and then select highly informative ID nodes for labeling using the K-Medoids algorithm. To prevent the filter from discarding valuable ID examples, we introduce a classifier that differentiates between the C known ID classes and an additional class representing OOD nodes (hence, a C+1 classifier). This classifier uses a weighted cross-entropy loss to balance the removal of OOD nodes while retaining informative ID nodes. Experimental results on four real-world datasets demonstrate that LEGO-Learn significantly outperforms leading methods, with up to a 6.62% improvement in ID classification accuracy and a 7.49% increase in AUROC for OOD detection."}, "https://arxiv.org/abs/2410.16882": {"title": "Large Language Model-based Augmentation for Imbalanced Node Classification on Text-Attributed Graphs", "link": "https://arxiv.org/abs/2410.16882", "description": "arXiv:2410.16882v1 Announce Type: cross \nAbstract: Node classification on graphs frequently encounters the challenge of class imbalance, leading to biased performance and posing significant risks in real-world applications. Although several data-centric solutions have been proposed, none of them focus on Text-Attributed Graphs (TAGs), and therefore overlook the potential of leveraging the rich semantics encoded in textual features for boosting the classification of minority nodes. Given this crucial gap, we investigate the possibility of augmenting graph data in the text space, leveraging the textual generation power of Large Language Models (LLMs) to handle imbalanced node classification on TAGs. Specifically, we propose a novel approach called LA-TAG (LLM-based Augmentation on Text-Attributed Graphs), which prompts LLMs to generate synthetic texts based on existing node texts in the graph. Furthermore, to integrate these synthetic text-attributed nodes into the graph, we introduce a text-based link predictor to connect the synthesized nodes with the existing nodes. Our experiments across multiple datasets and evaluation metrics show that our framework significantly outperforms traditional non-textual-based data augmentation strategies and specific node imbalance solutions. This highlights the promise of using LLMs to resolve imbalance issues on TAGs."}, "https://arxiv.org/abs/2307.03546": {"title": "Critical fragility in socio-technical systems", "link": "https://arxiv.org/abs/2307.03546", "description": "arXiv:2307.03546v3 Announce Type: replace \nAbstract: Socio-technical systems, where technological and human elements interact in a goal-oriented manner, provide important functional support to our societies. Here we draw attention to the underappreciated concept of timeliness -- i.e., system elements being available at the right place at the right time -- that has been ubiquitously and integrally adopted as a quality standard in the \\textit{modus operandi\\/} of socio-technical systems. We point out that a variety of incentives, often reinforced by competitive pressures, prompt system operators to myopically optimize for efficiencies, running the risk of inadvertently taking timeliness to the limit of its operational performance, correspondingly making the system critically fragile to perturbations by pushing the entire system towards the proverbial `edge of a cliff'. Invoking a stylized model for operational delays, we identify the limiting operational performance of timeliness, as a true critical point, where the smallest of perturbations can lead to a systemic collapse. Specifically for firm-to-firm production networks, we suggest that the proximity to \\textit{critical fragility\\/} is an important ingredient for understanding the fundamental ``excess volatility puzzle'' in economics. Further, in generality for optimizing socio-technical systems, we propose that critical fragility is a crucial aspect in managing the trade-off between efficiency and robustness."}, "https://arxiv.org/abs/2404.08236": {"title": "Interest Maximization in Social Networks", "link": "https://arxiv.org/abs/2404.08236", "description": "arXiv:2404.08236v2 Announce Type: replace \nAbstract: Nowadays, organizations use viral marketing strategies to promote their products through social networks. It is expensive to directly send the product promotional information to all the users in the network. In this context, Kempe et al. \\cite{kempe2003maximizing} introduced the Influence Maximization (IM) problem, which identifies $k$ most influential nodes (spreader nodes), such that the maximum number of people in the network adopts the promotional message.\n  Many variants of the IM problem have been studied in the literature, namely, Perfect Evangelising Set (PES), Perfect Awareness Problem (PAP), etc. In this work, we propose a maximization version of PAP called the \\IM{} problem. Different people have different levels of interest in a particular product. This is modeled by assigning an interest value to each node in the network. Then, the problem is to select $k$ initial spreaders such that the sum of the interest values of the people (nodes) who become aware of the message is maximized.\n  We study the \\IM{} problem under two popular diffusion models: the Linear Threshold Model (LTM) and the Independent Cascade Model (ICM). We show that the \\IM{} problem is NP-Hard under LTM. We give linear programming formulation for the problem under LTM. We propose four heuristic algorithms for the \\IM{} problem: \\LBE{} (\\LB{}), Maximum Degree First Heuristic (\\MD{}), \\PBE{} (\\PB{}), and Maximum Profit Based Greedy Heuristic (\\MP{}). Extensive experimentation has been carried out on many real-world benchmark data sets for both diffusion models. The results show that among the proposed heuristics, \\MP{} performs better in maximizing the interest value."}, "https://arxiv.org/abs/2410.17282": {"title": "Drivers of Electric Vehicle Adoption in Nigeria: An Extended UTAUT Framework Approach", "link": "https://arxiv.org/abs/2410.17282", "description": "arXiv:2410.17282v1 Announce Type: new \nAbstract: Electric vehicles (EVs) represent a significant advancement in automotive technology, utilizing electricity as a power source instead of traditional fossil fuels, while incorporating sophisticated navigation and autopilot systems. These vehicles align with multiple Sustainable Development Goals (SDGs) by offering a more environmentally sustainable alternative to internal combustion engine vehicles (ICEVs). Despite their potential, the adoption of EVs in developing nations such as Nigeria remains constrained. This study expands the Unified Theory of Acceptance and Use of Technology (UTAUT) framework by incorporating key enablers, including poor infrastructure, affordability issues, and government support, within the broader category of facilitating conditions. Additionally, it examines factors such as trust, performance expectations, social influences, and network externalities to identify the primary determinants influencing Nigerian consumers' propensity to adopt EVs. Results show that the percentage increase of H6 (facilitating conditions - behavioral intentions) compared to H5 (network externalities - behavioral intentions) is approximately 32.35%, indicating that traditional drivers significantly influence individuals' willingness to purchase EVs and are particularly strong factors in adoption decisions. The paper concludes with a discussion of these findings and proposes strategies for future research to further explore the barriers and drivers of EV adoption in Nigeria."}, "https://arxiv.org/abs/2410.17284": {"title": "American society keeps a lid on the number of deaths from guns and car accidents but not from mass shootings", "link": "https://arxiv.org/abs/2410.17284", "description": "arXiv:2410.17284v1 Announce Type: new \nAbstract: The number of deaths from car accidents and from the unlawful use of guns can be described by logistic growth curves. The annual rates of both have traced completed logistic trajectories following which they have been self-regulated for many decades at what seems to be a homeostatic equilibrium level through legislative actions. Exception constitutes the number of deaths from mass shootings, which has been so far tracing an exponential trajectory. Despite the fact that mass-shooting deaths represent only 0.1 percent of all gun deaths today, they are poised to continue growing exponentially until they become the major cause of gun deaths, short of unprecedented action by society."}, "https://arxiv.org/abs/2410.17285": {"title": "The Role of Community Building and Education as Key Pillar of Institutionalizing Responsible Quantum", "link": "https://arxiv.org/abs/2410.17285", "description": "arXiv:2410.17285v1 Announce Type: new \nAbstract: Quantum computing is an emerging technology whose positive and negative impacts on society are not yet fully known. As government, individuals, institutions, and corporations fund and develop this technology, they must ensure that they anticipate its impacts, prepare for its consequences, and steer its development in such a way that it enables the most good and prevents the most harm. However, individual stakeholders are not equipped to fully anticipate these consequences on their own it requires a diverse community that is well-informed about quantum computing and its impacts. Collaborations and community-building across domains incorporating a variety of viewpoints, especially those from stakeholders most likely to be harmed, are fundamental pillars of developing and deploying quantum computing responsibly. This paper reviews responsible quantum computing proposals and literature, highlights the challenges in implementing these, and presents strategies developed at IBM aimed at building a diverse community of users and stakeholders to support the responsible development of this technology."}, "https://arxiv.org/abs/2410.17286": {"title": "From Phytochemicals to Recipes: Health Indications and Culinary Uses of Herbs and Spices", "link": "https://arxiv.org/abs/2410.17286", "description": "arXiv:2410.17286v1 Announce Type: new \nAbstract: Herbs and spices each contain about 3000 phytochemicals on average and there is much traditional knowledge on their health benefits. However, there is a lack of systematic study to understand the relationship among herbs and spices, their phytochemical constituents, their potential health benefits, and their usage in regional cuisines. Here we use a network-based approach to elucidate established relationships and predict novel associations between the phytochemicals present in herbs and spices with health indications. Our top 100 inferred indication-phytochemical relationships rediscover 40% known relationships and 20% that have been inferred via gene-chemical interactions with high confidence. The remaining 40% are hypotheses generated in a principled way for further experimental investigations. We also develop an algorithm to find the minimum set of spices needed to cover a target group of health conditions. Drawing on spice usage patterns in several regional Indian cuisines, and a copy-mutate model for regional cuisine evolution, we characterize the spectrum of health conditions covered by existing regional cuisines. The spectrum of health conditions can expand through the nationalization/globalization of culinary practice."}, "https://arxiv.org/abs/2410.17287": {"title": "Metrics for Assessing Inclusivity and Empowerment of People for Supporting the Design of Inclusive Product Lifecycles", "link": "https://arxiv.org/abs/2410.17287", "description": "arXiv:2410.17287v1 Announce Type: new \nAbstract: Design of an inclusive product lifecycle is important for empowering people (stakeholders) with their meaningful inclusion in lifecycle processes. The aim is to use this as an enabler for transition to sustainability by balancing the power relations among the stakeholders. Design of an inclusive product lifecycle for empowerment requires that the nature of inclusion of stakeholders in the lifecycle is such that it leads to their empowerment. Empowerment processes provide opportunities for people to increase their power to sustain the development of inclusive product lifecycles. Analysing power relations is to balance the amount of power of stakeholders with their inclusion in different functions in an inclusive lifecycle design. Inclusivity addresses the context of the lifecycle process to determine who can be included in which phases of the lifecycle and the diversity of people to be empowered. We apply a novel empowerment and inclusivity framework to a series of real-life case studies from literature to identify the major dimensions of empowerment and inclusivity. By analysing the relationships between the dimensions of empowerment and inclusivity, we propose specific metrics for inclusivity and empowerment that have strong causal connections, indicating the kinds of inclusion that should lead to greater empowerment in product lifecycles."}, "https://arxiv.org/abs/2410.17364": {"title": "Opinion dynamics model of collaborative learning", "link": "https://arxiv.org/abs/2410.17364", "description": "arXiv:2410.17364v1 Announce Type: new \nAbstract: We propose a simple model to explore an educational phenomenon where the correct answer emerges from group discussion. We construct our model based on several plausible assumptions: (i) We tend to follow peers' opinions. However, if a peer's opinion is too different from yours, you are not much influenced. In other words, your opinion tends to align with peers' opinions, weighted by the similarity to yours. (ii) Discussion among group members helps the opinion to shift toward the correct answer even when the group members do not know it clearly. However, if everyone tells exactly the same, you often get lost and it becomes more difficult to find the correct answer. In other words, you can find the correct answer when everyone has largely different voices. (iii) We are sometimes stuck to our past. If you keep one opinion for a long time, such a memory works like an inertia in classical mechanics. We use our model to perform numerical investigations and find that the performance of a group is enhanced when initial opinions are diverse, that a lower memory capacity makes consensus occur faster, and that a small group size, typically three or four, is beneficial for better group performance."}, "https://arxiv.org/abs/2410.17367": {"title": "Generalizing Geometric Partition Entropy for the Estimation of Mutual Information in the Presence of Informative Outliers", "link": "https://arxiv.org/abs/2410.17367", "description": "arXiv:2410.17367v1 Announce Type: new \nAbstract: The recent introduction of geometric partition entropy brought a new viewpoint to non-parametric entropy quantification that incorporated the impacts of informative outliers, but its original formulation was limited to the context of a one-dimensional state space. A generalized definition of geometric partition entropy is now provided for samples within a bounded (finite measure) region of a d-dimensional vector space. The basic definition invokes the concept of a Voronoi diagram, but the computational complexity and reliability of Voronoi diagrams in high dimension make estimation by direct theoretical computation unreasonable. This leads to the development of approximation schemes that enable estimation that is faster than current methods by orders of magnitude. The partition intersection ($\\pi$) approximation, in particular, enables direct estimates of marginal entropy in any context resulting in an efficient and versatile mutual information estimator. This new measure-based paradigm for data driven information theory allows flexibility in the incorporation of geometry to vary the representation of outlier impact, which leads to a significant broadening in the applicability of established entropy-based concepts. The incorporation of informative outliers is illustrated through analysis of transient dynamics in the synchronization of coupled chaotic dynamical systems."}, "https://arxiv.org/abs/2410.17390": {"title": "Revealing The Secret Power: How Algorithms Can Influence Content Visibility on Social Media", "link": "https://arxiv.org/abs/2410.17390", "description": "arXiv:2410.17390v1 Announce Type: new \nAbstract: Online social media platforms significantly influence public debates by shaping the information users encounter. Content visibility on these platforms is regulated by recommendation algorithms designed to maximize user engagement using individual-level data, including personal preferences and interactions. These algorithms play a crucial role in information dissemination, yet their inner workings are often undisclosed, raising concerns about potential manipulation of visibility. While algorithms may be intended to limit the spread of harmful content, they can also be exploited to suppress dissenting voices without users' awareness. The suspicion that platforms deliberately reduce the visibility of certain users or content - commonly known as shadow banning - has garnered significant public attention, with numerous figures advocating for greater transparency around this practice. In this study, we perform a quantitative study geared to identify suspicious changes in content visibility on Twitter (now known as X). We build and study a dataset of over 13 million tweets from more than 5 million users discussing the Ukraine conflict, including each tweet's number of views and metadata, aiming to detect reduced or inflated visibility patterns. We investigate how visibility correlates with factors such as authors' stance, role, interaction networks, and content narratives. Our findings reveal significant variations in visibility, likely driven by algorithmic interventions. These results highlight the need for greater transparency in regulating online information ecosystems to prevent algorithmic manipulation that could undermine public discourse and the fairness of debates."}, "https://arxiv.org/abs/2410.17410": {"title": "Learning Graph Filters for Structure-Function Coupling based Hub Node Identification", "link": "https://arxiv.org/abs/2410.17410", "description": "arXiv:2410.17410v1 Announce Type: new \nAbstract: Over the past two decades, tools from network science have been leveraged to characterize the organization of both structural and functional networks of the brain. One such measure of network organization is hub node identification. Hubs are specialized nodes within a network that link distinct brain units corresponding to specialized functional processes. Conventional methods for identifying hub nodes utilize different types of centrality measures and participation coefficient to profile various aspects of nodal importance. These methods solely rely on the functional connectivity networks constructed from functional magnetic resonance imaging (fMRI), ignoring the structure-function coupling in the brain. In this paper, we introduce a graph signal processing (GSP) based hub detection framework that utilizes both the structural connectivity and the functional activation to identify hub nodes. The proposed framework models functional activity as graph signals on the structural connectivity. Hub nodes are then detected based on the premise that hub nodes are sparse, have higher level of activity compared to their neighbors, and the non-hub nodes' activity can be modeled as the output of a graph-based filter. Based on these assumptions, an optimization framework, GraFHub, is formulated to learn the coefficients of the optimal polynomial graph filter and detect the hub nodes. The proposed framework is evaluated on both simulated data and resting state fMRI (rs-fMRI) data from Human Connectome Project (HCP)."}, "https://arxiv.org/abs/2410.17496": {"title": "Measuring Network Dynamics of Opioid Overdose Deaths in the United States", "link": "https://arxiv.org/abs/2410.17496", "description": "arXiv:2410.17496v1 Announce Type: new \nAbstract: The US opioid overdose epidemic has been a major public health concern in recent decades. There has been increasing recognition that its etiology is rooted in part in the social contexts that mediate substance use and access; however, reliable statistical measures of social influence are lacking in the literature. We use Facebook's social connectedness index (SCI) as a proxy for real-life social networks across diverse spatial regions that help quantify social connectivity across different spatial units. This is a measure of the relative probability of connections between localities that offers a unique lens to understand the effects of social networks on health outcomes. We use SCI to develop a variable, called \"deaths in social proximity\", to measure the influence of social networks on opioid overdose deaths (OODs) in US counties. Our results show a statistically significant effect size for deaths in social proximity on OODs in counties in the United States, controlling for spatial proximity, as well as demographic and clinical covariates. The effect size of standardized deaths in social proximity in our cluster-robust linear regression model indicates that a one-standard-deviation increase, equal to 11.70 more deaths per 100,000 population in the social proximity of ego counties in the contiguous United States, is associated with thirteen more deaths per 100,000 population in ego counties. To further validate our findings, we performed a series of robustness checks using a network autocorrelation model to account for social network effects, a spatial autocorrelation model to capture spatial dependencies, and a two-way fixed-effect model to control for unobserved spatial and time-invariant characteristics. These checks consistently provide statistically robust evidence of positive social influence on OODs in US counties."}, "https://arxiv.org/abs/2410.17507": {"title": "Detecting fake review buyers using network structure: Direct evidence from Amazon", "link": "https://arxiv.org/abs/2410.17507", "description": "arXiv:2410.17507v1 Announce Type: new \nAbstract: Online reviews significantly impact consumers' decision-making process and firms' economic outcomes and are widely seen as crucial to the success of online markets. Firms, therefore, have a strong incentive to manipulate ratings using fake reviews. This presents a problem that academic researchers have tried to solve over two decades and on which platforms expend a large amount of resources. Nevertheless, the prevalence of fake reviews is arguably higher than ever. To combat this, we collect a dataset of reviews for thousands of Amazon products and develop a general and highly accurate method for detecting fake reviews. A unique difference between previous datasets and ours is that we directly observe which sellers buy fake reviews. Thus, while prior research has trained models using lab-generated reviews or proxies for fake reviews, we are able to train a model using actual fake reviews. We show that products that buy fake reviews are highly clustered in the product-reviewer network. Therefore, features constructed from this network are highly predictive of which products buy fake reviews. We show that our network-based approach is also successful at detecting fake reviews even without ground truth data, as unsupervised clustering methods can accurately identify fake review buyers by identifying clusters of products that are closely connected in the network. While text or metadata can be manipulated to evade detection, network-based features are more costly to manipulate because these features result directly from the inherent limitations of buying reviews from online review marketplaces, making our detection approach more robust to manipulation."}, "https://arxiv.org/abs/2410.17870": {"title": "Analysis of Parallel Boarding Methods in a Multi-Aisle Flying Wing Aircraft", "link": "https://arxiv.org/abs/2410.17870", "description": "arXiv:2410.17870v1 Announce Type: new \nAbstract: We examine the speed of different boarding methods in a proposed Flying Wing aircraft design with four aisles using an agent-based model. We study the effect of various passenger movement variables on the boarding process. We evaluate the impact of these factors on the boarding time when the boarding process runs sequentially and in parallel with the aisles of the Flying Wing layout. Then, we analyze the impact of an increase in the number of aisles on the relative speed of all boarding methods and conclude that methods utilizing boarding of the separate aisles simultaneously (parallel boarding) converge to the fastest boarding time given by the Steffen method. With parallel boarding of the aisles the relative advantage of the Steffen method compared to Windows-Middle-Aisle (WMA) or Back-to-front boarding decreases, from being 1.6-2.1 times as fast to being approximately equal for our fiducial Flying Wing seating arrangement. Standard methods such as Back-to-front or WMA are about twice as fast to board a four-aisle Flying Wing plane, compared to a single-aisle aircraft with the same number of passengers. We also investigate the difference between the optimal approach to parallel boarding, where consecutive passengers always enter separate aisles, and a less optimal but more practical approach. The practical approach is only up to 1.06 times slower than the optimal, meaning that the advantages of parallel boarding can be utilized without resorting to impractical boarding methods. Hence, the introduction of multiple aisles into aircraft seating design offers the prospect of significantly decreasing the boarding time for passengers, without the introduction of inconvenient boarding methods."}, "https://arxiv.org/abs/2410.17894": {"title": "Partially Proportional and Adaptive Similarity Indices", "link": "https://arxiv.org/abs/2410.17894", "description": "arXiv:2410.17894v1 Announce Type: new \nAbstract: A good deal of science and technology concepts and methods rely on comparing and relating entities in quantitative terms. Among the several possible approaches, similarity indices allow some interesting features, especially the ability to quantify how much two entities resemble one another. In this work, the Jaccard similarity for comparing non-zero real-valued vectors is modified so as to estimate similarity while focusing on the distinct parts of the signals. The resulting operator, which is called partially proportional similarity index, not only allows more strict comparisons, but also paves the way to develop an adaptive approach to similarity estimation in which the size and orientation of the comparisons adapt to those of a respective calibration field expressing how the observed features are related to original counterparts. Being a particularly relevant concept in data analysis and modeling, emphasis is placed on presenting and discussing the concept of calibration field and how they can be taken into account while performing similarity comparisons. Several results are described which illustrate the potential of the reported concepts and approaches for enhancing, and even simultaneously normalizing to some extent the representation of entities in terms of their features, as frequently required in scientific modeling and pattern recognition."}, "https://arxiv.org/abs/2410.17996": {"title": "An evolutionary game theory approach to modeling behavioral interaction in disclosing infection begins with an outbreak: COVID-19 as an example", "link": "https://arxiv.org/abs/2410.17996", "description": "arXiv:2410.17996v1 Announce Type: new \nAbstract: The global impact of the COVID-19 pandemic on the livelihoods of people worldwide prompted the implementation of a range of preventive measures at local, national, and international levels. Early in the outbreak, before the vaccine became accessible, voluntary quarantine and social isolation emerged as crucial strategies to curb the spread of infection. In this research, we present a game-theoretic model to elucidate the voluntary disclosure of exposure to infected individuals within communities. By employing a fractional derivative approach to illustrate disease propagation within the compartmental model, we determine the minimum level of voluntary disclosure required to disrupt the chain of transmission and allow the epidemic to fade. Our findings suggest that higher transmission rates and increased perceived severity of infection change the externality of disclosing infected exposure, thereby contributing to a rise in the proportion of individuals opting for quarantine and reducing disease incidence. We estimate behavioral parameters and transmission rates by fitting the model to hospitalized cases in Chile, South America. Results from our paper underscore the potential for public health authorities to influence and regulate voluntary disclosure of infection during emerging outbreaks through effective risk communication, emphasizing the severity of the disease, and providing accurate information about hospital capacity to the public."}, "https://arxiv.org/abs/2410.18004": {"title": "Dynamic models of gentrification", "link": "https://arxiv.org/abs/2410.18004", "description": "arXiv:2410.18004v1 Announce Type: new \nAbstract: The phenomenon of gentrification of an urban area is characterized by the displacement of lower-income residents due to rising living costs and an influx of wealthier individuals. This study presents an agent-based model that simulates urban gentrification through the relocation of three income groups -- low, middle, and high -- driven by living costs. The model incorporates economic and sociological theories to generate realistic neighborhood transition patterns. We introduce a temporal network-based measure to track the outflow of low-income residents and the inflow of middle- and high-income residents over time. Our experiments reveal that high-income residents trigger gentrification and that our network-based measure consistently detects gentrification patterns earlier than traditional count-based methods, potentially serving as an early detection tool in real-world scenarios. Moreover, the analysis also highlights how city density promotes gentrification. This framework offers valuable insights for understanding gentrification dynamics and informing urban planning and policy decisions."}, "https://arxiv.org/abs/2410.18012": {"title": "MiniFed : Integrating LLM-based Agentic-Workflow for Simulating FOMC Meeting", "link": "https://arxiv.org/abs/2410.18012", "description": "arXiv:2410.18012v1 Announce Type: new \nAbstract: The Federal Funds rate in the United States plays a significant role in both domestic and international financial markets. However, research has predominantly focused on the effects of adjustments to the Federal Funds rate rather than on the decision-making process itself. Recent advancements in large language models(LLMs) offer a potential method for reconstructing the original FOMC meetings, which are responsible for setting the Federal Funds rate. In this paper, we propose a five-stage FOMC meeting simulation framework, MiniFed, which employs LLM agents to simulate real-world FOMC meeting members and optimize the FOMC structure. This framework effectively revitalizes the FOMC meeting process and facilitates projections of the Federal Funds rate. Experimental results demonstrate that our proposed MiniFed framework achieves both high accuracy in Federal Funds rate projections and behavioral alignment with the agents' real-world counterparts. Given that few studies have focused on employing LLM agents to simulate large-scale real-world conferences, our work can serve as a benchmark for future developments."}, "https://arxiv.org/abs/2410.17290": {"title": "Disease Outbreak Detection and Forecasting: A Review of Methods and Data Sources", "link": "https://arxiv.org/abs/2410.17290", "description": "arXiv:2410.17290v1 Announce Type: cross \nAbstract: Infectious diseases occur when pathogens from other individuals or animals infect a person, resulting in harm to both individuals and society as a whole. The outbreak of such diseases can pose a significant threat to human health. However, early detection and tracking of these outbreaks have the potential to reduce the mortality impact. To address these threats, public health authorities have endeavored to establish comprehensive mechanisms for collecting disease data. Many countries have implemented infectious disease surveillance systems, with the detection of epidemics being a primary objective. The clinical healthcare system, local/state health agencies, federal agencies, academic/professional groups, and collaborating governmental entities all play pivotal roles within this system. Moreover, nowadays, search engines and social media platforms can serve as valuable tools for monitoring disease trends. The Internet and social media have become significant platforms where users share information about their preferences and relationships. This real-time information can be harnessed to gauge the influence of ideas and societal opinions, making it highly useful across various domains and research areas, such as marketing campaigns, financial predictions, and public health, among others. This article provides a review of the existing standard methods developed by researchers for detecting outbreaks using time series data. These methods leverage various data sources, including conventional data sources and social media data or Internet data sources. The review particularly concentrates on works published within the timeframe of 2015 to 2022."}, "https://arxiv.org/abs/2410.17587": {"title": "Predicting Company Growth by Econophysics informed Machine Learning", "link": "https://arxiv.org/abs/2410.17587", "description": "arXiv:2410.17587v1 Announce Type: cross \nAbstract: Predicting company growth is crucial for strategic adjustment, operational decision-making, risk assessment, and loan eligibility reviews. Traditional models for company growth often focus too much on theory, overlooking practical forecasting, or they rely solely on time series forecasting techniques, ignoring interpretability and the inherent mechanisms of company growth. In this paper, we propose a machine learning-based prediction framework that incorporates an econophysics model for company growth. Our model captures both the intrinsic growth mechanisms of companies led by scaling laws and the fluctuations influenced by random factors and individual decisions, demonstrating superior predictive performance compared with methods that use time series techniques alone. Its advantages are more pronounced in long-range prediction tasks. By explicitly modeling the baseline growth and volatility components, our model is more interpretable."}, "https://arxiv.org/abs/2211.06533": {"title": "Adaptive Joint Estimation of Temporal Vertex and Edge Signals", "link": "https://arxiv.org/abs/2211.06533", "description": "arXiv:2211.06533v3 Announce Type: replace \nAbstract: The adaptive estimation of coexisting temporal vertex (node) and edge signals on graphs is a critical task when a change in edge signals influences the temporal dynamics of the vertex signals. However, the current Graph Signal Processing algorithms mostly consider only the signals existing on the graph vertices and have neglected the fact that signals can reside on the edges. We propose an Adaptive Joint Vertex-Edge Estimation (AJVEE) algorithm for jointly estimating time-varying vertex and edge signals through a time-varying regression, incorporating both vertex signal filtering and edge signal filtering. Accompanying AJVEE is a newly proposed Adaptive Least Mean Square procedure based on the Hodge Laplacian (ALMS-Hodge), which is inspired by classical adaptive filters combining simplicial filtering and simplicial regression. AJVEE is able to operate jointly on the vertices and edges by merging two ALMS-Hodge specified on the vertices and edges into a unified formulation. A more generalized case extending AJVEE beyond the vertices and edges is being discussed. Experimenting on real-world traffic networks and population mobility networks, we have confirmed that our proposed AJVEE algorithm could accurately and jointly track time-varying vertex and edge signals on graphs."}, "https://arxiv.org/abs/2403.18857": {"title": "The Negative Participation Paradox in Three-Candidate Instant Runoff Elections", "link": "https://arxiv.org/abs/2403.18857", "description": "arXiv:2403.18857v3 Announce Type: replace \nAbstract: In this paper, we provide theoretical and empirical estimates for the likelihood of a negative participation paradox under instant runoff voting in three-candidate elections. We determine the probability of the paradox and related conditional probabilities based on the impartial anonymous culture and impartial culture models for both complete and partial ballots. We compare these results to the empirical likelihood of a negative participation paradox occurring under instant runoff voting in a large database of voter profiles which have been reduced to three candidates. Lastly, we analyze the relative likelihood of this paradox in comparison to other well-known paradoxes."}, "https://arxiv.org/abs/2410.18235": {"title": "Massive Genealogies Distinguish Frontier from Steady-State Internal Migration", "link": "https://arxiv.org/abs/2410.18235", "description": "arXiv:2410.18235v1 Announce Type: new \nAbstract: Recent studies of human migration have focused on modern issues of international economics, politics, urbanization, or commuting. Here we make use of very large anonymized genealogies which offer quantitative metrics and models before census data became available. In European and North American data from 1400 to 1950 we find two distinct patterns of lifetime migration. The steady-state pattern shows a universal power-law distribution of migration distance; by its early appearance it cannot be dependent on post-industrial technology. The frontier pattern, in contrast, is not scale-free with its much longer average distances. All migration distances are well fit by a three parameter model; the temporal and geographic patterns of the fitted parameters give new insight to American internal expansion 1620-1950. Frontier migration is also highly directional and asymmetric; gravity models do not apply. The American frontier pattern arose from the colonial-era steady-state within a generation, plateaued for three generations, then returned to a more mobile steady-state, a sequence paralleled by the Steppe migrations that brought the Bronze Age to Neolithic Europe. The transient frontier pattern is enabled by large-scale technological or numeric imbalance and geographic opportunity; when these forces abate, a new steady-state begins."}, "https://arxiv.org/abs/2410.18253": {"title": "Exploring Network Structure with the Density of States", "link": "https://arxiv.org/abs/2410.18253", "description": "arXiv:2410.18253v1 Announce Type: new \nAbstract: Community detection, as well as the identification of other structures like core periphery and disassortative patterns, is an important topic in network analysis. While most methods seek to find the best partition of the network according to some criteria, there is a body of results that suggest that a single network can have many good but distinct partitions. In this paper we introduce the density of states as a tool for studying the space of all possible network partitions. We demonstrate how to use the well known Wang-Landau method to compute a network's density of states. We show that, even using modularity to measure quality, the density of states can still rule out spurious structure in random networks and overcome resolution limits. We demonstrate how these methods can be used to find `building blocks', groups of nodes which are consistently found together in detected communities. This suggests an approach to partitioning based on exploration of the network's structure landscape rather than optimisation."}, "https://arxiv.org/abs/2410.18492": {"title": "Enhancing Information Diffusion Prediction by Addressing Noise in Social Connection Data", "link": "https://arxiv.org/abs/2410.18492", "description": "arXiv:2410.18492v1 Announce Type: new \nAbstract: With the increasing use of online social media platforms, information diffusion has become a prevalent phenomenon, making Information Diffusion Prediction (IDP) critical for various applications. Many studies utilize social connection data to improve prediction performance. However, previous research has largely overlooked the issue of noise within social connection data, which often contains irrelevant or misleading connections due to the diversity of social relationships and the presence of weak ties. Such noise can lead to inaccurate learning of user preferences and negatively affect prediction outcomes. To address this issue, we propose DIDP, a novel social connection denoising framework for information diffusion prediction. First, we introduce a graph learning encoder module that encodes the information diffusion hypergraph and social graph to obtain latent diffusion embeddings and social embeddings. Next, DIDP integrates a denoising diffusion module to adaptively remove different types of noise from the learned social embeddings in the latent space. Through multi-step noise injection and removal, the framework enhances the ability to learn robust embeddings. Additionally, we introduce a cross-domain contrastive learning module that maximizes the mutual information between the diffusion embeddings and the denoised social embeddings of the same user. This module guides the denoising process and facilitates cross-domain knowledge transfer. Experimental results show that DIDP significantly outperforms state-of-the-art methods in the information diffusion prediction task."}, "https://arxiv.org/abs/2410.18508": {"title": "Potemkin village of Russian science: The case of JINR", "link": "https://arxiv.org/abs/2410.18508", "description": "arXiv:2410.18508v1 Announce Type: new \nAbstract: This document reviews international cooperation of the Joint Institute of Nuclear Research (JINR). It shows that of the original 11 founding members of the JINR, only one -- Mongolia -- is still fully active within the organization. Out of 16 existing members four have their participation suspended. The review of the Topical Plan for JINR Research and International Cooperation 2024 shows that the number of 990 institutes collaborating with JINR is overestimated by at least 10% and probably closer to 25%. Countries with the highest number of participating institutes listed for 2024 are Russia, USA and Italy with 203, 77 and 49 institutes, respectively. JINR collaboration with impostor institutes on the occupied territories of Ukraine is presented as well as JINR publications which associate the occupied Ukrainian territories to Russia. JINR cooperation with organizations in Russia, which are under sanctions for their support of Russia's invasion of Ukraine and contribution to the war effort, is also discussed."}, "https://arxiv.org/abs/2410.18670": {"title": "Health Misinformation in Social Networks: A Survey of IT Approaches", "link": "https://arxiv.org/abs/2410.18670", "description": "arXiv:2410.18670v1 Announce Type: new \nAbstract: In this paper, we present a comprehensive survey on the pervasive issue of medical misinformation in social networks from the perspective of information technology. The survey aims at providing a systematic review of related research and helping researchers and practitioners navigate through this fast-changing field. Specifically, we first present manual and automatic approaches for fact-checking. We then explore fake news detection methods, using content, propagation features, or source features, as well as mitigation approaches for countering the spread of misinformation. We also provide a detailed list of several datasets on health misinformation and of publicly available tools. We conclude the survey with a discussion on the open challenges and future research directions in the battle against health misinformation."}, "https://arxiv.org/abs/2410.18742": {"title": "Continuous Dynamic Modeling via Neural ODEs for Popularity Trajectory Prediction", "link": "https://arxiv.org/abs/2410.18742", "description": "arXiv:2410.18742v1 Announce Type: new \nAbstract: Popularity prediction for information cascades has significant applications across various domains, including opinion monitoring and advertising recommendations. While most existing methods consider this as a discrete problem, popularity actually evolves continuously, exhibiting rich dynamic properties such as change rates and growth patterns. In this paper, we argue that popularity trajectory prediction is more practical, as it aims to forecast the entire trajectory of how popularity unfolds over arbitrary future time. This approach offers insights into both instantaneous popularity and the underlying dynamic properties. However, traditional methods for popularity trajectory prediction primarily rely on specific diffusion mechanism assumptions, which may not align well with real-world dynamics and compromise their performance. To address these limitations, we propose NODEPT, a novel approach based on neural ordinary differential equations (ODEs) for popularity trajectory prediction. NODEPT models the continuous dynamics of the underlying diffusion system using neural ODEs. We first employ an encoder to initialize the latent state representations of information cascades, consisting of two representation learning modules that capture the co-evolution structural characteristics and temporal patterns of cascades from different perspectives. More importantly, we then introduce an ODE-based generative module that learns the dynamics of the diffusion system in the latent space. Finally, a decoder transforms the latent state into the prediction of the future popularity trajectory. Our experimental results on three real-world datasets demonstrate the superiority and rationality of the proposed NODEPT method."}, "https://arxiv.org/abs/2410.18803": {"title": "Language-Agnostic Modeling of Source Reliability on Wikipedia", "link": "https://arxiv.org/abs/2410.18803", "description": "arXiv:2410.18803v1 Announce Type: new \nAbstract: Over the last few years, content verification through reliable sources has become a fundamental need to combat disinformation. Here, we present a language-agnostic model designed to assess the reliability of sources across multiple language editions of Wikipedia. Utilizing editorial activity data, the model evaluates source reliability within different articles of varying controversiality such as Climate Change, COVID-19, History, Media, and Biology topics. Crafting features that express domain usage across articles, the model effectively predicts source reliability, achieving an F1 Macro score of approximately 0.80 for English and other high-resource languages. For mid-resource languages, we achieve 0.65 while the performance of low-resource languages varies; in all cases, the time the domain remains present in the articles (which we dub as permanence) is one of the most predictive features. We highlight the challenge of maintaining consistent model performance across languages of varying resource levels and demonstrate that adapting models from higher-resource languages can improve performance. This work contributes not only to Wikipedia's efforts in ensuring content verifiability but in ensuring reliability across diverse user-generated content in various language communities."}, "https://arxiv.org/abs/2410.18898": {"title": "Aging and memory effects in social and economic dynamics", "link": "https://arxiv.org/abs/2410.18898", "description": "arXiv:2410.18898v1 Announce Type: new \nAbstract: In this doctoral thesis, we investigate the complex interplay between temporal dynamics associated with aging and memory and their effects on social and economic systems. To do so, we combine theoretical modeling, to explore the aging implications in threshold (peer pressure) models, and empirical analysis, to address the impact of temporal and spatial patterns in real complex systems, taking housing market as a case study."}, "https://arxiv.org/abs/2410.18393": {"title": "SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness", "link": "https://arxiv.org/abs/2410.18393", "description": "arXiv:2410.18393v1 Announce Type: cross \nAbstract: Social media is often the first place where communities discuss the latest societal trends. Prior works have utilized this platform to extract epidemic-related information (e.g. infections, preventive measures) to provide early warnings for epidemic prediction. However, these works only focused on English posts, while epidemics can occur anywhere in the world, and early discussions are often in the local, non-English languages. In this work, we introduce the first multilingual Event Extraction (EE) framework SPEED++ for extracting epidemic event information for a wide range of diseases and languages. To this end, we extend a previous epidemic ontology with 20 argument roles; and curate our multilingual EE dataset SPEED++ comprising 5.1K tweets in four languages for four diseases. Annotating data in every language is infeasible; thus we develop zero-shot cross-lingual cross-disease models (i.e., training only on English COVID data) utilizing multilingual pre-training and show their efficacy in extracting epidemic-related events for 65 diverse languages across different diseases. Experiments demonstrate that our framework can provide epidemic warnings for COVID-19 in its earliest stages in Dec 2019 (3 weeks before global discussions) from Chinese Weibo posts without any training in Chinese. Furthermore, we exploit our framework's argument extraction capabilities to aggregate community epidemic discussions like symptoms and cure measures, aiding misinformation detection and public attention monitoring. Overall, we lay a strong foundation for multilingual epidemic preparedness."}, "https://arxiv.org/abs/2401.11588": {"title": "Synergistic signatures of group mechanisms in higher-order systems", "link": "https://arxiv.org/abs/2401.11588", "description": "arXiv:2401.11588v2 Announce Type: replace \nAbstract: The interplay between causal mechanisms and emerging collective behaviors is a central aspect of understanding, controlling, and predicting complex networked systems. In our work, we investigate the relationship between higher-order mechanisms and higher-order behavioral observables in two representative models with group interactions: a simplicial Ising model and a social contagion model. In both systems, we find that group (higher-order) interactions show emergent synergistic (higher-order) behavior. The emergent synergy appears only at the group level and depends in a complex, non-linear way on the trade-off between the strengths of the low- and higher-order mechanisms and is invisible to low-order behavioral observables. Our work sets the basis for systematically investigating the relation between causal mechanisms and behavioral patterns in complex networked systems with group interactions, offering a robust methodological framework to tackle this challenging task."}, "https://arxiv.org/abs/2403.03050": {"title": "The microscopic origin of abrupt transitions in interdependent systems", "link": "https://arxiv.org/abs/2403.03050", "description": "arXiv:2403.03050v2 Announce Type: replace \nAbstract: Phase transitions are fundamental features of statistical physics. While the well-studied continuous phase transitions are known to be controlled by external \\textit{macroscopic} changes in the order parameter, the origin of abrupt transitions is not yet clear. Here we show that abrupt phase transitions may occur due to a unique internal \\textit{microscopic} cascading mechanism, resulting from dependency interactions. We experimentally unveil the underlying mechanism of the abrupt transition in interdependent superconducting networks to be governed by a unique metastable state of a long-living resistance cascading plateau. This plateau is characterized by spontaneous \\textit{microscopic} changes that last for \\textit{thousands} of seconds, followed by a \\textit{macroscopic} phase shift of the system. Similar microscopic mechanisms are expected to be found in a variety of systems showing abrupt transitions."}, "https://arxiv.org/abs/2403.18191": {"title": "The process of polarisation as a loss of dimensionality: measuring changes in polarisation using Singular Value Decomposition of network graphs", "link": "https://arxiv.org/abs/2403.18191", "description": "arXiv:2403.18191v2 Announce Type: replace \nAbstract: In this paper we present new methods that extend Baldassari and Gelman's theory of polarisation. They show that it is useful to define polarisation as increasing correlation between positions in an ideological field, which reduces political pluralism. We also draw from post-structuralist work which argues that deliberate development of these correlations is a feature of polarised regimes such as apartheid.\n  To measure polarisation in social networks, we use Random Dot Product Graphs to embed social networks in metric spaces. Singular Value Decomposition of a social network provides an embedded dimensionality which corresponds to the number of uncorrelated dimensions in the network. Each uncorrelated dimension in a social network represents a part of that society which allows two people from different groups to form a social connection, such as living in a racially integrated neighbourhood. A decrease in the optimal dimensionality for the embedding of the network graph means that the dimensions in the network are becoming more correlated, and therefore the network is becoming more polarised.\n  We apply this method to the communication interactions among New Zealand Twitter users discussing climate change issues from 2017 to 2023. We find that the discussion is more polarised after 2020 than before, as shown by a decrease in the dimensionality of the communication network. Second, we apply this method to discussions of the COP climate change conferences, showing that our methods agree with other researchers' detections of polarisation in this space. Finally, we use networks generated by stochastic block models to explore how an increase of the isolation between distinct communities, or the increase of the predominance of one community over the other, in the social networks decrease the embedded dimensionality and are therefore identifiable as polarisation processes."}, "https://arxiv.org/abs/2312.00626": {"title": "Forecasting trends in food security with real time data", "link": "https://arxiv.org/abs/2312.00626", "description": "arXiv:2312.00626v3 Announce Type: replace-cross \nAbstract: Early warning systems are an essential tool for effective humanitarian action. Advance warnings on impending disasters facilitate timely and targeted response which help save lives and livelihoods. In this work we present a quantitative methodology to forecast levels of food consumption for 60 consecutive days, at the sub-national level, in four countries: Mali, Nigeria, Syria, and Yemen. The methodology is built on publicly available data from the World Food Programme's global hunger monitoring system which collects, processes, and displays daily updates on key food security metrics, conflict, weather events, and other drivers of food insecurity. In this study we assessed the performance of various models including Autoregressive Integrated Moving Average (ARIMA), Extreme Gradient Boosting (XGBoost), Long Short Term Memory (LSTM) Network, Convolutional Neural Network (CNN), and Reservoir Computing (RC), by comparing their Root Mean Squared Error (RMSE) metrics. Our findings highlight Reservoir Computing as a particularly well-suited model in the field of food security given both its notable resistance to over-fitting on limited data samples and its efficient training capabilities. The methodology we introduce establishes the groundwork for a global, data-driven early warning system designed to anticipate and detect food insecurity."}, "https://arxiv.org/abs/2410.19064": {"title": "From a Tiny Slip to a Giant Leap: An LLM-Based Simulation for Fake News Evolution", "link": "https://arxiv.org/abs/2410.19064", "description": "arXiv:2410.19064v1 Announce Type: new \nAbstract: With the growing spread of misinformation online, research has increasingly focused on detecting and tracking fake news. However, an overlooked issue is that fake news does not naturally exist in social networks -- it often originates from distorted facts or deliberate fabrication by malicious actors. Understanding how true news gradually evolves into fake news is critical for early detection and prevention, reducing its spread and impact. Hence, in this paper, we take the first step toward simulating and revealing this evolution, proposing a Fake News evolUtion Simulation framEwork (FUSE) based on large language models (LLMs). Specifically, we employ LLM as agents to represent individuals in a simulated social network. We define four types of agents commonly observed in daily interactions: spreaders, who propagate information; commentators, who provide opinions and interpretations; verifiers, who check the accuracy of information; and bystanders, who passively observe without engaging. For simulated environments, we model various social network structures, such as high-clustering networks and scale-free networks, to mirror real-world network dynamics. Each day, the agents engage in belief exchanges, reflect on their thought processes, and reintroduce the news accordingly. Given the lack of prior work in this area, we developed a FUSE-EVAL evaluation framework to measure the deviation from true news during the fake news evolution process. The results show that FUSE successfully captures the underlying patterns of how true news transforms into fake news and accurately reproduces previously discovered instances of fake news, aligning closely with human evaluations. Moreover, our work provides insights into the fact that combating fake news should not be delayed until it has fully evolved; instead, prevention in advance is key to achieving better outcomes."}, "https://arxiv.org/abs/2410.19177": {"title": "Sentiment-Driven Community Detection in a Network of Perfume Preferences", "link": "https://arxiv.org/abs/2410.19177", "description": "arXiv:2410.19177v1 Announce Type: new \nAbstract: Network analysis is increasingly important across various fields, including the fragrance industry, where perfumes are represented as nodes and shared user preferences as edges in perfume networks. Community detection can uncover clusters of similar perfumes, providing insights into consumer preferences, enhancing recommendation systems, and informing targeted marketing strategies.\n  This study aims to apply community detection techniques to group perfumes favored by users into relevant clusters for better recommendations. We constructed a bipartite network from user reviews on the Persian retail platform \"Atrafshan,\" with nodes representing users and perfumes, and edges formed by positive comments. This network was transformed into a Perfume Co-Preference Network, connecting perfumes liked by the same users. By applying community detection algorithms, we identified clusters based on shared preferences, enhancing our understanding of user sentiment in the fragrance market.\n  To improve sentiment analysis, we integrated emojis and a user voting system for greater accuracy. Emojis, aligned with their Persian counterparts, captured the emotional tone of reviews, while user ratings for scent, longevity, and sillage refined sentiment classification. Edge weights were adjusted by combining adjacency values with user ratings in a 60:40 ratio, reflecting both connection strength and user preferences. These enhancements led to improved modularity of detected communities, resulting in more accurate perfume groupings.\n  This research pioneers the use of community detection in perfume networks, offering new insights into consumer preferences. Our advancements in sentiment analysis and edge weight refinement provide actionable insights for optimizing product recommendations and marketing strategies in the fragrance industry."}, "https://arxiv.org/abs/2410.19199": {"title": "Making Social Platforms Accessible: Emotion-Aware Speech Generation with Integrated Text Analysis", "link": "https://arxiv.org/abs/2410.19199", "description": "arXiv:2410.19199v1 Announce Type: new \nAbstract: Recent studies have outlined the accessibility challenges faced by blind or visually impaired, and less-literate people, in interacting with social networks, in-spite of facilitating technologies such as monotone text-to-speech (TTS) screen readers and audio narration of visual elements such as emojis. Emotional speech generation traditionally relies on human input of the expected emotion together with the text to synthesise, with additional challenges around data simplification (causing information loss) and duration inaccuracy, leading to lack of expressive emotional rendering. In real-life communications, the duration of phonemes can vary since the same sentence might be spoken in a variety of ways depending on the speakers' emotional states or accents (referred to as the one-to-many problem of text to speech generation). As a result, an advanced voice synthesis system is required to account for this unpredictability. We propose an end-to-end context-aware Text-to-Speech (TTS) synthesis system that derives the conveyed emotion from text input and synthesises audio that focuses on emotions and speaker features for natural and expressive speech, integrating advanced natural language processing (NLP) and speech synthesis techniques for real-time applications. Our system also showcases competitive inference time performance when benchmarked against the state-of-the-art TTS models, making it suitable for real-time accessibility applications."}, "https://arxiv.org/abs/2410.19214": {"title": "A Comprehensive Analysis of Social Tie Strength: Definitions, Prediction Methods, and Future Directions", "link": "https://arxiv.org/abs/2410.19214", "description": "arXiv:2410.19214v1 Announce Type: new \nAbstract: The rapid growth of online social networks has underscored the importance of understanding the intensity of user relationships, referred to as \"tie strength.\" Over the past few decades, extensive efforts have been made to assess tie strength in networks. However, the lack of ground-truth tie strength labels and the differing perspectives on tie strength among researchers have complicated the development of effective prediction methods for real-world applications. In our study, we first categorize mainstream understandings of tie strength into seven standardized definitions and verify their effectiveness by investigating the class distributions and correlations across these definitions. We also draw key insights into tie resilience from the perspective of tie dissolution that (1) stronger ties are more resilient than weaker ones, and (2) this tie resiliency ratio increases as the network evolves. We then conduct extensive experiments to evaluate existing tie strength prediction methods under these definitions, revealing that (1) neural network methods capable of learning from semantic features hold great potential for high performance, (2) models struggle under definitions that offer limited understandings of tie strength in the network, (3) existing models face imbalance issues that cannot be addressed by traditional quantity imbalance techniques, and (4) different definitions of tie strength allow for the inference of not only the current state but also the future state of a tie. Building on these findings, we propose strategies to improve existing methods and suggest several promising directions for future research."}, "https://arxiv.org/abs/2410.19440": {"title": "Strategic deployment of solar photovoltaics for achieving self-sufficiency in Europe throughout the energy transition", "link": "https://arxiv.org/abs/2410.19440", "description": "arXiv:2410.19440v1 Announce Type: new \nAbstract: Transition pathways for Europe to achieve carbon neutrality emphasize the need for a massive deployment of solar and wind energy. Global cost optimization would lead to installing most of the renewable capacity in a few resource-rich countries, but policy decisions could prioritize other factors. In this study, we focus on the effect of energy independence on Europe's energy system design. We show that self-sufficiency constraints lead to a more equitable distribution of costs and installed capacities across Europe. However, countries that typically depend on energy imports face cost increases of up to 150% to achieve complete self-sufficiency. Self-sufficiency particularly favours solar photovoltaic (PV) energy, and with declining PV module prices, alternative configurations like inverter dimensioning and horizontal tracking are beneficial enough to be part of the optimal solution for many countries. Moreover, we found that very large solar and wind annual installation rates are required, but they seem feasible in light of recent historical trends."}, "https://arxiv.org/abs/2410.19495": {"title": "Beyond One Solution: The Case for a Comprehensive Exploration of Solution Space in Community Detection", "link": "https://arxiv.org/abs/2410.19495", "description": "arXiv:2410.19495v1 Announce Type: new \nAbstract: This article explores the importance of examining the solution space in community detection, highlighting its role in achieving reliable results when dealing with real-world problems. A Bayesian framework is used to estimate the stability of the solution space and classify it into categories Single, Dominant, Multiple, Sparse or Empty. By applying this approach to real-world networks, the study highlights the importance of considering multiple solutions rather than relying on a single partition. This ensures more reliable results and efficient use of computational resources in community detection analysis."}, "https://arxiv.org/abs/2410.19556": {"title": "Mapping leadership and communities in EU-funded research through network analysis", "link": "https://arxiv.org/abs/2410.19556", "description": "arXiv:2410.19556v1 Announce Type: new \nAbstract: Horizon 2020 and Horizon Europe the EU programs supporting research and innovation through collaboration between companies, academic institutions, and research organisations. This paper introduces a novel methodology using open data on Horizon programs to analyse collaborations, leadership roles, and their evolution, with a focus on the North Adriatic Hydrogen Valley project in the hydrogen energy sector.\n  The methodology employs network analysis, transforming tabular data into weighted networks that represent collaborations between organisations. Centrality measures and community detection algorithms identify influential organisations and stable partnerships over time. To ensure robust and reliable results, the methodology addresses challenges such as input-ordering bias and result variability, while the exploration of the solution space enhances the accuracy of identified collaboration patterns.\n  The case study reveals key leaders and stable communities within the hydrogen energy sector, providing valuable insights for policymakers and organisations fostering innovation through sustained collaborations. The proposed methodology effectively identifies influential organisations and tracks the stability of research collaborations. The insights gained are valuable for policymakers and organisations seeking to foster innovation through sustained partnerships. This approach can be extended to other sectors, offering a framework for understanding the impact of EU research funding on collaboration and leadership dynamics."}, "https://arxiv.org/abs/2410.19651": {"title": "Community detection on directed networks with missing edges", "link": "https://arxiv.org/abs/2410.19651", "description": "arXiv:2410.19651v1 Announce Type: new \nAbstract: Identifying significant community structures in networks with incomplete data is a challenging task, as the reliability of solutions diminishes with increasing levels of missing information. However, in many empirical contexts, some information about the uncertainty in the network measurements can be estimated. In this work, we extend the recently developed Flow Stability framework, originally designed for detecting communities in time-varying networks, to address the problem of community detection in weighted, directed networks with missing links. Our approach leverages known uncertainty levels in nodes' out-degrees to enhance the robustness of community detection. Through comparisons on synthetic networks and a real-world network of messaging channels on the Telegram platform, we demonstrate that our method delivers more reliable community structures, even when a significant portion of data is missing."}, "https://arxiv.org/abs/2410.19150": {"title": "A Test of Time: Predicting the Sustainable Success of Online Collaboration in Wikipedia", "link": "https://arxiv.org/abs/2410.19150", "description": "arXiv:2410.19150v1 Announce Type: cross \nAbstract: The Internet has significantly expanded the potential for global collaboration, allowing millions of users to contribute to collective projects like Wikipedia. While prior work has assessed the success of online collaborations, most approaches are time-agnostic, evaluating success without considering its longevity. Research on the factors that ensure the long-term preservation of high-quality standards in online collaboration is scarce. In this study, we address this gap. We propose a novel metric, `Sustainable Success,' which measures the ability of collaborative efforts to maintain their quality over time. Using Wikipedia as a case study, we introduce the SustainPedia dataset, which compiles data from over 40K Wikipedia articles, including each article's sustainable success label and more than 300 explanatory features such as edit history, user experience, and team composition. Using this dataset, we develop machine learning models to predict the sustainable success of Wikipedia articles. Our best-performing model achieves a high AU-ROC score of 0.88 on average. Our analysis reveals important insights. For example, we find that the longer an article takes to be recognized as high-quality, the more likely it is to maintain that status over time (i.e., be sustainable). Additionally, user experience emerged as the most critical predictor of sustainability. Our analysis provides insights into broader collective actions beyond Wikipedia (e.g., online activism, crowdsourced open-source software), where the same social dynamics that drive success on Wikipedia might play a role. We make all data and code used for this study publicly available for further research."}, "https://arxiv.org/abs/2410.19193": {"title": "Enriching GNNs with Text Contextual Representations for Detecting Disinformation Campaigns on Social Media", "link": "https://arxiv.org/abs/2410.19193", "description": "arXiv:2410.19193v1 Announce Type: cross \nAbstract: Disinformation on social media poses both societal and technical challenges. While previous studies have integrated textual information into propagation networks, they have yet to fully leverage the advancements in Transformer-based language models for high-quality contextual text representations. This work investigates the impact of incorporating textual features into Graph Neural Networks (GNNs) for fake news detection. Our experiments demonstrate that contextual representations improve performance by 9.3% in Macro F1 over static ones and 33.8% over GNNs without textual features. However, noisy data augmentation degrades performance and increases instability. We expect our methodology to open avenues for further research, and all code is made publicly available."}, "https://arxiv.org/abs/2410.19205": {"title": "Overcoming Non-Submodularity: Constant Approximation for Network Immunization", "link": "https://arxiv.org/abs/2410.19205", "description": "arXiv:2410.19205v1 Announce Type: cross \nAbstract: Given a network with an ongoing epidemic, the network immunization problem seeks to identify a fixed number of nodes to immunize in order to maximize the number of infections prevented. One of the fundamental computational challenges in network immunization is that the objective function is generally neither submodular nor supermodular. As a result, no efficient algorithm is known to consistently find a solution with a constant approximation guarantee. Traditionally, this problem is addressed using proxy objectives, which offer better approximation properties. However, converting to these indirect optimizations often introduces losses in effectiveness. In this paper, we overcome these fundamental barriers by utilizing the underlying stochastic structures of the diffusion process. Similar to the traditional influence objective, the immunization objective is an expectation that can be expressed as the sum of objectives over deterministic instances. However, unlike the former, some of these terms are not submodular. The key step is proving that this sum has a bounded deviation from submodularity, thereby enabling the greedy algorithm to achieve constant factor approximation. We show that this approximation still stands considering a variety of immunization settings and spread models."}, "https://arxiv.org/abs/2410.19272": {"title": "Coordinated Reply Attacks in Influence Operations: Characterization and Detection", "link": "https://arxiv.org/abs/2410.19272", "description": "arXiv:2410.19272v1 Announce Type: cross \nAbstract: Coordinated reply attacks are a tactic observed in online influence operations and other coordinated campaigns to support or harass targeted individuals, or influence them or their followers. Despite its potential to influence the public, past studies have yet to analyze or provide a methodology to detect this tactic. In this study, we characterize coordinated reply attacks in the context of influence operations on Twitter. Our analysis reveals that the primary targets of these attacks are influential people such as journalists, news media, state officials, and politicians.\n  We propose two supervised machine-learning models, one to classify tweets to determine whether they are targeted by a reply attack, and one to classify accounts that reply to a targeted tweet to determine whether they are part of a coordinated attack. The classifiers achieve AUC scores of 0.88 and 0.97, respectively. These results indicate that accounts involved in reply attacks can be detected, and the targeted accounts themselves can serve as sensors for influence operation detection."}, "https://arxiv.org/abs/2410.19333": {"title": "Most Swiss-system tournaments are unfair: Evidence from chess", "link": "https://arxiv.org/abs/2410.19333", "description": "arXiv:2410.19333v1 Announce Type: cross \nAbstract: Swiss-system is an increasingly popular tournament format as it provides an attractive trade-off between the number of matches and ranking accuracy. However, few empirical research consider the optimal design of the Swiss-system. We contribute to this issue by investigating the fairness of Swiss-system chess competitions with an odd number of rounds, where half of the players have an extra game with white pieces. They are proven to enjoy a significant advantage and to be overrepresented among both the highest-ranked and outperforming players. Therefore, Swiss-system tournaments should have an even number of rounds and use a pairing mechanism that guarantees a balanced colour assignment."}, "https://arxiv.org/abs/2410.19532": {"title": "The replacement number dynamics in SIR-type epidemic models I: From SSISS to RND picture", "link": "https://arxiv.org/abs/2410.19532", "description": "arXiv:2410.19532v1 Announce Type: cross \nAbstract: In SIR-type epidemic models time derivative of prevalence $I$ can always be cast into the form $\\dot{I}=(X-1)I$, where $X$ is the replacement number and recovery rate is normalized to one. Assuming $\\dot{X}=f(X,I)$ for some smooth function $f$ defines a \"replacement number dynamics\" (RND). Choosing transmission coefficients $\\beta_1>\\beta_2$, any such system uniquely maps to an isomorphic \"SSISS model\", i.e. an abstract SIR-type 3-compartment model. Extending to negative values $\\beta_i<0$ takes care of demographic dynamics with compartment dependent birth and death rates. Fixing $f$ and varying $\\beta_i$ generates a family of isomorphic SSISS systems, the \"SSISS fiber\" $\\cal{F}(f)$}. A symmetry group $G_\\sigma$ acts freely and transitively on fibers $\\cal{F}(f)$, so SSISS systems become a principal $G_\\sigma$-fiber bundle over the space of RND systems. Choosing two specific 6-parameter polynomials $f$ at most quadratic in $(X,I)$ covers a large class of models in the literature, with in total up to 17 (largely redundant!) parameters, including also reactive social behavior models. Epidemiological admissibility conditions guarantee forward boundedness and absence of periodic solutions in all these models. Part II of this work will prove existence and stability properties of endemic equilibria, which in RND picture simply boils down to analyzing zeros of the parabolas $f|_{I=0}$ and $f|_{X=1}$. This will cover and extend well known results for a wide range of models by a unifying approach while also closing some open issues in the literature."}, "https://arxiv.org/abs/2410.19685": {"title": "The Sound of Silence in Social Networks", "link": "https://arxiv.org/abs/2410.19685", "description": "arXiv:2410.19685v1 Announce Type: cross \nAbstract: We generalize the classic multi-agent DeGroot model for opinion dynamics to incorporate the Spiral of Silence theory from political science. This theory states that individuals may withhold their opinions when they perceive them to be in the minority. As in the DeGroot model, a community of agents is represented as a weighted directed graph whose edges indicate how much agents influence one another. However, agents whose current opinions are in the minority become silent (i.e., they do not express their opinion). Two models for opinion update are then introduced. In the memoryless opinion model ($\\mbox{SOM}^-$), agents update their opinion by taking the weighted average of their non-silent neighbors' opinions. In the memory based opinion model ($\\mbox{SOM}^+$), agents update their opinions by taking the weighted average of the opinions of all their neighbors, but for silent neighbors, their most recent opinion is considered.\n  We show that for $\\mbox{SOM}^-$ convergence to consensus is guaranteed for clique graphs but, unlike for the classic DeGroot, not guaranteed for strongly-connected aperiodic graphs. In contrast, we show that for $\\mbox{SOM}^+$ convergence to consensus is not guaranteed even for clique graphs. We showcase our models through simulations offering experimental insights that align with key aspects of the Spiral of Silence theory. These findings reveal the impact of silence dynamics on opinion formation and highlight the limitations of consensus in more nuanced social models."}, "https://arxiv.org/abs/2410.19864": {"title": "Thermostatistical Evaluation of Economic Activity", "link": "https://arxiv.org/abs/2410.19864", "description": "arXiv:2410.19864v1 Announce Type: new \nAbstract: We present an analysis of Bogot'{a}'s sports sector through thermostatistical models applied to economic systems. The study investigates the cross-price elasticity of income ($\\lambda$) to determine whether sports services in Bogot'{a} are normal or inferior goods. Analyzing data from the Sports Satellite Account of Bogot'{a} (CSDB) from 2018 to 2022, we find that demand for sports services is highly elastic, particularly during economic upturns, indicating they are seen as normal or luxury goods. We also calculate the partition function, entropy, and heat capacity, showing consistency with the Boltzmann Principle, which indicates a strong correlation between microstates and the macroeconomic state, supporting the statistical thermodynamic framework. Furthermore, the study employs geometrothermodynamics to assess system stability using Kretschmann and Ricci scalars to identify economic singularities, especially during the pandemic, highlighting its disruptive impact. This approach provides a nuanced understanding of system stability and the effects of external shocks like COVID-19 on the economic structure. Our analysis demonstrates that Bogot'{a}'s sports sector responds elastically to GDP changes, with stability influenced by various macroeconomic factors. However, a decline in heat capacity as economic temperature rises suggests potential growth limitations, necessitating further research to fully grasp the sector's long-term outlook."}, "https://arxiv.org/abs/2410.19873": {"title": "Forecasting the Mix of World Energy Needs by mid-21st Century", "link": "https://arxiv.org/abs/2410.19873", "description": "arXiv:2410.19873v1 Announce Type: new \nAbstract: The logistic function is used to forecast energy consumed worldwide. The logistic substitution model is used to describe the energy mix since 1965 presenting a picture significantly different from the one covering the previous 100 years. In the new picture the share of heavy pollutants, i.e. coal plus oil, keeps declining systematically in favor of natural gas and renewables (wind, geothermal, solar, biomass, and waste), the share of which grows rapidly. The shares of these three energy sources (coal plus oil, natural gas, and renewables) are poised to reach around 30 percent each by the mid-21st century. Nuclear and hydroelectric energy, both with rather stable shares, are responsible for the remaining 10 percent, which goes mostly to hydroelectric.\n  Zooming into the composition of renewables we find that today's dominant wind power is about to begin losing share to solar energy, which will overtake wind after 2024 and account for more than 90 percent of all renewables by mid-21st century, by which time geothermal, biomass, and other renewable energy sources will have dropped to insignificant levels.\n  Forecasts in exajouls are given for all energy sources up to 2050."}, "https://arxiv.org/abs/2410.19966": {"title": "Maximizing User Engagement in Social Networks: A Game-Theoretic Approach to Network Participation and Resource Sharing", "link": "https://arxiv.org/abs/2410.19966", "description": "arXiv:2410.19966v1 Announce Type: new \nAbstract: We propose a game-theoretic framework to model and optimize user engagement in cooperative activities over social networks. While traditional diffusion models suggest that individuals are only influenced by their neighbors, empirical evidence shows that diffusion alone does not fully explain network evolution, and non-diffusion factors play a significant role in network growth. We model network participation and resource-sharing as strategic games involving boundedly rational players to address this gap between the analytical models and empirical evidence. Specifically, we employ Log-Linear Learning (LLL), a version of noisy best response, to capture players' decision-making strategies. By incorporating stochastic decision models like LLL, our framework integrates both diffusion and non-diffusion dynamics into network evolution dynamics. Through equilibrium analysis and simulations, we demonstrate that our model aligns with theoretical predictions from existing analytical frameworks and empirical observations across various initial network configurations. Our second contribution is a novel method for selecting anchor nodes to enhance user participation. This approach allows system designers to identify anchor nodes and compute their incentives in real time under a more realistic information requirement constraints as compared to the existing approaches. The proposed approach adapts to changing network conditions by reallocating resources from less impactful to more influential nodes. Furthermore, the method is resilient to anchor node failures, ensuring sustained and continuous network participation."}, "https://arxiv.org/abs/2410.20070": {"title": "hateUS -- Analysis, impact of Social media use and Hate speech over University Student platforms: Case study, Problems, and Solutions", "link": "https://arxiv.org/abs/2410.20070", "description": "arXiv:2410.20070v1 Announce Type: new \nAbstract: The use of social media applications, hate speech engagement, and public debates among teenagers, primarily by university and college students, is growing day by day. The feelings of tremendous stress, anxiety, and depression via social media among our youths have a direct impact on their daily lives and personal workspace apart from delayed sleep, social media addictions, and memory loss. The use of NO phone times and NO phone zones is now popular in workplaces and family cultures. The use of hate speech, negotiations, and toxic words can lead to verbal abuse and cybercrime. Growing concern of mobile device security, cyberbullying, ransomware attacks, and mental health issues are another serious impact of social media among university students. The future challenges including health issues of social media use and hate speech has a serious impact on livelihood, freedom, and diverse communities of university students. Our case study is related to social media use and hate speech related to public debates over university students. We have presented the analysis and impact of social media and hate speech with several conclusions, cybercrimes, and components. The use of questionnaires for collecting primary data over university students help in the analysis of case study. The conclusion of case study and future scope of the research is extremely important to counter negative impacts."}, "https://arxiv.org/abs/2410.20170": {"title": "Cyberbullying or just Sarcasm? Unmasking Coordinated Networks on Reddit", "link": "https://arxiv.org/abs/2410.20170", "description": "arXiv:2410.20170v1 Announce Type: new \nAbstract: With the rapid growth of social media usage, a common trend has emerged where users often make sarcastic comments on posts. While sarcasm can sometimes be harmless, it can blur the line with cyberbullying, especially when used in negative or harmful contexts. This growing issue has been exacerbated by the anonymity and vast reach of the internet, making cyberbullying a significant concern on platforms like Reddit. Our research focuses on distinguishing cyberbullying from sarcasm, particularly where online language nuances make it difficult to discern harmful intent. This study proposes a framework using natural language processing (NLP) and machine learning to differentiate between the two, addressing the limitations of traditional sentiment analysis in detecting nuanced behaviors. By analyzing a custom dataset scraped from Reddit, we achieved a 95.15% accuracy in distinguishing harmful content from sarcasm. Our findings also reveal that teenagers and minority groups are particularly vulnerable to cyberbullying. Additionally, our research uncovers coordinated graphs of groups involved in cyberbullying, identifying common patterns in their behavior. This research contributes to improving detection capabilities for safer online communities."}, "https://arxiv.org/abs/2410.20237": {"title": "The dynamics of strategic voting: pathways to consensus and gridlock", "link": "https://arxiv.org/abs/2410.20237", "description": "arXiv:2410.20237v1 Announce Type: new \nAbstract: The outcomes of democratic elections rest on individuals' decision-making that is driven by their varying preferences and beliefs. Individuals may prefer consensus to gridlock, or gridlock to consensus, and information may be fractured via echo-chambers. To understand the role of these factors in whether or not elections reach consensus, we develop and explore a computational model in which voters have varying party affiliations, preferences, beliefs, and voting strategies. Voters may change their voting strategies either by imitating others or reconsidering their strategy individually. Preferences are orderings of the following election outcomes: a voter's party winning a super-majority, the opposing party winning such a majority, and gridlock. Voters beliefs and decisions are shaped by their social networks, and thus are heterogeneous in the population. We observe a \"tipping point\" phenomenon wherein the voters' initial strategies and randomness impact whether the minority party voters vote to create gridlock or consensus. A positive feedback loop secures such voters into one behaviour or the other. Consensus is reached by the minority party evolving to prefer consensus, which in turn drives the majority to also prefer consensus due to the influence of social learning. Further, consensus is promoted by an uneven distribution of party affiliation, and undermined when it is even. We also find that a moderate prevalence or strength of echo-chambers can boost consensus, since they can quell voters' desires for gridlock."}, "https://arxiv.org/abs/2410.20350": {"title": "Beyond Trivial Edges: A Fractional Approach to Cohesive Subgraph Detection in Hypergraphs", "link": "https://arxiv.org/abs/2410.20350", "description": "arXiv:2410.20350v1 Announce Type: new \nAbstract: Hypergraphs serve as a powerful tool for modeling complex relationships across domains like social networks, transactions, and recommendation systems. The (k,g)-core model effectively identifies cohesive subgraphs by assessing internal connections and co-occurrence patterns, but it is susceptible to inflated cohesiveness due to trivial hyperedges. To address this, we propose the $(k,g,p)$-core model, which incorporates the relative importance of hyperedges for more accurate subgraph detection. We develop both Na\\\"ive and Advanced pruning algorithms, demonstrating through extensive experiments that our approach reduces the execution frequency of costly operations by 51.9% on real-world datasets."}, "https://arxiv.org/abs/2410.20378": {"title": "Massive Retail Location Choice as a Human Flow-Covering Problem", "link": "https://arxiv.org/abs/2410.20378", "description": "arXiv:2410.20378v1 Announce Type: new \nAbstract: In this article we reframe the classic problem of massive location choice for retail chains, introducing an alternative approach. Traditional methodologies of massive location choice models encounter limitations rooted in assumptions such as power-law distance decay and oversimplified travel patterns. In response, we present a spatial operations research model aimed at maximizing customer coverage, using massive individual trajectories as a \"sampling\" of human flows, and thus the model is robust. Formulating the retail location selection problem as a set-covering problem, we propose a greedy solution. Through a case study in Shenzhen utilizing real-world individual trajectory data, our approach demonstrates substantial improvements over prevailing location choices."}, "https://arxiv.org/abs/2410.20491": {"title": "Nanopore DNA Sequencing Technology: A Sociological Perspective", "link": "https://arxiv.org/abs/2410.20491", "description": "arXiv:2410.20491v1 Announce Type: new \nAbstract: Nanopore sequencing, a next-generation sequencing technology, holds the potential to revolutionize multiple facets of life sciences, forensics, and healthcare. While previous research has focused on its technical intricacies and biomedical applications, this paper offers a unique perspective by scrutinizing the societal dimensions (ethical, legal, and social implications) of nanopore sequencing. Employing the lenses of Diffusion and Action Network Theory, we examine the dissemination of nanopore sequencing in society as a potential consumer product, contributing to the field of the sociology of technology. We investigate the possibility of interactions between human and nonhuman actors in developing nanopore technology to analyse how various stakeholders, such as companies, regulators, and researchers, shape the trajectory of the growth of nanopore sequencing. This work offers insights into the social construction of nanopore sequencing, shedding light on the actors, power dynamics, and socio-technical networks that shape its adoption and societal impact. Understanding the sociological dimensions of this transformative technology is vital for responsible development, equitable distribution, and inclusive integration into diverse societal contexts."}, "https://arxiv.org/abs/2410.20543": {"title": "Investigation into the Spread of Misinformation about UK Prime Ministers on Twitter", "link": "https://arxiv.org/abs/2410.20543", "description": "arXiv:2410.20543v1 Announce Type: new \nAbstract: Misinformation presents threats to societal mental well-being, public health initiatives, as well as satisfaction in democracy. Those who spread misinformation can leverage cognitive biases to make others more likely to believe and share their misinformation unquestioningly. For example, by sharing misinformation whilst claiming to be someone from a highly respectable profession, a propagandist may seek to increase the effectiveness of their campaign using authority bias. Using retweet data from the spread of misinformation about two former UK Prime Ministers (Boris Johnson and Theresa May), we find that 3.1% of those who retweeted such misinformation claimed to be teachers or lecturers (20.7% of those who claimed to have a profession in their Twitter bio field in our sample), despite such professions representing under 1.15% of the UK population. Whilst polling data shows teachers and healthcare workers are amongst the most trusted professions in society, these were amongst the most popular professions that those in our sample claimed to have."}, "https://arxiv.org/abs/2410.20627": {"title": "DHPrep: Deep Hawkes Process based Dynamic Network Representation", "link": "https://arxiv.org/abs/2410.20627", "description": "arXiv:2410.20627v1 Announce Type: new \nAbstract: Networks representation aims to encode vertices into a low-dimensional space, while preserving the original network structures and properties. Most existing methods focus on static network structure without considering temporal dynamics. However, in real world, most networks (e.g., social and biological networks) are dynamic in nature and are constantly evolving over time. Such temporal dynamics are critical in representations learning, especially for predicting dynamic networks behaviors. To this end, a Deep Hawkes Process based Dynamic Networks Representation algorithm (DHPrep) is proposed in this paper, which is capable of capturing temporal dynamics of dynamic networks. Specifically, DHPrep incorporates both structural information and temporal dynamics to learn vertices representations that can model the edge formation process for a vertex pair, where the structural information is used to capture the historical impact from their neighborhood, and the temporal dynamics utilize this historical information and apply Hawkes point process to model the edges formation process. Moreover, a temporal smoother is further imposed to ensure the representations evolve smoothly over time. To evaluate the effectiveness of DHPrep, extensive experiments are carried out using four real-world datasets. Experimental results reveal that our DHPrep algorithm outperforms state-of-the-art baseline methods in various tasks including link prediction and vertices recommendation."}, "https://arxiv.org/abs/2410.20859": {"title": "Leveraging AI and Sentiment Analysis for Forecasting Election Outcomes in Mauritius", "link": "https://arxiv.org/abs/2410.20859", "description": "arXiv:2410.20859v1 Announce Type: new \nAbstract: This study explores the use of AI-driven sentiment analysis as a novel tool for forecasting election outcomes, focusing on Mauritius' 2024 elections. In the absence of reliable polling data, we analyze media sentiment toward two main political parties L'Alliance Lepep and L'Alliance Du Changement by classifying news articles from prominent Mauritian media outlets as positive, negative, or neutral. We employ a multilingual BERT-based model and a custom Sentiment Scoring Algorithm to quantify sentiment dynamics and apply the Sentiment Impact Score (SIS) for measuring sentiment influence over time. Our forecast model suggests L'Alliance Du Changement is likely to secure a minimum of 37 seats, while L'Alliance Lepep is predicted to obtain the remaining 23 seats out of the 60 available. Findings indicate that positive media sentiment strongly correlates with projected electoral gains, underscoring the role of media in shaping public perception. This approach not only mitigates media bias through adjusted scoring but also serves as a reliable alternative to traditional polling. The study offers a scalable methodology for political forecasting in regions with limited polling infrastructure and contributes to advancements in the field of political data science."}, "https://arxiv.org/abs/2410.21133": {"title": "Revealing the core-periphery structure of cities", "link": "https://arxiv.org/abs/2410.21133", "description": "arXiv:2410.21133v1 Announce Type: new \nAbstract: The distribution of urban services reveals critical patterns of human activity and accessibility. Proximity to amenities like restaurants, banks, and hospitals can reduce access barriers, but these services are often unevenly distributed, exacerbating spatial inequalities and socioeconomic disparities. In this study, we present a novel accessibility measure based on the spatial distribution of Points of Interest (POIs) within cities. Using the radial distribution function from statistical physics, we analyze the dispersion of services across different urban zones, combining local and remote access to services. This approach allows us to identify a city's central core, intermediate areas or secondary cores, and its periphery. Comparing the areas that we find with the resident population distribution highlights clusters of urban services and helps uncover disparities in access to opportunities."}, "https://arxiv.org/abs/2410.21187": {"title": "A cross-platform analysis of polarization and echo chambers in climate change discussions", "link": "https://arxiv.org/abs/2410.21187", "description": "arXiv:2410.21187v1 Announce Type: new \nAbstract: With the intensification of climate change discussion, social media has become prominent in disseminating reliable and unreliable content. In this study, we present a cross-platform analysis on Youtube and Twitter, and examine the polarization and echo chambers in social media discussions in four datasets related to climate change: COP27, IPCC, Climate Refugees, and Do\\~{n}ana. We have identified communities of users spreading misinformation on Twitter, although they remain relatively isolated from the rest of the network. The analysis by interaction type reveals that climate change sceptics use mentions to draw the attention of other communities. The YouTube posts referenced on Twitter reveal a strong correlation in the community organisation of social media, suggesting a platform alignment. Moreover, we report the presence of echo chambers in YouTube post-sharing related to mainstream and sceptical content."}, "https://arxiv.org/abs/2410.21189": {"title": "Intersectional inequalities in social networks", "link": "https://arxiv.org/abs/2410.21189", "description": "arXiv:2410.21189v1 Announce Type: new \nAbstract: Social networks are shaped by complex, intersecting identities that drive our connection preferences. These preferences weave networks where certain groups hold privileged positions, while others become marginalized. While previous research has examined the impact of single-dimensional identities on inequalities of social capital, social disparities accumulate nonlinearly, further harming individuals at the intersection of multiple disadvantaged groups. However, how multidimensional connection preferences affect network dynamics and in what forms they amplify or attenuate inequalities remains unclear. In this work, we systematically analyze the impact of multidimensionality on social capital inequalities through the lens of intersectionality. To this end, we operationalize several notions of intersectional inequality in networks. Using a network model, we reveal how attribute correlation (or consolidation) combined with biased multidimensional preferences lead to the emergence of counterintuitive patterns of inequality that are unobservable in one-dimensional systems. We calibrate the model with real-world high school friendship data and derive analytical closed-form expressions for the predicted inequalities, finding that the model's predictions match the observed data with remarkable accuracy. These findings hold significant implications for addressing social disparities and inform strategies for creating more equitable networks."}, "https://arxiv.org/abs/2410.20325": {"title": "Domain Specific Data Distillation and Multi-modal Embedding Generation", "link": "https://arxiv.org/abs/2410.20325", "description": "arXiv:2410.20325v1 Announce Type: cross \nAbstract: The challenge of creating domain-centric embeddings arises from the abundance of unstructured data and the scarcity of domain-specific structured data. Conventional embedding techniques often rely on either modality, limiting their applicability and efficacy. This paper introduces a novel modeling approach that leverages structured data to filter noise from unstructured data, resulting in embeddings with high precision and recall for domain-specific attribute prediction. The proposed model operates within a Hybrid Collaborative Filtering (HCF) framework, where generic entity representations are fine-tuned through relevant item prediction tasks. Our experiments, focusing on the cloud computing domain, demonstrate that HCF-based embeddings outperform AutoEncoder-based embeddings (using purely unstructured data), achieving a 28% lift in precision and an 11% lift in recall for domain-specific attribute prediction."}, "https://arxiv.org/abs/2410.20366": {"title": "Rethinking Reconstruction-based Graph-Level Anomaly Detection: Limitations and a Simple Remedy", "link": "https://arxiv.org/abs/2410.20366", "description": "arXiv:2410.20366v1 Announce Type: cross \nAbstract: Graph autoencoders (Graph-AEs) learn representations of given graphs by aiming to accurately reconstruct them. A notable application of Graph-AEs is graph-level anomaly detection (GLAD), whose objective is to identify graphs with anomalous topological structures and/or node features compared to the majority of the graph population. Graph-AEs for GLAD regard a graph with a high mean reconstruction error (i.e. mean of errors from all node pairs and/or nodes) as anomalies. Namely, the methods rest on the assumption that they would better reconstruct graphs with similar characteristics to the majority. We, however, report non-trivial counter-examples, a phenomenon we call reconstruction flip, and highlight the limitations of the existing Graph-AE-based GLAD methods. Specifically, we empirically and theoretically investigate when this assumption holds and when it fails. Through our analyses, we further argue that, while the reconstruction errors for a given graph are effective features for GLAD, leveraging the multifaceted summaries of the reconstruction errors, beyond just mean, can further strengthen the features. Thus, we propose a novel and simple GLAD method, named MUSE. The key innovation of MUSE involves taking multifaceted summaries of reconstruction errors as graph features for GLAD. This surprisingly simple method obtains SOTA performance in GLAD, performing best overall among 14 methods across 10 datasets."}, "https://arxiv.org/abs/2410.20619": {"title": "Evolving interdisciplinary contributions to global societal challenges: A 50-year overview", "link": "https://arxiv.org/abs/2410.20619", "description": "arXiv:2410.20619v1 Announce Type: cross \nAbstract: Addressing global societal challenges necessitates insights and expertise that transcend the boundaries of individual disciplines. In recent decades, interdisciplinary collaboration has been recognised as a vital driver of innovation and effective problem-solving, with the potential to profoundly influence policy and practice worldwide. However, quantitative evidence remains limited regarding how cross-disciplinary efforts contribute to societal challenges, as well as the evolving roles and relevance of specific disciplines in addressing these issues. To fill this gap, this study examines the long-term evolution of interdisciplinary contributions to the United Nations' Sustainable Development Goals (SDGs), drawing on extensive bibliometric data from OpenAlex. By analysing publication and citation trends across 19 research fields from 1970 to 2022, we reveal how the relative presence of different disciplines in addressing particular SDGs has shifted over time. Our results also provide unique evidence of the increasing interconnection between fields since the 2000s, coinciding with the United Nations' initiative to tackle global societal challenges through interdisciplinary efforts. These insights will benefit policymakers and practitioners as they reflect on past progress and plan for future action, particularly with the SDG target deadline approaching in the next five years."}, "https://arxiv.org/abs/2410.21019": {"title": "Economic Integration of Africa in the 21st Century: Complex Network and Panel Regression Analysis", "link": "https://arxiv.org/abs/2410.21019", "description": "arXiv:2410.21019v1 Announce Type: cross \nAbstract: Global and regional integration has grown significantly in recent decades, boosting intra-African trade and positively impacting national economies through trade diversification and sustainable development. However, existing measures of economic integration often fail to capture the complex interactions among trading partners. This study addresses this gap by using complex network analysis and dynamic panel regression techniques to identify factors driving economic integration in Africa, based on data from 2002 to 2019. The results show that economic development, institutional quality, regional trade agreements, human capital, FDI, and infrastructure positively influence a country's position in the African trade network. Conversely, trade costs, the global financial crisis, and regional overlapping memberships negatively affect network based integration. Our findings suggest that enhancing a country's connectivity in the African trade network involves identifying key economic and institutional factors of trade partners and strategically focusing on continent-wide agreements rather than just regional ones to boost economic growth."}, "https://arxiv.org/abs/2305.16590": {"title": "Seeding with Differentially Private Network Information", "link": "https://arxiv.org/abs/2305.16590", "description": "arXiv:2305.16590v3 Announce Type: replace \nAbstract: In public health interventions such as distributing pre-exposure prophylaxis (PrEP) for HIV prevention, decision makers rely on seeding algorithms to identify key individuals who can amplify the impact of their interventions. In such cases, constructing a complete sexual activity network is often infeasible due to privacy concerns. Instead, contact tracing can provide influence samples, i.e., sequences of sexual contacts without requiring complete network information. This brings two challenges: protecting individual privacy in the contact data and adapting seeding algorithms to work effectively with incomplete network information. To solve these two problems, we study privacy guarantees for influence maximization algorithms when the social network is unknown and the inputs are samples of prior influence cascades that are collected at random and need privacy protection. Building on recent results that address seeding with costly network information, our privacy-preserving algorithms introduce randomization in the collected data or the algorithm output, and can bound the privacy loss of each node (or group of nodes) in deciding to include their data in the algorithm input. We provide theoretical guarantees of seeding performance with a limited sample size subject to differential privacy budgets in both central and local privacy regimes. Simulations on synthetic random graphs and empirically grounded sexual contacts of men who have sex with men reveal the diminishing value of network information with decreasing privacy budget in both regimes and graceful decrease in performance with decreasing privacy budget in the central regime. Achieving good performance with local privacy guarantees requires relatively higher privacy budgets that confirm our theoretical expectations."}, "https://arxiv.org/abs/2309.15728": {"title": "Line Graph Neural Networks for Link Weight Prediction", "link": "https://arxiv.org/abs/2309.15728", "description": "arXiv:2309.15728v2 Announce Type: replace \nAbstract: In real-world networks, predicting the weight (strength) of links is as crucial as predicting the existence of the links themselves. Previous studies have primarily used shallow graph features for link weight prediction, limiting the prediction performance. In this paper, we propose a new link weight prediction method, namely Line Graph Neural Networks for Link Weight Prediction (LGLWP), which learns deeper graph features through deep learning. In our method, we first extract the enclosing subgraph around a target link and then employ a weighted graph labeling algorithm to label the subgraph nodes. Next, we transform the subgraph into the line graph and apply graph convolutional neural networks to learn the node embeddings in the line graph, which can represent the links in the original subgraph. Finally, the node embeddings are fed into a fully-connected neural network to predict the weight of the target link, treated as a regression problem. Our method directly learns link features, surpassing previous methods that splice node features for link weight prediction. Experimental results on six network datasets of various sizes and types demonstrate that our method outperforms state-of-the-art methods."}, "https://arxiv.org/abs/2401.05065": {"title": "Universal Statistics of Competition in Democratic Elections", "link": "https://arxiv.org/abs/2401.05065", "description": "arXiv:2401.05065v3 Announce Type: replace \nAbstract: Elections for public offices in democratic nations are large-scale examples of collective decision-making. As a complex system with a multitude of interactions among agents, we can anticipate that universal macroscopic patterns could emerge independent of microscopic details. Despite the availability of empirical election data, such universality, valid at all scales, countries, and elections, has not yet been observed. In this work, we propose a parameter-free voting model and analytically show that the distribution of the victory margin is driven by that of the voter turnout, and a scaled measure depending on margin and turnout leads to a robust universality. This is demonstrated using empirical election data from $34$ countries, spanning multiple decades and electoral scales. The deviations from the model predictions and universality indicate possible electoral malpractices. We argue that this universality is a stylized fact indicating the competitive nature of electoral outcomes."}, "https://arxiv.org/abs/2303.09720": {"title": "Modeling urbanization dynamics by labor force migration", "link": "https://arxiv.org/abs/2303.09720", "description": "arXiv:2303.09720v2 Announce Type: replace-cross \nAbstract: Individual participants in human society collectively exhibit aggregation behavior. In this study, we present a simple microscopic model of labor force migration based on the active Brownian particles framework. In particular, agent-based simulations show that the model produces clusters of agents from a random initial distribution. Furthermore, two empirical regularities called Zipf's and Okun's laws were observed. To reveal the mechanism underlying the reproduced aggregation phenomena, we use our microscopic model to derive an extended Keller--Segel system, which is a classic model describing the aggregation behavior of biological organisms called taxis. The obtained macroscopic system indicates that the concentration of the workforce in the real world can be explained through a new type of taxis central to human behavior, highlighting the relevance of urbanization to blow-up phenomena in the derived PDE system. We then characterize the transition between the aggregation and diffusion regimes both analytically and computationally. The predicted long-term dynamics of urbanization -- originating in the asymmetric natures of employed and unemployed agents -- are compared with global empirical data, particularly in the realms of labor statistics and urban indicators."}, "https://arxiv.org/abs/2402.08918": {"title": "SimMLP: Training MLPs on Graphs without Supervision", "link": "https://arxiv.org/abs/2402.08918", "description": "arXiv:2402.08918v2 Announce Type: replace-cross \nAbstract: Graph Neural Networks (GNNs) have demonstrated their effectiveness in various graph learning tasks, yet their reliance on neighborhood aggregation during inference poses challenges for deployment in latency-sensitive applications, such as real-time financial fraud detection. To address this limitation, recent studies have proposed distilling knowledge from teacher GNNs into student Multi-Layer Perceptrons (MLPs) trained on node content, aiming to accelerate inference. However, these approaches often inadequately explore structural information when inferring unseen nodes. To this end, we introduce SimMLP, a Self-supervised framework for learning MLPs on graphs, designed to fully integrate rich structural information into MLPs. Notably, SimMLP is the first MLP-learning method that can achieve equivalence to GNNs in the optimal case. The key idea is to employ self-supervised learning to align the representations encoded by graph context-aware GNNs and neighborhood dependency-free MLPs, thereby fully integrating the structural information into MLPs. We provide a comprehensive theoretical analysis, demonstrating the equivalence between SimMLP and GNNs based on mutual information and inductive bias, highlighting SimMLP's advanced structural learning capabilities. Additionally, we conduct extensive experiments on 20 benchmark datasets, covering node classification, link prediction, and graph classification, to showcase SimMLP's superiority over state-of-the-art baselines, particularly in scenarios involving unseen nodes (e.g., inductive and cold-start node classification) where structural insights are crucial. Our codes are available at: https://github.com/Zehong-Wang/SimMLP."}, "https://arxiv.org/abs/2410.21321": {"title": "User-Aware Multilingual Abusive Content Detection in Social Media", "link": "https://arxiv.org/abs/2410.21321", "description": "arXiv:2410.21321v1 Announce Type: new \nAbstract: Despite growing efforts to halt distasteful content on social media, multilingualism has added a new dimension to this problem. The scarcity of resources makes the challenge even greater when it comes to low-resource languages. This work focuses on providing a novel method for abusive content detection in multiple low-resource Indic languages. Our observation indicates that a post's tendency to attract abusive comments, as well as features such as user history and social context, significantly aid in the detection of abusive content. The proposed method first learns social and text context features in two separate modules. The integrated representation from these modules is learned and used for the final prediction. To evaluate the performance of our method against different classical and state-of-the-art methods, we have performed extensive experiments on SCIDN and MACI datasets consisting of 1.5M and 665K multilingual comments, respectively. Our proposed method outperforms state-of-the-art baseline methods with an average increase of 4.08% and 9.52% in F1-scores on SCIDN and MACI datasets, respectively."}, "https://arxiv.org/abs/2410.21344": {"title": "Estimating the epidemic threshold under individual vaccination behaviour and adaptive social connections: A game-theoretic complex network model", "link": "https://arxiv.org/abs/2410.21344", "description": "arXiv:2410.21344v1 Announce Type: new \nAbstract: Information dissemination intricately intertwines with the dynamics of infectious diseases in the contemporary interconnected world. Recognizing the critical role of public awareness, individual vaccination choices appear to be an essential factor in collective efforts against emerging health threats. This study aims to characterize disease transmission dynamics under evolving social connections, information sharing, and individual vaccination decisions. To address this important problem, we present an integrated behaviour-prevalence model on an adaptive multiplex network. While the physical layer (layer-II) focuses on disease transmission under vaccination, the virtual layer (layer-I), representing individuals' social contacts, is adaptive and deals with information dissemination, resulting in the dynamics of vaccination choice in a socially influenced environment. Utilizing the microscopic Markov Chain Method (MMCM), we derive analytical expressions of the epidemic threshold for populations with different levels of perceived vaccine risk. It indicates that the adaptive nature of social contacts contributes to the higher epidemic threshold compared to non-adaptive scenarios, and numerical simulations also support that. The network topology, such as the power-law exponent of a scale-free network, also significantly influences the spreading of infections in the network population. We also observe that vaccine uptake increases proportionately with the number of individuals with a higher perceived infection risk or a higher sensitivity of an individual to their non-vaccinated neighbours. As a result, our findings provide insights for public health officials in developing vaccination programs in light of the evolution of social connections, information dissemination, and vaccination choice in the digital era."}, "https://arxiv.org/abs/2410.21356": {"title": "Modeling The Sharing and Diffusion Of Fake News in Social Media", "link": "https://arxiv.org/abs/2410.21356", "description": "arXiv:2410.21356v1 Announce Type: new \nAbstract: The use of social media platforms has been gradually increasing and fake news spreading is becoming an alarming issue nowadays. The spreading of fake news means disseminating false, confusing, and spurious information which hurts families, communities etc. As a result, this issue has to be resolved sooner so that we can limit the spread of fake news in the virtual world. One needs to identify the fake news spreader to address this issue. In this research, we have tried to reveal the users who are most likely to share fake news as well as the spread prediction that shared pieces of fake news in the social network. We take into account the users information, such as follower counts, like counts, and retweet counts along with users topical interests on different topics as well as connection strength by considering the follower-following ratio. We also consider the complexity features, stylistic features, and psychological effects of news. Finally, we applied different machine-learning algorithms to evaluate the performance of the proposed model. Our observation is that the probability of spreading a piece of news shared by users having more followers as well as more likes and retweet counts (aka influential users) is higher compared with other users."}, "https://arxiv.org/abs/2410.21554": {"title": "Information diffusion assumptions can distort our understanding of social network dynamics", "link": "https://arxiv.org/abs/2410.21554", "description": "arXiv:2410.21554v1 Announce Type: new \nAbstract: To analyze the flow of information online, experts often rely on platform-provided data from social media companies, which typically attribute all resharing actions to an original poster. This obscures the true dynamics of how information spreads online, as users can be exposed to content in various ways. While most researchers analyze data as it is provided by the platform and overlook this issue, some attempt to infer the structure of these information cascades. However, the absence of ground truth about actual diffusion cascades makes verifying the efficacy of these efforts impossible. This study investigates the implications of the common practice of ignoring reconstruction all together. Two case studies involving data from Twitter and Bluesky reveal that reconstructing cascades significantly alters the identification of influential users, therefore affecting downstream analyses in general. We also propose a novel reconstruction approach that allows us to evaluate the effects of different assumptions made during the cascade inference procedure. Analysis of the diffusion of over 40,000 true and false news stories on Twitter reveals that the assumptions made during the reconstruction procedure drastically distort both microscopic and macroscopic properties of cascade networks. This work highlights the challenges of studying information spreading processes on complex networks and has significant implications for the broader study of digital platforms."}, "https://arxiv.org/abs/2410.21589": {"title": "The Toxicity Phenomenon Across Social Media", "link": "https://arxiv.org/abs/2410.21589", "description": "arXiv:2410.21589v1 Announce Type: new \nAbstract: Social media platforms have evolved rapidly in modernity without strong regulation. One clear obstacle faced by current users is that of toxicity. Toxicity on social media manifests through a number of forms, including harassment, negativity, misinformation or other means of divisiveness. In this paper, we characterize literature surrounding toxicity, formalize a definition of toxicity, propose a novel cycle of internet extremism, list current approaches to toxicity detection, outline future directions to minimize toxicity in future social media endeavors, and identify current gaps in research space. We present a novel perspective of the negative impacts of social media platforms and fill a gap in literature to help improve the future of social media platforms."}, "https://arxiv.org/abs/2410.21996": {"title": "Multi-layer network analysis of deliberation in an online discussion platform: the case of Reddit", "link": "https://arxiv.org/abs/2410.21996", "description": "arXiv:2410.21996v1 Announce Type: new \nAbstract: This paper uses a multi-layer network model to study deliberation in online discussion platforms, focusing on the Reddit platform. The model comprises two layers: a discussion layer, which represents the comment-to-comment replies as a hierarchical tree, and an actor layer, which represent the actor-to-actor reply interactions. The interlayer links represent user-comment ownership. We further propose several different network metrics to characterise the level of deliberation in discussion threads, and apply the model and metrics to a large Reddit dataset containing posts from 72 subreddits focused on different topics. We compare the level of deliberation that occurs on different subreddits, finding that subreddits that are based on geographical regions or focus on sports have the highest levels of deliberation. Analysis of the actor layer reveals several features consistent across all subreddits, such as small-world characteristics and similar numbers of highly active users."}, "https://arxiv.org/abs/2410.22015": {"title": "City-Scale Assessment of Pedestrian Exposure to Air Pollution: A Case Study in Barcelona", "link": "https://arxiv.org/abs/2410.22015", "description": "arXiv:2410.22015v1 Announce Type: new \nAbstract: Air pollution is a pressing environmental risk to public health, particularly in cities where population density and pollution levels are high. Traditional methods for exposure analysis often rely on census data, but recent studies highlight the impact of daily mobility on individuals' exposure. Here, we develop a methodology to determine unprecedented pedestrian exposure estimates at the city scale by combining sidewalk pedestrian flows with high-resolution (25 m x 25 m) NO2 data from bias-corrected predictions of the air quality system CALIOPE-Urban. Applied to Barcelona (Spain) for the year 2019, we show that pedestrian flow and NO2 levels exhibit negligible temporal correlation. While short-term (hourly) exposure is driven by pedestrian mobility, long-term (monthly) exposure is dominated by NO2 patterns. We identify strong spatial gradients of exposure, highlighting the importance for high-resolution solutions at the sidewalks scale. Finally, we determine that exposure mitigation strategies should consider different citizen subgroups based on their mobility and preferred routes, as significant differences were found between residential and pedestrian exposure. Our results provide exposure indicators designed for city planners and policymakers, helping to prioritize mitigation measures where and when they are most needed."}, "https://arxiv.org/abs/2410.22115": {"title": "Competition between simple and complex contagion on temporal networks", "link": "https://arxiv.org/abs/2410.22115", "description": "arXiv:2410.22115v1 Announce Type: new \nAbstract: Behavioral adoptions are influenced by peers in different ways. While some individuals may change after a single incoming influence, others need multiple cumulated attempts. These two mechanism, known as the simple and the complex contagions, often occur together in social phenomena alongside personal factors determining individual adoptions. Here we aim to identify, which of these contagion mechanism dominate a spreading process propagated by time-varying interactions. We consider three types of spreading scenarios: ones pre-dominated by simple or complex contagion, and mixed dynamics where the dominant mechanism changes during the unfolding of the spreading process. We propose different methods to analytically identify the transitions between these three scenarios and compare them with numerical simulations. This work offers new insights into social contagion dynamics on temporal networks, without assuming prior knowledge about individual's contagion mechanism driving their adoption decisions."}, "https://arxiv.org/abs/2410.22142": {"title": "A Data-Driven Analysis of the Sovereign Citizens Movement on Telegram", "link": "https://arxiv.org/abs/2410.22142", "description": "arXiv:2410.22142v1 Announce Type: new \nAbstract: Online communities of known extremist groups like the alt-right and QAnon have been well explored in past work. However, we find that an extremist group called Sovereign Citizens is relatively unexplored despite its existence since the 1970s. Their main belief is delegitimizing the established government with a tactic called paper terrorism, clogging courts with pseudolegal claims. In recent years, their activities have escalated to threats like forcefully claiming property ownership and participating in the Capitol Riot. This paper aims to shed light on Sovereign Citizens' online activities by examining two Telegram channels, each belonging to an identified Sovereign Citizen individual. We collect over 888K text messages and apply NLP techniques. We find that the two channels differ in the topics they discussed, demonstrating different focuses. Further, the two channels exhibit less toxic content compared to other extremist groups like QAnon. Finally, we find indications of overlapping beliefs between the two channels and QAnon, suggesting a merging or complementing of beliefs."}, "https://arxiv.org/abs/2410.22186": {"title": "Balanced Bidirectional Breadth-First Search on Scale-Free Networks", "link": "https://arxiv.org/abs/2410.22186", "description": "arXiv:2410.22186v1 Announce Type: new \nAbstract: To find a shortest path between two nodes $s_0$ and $s_1$ in a given graph, a classical approach is to start a Breadth-First Search (BFS) from $s_0$ and run it until the search discovers $s_1$. Alternatively, one can start two Breadth-First Searches, one from $s_0$ and one from $s_1$, and alternate their layer expansions until they meet. This bidirectional BFS can be balanced by always expanding a layer on the side that has discovered fewer vertices so far. This usually results in significant speedups in real-world networks, and it has been shown that this indeed yields sublinear running time on scale-free graph models such as Chung-Lu graphs and hyperbolic random graphs.\n  We improve this layer-balanced bidirectional BFS approach by using a finer balancing technique. Instead of comparing the size of the two BFS trees after each layer expansion, we perform this comparison after each vertex expansion. This gives rise to two algorithms that run faster than the layer-balanced bidirectional BFS on scale-free networks with power-law exponent $\\tau \\in (2,3)$. The first one is an approximate shortest-path algorithm that outputs a path of length at most 1 longer than the shortest path in time $n^{(\\tau-2)/(\\tau-1)+o(1)}$. The second one is an exact shortest-path algorithm running in time $n^{1/2+o(1)}$. These runtime bounds hold with high probability when $s_0$ and $s_1$ are chosen uniformly at random among the $n$ vertices of the graph. We also develop an edge-balanced bidirectional BFS algorithm that works under adversarial conditions. This approximate shortest-path algorithm runs in time $n^{1/2+o(1)}$ with high probability when the adversary is allowed to choose $s_0$ and $s_1$ based on their (expected) degree. We complement our theoretical results with experiments on Chung-Lu graphs, Geometric Inhomogeneous Random Graphs, and real-world networks."}, "https://arxiv.org/abs/2410.21484": {"title": "A Systematic Review of Machine Learning in Sports Betting: Techniques, Challenges, and Future Directions", "link": "https://arxiv.org/abs/2410.21484", "description": "arXiv:2410.21484v1 Announce Type: cross \nAbstract: The sports betting industry has experienced rapid growth, driven largely by technological advancements and the proliferation of online platforms. Machine learning (ML) has played a pivotal role in the transformation of this sector by enabling more accurate predictions, dynamic odds-setting, and enhanced risk management for both bookmakers and bettors. This systematic review explores various ML techniques, including support vector machines, random forests, and neural networks, as applied in different sports such as soccer, basketball, tennis, and cricket. These models utilize historical data, in-game statistics, and real-time information to optimize betting strategies and identify value bets, ultimately improving profitability. For bookmakers, ML facilitates dynamic odds adjustment and effective risk management, while bettors leverage data-driven insights to exploit market inefficiencies. This review also underscores the role of ML in fraud detection, where anomaly detection models are used to identify suspicious betting patterns. Despite these advancements, challenges such as data quality, real-time decision-making, and the inherent unpredictability of sports outcomes remain. Ethical concerns related to transparency and fairness are also of significant importance. Future research should focus on developing adaptive models that integrate multimodal data and manage risk in a manner akin to financial portfolios. This review provides a comprehensive examination of the current applications of ML in sports betting, and highlights both the potential and the limitations of these technologies."}, "https://arxiv.org/abs/2410.21634": {"title": "Faster Local Solvers for Graph Diffusion Equations", "link": "https://arxiv.org/abs/2410.21634", "description": "arXiv:2410.21634v1 Announce Type: cross \nAbstract: Efficient computation of graph diffusion equations (GDEs), such as Personalized PageRank, Katz centrality, and the Heat kernel, is crucial for clustering, training neural networks, and many other graph-related problems. Standard iterative methods require accessing the whole graph per iteration, making them time-consuming for large-scale graphs. While existing local solvers approximate diffusion vectors through heuristic local updates, they often operate sequentially and are typically designed for specific diffusion types, limiting their applicability. Given that diffusion vectors are highly localizable, as measured by the participation ratio, this paper introduces a novel framework for approximately solving GDEs using a local diffusion process. This framework reveals the suboptimality of existing local solvers. Furthermore, our approach effectively localizes standard iterative solvers by designing simple and provably sublinear time algorithms. These new local solvers are highly parallelizable, making them well-suited for implementation on GPUs. We demonstrate the effectiveness of our framework in quickly obtaining approximate diffusion vectors, achieving up to a hundred-fold speed improvement, and its applicability to large-scale dynamic graphs. Our framework could also facilitate more efficient local message-passing mechanisms for GNNs."}, "https://arxiv.org/abs/2306.14142": {"title": "Estimating Policy Effects in a Social Network with Independent Set Sampling", "link": "https://arxiv.org/abs/2306.14142", "description": "arXiv:2306.14142v4 Announce Type: replace \nAbstract: Evaluating the impact of policy interventions on respondents who are embedded in a social network is often challenging due to the presence of network interference within the treatment groups, as well as between treatment and non-treatment groups throughout the network. In this paper, we propose a novel empirical strategy that combines network sampling based on the identification of independent sets with a stochastic actor-oriented model (SAOM) to infer the direct and net effects of a policy. By assigning respondents from an independent set to the treatment, we are able to block direct spillover of the treatment among the treated respondents for an extended period of time, during which the direct effect of the treatment can be isolated from the associated network interference. We empirically demonstrate this using a simulation-based evaluation of a fictitious policy implementation using both real-life and generated networks, and use a counterfactual approach to estimate the treatment effect of the policy. Our results highlight the effectiveness of our proposed empirical strategy, and notably, the role of network sampling techniques in influencing the evaluation of policy effects. The findings from this study have the potential to help researchers and policymakers with planning, designing, and anticipating policy responses in a networked society."}, "https://arxiv.org/abs/2311.00848": {"title": "ABCD: Algorithm for Balanced Component Discovery in Signed Networks", "link": "https://arxiv.org/abs/2311.00848", "description": "arXiv:2311.00848v2 Announce Type: replace \nAbstract: The largest balanced element in signed graphs plays a vital role in helping researchers understand the fundamental structure of the graph, as it reveals valuable information about the complex relationships between vertices in the network. The challenge is an NP-hard problem; there is no current baseline to evaluate state-of-the-art signed graphs derived from real networks. In this paper, we propose a scalable state-of-the-art approach for the maximum balanced sub-graph detection in the network of \\emph{any} size. The proposed approach finds the largest balanced sub-graph by considering only the top $K$ balanced states with the lowest frustration index. We show that the ABCD method selects a subset from an extensive signed network with millions of vertices and edges, and the size of the discovered subset is double that of the state-of-the-art in a similar time frame."}, "https://arxiv.org/abs/2311.00869": {"title": "Scaling Frustration Index and Corresponding Balanced State Discovery for Real Signed Graphs", "link": "https://arxiv.org/abs/2311.00869", "description": "arXiv:2311.00869v2 Announce Type: replace \nAbstract: Structural balance modeling for signed graph networks presents how to model the sources of conflicts. The state-of-the-art focuses on computing the frustration index of a signed graph, a critical step toward solving problems in social and sensor networks and scientific modeling. The proposed approaches do not scale to large signed networks of tens of millions of vertices and edges. In this paper, we propose two efficient algorithms, a tree-based \\emph{graphBpp} and a gradient descent-based \\emph{graphL}. We show that both algorithms outperform state-of-art in terms of efficiency and effectiveness for discovering the balanced state for \\emph{any} size of the network. We introduce the first comparison for large graphs for the exact, tree-based, and gradient descent-based methods. The speedup of the methods is around \\emph{300+ times faster} than the state-of-the-art for large signed graphs. We find that the exact method excels at optimally finding the frustration for small graphs only. \\emph{graphBpp} scales this approximation to large signed graphs at the cost of accuracy. \\emph{graphL} produces a state with a lower frustration at the cost of selecting a proper variable initialization and hyperparameter tuning."}, "https://arxiv.org/abs/2101.01425": {"title": "Het-node2vec: second order random walk sampling for heterogeneous multigraphs embedding", "link": "https://arxiv.org/abs/2101.01425", "description": "arXiv:2101.01425v3 Announce Type: replace-cross \nAbstract: Many real-world problems are naturally modeled as heterogeneous graphs, where nodes and edges represent multiple types of entities and relations. Existing learning models for heterogeneous graph representation usually depend on the computation of specific and user-defined heterogeneous paths, or in the application of large and often not scalable deep neural network architectures. We propose Het-node2vec, an extension of the node2vec algorithm, designed for embedding heterogeneous graphs. Het-node2vec addresses the challenge of capturing the topological and structural characteristics of graphs and the semantic information underlying the different types of nodes and edges of heterogeneous graphs, by introducing a simple stochastic node and edge type switching strategy in second order random walk processes. The proposed approach also introduces an ''attention mechanism'' to focus the random walks on specific node and edge types, thus allowing more accurate embeddings and more focused predictions on specific node and edge types of interest. Empirical results on benchmark datasets show that Hetnode2vec achieves comparable or superior performance with respect to state-of-the-art methods for heterogeneous graphs in node label and edge prediction tasks."}, "https://arxiv.org/abs/2410.22369": {"title": "Inequality in a model of capitalist economy", "link": "https://arxiv.org/abs/2410.22369", "description": "arXiv:2410.22369v1 Announce Type: new \nAbstract: We analyze inequality aspects of the agent-based model of capitalist economy named it Social Architecture of Capitalism that has been introduced by Ian Wright. The model contemplates two main types of agents, workers and capitalists, which can also be unemployed. Starting from a state where all agents are unemployed and possess the same initial wealth, the system, governed by a few simple rules, quickly self-organizes into two classes. After a transient, the model reproduces the statistics of many relevant macroeconomic quantities of real economies worldwide, notably the Boltzmann-Pareto regimes of the distributions of wealth and income. We perform extensive simulations testing the role of the model parameters (number of agents, total wealth, and salary range) on the resulting distribution of wealth and income, the social distribution of agents, and other stylized facts of the dynamics. Our main finding is that, according to the model, in an economy where total wealth is conserved and with a fixed average wage, the increase in wealth per capita comes with more inequality."}, "https://arxiv.org/abs/2410.22386": {"title": "Mobile Phone Application Data for Activity Plan Generation", "link": "https://arxiv.org/abs/2410.22386", "description": "arXiv:2410.22386v1 Announce Type: new \nAbstract: Activity-based models in transport are crucial for providing a comprehensive and realistic understanding of individuals' activity-travel patterns. Traditionally, travel surveys have been used to develop these models, but they are often costly and have small sample sizes. Mobile phone application data, one example of emerging data sources, offers an alternative with wider population coverage over extended periods for developing activity-based models. However, the challenges of using these data include sampling biases in the population coverage and individual-level data sparsity due to intermittent and irregular data collection. To synthesise activity-travel plans, we propose a novel model that combines mobile phone application data with travel survey data, addressing their limitations. Our generative model simulates multiple average weekday activity schedules for over 263,000 individuals living in Sweden, approximately 2.6% of Sweden's population. We also introduce a temporal-score approach to improve home and work location identification approaches. We assess the model's performance against an existing large-scale agent-based model of Sweden (SySMo) and a dummy model using only mobile application data. The generated activity-travel plans are comparable to the SySMo model's output and significantly surpass the dummy model's results, suggesting the proposed model's capability to generate reasonable activity-travel schedules. The proposed model is adaptable to other regions with similar travel surveys and emerging data sources, like call detail records, advancing the use of these data for activity-based models in a cost-effective, easily updated manner."}, "https://arxiv.org/abs/2410.22577": {"title": "Do Stubborn Users Always Cause More Polarization and Disagreement? A Mathematical Study", "link": "https://arxiv.org/abs/2410.22577", "description": "arXiv:2410.22577v1 Announce Type: new \nAbstract: We study how the stubbornness of social network users influences opinion polarization and disagreement. Our work is in the context of the popular Friedkin-Johnson opinion formation model, where users update their opinion as a function of the opinion of their connections and their own innate opinion. Stubbornness then is formulated in terms of the stress a user puts on its innate opinion.\n  We examine two scenarios: one where all nodes have uniform stubbornness levels (homogeneous) and another where stubbornness varies among nodes (inhomogeneous). In the homogeneous scenario, we prove that as the network's stubbornness factor increases, the polarization and disagreement index grows. In the more general inhomogeneous scenario, our findings surprisingly demonstrate that increasing the stubbornness of some users (particularly, neutral/unbiased users) can reduce the polarization and disagreement. We characterize specific conditions under which this phenomenon occurs. Finally, we conduct an extensive set of experiments on real-world network data to corroborate and complement our theoretical findings."}, "https://arxiv.org/abs/2410.22716": {"title": "Exposing Cross-Platform Coordinated Inauthentic Activity in the Run-Up to the 2024 U", "link": "https://arxiv.org/abs/2410.22716", "description": "arXiv:2410.22716v1 Announce Type: new \nAbstract: Coordinated information operations remain a persistent challenge on social media, despite platform efforts to curb them. While previous research has primarily focused on identifying these operations within individual platforms, this study shows that coordination frequently transcends platform boundaries. Leveraging newly collected data of online conversations related to the 2024 U.S. Election across $\\mathbb{X}$ (formerly, Twitter), Facebook, and Telegram, we construct similarity networks to detect coordinated communities exhibiting suspicious sharing behaviors within and across platforms. Proposing an advanced coordination detection model, we reveal evidence of potential foreign interference, with Russian-affiliated media being systematically promoted across Telegram and $\\mathbb{X}$. Our analysis also uncovers substantial intra- and cross-platform coordinated inauthentic activity, driving the spread of highly partisan, low-credibility, and conspiratorial content. These findings highlight the urgent need for regulatory measures that extend beyond individual platforms to effectively address the growing challenge of cross-platform coordinated influence campaigns."}, "https://arxiv.org/abs/2410.22896": {"title": "Modelling vehicle and pedestrian collective dynamics: Challenges and advancements", "link": "https://arxiv.org/abs/2410.22896", "description": "arXiv:2410.22896v1 Announce Type: new \nAbstract: In our urbanised societies, the management and regulation of traffic and pedestrian flows is of considerable interest for public safety, economic development, and the conservation of the environment. However, modelling and controlling the collective dynamics of vehicles and pedestrians raises several challenges. Not only are the individual entities self-propelled and hard to describe, but their complex nonlinear physical and social interactions makes the multi-agent problem of crowd and traffic flow even more involved. In this chapter, we purport to review the suitability and limitations of classical modelling approaches through four examples of collective behaviour: stop-and-go waves in traffic flow, lane formation, long-term avoidance behaviour, and load balancing in pedestrian dynamics. While stop-and-go dynamics and lane formation can both be addressed by basic reactive models (at least to some extent), the latter two require anticipation and/or coordination at the level of the group. The results highlight the limitations of classical force-based models, but also the need for long-term anticipation mechanisms and multiscale modelling approaches. In response, we review new developments and modelling concepts."}, "https://arxiv.org/abs/2410.22897": {"title": "A Graph-Based Model for Vehicle-Centric Data Sharing Ecosystem", "link": "https://arxiv.org/abs/2410.22897", "description": "arXiv:2410.22897v1 Announce Type: new \nAbstract: The development of technologies has prompted a paradigm shift in the automotive industry, with an increasing focus on connected services and autonomous driving capabilities. This transformation allows vehicles to collect and share vast amounts of vehicle-specific and personal data. While these technological advancements offer enhanced user experiences, they also raise privacy concerns. To understand the ecosystem of data collection and sharing in modern vehicles, we adopted the ontology 101 methodology to incorporate information extracted from different sources, including analysis of privacy policies using GPT-4, a small-scale systematic literature review, and an existing ontology, to develop a high-level conceptual graph-based model, aiming to get insights into how modern vehicles handle data exchange among different parties. This serves as a foundational model with the flexibility and scalability to further expand for modelling and analysing data sharing practices across diverse contexts. Two realistic examples were developed to demonstrate the usefulness and effectiveness of discovering insights into privacy regarding vehicle-related data sharing. We also recommend several future research directions, such as exploring advanced ontology languages for reasoning tasks, supporting topological analysis for discovering data privacy risks/concerns, and developing useful tools for comparative analysis, to strengthen the understanding of the vehicle-centric data sharing ecosystem."}, "https://arxiv.org/abs/2410.22917": {"title": "Seasonal social dilemmas", "link": "https://arxiv.org/abs/2410.22917", "description": "arXiv:2410.22917v1 Announce Type: new \nAbstract: Social dilemmas where the good of a group is at odds with individual interests are usually considered as static -- the dilemma does not change over time. In the COVID-19 pandemic, social dilemmas occurred in the mitigation of epidemic spread: Should I reduce my contacts or wear a mask to protect others? In the context of respiratory diseases, which are predominantly spreading during the winter months, some of these situations re-occur seasonally. We couple a game theoretical model, where individuals can adjust their behavior, to an epidemiological model with seasonal forcing. We find that social dilemmas can occur annually and that behavioral reactions to them can either decrease or increase the peaks of infections in a population. Our work has not only implications for seasonal infectious diseases, but also more generally for oscillatory social dilemmas: A complex interdependence between behavior and external dynamics emerges. To be effective and to exploit behavioral dynamics, intervention measures to mitigate re-occuring social dilemmas have to be timed carefully."}, "https://arxiv.org/abs/2410.23025": {"title": "Decarbonisation of industry and the energy system: exploring mutual impacts and investment planning", "link": "https://arxiv.org/abs/2410.23025", "description": "arXiv:2410.23025v1 Announce Type: new \nAbstract: The decarbonisation of the energy system is crucial for achieving climate goals and is inherently linked to the decarbonisation of industry. Despite this, few studies explore the simultaneous impacts of decarbonising both sectors. This paper aims to examine how industrial decarbonisation in Europe affects the energy system and vice versa. To address this, an industry model incorporating key heavy industry sectors across six European countries is combined with an energy system model for electricity and hydrogen covering fifteen European regions, refered to as the EU-15, divided into eleven zones. The study evaluates various policy scenarios under different conditions.The results demonstrate that industrial decarbonisation leads to a significant increase in electricity and hydrogen demand. This additional demand for electricity is largely met through renewable energy sources, while hydrogen supply is predominantly addressed by blue hydrogen production when fossil fuels are authorized and the system lacks renewable energy. This increased demand results in higher prices with considerable regional disparities. Furthermore, the findings reveal that, regardless of the scenario, the electricity mix in the EU-15 remains predominantly renewable, exceeding 85%.A reduction in carbon taxes lowers the prices of electricity and hydrogen, but does not increase consumption, as the lower carbon tax makes the continued use of fossil fuels more attractive to industry. In scenarios that enforce a phase-out of fossil fuels, electricity prices rise, leading to a greater reliance on imports of low-carbon hydrogen and methanol. Results also suggest that domestic hydrogen production benefits from synergies between electrolytic hydrogen and blue hydrogen, helping to maintain competitive prices."}, "https://arxiv.org/abs/2410.23177": {"title": "Far-right party influence on polarization dynamics in electoral campaign", "link": "https://arxiv.org/abs/2410.23177", "description": "arXiv:2410.23177v1 Announce Type: new \nAbstract: Political polarization has attracted increasing attention in recent years, driven by the rise of social media and the global emergence of far-right populist movements. This study investigates the dynamics of structural polarization during electoral campaigns in multi-party systems, with a particular focus on the presence of far-right actors and their influence on polarization patterns and hate speech. Using retweet networks as a measure of structural polarization, we analyze two case studies in Spain: the 2022 Andalusia regional elections, where the far-right party Vox was a significant contender, and the 2019 Barcelona city council elections, where the party had no representation. Our results reveal that the presence of a far-right party intensifies polarization, leading to the formation of two distinct ideological blocks aligned along left-right ideological axes, as observed in Andalusia. In contrast, the Catalan independence movement in Barcelona diluted the alignment of voters, resulting in a more complex, multi-axis polarization landscape. We also explore the relationship between polarization and hate speech, finding an anti-correlation between them in both cases. Our findings underscore the significant role of far-right movements in driving political polarization and the nuanced effects of different political contexts on polarization dynamics."}, "https://arxiv.org/abs/2311.03857": {"title": "Structure and inference in hypergraphs with node attributes", "link": "https://arxiv.org/abs/2311.03857", "description": "arXiv:2311.03857v2 Announce Type: replace \nAbstract: Many networked datasets with units interacting in groups of two or more, encoded with hypergraphs, are accompanied by extra information about nodes, such as the role of an individual in a workplace. Here we show how these node attributes can be used to improve our understanding of the structure resulting from higher-order interactions. We consider the problem of community detection in hypergraphs and develop a principled model that combines higher-order interactions and node attributes to better represent the observed interactions and to detect communities more accurately than using either of these types of information alone. The method learns automatically from the input data the extent to which structure and attributes contribute to explain the data, down weighing or discarding attributes if not informative. Our algorithmic implementation is efficient and scales to large hypergraphs and interactions of large numbers of units. We apply our method to a variety of systems, showing strong performance in hyperedge prediction tasks and in selecting community divisions that correlate with attributes when these are informative, but discarding them otherwise. Our approach illustrates the advantage of using informative node attributes when available with higher-order data."}, "https://arxiv.org/abs/2410.23502": {"title": "One rule does not fit all: deviations from universality in human mobility modeling", "link": "https://arxiv.org/abs/2410.23502", "description": "arXiv:2410.23502v1 Announce Type: new \nAbstract: The accurate modeling of individual movement in cities has significant implications for policy decisions across various sectors. Existing research emphasizes the universality of human mobility, positing that simple models can capture population-level movements. However, population-level accuracy does not guarantee consistent performance across all individuals. By overlooking individual differences, universality laws may accurately describe certain groups while less precisely representing others, resulting in aggregate accuracy from a balance of discrepancies. Using large-scale mobility data, we assess individual-level accuracy of a universal model, the Exploration and Preferential Return (EPR), by examining deviations from expected behavior in two scaling laws - one related to exploration and the other to return patterns. Our findings reveal that, while the model can describe population-wide movement patterns, it displays widespread deviations linked to individuals' behavioral traits, socioeconomic status, and lifestyles, contradicting model assumptions like non-bursty exploration and preferential return. Specifically, individuals poorly represented by the EPR model tend to visit routine locations in sequences, exploring rarely but in a bursty manner when they do. Among socioeconomic factors, income most strongly correlates with significant deviations. Consequently, spatial inhomogeneity emerges in model accuracy, with lower performance concentrated in urbanized, densely populated areas, underscoring policy implications. Our results show that emphasizing population-wide models can propagate socioeconomic inequalities by poorly representing vulnerable population sectors."}, "https://arxiv.org/abs/2410.23559": {"title": "Geographic Space as Manifolds", "link": "https://arxiv.org/abs/2410.23559", "description": "arXiv:2410.23559v1 Announce Type: new \nAbstract: The communications and interrelations between different locations on the Earth's surface have far-reaching implications for both social and natural systems. Effective spatial analytics ideally require a spatial representation, where geographic principles are succinctly expressed within a defined metric space. However, common spatial representations, including map-based or network-based approaches, fall short by incompletely or inaccurately defining this metric space. Here we show, by introducing an inverse friction factor that captures the spatial constraints in spatial networks, that a homogeneous, low-dimensional spatial representation - termed the Geographic Manifold - can be achieved. We illustrate the effectiveness of the Geographic Manifold in two classic scenarios of spatial analytics - location choice and propagation, where the otherwise complicated analyses are reduced to straightforward regular partitioning and concentric diffusing, respectively on the manifold with a high degree of accuracy. We further empirically explain and formally prove the general existence of the Geographic Manifold, which is grounded in the intrinsic Euclidean low-dimensional statistical physics properties of geographic phenomena. This work represents a step towards formalizing Tobler's famous First Law of Geography from a geometric approach, where a regularized geospace thereby yielded is expected to contribute in learning abstract spatial structure representations for understanding and optimization purposes."}, "https://arxiv.org/abs/2410.23638": {"title": "Unearthing a Billion Telegram Posts about the 2024 U", "link": "https://arxiv.org/abs/2410.23638", "description": "arXiv:2410.23638v1 Announce Type: new \nAbstract: With its lenient moderation policies and long-standing associations with potentially unlawful activities, Telegram has become an incubator for problematic content, frequently featuring conspiratorial, hyper-partisan, and fringe narratives. In the political sphere, these concerns are amplified by reports of Telegram channels being used to organize violent acts, such as those that occurred during the Capitol Hill attack on January 6, 2021. As the 2024 U.S. election approaches, Telegram remains a focal arena for societal and political discourse, warranting close attention from the research community, regulators, and the media. Based on these premises, we introduce and release a Telegram dataset focused on the 2024 U.S. Presidential Election, featuring over 30,000 chats and half a billion messages, including chat details, profile pictures, messages, and user information. We constructed a network of chats and analyzed the 500 most central ones, examining their shared messages. This resource represents the largest public Telegram dataset to date, offering an unprecedented opportunity to study political discussion on Telegram in the lead-up to the 2024 U.S. election. We will continue to collect data until the end of 2024, and routinely update the dataset released at: https://github.com/leonardo-blas/usc-tg-24-us-election"}, "https://arxiv.org/abs/2410.23688": {"title": "The Influence of Ridership Weighting on Targeting and Recovery Strategies for Urban Rail Rapid Transit Systems", "link": "https://arxiv.org/abs/2410.23688", "description": "arXiv:2410.23688v1 Announce Type: new \nAbstract: The resilience of urban rapid transit systems (URTs) to a rapidly evolving threat space is of much concern. Extreme rainfall events are both intensifying and growing more frequent under continuing climate change, exposing transit systems to flooding, while cyber threats and emerging technologies such as unmanned aerial vehicles are exposing such systems to targeted disruptions. An imperative has emerged to model how networked infrastructure systems fail and devise strategies to efficiently recover from disruptions. Passenger flow approaches can quantify more dimensions of resilience than network science-based approaches, but the former typically requires granular data from automatic fare collection and suffers from large runtime complexities. Some attempts have been made to include accessible low-resolution ridership data in topological frameworks. However, there is yet to be a systematic investigation of the effects of incorporating low-dimensional, coarsely-averaged ridership volume into topological network science methodologies. We simulate targeted attack and recovery sequences using station-level ridership, four centrality measures, and weighted combinations thereof. Resilience is quantified using two topological measures of performance: the node count of a network's giant connected component (GCC), and a new measure termed the \"highest ridership connected component\" (HRCC). Three transit systems are used as case studies: the subways of Boston, New York, and Osaka. Results show that centrality-based strategies are most effective when measuring performance via GCC, while centrality-ridership hybrid strategies perform strongest by HRCC. We show that the most effective strategies vary by network characteristics and the mission goals of emergency managers, highlighting the need to plan for strategic adversaries and rapid recovery according to each city's unique needs."}, "https://arxiv.org/abs/2410.23740": {"title": "Training on inclusivity and cultural diversity in the ALICE Collaboration", "link": "https://arxiv.org/abs/2410.23740", "description": "arXiv:2410.23740v1 Announce Type: new \nAbstract: Large experimental Collaborations at the LHC (ALICE, ATLAS, CMS, and LHCb) bring together over 13,000 people from hundreds of institutes over the world. There are many (working) cultures inside these international Collaborations. It is important to acknowledge that cultural differences exist and manifest in our workspaces in various ways such as communication style, attitude and expectations, and ways to provide feedback on work, just to name a few examples.\n  Various studies over the world have raised concerns on mental wellbeing of academics, especially PhD students. A survey on mental wellbeing was conducted among all major LHC Collaborations in 2023. The survey attracted a total of 404 replies ranging from students to experienced researchers. Although the number of responses is not small, it corresponds to only a few per cent of the whole LHC community, and hence we cannot be sure of how representative the sample is. However, this limited sample contained worrisome findings on wellbeing among LHC researchers.\n  While moderate stress can enhance working performance and help in keeping deadlines, stress should remain in manageable level and one should take care of adequate recovery. However, there are some stress factors related to environment and ways we work that cause unnecessary load. In these proceedings, we will raise a few results on the LHC mental wellbeing survey and discuss trainings on inclusivity and cultural diversity in ALICE. These trainings aim to create a welcoming and positive work environment by strengthening the sense of community and commitment to the Collaboration. These, in turn, can have a positive impact both on wellbeing and productivity."}, "https://arxiv.org/abs/2410.24065": {"title": "Social contagion with emotional group interactions", "link": "https://arxiv.org/abs/2410.24065", "description": "arXiv:2410.24065v1 Announce Type: new \nAbstract: Individual decisions and behaviors are shaped not only by direct interactions with others but also by the collective emotional dynamics within groups. In this work, we introduce the signed simplicial contagion model, integrating both pairwise and emotional group interactions to investigate contagion dynamics in signed networks. Through mean field analysis and numerical simulations, we show that emotional group interactions can induce discontinuous phase transitions, bistable behavior, and hysteresis loops. However, as the proportion of negative edges q increases, the influence of group interactions weakens under a given transmission strength, driving a shift from discontinuous to continuous phase transitions. Our findings reveal that pairwise and group interactions respond differently to changes in q: group interactions display nonlinear sensitivity, while pairwise interactions exhibit a more gradual, linear response. This divergence shifts the dominant mechanisms of contagion, depending on the levels of trust and distrust in the network, providing deeper insights into how emotional relational shape the spread of contagion in social systems."}, "https://arxiv.org/abs/2410.23420": {"title": "Projections of Earth's Technosphere: Luminosity and Mass as Limits to Growth", "link": "https://arxiv.org/abs/2410.23420", "description": "arXiv:2410.23420v1 Announce Type: cross \nAbstract: Earth remains the only known example of a planet with technology, and future projections of Earth's trajectory provide a basis and motivation for approaching the search for extraterrestrial technospheres. Conventional approaches toward projecting Earth's technosphere include applications of the Kardashev scale, which suggest the possibility that energy-intensive civilizations may expand to harness the entire energy output available to their planet, host star, or even the entire galaxy. In this study, we argue that the Kardashev scale is better understood as a \"luminosity limit\" that describes the maximum capacity for a civilization to harvest luminous stellar energy across a given spatial domain, and we note that thermodynamic efficiency will always keep a luminosity-limited technosphere from actually reaching this theoretical limit. We suggest the possibility that an advanced technosphere might evolve beyond this luminosity limit to draw its energy directly from harvesting stellar mass, and we also discuss possible trajectories that could exist between Earth today and such hypothetical \"stellivores.\" We develop a framework to describe trajectories for long-lived technospheres that optimize their growth strategies between exploration and exploitation, unlike Earth today. We note that analyses of compact accreting stars could provide ways to test the stellivore hypothesis, and we more broadly suggest an expansion of technosignature search strategies beyond those that reside exactly at the luminosity limit."}, "https://arxiv.org/abs/2410.23432": {"title": "Web Scraping for Research: Legal, Ethical, Institutional, and Scientific Considerations", "link": "https://arxiv.org/abs/2410.23432", "description": "arXiv:2410.23432v1 Announce Type: cross \nAbstract: Scientists across disciplines often use data from the internet to conduct research, generating valuable insights about human behavior. However, as generative AI relying on massive text corpora becomes increasingly valuable, platforms have greatly restricted access to data through official channels. As a result, researchers will likely engage in more web scraping to collect data, introducing new challenges and concerns for researchers. This paper proposes a comprehensive framework for web scraping in social science research for U.S.-based researchers, examining the legal, ethical, institutional, and scientific factors that researchers should consider when scraping the web. We present an overview of the current regulatory environment impacting when and how researchers can access, collect, store, and share data via scraping. We then provide researchers with recommendations to conduct scraping in a scientifically legitimate and ethical manner. We aim to equip researchers with the relevant information to mitigate risks and maximize the impact of their research amidst this evolving data access landscape."}, "https://arxiv.org/abs/2410.23799": {"title": "Clustering Coefficient Reflecting Pairwise Relationships within Hyperedges", "link": "https://arxiv.org/abs/2410.23799", "description": "arXiv:2410.23799v1 Announce Type: cross \nAbstract: Hypergraphs are generalizations of simple graphs that allow for the representation of complex group interactions beyond pairwise relationships. Clustering coefficients, which quantify the local link density in networks, have been widely studied even for hypergraphs. However, existing definitions of clustering coefficients for hypergraphs do not fully capture the pairwise relationships within hyperedges. In this study, we propose a novel clustering coefficient for hypergraphs that addresses this limitation by transforming the hypergraph into a weighted graph and calculating the clustering coefficient on the resulting graph. Our definition reflects the local link density more accurately than existing definitions. We demonstrate through theoretical evaluation on higher-order motifs that the proposed definition is consistent with the clustering coefficient for simple graphs and effectively captures relationships within hyperedges missed by existing definitions. Empirical evaluation on real-world hypergraph datasets shows that our definition exhibits similar overall clustering tendencies as existing definitions while providing more precise measurements, especially for hypergraphs with larger hyperedges. The proposed clustering coefficient has the potential to reveal structural characteristics in complex hypergraphs that are not detected by existing definitions, leading to a deeper understanding of the underlying interaction patterns in complex hypergraphs."}, "https://arxiv.org/abs/2410.23855": {"title": "RAGraph: A General Retrieval-Augmented Graph Learning Framework", "link": "https://arxiv.org/abs/2410.23855", "description": "arXiv:2410.23855v1 Announce Type: cross \nAbstract: Graph Neural Networks (GNNs) have become essential in interpreting relational data across various domains, yet, they often struggle to generalize to unseen graph data that differs markedly from training instances. In this paper, we introduce a novel framework called General Retrieval-Augmented Graph Learning (RAGraph), which brings external graph data into the general graph foundation model to improve model generalization on unseen scenarios. On the top of our framework is a toy graph vector library that we established, which captures key attributes, such as features and task-specific label information. During inference, the RAGraph adeptly retrieves similar toy graphs based on key similarities in downstream tasks, integrating the retrieved data to enrich the learning context via the message-passing prompting mechanism. Our extensive experimental evaluations demonstrate that RAGraph significantly outperforms state-of-the-art graph learning methods in multiple tasks such as node classification, link prediction, and graph classification across both dynamic and static datasets. Furthermore, extensive testing confirms that RAGraph consistently maintains high performance without the need for task-specific fine-tuning, highlighting its adaptability, robustness, and broad applicability."}, "https://arxiv.org/abs/2301.11486": {"title": "Sub-Standards and Mal-Practices: Misinformation's Role in Insular, Polarized, and Toxic Interactions on Reddit", "link": "https://arxiv.org/abs/2301.11486", "description": "arXiv:2301.11486v3 Announce Type: replace \nAbstract: In this work, we examine the influence of unreliable information on political incivility and toxicity on the social media platform Reddit. We show that comments on articles from unreliable news websites are posted more often in right-leaning subreddits and that within individual subreddits, comments, on average, are 32% more likely to be toxic compared to comments on reliable news articles. Using a regression model, we show that these results hold after accounting for partisanship and baseline toxicity rates within individual subreddits. Utilizing a zero-inflated negative binomial regression, we further show that as the toxicity of subreddits increases, users are more likely to comment on posts from known unreliable websites. Finally, modeling user interactions with an exponential random graph model, we show that when reacting to a Reddit submission that links to a website known for spreading unreliable information, users are more likely to be toxic to users of different political beliefs. Our results collectively illustrate that low-quality/unreliable information not only predicts increased toxicity but also polarizing interactions between users of different political orientations."}, "https://arxiv.org/abs/2307.10349": {"title": "Twits, Toxic Tweets, and Tribal Tendencies: Trends in Politically Polarized Posts on Twitter", "link": "https://arxiv.org/abs/2307.10349", "description": "arXiv:2307.10349v2 Announce Type: replace \nAbstract: Social media platforms are often blamed for exacerbating political polarization and worsening public dialogue. Many claim that hyperpartisan users post pernicious content, slanted to their political views, inciting contentious and toxic conversations. However, what factors are actually associated with increased online toxicity and negative interactions? In this work, we explore the role that partisanship and affective polarization play in contributing to toxicity both on an individual user level and a topic level on Twitter/X. To do this, we train and open-source a DeBERTa-based toxicity detector with a contrastive objective that outperforms the Google Jigsaw Perspective Toxicity detector on the Civil Comments test dataset. Then, after collecting 89.6 million tweets from 43,151 Twitter/X users, we determine how several account-level characteristics, including partisanship along the US left-right political spectrum and account age, predict how often users post toxic content. Fitting a Generalized Additive Model to our data, we find that the diversity of views and the toxicity of the other accounts with which that user engages has a more marked effect on their own toxicity. Namely, toxic comments are correlated with users who engage with a wider array of political views. Performing topic analysis on the toxic content posted by these accounts using the large language model MPNet and a version of the DP-Means clustering algorithm, we find similar behavior across 5,288 individual topics, with users becoming more toxic as they engage with a wider diversity of politically charged topics."}, "https://arxiv.org/abs/2402.02518": {"title": "Unifying Generation and Prediction on Graphs with Latent Graph Diffusion", "link": "https://arxiv.org/abs/2402.02518", "description": "arXiv:2402.02518v2 Announce Type: replace-cross \nAbstract: In this paper, we propose the first framework that enables solving graph learning tasks of all levels (node, edge and graph) and all types (generation, regression and classification) using one formulation. We first formulate prediction tasks including regression and classification into a generic (conditional) generation framework, which enables diffusion models to perform deterministic tasks with provable guarantees. We then propose Latent Graph Diffusion (LGD), a generative model that can generate node, edge, and graph-level features of all categories simultaneously. We achieve this goal by embedding the graph structures and features into a latent space leveraging a powerful encoder and decoder, then training a diffusion model in the latent space. LGD is also capable of conditional generation through a specifically designed cross-attention mechanism. Leveraging LGD and the ``all tasks as generation'' formulation, our framework is capable of solving graph tasks of various levels and types. We verify the effectiveness of our framework with extensive experiments, where our models achieve state-of-the-art or highly competitive results across a wide range of generation and regression tasks."}, "https://arxiv.org/abs/2402.11821": {"title": "Microstructures and Accuracy of Graph Recall by Large Language Models", "link": "https://arxiv.org/abs/2402.11821", "description": "arXiv:2402.11821v3 Announce Type: replace-cross \nAbstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structural patterns) in their recall. We find that LLMs not only underperform often in graph recall, but also tend to favor more triangles and alternating 2-paths. Moreover, we find that more advanced LLMs have a striking dependence on the domain that a real-world graph comes from -- by yielding the best recall accuracy when the graph is narrated in a language style consistent with its original domain."}, "https://arxiv.org/abs/2411.00008": {"title": "Growth of Science and Women: Methodological Challenges of Using Structured Big Data", "link": "https://arxiv.org/abs/2411.00008", "description": "arXiv:2411.00008v1 Announce Type: new \nAbstract: In this research, we quantify an inflow of women into science in the past three decades. Structured Big Data allow us to estimate the contribution of women scientists to the growth of science by disciplines (N = STEMM 14 disciplines) and over time (1990-2023). A monolithic segment of STEMM science emerges from this research as divided between the disciplines in which the growth was powerfully driven by women - and the disciplines in which the role of women was marginal. There are four disciplines in which 50% of currently publishing scientists are women; and five disciplines in which more than 50% of currently young scientists are women. But there is also a cluster of four highly mathematized disciplines (MATH, COMP, PHYS, and ENG) in which the growth of science is only marginally driven by women. Digital traces left by scientists in their publications indexed in global datasets open two new dimensions in large-scale academic profession studies: time and gender. The growth of science in Europe was accompanied by growth in the number of women scientists, but with powerful cross-disciplinary and cross-generational differentiations. We examined the share of women scientists coming from ten different age cohorts for 32 European and four comparator countries (the USA, Canada, Australia, and Japan). Our study sample was N = 1,740,985 scientists (including 39.40% women scientists). Three critical methodological challenges of using structured Big Data of the bibliometric type were discussed: gender determination, academic age determination, and discipline determination."}, "https://arxiv.org/abs/2411.00019": {"title": "Comment on Lubineau et al", "link": "https://arxiv.org/abs/2411.00019", "description": "arXiv:2411.00019v1 Announce Type: new \nAbstract: In 2023, Lubineau et al. published an article [1] detailing several experiments carried out with dyslexic readers. These authors attempted to measure the change in reading performance under different reading conditions using flickering devices. Beyond the low-frequency systems which have nevertheless shown their interest for some cases, we restrict here our response to the high-frequency systems, i.e. electronically controlled glasses (Lexilens) and lamps, designed and built upon recent work by Le Floch and Ropars [2]. Lubineau et al. found no significant change in reading performance at either low or high frequencies and concluded that these devices provide no or minor benefits. Unfortunately, experimental misunderstandings and some methodological issues invalidate namely the main conclusion concerning the high frequency systems."}, "https://arxiv.org/abs/2411.00035": {"title": "Home Swapping -- An Innovative Approach to Reduce Traffic Congestion and Carbon Emissions", "link": "https://arxiv.org/abs/2411.00035", "description": "arXiv:2411.00035v1 Announce Type: new \nAbstract: Urban traffic congestion, worsened by the rapid urbanization and the increasing prevalence of private vehicles, has significantly increased commuting time for everyone. In this paper, we used a dataset with over 400,000 real mobility trajectories of individuals spanning 9 days in a major Chinese city to investigate an innovative approach to swap homes between households in addressing the challenge of peak-hour traffic congestion. We observed that, empirically, households choose their home location strategically such that the average commuting distance is roughly 3 times less than that when their home is randomly located, showing features of self-organization. Remarkably, we found that the average commuting distance can be further reduced by 50% through home swapping at the city-level, leading to a large reduction in traffic congestion. To make home swapping more realistic, we swap homes only if the following socio-demographic factors including the distance from the city center, housing price and amenity accessibility are preserved for both households, such that the average commuting distance can still be reduced by 13%. As both home-workplace distance and traffic congestion are reduced, as a side benefit, carbon emissions from vehicles are also greatly reduced by almost 80%, and 40% when socio-demographic factors are considered. The distance from the city center is shown to be the most influential factor affecting the benefit brought by home swapping, and further analysis indicates that developing a polycentric city layout could significantly enhance such benefit. This study suggests that mitigating traffic congestion requires a long-term, holistic and strategic approach to urban planning, suggesting a need for coordinating individual residence locations and a polycentric city layout."}, "https://arxiv.org/abs/2411.00036": {"title": "Coupling quantum-like cognition with the neuronal networks within generalized probability theory", "link": "https://arxiv.org/abs/2411.00036", "description": "arXiv:2411.00036v1 Announce Type: new \nAbstract: The recent years are characterized by intensive applications of the methodology and mathematical apparatus of quantum theory, quantum-like modeling, in cognition, psychology, and decision making. In spite of the successful applications of this approach to a variety of psychological effects, e.g., the order, conjunction, disjunction, and response replicability effects, one may (but need not) feel dissatisfaction due to the absence of clear coupling to the neurophysiological processes in the brain. For the moment, this is just a phenomenological approach. In this paper we construct the quantum-like representation of the networks of communicating neurons. It is based not on standard quantum theory, but on generalized probability theory (GPT) with the emphasis of the operational measurement approach. We employ GPT's version which is based on ordered linear state space (instead of complex Hilbert space). A network of communicating neurons is described as a weighted ordered graph that in turn is encoded by its weight matrix. The state space of weight matrices is embedded in GPT with effect-observables and state updates within measurement instruments theory. The latter plays the crucial role. This GPT based model shows the basic quantum-like effects, as e.g. the order, non-repeatability, and disjunction effects; the latter is also known as interference of decisions. This GPT coupling also supports quantum-like modeling in medical diagnostic for neurological diseases, as depression and epilepsy. Although the paper is concentrated on cognition and neuronal networks, the formalism and methodology can be straightforwardly applied to a variety of biological and social networks."}, "https://arxiv.org/abs/2411.00050": {"title": "Is the Italian university ready for a gender leap? A regional perspective", "link": "https://arxiv.org/abs/2411.00050", "description": "arXiv:2411.00050v1 Announce Type: new \nAbstract: A discussion on the readiness of Italian universities to address gender-related issues from a regional standpoint is proposed. A statistical analysis is conducted on data of all scholars enrolled in Italian universities from 2000 to 2023 to investigate why the glass ceiling of the full professor position remains so challenging to break in almost all scientific fields and across all regions of Italy."}, "https://arxiv.org/abs/2411.00061": {"title": "Exploring Fermi's Paradox using an Intragalactic Colonization Model", "link": "https://arxiv.org/abs/2411.00061", "description": "arXiv:2411.00061v1 Announce Type: new \nAbstract: We explore Fermi's Paradox via a system of differential equations and using simulations of dispersal and interactions between competing interplanetary civilizations. To quantify the resources and potentials of these worlds, three different state variables representing population, environment, and technology, are used. When encounters occur between two different civilizations, the deterministic Lanchester Battle Model is used to determine the outcome of the conflict. We use the Unity engine to simulate the possible outcomes of colonization by different types of civilizations to further investigate Fermi's question.\n  When growth rates of population, technology and nature are out of balance, planetary civilizations can collapse. If the balance is adequate, then some civilizations can develop into dominating ones; nevertheless, they leave large spatial gaps in the distribution of their colonies. The unexpected result is that small civilizations can be left in existence by dominating civilizations in a galaxy due to those large gaps. Our results provide some insights into the validity of various solutions to Fermi's Paradox."}, "https://arxiv.org/abs/2411.00249": {"title": "GraphC: Parameter-free Hierarchical Clustering of Signed Graph Networks v2", "link": "https://arxiv.org/abs/2411.00249", "description": "arXiv:2411.00249v1 Announce Type: new \nAbstract: Spectral clustering methodologies, when extended to accommodate signed graphs, have encountered notable limitations in effectively encapsulating inherent grouping relationships. Recent findings underscore a substantial deterioration in the efficacy of spectral clustering methods when applied to expansive signed networks. We introduce a scalable hierarchical Graph Clustering algorithm denominated GraphC. This algorithm excels at discerning optimal clusters within signed networks of varying magnitudes. GraphC aims to preserve the positive edge fractions within communities during partitioning while concurrently maximizing the negative edge fractions between communities. Importantly, GraphC does not require a predetermined cluster count (denoted as k). Empirical substantiation of GraphC 's efficacy is provided through a comprehensive evaluation involving fourteen datasets juxtaposed against ten baseline signed graph clustering algorithms. The algorithm's scalability is demonstrated through its application to extensive signed graphs drawn from Amazon-sourced datasets, each comprising tens of millions of vertices and edges. A noteworthy accomplishment is evidenced, with an average cumulative enhancement of 18.64% (consisting of the summation of positive edge fractions within communities and negative edge fractions between communities) over the second-best baseline for each respective signed graph. It is imperative to note that this evaluation excludes instances wherein all baseline algorithms failed to execute comprehensively."}, "https://arxiv.org/abs/2411.00262": {"title": "Content Aware Analysis of Scholarly Networks: A Case Study on CORD19 Dataset", "link": "https://arxiv.org/abs/2411.00262", "description": "arXiv:2411.00262v1 Announce Type: new \nAbstract: This paper investigates the relationships among key elements of scientific research network, namely articles, researchers, and journals. We introduce a novel approach to use semantic information through the HITS algorithm based propagation of topic information in the network. The topic information is derived by using the Named Entity Recognition and Entity Linkage. In our case, MedCAT is used to extract the topics from the CORD19 Dataset, which is a corpus of academic articles about COVID-19 and coronavirus scientific network. Our approach focuses on the COVID-19 domain, utilizing the CORD-19 dataset to demonstrate the efficacy of integrating topic-related information within the citation framework. Through the application of a hybrid HITS algorithm, we show that incorporating topic data significantly influences article rankings, revealing deeper insights into the structure of the academic community."}, "https://arxiv.org/abs/2411.00376": {"title": "A Public Dataset Tracking Social Media Discourse about the 2024 U", "link": "https://arxiv.org/abs/2411.00376", "description": "arXiv:2411.00376v1 Announce Type: new \nAbstract: In this paper, we introduce the first release of a large-scale dataset capturing discourse on $\\mathbb{X}$ (a.k.a., Twitter) related to the upcoming 2024 U.S. Presidential Election. Our dataset comprises 22 million publicly available posts on X.com, collected from May 1, 2024, to July 31, 2024, using a custom-built scraper, which we describe in detail. By employing targeted keywords linked to key political figures, events, and emerging issues, we aligned data collection with the election cycle to capture evolving public sentiment and the dynamics of political engagement on social media. This dataset offers researchers a robust foundation to investigate critical questions about the influence of social media in shaping political discourse, the propagation of election-related narratives, and the spread of misinformation. We also present a preliminary analysis that highlights prominent hashtags and keywords within the dataset, offering initial insights into the dominant themes and conversations occurring in the lead-up to the election. Our dataset is available at: url{https://github.com/sinking8/usc-x-24-us-election"}, "https://arxiv.org/abs/2411.00606": {"title": "Domain-Informed Negative Sampling Strategies for Dynamic Graph Embedding in Meme Stock-Related Social Networks", "link": "https://arxiv.org/abs/2411.00606", "description": "arXiv:2411.00606v1 Announce Type: new \nAbstract: Social network platforms like Reddit are increasingly impacting real-world economics. Meme stocks are a recent phenomena where price movements are driven by retail investors organising themselves via social networks. To study the impact of social networks on meme stocks, the first step is to analyse these networks. Going forward, predicting meme stocks' returns would require to predict dynamic interactions first. This is different from conventional link prediction, frequently applied in e.g. recommendation systems. For this task, it is essential to predict more complex interaction dynamics, such as the exact timing and interaction types like loops. These are crucial for linking the network to meme stock price movements. Dynamic graph embedding (DGE) has recently emerged as a promising approach for modeling dynamic graph-structured data. However, current negative sampling strategies, an important component of DGE, are designed for conventional dynamic link prediction and do not capture the specific patterns present in meme stock-related social networks. This limits the training and evaluation of DGE models in analysing such social networks. To overcome this drawback, we propose novel negative sampling strategies based on the analysis of real meme stock-related social networks and financial knowledge. Our experiments show that the proposed negative sampling strategy can better evaluate and train DGE models targeted at meme stock-related social networks compared to existing baselines."}, "https://arxiv.org/abs/2411.00612": {"title": "How to Bridge Spatial and Temporal Heterogeneity in Link Prediction? A Contrastive Method", "link": "https://arxiv.org/abs/2411.00612", "description": "arXiv:2411.00612v1 Announce Type: new \nAbstract: Temporal Heterogeneous Networks play a crucial role in capturing the dynamics and heterogeneity inherent in various real-world complex systems, rendering them a noteworthy research avenue for link prediction. However, existing methods fail to capture the fine-grained differential distribution patterns and temporal dynamic characteristics, which we refer to as spatial heterogeneity and temporal heterogeneity. To overcome such limitations, we propose a novel \\textbf{C}ontrastive Learning-based \\textbf{L}ink \\textbf{P}rediction model, \\textbf{CLP}, which employs a multi-view hierarchical self-supervised architecture to encode spatial and temporal heterogeneity. Specifically, aiming at spatial heterogeneity, we develop a spatial feature modeling layer to capture the fine-grained topological distribution patterns from node- and edge-level representations, respectively. Furthermore, aiming at temporal heterogeneity, we devise a temporal information modeling layer to perceive the evolutionary dependencies of dynamic graph topologies from time-level representations. Finally, we encode the spatial and temporal distribution heterogeneity from a contrastive learning perspective, enabling a comprehensive self-supervised hierarchical relation modeling for the link prediction task. Extensive experiments conducted on four real-world dynamic heterogeneous network datasets verify that our \\mymodel consistently outperforms the state-of-the-art models, demonstrating an average improvement of 10.10\\%, 13.44\\% in terms of AUC and AP, respectively."}, "https://arxiv.org/abs/2411.00714": {"title": "Self-reinforcing cascades: A spreading model for beliefs or products of varying intensity or quality", "link": "https://arxiv.org/abs/2411.00714", "description": "arXiv:2411.00714v1 Announce Type: new \nAbstract: Models of how things spread often assume that transmission mechanisms are fixed over time. However, social contagions--the spread of ideas, beliefs, innovations--can lose or gain in momentum as they spread: ideas can get reinforced, beliefs strengthened, products refined. We study the impacts of such self-reinforcement mechanisms in cascade dynamics. We use different mathematical modeling techniques to capture the recursive, yet changing nature of the process. We find a critical regime with a range of power-law cascade size distributions with varying scaling exponents. This regime clashes with classic models, where criticality requires fine tuning at a precise critical point. Self-reinforced cascades produce critical-like behavior over a wide range of parameters, which may help explain the ubiquity of power-law distributions in empirical social data."}, "https://arxiv.org/abs/2411.00028": {"title": "Synergizing LLM Agents and Knowledge Graph for Socioeconomic Prediction in LBSN", "link": "https://arxiv.org/abs/2411.00028", "description": "arXiv:2411.00028v1 Announce Type: cross \nAbstract: The fast development of location-based social networks (LBSNs) has led to significant changes in society, resulting in popular studies of using LBSN data for socioeconomic prediction, e.g., regional population and commercial activity estimation. Existing studies design various graphs to model heterogeneous LBSN data, and further apply graph representation learning methods for socioeconomic prediction. However, these approaches heavily rely on heuristic ideas and expertise to extract task-relevant knowledge from diverse data, which may not be optimal for specific tasks. Additionally, they tend to overlook the inherent relationships between different indicators, limiting the prediction accuracy. Motivated by the remarkable abilities of large language models (LLMs) in commonsense reasoning, embedding, and multi-agent collaboration, in this work, we synergize LLM agents and knowledge graph for socioeconomic prediction. We first construct a location-based knowledge graph (LBKG) to integrate multi-sourced LBSN data. Then we leverage the reasoning power of LLM agent to identify relevant meta-paths in the LBKG for each type of socioeconomic prediction task, and design a semantic-guided attention module for knowledge fusion with meta-paths. Moreover, we introduce a cross-task communication mechanism to further enhance performance by enabling knowledge sharing across tasks at both LLM agent and KG levels. On the one hand, the LLM agents for different tasks collaborate to generate more diverse and comprehensive meta-paths. On the other hand, the embeddings from different tasks are adaptively merged for better socioeconomic prediction. Experiments on two datasets demonstrate the effectiveness of the synergistic design between LLM and KG, providing insights for information sharing across socioeconomic prediction tasks."}, "https://arxiv.org/abs/2411.00398": {"title": "Spatial public goods games on any population structure", "link": "https://arxiv.org/abs/2411.00398", "description": "arXiv:2411.00398v1 Announce Type: cross \nAbstract: Understanding the emergence of cooperation in spatially structured populations has advanced significantly in the context of pairwise games, but the fundamental theory of group-based public goods games (PGGs) remains less explored. Here, we provide theoretical conditions under which cooperation thrive in spatial PGGs on any population structure, which are accurate under weak selection. We find that PGGs can support cooperation across all kinds of model details and on almost all network structures in contrast to pairwise games. For example, a class of networks that would otherwise fail to produce cooperation, such as star graphs, are particularly conducive to cooperation in spatial PGGs. This fundamental advantage of spatial PGGs derives from reciprocity through second-order interactions, allowing local structures such as the clustering coefficient to play positive roles. We also verify the robustness of spatial PGGs on empirical networks where pairwise games cannot support cooperation, which implies that PGGs could be a universal interaction mode in real-world systems."}, "https://arxiv.org/abs/2411.00644": {"title": "What can we learn from marketing skills as a bipartite network from accredited programs?", "link": "https://arxiv.org/abs/2411.00644", "description": "arXiv:2411.00644v1 Announce Type: cross \nAbstract: The relationship between professional skills and higher education programs is modeled as a non-directed bipartite network with binary entries representing the links between 28 skills (as captured by the occupational information network, O*NET) and 258 graduate program summaries (as captured by commercial brochures of graduate programs in marketing with accreditation standards of the Association to Advance Collegiate Schools of Business). While descriptive analysis for skills suggests a qualitative lack of alignment between the job demands captured by O*NET, inferential analyses based on exponential random graph model estimates show that skills' popularity and homophily coexist with a systematic yet weak alignment to job demands for marketing managers."}, "https://arxiv.org/abs/2411.00702": {"title": "A graph-based approach to extracting narrative signals from public discourse", "link": "https://arxiv.org/abs/2411.00702", "description": "arXiv:2411.00702v1 Announce Type: cross \nAbstract: Narratives are key interpretative devices by which humans make sense of political reality. As the significance of narratives for understanding current societal issues such as polarization and misinformation becomes increasingly evident, there is a growing demand for methods that support their empirical analysis. To this end, we propose a graph-based formalism and machine-guided method for extracting, representing, and analyzing selected narrative signals from digital textual corpora, based on Abstract Meaning Representation (AMR). The formalism and method introduced here specifically cater to the study of political narratives that figure in texts from digital media such as archived political speeches, social media posts, political manifestos and transcripts of parliamentary debates. We conceptualize these political narratives as a type of ontological narratives: stories by which actors position themselves as political beings, and which are akin to political worldviews in which actors present their normative vision of the world, or aspects thereof. We approach the study of such political narratives as a problem of information retrieval: starting from a textual corpus, we first extract a graph-like representation of the meaning of each sentence in the corpus using AMR. Drawing on transferable concepts from narratology, we then apply a set of heuristics to filter these graphs for representations of 1) actors, 2) the events in which these actors figure, and 3) traces of the perspectivization of these events. We approach these references to actors, events, and instances of perspectivization as core narrative signals that initiate a further analysis by alluding to larger political narratives. By means of a case study of State of the European Union addresses, we demonstrate how the formalism can be used to inductively surface signals of political narratives from public discourse."}, "https://arxiv.org/abs/2405.00636": {"title": "Robustness of graph embedding methods for community detection", "link": "https://arxiv.org/abs/2405.00636", "description": "arXiv:2405.00636v2 Announce Type: replace \nAbstract: This study investigates the robustness of graph embedding methods for community detection in the face of network perturbations, specifically edge deletions. Graph embedding techniques, which represent nodes as low-dimensional vectors, are widely used for various graph machine learning tasks due to their ability to capture structural properties of networks effectively. However, the impact of perturbations on the performance of these methods remains relatively understudied. The research considers state-of-the-art graph embedding methods from two families: matrix factorization (e.g., LE, LLE, HOPE, M-NMF) and random walk-based (e.g., DeepWalk, LINE, node2vec). Through experiments conducted on both synthetic and real-world networks, the study reveals varying degrees of robustness within each family of graph embedding methods. The robustness is found to be influenced by factors such as network size, initial community partition strength, and the type of perturbation. Notably, node2vec and LLE consistently demonstrate higher robustness for community detection across different scenarios, including networks with degree and community size heterogeneity. These findings highlight the importance of selecting an appropriate graph embedding method based on the specific characteristics of the network and the task at hand, particularly in scenarios where robustness to perturbations is crucial."}, "https://arxiv.org/abs/2404.03139": {"title": "Theoretical and Empirical Insights into the Origins of Degree Bias in Graph Neural Networks", "link": "https://arxiv.org/abs/2404.03139", "description": "arXiv:2404.03139v2 Announce Type: replace-cross \nAbstract: Graph Neural Networks (GNNs) often perform better for high-degree nodes than low-degree nodes on node classification tasks. This degree bias can reinforce social marginalization by, e.g., privileging celebrities and other high-degree actors in social networks during social and content recommendation. While researchers have proposed numerous hypotheses for why GNN degree bias occurs, we find via a survey of 38 degree bias papers that these hypotheses are often not rigorously validated, and can even be contradictory. Thus, we provide an analysis of the origins of degree bias in message-passing GNNs with different graph filters. We prove that high-degree test nodes tend to have a lower probability of misclassification regardless of how GNNs are trained. Moreover, we show that degree bias arises from a variety of factors that are associated with a node's degree (e.g., homophily of neighbors, diversity of neighbors). Furthermore, we show that during training, some GNNs may adjust their loss on low-degree nodes more slowly than on high-degree nodes; however, with sufficiently many epochs of training, message-passing GNNs can achieve their maximum possible training accuracy, which is not significantly limited by their expressive power. Throughout our analysis, we connect our findings to previously-proposed hypotheses for the origins of degree bias, supporting and unifying some while drawing doubt to others. We validate our theoretical findings on 8 common real-world networks, and based on our theoretical and empirical insights, describe a roadmap to alleviate degree bias."}, "https://arxiv.org/abs/2405.00957": {"title": "IntraMix: Intra-Class Mixup Generation for Accurate Labels and Neighbors", "link": "https://arxiv.org/abs/2405.00957", "description": "arXiv:2405.00957v2 Announce Type: replace-cross \nAbstract: Graph Neural Networks (GNNs) have shown great performance in various tasks, with the core idea of learning from data labels and aggregating messages within the neighborhood of nodes. However, the common challenges in graphs are twofold: insufficient accurate (high-quality) labels and limited neighbors for nodes, resulting in weak GNNs. Existing graph augmentation methods typically address only one of these challenges, often adding training costs or relying on oversimplified or knowledge-intensive strategies, limiting their generalization. To simultaneously address both challenges faced by graphs in a generalized way, we propose an elegant method called IntraMix. Considering the incompatibility of vanilla Mixup with the complex topology of graphs, IntraMix innovatively employs Mixup among inaccurate labeled data of the same class, generating high-quality labeled data at minimal cost. Additionally, it finds data with high confidence of being clustered into the same group as the generated data to serve as their neighbors, thereby enriching the neighborhoods of graphs. IntraMix efficiently tackles both issues faced by graphs and challenges the prior notion of the limited effectiveness of Mixup in node classification. IntraMix is a theoretically grounded plug-in-play method that can be readily applied to all GNNs. Extensive experiments demonstrate the effectiveness of IntraMix across various GNNs and datasets. Our code is available at: https://github.com/Zhengsh123/IntraMix."}, "https://arxiv.org/abs/2411.00808": {"title": "Astronomical outreach and education in marginalised and indigenous communities: astronomy as a tool for social development", "link": "https://arxiv.org/abs/2411.00808", "description": "arXiv:2411.00808v1 Announce Type: new \nAbstract: The way we look at the sky is connected to the cosmological paradigm embraced by the society we live in. On the other hand, several astronomical concepts reinforce the idea of a common humanity. Yet, scientific outreach is frequenty reaching out only to a specific part of the world population, often excluding people living in extreme social vulnerability, victims of violence and prejudice, fighting for their lives and for the right of living according to their traditions. We present two outreach projects, developed in Brazil, funded by the Office of Astronomy for Development (OAD) of the International Astronomical Union (IAU), i.e. 'Under Other Skies' and 'OruMbya', which tackle the importance of ethno-astronomy, and the collaboration with leaders and cultural agents of marginalised communities. We also describe an educational project born in the favela of Cantagalo Pav\\~ao Pava\\~azinho (PPG), in Rio de Janeiro, during the COVID19 pandemic, which started a collaboration with local educators and artists to offer classes of astronomy and English language to children in the favela"}, "https://arxiv.org/abs/2411.00975": {"title": "Analyzing Social Networks of Actors in Movies and TV Shows", "link": "https://arxiv.org/abs/2411.00975", "description": "arXiv:2411.00975v1 Announce Type: new \nAbstract: The paper offers a comprehensive analysis of social networks among movie actors and directors in the film industry. Utilizing data from IMDb and Netflix, we leverage Python and NetworkX to uncover valuable insights into the movie industry's intricate web of collaborations. Key findings include identifying the top actors and directors in the OTT sector, tracking the rise of movies on OTT platforms, and analyzing centrality measures for actors. We also explore the hidden patterns within the movie data, unveiling the shortest paths between actors and predicting future collaborations. Cluster analysis categorizes movies based on various criteria, revealing the most insular and liberal clusters and identifying crossover actors bridging different segments of the industry. The study highlights that actors predominantly collaborate within language groups, transcending national boundaries. We investigate the degree of isolation of Bollywood from global cinema and identify actors working across world clusters. The project provides valuable insights into the evolving dynamics of the film industry and the impact of OTT platforms, benefiting industry professionals, scholars, and enthusiasts."}, "https://arxiv.org/abs/2411.01143": {"title": "A Large-scale Time-aware Agents Simulation for Influencer Selection in Digital Advertising Campaigns", "link": "https://arxiv.org/abs/2411.01143", "description": "arXiv:2411.01143v1 Announce Type: new \nAbstract: In the digital world, influencers are pivotal as opinion leaders, shaping the views and choices of their influencees. Modern advertising often follows this trend, where marketers choose appropriate influencers for product endorsements, based on thorough market analysis. Previous studies on influencer selection have typically relied on numerical representations of individual opinions and interactions, a method that simplifies the intricacies of social dynamics. In this work, we first introduce a Time-aware Influencer Simulator (TIS), helping promoters identify and select the right influencers to market their products, based on LLM simulation. To validate our approach, we conduct experiments on the public advertising campaign dataset SAGraph which encompasses social relationships, posts, and user interactions. The results show that our method outperforms traditional numerical feature-based approaches and methods using limited LLM agents. Our research shows that simulating user timelines and content lifecycles over time simplifies scaling, allowing for large-scale agent simulations in social networks. Additionally, LLM-based agents for social recommendations and advertising offer substantial benefits for decision-making in promotional campaigns."}, "https://arxiv.org/abs/2411.01239": {"title": "I've Heard This Before: Initial Results on Tiktok's Impact On the Re-Popularization of Songs", "link": "https://arxiv.org/abs/2411.01239", "description": "arXiv:2411.01239v1 Announce Type: new \nAbstract: With over a billion active users, TikTok's video-sharing service is currently one of the largest social media websites. This rise in TikTok's popularity has made the website a central platform for music discovery. In this paper, we analyze how TikTok helps to revitalize older songs. To do so, we use both the popularity of songs shared on TikTok and how the platform allows songs to propagate to other places on the Web. We analyze data from TokBoard, a website measuring such popularity over time, and Google Trends, which captures songs' overall Web search interest. Our analysis initially focuses on whether TokBoard can cause (Granger Causality) popularity on Google Trends. Next, we examine whether TikTok and Google Trends share the same virality patterns (via a Bass Model). To our knowledge, we are one of the first works to study song re-popularization via TikTok."}, "https://arxiv.org/abs/2411.01242": {"title": "Assessing the Impact of Sampling, Remixes, and Covers on Original Song Popularity", "link": "https://arxiv.org/abs/2411.01242", "description": "arXiv:2411.01242v1 Announce Type: new \nAbstract: Music digitalization has introduced new forms of composition known as \"musical borrowings\", where composers use elements of existing songs -- such as melodies, lyrics, or beats -- to create new songs. Using Who Sampled data and Google Trends, we examine how the popularity of a borrowing song affects the original. Employing Regression Discontinuity Design (RDD) for short-term effects and Granger Causality for long-term impacts, we find evidence of causal popularity boosts in some cases. Borrowee songs can revive interest in older tracks, underscoring economic dynamics that may support fairer compensation in the music industry."}, "https://arxiv.org/abs/2411.01330": {"title": "Unfiltered Conversations: A Dataset of 2024 U", "link": "https://arxiv.org/abs/2411.01330", "description": "arXiv:2411.01330v1 Announce Type: new \nAbstract: Truth Social, launched as a social media platform with a focus on free speech, has become a prominent space for political discourse, attracting a user base with diverse, yet often conservative, viewpoints. As an emerging platform with minimal content moderation, Truth Social has facilitated discussions around contentious social and political issues but has also seen the spread of conspiratorial and hyper-partisan narratives. In this paper, we introduce and release a comprehensive dataset capturing activity on Truth Social related to the upcoming 2024 U.S. Presidential Election, including posts, replies, user interactions, content and media. This dataset comprises 1.5 million posts published between February, 2024 and October 2024, and encompasses key user engagement features and posts metadata. Data collection began in June 2024, though it includes posts published earlier, with the oldest post dating back to February 2022. This offers researchers a unique resource to study communication patterns, the formation of online communities, and the dissemination of information within Truth Social in the run-up to the election. By providing an in-depth view of Truth Social's user dynamics and content distribution, this dataset aims to support further research on political discourse within an alt-tech social media platform. The dataset is publicly available at https://github.com/kashish-s/TruthSocial_2024ElectionInitiative"}, "https://arxiv.org/abs/2411.01394": {"title": "Centrality in Collaboration: A Novel Algorithm for Social Partitioning Gradients in Community Detection for Multiple Oncology Clinical Trial Enrollments", "link": "https://arxiv.org/abs/2411.01394", "description": "arXiv:2411.01394v1 Announce Type: new \nAbstract: Patients at a comprehensive cancer center who do not achieve cure or remission following standard treatments often become candidates for clinical trials. Patients who participate in a clinical trial may be suitable for other studies. A key factor influencing patient enrollment in subsequent clinical trials is the structured collaboration between oncologists and most responsible physicians. Possible identification of these collaboration networks can be achieved through the analysis of patient movements between clinical trial intervention types with social network analysis and community detection algorithms. In the detection of oncologist working groups, the present study evaluates three community detection algorithms: Girvan-Newman, Louvain and an algorithm developed by the author. Girvan-Newman identifies each intervention as their own community, while Louvain groups interventions in a manner that is difficult to interpret. In contrast, the author's algorithm groups interventions in a way that is both intuitive and informative, with a gradient effect that is particularly useful for epidemiological research. This lays the groundwork for future subgroup analysis of clustered interventions."}, "https://arxiv.org/abs/2411.01424": {"title": "Effective Community Detection Over Streaming Bipartite Networks (Technical Report)", "link": "https://arxiv.org/abs/2411.01424", "description": "arXiv:2411.01424v1 Announce Type: new \nAbstract: The streaming bipartite graph is extensively used to model the dynamic relationship between two types of entities in many real-world applications, such as movie recommendations, location-based services, and online shopping. Since it contains abundant information, discovering the dense subgraph with high structural cohesiveness (i.e., community detection) in the bipartite streaming graph is becoming a valuable problem. Inspired by this, in this paper, we study the structure of community on the butterfly motif in the bipartite graph. We propose a novel problem, named Community Detection over Streaming Bipartite Network (CD-SBN), which aims to retrieve qualified communities with user-specific query keywords and high structural cohesiveness at snapshot and continuous scenarios. In particular, we formulate the user relationship score in the weighted bipartite network via the butterfly pattern and define a novel $(k,r,\\sigma)$-bitruss as the community structure. To efficiently tackle the CD-SBN problem, we design effective pruning strategies to rule out false alarms of $(k,r,\\sigma)$-bitruss and propose a hierarchical synopsis to facilitate the CD-SBN processing. Due to the dynamic of streaming bipartite networks, we devise an efficient procedure for incremental graph maintenance. We develop an efficient algorithm to answer the snapshot and continuous CD-SBN query by traversing the synopsis and applying the pruning strategies. With extensive experiments, we demonstrate the efficiency and effectiveness of our proposed CD-SBN processing approach over real/synthetic streaming bipartite networks."}, "https://arxiv.org/abs/2411.01730": {"title": "How time and pollster history affect U", "link": "https://arxiv.org/abs/2411.01730", "description": "arXiv:2411.01730v1 Announce Type: new \nAbstract: In the months leading up to political elections in the United States, forecasts are widespread and take on multiple forms, including projections of what party will win the popular vote, state ratings, and predictions of vote margins at the state level. It can be challenging to evaluate how accuracy changes in the lead up to Election Day or to put probabilistic forecasts into historical context. Moreover, forecasts differ between analysts, highlighting the many choices in the forecasting process. With this as motivation, here we take a more comprehensive view and begin to unpack some of the choices involved in election forecasting. Building on a prior compartmental model of election dynamics, we present the forecasts of this model across months, years, and types of race. By gathering together monthly forecasts of presidential, senatorial, and gubernatorial races from 2004--2022, we provide a larger-scale perspective and discuss how treating polling data in different ways affects forecast accuracy. We conclude with our 2024 election forecasts (upcoming at the time of writing)."}, "https://arxiv.org/abs/2411.01852": {"title": "Auditing Political Exposure Bias: Algorithmic Amplification on Twitter/X Approaching the 2024 U", "link": "https://arxiv.org/abs/2411.01852", "description": "arXiv:2411.01852v1 Announce Type: new \nAbstract: Approximately 50% of tweets in X's user timelines are personalized recommendations from accounts they do not follow. This raises a critical question: what political content are users exposed to beyond their established networks, and how might this influence democratic discourse online? Due to the black-box nature and constant evolution of social media algorithms, much remains unknown about this aspect of users' content exposure, particularly as it pertains to potential biases in algorithmic curation. Prior research has shown that certain political groups and media sources are amplified within users' in-network tweets. However, the extent to which this amplification affects out-of-network recommendations remains unclear. As the 2024 U.S. Election approaches, addressing this question is essential for understanding the influence of algorithms on online political content consumption and its potential impact on users' perspectives. In this paper, we conduct a three-week audit of X's algorithmic content recommendations using a set of 120 sock-puppet monitoring accounts that capture tweets in their personalized ``For You'' timelines. Our objective is to quantify out-of-network content exposure for right- and left-leaning user profiles and to assess any potential biases in political exposure. Our findings indicate that X's algorithm skews exposure toward a few high-popularity accounts across all users, with right-leaning users experiencing the highest level of exposure inequality. Both left- and right-leaning users encounter amplified exposure to accounts aligned with their own political views and reduced exposure to opposing viewpoints. Additionally, we observe a right-leaning bias in exposure for new accounts within their default timelines."}, "https://arxiv.org/abs/2411.01947": {"title": "HACD: Harnessing Attribute Semantics and Mesoscopic Structure for Community Detection", "link": "https://arxiv.org/abs/2411.01947", "description": "arXiv:2411.01947v1 Announce Type: new \nAbstract: Community detection plays a pivotal role in uncovering closely connected subgraphs, aiding various real-world applications such as recommendation systems and anomaly detection. With the surge of rich information available for entities in real-world networks, the community detection problem in attributed networks has attracted widespread attention. While previous research has effectively leveraged network topology and attribute information for attributed community detection, these methods overlook two critical issues: (i) the semantic similarity between node attributes within the community, and (ii) the inherent mesoscopic structure, which differs from the pairwise connections of the micro-structure. To address these limitations, we propose HACD, a novel attributed community detection model based on heterogeneous graph attention networks. HACD treats node attributes as another type of node, constructs attributed networks into heterogeneous graph structures and employs attribute-level attention mechanisms to capture semantic similarity. Furthermore, HACD introduces a community membership function to explore mesoscopic community structures, enhancing the robustness of detected communities. Extensive experiments demonstrate the effectiveness and efficiency of HACD, outperforming state-of-the-art methods in attributed community detection tasks. Our code is publicly available at https://github.com/Anniran1/HACD1-wsdm."}, "https://arxiv.org/abs/2411.01990": {"title": "Measuring the Variability of Outcomes of the New Natural Daylighting Requirements in the Basque Country's Habitability Decree", "link": "https://arxiv.org/abs/2411.01990", "description": "arXiv:2411.01990v1 Announce Type: new \nAbstract: The Basque Country's Habitability Decree of 2022 redefines natural lighting requirements in residential spaces, increasing the Wall-to-Floor Ratio (WFR) compared to previous municipal and sectoral standards, based on the depth of the space considered. This regulatory adjustment seeks to optimize the quality of natural light, a key factor for well-being. However, the application of a geometric rule based on fixed proportions raises questions about the variability of achievable lighting results and their suitability for residential comfort needs. This study analyses the implementation of the regulation in two bedroom models, BR1 and BR2, selected as the typical and the deepest, but with less than 4 meters and meeting the rest of the dimensional requirements, respectively. Through simulations of the Daylight Factor (DF) using window proportions of 1:1, 3:2, 2:1, and 1:2 under a CIE overcast sky model, the dispersion in the obtained natural lighting has been examined. Such analysis allows for the quantification of lighting result variability within the regulatory parameters, exploring the complexity of meeting lighting requirements through an approach based exclusively on geometric proportions. Additionally, the impact of architectural and urban variables such as building height, the presence of courtyards, street width, and balcony depth has been considered. These factors, often overlooked in regulations and indirectly considered in D80/2022, play an important role in modulating natural light and lighting quality. At the end, the paper comments on the advantages and limitations of regulating natural light through arithmetic or geometric rules like the WFR, comparing this method with approaches based on detailed room-by-room analysis."}, "https://arxiv.org/abs/2411.02240": {"title": "Kinetic exchange opinion dynamics for the battleground-states in the 2024 US presidential elections", "link": "https://arxiv.org/abs/2411.02240", "description": "arXiv:2411.02240v1 Announce Type: new \nAbstract: The strongly polarizing political discourse in the U. S. implies that a small minority of the population, determining the outcome of the presidential elections in a few so called battleground-states, also determines the outcome of the overall election. Given the almost equal distributions of the electoral college members in the so-called blue and red states, the members elected from these battleground states would determine the election results. We build a kinetic exchange opinion model that takes into account the dynamical nature of the opinions of the individuals in the battleground states and the already determined core voters of the non-battleground states. In a fully connected graph, we consider the interaction among the population in the battleground states while the agents in the non-battleground states are assumed to have fixed opinions. We provide the analytical results and numerical simulations using realistic parameters from the opinion poll of the previous election's data. Counter-intuitively, a more noisy environment predicts a higher chance of the Democrats' win."}, "https://arxiv.org/abs/2411.02268": {"title": "Memory-Efficient Community Detection on Large Graphs Using Weighted Sketches", "link": "https://arxiv.org/abs/2411.02268", "description": "arXiv:2411.02268v1 Announce Type: new \nAbstract: Community detection in graphs identifies groups of nodes with denser connections within the groups than between them, and while existing studies often focus on optimizing detection performance, memory constraints become critical when processing large graphs on shared-memory systems. We recently proposed efficient implementations of the Louvain, Leiden, and Label Propagation Algorithms (LPA) for community detection. However, these incur significant memory overhead from the use of collision-free per-thread hashtables. To address this, we introduce memory-efficient alternatives using weighted Misra-Gries (MG) sketches, which replace the per-thread hashtables, and reduce memory demands in Louvain, Leiden, and LPA implementations - while incurring only a minor quality drop (up to 1%) and moderate runtime penalties. We believe that these approaches, though slightly slower, are well-suited for parallel processing and could outperform current memory-intensive techniques on systems with many threads."}, "https://arxiv.org/abs/2411.00813": {"title": "Personality Analysis from Online Short Video Platforms with Multi-domain Adaptation", "link": "https://arxiv.org/abs/2411.00813", "description": "arXiv:2411.00813v1 Announce Type: cross \nAbstract: Personality analysis from online short videos has gained prominence due to its applications in personalized recommendation systems, sentiment analysis, and human-computer interaction. Traditional assessment methods, such as questionnaires based on the Big Five Personality Framework, are limited by self-report biases and are impractical for large-scale or real-time analysis. Leveraging the rich, multi-modal data present in short videos offers a promising alternative for more accurate personality inference. However, integrating these diverse and asynchronous modalities poses significant challenges, particularly in aligning time-varying data and ensuring models generalize well to new domains with limited labeled data. In this paper, we propose a novel multi-modal personality analysis framework that addresses these challenges by synchronizing and integrating features from multiple modalities and enhancing model generalization through domain adaptation. We introduce a timestamp-based modality alignment mechanism that synchronizes data based on spoken word timestamps, ensuring accurate correspondence across modalities and facilitating effective feature integration. To capture temporal dependencies and inter-modal interactions, we employ Bidirectional Long Short-Term Memory networks and self-attention mechanisms, allowing the model to focus on the most informative features for personality prediction. Furthermore, we develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to improve performance in target domains with scarce labeled data. Extensive experiments on real-world datasets demonstrate that our framework significantly outperforms existing methods in personality prediction tasks, highlighting its effectiveness in capturing complex behavioral cues and robustness in adapting to new domains."}, "https://arxiv.org/abs/2411.00823": {"title": "Mobility-LLM: Learning Visiting Intentions and Travel Preferences from Human Mobility Data with Large Language Models", "link": "https://arxiv.org/abs/2411.00823", "description": "arXiv:2411.00823v1 Announce Type: cross \nAbstract: Location-based services (LBS) have accumulated extensive human mobility data on diverse behaviors through check-in sequences. These sequences offer valuable insights into users' intentions and preferences. Yet, existing models analyzing check-in sequences fail to consider the semantics contained in these sequences, which closely reflect human visiting intentions and travel preferences, leading to an incomplete comprehension. Drawing inspiration from the exceptional semantic understanding and contextual information processing capabilities of large language models (LLMs) across various domains, we present Mobility-LLM, a novel framework that leverages LLMs to analyze check-in sequences for multiple tasks. Since LLMs cannot directly interpret check-ins, we reprogram these sequences to help LLMs comprehensively understand the semantics of human visiting intentions and travel preferences. Specifically, we introduce a visiting intention memory network (VIMN) to capture the visiting intentions at each record, along with a shared pool of human travel preference prompts (HTPP) to guide the LLM in understanding users' travel preferences. These components enhance the model's ability to extract and leverage semantic information from human mobility data effectively. Extensive experiments on four benchmark datasets and three downstream tasks demonstrate that our approach significantly outperforms existing models, underscoring the effectiveness of Mobility-LLM in advancing our understanding of human mobility data within LBS contexts."}, "https://arxiv.org/abs/2411.00825": {"title": "Transparent Tagging for Strategic Social Nudges on User-Generated Misinformation", "link": "https://arxiv.org/abs/2411.00825", "description": "arXiv:2411.00825v1 Announce Type: cross \nAbstract: Social network platforms (SNP), such as X and TikTok, rely heavily on user-generated content to attract users and advertisers, yet they have limited control over content provision, which leads to the proliferation of misinformation across platforms. As countermeasures, SNPs have implemented various policies, such as tweet labeling, to notify users about potentially misleading information, influencing users' responses, either favorably or unfavorably, to the tagged contents. The population-level response creates a social nudge to the content provider that encourages it to supply more authentic content without exerting direct control over the provider. Yet, when designing such tagging policies to leverage social nudges, SNP must be cautious about the potential misdetection of misinformation (wrongly detecting factual content as misinformation and vice versa), which impairs its credibility to generic users and, hence, its ability to create social nudges. This work establishes a Bayesian persuaded branching process to study SNP's tagging policy design under misdetection. Misinformation circulation is modeled by a multi-type branching process, where users are persuaded through tagging to give positive and negative comments that influence the spread of misinformation. When translated into posterior belief space, the SNP's problem is reduced to an equality-constrained convex optimization, the optimal condition of which is given by the Lagrangian characterization. The key finding is that SNP's optimal policy is simply transparent tagging, i.e., revealing the content's authenticity to the user, albeit midsection, which nudges the provider not to generate misinformation. We corroborate our findings using numerical simulations."}, "https://arxiv.org/abs/2411.01080": {"title": "Identifiability analysis of vaccination decision-making dynamics", "link": "https://arxiv.org/abs/2411.01080", "description": "arXiv:2411.01080v1 Announce Type: cross \nAbstract: Variations in individuals' perceptions of vaccination and decision-making processes can give rise to poor vaccination coverage. The future vaccination promotion programs will benefit from understanding this heterogeneity amongst groups within a population and, accordingly, tailoring the communication strategies. Motivated by this, we developed a mechanistic model consisting of a system of ordinary differential equations that\n  categorizes individuals based on two factors: (i) perceived payoff gains for vaccination and (ii} decision-making strategies where we assumed that individuals may behave as either myopic rationalists, going for a dose of vaccine if doing so maximizes their perceived payoff gain, or success-based learners, waiting to observe feedback on vaccination before deciding.\n  We then investigated the global identifiability of group proportions and perceived payoff gains, that is, the possibility of globally retrieving these parameters by observing the error-free cumulative proportion of vaccinated individuals over time.\n  To do so, for each group, we assumed a piecewise constant payoff gain and, for each time interval, obtained the so-called generalized input-output equation. We then proved the global identifiability of these parameters under certain conditions.\n  Global identifiability opens the door to reliable estimations of the group proportions and their perceived payoffs."}, "https://arxiv.org/abs/2411.01169": {"title": "Bi-Level Graph Structure Learning for Next POI Recommendation", "link": "https://arxiv.org/abs/2411.01169", "description": "arXiv:2411.01169v1 Announce Type: cross \nAbstract: Next point-of-interest (POI) recommendation aims to predict a user's next destination based on sequential check-in history and a set of POI candidates. Graph neural networks (GNNs) have demonstrated a remarkable capability in this endeavor by exploiting the extensive global collaborative signals present among POIs. However, most of the existing graph-based approaches construct graph structures based on pre-defined heuristics, failing to consider inherent hierarchical structures of POI features such as geographical locations and visiting peaks, or suffering from noisy and incomplete structures in graphs. To address the aforementioned issues, this paper presents a novel Bi-level Graph Structure Learning (BiGSL) for next POI recommendation. BiGSL first learns a hierarchical graph structure to capture the fine-to-coarse connectivity between POIs and prototypes, and then uses a pairwise learning module to dynamically infer relationships between POI pairs and prototype pairs. Based on the learned bi-level graphs, our model then employs a multi-relational graph network that considers both POI- and prototype-level neighbors, resulting in improved POI representations. Our bi-level structure learning scheme is more robust to data noise and incompleteness, and improves the exploration ability for recommendation by alleviating sparsity issues. Experimental results on three real-world datasets demonstrate the superiority of our model over existing state-of-the-art methods, with a significant improvement in recommendation accuracy and exploration performance."}, "https://arxiv.org/abs/2411.01329": {"title": "Cloned Identity Detection in Social-Sensor Clouds based on Incomplete Profiles", "link": "https://arxiv.org/abs/2411.01329", "description": "arXiv:2411.01329v1 Announce Type: cross \nAbstract: We propose a novel approach to effectively detect cloned identities of social-sensor cloud service providers (i.e. social media users) in the face of incomplete non-privacy-sensitive profile data. Named ICD-IPD, the proposed approach first extracts account pairs with similar usernames or screen names from a given set of user accounts collected from a social media. It then learns a multi-view representation associated with a given account and extracts two categories of features for every single account. These two categories of features include profile and Weighted Generalised Canonical Correlation Analysis (WGCCA)-based features that may potentially contain missing values. To counter the impact of such missing values, a missing value imputer will next impute the missing values of the aforementioned profile and WGCCA-based features. After that, the proposed approach further extracts two categories of augmented features for each account pair identified previously, namely, 1) similarity and 2) differences-based features. Finally, these features are concatenated and fed into a Light Gradient Boosting Machine classifier to detect identity cloning. We evaluated and compared the proposed approach against the existing state-of-the-art identity cloning approaches and other machine or deep learning models atop a real-world dataset. The experimental results show that the proposed approach outperforms the state-of-the-art approaches and models in terms of Precision, Recall and F1-score."}, "https://arxiv.org/abs/2411.01379": {"title": "Exploring the MBTI distribution among Chinese undergraduate physics students: the influence of family income on career trajectories", "link": "https://arxiv.org/abs/2411.01379", "description": "arXiv:2411.01379v1 Announce Type: cross \nAbstract: This study investigated the distribution of MBTI personality types among physics undergraduates at Zhejiang University and analyzed their career aspirations, family income and mental health status. A comprehensive survey of 68 questions, including 52 multiple-choice questions and 16 short-answer questions, assessed various personality traits and their correlation with academic intent and emotional state. The results showed that INTJ and INTP personality types showed the most significant tendency to pursue academic research. They usually come from middle and above-class backgrounds. Notably, these two personality types accounted for about 41\\% of the total surveyed population, while NT types combined accounted for about 58\\%. This indicates that although NT personality is more suitable for academic research, students with introverted tendencies are more suitable. Research has further shown that people with ISTJ personalities often exhibit unexpectedly strong academic interests. In addition, individuals who aspire to enter academia generally maintain a stable state of mind, as evidenced by their consistent work efficiency in the face of personal challenges. Interestingly, although some aspiring academics come from financially stable families, most have encountered financial constraints. These results contribute to a deeper understanding of how personality traits and socioeconomic factors influence the academic career paths of physics students."}, "https://arxiv.org/abs/2411.01410": {"title": "PageRank Bandits for Link Prediction", "link": "https://arxiv.org/abs/2411.01410", "description": "arXiv:2411.01410v1 Announce Type: cross \nAbstract: Link prediction is a critical problem in graph learning with broad applications such as recommender systems and knowledge graph completion. Numerous research efforts have been directed at solving this problem, including approaches based on similarity metrics and Graph Neural Networks (GNN). However, most existing solutions are still rooted in conventional supervised learning, which makes it challenging to adapt over time to changing customer interests and to address the inherent dilemma of exploitation versus exploration in link prediction. To tackle these challenges, this paper reformulates link prediction as a sequential decision-making process, where each link prediction interaction occurs sequentially. We propose a novel fusion algorithm, PRB (PageRank Bandits), which is the first to combine contextual bandits with PageRank for collaborative exploitation and exploration. We also introduce a new reward formulation and provide a theoretical performance guarantee for PRB. Finally, we extensively evaluate PRB in both online and offline settings, comparing it with bandit-based and graph-based methods. The empirical success of PRB demonstrates the value of the proposed fusion approach. Our code is released at https://github.com/jiaruzouu/PRB."}, "https://arxiv.org/abs/2411.02003": {"title": "Against Multifaceted Graph Heterogeneity via Asymmetric Federated Prompt Learning", "link": "https://arxiv.org/abs/2411.02003", "description": "arXiv:2411.02003v1 Announce Type: cross \nAbstract: Federated Graph Learning (FGL) aims to collaboratively and privately optimize graph models on divergent data for different tasks. A critical challenge in FGL is to enable effective yet efficient federated optimization against multifaceted graph heterogeneity to enhance mutual performance. However, existing FGL works primarily address graph data heterogeneity and perform incapable of graph task heterogeneity. To address the challenge, we propose a Federated Graph Prompt Learning (FedGPL) framework to efficiently enable prompt-based asymmetric graph knowledge transfer between multifaceted heterogeneous federated participants. Generally, we establish a split federated framework to preserve universal and domain-specific graph knowledge, respectively. Moreover, we develop two algorithms to eliminate task and data heterogeneity for advanced federated knowledge preservation. First, a Hierarchical Directed Transfer Aggregator (HiDTA) delivers cross-task beneficial knowledge that is hierarchically distilled according to the directional transferability. Second, a Virtual Prompt Graph (VPG) adaptively generates graph structures to enhance data utility by distinguishing dominant subgraphs and neutralizing redundant ones. We conduct theoretical analyses and extensive experiments to demonstrate the significant accuracy and efficiency effectiveness of FedGPL against multifaceted graph heterogeneity compared to state-of-the-art baselines on large-scale federated graph datasets."}, "https://arxiv.org/abs/2411.02005": {"title": "Towards a valid bibliometric measure of epistemic breadth of researchers", "link": "https://arxiv.org/abs/2411.02005", "description": "arXiv:2411.02005v1 Announce Type: cross \nAbstract: The concept of epistemic breadth of the work of a researcher refers to the scope of their knowledge claims, as reflected in published research reports. Studies of epistemic breadth have been hampered by the lack of a validated measure of the concept. Here we introduce a knowledge space approach to the measurement of epistemic breadth and propose to use the semantic similarity network of an author's publication record to operationalize a measure. In this approach, each paper has its own location in a common abstract vector space based on its content. Proximity in knowledge space corresponds to thematic similarity of publications. Candidate measures of epistemic breadth derived from aggregate similarity values of researchers' bodies of work are tested against external validation data of researchers known to have made a major change in research topic and against self-citation data. We find that some candidate measures co-vary well with known epistemic breadth of researchers in the empirical data and can serve as valid indicators of the concept."}, "https://arxiv.org/abs/2411.02045": {"title": "Conversations with Data: How Data Journalism Affects Online Comments in the New York Times", "link": "https://arxiv.org/abs/2411.02045", "description": "arXiv:2411.02045v1 Announce Type: cross \nAbstract: Users in the data age have access to more data than ever before, but little is known how they interact with it. Using transparency and multimedia, data journalism (DJ) lets users explore and interpret data on their own. This study examines how DJ affects online comments as a case study of user interactions with data. The corpus comprises 6,400 stories and their comment sections from the DJ and other sections of the New York Times, from 2014-2022. Results indicate that DJ is positively associated with higher level of interactivity between the users. This relationship is mediated by statistical information, information sources, and static visualizations. However, there is a low level of interactivity with the content; consequently, only part of the users use it. The results demonstrate how data accessibility through DJ engages the users in conversation. According to deliberation theory, this creates a conducive environment for democratic processes."}, "https://arxiv.org/abs/2411.02058": {"title": "Intrinsic Dimensionality of Fermi-Pasta-Ulam-Tsingou High-Dimensional Trajectories Through Manifold Learning", "link": "https://arxiv.org/abs/2411.02058", "description": "arXiv:2411.02058v1 Announce Type: cross \nAbstract: A data-driven approach based on unsupervised machine learning is proposed to infer the intrinsic dimensions $m^{\\ast}$ of the high-dimensional trajectories of the Fermi-Pasta-Ulam-Tsingou (FPUT) model. Principal component analysis (PCA) is applied to trajectory data consisting of $n_s = 4,000,000$ datapoints, of the FPUT $\\beta$ model with $N = 32$ coupled oscillators, revealing a critical relationship between $m^{\\ast}$ and the model's nonlinear strength. For weak nonlinearities, $m^{\\ast} \\ll n$, where $n = 2N$. In contrast, for strong nonlinearities, $m^{\\ast} \\rightarrow n - 1$, consistently with the ergodic hypothesis. Furthermore, one of the potential limitations of PCA is addressed through an analysis with t-distributed stochastic neighbor embedding ($t$-SNE). Accordingly, we found strong evidence suggesting that the datapoints lie near or on a curved low-dimensional manifold for weak nonlinearities."}, "https://arxiv.org/abs/2411.02183": {"title": "Vehicles, Pedestrians, and E-bikes: a Three-party Game at Right-turn-on-red Crossroads Revealing the Dual and Irrational Role of E-bikes that Risks Traffic Safety", "link": "https://arxiv.org/abs/2411.02183", "description": "arXiv:2411.02183v1 Announce Type: cross \nAbstract: The widespread use of e-bikes has facilitated short-distance travel yet led to confusion and safety problems in road traffic. This study focuses on the dual characteristics of e-bikes in traffic conflicts: they resemble pedestrians when interacting with motor vehicles and behave like motor vehicles when in conflict with pedestrians, which raises the right of way concerns when potential conflicts are at stake. Using the Quantal Response Equilibrium model, this research analyzes the behavioral choice differences of three groups of road users (vehicle-pedestrian, vehicle-e-bike, e-bike-pedestrian) at right-turn-on-red crossroads in right-turning lines and straight-going lines conflict scenarios. The results show that the behavior of e-bikes is more similar to that of motor vehicles than pedestrians overall, and their interactions with either pedestrians or motor vehicles do not establish a reasonable order, increasing the likelihood of confusion and conflict. In contrast, a mutual understanding has developed between motor vehicles and pedestrians, where motor vehicles tend to yield, and pedestrians tend to cross. By clarifying the game theoretical model and introducing the rationality parameter, this study precisely locates the role of e-bikes among road users, which provides a reliable theoretical basis for optimizing traffic regulations."}, "https://arxiv.org/abs/2306.13400": {"title": "Network community detection via neural embeddings", "link": "https://arxiv.org/abs/2306.13400", "description": "arXiv:2306.13400v2 Announce Type: replace \nAbstract: Recent advances in machine learning research have produced powerful neural graph embedding methods, which learn useful, low-dimensional vector representations of network data. These neural methods for graph embedding excel in graph machine learning tasks and are now widely adopted. However, how and why these methods work -- particularly how network structure gets encoded in the embedding -- remain largely unexplained. Here, we show that node2vec -- shallow, linear neural network -- encodes communities into separable clusters better than random partitioning down to the information-theoretic detectability limit for the stochastic block models. We show that this is due to the equivalence between the embedding learned by node2vec and the spectral embedding via the eigenvectors of the symmetric normalized Laplacian matrix. Numerical simulations demonstrate that node2vec is capable of learning communities on sparse graphs generated by the stochastic blockmodel, as well as on sparse degree-heterogeneous networks. Our results highlight the features of graph neural networks that enable them to separate communities in embedding space."}, "https://arxiv.org/abs/2310.02095": {"title": "A Survey on the Role of Crowds in Combating Online Misinformation: Annotators, Evaluators, and Creators", "link": "https://arxiv.org/abs/2310.02095", "description": "arXiv:2310.02095v2 Announce Type: replace \nAbstract: Online misinformation poses a global risk with significant real-world consequences. To combat misinformation, current research relies on professionals like journalists and fact-checkers for annotating and debunking misinformation, and develops automated machine learning methods for detecting misinformation. Complementary to these approaches, recent research has increasingly concentrated on utilizing the power of ordinary social media users, a.k.a. \"crowd\", who act as eyes-on-the-ground proactively questioning and countering misinformation. Notably, recent studies show that 96% of counter-misinformation responses originate from them. Acknowledging their prominent role, we present the first systematic and comprehensive survey of research papers that actively leverage the crowds to combat misinformation.\n  We first identify 88 papers related to crowd-based efforts, following a meticulous annotation process adhering to the PRISMA framework. We then present key statistics related to misinformation, counter-misinformation, and crowd input in different formats and topics. Upon holistic analysis of the papers, we introduce a novel taxonomy of the roles played by the crowds: (i)annotators who actively identify misinformation; (ii)evaluators who assess counter-misinformation effectiveness; (iii)creators who create counter-misinformation. This taxonomy explores the crowd's capabilities in misinformation detection, identifies prerequisites for effective counter-misinformation, and analyzes crowd-generated counter-misinformation. Then, we delve into (i)distinguishing individual, collaborative, and machine-assisted labeling for annotators; (ii)analyzing the effectiveness of counter-misinformation through surveys, interviews, and in-lab experiments for evaluators; and (iii)characterizing creation patterns and creator profiles for creators. Finally, we outline potential future research in this field."}, "https://arxiv.org/abs/1706.09072": {"title": "Decomposing Network Influence: Social Influence Regression", "link": "https://arxiv.org/abs/1706.09072", "description": "arXiv:1706.09072v2 Announce Type: replace-cross \nAbstract: Understanding network influence and its determinants are key challenges in political science and network analysis. Traditional latent variable models position actors within a social space based on network dependencies but often do not elucidate the underlying factors driving these interactions. To overcome this limitation, we propose the Social Influence Regression (SIR) model, an extension of vector autoregression tailored for relational data that incorporates exogenous covariates into the estimation of influence patterns. The SIR model captures influence dynamics via a pair of $n \\times n$ matrices that quantify how the actions of one actor affect the future actions of another. This framework not only provides a statistical mechanism for explaining actor influence based on observable traits but also improves computational efficiency through an iterative block coordinate descent method. We showcase the SIR model's capabilities by applying it to monthly conflict events between countries, using data from the Integrated Crisis Early Warning System (ICEWS). Our findings demonstrate the SIR model's ability to elucidate complex influence patterns within networks by linking them to specific covariates. This paper's main contributions are: (1) introducing a model that explains third-order dependencies through exogenous covariates and (2) offering an efficient estimation approach that scales effectively with large, complex networks."}, "https://arxiv.org/abs/2411.02403": {"title": "A Persuasion-Based Prompt Learning Approach to Improve Smishing Detection through Data Augmentation", "link": "https://arxiv.org/abs/2411.02403", "description": "arXiv:2411.02403v1 Announce Type: new \nAbstract: Smishing, which aims to illicitly obtain personal information from unsuspecting victims, holds significance due to its negative impacts on our society. In prior studies, as a tool to counteract smishing, machine learning (ML) has been widely adopted, which filters and blocks smishing messages before they reach potential victims. However, a number of challenges remain in ML-based smishing detection, with the scarcity of annotated datasets being one major hurdle. Specifically, given the sensitive nature of smishing-related data, there is a lack of publicly accessible data that can be used for training and evaluating ML models. Additionally, the nuanced similarities between smishing messages and other types of social engineering attacks such as spam messages exacerbate the challenge of smishing classification with limited resources. To tackle this challenge, we introduce a novel data augmentation method utilizing a few-shot prompt learning approach. What sets our approach apart from extant methods is the use of the principles of persuasion, a psychology theory which explains the underlying mechanisms of smishing. By designing prompts grounded in the persuasion principles, our augmented dataset could effectively capture various, important aspects of smishing messages, enabling ML models to be effectively trained. Our evaluation within a real-world context demonstrates that our augmentation approach produces more diverse and higher-quality smishing data instances compared to other cutting-edging approaches, leading to substantial improvements in the ability of ML models to detect the subtle characteristics of smishing messages. Moreover, our additional analyses reveal that the performance improvement provided by our approach is more pronounced when used with ML models that have a larger number of parameters, demonstrating its effectiveness in training large-scale ML models."}, "https://arxiv.org/abs/2411.02434": {"title": "Analysis of the inference of ratings and rankings on Higher Order Networks with complex topologies", "link": "https://arxiv.org/abs/2411.02434", "description": "arXiv:2411.02434v1 Announce Type: new \nAbstract: The inference of rankings plays a central role in the theory of social choice, which seeks to establish preferences from collectively generated data, such as pairwise comparisons. Examples include political elections, ranking athletes based on competition results, ordering web pages in search engines using hyperlink networks, and generating recommendations in online stores based on user behavior. Various methods have been developed to infer rankings from incomplete or conflicting data. One such method, HodgeRank, introduced by Jiang et al.~\\cite{jiang2011statistical}, utilizes Hodge decomposition of cochains in Higher Order Networks to disentangle gradient and cyclical components contributing to rating scores, enabling a parsimonious inference of ratings and rankings for lists of items. This paper presents a systematic study of HodgeRank's performance under the influence of quenched disorder and across networks with complex topologies generated by four different network models. The results reveal a transition from a regime of perfect trieval of true rankings to one of imperfect trieval as the strength of the quenched disorder increases. A range of observables are analyzed, and their scaling behavior with respect to the network model parameters is characterized. This work advances the understanding of social choice theory and the inference of ratings and rankings within complex network structures."}, "https://arxiv.org/abs/2411.02866": {"title": "Double Whammy: Stealthy Data Manipulation aided Reconstruction Attack on Graph Federated Learning", "link": "https://arxiv.org/abs/2411.02866", "description": "arXiv:2411.02866v1 Announce Type: new \nAbstract: Recent research has constructed successful graph reconstruction attack (GRA) on GFL. But these attacks are still challenged in aspects of effectiveness and stealth. To address the issues, we propose the first Data Manipulation aided Reconstruction attack on GFL, dubbed as DMan4Rec. The malicious client is born to manipulate its locally collected data to enhance graph stealing privacy from benign ones, so as to construct double whammy on GFL. It differs from previous work in three terms: (1) effectiveness - to fully utilize the sparsity and feature smoothness of the graph, novel penalty terms are designed adaptive to diverse similarity functions for connected and unconnected node pairs, as well as incorporation label smoothing on top of the original cross-entropy loss. (2) scalability - DMan4Rec is capable of both white-box and black-box attacks via training a supervised model to infer the posterior probabilities obtained from limited queries (3) stealthiness - by manipulating the malicious client's node features, it can maintain the overall graph structure's invariance and conceal the attack. Comprehensive experiments on four real datasets and three GNN models demonstrate that DMan4Rec achieves the state-of-the-art (SOTA) attack performance, e.g., the attack AUC and precision improved by 9.2% and 10.5% respectively compared with the SOTA baselines. Particularly, DMan4Rec achieves an AUC score and a precision score of up to 99.59% and 99.56%, respectively in black-box setting. Nevertheless, the complete overlap of the distribution graphs supports the stealthiness of the attack. Besides, DMan4Rec still beats the defensive GFL, which alarms a new threat to GFL."}, "https://arxiv.org/abs/2411.01844": {"title": "DeMod: A Holistic Tool with Explainable Detection and Personalized Modification for Toxicity Censorship", "link": "https://arxiv.org/abs/2411.01844", "description": "arXiv:2411.01844v1 Announce Type: cross \nAbstract: Although there have been automated approaches and tools supporting toxicity censorship for social posts, most of them focus on detection. Toxicity censorship is a complex process, wherein detection is just an initial task and a user can have further needs such as rationale understanding and content modification. For this problem, we conduct a needfinding study to investigate people's diverse needs in toxicity censorship and then build a ChatGPT-based censorship tool named DeMod accordingly. DeMod is equipped with the features of explainable Detection and personalized Modification, providing fine-grained detection results, detailed explanations, and personalized modification suggestions. We also implemented the tool and recruited 35 Weibo users for evaluation. The results suggest DeMod's multiple strengths like the richness of functionality, the accuracy of censorship, and ease of use. Based on the findings, we further propose several insights into the design of content censorship systems."}, "https://arxiv.org/abs/2411.02097": {"title": "Eco-evolutionary constraints for the endemicity of rapidly evolving viruses", "link": "https://arxiv.org/abs/2411.02097", "description": "arXiv:2411.02097v1 Announce Type: cross \nAbstract: Antigenic escape constitutes the main mechanism allowing rapidly evolving viruses to achieve endemicity. Beyond granting immune escape, empirical evidence also suggests that mutations of viruses might increase their inter-host transmissibility. While both mechanisms are well-studied individually, their combined effects on viral endemicity remain to be explored. Here we propose a minimal eco-evolutionary framework to simulate epidemic outbreaks generated by pathogens evolving both their transmissibility and immune escape. Our findings uncover a very rich phenomenology arising from the complex interplay between both evolutionary pathways and the underlying contagion dynamics. We first show that contagions at the population level constrain the effective evolution of the virus, accelerating the increase in transmissibility in the first epidemic wave while favoring antigenic variation in the transition to the endemic phase. Our results also reveal that accounting for both evolutionary pathways changes the features of the viruses more prone to become endemic. While chances for endemicity increase with infectiousness of the wild-type variant for viruses not evolving their transmissibility, a non-monotonic behavior is observed when the latter mechanism is included, favoring less transmissible viruses and impairing those ones with intermediate infectiousness."}, "https://arxiv.org/abs/2411.02401": {"title": "A Civilian Astronomer's Guide to UAP Research", "link": "https://arxiv.org/abs/2411.02401", "description": "arXiv:2411.02401v1 Announce Type: cross \nAbstract: Unidentified Anomalous Phenomena (UAP) have historically been stigmatized and regarded as pseudoscience due to a general lack of robust evidence. Recently, however, the subject has gained interest among astronomers and the military. This review explores how astronomers can enhance our understanding of these enigmatic phenomena by focusing on empirical tests of specific hypotheses (e.g. the hypothesis of extraterrestrial visitations) rather than solely collecting and categorizing data. We compare the investigation of UAP to the process of calibration and interpretations of astronomical discoveries and propose a toy model involving a network of neuro-interface extraterrestrial probes to model exotic UAP. This model aids in predicting probe signatures and behaviour, improving detection methods, and addressing ethical concerns in UAP research."}, "https://arxiv.org/abs/2411.02405": {"title": "Accuracy nudges are not effective against non-harmful deepfakes", "link": "https://arxiv.org/abs/2411.02405", "description": "arXiv:2411.02405v1 Announce Type: cross \nAbstract: I conducted a preregistered survey experiment (n=525) to assess the effectiveness of \"accuracy nudges\" against deepfakes (osf.io/69x17). The results, based on a sample of Colombian participants, replicated previous findings showing that prompting participants to assess the accuracy of a headline at the beginning of the survey significantly decreased their intention to share fake news. However, this effect was not significant when applied to a non-harmful AI-generated video."}, "https://arxiv.org/abs/2411.02542": {"title": "Enhancing Graph Neural Networks in Large-scale Traffic Incident Analysis with Concurrency Hypothesis", "link": "https://arxiv.org/abs/2411.02542", "description": "arXiv:2411.02542v1 Announce Type: cross \nAbstract: Despite recent progress in reducing road fatalities, the persistently high rate of traffic-related deaths highlights the necessity for improved safety interventions. Leveraging large-scale graph-based nationwide road network data across 49 states in the USA, our study first posits the Concurrency Hypothesis from intuitive observations, suggesting a significant likelihood of incidents occurring at neighboring nodes within the road network. To quantify this phenomenon, we introduce two novel metrics, Average Neighbor Crash Density (ANCD) and Average Neighbor Crash Continuity (ANCC), and subsequently employ them in statistical tests to validate the hypothesis rigorously. Building upon this foundation, we propose the Concurrency Prior (CP) method, a powerful approach designed to enhance the predictive capabilities of general Graph Neural Network (GNN) models in semi-supervised traffic incident prediction tasks. Our method allows GNNs to incorporate concurrent incident information, as mentioned in the hypothesis, via tokenization with negligible extra parameters.\n  The extensive experiments, utilizing real-world data across states and cities in the USA, demonstrate that integrating CP into 12 state-of-the-art GNN architectures leads to significant improvements, with gains ranging from 3% to 13% in F1 score and 1.3% to 9% in AUC metrics. The code is publicly available at https://github.com/xiwenc1/Incident-GNN-CP."}, "https://arxiv.org/abs/2411.02660": {"title": "Target search on networks-within-networks with applications to protein-DNA interactions", "link": "https://arxiv.org/abs/2411.02660", "description": "arXiv:2411.02660v1 Announce Type: cross \nAbstract: We present a novel framework for understanding node target search in systems organized as hierarchical networks-within-networks. Our work generalizes traditional search models on complex networks, where the mean-first passage time is typically inversely proportional to the node degree. However, real-world search processes often span multiple network layers, such as moving from an external environment into a local network, and then navigating several internal states. This multilayered complexity appears in scenarios such as international travel networks, tracking email spammers, and the dynamics of protein-DNA interactions in cells. Our theory addresses these complex systems by modeling them as a three-layer multiplex network: an external source layer, an intermediate spatial layer, and an internal state layer. We derive general closed-form solutions for the steady-state flux through a target node, which serves as a proxy for inverse mean-first passage time. Our results reveal a universal relationship between search efficiency and network-specific parameters. This work extends the current understanding of multiplex networks by focusing on systems with hierarchically connected layers. Our findings have broad implications for fields ranging from epidemiology to cellular biology and provide a more comprehensive understanding of search dynamics in complex, multilayered environments."}, "https://arxiv.org/abs/2411.02739": {"title": "Ensemble inequivalence and phase transitions in unlabeled networks", "link": "https://arxiv.org/abs/2411.02739", "description": "arXiv:2411.02739v1 Announce Type: cross \nAbstract: We discover a first-order phase transition in the canonical ensemble of random unlabeled networks with a prescribed average number of links. The transition is caused by the nonconcavity of microcanonical entropy. Above the critical point, the canonical and microcanonical ensembles are equivalent and have a well-behaved thermodynamic limit. Below the critical point, the ensemble equivalence is broken, and the canonical ensemble is a mixture of phases: empty networks and networks with logarithmic average degrees. As a consequence, networks with bounded average degrees do not survive in the thermodynamic limit, decaying into the empty phase. The celebrated percolation transition in labeled networks is thus absent in unlabeled networks. In view of these differences between labeled and unlabeled ensembles, the question of which one should be used as a null model of different real-world networks cannot be ignored."}, "https://arxiv.org/abs/2411.02879": {"title": "A new family of ladder operators for macroscopic systems, with applications", "link": "https://arxiv.org/abs/2411.02879", "description": "arXiv:2411.02879v1 Announce Type: cross \nAbstract: In a series of recent scientific contributions the role of bosonic and fermionic ladder operators in a macroscopic realm has been investigated. Creation, annihilation and number operators have been used in very different contexts, all sharing the same common main feature, i.e. the relevance of {\\em discrete changes} in the description of the system. The main problem when using this approach is that computations are easy for Hamiltonians which are quadratic in the ladder operators, but become very complicated, both at the analytical and at the numerical level, when the Hamiltonian is not quadratic. In this paper we propose a possible alternative approach, again based on some sort of ladder operators, but for which an analytic solution can often be deduced without particular difficulties. We describe our proposal with few applications, mostly related to different versions of a predator-prey model, and to love affairs (from a decision-making point of view)."}, "https://arxiv.org/abs/2411.02919": {"title": "New type of chaotic solutions found in Gravity model of network transport", "link": "https://arxiv.org/abs/2411.02919", "description": "arXiv:2411.02919v1 Announce Type: cross \nAbstract: The gravity model is a mathematical model that applies Newton's universal law of gravitation to socio-economic transport phenomena and has been widely used to describe world trade, intercity traffic flows, and business transactions for more than several decades. However, its strong nonlinearity and diverse network topology make a theoretical analysis difficult, and only a short history of studies on its stability exist. In this study, the stability of gravity models defined on networks with few nodes is analyzed in detail using numerical simulations. It was found that, other than the previously known transition of stationary solutions from a unique diffusion solution to multiple localized solutions, parameter regions exist where periodic solutions with the same repeated motions and chaotic solutions with no periods are realized. The smallest network with chaotic solutions was found to be a ring with seven nodes, which produced a new type of chaotic solution in the form of a mixture of right and left periodic solutions."}, "https://arxiv.org/abs/2411.03331": {"title": "Hypergraphs as Weighted Directed Self-Looped Graphs: Spectral Properties, Clustering, Cheeger Inequality", "link": "https://arxiv.org/abs/2411.03331", "description": "arXiv:2411.03331v1 Announce Type: new \nAbstract: Hypergraphs naturally arise when studying group relations and have been widely used in the field of machine learning. There has not been a unified formulation of hypergraphs, yet the recently proposed edge-dependent vertex weights (EDVW) modeling is one of the most generalized modeling methods of hypergraphs, i.e., most existing hypergraphs can be formulated as EDVW hypergraphs without any information loss to the best of our knowledge. However, the relevant algorithmic developments on EDVW hypergraphs remain nascent: compared to spectral graph theories, the formulations are incomplete, the spectral clustering algorithms are not well-developed, and one result regarding hypergraph Cheeger Inequality is even incorrect. To this end, deriving a unified random walk-based formulation, we propose our definitions of hypergraph Rayleigh Quotient, NCut, boundary/cut, volume, and conductance, which are consistent with the corresponding definitions on graphs. Then, we prove that the normalized hypergraph Laplacian is associated with the NCut value, which inspires our HyperClus-G algorithm for spectral clustering on EDVW hypergraphs. Finally, we prove that HyperClus-G can always find an approximately linearly optimal partitioning in terms of Both NCut and conductance. Additionally, we provide extensive experiments to validate our theoretical findings from an empirical perspective."}, "https://arxiv.org/abs/2411.03333": {"title": "Analysis of Bipartite Networks in Anime Series: Textual Analysis, Topic Clustering, and Modeling", "link": "https://arxiv.org/abs/2411.03333", "description": "arXiv:2411.03333v1 Announce Type: new \nAbstract: This article analyzes a specific bipartite network that shows the relationships between users and anime, examining how the descriptions of anime influence the formation of user communities. In particular, we introduce a new variable that quantifies the frequency with which words from a description appear in specific word clusters. These clusters are generated from a bigram analysis derived from all descriptions in the database. This approach fully characterizes the dynamics of these communities and shows how textual content affect the cohesion and structure of the social network among anime enthusiasts. Our findings suggest that there may be significant implications for the design of recommendation systems and the enhancement of user experience on anime platforms."}, "https://arxiv.org/abs/2411.03335": {"title": "Asymmetric Weighted Cascade Model for Competitive Influence Maximization", "link": "https://arxiv.org/abs/2411.03335", "description": "arXiv:2411.03335v1 Announce Type: new \nAbstract: We introduce a modified Weighted Cascade model that integrates asymmetric budgets and product scores, providing new insights into the Generalized Asymmetric Influence Maximization problem, which we establish as NP-hard. Our simulations demonstrate that players with higher budgets possess a distinct advantage in networks characterized by larger diameters, whereas players with superior product scores exhibit a significant advantage in networks with smaller diameters. Moreover, we identify a robust linear relationship between graph size and the magnitude of influenced nodes. In densely connected networks we derive bounds for the probabilities of influence that are independent of network size. Our examination of Nash equilibria in this domain underscores the absence of a guaranteed pure Nash equilibrium, suggesting that the strategic enhancement of budgets or product scores may yield more substantial benefits than the pursuit of an optimal strategy in this context."}, "https://arxiv.org/abs/2411.03394": {"title": "Physical modelling of global macrosystems evolution", "link": "https://arxiv.org/abs/2411.03394", "description": "arXiv:2411.03394v1 Announce Type: new \nAbstract: The dynamics of growth and innovation often exhibit sudden, explosive surges, where systems remain quasi stable for extended periods before accelerating dramatically-often surpassing traditional exponential growth. This pattern is evident across various domains, including world population increases and rapid technological advancements. Although these phenomena share common characteristics, they are driven by different underlying mechanisms. In this paper, we introduce a unified framework to capture these phenomenologies through a theory of combinatorial innovation. Inspired by the Theory of the Adjacent Possible, we model growth and innovation as emerging from the recombination processes of existing elements of a system. By formalizing these qualitative ideas, we provide a mathematical structure that explains diverse phenomena, enables cross-system comparisons, and offers grounded predictions for future growth trajectories. Our approach distils the complexity of innovation into a more accessible yet robust framework, paving the way for a deeper and more flexible mathematical understanding of growth and innovation processes."}, "https://arxiv.org/abs/2411.03605": {"title": "Automation Will Set Occupational Mobility Free: Structural Changes in the Occupation Network", "link": "https://arxiv.org/abs/2411.03605", "description": "arXiv:2411.03605v1 Announce Type: new \nAbstract: Occupational mobility is an emergent strategy to cope with technological unemployment by facilitating efficient labor redeployment. However, previous studies analyzing networks show that the boundaries to smooth mobility are constrained by a fragmented structure in the occupation network. In this study, positing that this structure will significantly change due to automation, we propose the skill automation view, which asserts that automation substitutes for skills, not for occupations, and simulate a scenario of skill automation drawing on percolation theory. We sequentially remove skills from the occupation-skill bipartite network and investigate the structural changes in the projected occupation network. The results show that the accumulation of small changes (the emergence of bridges between occupations due to skill automation) triggers significant structural changes in the occupation network. The structural changes accelerate as the components integrate into a new giant component. This result suggests that automation mitigates the bottlenecks to smooth occupational mobility."}, "https://arxiv.org/abs/2411.03859": {"title": "UniTraj: Universal Human Trajectory Modeling from Billion-Scale Worldwide Traces", "link": "https://arxiv.org/abs/2411.03859", "description": "arXiv:2411.03859v1 Announce Type: cross \nAbstract: Human trajectory modeling is essential for deciphering movement patterns and supporting advanced applications across various domains. However, existing methods are often tailored to specific tasks and regions, resulting in limitations related to task specificity, regional dependency, and data quality sensitivity. Addressing these challenges requires a universal human trajectory foundation model capable of generalizing and scaling across diverse tasks and geographic contexts. To this end, we propose UniTraj, a Universal human Trajectory foundation model that is task-adaptive, region-independent, and highly generalizable. To further enhance performance, we construct WorldTrace, the first large-scale, high-quality, globally distributed dataset sourced from open web platforms, encompassing 2.45 million trajectories with billions of points across 70 countries. Through multiple resampling and masking strategies designed for pre-training, UniTraj effectively overcomes geographic and task constraints, adapting to heterogeneous data quality. Extensive experiments across multiple trajectory analysis tasks and real-world datasets demonstrate that UniTraj consistently outperforms existing approaches in terms of scalability and adaptability. These results underscore the potential of UniTraj as a versatile, robust solution for a wide range of trajectory analysis applications, with WorldTrace serving as an ideal but non-exclusive foundation for training."}, "https://arxiv.org/abs/2411.03865": {"title": "AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making", "link": "https://arxiv.org/abs/2411.03865", "description": "arXiv:2411.03865v1 Announce Type: cross \nAbstract: Traditional interactive environments limit agents' intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety."}, "https://arxiv.org/abs/2411.03938": {"title": "Where postdoctoral journeys lead", "link": "https://arxiv.org/abs/2411.03938", "description": "arXiv:2411.03938v1 Announce Type: cross \nAbstract: Postdoctoral training is a career stage often described as a demanding and anxiety-laden time when many promising PhDs see their academic dreams slip away due to circumstances beyond their control. We use a unique data set of academic publishing and careers to chart the more or less successful postdoctoral paths. We build a measure of academic success on the citation patterns two to five years into a faculty career. Then, we monitor how students' postdoc positions -- in terms of relocation, change of topic, and early well-cited papers -- relate to their early-career success. One key finding is that the postdoc period seems more important than the doctoral training to achieve this form of success. This is especially interesting in light of the many studies of academic faculty hiring that link Ph.D. granting institutions and hires, omitting the postdoc stage. Another group of findings can be summarized as a Goldilocks principle: it seems beneficial to change one's direction, but not too much."}, "https://arxiv.org/abs/2211.13601": {"title": "Models of Opinion Dynamics with Random Parametrisation", "link": "https://arxiv.org/abs/2211.13601", "description": "arXiv:2211.13601v2 Announce Type: replace \nAbstract: We analyse a generalisation of the Galam model of binary opinion dynamics in which iterative discussions take place in local groups of individuals and study the effects of random deviations from the group majority. The probability of a deviation or flip depends on the magnitude of the majority. Depending on the values of the flip parameters which give the probability of a deviation, the model shows a wide variety of behaviour. We are interested in the characteristics of the model when the flip parameters are themselves randomly selected, following some probability distribution. Examples of these characteristics are whether large majorities and ties are attractors or repulsors, or the number of fixed points in the dynamics of the model. Which of the features of the model are likely to appear? Which ones are unlikely because they only present as events of low probability with respect to the distribution of the flip parameters? Answers to such questions allow us to distinguish mathematical properties which are stable under a variety of assumptions on the distribution of the flip parameters from features which are very rare and thus more of theoretical than practical interest. In this article, we present both exact numerical results for specific distributions of the flip parameters and small discussion groups and rigorous results in the form of limit theorems for large discussion groups. Small discussion groups model friend or work groups -- people that personally know each other and frequently spend time together. Large groups represent scenarios such as social media or political entities such as cities, states, or countries."}, "https://arxiv.org/abs/2404.11937": {"title": "Comprehensive Review and New Analysis Software for Single-file Pedestrian Experiments", "link": "https://arxiv.org/abs/2404.11937", "description": "arXiv:2404.11937v2 Announce Type: replace \nAbstract: This paper offers a comprehensive examination of single-file experiments within the field of pedestrian dynamics, providing a review from both theoretical and analytical perspectives. It begins by tracing the historical context of single-file movement studies in pedestrian dynamics. The significance of understanding the fundamental relationships between density, speed, and flow in pedestrian dynamics is explored through the lens of simple single-file systems. Furthermore, we examine various traffic systems involving human or non-human entities such as ants, mice, bicycles, and cars, and provide insights. We explore the types of experimental setups, data collection methods, and factors that influence pedestrian movement. We also define and explain the common concepts related to single-file movement, particularly in experimental research. Finally, we present a Python tool named \"SingleFileMovementAnalysis\" designed for analyzing single-file experimental data, specifically head trajectories. This tool provides a unified approach for computing movement metrics like speed, density, and headway. The article aims to stimulate further research and underscore the areas where future researchers can contribute to the advancement and improvement of single-file studies."}, "https://arxiv.org/abs/2204.13360": {"title": "Probabilistic Voting Models with Varying Speeds of Correlation Decay", "link": "https://arxiv.org/abs/2204.13360", "description": "arXiv:2204.13360v2 Announce Type: replace-cross \nAbstract: We model voting behaviour in the multi-group setting of a two-tier voting system using sequences of de Finetti measures. Our model is defined by using the de Finetti representation of a probability measure (i.e. as a mixture of conditionally independent probability measures) describing voting behaviour. The de Finetti measure describes the interaction between voters and possible outside influences on them. We assume that for each population size there is a (potentially) different de Finetti measure, and as the population grows, the sequence of de Finetti measures converges weakly to the Dirac measure at the origin, representing a tendency toward weakening social cohesion as the population grows large. The resulting model covers a wide variety of behaviour, ranging from independent voting in the limit under fast convergence, a critical convergence speed with its own pattern of behaviour, to a subcritical convergence speed which yields a model in line with empirical evidence of real-world voting data, contrary to previous probabilistic models used in the study of voting. These models can be used, e.g., to study the problem of optimal voting weights in two-tier voting systems."}, "https://arxiv.org/abs/2208.04848": {"title": "The low-rank hypothesis of complex systems", "link": "https://arxiv.org/abs/2208.04848", "description": "arXiv:2208.04848v4 Announce Type: replace-cross \nAbstract: Complex systems are high-dimensional nonlinear dynamical systems with intricate interactions among their constituents. To make interpretable predictions about their large-scale behavior, it is typically assumed, without a clear statement, that these dynamics can be reduced to a few number of equations involving a low-rank matrix describing the network of interactions -- what we call the low-rank hypothesis. Our paper sheds light on this assumption and questions its validity. By leveraging fundamental theorems on singular value decomposition, we expose the hypothesis for various random graphs, either by making explicit their low-rank formulation or by demonstrating the exponential decrease of their singular values. Notably, we verify the hypothesis experimentally for real networks by revealing the rapid decrease of their singular values, which has major consequences on their effective ranks. We then evaluate the impact of the low-rank hypothesis for general dynamical systems on networks through an optimal dimension reduction. This allows us to prove that recurrent neural networks can be exactly reduced, and to connect the rapidly decreasing singular values of real networks to the dimension reduction error of the nonlinear dynamics they support, be it microbial, neuronal or epidemiological. Finally, we prove that higher-order interactions naturally emerge from the dimension reduction, thus providing theoretical insights into the origin of higher-order interactions in complex systems."}, "https://arxiv.org/abs/2312.02401": {"title": "Enhancing Content Moderation with Culturally-Aware Models", "link": "https://arxiv.org/abs/2312.02401", "description": "arXiv:2312.02401v2 Announce Type: replace-cross \nAbstract: Content moderation on a global scale must navigate a complex array of local cultural distinctions, which can hinder effective enforcement. While global policies aim for consistency and broad applicability, they often miss the subtleties of regional language interpretation, cultural beliefs, and local legislation. This work introduces a flexible framework that enhances foundation language models with cultural knowledge. Our approach involves fine-tuning encoder-decoder models on media-diet data to capture cultural nuances, and applies a continued training regime to effectively integrate these models into a content moderation pipeline. We evaluate this framework in a case study of an online podcast platform with content spanning various regions. The results show that our culturally adapted models improve the accuracy of local violation detection and offer explanations that align more closely with regional cultural norms. Our findings reinforce the need for an adaptable content moderation approach that remains flexible in response to the diverse cultural landscapes it operates in and represents a step towards a more equitable and culturally sensitive framework for content moderation, demonstrating what is achievable in this domain."}}