{"http://arxiv.org/abs/2310.03435": {"title": "Variational Inference for GARCH-family Models", "link": "http://arxiv.org/abs/2310.03435", "description": "The Bayesian estimation of GARCH-family models has been typically addressed\nthrough Monte Carlo sampling. Variational Inference is gaining popularity and\nattention as a robust approach for Bayesian inference in complex machine\nlearning models; however, its adoption in econometrics and finance is limited.\nThis paper discusses the extent to which Variational Inference constitutes a\nreliable and feasible alternative to Monte Carlo sampling for Bayesian\ninference in GARCH-like models. Through a large-scale experiment involving the\nconstituents of the S&amp;P 500 index, several Variational Inference optimizers, a\nvariety of volatility models, and a case study, we show that Variational\nInference is an attractive, remarkably well-calibrated, and competitive method\nfor Bayesian learning."}, "http://arxiv.org/abs/2310.03521": {"title": "Cutting Feedback in Misspecified Copula Models", "link": "http://arxiv.org/abs/2310.03521", "description": "In copula models the marginal distributions and copula function are specified\nseparately. We treat these as two modules in a modular Bayesian inference\nframework, and propose conducting modified Bayesian inference by ``cutting\nfeedback''. Cutting feedback limits the influence of potentially misspecified\nmodules in posterior inference. We consider two types of cuts. The first limits\nthe influence of a misspecified copula on inference for the marginals, which is\na Bayesian analogue of the popular Inference for Margins (IFM) estimator. The\nsecond limits the influence of misspecified marginals on inference for the\ncopula parameters by using a rank likelihood to define the cut model. We\nestablish that if only one of the modules is misspecified, then the appropriate\ncut posterior gives accurate uncertainty quantification asymptotically for the\nparameters in the other module. Computation of the cut posteriors is difficult,\nand new variational inference methods to do so are proposed. The efficacy of\nthe new methodology is demonstrated using both simulated data and a substantive\nmultivariate time series application from macroeconomic forecasting. In the\nlatter, cutting feedback from misspecified marginals to a 1096 dimension copula\nimproves posterior inference and predictive accuracy greatly, compared to\nconventional Bayesian inference."}, "http://arxiv.org/abs/2205.04345": {"title": "Joint diagnostic test of regression discontinuity designs: multiple testing problem", "link": "http://arxiv.org/abs/2205.04345", "description": "Current diagnostic tests for regression discontinuity (RD) design face a\nmultiple testing problem. We find a massive over-rejection of the identifying\nrestriction among empirical RD studies published in top-five economics\njournals. Each test achieves a nominal size of 5%; however, the median number\nof tests per study is 12. Consequently, more than one-third of studies reject\nat least one of these tests and their diagnostic procedures are invalid for\njustifying the identifying assumption. We offer a joint testing procedure to\nresolve the multiple testing problem. Our procedure is based on a new joint\nasymptotic normality of local linear estimates and local polynomial density\nestimates. In simulation studies, our joint testing procedures outperform the\nBonferroni correction. We implement the procedure as an R package, rdtest, with\ntwo empirical examples in its vignettes."}, "http://arxiv.org/abs/2212.04620": {"title": "On the Non-Identification of Revenue Production Functions", "link": "http://arxiv.org/abs/2212.04620", "description": "Production functions are potentially misspecified when revenue is used as a\nproxy for output. I formalize and strengthen this common knowledge by showing\nthat neither the production function nor Hicks-neutral productivity can be\nidentified with such a revenue proxy. This result holds under the standard\nassumptions used in the literature for a large class of production functions,\nincluding all commonly used parametric forms. Among the prevalent approaches to\naddress this issue, only those that impose assumptions on the underlying demand\nsystem can possibly identify the production function."}, "http://arxiv.org/abs/2307.13364": {"title": "Tuning-free testing of factor regression against factor-augmented sparse alternatives", "link": "http://arxiv.org/abs/2307.13364", "description": "This study introduces a bootstrap test of the validity of factor regression\nwithin a high-dimensional factor-augmented sparse regression model that\nintegrates factor and sparse regression techniques. The test provides a means\nto assess the suitability of the classical dense factor regression model\ncompared to a sparse plus dense alternative augmenting factor regression with\nidiosyncratic shocks. Our proposed test does not require tuning parameters,\neliminates the need to estimate covariance matrices, and offers simplicity in\nimplementation. The validity of the test is theoretically established under\ntime-series dependence. Through simulation experiments, we demonstrate the\nfavorable finite sample performance of our procedure. Moreover, using the\nFRED-MD dataset, we apply the test and reject the adequacy of the classical\nfactor regression model when the dependent variable is inflation but not when\nit is industrial production. These findings offer insights into selecting\nappropriate models for high-dimensional datasets."}, "http://arxiv.org/abs/2201.12936": {"title": "Pigeonhole Design: Balancing Sequential Experiments from an Online Matching Perspective", "link": "http://arxiv.org/abs/2201.12936", "description": "Practitioners and academics have long appreciated the benefits of covariate\nbalancing when they conduct randomized experiments. For web-facing firms\nrunning online A/B tests, however, it still remains challenging in balancing\ncovariate information when experimental subjects arrive sequentially. In this\npaper, we study an online experimental design problem, which we refer to as the\n\"Online Blocking Problem.\" In this problem, experimental subjects with\nheterogeneous covariate information arrive sequentially and must be immediately\nassigned into either the control or the treated group. The objective is to\nminimize the total discrepancy, which is defined as the minimum weight perfect\nmatching between the two groups. To solve this problem, we propose a randomized\ndesign of experiment, which we refer to as the \"Pigeonhole Design.\" The\npigeonhole design first partitions the covariate space into smaller spaces,\nwhich we refer to as pigeonholes, and then, when the experimental subjects\narrive at each pigeonhole, balances the number of control and treated subjects\nfor each pigeonhole. We analyze the theoretical performance of the pigeonhole\ndesign and show its effectiveness by comparing against two well-known benchmark\ndesigns: the match-pair design and the completely randomized design. We\nidentify scenarios when the pigeonhole design demonstrates more benefits over\nthe benchmark design. To conclude, we conduct extensive simulations using\nYahoo! data to show a 10.2% reduction in variance if we use the pigeonhole\ndesign to estimate the average treatment effect."}, "http://arxiv.org/abs/2310.04576": {"title": "Finite Sample Performance of a Conduct Parameter Test in Homogenous Goods Markets", "link": "http://arxiv.org/abs/2310.04576", "description": "We assess the finite sample performance of the conduct parameter test in\nhomogeneous goods markets. Statistical power rises with an increase in the\nnumber of markets, a larger conduct parameter, and a stronger demand rotation\ninstrument. However, even with a moderate number of markets and five firms,\nregardless of instrument strength and the utilization of optimal instruments,\nrejecting the null hypothesis of perfect competition remains challenging. Our\nfindings indicate that empirical results that fail to reject perfect\ncompetition are a consequence of the limited number of markets rather than\nmethodological deficiencies."}, "http://arxiv.org/abs/2310.04853": {"title": "On changepoint detection in functional data using empirical energy distance", "link": "http://arxiv.org/abs/2310.04853", "description": "We propose a novel family of test statistics to detect the presence of\nchangepoints in a sequence of dependent, possibly multivariate,\nfunctional-valued observations. Our approach allows to test for a very general\nclass of changepoints, including the \"classical\" case of changes in the mean,\nand even changes in the whole distribution. Our statistics are based on a\ngeneralisation of the empirical energy distance; we propose weighted\nfunctionals of the energy distance process, which are designed in order to\nenhance the ability to detect breaks occurring at sample endpoints. The\nlimiting distribution of the maximally selected version of our statistics\nrequires only the computation of the eigenvalues of the covariance function,\nthus being readily implementable in the most commonly employed packages, e.g.\nR. We show that, under the alternative, our statistics are able to detect\nchangepoints occurring even very close to the beginning/end of the sample. In\nthe presence of multiple changepoints, we propose a binary segmentation\nalgorithm to estimate the number of breaks and the locations thereof.\nSimulations show that our procedures work very well in finite samples. We\ncomplement our theory with applications to financial and temperature data."}, "http://arxiv.org/abs/2310.05311": {"title": "Identification and Estimation in a Class of Potential Outcomes Models", "link": "http://arxiv.org/abs/2310.05311", "description": "This paper develops a class of potential outcomes models characterized by\nthree main features: (i) Unobserved heterogeneity can be represented by a\nvector of potential outcomes and a type describing the manner in which an\ninstrument determines the choice of treatment; (ii) The availability of an\ninstrumental variable that is conditionally independent of unobserved\nheterogeneity; and (iii) The imposition of convex restrictions on the\ndistribution of unobserved heterogeneity. The proposed class of models\nencompasses multiple classical and novel research designs, yet possesses a\ncommon structure that permits a unifying analysis of identification and\nestimation. In particular, we establish that these models share a common\nnecessary and sufficient condition for identifying certain causal parameters.\nOur identification results are constructive in that they yield estimating\nmoment conditions for the parameters of interest. Focusing on a leading special\ncase of our framework, we further show how these estimating moment conditions\nmay be modified to be doubly robust. The corresponding double robust estimators\nare shown to be asymptotically normally distributed, bootstrap based inference\nis shown to be asymptotically valid, and the semi-parametric efficiency bound\nis derived for those parameters that are root-n estimable. We illustrate the\nusefulness of our results for developing, identifying, and estimating causal\nmodels through an empirical evaluation of the role of mental health as a\nmediating variable in the Moving To Opportunity experiment."}, "http://arxiv.org/abs/2310.05761": {"title": "Robust Minimum Distance Inference in Structural Models", "link": "http://arxiv.org/abs/2310.05761", "description": "This paper proposes minimum distance inference for a structural parameter of\ninterest, which is robust to the lack of identification of other structural\nnuisance parameters. Some choices of the weighting matrix lead to asymptotic\nchi-squared distributions with degrees of freedom that can be consistently\nestimated from the data, even under partial identification. In any case,\nknowledge of the level of under-identification is not required. We study the\npower of our robust test. Several examples show the wide applicability of the\nprocedure and a Monte Carlo investigates its finite sample performance. Our\nidentification-robust inference method can be applied to make inferences on\nboth calibrated (fixed) parameters and any other structural parameter of\ninterest. We illustrate the method's usefulness by applying it to a structural\nmodel on the non-neutrality of monetary policy, as in \\cite{nakamura2018high},\nwhere we empirically evaluate the validity of the calibrated parameters and we\ncarry out robust inference on the slope of the Phillips curve and the\ninformation effect."}, "http://arxiv.org/abs/2302.13066": {"title": "Estimating Fiscal Multipliers by Combining Statistical Identification with Potentially Endogenous Proxies", "link": "http://arxiv.org/abs/2302.13066", "description": "Different proxy variables used in fiscal policy SVARs lead to contradicting\nconclusions regarding the size of fiscal multipliers. In this paper, we show\nthat the conflicting results are due to violations of the exogeneity\nassumptions, i.e. the commonly used proxies are endogenously related to the\nstructural shocks. We propose a novel approach to include proxy variables into\na Bayesian non-Gaussian SVAR, tailored to accommodate potentially endogenous\nproxy variables. Using our model, we show that increasing government spending\nis a more effective tool to stimulate the economy than reducing taxes. We\nconstruct new exogenous proxies that can be used in the traditional proxy VAR\napproach resulting in similar estimates compared to our proposed hybrid SVAR\nmodel."}, "http://arxiv.org/abs/2303.01863": {"title": "Constructing High Frequency Economic Indicators by Imputation", "link": "http://arxiv.org/abs/2303.01863", "description": "Monthly and weekly economic indicators are often taken to be the largest\ncommon factor estimated from high and low frequency data, either separately or\njointly. To incorporate mixed frequency information without directly modeling\nthem, we target a low frequency diffusion index that is already available, and\ntreat high frequency values as missing. We impute these values using multiple\nfactors estimated from the high frequency data. In the empirical examples\nconsidered, static matrix completion that does not account for serial\ncorrelation in the idiosyncratic errors yields imprecise estimates of the\nmissing values irrespective of how the factors are estimated. Single equation\nand systems-based dynamic procedures that account for serial correlation yield\nimputed values that are closer to the observed low frequency ones. This is the\ncase in the counterfactual exercise that imputes the monthly values of consumer\nsentiment series before 1978 when the data was released only on a quarterly\nbasis. This is also the case for a weekly version of the CFNAI index of\neconomic activity that is imputed using seasonally unadjusted data. The imputed\nseries reveals episodes of increased variability of weekly economic information\nthat are masked by the monthly data, notably around the 2014-15 collapse in oil\nprices."}, "http://arxiv.org/abs/2310.06242": {"title": "Treatment Choice, Mean Square Regret and Partial Identification", "link": "http://arxiv.org/abs/2310.06242", "description": "We consider a decision maker who faces a binary treatment choice when their\nwelfare is only partially identified from data. We contribute to the literature\nby anchoring our finite-sample analysis on mean square regret, a decision\ncriterion advocated by Kitagawa, Lee, and Qiu (2022). We find that optimal\nrules are always fractional, irrespective of the width of the identified set\nand precision of its estimate. The optimal treatment fraction is a simple\nlogistic transformation of the commonly used t-statistic multiplied by a factor\ncalculated by a simple constrained optimization. This treatment fraction gets\ncloser to 0.5 as the width of the identified set becomes wider, implying the\ndecision maker becomes more cautious against the adversarial Nature."}, "http://arxiv.org/abs/2009.01995": {"title": "Instrument Validity for Heterogeneous Causal Effects", "link": "http://arxiv.org/abs/2009.01995", "description": "This paper provides a general framework for testing instrument validity in\nheterogeneous causal effect models. The generalization includes the cases where\nthe treatment can be multivalued ordered or unordered. Based on a series of\ntestable implications, we propose a nonparametric test which is proved to be\nasymptotically size controlled and consistent. Compared to the tests in the\nliterature, our test can be applied in more general settings and may achieve\npower improvement. Refutation of instrument validity by the test helps detect\ninvalid instruments that may yield implausible results on causal effects.\nEvidence that the test performs well on finite samples is provided via\nsimulations. We revisit the empirical study on return to schooling to\ndemonstrate application of the proposed test in practice. An extended\ncontinuous mapping theorem and an extended delta method, which may be of\nindependent interest, are provided to establish the asymptotic distribution of\nthe test statistic under null."}, "http://arxiv.org/abs/2009.07551": {"title": "Manipulation-Robust Regression Discontinuity Designs", "link": "http://arxiv.org/abs/2009.07551", "description": "We present a new identification condition for regression discontinuity\ndesigns. We replace the local randomization of Lee (2008) with two restrictions\non its threat, namely, the manipulation of the running variable. Furthermore,\nwe provide the first auxiliary assumption of McCrary's (2008) diagnostic test\nto detect manipulation. Based on our auxiliary assumption, we derive a novel\nexpression of moments that immediately implies the worst-case bounds of Gerard,\nRokkanen, and Rothe (2020) and an enhanced interpretation of their target\nparameters. We highlight two issues: an overlooked source of identification\nfailure, and a missing auxiliary assumption to detect manipulation. In the case\nstudies, we illustrate our solution to these issues using institutional details\nand economic theories."}, "http://arxiv.org/abs/2205.02274": {"title": "Reducing Marketplace Interference Bias Via Shadow Prices", "link": "http://arxiv.org/abs/2205.02274", "description": "Marketplace companies rely heavily on experimentation when making changes to\nthe design or operation of their platforms. The workhorse of experimentation is\nthe randomized controlled trial (RCT), or A/B test, in which users are randomly\nassigned to treatment or control groups. However, marketplace interference\ncauses the Stable Unit Treatment Value Assumption (SUTVA) to be violated,\nleading to bias in the standard RCT metric. In this work, we propose techniques\nfor platforms to run standard RCTs and still obtain meaningful estimates\ndespite the presence of marketplace interference. We specifically consider a\ngeneralized matching setting, in which the platform explicitly matches supply\nwith demand via a linear programming algorithm. Our first proposal is for the\nplatform to estimate the value of global treatment and global control via\noptimization. We prove that this approach is unbiased in the fluid limit. Our\nsecond proposal is to compare the average shadow price of the treatment and\ncontrol groups rather than the total value accrued by each group. We prove that\nthis technique corresponds to the correct first-order approximation (in a\nTaylor series sense) of the value function of interest even in a finite-size\nsystem. We then use this result to prove that, under reasonable assumptions,\nour estimator is less biased than the RCT estimator. At the heart of our result\nis the idea that it is relatively easy to model interference in matching-driven\nmarketplaces since, in such markets, the platform intermediates the spillover."}, "http://arxiv.org/abs/2208.09638": {"title": "Optimal Pre-Analysis Plans: Statistical Decisions Subject to Implementability", "link": "http://arxiv.org/abs/2208.09638", "description": "What is the purpose of pre-analysis plans, and how should they be designed?\nWe propose a principal-agent model where a decision-maker relies on selective\nbut truthful reports by an analyst. The analyst has data access, and\nnon-aligned objectives. In this model, the implementation of statistical\ndecision rules (tests, estimators) requires an incentive-compatible mechanism.\nWe first characterize which decision rules can be implemented. We then\ncharacterize optimal statistical decision rules subject to implementability. We\nshow that implementation requires pre-analysis plans. Focussing specifically on\nhypothesis tests, we show that optimal rejection rules pre-register a valid\ntest for the case when all data is reported, and make worst-case assumptions\nabout unreported data. Optimal tests can be found as a solution to a\nlinear-programming problem."}, "http://arxiv.org/abs/2302.11505": {"title": "Decomposition and Interpretation of Treatment Effects in Settings with Delayed Outcomes", "link": "http://arxiv.org/abs/2302.11505", "description": "This paper studies settings where the analyst is interested in identifying\nand estimating the average causal effect of a binary treatment on an outcome.\nWe consider a setup in which the outcome realization does not get immediately\nrealized after the treatment assignment, a feature that is ubiquitous in\nempirical settings. The period between the treatment and the realization of the\noutcome allows other observed actions to occur and affect the outcome. In this\ncontext, we study several regression-based estimands routinely used in\nempirical work to capture the average treatment effect and shed light on\ninterpreting them in terms of ceteris paribus effects, indirect causal effects,\nand selection terms. We obtain three main and related takeaways. First, the\nthree most popular estimands do not generally satisfy what we call \\emph{strong\nsign preservation}, in the sense that these estimands may be negative even when\nthe treatment positively affects the outcome conditional on any possible\ncombination of other actions. Second, the most popular regression that includes\nthe other actions as controls satisfies strong sign preservation \\emph{if and\nonly if} these actions are mutually exclusive binary variables. Finally, we\nshow that a linear regression that fully stratifies the other actions leads to\nestimands that satisfy strong sign preservation."}, "http://arxiv.org/abs/2302.13455": {"title": "Nickell Bias in Panel Local Projection: Financial Crises Are Worse Than You Think", "link": "http://arxiv.org/abs/2302.13455", "description": "Local Projection is widely used for impulse response estimation, with the\nFixed Effect (FE) estimator being the default for panel data. This paper\nhighlights the presence of Nickell bias for all regressors in the FE estimator,\neven if lagged dependent variables are absent in the regression. This bias is\nthe consequence of the inherent panel predictive specification. We recommend\nusing the split-panel jackknife estimator to eliminate the asymptotic bias and\nrestore the standard statistical inference. Revisiting three macro-finance\nstudies on the linkage between financial crises and economic contraction, we\nfind that the FE estimator substantially underestimates the post-crisis\neconomic losses."}, "http://arxiv.org/abs/2310.07151": {"title": "Identification and Estimation of a Semiparametric Logit Model using Network Data", "link": "http://arxiv.org/abs/2310.07151", "description": "This paper studies the identification and estimation of a semiparametric\nbinary network model in which the unobserved social characteristic is\nendogenous, that is, the unobserved individual characteristic influences both\nthe binary outcome of interest and how links are formed within the network. The\nexact functional form of the latent social characteristic is not known. The\nproposed estimators are obtained based on matching pairs of agents whose\nnetwork formation distributions are the same. The consistency and the\nasymptotic distribution of the estimators are proposed. The finite sample\nproperties of the proposed estimators in a Monte-Carlo simulation are assessed.\nWe conclude this study with an empirical application."}, "http://arxiv.org/abs/2310.07558": {"title": "Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning", "link": "http://arxiv.org/abs/2310.07558", "description": "We study the dynamic pricing problem where the demand function is\nnonparametric and H\\\"older smooth, and we focus on adaptivity to the unknown\nH\\\"older smoothness parameter $\\beta$ of the demand function. Traditionally the\noptimal dynamic pricing algorithm heavily relies on the knowledge of $\\beta$ to\nachieve a minimax optimal regret of\n$\\widetilde{O}(T^{\\frac{\\beta+1}{2\\beta+1}})$. However, we highlight the\nchallenge of adaptivity in this dynamic pricing problem by proving that no\npricing policy can adaptively achieve this minimax optimal regret without\nknowledge of $\\beta$. Motivated by the impossibility result, we propose a\nself-similarity condition to enable adaptivity. Importantly, we show that the\nself-similarity condition does not compromise the problem's inherent complexity\nsince it preserves the regret lower bound\n$\\Omega(T^{\\frac{\\beta+1}{2\\beta+1}})$. Furthermore, we develop a\nsmoothness-adaptive dynamic pricing algorithm and theoretically prove that the\nalgorithm achieves this minimax optimal regret bound without the prior\nknowledge $\\beta$."}, "http://arxiv.org/abs/1910.07452": {"title": "Identifying Network Ties from Panel Data: Theory and an Application to Tax Competition", "link": "http://arxiv.org/abs/1910.07452", "description": "Social interactions determine many economic behaviors, but information on\nsocial ties does not exist in most publicly available and widely used datasets.\nWe present results on the identification of social networks from observational\npanel data that contains no information on social ties between agents. In the\ncontext of a canonical social interactions model, we provide sufficient\nconditions under which the social interactions matrix, endogenous and exogenous\nsocial effect parameters are all globally identified. While this result is\nrelevant across different estimation strategies, we then describe how\nhigh-dimensional estimation techniques can be used to estimate the interactions\nmodel based on the Adaptive Elastic Net GMM method. We employ the method to\nstudy tax competition across US states. We find the identified social\ninteractions matrix implies tax competition differs markedly from the common\nassumption of competition between geographically neighboring states, providing\nfurther insights for the long-standing debate on the relative roles of factor\nmobility and yardstick competition in driving tax setting behavior across\nstates. Most broadly, our identification and application show the analysis of\nsocial interactions can be extended to economic realms where no network data\nexists."}, "http://arxiv.org/abs/2308.00913": {"title": "The Bayesian Context Trees State Space Model for time series modelling and forecasting", "link": "http://arxiv.org/abs/2308.00913", "description": "A hierarchical Bayesian framework is introduced for developing rich mixture\nmodels for real-valued time series, partly motivated by important applications\nin financial time series analysis. At the top level, meaningful discrete states\nare identified as appropriately quantised values of some of the most recent\nsamples. These observable states are described as a discrete context-tree\nmodel. At the bottom level, a different, arbitrary model for real-valued time\nseries -- a base model -- is associated with each state. This defines a very\ngeneral framework that can be used in conjunction with any existing model class\nto build flexible and interpretable mixture models. We call this the Bayesian\nContext Trees State Space Model, or the BCT-X framework. Efficient algorithms\nare introduced that allow for effective, exact Bayesian inference and learning\nin this setting; in particular, the maximum a posteriori probability (MAP)\ncontext-tree model can be identified. These algorithms can be updated\nsequentially, facilitating efficient online forecasting. The utility of the\ngeneral framework is illustrated in two particular instances: When\nautoregressive (AR) models are used as base models, resulting in a nonlinear AR\nmixture model, and when conditional heteroscedastic (ARCH) models are used,\nresulting in a mixture model that offers a powerful and systematic way of\nmodelling the well-known volatility asymmetries in financial data. In\nforecasting, the BCT-X methods are found to outperform state-of-the-art\ntechniques on simulated and real-world data, both in terms of accuracy and\ncomputational requirements. In modelling, the BCT-X structure finds natural\nstructure present in the data. In particular, the BCT-ARCH model reveals a\nnovel, important feature of stock market index data, in the form of an enhanced\nleverage effect."}}