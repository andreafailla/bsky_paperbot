{"http://arxiv.org/abs/2310.03435": {"title": "Variational Inference for GARCH-family Models", "link": "http://arxiv.org/abs/2310.03435", "description": "The Bayesian estimation of GARCH-family models has been typically addressed\nthrough Monte Carlo sampling. Variational Inference is gaining popularity and\nattention as a robust approach for Bayesian inference in complex machine\nlearning models; however, its adoption in econometrics and finance is limited.\nThis paper discusses the extent to which Variational Inference constitutes a\nreliable and feasible alternative to Monte Carlo sampling for Bayesian\ninference in GARCH-like models. Through a large-scale experiment involving the\nconstituents of the S&amp;P 500 index, several Variational Inference optimizers, a\nvariety of volatility models, and a case study, we show that Variational\nInference is an attractive, remarkably well-calibrated, and competitive method\nfor Bayesian learning."}, "http://arxiv.org/abs/2310.03521": {"title": "Cutting Feedback in Misspecified Copula Models", "link": "http://arxiv.org/abs/2310.03521", "description": "In copula models the marginal distributions and copula function are specified\nseparately. We treat these as two modules in a modular Bayesian inference\nframework, and propose conducting modified Bayesian inference by ``cutting\nfeedback''. Cutting feedback limits the influence of potentially misspecified\nmodules in posterior inference. We consider two types of cuts. The first limits\nthe influence of a misspecified copula on inference for the marginals, which is\na Bayesian analogue of the popular Inference for Margins (IFM) estimator. The\nsecond limits the influence of misspecified marginals on inference for the\ncopula parameters by using a rank likelihood to define the cut model. We\nestablish that if only one of the modules is misspecified, then the appropriate\ncut posterior gives accurate uncertainty quantification asymptotically for the\nparameters in the other module. Computation of the cut posteriors is difficult,\nand new variational inference methods to do so are proposed. The efficacy of\nthe new methodology is demonstrated using both simulated data and a substantive\nmultivariate time series application from macroeconomic forecasting. In the\nlatter, cutting feedback from misspecified marginals to a 1096 dimension copula\nimproves posterior inference and predictive accuracy greatly, compared to\nconventional Bayesian inference."}, "http://arxiv.org/abs/2205.04345": {"title": "Joint diagnostic test of regression discontinuity designs: multiple testing problem", "link": "http://arxiv.org/abs/2205.04345", "description": "Current diagnostic tests for regression discontinuity (RD) design face a\nmultiple testing problem. We find a massive over-rejection of the identifying\nrestriction among empirical RD studies published in top-five economics\njournals. Each test achieves a nominal size of 5%; however, the median number\nof tests per study is 12. Consequently, more than one-third of studies reject\nat least one of these tests and their diagnostic procedures are invalid for\njustifying the identifying assumption. We offer a joint testing procedure to\nresolve the multiple testing problem. Our procedure is based on a new joint\nasymptotic normality of local linear estimates and local polynomial density\nestimates. In simulation studies, our joint testing procedures outperform the\nBonferroni correction. We implement the procedure as an R package, rdtest, with\ntwo empirical examples in its vignettes."}, "http://arxiv.org/abs/2212.04620": {"title": "On the Non-Identification of Revenue Production Functions", "link": "http://arxiv.org/abs/2212.04620", "description": "Production functions are potentially misspecified when revenue is used as a\nproxy for output. I formalize and strengthen this common knowledge by showing\nthat neither the production function nor Hicks-neutral productivity can be\nidentified with such a revenue proxy. This result holds under the standard\nassumptions used in the literature for a large class of production functions,\nincluding all commonly used parametric forms. Among the prevalent approaches to\naddress this issue, only those that impose assumptions on the underlying demand\nsystem can possibly identify the production function."}, "http://arxiv.org/abs/2307.13364": {"title": "Tuning-free testing of factor regression against factor-augmented sparse alternatives", "link": "http://arxiv.org/abs/2307.13364", "description": "This study introduces a bootstrap test of the validity of factor regression\nwithin a high-dimensional factor-augmented sparse regression model that\nintegrates factor and sparse regression techniques. The test provides a means\nto assess the suitability of the classical dense factor regression model\ncompared to a sparse plus dense alternative augmenting factor regression with\nidiosyncratic shocks. Our proposed test does not require tuning parameters,\neliminates the need to estimate covariance matrices, and offers simplicity in\nimplementation. The validity of the test is theoretically established under\ntime-series dependence. Through simulation experiments, we demonstrate the\nfavorable finite sample performance of our procedure. Moreover, using the\nFRED-MD dataset, we apply the test and reject the adequacy of the classical\nfactor regression model when the dependent variable is inflation but not when\nit is industrial production. These findings offer insights into selecting\nappropriate models for high-dimensional datasets."}, "http://arxiv.org/abs/2201.12936": {"title": "Pigeonhole Design: Balancing Sequential Experiments from an Online Matching Perspective", "link": "http://arxiv.org/abs/2201.12936", "description": "Practitioners and academics have long appreciated the benefits of covariate\nbalancing when they conduct randomized experiments. For web-facing firms\nrunning online A/B tests, however, it still remains challenging in balancing\ncovariate information when experimental subjects arrive sequentially. In this\npaper, we study an online experimental design problem, which we refer to as the\n\"Online Blocking Problem.\" In this problem, experimental subjects with\nheterogeneous covariate information arrive sequentially and must be immediately\nassigned into either the control or the treated group. The objective is to\nminimize the total discrepancy, which is defined as the minimum weight perfect\nmatching between the two groups. To solve this problem, we propose a randomized\ndesign of experiment, which we refer to as the \"Pigeonhole Design.\" The\npigeonhole design first partitions the covariate space into smaller spaces,\nwhich we refer to as pigeonholes, and then, when the experimental subjects\narrive at each pigeonhole, balances the number of control and treated subjects\nfor each pigeonhole. We analyze the theoretical performance of the pigeonhole\ndesign and show its effectiveness by comparing against two well-known benchmark\ndesigns: the match-pair design and the completely randomized design. We\nidentify scenarios when the pigeonhole design demonstrates more benefits over\nthe benchmark design. To conclude, we conduct extensive simulations using\nYahoo! data to show a 10.2% reduction in variance if we use the pigeonhole\ndesign to estimate the average treatment effect."}, "http://arxiv.org/abs/2310.04576": {"title": "Finite Sample Performance of a Conduct Parameter Test in Homogenous Goods Markets", "link": "http://arxiv.org/abs/2310.04576", "description": "We assess the finite sample performance of the conduct parameter test in\nhomogeneous goods markets. Statistical power rises with an increase in the\nnumber of markets, a larger conduct parameter, and a stronger demand rotation\ninstrument. However, even with a moderate number of markets and five firms,\nregardless of instrument strength and the utilization of optimal instruments,\nrejecting the null hypothesis of perfect competition remains challenging. Our\nfindings indicate that empirical results that fail to reject perfect\ncompetition are a consequence of the limited number of markets rather than\nmethodological deficiencies."}, "http://arxiv.org/abs/2310.04853": {"title": "On changepoint detection in functional data using empirical energy distance", "link": "http://arxiv.org/abs/2310.04853", "description": "We propose a novel family of test statistics to detect the presence of\nchangepoints in a sequence of dependent, possibly multivariate,\nfunctional-valued observations. Our approach allows to test for a very general\nclass of changepoints, including the \"classical\" case of changes in the mean,\nand even changes in the whole distribution. Our statistics are based on a\ngeneralisation of the empirical energy distance; we propose weighted\nfunctionals of the energy distance process, which are designed in order to\nenhance the ability to detect breaks occurring at sample endpoints. The\nlimiting distribution of the maximally selected version of our statistics\nrequires only the computation of the eigenvalues of the covariance function,\nthus being readily implementable in the most commonly employed packages, e.g.\nR. We show that, under the alternative, our statistics are able to detect\nchangepoints occurring even very close to the beginning/end of the sample. In\nthe presence of multiple changepoints, we propose a binary segmentation\nalgorithm to estimate the number of breaks and the locations thereof.\nSimulations show that our procedures work very well in finite samples. We\ncomplement our theory with applications to financial and temperature data."}, "http://arxiv.org/abs/2310.05311": {"title": "Identification and Estimation in a Class of Potential Outcomes Models", "link": "http://arxiv.org/abs/2310.05311", "description": "This paper develops a class of potential outcomes models characterized by\nthree main features: (i) Unobserved heterogeneity can be represented by a\nvector of potential outcomes and a type describing the manner in which an\ninstrument determines the choice of treatment; (ii) The availability of an\ninstrumental variable that is conditionally independent of unobserved\nheterogeneity; and (iii) The imposition of convex restrictions on the\ndistribution of unobserved heterogeneity. The proposed class of models\nencompasses multiple classical and novel research designs, yet possesses a\ncommon structure that permits a unifying analysis of identification and\nestimation. In particular, we establish that these models share a common\nnecessary and sufficient condition for identifying certain causal parameters.\nOur identification results are constructive in that they yield estimating\nmoment conditions for the parameters of interest. Focusing on a leading special\ncase of our framework, we further show how these estimating moment conditions\nmay be modified to be doubly robust. The corresponding double robust estimators\nare shown to be asymptotically normally distributed, bootstrap based inference\nis shown to be asymptotically valid, and the semi-parametric efficiency bound\nis derived for those parameters that are root-n estimable. We illustrate the\nusefulness of our results for developing, identifying, and estimating causal\nmodels through an empirical evaluation of the role of mental health as a\nmediating variable in the Moving To Opportunity experiment."}, "http://arxiv.org/abs/2310.05761": {"title": "Robust Minimum Distance Inference in Structural Models", "link": "http://arxiv.org/abs/2310.05761", "description": "This paper proposes minimum distance inference for a structural parameter of\ninterest, which is robust to the lack of identification of other structural\nnuisance parameters. Some choices of the weighting matrix lead to asymptotic\nchi-squared distributions with degrees of freedom that can be consistently\nestimated from the data, even under partial identification. In any case,\nknowledge of the level of under-identification is not required. We study the\npower of our robust test. Several examples show the wide applicability of the\nprocedure and a Monte Carlo investigates its finite sample performance. Our\nidentification-robust inference method can be applied to make inferences on\nboth calibrated (fixed) parameters and any other structural parameter of\ninterest. We illustrate the method's usefulness by applying it to a structural\nmodel on the non-neutrality of monetary policy, as in \\cite{nakamura2018high},\nwhere we empirically evaluate the validity of the calibrated parameters and we\ncarry out robust inference on the slope of the Phillips curve and the\ninformation effect."}, "http://arxiv.org/abs/2302.13066": {"title": "Estimating Fiscal Multipliers by Combining Statistical Identification with Potentially Endogenous Proxies", "link": "http://arxiv.org/abs/2302.13066", "description": "Different proxy variables used in fiscal policy SVARs lead to contradicting\nconclusions regarding the size of fiscal multipliers. In this paper, we show\nthat the conflicting results are due to violations of the exogeneity\nassumptions, i.e. the commonly used proxies are endogenously related to the\nstructural shocks. We propose a novel approach to include proxy variables into\na Bayesian non-Gaussian SVAR, tailored to accommodate potentially endogenous\nproxy variables. Using our model, we show that increasing government spending\nis a more effective tool to stimulate the economy than reducing taxes. We\nconstruct new exogenous proxies that can be used in the traditional proxy VAR\napproach resulting in similar estimates compared to our proposed hybrid SVAR\nmodel."}, "http://arxiv.org/abs/2303.01863": {"title": "Constructing High Frequency Economic Indicators by Imputation", "link": "http://arxiv.org/abs/2303.01863", "description": "Monthly and weekly economic indicators are often taken to be the largest\ncommon factor estimated from high and low frequency data, either separately or\njointly. To incorporate mixed frequency information without directly modeling\nthem, we target a low frequency diffusion index that is already available, and\ntreat high frequency values as missing. We impute these values using multiple\nfactors estimated from the high frequency data. In the empirical examples\nconsidered, static matrix completion that does not account for serial\ncorrelation in the idiosyncratic errors yields imprecise estimates of the\nmissing values irrespective of how the factors are estimated. Single equation\nand systems-based dynamic procedures that account for serial correlation yield\nimputed values that are closer to the observed low frequency ones. This is the\ncase in the counterfactual exercise that imputes the monthly values of consumer\nsentiment series before 1978 when the data was released only on a quarterly\nbasis. This is also the case for a weekly version of the CFNAI index of\neconomic activity that is imputed using seasonally unadjusted data. The imputed\nseries reveals episodes of increased variability of weekly economic information\nthat are masked by the monthly data, notably around the 2014-15 collapse in oil\nprices."}, "http://arxiv.org/abs/2310.06242": {"title": "Treatment Choice, Mean Square Regret and Partial Identification", "link": "http://arxiv.org/abs/2310.06242", "description": "We consider a decision maker who faces a binary treatment choice when their\nwelfare is only partially identified from data. We contribute to the literature\nby anchoring our finite-sample analysis on mean square regret, a decision\ncriterion advocated by Kitagawa, Lee, and Qiu (2022). We find that optimal\nrules are always fractional, irrespective of the width of the identified set\nand precision of its estimate. The optimal treatment fraction is a simple\nlogistic transformation of the commonly used t-statistic multiplied by a factor\ncalculated by a simple constrained optimization. This treatment fraction gets\ncloser to 0.5 as the width of the identified set becomes wider, implying the\ndecision maker becomes more cautious against the adversarial Nature."}, "http://arxiv.org/abs/2009.01995": {"title": "Instrument Validity for Heterogeneous Causal Effects", "link": "http://arxiv.org/abs/2009.01995", "description": "This paper provides a general framework for testing instrument validity in\nheterogeneous causal effect models. The generalization includes the cases where\nthe treatment can be multivalued ordered or unordered. Based on a series of\ntestable implications, we propose a nonparametric test which is proved to be\nasymptotically size controlled and consistent. Compared to the tests in the\nliterature, our test can be applied in more general settings and may achieve\npower improvement. Refutation of instrument validity by the test helps detect\ninvalid instruments that may yield implausible results on causal effects.\nEvidence that the test performs well on finite samples is provided via\nsimulations. We revisit the empirical study on return to schooling to\ndemonstrate application of the proposed test in practice. An extended\ncontinuous mapping theorem and an extended delta method, which may be of\nindependent interest, are provided to establish the asymptotic distribution of\nthe test statistic under null."}, "http://arxiv.org/abs/2009.07551": {"title": "Manipulation-Robust Regression Discontinuity Designs", "link": "http://arxiv.org/abs/2009.07551", "description": "We present a new identification condition for regression discontinuity\ndesigns. We replace the local randomization of Lee (2008) with two restrictions\non its threat, namely, the manipulation of the running variable. Furthermore,\nwe provide the first auxiliary assumption of McCrary's (2008) diagnostic test\nto detect manipulation. Based on our auxiliary assumption, we derive a novel\nexpression of moments that immediately implies the worst-case bounds of Gerard,\nRokkanen, and Rothe (2020) and an enhanced interpretation of their target\nparameters. We highlight two issues: an overlooked source of identification\nfailure, and a missing auxiliary assumption to detect manipulation. In the case\nstudies, we illustrate our solution to these issues using institutional details\nand economic theories."}, "http://arxiv.org/abs/2205.02274": {"title": "Reducing Marketplace Interference Bias Via Shadow Prices", "link": "http://arxiv.org/abs/2205.02274", "description": "Marketplace companies rely heavily on experimentation when making changes to\nthe design or operation of their platforms. The workhorse of experimentation is\nthe randomized controlled trial (RCT), or A/B test, in which users are randomly\nassigned to treatment or control groups. However, marketplace interference\ncauses the Stable Unit Treatment Value Assumption (SUTVA) to be violated,\nleading to bias in the standard RCT metric. In this work, we propose techniques\nfor platforms to run standard RCTs and still obtain meaningful estimates\ndespite the presence of marketplace interference. We specifically consider a\ngeneralized matching setting, in which the platform explicitly matches supply\nwith demand via a linear programming algorithm. Our first proposal is for the\nplatform to estimate the value of global treatment and global control via\noptimization. We prove that this approach is unbiased in the fluid limit. Our\nsecond proposal is to compare the average shadow price of the treatment and\ncontrol groups rather than the total value accrued by each group. We prove that\nthis technique corresponds to the correct first-order approximation (in a\nTaylor series sense) of the value function of interest even in a finite-size\nsystem. We then use this result to prove that, under reasonable assumptions,\nour estimator is less biased than the RCT estimator. At the heart of our result\nis the idea that it is relatively easy to model interference in matching-driven\nmarketplaces since, in such markets, the platform intermediates the spillover."}, "http://arxiv.org/abs/2208.09638": {"title": "Optimal Pre-Analysis Plans: Statistical Decisions Subject to Implementability", "link": "http://arxiv.org/abs/2208.09638", "description": "What is the purpose of pre-analysis plans, and how should they be designed?\nWe propose a principal-agent model where a decision-maker relies on selective\nbut truthful reports by an analyst. The analyst has data access, and\nnon-aligned objectives. In this model, the implementation of statistical\ndecision rules (tests, estimators) requires an incentive-compatible mechanism.\nWe first characterize which decision rules can be implemented. We then\ncharacterize optimal statistical decision rules subject to implementability. We\nshow that implementation requires pre-analysis plans. Focussing specifically on\nhypothesis tests, we show that optimal rejection rules pre-register a valid\ntest for the case when all data is reported, and make worst-case assumptions\nabout unreported data. Optimal tests can be found as a solution to a\nlinear-programming problem."}, "http://arxiv.org/abs/2302.11505": {"title": "Decomposition and Interpretation of Treatment Effects in Settings with Delayed Outcomes", "link": "http://arxiv.org/abs/2302.11505", "description": "This paper studies settings where the analyst is interested in identifying\nand estimating the average causal effect of a binary treatment on an outcome.\nWe consider a setup in which the outcome realization does not get immediately\nrealized after the treatment assignment, a feature that is ubiquitous in\nempirical settings. The period between the treatment and the realization of the\noutcome allows other observed actions to occur and affect the outcome. In this\ncontext, we study several regression-based estimands routinely used in\nempirical work to capture the average treatment effect and shed light on\ninterpreting them in terms of ceteris paribus effects, indirect causal effects,\nand selection terms. We obtain three main and related takeaways. First, the\nthree most popular estimands do not generally satisfy what we call \\emph{strong\nsign preservation}, in the sense that these estimands may be negative even when\nthe treatment positively affects the outcome conditional on any possible\ncombination of other actions. Second, the most popular regression that includes\nthe other actions as controls satisfies strong sign preservation \\emph{if and\nonly if} these actions are mutually exclusive binary variables. Finally, we\nshow that a linear regression that fully stratifies the other actions leads to\nestimands that satisfy strong sign preservation."}, "http://arxiv.org/abs/2302.13455": {"title": "Nickell Bias in Panel Local Projection: Financial Crises Are Worse Than You Think", "link": "http://arxiv.org/abs/2302.13455", "description": "Local Projection is widely used for impulse response estimation, with the\nFixed Effect (FE) estimator being the default for panel data. This paper\nhighlights the presence of Nickell bias for all regressors in the FE estimator,\neven if lagged dependent variables are absent in the regression. This bias is\nthe consequence of the inherent panel predictive specification. We recommend\nusing the split-panel jackknife estimator to eliminate the asymptotic bias and\nrestore the standard statistical inference. Revisiting three macro-finance\nstudies on the linkage between financial crises and economic contraction, we\nfind that the FE estimator substantially underestimates the post-crisis\neconomic losses."}, "http://arxiv.org/abs/2310.07151": {"title": "Identification and Estimation of a Semiparametric Logit Model using Network Data", "link": "http://arxiv.org/abs/2310.07151", "description": "This paper studies the identification and estimation of a semiparametric\nbinary network model in which the unobserved social characteristic is\nendogenous, that is, the unobserved individual characteristic influences both\nthe binary outcome of interest and how links are formed within the network. The\nexact functional form of the latent social characteristic is not known. The\nproposed estimators are obtained based on matching pairs of agents whose\nnetwork formation distributions are the same. The consistency and the\nasymptotic distribution of the estimators are proposed. The finite sample\nproperties of the proposed estimators in a Monte-Carlo simulation are assessed.\nWe conclude this study with an empirical application."}, "http://arxiv.org/abs/2310.07558": {"title": "Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning", "link": "http://arxiv.org/abs/2310.07558", "description": "We study the dynamic pricing problem where the demand function is\nnonparametric and H\\\"older smooth, and we focus on adaptivity to the unknown\nH\\\"older smoothness parameter $\\beta$ of the demand function. Traditionally the\noptimal dynamic pricing algorithm heavily relies on the knowledge of $\\beta$ to\nachieve a minimax optimal regret of\n$\\widetilde{O}(T^{\\frac{\\beta+1}{2\\beta+1}})$. However, we highlight the\nchallenge of adaptivity in this dynamic pricing problem by proving that no\npricing policy can adaptively achieve this minimax optimal regret without\nknowledge of $\\beta$. Motivated by the impossibility result, we propose a\nself-similarity condition to enable adaptivity. Importantly, we show that the\nself-similarity condition does not compromise the problem's inherent complexity\nsince it preserves the regret lower bound\n$\\Omega(T^{\\frac{\\beta+1}{2\\beta+1}})$. Furthermore, we develop a\nsmoothness-adaptive dynamic pricing algorithm and theoretically prove that the\nalgorithm achieves this minimax optimal regret bound without the prior\nknowledge $\\beta$."}, "http://arxiv.org/abs/1910.07452": {"title": "Identifying Network Ties from Panel Data: Theory and an Application to Tax Competition", "link": "http://arxiv.org/abs/1910.07452", "description": "Social interactions determine many economic behaviors, but information on\nsocial ties does not exist in most publicly available and widely used datasets.\nWe present results on the identification of social networks from observational\npanel data that contains no information on social ties between agents. In the\ncontext of a canonical social interactions model, we provide sufficient\nconditions under which the social interactions matrix, endogenous and exogenous\nsocial effect parameters are all globally identified. While this result is\nrelevant across different estimation strategies, we then describe how\nhigh-dimensional estimation techniques can be used to estimate the interactions\nmodel based on the Adaptive Elastic Net GMM method. We employ the method to\nstudy tax competition across US states. We find the identified social\ninteractions matrix implies tax competition differs markedly from the common\nassumption of competition between geographically neighboring states, providing\nfurther insights for the long-standing debate on the relative roles of factor\nmobility and yardstick competition in driving tax setting behavior across\nstates. Most broadly, our identification and application show the analysis of\nsocial interactions can be extended to economic realms where no network data\nexists."}, "http://arxiv.org/abs/2308.00913": {"title": "The Bayesian Context Trees State Space Model for time series modelling and forecasting", "link": "http://arxiv.org/abs/2308.00913", "description": "A hierarchical Bayesian framework is introduced for developing rich mixture\nmodels for real-valued time series, partly motivated by important applications\nin financial time series analysis. At the top level, meaningful discrete states\nare identified as appropriately quantised values of some of the most recent\nsamples. These observable states are described as a discrete context-tree\nmodel. At the bottom level, a different, arbitrary model for real-valued time\nseries -- a base model -- is associated with each state. This defines a very\ngeneral framework that can be used in conjunction with any existing model class\nto build flexible and interpretable mixture models. We call this the Bayesian\nContext Trees State Space Model, or the BCT-X framework. Efficient algorithms\nare introduced that allow for effective, exact Bayesian inference and learning\nin this setting; in particular, the maximum a posteriori probability (MAP)\ncontext-tree model can be identified. These algorithms can be updated\nsequentially, facilitating efficient online forecasting. The utility of the\ngeneral framework is illustrated in two particular instances: When\nautoregressive (AR) models are used as base models, resulting in a nonlinear AR\nmixture model, and when conditional heteroscedastic (ARCH) models are used,\nresulting in a mixture model that offers a powerful and systematic way of\nmodelling the well-known volatility asymmetries in financial data. In\nforecasting, the BCT-X methods are found to outperform state-of-the-art\ntechniques on simulated and real-world data, both in terms of accuracy and\ncomputational requirements. In modelling, the BCT-X structure finds natural\nstructure present in the data. In particular, the BCT-ARCH model reveals a\nnovel, important feature of stock market index data, in the form of an enhanced\nleverage effect."}, "http://arxiv.org/abs/2310.07790": {"title": "Integration or fragmentation? A closer look at euro area financial markets", "link": "http://arxiv.org/abs/2310.07790", "description": "This paper examines the degree of integration at euro area financial markets.\nTo that end, we estimate overall and country-specific integration indices based\non a panel vector-autoregression with factor stochastic volatility. Our results\nindicate a more heterogeneous bond market compared to the market for lending\nrates. At both markets, the global financial crisis and the sovereign debt\ncrisis led to a severe decline in financial integration, which fully recovered\nsince then. We furthermore identify countries that deviate from their peers\neither by responding differently to crisis events or by taking on different\nroles in the spillover network. The latter analysis reveals two set of\ncountries, namely a main body of countries that receives and transmits\nspillovers and a second, smaller group of spillover absorbing economies.\nFinally, we demonstrate by estimating an augmented Taylor rule that euro area\nshort-term interest rates are positively linked to the level of integration on\nthe bond market."}, "http://arxiv.org/abs/2310.07839": {"title": "Marital Sorting, Household Inequality and Selection", "link": "http://arxiv.org/abs/2310.07839", "description": "Using CPS data for 1976 to 2022 we explore how wage inequality has evolved\nfor married couples with both spouses working full time full year, and its\nimpact on household income inequality. We also investigate how marriage sorting\npatterns have changed over this period. To determine the factors driving income\ninequality we estimate a model explaining the joint distribution of wages which\naccounts for the spouses' employment decisions. We find that income inequality\nhas increased for these households and increased assortative matching of wages\nhas exacerbated the inequality resulting from individual wage growth. We find\nthat positive sorting partially reflects the correlation across unobservables\ninfluencing both members' of the marriage wages. We decompose the changes in\nsorting patterns over the 47 years comprising our sample into structural,\ncomposition and selection effects and find that the increase in positive\nsorting primarily reflects the increased skill premia for both observed and\nunobserved characteristics."}, "http://arxiv.org/abs/2310.08063": {"title": "Uniform Inference for Nonlinear Endogenous Treatment Effects with High-Dimensional Covariates", "link": "http://arxiv.org/abs/2310.08063", "description": "Nonlinearity and endogeneity are common in causal effect studies with\nobservational data. In this paper, we propose new estimation and inference\nprocedures for nonparametric treatment effect functions with endogeneity and\npotentially high-dimensional covariates. The main innovation of this paper is\nthe double bias correction procedure for the nonparametric instrumental\nvariable (NPIV) model under high dimensions. We provide a useful uniform\nconfidence band of the marginal effect function, defined as the derivative of\nthe nonparametric treatment function. The asymptotic honesty of the confidence\nband is verified in theory. Simulations and an empirical study of air pollution\nand migration demonstrate the validity of our procedures."}, "http://arxiv.org/abs/2310.08115": {"title": "Model-Agnostic Covariate-Assisted Inference on Partially Identified Causal Effects", "link": "http://arxiv.org/abs/2310.08115", "description": "Many causal estimands are only partially identifiable since they depend on\nthe unobservable joint distribution between potential outcomes. Stratification\non pretreatment covariates can yield sharper partial identification bounds;\nhowever, unless the covariates are discrete with relatively small support, this\napproach typically requires consistent estimation of the conditional\ndistributions of the potential outcomes given the covariates. Thus, existing\napproaches may fail under model misspecification or if consistency assumptions\nare violated. In this study, we propose a unified and model-agnostic\ninferential approach for a wide class of partially identified estimands, based\non duality theory for optimal transport problems. In randomized experiments,\nour approach can wrap around any estimates of the conditional distributions and\nprovide uniformly valid inference, even if the initial estimates are\narbitrarily inaccurate. Also, our approach is doubly robust in observational\nstudies. Notably, this property allows analysts to use the multiplier bootstrap\nto select covariates and models without sacrificing validity even if the true\nmodel is not included. Furthermore, if the conditional distributions are\nestimated at semiparametric rates, our approach matches the performance of an\noracle with perfect knowledge of the outcome model. Finally, we propose an\nefficient computational framework, enabling implementation on many practical\nproblems in causal inference."}, "http://arxiv.org/abs/2310.08173": {"title": "Structural Vector Autoregressions and Higher Moments: Challenges and Solutions in Small Samples", "link": "http://arxiv.org/abs/2310.08173", "description": "Generalized method of moments estimators based on higher-order moment\nconditions derived from independent shocks can be used to identify and estimate\nthe simultaneous interaction in structural vector autoregressions. This study\nhighlights two problems that arise when using these estimators in small\nsamples. First, imprecise estimates of the asymptotically efficient weighting\nmatrix and the asymptotic variance lead to volatile estimates and inaccurate\ninference. Second, many moment conditions lead to a small sample scaling bias\ntowards innovations with a variance smaller than the normalizing unit variance\nassumption. To address the first problem, I propose utilizing the assumption of\nindependent structural shocks to estimate the efficient weighting matrix and\nthe variance of the estimator. For the second issue, I propose incorporating a\ncontinuously updated scaling term into the weighting matrix, eliminating the\nscaling bias. To demonstrate the effectiveness of these measures, I conducted a\nMonte Carlo simulation which shows a significant improvement in the performance\nof the estimator."}, "http://arxiv.org/abs/2310.08536": {"title": "Real-time Prediction of the Great Recession and the Covid-19 Recession", "link": "http://arxiv.org/abs/2310.08536", "description": "A series of standard and penalized logistic regression models is employed to\nmodel and forecast the Great Recession and the Covid-19 recession in the US.\nThese two recessions are scrutinized by closely examining the movement of five\nchosen predictors, their regression coefficients, and the predicted\nprobabilities of recession. The empirical analysis explores the predictive\ncontent of numerous macroeconomic and financial indicators with respect to NBER\nrecession indicator. The predictive ability of the underlying models is\nevaluated using a set of statistical evaluation metrics. The results strongly\nsupport the application of penalized logistic regression models in the area of\nrecession prediction. Specifically, the analysis indicates that a mixed usage\nof different penalized logistic regression models over different forecast\nhorizons largely outperform standard logistic regression models in the\nprediction of Great recession in the US, as they achieve higher predictive\naccuracy across 5 different forecast horizons. The Great Recession is largely\npredictable, whereas the Covid-19 recession remains unpredictable, given that\nthe Covid-19 pandemic is a real exogenous event. The results are validated by\nconstructing via principal component analysis (PCA) on a set of selected\nvariables a recession indicator that suffers less from publication lags and\nexhibits a very high correlation with the NBER recession indicator."}, "http://arxiv.org/abs/2210.11355": {"title": "Network Synthetic Interventions: A Causal Framework for Panel Data Under Network Interference", "link": "http://arxiv.org/abs/2210.11355", "description": "We propose a generalization of the synthetic controls and synthetic\ninterventions methodology to incorporate network interference. We consider the\nestimation of unit-specific potential outcomes from panel data in the presence\nof spillover across units and unobserved confounding. Key to our approach is a\nnovel latent factor model that takes into account network interference and\ngeneralizes the factor models typically used in panel data settings. We propose\nan estimator, Network Synthetic Interventions (NSI), and show that it\nconsistently estimates the mean outcomes for a unit under an arbitrary set of\ncounterfactual treatments for the network. We further establish that the\nestimator is asymptotically normal. We furnish two validity tests for whether\nthe NSI estimator reliably generalizes to produce accurate counterfactual\nestimates. We provide a novel graph-based experiment design that guarantees the\nNSI estimator produces accurate counterfactual estimates, and also analyze the\nsample complexity of the proposed design. We conclude with simulations that\ncorroborate our theoretical findings."}, "http://arxiv.org/abs/2310.08672": {"title": "Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal", "link": "http://arxiv.org/abs/2310.08672", "description": "In many settings, interventions may be more effective for some individuals\nthan others, so that targeting interventions may be beneficial. We analyze the\nvalue of targeting in the context of a large-scale field experiment with over\n53,000 college students, where the goal was to use \"nudges\" to encourage\nstudents to renew their financial-aid applications before a non-binding\ndeadline. We begin with baseline approaches to targeting. First, we target\nbased on a causal forest that estimates heterogeneous treatment effects and\nthen assigns students to treatment according to those estimated to have the\nhighest treatment effects. Next, we evaluate two alternative targeting\npolicies, one targeting students with low predicted probability of renewing\nfinancial aid in the absence of the treatment, the other targeting those with\nhigh probability. The predicted baseline outcome is not the ideal criterion for\ntargeting, nor is it a priori clear whether to prioritize low, high, or\nintermediate predicted probability. Nonetheless, targeting on low baseline\noutcomes is common in practice, for example because the relationship between\nindividual characteristics and treatment effects is often difficult or\nimpossible to estimate with historical data. We propose hybrid approaches that\nincorporate the strengths of both predictive approaches (accurate estimation)\nand causal approaches (correct criterion); we show that targeting intermediate\nbaseline outcomes is most effective, while targeting based on low baseline\noutcomes is detrimental. In one year of the experiment, nudging all students\nimproved early filing by an average of 6.4 percentage points over a baseline\naverage of 37% filing, and we estimate that targeting half of the students\nusing our preferred policy attains around 75% of this benefit."}, "http://arxiv.org/abs/2310.09013": {"title": "Smoothed instrumental variables quantile regression", "link": "http://arxiv.org/abs/2310.09013", "description": "In this article, I introduce the sivqr command, which estimates the\ncoefficients of the instrumental variables (IV) quantile regression model\nintroduced by Chernozhukov and Hansen (2005). The sivqr command offers several\nadvantages over the existing ivqreg and ivqreg2 commands for estimating this IV\nquantile regression model, which complements the alternative \"triangular model\"\nbehind cqiv and the \"local quantile treatment effect\" model of ivqte.\nComputationally, sivqr implements the smoothed estimator of Kaplan and Sun\n(2017), who show that smoothing improves both computation time and statistical\naccuracy. Standard errors are computed analytically or by Bayesian bootstrap;\nfor non-iid sampling, sivqr is compatible with bootstrap. I discuss syntax and\nthe underlying methodology, and I compare sivqr with other commands in an\nexample."}, "http://arxiv.org/abs/2310.09105": {"title": "Estimating Individual Responses when Tomorrow Matters", "link": "http://arxiv.org/abs/2310.09105", "description": "We propose a regression-based approach to estimate how individuals'\nexpectations influence their responses to a counterfactual change. We provide\nconditions under which average partial effects based on regression estimates\nrecover structural effects. We propose a practical three-step estimation method\nthat relies on subjective beliefs data. We illustrate our approach in a model\nof consumption and saving, focusing on the impact of an income tax that not\nonly changes current income but also affects beliefs about future income. By\napplying our approach to survey data from Italy, we find that considering\nindividuals' beliefs matter for evaluating the impact of tax policies on\nconsumption decisions."}, "http://arxiv.org/abs/2210.09828": {"title": "Modelling Large Dimensional Datasets with Markov Switching Factor Models", "link": "http://arxiv.org/abs/2210.09828", "description": "We study a novel large dimensional approximate factor model with regime\nchanges in the loadings driven by a latent first order Markov process. By\nexploiting the equivalent linear representation of the model, we first recover\nthe latent factors by means of Principal Component Analysis. We then cast the\nmodel in state-space form, and we estimate loadings and transition\nprobabilities through an EM algorithm based on a modified version of the\nBaum-Lindgren-Hamilton-Kim filter and smoother that makes use of the factors\npreviously estimated. Our approach is appealing as it provides closed form\nexpressions for all estimators. More importantly, it does not require knowledge\nof the true number of factors. We derive the theoretical properties of the\nproposed estimation procedure, and we show their good finite sample performance\nthrough a comprehensive set of Monte Carlo experiments. The empirical\nusefulness of our approach is illustrated through an application to a large\nportfolio of stocks."}, "http://arxiv.org/abs/2210.13562": {"title": "Prediction intervals for economic fixed-event forecasts", "link": "http://arxiv.org/abs/2210.13562", "description": "The fixed-event forecasting setup is common in economic policy. It involves a\nsequence of forecasts of the same (`fixed') predictand, so that the difficulty\nof the forecasting problem decreases over time. Fixed-event point forecasts are\ntypically published without a quantitative measure of uncertainty. To construct\nsuch a measure, we consider forecast postprocessing techniques tailored to the\nfixed-event case. We develop regression methods that impose constraints\nmotivated by the problem at hand, and use these methods to construct prediction\nintervals for gross domestic product (GDP) growth in Germany and the US."}, "http://arxiv.org/abs/2302.02747": {"title": "Testing Quantile Forecast Optimality", "link": "http://arxiv.org/abs/2302.02747", "description": "Quantile forecasts made across multiple horizons have become an important\noutput of many financial institutions, central banks and international\norganisations. This paper proposes misspecification tests for such quantile\nforecasts that assess optimality over a set of multiple forecast horizons\nand/or quantiles. The tests build on multiple Mincer-Zarnowitz quantile\nregressions cast in a moment equality framework. Our main test is for the null\nhypothesis of autocalibration, a concept which assesses optimality with respect\nto the information contained in the forecasts themselves. We provide an\nextension that allows to test for optimality with respect to larger information\nsets and a multivariate extension. Importantly, our tests do not just inform\nabout general violations of optimality, but may also provide useful insights\ninto specific forms of sub-optimality. A simulation study investigates the\nfinite sample performance of our tests, and two empirical applications to\nfinancial returns and U.S. macroeconomic series illustrate that our tests can\nyield interesting insights into quantile forecast sub-optimality and its\ncauses."}, "http://arxiv.org/abs/2305.00700": {"title": "Double and Single Descent in Causal Inference with an Application to High-Dimensional Synthetic Control", "link": "http://arxiv.org/abs/2305.00700", "description": "Motivated by a recent literature on the double-descent phenomenon in machine\nlearning, we consider highly over-parameterized models in causal inference,\nincluding synthetic control with many control units. In such models, there may\nbe so many free parameters that the model fits the training data perfectly. We\nfirst investigate high-dimensional linear regression for imputing wage data and\nestimating average treatment effects, where we find that models with many more\ncovariates than sample size can outperform simple ones. We then document the\nperformance of high-dimensional synthetic control estimators with many control\nunits. We find that adding control units can help improve imputation\nperformance even beyond the point where the pre-treatment fit is perfect. We\nprovide a unified theoretical perspective on the performance of these\nhigh-dimensional models. Specifically, we show that more complex models can be\ninterpreted as model-averaging estimators over simpler ones, which we link to\nan improvement in average performance. This perspective yields concrete\ninsights into the use of synthetic control when control units are many relative\nto the number of pre-treatment periods."}, "http://arxiv.org/abs/2310.09398": {"title": "An In-Depth Examination of Requirements for Disclosure Risk Assessment", "link": "http://arxiv.org/abs/2310.09398", "description": "The use of formal privacy to protect the confidentiality of responses in the\n2020 Decennial Census of Population and Housing has triggered renewed interest\nand debate over how to measure the disclosure risks and societal benefits of\nthe published data products. Following long-established precedent in economics\nand statistics, we argue that any proposal for quantifying disclosure risk\nshould be based on pre-specified, objective criteria. Such criteria should be\nused to compare methodologies to identify those with the most desirable\nproperties. We illustrate this approach, using simple desiderata, to evaluate\nthe absolute disclosure risk framework, the counterfactual framework underlying\ndifferential privacy, and prior-to-posterior comparisons. We conclude that\nsatisfying all the desiderata is impossible, but counterfactual comparisons\nsatisfy the most while absolute disclosure risk satisfies the fewest.\nFurthermore, we explain that many of the criticisms levied against differential\nprivacy would be levied against any technology that is not equivalent to\ndirect, unrestricted access to confidential data. Thus, more research is\nneeded, but in the near-term, the counterfactual approach appears best-suited\nfor privacy-utility analysis."}, "http://arxiv.org/abs/2310.09545": {"title": "A Semiparametric Instrumented Difference-in-Differences Approach to Policy Learning", "link": "http://arxiv.org/abs/2310.09545", "description": "Recently, there has been a surge in methodological development for the\ndifference-in-differences (DiD) approach to evaluate causal effects. Standard\nmethods in the literature rely on the parallel trends assumption to identify\nthe average treatment effect on the treated. However, the parallel trends\nassumption may be violated in the presence of unmeasured confounding, and the\naverage treatment effect on the treated may not be useful in learning a\ntreatment assignment policy for the entire population. In this article, we\npropose a general instrumented DiD approach for learning the optimal treatment\npolicy. Specifically, we establish identification results using a binary\ninstrumental variable (IV) when the parallel trends assumption fails to hold.\nAdditionally, we construct a Wald estimator, novel inverse probability\nweighting (IPW) estimators, and a class of semiparametric efficient and\nmultiply robust estimators, with theoretical guarantees on consistency and\nasymptotic normality, even when relying on flexible machine learning algorithms\nfor nuisance parameters estimation. Furthermore, we extend the instrumented DiD\nto the panel data setting. We evaluate our methods in extensive simulations and\na real data application."}, "http://arxiv.org/abs/2310.09597": {"title": "Adaptive maximization of social welfare", "link": "http://arxiv.org/abs/2310.09597", "description": "We consider the problem of repeatedly choosing policies to maximize social\nwelfare. Welfare is a weighted sum of private utility and public revenue.\nEarlier outcomes inform later policies. Utility is not observed, but indirectly\ninferred. Response functions are learned through experimentation.\n\nWe derive a lower bound on regret, and a matching adversarial upper bound for\na variant of the Exp3 algorithm. Cumulative regret grows at a rate of\n$T^{2/3}$. This implies that (i) welfare maximization is harder than the\nmulti-armed bandit problem (with a rate of $T^{1/2}$ for finite policy sets),\nand (ii) our algorithm achieves the optimal rate. For the stochastic setting,\nif social welfare is concave, we can achieve a rate of $T^{1/2}$ (for\ncontinuous policy sets), using a dyadic search algorithm.\n\nWe analyze an extension to nonlinear income taxation, and sketch an extension\nto commodity taxation. We compare our setting to monopoly pricing (which is\neasier), and price setting for bilateral trade (which is harder)."}, "http://arxiv.org/abs/2008.08387": {"title": "A Novel Approach to Predictive Accuracy Testing in Nested Environments", "link": "http://arxiv.org/abs/2008.08387", "description": "We introduce a new approach for comparing the predictive accuracy of two\nnested models that bypasses the difficulties caused by the degeneracy of the\nasymptotic variance of forecast error loss differentials used in the\nconstruction of commonly used predictive comparison statistics. Our approach\ncontinues to rely on the out of sample MSE loss differentials between the two\ncompeting models, leads to nuisance parameter free Gaussian asymptotics and is\nshown to remain valid under flexible assumptions that can accommodate\nheteroskedasticity and the presence of mixed predictors (e.g. stationary and\nlocal to unit root). A local power analysis also establishes its ability to\ndetect departures from the null in both stationary and persistent settings.\nSimulations calibrated to common economic and financial applications indicate\nthat our methods have strong power with good size control across commonly\nencountered sample sizes."}, "http://arxiv.org/abs/2211.07506": {"title": "Type I Tobit Bayesian Additive Regression Trees for Censored Outcome Regression", "link": "http://arxiv.org/abs/2211.07506", "description": "Censoring occurs when an outcome is unobserved beyond some threshold value.\nMethods that do not account for censoring produce biased predictions of the\nunobserved outcome. This paper introduces Type I Tobit Bayesian Additive\nRegression Tree (TOBART-1) models for censored outcomes. Simulation results and\nreal data applications demonstrate that TOBART-1 produces accurate predictions\nof censored outcomes. TOBART-1 provides posterior intervals for the conditional\nexpectation and other quantities of interest. The error term distribution can\nhave a large impact on the expectation of the censored outcome. Therefore the\nerror is flexibly modeled as a Dirichlet process mixture of normal\ndistributions."}, "http://arxiv.org/abs/2302.02866": {"title": "Out of Sample Predictability in Predictive Regressions with Many Predictor Candidates", "link": "http://arxiv.org/abs/2302.02866", "description": "This paper is concerned with detecting the presence of out of sample\npredictability in linear predictive regressions with a potentially large set of\ncandidate predictors. We propose a procedure based on out of sample MSE\ncomparisons that is implemented in a pairwise manner using one predictor at a\ntime and resulting in an aggregate test statistic that is standard normally\ndistributed under the global null hypothesis of no linear predictability.\nPredictors can be highly persistent, purely stationary or a combination of\nboth. Upon rejection of the null hypothesis we subsequently introduce a\npredictor screening procedure designed to identify the most active predictors.\nAn empirical application to key predictors of US economic activity illustrates\nthe usefulness of our methods and highlights the important forward looking role\nplayed by the series of manufacturing new orders."}, "http://arxiv.org/abs/2305.12883": {"title": "Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors", "link": "http://arxiv.org/abs/2305.12883", "description": "In recent years, there has been a significant growth in research focusing on\nminimum $\\ell_2$ norm (ridgeless) interpolation least squares estimators.\nHowever, the majority of these analyses have been limited to a simple\nregression error structure, assuming independent and identically distributed\nerrors with zero mean and common variance. In this paper, we explore prediction\nrisk as well as estimation risk under more general regression error\nassumptions, highlighting the benefits of overparameterization in a finite\nsample. We find that including a large number of unimportant parameters\nrelative to the sample size can effectively reduce both risks. Notably, we\nestablish that the estimation difficulties associated with the variance\ncomponents of both risks can be summarized through the trace of the\nvariance-covariance matrix of the regression errors."}, "http://arxiv.org/abs/2203.09001": {"title": "Selection and parallel trends", "link": "http://arxiv.org/abs/2203.09001", "description": "We study the role of selection into treatment in difference-in-differences\n(DiD) designs. We derive necessary and sufficient conditions for parallel\ntrends assumptions under general classes of selection mechanisms. These\nconditions characterize the empirical content of parallel trends. For settings\nwhere the necessary conditions are questionable, we propose tools for\nselection-based sensitivity analysis. We also provide templates for justifying\nDiD in applications with and without covariates. A reanalysis of the causal\neffect of NSW training programs demonstrates the usefulness of our\nselection-based approach to sensitivity analysis."}, "http://arxiv.org/abs/2207.07318": {"title": "Flexible global forecast combinations", "link": "http://arxiv.org/abs/2207.07318", "description": "Forecast combination -- the aggregation of individual forecasts from multiple\nexperts or models -- is a proven approach to economic forecasting. To date,\nresearch on economic forecasting has concentrated on local combination methods,\nwhich handle separate but related forecasting tasks in isolation. Yet, it has\nbeen known for over two decades in the machine learning community that global\nmethods, which exploit task-relatedness, can improve on local methods that\nignore it. Motivated by the possibility for improvement, this paper introduces\na framework for globally combining forecasts while being flexible to the level\nof task-relatedness. Through our framework, we develop global versions of\nseveral existing forecast combinations. To evaluate the efficacy of these new\nglobal forecast combinations, we conduct extensive comparisons using synthetic\nand real data. Our real data comparisons, which involve forecasts of core\neconomic indicators in the Eurozone, provide empirical evidence that the\naccuracy of global combinations of economic forecasts can surpass local\ncombinations."}, "http://arxiv.org/abs/2210.01938": {"title": "Probability of Causation with Sample Selection: A Reanalysis of the Impacts of J\\'ovenes en Acci\\'on on Formality", "link": "http://arxiv.org/abs/2210.01938", "description": "This paper identifies the probability of causation when there is sample\nselection. We show that the probability of causation is partially identified\nfor individuals who are always observed regardless of treatment status and\nderive sharp bounds under three increasingly restrictive sets of assumptions.\nThe first set imposes an exogenous treatment and a monotone sample selection\nmechanism. To tighten these bounds, the second set also imposes the monotone\ntreatment response assumption, while the third set additionally imposes a\nstochastic dominance assumption. Finally, we use experimental data from the\nColombian job training program J\\'ovenes en Acci\\'on to empirically illustrate\nour approach's usefulness. We find that, among always-employed women, at least\n18% and at most 24% transitioned to the formal labor market because of the\nprogram."}, "http://arxiv.org/abs/2212.06312": {"title": "Policy learning for many outcomes of interest: Combining optimal policy trees with multi-objective Bayesian optimisation", "link": "http://arxiv.org/abs/2212.06312", "description": "Methods for learning optimal policies use causal machine learning models to\ncreate human-interpretable rules for making choices around the allocation of\ndifferent policy interventions. However, in realistic policy-making contexts,\ndecision-makers often care about trade-offs between outcomes, not just\nsingle-mindedly maximising utility for one outcome. This paper proposes an\napproach termed Multi-Objective Policy Learning (MOPoL) which combines optimal\ndecision trees for policy learning with a multi-objective Bayesian optimisation\napproach to explore the trade-off between multiple outcomes. It does this by\nbuilding a Pareto frontier of non-dominated models for different hyperparameter\nsettings which govern outcome weighting. The key here is that a low-cost greedy\ntree can be an accurate proxy for the very computationally costly optimal tree\nfor the purposes of making decisions which means models can be repeatedly fit\nto learn a Pareto frontier. The method is applied to a real-world case-study of\nnon-price rationing of anti-malarial medication in Kenya."}, "http://arxiv.org/abs/2302.08002": {"title": "Deep Learning Enhanced Realized GARCH", "link": "http://arxiv.org/abs/2302.08002", "description": "We propose a new approach to volatility modeling by combining deep learning\n(LSTM) and realized volatility measures. This LSTM-enhanced realized GARCH\nframework incorporates and distills modeling advances from financial\neconometrics, high frequency trading data and deep learning. Bayesian inference\nvia the Sequential Monte Carlo method is employed for statistical inference and\nforecasting. The new framework can jointly model the returns and realized\nvolatility measures, has an excellent in-sample fit and superior predictive\nperformance compared to several benchmark models, while being able to adapt\nwell to the stylized facts in volatility. The performance of the new framework\nis tested using a wide range of metrics, from marginal likelihood, volatility\nforecasting, to tail risk forecasting and option pricing. We report on a\ncomprehensive empirical study using 31 widely traded stock indices over a time\nperiod that includes COVID-19 pandemic."}, "http://arxiv.org/abs/2303.10019": {"title": "Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices", "link": "http://arxiv.org/abs/2303.10019", "description": "This paper presents a new method for combining (or aggregating or ensembling)\nmultivariate probabilistic forecasts, considering dependencies between\nquantiles and marginals through a smoothing procedure that allows for online\nlearning. We discuss two smoothing methods: dimensionality reduction using\nBasis matrices and penalized smoothing. The new online learning algorithm\ngeneralizes the standard CRPS learning framework into multivariate dimensions.\nIt is based on Bernstein Online Aggregation (BOA) and yields optimal asymptotic\nlearning properties. The procedure uses horizontal aggregation, i.e.,\naggregation across quantiles. We provide an in-depth discussion on possible\nextensions of the algorithm and several nested cases related to the existing\nliterature on online forecast combination. We apply the proposed methodology to\nforecasting day-ahead electricity prices, which are 24-dimensional\ndistributional forecasts. The proposed method yields significant improvements\nover uniform combination in terms of continuous ranked probability score\n(CRPS). We discuss the temporal evolution of the weights and hyperparameters\nand present the results of reduced versions of the preferred model. A fast C++\nimplementation of the proposed algorithm will be made available in connection\nwith this paper as an open-source R-Package on CRAN."}, "http://arxiv.org/abs/2309.02072": {"title": "DeepVol: A Pre-Trained Universal Asset Volatility Model", "link": "http://arxiv.org/abs/2309.02072", "description": "This paper introduces DeepVol, a pre-trained deep learning volatility model\nthat is more general than traditional econometric models. DeepVol leverage the\npower of transfer learning to effectively capture and model the volatility\ndynamics of all financial assets, including previously unseen ones, using a\nsingle universal model. This contrasts to the usual practice in the\neconometrics literature, which trains a separate model for each asset. The\nintroduction of DeepVol opens up new avenues for volatility modeling in the\nfinance industry, potentially transforming the way volatility is predicted."}, "http://arxiv.org/abs/2310.11680": {"title": "Trimmed Mean Group Estimation of Average Treatment Effects in Ultra Short T Panels under Correlated Heterogeneity", "link": "http://arxiv.org/abs/2310.11680", "description": "Under correlated heterogeneity, the commonly used two-way fixed effects\nestimator is biased and can lead to misleading inference. This paper proposes a\nnew trimmed mean group (TMG) estimator which is consistent at the irregular\nrate of n^{1/3} even if the time dimension of the panel is as small as the\nnumber of its regressors. Extensions to panels with time effects are provided,\nand a Hausman-type test of correlated heterogeneity is proposed. Small sample\nproperties of the TMG estimator (with and without time effects) are\ninvestigated by Monte Carlo experiments and shown to be satisfactory and\nperform better than other trimmed estimators proposed in the literature. The\nproposed test of correlated heterogeneity is also shown to have the correct\nsize and satisfactory power. The utility of the TMG approach is illustrated\nwith an empirical application."}, "http://arxiv.org/abs/2310.11962": {"title": "Machine Learning for Staggered Difference-in-Differences and Dynamic Treatment Effect Heterogeneity", "link": "http://arxiv.org/abs/2310.11962", "description": "We combine two recently proposed nonparametric difference-in-differences\nmethods, extending them to enable the examination of treatment effect\nheterogeneity in the staggered adoption setting using machine learning. The\nproposed method, machine learning difference-in-differences (MLDID), allows for\nestimation of time-varying conditional average treatment effects on the\ntreated, which can be used to conduct detailed inference on drivers of\ntreatment effect heterogeneity. We perform simulations to evaluate the\nperformance of MLDID and find that it accurately identifies the true predictors\nof treatment effect heterogeneity. We then use MLDID to evaluate the\nheterogeneous impacts of Brazil's Family Health Program on infant mortality,\nand find those in poverty and urban locations experienced the impact of the\npolicy more quickly than other subgroups."}, "http://arxiv.org/abs/2310.11969": {"title": "Survey calibration for causal inference: a simple method to balance covariate distributions", "link": "http://arxiv.org/abs/2310.11969", "description": "This paper proposes a simple method for balancing distributions of covariates\nfor causal inference based on observational studies. The method makes it\npossible to balance an arbitrary number of quantiles (e.g., medians, quartiles,\nor deciles) together with means if necessary. The proposed approach is based on\nthe theory of calibration estimators (Deville and S\\\"arndal 1992), in\nparticular, calibration estimators for quantiles, proposed by Harms and\nDuchesne (2006). By modifying the entropy balancing method and the covariate\nbalancing propensity score method, it is possible to balance the distributions\nof the treatment and control groups. The method does not require numerical\nintegration, kernel density estimation or assumptions about the distributions;\nvalid estimates can be obtained by drawing on existing asymptotic theory.\nResults of a simulation study indicate that the method efficiently estimates\naverage treatment effects on the treated (ATT), the average treatment effect\n(ATE), the quantile treatment effect on the treated (QTT) and the quantile\ntreatment effect (QTE), especially in the presence of non-linearity and\nmis-specification of the models. The proposed methods are implemented in an\nopen source R package jointCalib."}, "http://arxiv.org/abs/2203.04080": {"title": "On Robust Inference in Time Series Regression", "link": "http://arxiv.org/abs/2203.04080", "description": "Least squares regression with heteroskedasticity and autocorrelation\nconsistent (HAC) standard errors has proved very useful in cross section\nenvironments. However, several major difficulties, which are generally\noverlooked, must be confronted when transferring the HAC estimation technology\nto time series environments. First, in plausible time-series environments\ninvolving failure of strong exogeneity, OLS parameter estimates can be\ninconsistent, so that HAC inference fails even asymptotically. Second, most\neconomic time series have strong autocorrelation, which renders HAC regression\nparameter estimates highly inefficient. Third, strong autocorrelation similarly\nrenders HAC conditional predictions highly inefficient. Finally, The structure\nof popular HAC estimators is ill-suited for capturing the autoregressive\nautocorrelation typically present in economic time series, which produces large\nsize distortions and reduced power in HACbased hypothesis testing, in all but\nthe largest samples. We show that all four problems are largely avoided by the\nuse of a simple dynamic regression procedure, which is easily implemented. We\ndemonstrate the advantages of dynamic regression with detailed simulations\ncovering a range of practical issues."}, "http://arxiv.org/abs/2308.15338": {"title": "Another Look at the Linear Probability Model and Nonlinear Index Models", "link": "http://arxiv.org/abs/2308.15338", "description": "We reassess the use of linear models to approximate response probabilities of\nbinary outcomes, focusing on average partial effects (APE). We confirm that\nlinear projection parameters coincide with APEs in certain scenarios. Through\nsimulations, we identify other cases where OLS does or does not approximate\nAPEs and find that having large fraction of fitted values in [0, 1] is neither\nnecessary nor sufficient. We also show nonlinear least squares estimation of\nthe ramp model is consistent and asymptotically normal and is equivalent to\nusing OLS on an iteratively trimmed sample to reduce bias. Our findings offer\npractical guidance for empirical research."}, "http://arxiv.org/abs/2309.10642": {"title": "Testing and correcting sample selection in academic achievement comparisons", "link": "http://arxiv.org/abs/2309.10642", "description": "Country comparisons using standardized test scores may in some cases be\nmisleading unless we make sure that the potential sample selection bias created\nby drop-outs and non-enrollment patterns does not alter the analysis. In this\npaper, I propose an answer to this issue which consists of identifying the\ncounterfactual distribution of achievement (I mean the distribution of\nachievement if there was hypothetically no selection) from the observed\ndistribution of achievements. International comparison measures like means,\nquantiles, and inequality measures have to be computed using that\ncounterfactual distribution which is statistically closer to the observed one\nfor a low proportion of out-of-school children. I identify the quantiles of\nthat latent distribution by readjusting the percentile levels of the observed\nquantile function of achievement. Because the data on test scores is by nature\ntruncated, I have to rely on auxiliary data to borrow identification power. I\nfinally applied my method to compute selection corrected means using PISA 2018\nand PASEC 2019 and I found that ranking/comparisons can change."}, "http://arxiv.org/abs/2310.01104": {"title": "Multi-period static hedging of European options", "link": "http://arxiv.org/abs/2310.01104", "description": "We consider the hedging of European options when the price of the underlying\nasset follows a single-factor Markovian framework. By working in such a\nsetting, Carr and Wu \\cite{carr2014static} derived a spanning relation between\na given option and a continuum of shorter-term options written on the same\nasset. In this paper, we have extended their approach to simultaneously include\noptions over multiple short maturities. We then show a practical implementation\nof this with a finite set of shorter-term options to determine the hedging\nerror using a Gaussian Quadrature method. We perform a wide range of\nexperiments for both the \\textit{Black-Scholes} and \\textit{Merton Jump\nDiffusion} models, illustrating the comparative performance of the two methods."}, "http://arxiv.org/abs/2310.02414": {"title": "On Optimal Set Estimation for Partially Identified Binary Choice Models", "link": "http://arxiv.org/abs/2310.02414", "description": "In this paper we reconsider the notion of optimality in estimation of\npartially identified models. We illustrate the general problem in the context\nof a semiparametric binary choice model with discrete covariates as an example\nof a model which is partially identified as shown in, e.g. Bierens and Hartog\n(1988). A set estimator for the regression coefficients in the model can be\nconstructed by implementing the Maximum Score procedure proposed by Manski\n(1975). For many designs this procedure converges to the identified set for\nthese parameters, and so in one sense is optimal. But as shown in Komarova\n(2013) for other cases the Maximum Score objective function gives an outer\nregion of the identified set. This motivates alternative methods that are\noptimal in one sense that they converge to the identified region in all\ndesigns, and we propose and compare such procedures. One is a Hodges type\nestimator combining the Maximum Score estimator with existing procedures. A\nsecond is a two step estimator using a Maximum Score type objective function in\nthe second step. Lastly we propose a new random set quantile estimator,\nmotivated by definitions introduced in Molchanov (2006). Extensions of these\nideas for the cross sectional model to static and dynamic discrete panel data\nmodels are also provided."}, "http://arxiv.org/abs/2310.12825": {"title": "Nonparametric Regression with Dyadic Data", "link": "http://arxiv.org/abs/2310.12825", "description": "This paper studies the identification and estimation of a nonparametric\nnonseparable dyadic model where the structural function and the distribution of\nthe unobservable random terms are assumed to be unknown. The identification and\nthe estimation of the distribution of the unobservable random term are also\nproposed. I assume that the structural function is continuous and strictly\nincreasing in the unobservable heterogeneity. I propose suitable normalization\nfor the identification by allowing the structural function to have some\ndesirable properties such as homogeneity of degree one in the unobservable\nrandom term and some of its observables. The consistency and the asymptotic\ndistribution of the estimators are proposed. The finite sample properties of\nthe proposed estimators in a Monte-Carlo simulation are assessed."}, "http://arxiv.org/abs/2310.12863": {"title": "Moment-dependent phase transitions in high-dimensional Gaussian approximations", "link": "http://arxiv.org/abs/2310.12863", "description": "High-dimensional central limit theorems have been intensively studied with\nmost focus being on the case where the data is sub-Gaussian or sub-exponential.\nHowever, heavier tails are omnipresent in practice. In this article, we study\nthe critical growth rates of dimension $d$ below which Gaussian approximations\nare asymptotically valid but beyond which they are not. We are particularly\ninterested in how these thresholds depend on the number of moments $m$ that the\nobservations possess. For every $m\\in(2,\\infty)$, we construct i.i.d. random\nvectors $\\textbf{X}_1,...,\\textbf{X}_n$ in $\\mathbb{R}^d$, the entries of which\nare independent and have a common distribution (independent of $n$ and $d$)\nwith finite $m$th absolute moment, and such that the following holds: if there\nexists an $\\varepsilon\\in(0,\\infty)$ such that $d/n^{m/2-1+\\varepsilon}\\not\\to\n0$, then the Gaussian approximation error (GAE) satisfies $$\n\n\\limsup_{n\\to\\infty}\\sup_{t\\in\\mathbb{R}}\\left[\\mathbb{P}\\left(\\max_{1\\leq\nj\\leq d}\\frac{1}{\\sqrt{n}}\\sum_{i=1}^n\\textbf{X}_{ij}\\leq\nt\\right)-\\mathbb{P}\\left(\\max_{1\\leq j\\leq d}\\textbf{Z}_j\\leq\nt\\right)\\right]=1,$$ where $\\textbf{Z} \\sim\n\\mathsf{N}_d(\\textbf{0}_d,\\mathbf{I}_d)$. On the other hand, a result in\nChernozhukov et al. (2023a) implies that the left-hand side above is zero if\njust $d/n^{m/2-1-\\varepsilon}\\to 0$ for some $\\varepsilon\\in(0,\\infty)$. In\nthis sense, there is a moment-dependent phase transition at the threshold\n$d=n^{m/2-1}$ above which the limiting GAE jumps from zero to one."}, "http://arxiv.org/abs/2209.11840": {"title": "Revisiting the Analysis of Matched-Pair and Stratified Experiments in the Presence of Attrition", "link": "http://arxiv.org/abs/2209.11840", "description": "In this paper we revisit some common recommendations regarding the analysis\nof matched-pair and stratified experimental designs in the presence of\nattrition. Our main objective is to clarify a number of well-known claims about\nthe practice of dropping pairs with an attrited unit when analyzing\nmatched-pair designs. Contradictory advice appears in the literature about\nwhether or not dropping pairs is beneficial or harmful, and stratifying into\nlarger groups has been recommended as a resolution to the issue. To address\nthese claims, we derive the estimands obtained from the difference-in-means\nestimator in a matched-pair design both when the observations from pairs with\nan attrited unit are retained and when they are dropped. We find limited\nevidence to support the claims that dropping pairs helps recover the average\ntreatment effect, but we find that it may potentially help in recovering a\nconvex weighted average of conditional average treatment effects. We report\nsimilar findings for stratified designs when studying the estimands obtained\nfrom a regression of outcomes on treatment with and without strata fixed\neffects."}, "http://arxiv.org/abs/2210.04523": {"title": "An identification and testing strategy for proxy-SVARs with weak proxies", "link": "http://arxiv.org/abs/2210.04523", "description": "When proxies (external instruments) used to identify target structural shocks\nare weak, inference in proxy-SVARs (SVAR-IVs) is nonstandard and the\nconstruction of asymptotically valid confidence sets for the impulse responses\nof interest requires weak-instrument robust methods. In the presence of\nmultiple target shocks, test inversion techniques require extra restrictions on\nthe proxy-SVAR parameters other those implied by the proxies that may be\ndifficult to interpret and test. We show that frequentist asymptotic inference\nin these situations can be conducted through Minimum Distance estimation and\nstandard asymptotic methods if the proxy-SVAR can be identified by using\n`strong' instruments for the non-target shocks; i.e. the shocks which are not\nof primary interest in the analysis. The suggested identification strategy\nhinges on a novel pre-test for the null of instrument relevance based on\nbootstrap resampling which is not subject to pre-testing issues, in the sense\nthat the validity of post-test asymptotic inferences is not affected by the\noutcomes of the test. The test is robust to conditionally heteroskedasticity\nand/or zero-censored proxies, is computationally straightforward and applicable\nregardless of the number of shocks being instrumented. Some illustrative\nexamples show the empirical usefulness of the suggested identification and\ntesting strategy."}, "http://arxiv.org/abs/2301.07241": {"title": "Unconditional Quantile Partial Effects via Conditional Quantile Regression", "link": "http://arxiv.org/abs/2301.07241", "description": "This paper develops a semi-parametric procedure for estimation of\nunconditional quantile partial effects using quantile regression coefficients.\nThe estimator is based on an identification result showing that, for continuous\ncovariates, unconditional quantile effects are a weighted average of\nconditional ones at particular quantile levels that depend on the covariates.\nWe propose a two-step estimator for the unconditional effects where in the\nfirst step one estimates a structural quantile regression model, and in the\nsecond step a nonparametric regression is applied to the first step\ncoefficients. We establish the asymptotic properties of the estimator, say\nconsistency and asymptotic normality. Monte Carlo simulations show numerical\nevidence that the estimator has very good finite sample performance and is\nrobust to the selection of bandwidth and kernel. To illustrate the proposed\nmethod, we study the canonical application of the Engel's curve, i.e. food\nexpenditures as a share of income."}, "http://arxiv.org/abs/2302.04380": {"title": "Covariate Adjustment in Experiments with Matched Pairs", "link": "http://arxiv.org/abs/2302.04380", "description": "This paper studies inference on the average treatment effect in experiments\nin which treatment status is determined according to \"matched pairs\" and it is\nadditionally desired to adjust for observed, baseline covariates to gain\nfurther precision. By a \"matched pairs\" design, we mean that units are sampled\ni.i.d. from the population of interest, paired according to observed, baseline\ncovariates and finally, within each pair, one unit is selected at random for\ntreatment. Importantly, we presume that not all observed, baseline covariates\nare used in determining treatment assignment. We study a broad class of\nestimators based on a \"doubly robust\" moment condition that permits us to study\nestimators with both finite-dimensional and high-dimensional forms of covariate\nadjustment. We find that estimators with finite-dimensional, linear adjustments\nneed not lead to improvements in precision relative to the unadjusted\ndifference-in-means estimator. This phenomenon persists even if the adjustments\nare interacted with treatment; in fact, doing so leads to no changes in\nprecision. However, gains in precision can be ensured by including fixed\neffects for each of the pairs. Indeed, we show that this adjustment is the\n\"optimal\" finite-dimensional, linear adjustment. We additionally study two\nestimators with high-dimensional forms of covariate adjustment based on the\nLASSO. For each such estimator, we show that it leads to improvements in\nprecision relative to the unadjusted difference-in-means estimator and also\nprovide conditions under which it leads to the \"optimal\" nonparametric,\ncovariate adjustment. A simulation study confirms the practical relevance of\nour theoretical analysis, and the methods are employed to reanalyze data from\nan experiment using a \"matched pairs\" design to study the effect of\nmacroinsurance on microenterprise."}, "http://arxiv.org/abs/2305.03134": {"title": "Debiased inference for dynamic nonlinear models with two-way fixed effects", "link": "http://arxiv.org/abs/2305.03134", "description": "Panel data models often use fixed effects to account for unobserved\nheterogeneities. These fixed effects are typically incidental parameters and\ntheir estimators converge slowly relative to the square root of the sample\nsize. In the maximum likelihood context, this induces an asymptotic bias of the\nlikelihood function. Test statistics derived from the asymptotically biased\nlikelihood, therefore, no longer follow their standard limiting distributions.\nThis causes severe distortions in test sizes. We consider a generic class of\ndynamic nonlinear models with two-way fixed effects and propose an analytical\nbias correction method for the likelihood function. We formally show that the\nlikelihood ratio, the Lagrange-multiplier, and the Wald test statistics derived\nfrom the corrected likelihood follow their standard asymptotic distributions. A\nbias-corrected estimator of the structural parameters can also be derived from\nthe corrected likelihood function. We evaluate the performance of our bias\ncorrection procedure through simulations and an empirical example."}, "http://arxiv.org/abs/2310.13240": {"title": "Transparency challenges in policy evaluation with causal machine learning -- improving usability and accountability", "link": "http://arxiv.org/abs/2310.13240", "description": "Causal machine learning tools are beginning to see use in real-world policy\nevaluation tasks to flexibly estimate treatment effects. One issue with these\nmethods is that the machine learning models used are generally black boxes,\ni.e., there is no globally interpretable way to understand how a model makes\nestimates. This is a clear problem in policy evaluation applications,\nparticularly in government, because it is difficult to understand whether such\nmodels are functioning in ways that are fair, based on the correct\ninterpretation of evidence and transparent enough to allow for accountability\nif things go wrong. However, there has been little discussion of transparency\nproblems in the causal machine learning literature and how these might be\novercome. This paper explores why transparency issues are a problem for causal\nmachine learning in public policy evaluation applications and considers ways\nthese problems might be addressed through explainable AI tools and by\nsimplifying models in line with interpretable AI principles. It then applies\nthese ideas to a case-study using a causal forest model to estimate conditional\naverage treatment effects for a hypothetical change in the school leaving age\nin Australia. It shows that existing tools for understanding black-box\npredictive models are poorly suited to causal machine learning and that\nsimplifying the model to make it interpretable leads to an unacceptable\nincrease in error (in this application). It concludes that new tools are needed\nto properly understand causal machine learning models and the algorithms that\nfit them."}, "http://arxiv.org/abs/2308.04276": {"title": "Causal Interpretation of Linear Social Interaction Models with Endogenous Networks", "link": "http://arxiv.org/abs/2308.04276", "description": "This study investigates the causal interpretation of linear social\ninteraction models in the presence of endogeneity in network formation under a\nheterogeneous treatment effects framework. We consider an experimental setting\nin which individuals are randomly assigned to treatments while no interventions\nare made for the network structure. We show that running a linear regression\nignoring network endogeneity is not problematic for estimating the average\ndirect treatment effect. However, it leads to sample selection bias and\nnegative-weights problem for the estimation of the average spillover effect. To\novercome these problems, we propose using potential peer treatment as an\ninstrumental variable (IV), which is automatically a valid IV for actual\nspillover exposure. Using this IV, we examine two IV-based estimands and\ndemonstrate that they have a local average treatment-effect-type causal\ninterpretation for the spillover effect."}, "http://arxiv.org/abs/2309.01889": {"title": "The Local Projection Residual Bootstrap for AR(1) Models", "link": "http://arxiv.org/abs/2309.01889", "description": "This paper proposes a local projection residual bootstrap method to construct\nconfidence intervals for impulse response coefficients of AR(1) models. Our\nbootstrap method is based on the local projection (LP) approach and a residual\nbootstrap procedure. We present theoretical results for our bootstrap method\nand proposed confidence intervals. First, we prove the uniform consistency of\nthe LP-residual bootstrap over a large class of AR(1) models that allow for a\nunit root. Then, we prove the asymptotic validity of our confidence intervals\nover the same class of AR(1) models. Finally, we show that the LP-residual\nbootstrap provides asymptotic refinements for confidence intervals on a\nrestricted class of AR(1) models relative to those required for the uniform\nconsistency of our bootstrap."}, "http://arxiv.org/abs/2310.13785": {"title": "Bayesian Estimation of Panel Models under Potentially Sparse Heterogeneity", "link": "http://arxiv.org/abs/2310.13785", "description": "We incorporate a version of a spike and slab prior, comprising a pointmass at\nzero (\"spike\") and a Normal distribution around zero (\"slab\") into a dynamic\npanel data framework to model coefficient heterogeneity. In addition to\nhomogeneity and full heterogeneity, our specification can also capture sparse\nheterogeneity, that is, there is a core group of units that share common\nparameters and a set of deviators with idiosyncratic parameters. We fit a model\nwith unobserved components to income data from the Panel Study of Income\nDynamics. We find evidence for sparse heterogeneity for balanced panels\ncomposed of individuals with long employment histories."}, "http://arxiv.org/abs/2310.14068": {"title": "Unobserved Grouped Heteroskedasticity and Fixed Effects", "link": "http://arxiv.org/abs/2310.14068", "description": "This paper extends the linear grouped fixed effects (GFE) panel model to\nallow for heteroskedasticity from a discrete latent group variable. Key\nfeatures of GFE are preserved, such as individuals belonging to one of a finite\nnumber of groups and group membership is unrestricted and estimated. Ignoring\ngroup heteroskedasticity may lead to poor classification, which is detrimental\nto finite sample bias and standard errors of estimators. I introduce the\n\"weighted grouped fixed effects\" (WGFE) estimator that minimizes a weighted\naverage of group sum of squared residuals. I establish $\\sqrt{NT}$-consistency\nand normality under a concept of group separation based on second moments. A\ntest of group homoskedasticity is discussed. A fast computation procedure is\nprovided. Simulations show that WGFE outperforms alternatives that exclude\nsecond moment information. I demonstrate this approach by considering the link\nbetween income and democracy and the effect of unionization on earnings."}, "http://arxiv.org/abs/2310.14142": {"title": "On propensity score matching with a diverging number of matches", "link": "http://arxiv.org/abs/2310.14142", "description": "This paper reexamines Abadie and Imbens (2016)'s work on propensity score\nmatching for average treatment effect estimation. We explore the asymptotic\nbehavior of these estimators when the number of nearest neighbors, $M$, grows\nwith the sample size. It is shown, hardly surprising but technically\nnontrivial, that the modified estimators can improve upon the original\nfixed-$M$ estimators in terms of efficiency. Additionally, we demonstrate the\npotential to attain the semiparametric efficiency lower bound when the\npropensity score achieves \"sufficient\" dimension reduction, echoing Hahn\n(1998)'s insight about the role of dimension reduction in propensity\nscore-based causal inference."}, "http://arxiv.org/abs/2310.14438": {"title": "BVARs and Stochastic Volatility", "link": "http://arxiv.org/abs/2310.14438", "description": "Bayesian vector autoregressions (BVARs) are the workhorse in macroeconomic\nforecasting. Research in the last decade has established the importance of\nallowing time-varying volatility to capture both secular and cyclical\nvariations in macroeconomic uncertainty. This recognition, together with the\ngrowing availability of large datasets, has propelled a surge in recent\nresearch in building stochastic volatility models suitable for large BVARs.\nSome of these new models are also equipped with additional features that are\nespecially desirable for large systems, such as order invariance -- i.e.,\nestimates are not dependent on how the variables are ordered in the BVAR -- and\nrobustness against COVID-19 outliers. Estimation of these large, flexible\nmodels is made possible by the recently developed equation-by-equation approach\nthat drastically reduces the computational cost of estimating large systems.\nDespite these recent advances, there remains much ongoing work, such as the\ndevelopment of parsimonious approaches for time-varying coefficients and other\ntypes of nonlinearities in large BVARs."}, "http://arxiv.org/abs/2310.14983": {"title": "Causal clustering: design of cluster experiments under network interference", "link": "http://arxiv.org/abs/2310.14983", "description": "This paper studies the design of cluster experiments to estimate the global\ntreatment effect in the presence of spillovers on a single network. We provide\nan econometric framework to choose the clustering that minimizes the worst-case\nmean-squared error of the estimated global treatment effect. We show that the\noptimal clustering can be approximated as the solution of a novel penalized\nmin-cut optimization problem computed via off-the-shelf semi-definite\nprogramming algorithms. Our analysis also characterizes easy-to-check\nconditions to choose between a cluster or individual-level randomization. We\nillustrate the method's properties using unique network data from the universe\nof Facebook's users and existing network data from a field experiment."}, "http://arxiv.org/abs/2004.08318": {"title": "Causal Inference under Outcome-Based Sampling with Monotonicity Assumptions", "link": "http://arxiv.org/abs/2004.08318", "description": "We study causal inference under case-control and case-population sampling.\nSpecifically, we focus on the binary-outcome and binary-treatment case, where\nthe parameters of interest are causal relative and attributable risks defined\nvia the potential outcome framework. It is shown that strong ignorability is\nnot always as powerful as it is under random sampling and that certain\nmonotonicity assumptions yield comparable results in terms of sharp identified\nintervals. Specifically, the usual odds ratio is shown to be a sharp identified\nupper bound on causal relative risk under the monotone treatment response and\nmonotone treatment selection assumptions. We offer algorithms for inference on\nthe causal parameters that are aggregated over the true population distribution\nof the covariates. We show the usefulness of our approach by studying three\nempirical examples: the benefit of attending private school for entering a\nprestigious university in Pakistan; the relationship between staying in school\nand getting involved with drug-trafficking gangs in Brazil; and the link\nbetween physicians' hours and size of the group practice in the United States."}, "http://arxiv.org/abs/2108.07455": {"title": "Causal Inference with Noncompliance and Unknown Interference", "link": "http://arxiv.org/abs/2108.07455", "description": "We consider a causal inference model in which individuals interact in a\nsocial network and they may not comply with the assigned treatments. In\nparticular, we suppose that the form of network interference is unknown to\nresearchers. To estimate meaningful causal parameters in this situation, we\nintroduce a new concept of exposure mapping, which summarizes potentially\ncomplicated spillover effects into a fixed dimensional statistic of\ninstrumental variables. We investigate identification conditions for the\nintention-to-treat effects and the average treatment effects for compliers,\nwhile explicitly considering the possibility of misspecification of exposure\nmapping. Based on our identification results, we develop nonparametric\nestimation procedures via inverse probability weighting. Their asymptotic\nproperties, including consistency and asymptotic normality, are investigated\nusing an approximate neighborhood interference framework. For an empirical\nillustration, we apply our method to experimental data on the anti-conflict\nintervention school program. The proposed methods are readily available with\nthe companion R package latenetwork."}, "http://arxiv.org/abs/2112.03872": {"title": "Nonparametric Treatment Effect Identification in School Choice", "link": "http://arxiv.org/abs/2112.03872", "description": "This paper studies nonparametric identification and estimation of causal\neffects in centralized school assignment. In many centralized assignment\nsettings, students are subjected to both lottery-driven variation and\nregression discontinuity (RD) driven variation. We characterize the full set of\nidentified atomic treatment effects (aTEs), defined as the conditional average\ntreatment effect between a pair of schools, given student characteristics.\nAtomic treatment effects are the building blocks of more aggregated notions of\ntreatment contrasts, and common approaches estimating aggregations of aTEs can\nmask important heterogeneity. In particular, many aggregations of aTEs put zero\nweight on aTEs driven by RD variation, and estimators of such aggregations put\nasymptotically vanishing weight on the RD-driven aTEs. We develop a diagnostic\ntool for empirically assessing the weight put on aTEs driven by RD variation.\nLastly, we provide estimators and accompanying asymptotic results for inference\non aggregations of RD-driven aTEs."}, "http://arxiv.org/abs/2203.01425": {"title": "A Modern Gauss-Markov Theorem? Really?", "link": "http://arxiv.org/abs/2203.01425", "description": "We show that the theorems in Hansen (2021a) (the version accepted by\nEconometrica), except for one, are not new as they coincide with classical\ntheorems like the good old Gauss-Markov or Aitken Theorem, respectively; the\nexceptional theorem is incorrect. Hansen (2021b) corrects this theorem. As a\nresult, all theorems in the latter version coincide with the above mentioned\nclassical theorems. Furthermore, we also show that the theorems in Hansen\n(2022) (the version published in Econometrica) either coincide with the\nclassical theorems just mentioned, or contain extra assumptions that are alien\nto the Gauss-Markov or Aitken Theorem."}, "http://arxiv.org/abs/2204.12723": {"title": "Information-theoretic limitations of data-based price discrimination", "link": "http://arxiv.org/abs/2204.12723", "description": "This paper studies third-degree price discrimination (3PD) based on a random\nsample of valuation and covariate data, where the covariate is continuous, and\nthe distribution of the data is unknown to the seller. The main results of this\npaper are twofold. The first set of results is pricing strategy independent and\nreveals the fundamental information-theoretic limitation of any data-based\npricing strategy in revenue generation for two cases: 3PD and uniform pricing.\nThe second set of results proposes the $K$-markets empirical revenue\nmaximization (ERM) strategy and shows that the $K$-markets ERM and the uniform\nERM strategies achieve the optimal rate of convergence in revenue to that\ngenerated by their respective true-distribution 3PD and uniform pricing optima.\nOur theoretical and numerical results suggest that the uniform (i.e.,\n$1$-market) ERM strategy generates a larger revenue than the $K$-markets ERM\nstrategy when the sample size is small enough, and vice versa."}, "http://arxiv.org/abs/2304.12698": {"title": "Enhanced multilayer perceptron with feature selection and grid search for travel mode choice prediction", "link": "http://arxiv.org/abs/2304.12698", "description": "Accurate and reliable prediction of individual travel mode choices is crucial\nfor developing multi-mode urban transportation systems, conducting\ntransportation planning and formulating traffic demand management strategies.\nTraditional discrete choice models have dominated the modelling methods for\ndecades yet suffer from strict model assumptions and low prediction accuracy.\nIn recent years, machine learning (ML) models, such as neural networks and\nboosting models, are widely used by researchers for travel mode choice\nprediction and have yielded promising results. However, despite the superior\nprediction performance, a large body of ML methods, especially the branch of\nneural network models, is also limited by overfitting and tedious model\nstructure determination process. To bridge this gap, this study proposes an\nenhanced multilayer perceptron (MLP; a neural network) with two hidden layers\nfor travel mode choice prediction; this MLP is enhanced by XGBoost (a boosting\nmethod) for feature selection and a grid search method for optimal hidden\nneurone determination of each hidden layer. The proposed method was trained and\ntested on a real resident travel diary dataset collected in Chengdu, China."}, "http://arxiv.org/abs/2306.02584": {"title": "Synthetic Regressing Control Method", "link": "http://arxiv.org/abs/2306.02584", "description": "Estimating weights in the synthetic control method, typically resulting in\nsparse weights where only a few control units have non-zero weights, involves\nan optimization procedure that simultaneously selects and aligns control units\nto closely match the treated unit. However, this simultaneous selection and\nalignment of control units may lead to a loss of efficiency. Another concern\narising from the aforementioned procedure is its susceptibility to\nunder-fitting due to imperfect pre-treatment fit. It is not uncommon for the\nlinear combination, using nonnegative weights, of pre-treatment period outcomes\nfor the control units to inadequately approximate the pre-treatment outcomes\nfor the treated unit. To address both of these issues, this paper proposes a\nsimple and effective method called Synthetic Regressing Control (SRC). The SRC\nmethod begins by performing the univariate linear regression to appropriately\nalign the pre-treatment periods of the control units with the treated unit.\nSubsequently, a SRC estimator is obtained by synthesizing (taking a weighted\naverage) the fitted controls. To determine the weights in the synthesis\nprocedure, we propose an approach that utilizes a criterion of unbiased risk\nestimator. Theoretically, we show that the synthesis way is asymptotically\noptimal in the sense of achieving the lowest possible squared error. Extensive\nnumerical experiments highlight the advantages of the SRC method."}, "http://arxiv.org/abs/2308.12470": {"title": "Scalable Estimation of Multinomial Response Models with Uncertain Consideration Sets", "link": "http://arxiv.org/abs/2308.12470", "description": "A standard assumption in the fitting of unordered multinomial response models\nfor $J$ mutually exclusive nominal categories, on cross-sectional or\nlongitudinal data, is that the responses arise from the same set of $J$\ncategories between subjects. However, when responses measure a choice made by\nthe subject, it is more appropriate to assume that the distribution of\nmultinomial responses is conditioned on a subject-specific consideration set,\nwhere this consideration set is drawn from the power set of $\\{1,2,\\ldots,J\\}$.\nBecause the cardinality of this power set is exponential in $J$, estimation is\ninfeasible in general. In this paper, we provide an approach to overcoming this\nproblem. A key step in the approach is a probability model over consideration\nsets, based on a general representation of probability distributions on\ncontingency tables, which results in mixtures of independent consideration\nmodels. Although the support of this distribution is exponentially large, the\nposterior distribution over consideration sets given parameters is typically\nsparse, and is easily sampled in an MCMC scheme. We show posterior consistency\nof the parameters of the conditional response model and the distribution of\nconsideration sets. The effectiveness of the methodology is documented in\nsimulated longitudinal data sets with $J=100$ categories and real data from the\ncereal market with $J=68$ brands."}, "http://arxiv.org/abs/2310.15512": {"title": "Inference for Rank-Rank Regressions", "link": "http://arxiv.org/abs/2310.15512", "description": "Slope coefficients in rank-rank regressions are popular measures of\nintergenerational mobility, for instance in regressions of a child's income\nrank on their parent's income rank. In this paper, we first point out that\ncommonly used variance estimators such as the homoskedastic or robust variance\nestimators do not consistently estimate the asymptotic variance of the OLS\nestimator in a rank-rank regression. We show that the probability limits of\nthese estimators may be too large or too small depending on the shape of the\ncopula of child and parent incomes. Second, we derive a general asymptotic\ntheory for rank-rank regressions and provide a consistent estimator of the OLS\nestimator's asymptotic variance. We then extend the asymptotic theory to other\nregressions involving ranks that have been used in empirical work. Finally, we\napply our new inference methods to three empirical studies. We find that the\nconfidence intervals based on estimators of the correct variance may sometimes\nbe substantially shorter and sometimes substantially longer than those based on\ncommonly used variance estimators. The differences in confidence intervals\nconcern economically meaningful values of mobility and thus lead to different\nconclusions when comparing mobility in U.S. commuting zones with mobility in\nother countries."}, "http://arxiv.org/abs/2310.15796": {"title": "Testing for equivalence of pre-trends in Difference-in-Differences estimation", "link": "http://arxiv.org/abs/2310.15796", "description": "The plausibility of the ``parallel trends assumption'' in\nDifference-in-Differences estimation is usually assessed by a test of the null\nhypothesis that the difference between the average outcomes of both groups is\nconstant over time before the treatment. However, failure to reject the null\nhypothesis does not imply the absence of differences in time trends between\nboth groups. We provide equivalence tests that allow researchers to find\nevidence in favor of the parallel trends assumption and thus increase the\ncredibility of their treatment effect estimates. While we motivate our tests in\nthe standard two-way fixed effects model, we discuss simple extensions to\nsettings in which treatment adoption is staggered over time."}, "http://arxiv.org/abs/1712.04802": {"title": "Fisher-Schultz Lecture: Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments, with an Application to Immunization in India", "link": "http://arxiv.org/abs/1712.04802", "description": "We propose strategies to estimate and make inference on key features of\nheterogeneous effects in randomized experiments. These key features include\nbest linear predictors of the effects using machine learning proxies, average\neffects sorted by impact groups, and average characteristics of most and least\nimpacted units. The approach is valid in high dimensional settings, where the\neffects are proxied (but not necessarily consistently estimated) by predictive\nand causal machine learning methods. We post-process these proxies into\nestimates of the key features. Our approach is generic, it can be used in\nconjunction with penalized methods, neural networks, random forests, boosted\ntrees, and ensemble methods, both predictive and causal. Estimation and\ninference are based on repeated data splitting to avoid overfitting and achieve\nvalidity. We use quantile aggregation of the results across many potential\nsplits, in particular taking medians of p-values and medians and other\nquantiles of confidence intervals. We show that quantile aggregation lowers\nestimation risks over a single split procedure, and establish its principal\ninferential properties. Finally, our analysis reveals ways to build provably\nbetter machine learning proxies through causal learning: we can use the\nobjective functions that we develop to construct the best linear predictors of\nthe effects, to obtain better machine learning proxies in the initial step. We\nillustrate the use of both inferential tools and causal learners with a\nrandomized field experiment that evaluates a combination of nudges to stimulate\ndemand for immunization in India."}, "http://arxiv.org/abs/2301.09016": {"title": "Inference for Two-stage Experiments under Covariate-Adaptive Randomization", "link": "http://arxiv.org/abs/2301.09016", "description": "This paper studies inference in two-stage randomized experiments under\ncovariate-adaptive randomization. In the initial stage of this experimental\ndesign, clusters (e.g., households, schools, or graph partitions) are\nstratified and randomly assigned to control or treatment groups based on\ncluster-level covariates. Subsequently, an independent second-stage design is\ncarried out, wherein units within each treated cluster are further stratified\nand randomly assigned to either control or treatment groups, based on\nindividual-level covariates. Under the homogeneous partial interference\nassumption, I establish conditions under which the proposed\ndifference-in-\"average of averages\" estimators are consistent and\nasymptotically normal for the corresponding average primary and spillover\neffects and develop consistent estimators of their asymptotic variances.\nCombining these results establishes the asymptotic validity of tests based on\nthese estimators. My findings suggest that ignoring covariate information in\nthe design stage can result in efficiency loss, and commonly used inference\nmethods that ignore or improperly use covariate information can lead to either\nconservative or invalid inference. Finally, I apply these results to studying\noptimal use of covariate information under covariate-adaptive randomization in\nlarge samples, and demonstrate that a specific generalized matched-pair design\nachieves minimum asymptotic variance for each proposed estimator. The practical\nrelevance of the theoretical results is illustrated through a simulation study\nand an empirical application."}, "http://arxiv.org/abs/2306.12003": {"title": "Difference-in-Differences with Interference: A Finite Population Perspective", "link": "http://arxiv.org/abs/2306.12003", "description": "In many scenarios, such as the evaluation of place-based policies, potential\noutcomes are not only dependent upon the unit's own treatment but also its\nneighbors' treatment. Despite this, \"difference-in-differences\" (DID) type\nestimators typically ignore such interference among neighbors. I show in this\npaper that the canonical DID estimators generally fail to identify interesting\ncausal effects in the presence of neighborhood interference. To incorporate\ninterference structure into DID estimation, I propose doubly robust estimators\nfor the direct average treatment effect on the treated as well as the average\nspillover effects under a modified parallel trends assumption. When spillover\neffects are of interest, we often sample the entire population. Thus, I adopt a\nfinite population perspective in the sense that the estimands are defined as\npopulation averages and inference is conditional on the attributes of all\npopulation units. The approach in this paper relaxes common restrictions in the\nliterature, such as partial interference and correctly specified spillover\nfunctions. Moreover, robust inference is discussed based on the asymptotic\ndistribution of the proposed estimators."}, "http://arxiv.org/abs/2310.16281": {"title": "Improving Robust Decisions with Data", "link": "http://arxiv.org/abs/2310.16281", "description": "A decision-maker (DM) faces uncertainty governed by a data-generating process\n(DGP), which is only known to belong to a set of sequences of independent but\npossibly non-identical distributions. A robust decision maximizes the DM's\nexpected payoff against the worst possible DGP in this set. This paper studies\nhow such robust decisions can be improved with data, where improvement is\nmeasured by expected payoff under the true DGP. In this paper, I fully\ncharacterize when and how such an improvement can be guaranteed under all\npossible DGPs and develop inference methods to achieve it. These inference\nmethods are needed because, as this paper shows, common inference methods\n(e.g., maximum likelihood or Bayesian) often fail to deliver such an\nimprovement. Importantly, the developed inference methods are given by simple\naugmentations to standard inference procedures, and are thus easy to implement\nin practice."}, "http://arxiv.org/abs/2310.16290": {"title": "Fair Adaptive Experiments", "link": "http://arxiv.org/abs/2310.16290", "description": "Randomized experiments have been the gold standard for assessing the\neffectiveness of a treatment or policy. The classical complete randomization\napproach assigns treatments based on a prespecified probability and may lead to\ninefficient use of data. Adaptive experiments improve upon complete\nrandomization by sequentially learning and updating treatment assignment\nprobabilities. However, their application can also raise fairness and equity\nconcerns, as assignment probabilities may vary drastically across groups of\nparticipants. Furthermore, when treatment is expected to be extremely\nbeneficial to certain groups of participants, it is more appropriate to expose\nmany of these participants to favorable treatment. In response to these\nchallenges, we propose a fair adaptive experiment strategy that simultaneously\nenhances data use efficiency, achieves an envy-free treatment assignment\nguarantee, and improves the overall welfare of participants. An important\nfeature of our proposed strategy is that we do not impose parametric modeling\nassumptions on the outcome variables, making it more versatile and applicable\nto a wider array of applications. Through our theoretical investigation, we\ncharacterize the convergence rate of the estimated treatment effects and the\nassociated standard deviations at the group level and further prove that our\nadaptive treatment assignment algorithm, despite not having a closed-form\nexpression, approaches the optimal allocation rule asymptotically. Our proof\nstrategy takes into account the fact that the allocation decisions in our\ndesign depend on sequentially accumulated data, which poses a significant\nchallenge in characterizing the properties and conducting statistical inference\nof our method. We further provide simulation evidence to showcase the\nperformance of our fair adaptive experiment strategy."}, "http://arxiv.org/abs/2310.16638": {"title": "Covariate Shift Adaptation Robust to Density-Ratio Estimation", "link": "http://arxiv.org/abs/2310.16638", "description": "Consider a scenario where we have access to train data with both covariates\nand outcomes while test data only contains covariates. In this scenario, our\nprimary aim is to predict the missing outcomes of the test data. With this\nobjective in mind, we train parametric regression models under a covariate\nshift, where covariate distributions are different between the train and test\ndata. For this problem, existing studies have proposed covariate shift\nadaptation via importance weighting using the density ratio. This approach\naverages the train data losses, each weighted by an estimated ratio of the\ncovariate densities between the train and test data, to approximate the\ntest-data risk. Although it allows us to obtain a test-data risk minimizer, its\nperformance heavily relies on the accuracy of the density ratio estimation.\nMoreover, even if the density ratio can be consistently estimated, the\nestimation errors of the density ratio also yield bias in the estimators of the\nregression model's parameters of interest. To mitigate these challenges, we\nintroduce a doubly robust estimator for covariate shift adaptation via\nimportance weighting, which incorporates an additional estimator for the\nregression function. Leveraging double machine learning techniques, our\nestimator reduces the bias arising from the density ratio estimation errors. We\ndemonstrate the asymptotic distribution of the regression parameter estimator.\nNotably, our estimator remains consistent if either the density ratio estimator\nor the regression function is consistent, showcasing its robustness against\npotential errors in density ratio estimation. Finally, we confirm the soundness\nof our proposed method via simulation studies."}, "http://arxiv.org/abs/2310.16819": {"title": "CATE Lasso: Conditional Average Treatment Effect Estimation with High-Dimensional Linear Regression", "link": "http://arxiv.org/abs/2310.16819", "description": "In causal inference about two treatments, Conditional Average Treatment\nEffects (CATEs) play an important role as a quantity representing an\nindividualized causal effect, defined as a difference between the expected\noutcomes of the two treatments conditioned on covariates. This study assumes\ntwo linear regression models between a potential outcome and covariates of the\ntwo treatments and defines CATEs as a difference between the linear regression\nmodels. Then, we propose a method for consistently estimating CATEs even under\nhigh-dimensional and non-sparse parameters. In our study, we demonstrate that\ndesirable theoretical properties, such as consistency, remain attainable even\nwithout assuming sparsity explicitly if we assume a weaker assumption called\nimplicit sparsity originating from the definition of CATEs. In this assumption,\nwe suppose that parameters of linear models in potential outcomes can be\ndivided into treatment-specific and common parameters, where the\ntreatment-specific parameters take difference values between each linear\nregression model, while the common parameters remain identical. Thus, in a\ndifference between two linear regression models, the common parameters\ndisappear, leaving only differences in the treatment-specific parameters.\nConsequently, the non-zero parameters in CATEs correspond to the differences in\nthe treatment-specific parameters. Leveraging this assumption, we develop a\nLasso regression method specialized for CATE estimation and present that the\nestimator is consistent. Finally, we confirm the soundness of the proposed\nmethod by simulation studies."}, "http://arxiv.org/abs/2203.06685": {"title": "Encompassing Tests for Nonparametric Regressions", "link": "http://arxiv.org/abs/2203.06685", "description": "We set up a formal framework to characterize encompassing of nonparametric\nmodels through the L2 distance. We contrast it to previous literature on the\ncomparison of nonparametric regression models. We then develop testing\nprocedures for the encompassing hypothesis that are fully nonparametric. Our\ntest statistics depend on kernel regression, raising the issue of bandwidth's\nchoice. We investigate two alternative approaches to obtain a \"small bias\nproperty\" for our test statistics. We show the validity of a wild bootstrap\nmethod. We empirically study the use of a data-driven bandwidth and illustrate\nthe attractive features of our tests for small and moderate samples."}, "http://arxiv.org/abs/2212.11012": {"title": "Partly Linear Instrumental Variables Regressions without Smoothing on the Instruments", "link": "http://arxiv.org/abs/2212.11012", "description": "We consider a semiparametric partly linear model identified by instrumental\nvariables. We propose an estimation method that does not smooth on the\ninstruments and we extend the Landweber-Fridman regularization scheme to the\nestimation of this semiparametric model. We then show the asymptotic normality\nof the parametric estimator and obtain the convergence rate for the\nnonparametric estimator. Our estimator that does not smooth on the instruments\ncoincides with a typical estimator that does smooth on the instruments but\nkeeps the respective bandwidth fixed as the sample size increases. We propose a\ndata driven method for the selection of the regularization parameter, and in a\nsimulation study we show the attractive performance of our estimators."}, "http://arxiv.org/abs/2212.11112": {"title": "A Bootstrap Specification Test for Semiparametric Models with Generated Regressors", "link": "http://arxiv.org/abs/2212.11112", "description": "This paper provides a specification test for semiparametric models with\nnonparametrically generated regressors. Such variables are not observed by the\nresearcher but are nonparametrically identified and estimable. Applications of\nthe test include models with endogenous regressors identified by control\nfunctions, semiparametric sample selection models, or binary games with\nincomplete information. The statistic is built from the residuals of the\nsemiparametric model. A novel wild bootstrap procedure is shown to provide\nvalid critical values. We consider nonparametric estimators with an automatic\nbias correction that makes the test implementable without undersmoothing. In\nsimulations the test exhibits good small sample performances, and an\napplication to women's labor force participation decisions shows its\nimplementation in a real data context."}, "http://arxiv.org/abs/2305.07993": {"title": "The Nonstationary Newsvendor with (and without) Predictions", "link": "http://arxiv.org/abs/2305.07993", "description": "The classic newsvendor model yields an optimal decision for a \"newsvendor\"\nselecting a quantity of inventory, under the assumption that the demand is\ndrawn from a known distribution. Motivated by applications such as cloud\nprovisioning and staffing, we consider a setting in which newsvendor-type\ndecisions must be made sequentially, in the face of demand drawn from a\nstochastic process that is both unknown and nonstationary. All prior work on\nthis problem either (a) assumes that the level of nonstationarity is known, or\n(b) imposes additional statistical assumptions that enable accurate predictions\nof the unknown demand.\n\nWe study the Nonstationary Newsvendor, with and without predictions. We\nfirst, in the setting without predictions, design a policy which we prove (via\nmatching upper and lower bounds) achieves order-optimal regret -- ours is the\nfirst policy to accomplish this without being given the level of\nnonstationarity of the underlying demand. We then, for the first time,\nintroduce a model for generic (i.e. with no statistical assumptions)\npredictions with arbitrary accuracy, and propose a policy that incorporates\nthese predictions without being given their accuracy. We upper bound the regret\nof this policy, and show that it matches the best achievable regret had the\naccuracy of the predictions been known. Finally, we empirically validate our\nnew policy with experiments based on two real-world datasets containing\nthousands of time-series, showing that it succeeds in closing approximately 74%\nof the gap between the best approaches based on nonstationarity and predictions\nalone."}, "http://arxiv.org/abs/2310.16849": {"title": "Correlation structure analysis of the global agricultural futures market", "link": "http://arxiv.org/abs/2310.16849", "description": "This paper adopts the random matrix theory (RMT) to analyze the correlation\nstructure of the global agricultural futures market from 2000 to 2020. It is\nfound that the distribution of correlation coefficients is asymmetric and right\nskewed, and many eigenvalues of the correlation matrix deviate from the RMT\nprediction. The largest eigenvalue reflects a collective market effect common\nto all agricultural futures, the other largest deviating eigenvalues can be\nimplemented to identify futures groups, and there are modular structures based\non regional properties or agricultural commodities among the significant\nparticipants of their corresponding eigenvectors. Except for the smallest\neigenvalue, other smallest deviating eigenvalues represent the agricultural\nfutures pairs with highest correlations. This paper can be of reference and\nsignificance for using agricultural futures to manage risk and optimize asset\nallocation."}, "http://arxiv.org/abs/2310.16850": {"title": "The impact of the Russia-Ukraine conflict on the extreme risk spillovers between agricultural futures and spots", "link": "http://arxiv.org/abs/2310.16850", "description": "The ongoing Russia-Ukraine conflict between two major agricultural powers has\nposed significant threats and challenges to the global food system and world\nfood security. Focusing on the impact of the conflict on the global\nagricultural market, we propose a new analytical framework for tail dependence,\nand combine the Copula-CoVaR method with the ARMA-GARCH-skewed Student-t model\nto examine the tail dependence structure and extreme risk spillover between\nagricultural futures and spots over the pre- and post-outbreak periods. Our\nresults indicate that the tail dependence structures in the futures-spot\nmarkets of soybean, maize, wheat, and rice have all reacted to the\nRussia-Ukraine conflict. Furthermore, the outbreak of the conflict has\nintensified risks of the four agricultural markets in varying degrees, with the\nwheat market being affected the most. Additionally, all the agricultural\nfutures markets exhibit significant downside and upside risk spillovers to\ntheir corresponding spot markets before and after the outbreak of the conflict,\nwhereas the strengths of these extreme risk spillover effects demonstrate\nsignificant asymmetries at the directional (downside versus upside) and\ntemporal (pre-outbreak versus post-outbreak) levels."}, "http://arxiv.org/abs/2310.17278": {"title": "Dynamic Factor Models: a Genealogy", "link": "http://arxiv.org/abs/2310.17278", "description": "Dynamic factor models have been developed out of the need of analyzing and\nforecasting time series in increasingly high dimensions. While mathematical\nstatisticians faced with inference problems in high-dimensional observation\nspaces were focusing on the so-called spiked-model-asymptotics, econometricians\nadopted an entirely and considerably more effective asymptotic approach, rooted\nin the factor models originally considered in psychometrics. The so-called\ndynamic factor model methods, in two decades, has grown into a wide and\nsuccessful body of techniques that are widely used in central banks, financial\ninstitutions, economic and statistical institutes. The objective of this\nchapter is not an extensive survey of the topic but a sketch of its historical\ngrowth, with emphasis on the various assumptions and interpretations, and a\nfamily tree of its main variants."}, "http://arxiv.org/abs/2310.17473": {"title": "Bayesian SAR model with stochastic volatility and multiple time-varying weights", "link": "http://arxiv.org/abs/2310.17473", "description": "A novel spatial autoregressive model for panel data is introduced, which\nincorporates multilayer networks and accounts for time-varying relationships.\nMoreover, the proposed approach allows the structural variance to evolve\nsmoothly over time and enables the analysis of shock propagation in terms of\ntime-varying spillover effects. The framework is applied to analyse the\ndynamics of international relationships among the G7 economies and their impact\non stock market returns and volatilities. The findings underscore the\nsubstantial impact of cooperative interactions and highlight discernible\ndisparities in network exposure across G7 nations, along with nuanced patterns\nin direct and indirect spillover effects."}, "http://arxiv.org/abs/2310.17496": {"title": "Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach", "link": "http://arxiv.org/abs/2310.17496", "description": "In modern recommendation systems, the standard pipeline involves training\nmachine learning models on historical data to predict user behaviors and\nimprove recommendations continuously. However, these data training loops can\nintroduce interference in A/B tests, where data generated by control and\ntreatment algorithms, potentially with different distributions, are combined.\nTo address these challenges, we introduce a novel approach called weighted\ntraining. This approach entails training a model to predict the probability of\neach data point appearing in either the treatment or control data and\nsubsequently applying weighted losses during model training. We demonstrate\nthat this approach achieves the least variance among all estimators without\ncausing shifts in the training distributions. Through simulation studies, we\ndemonstrate the lower bias and variance of our approach compared to other\nmethods."}, "http://arxiv.org/abs/2310.17571": {"title": "Inside the black box: Neural network-based real-time prediction of US recessions", "link": "http://arxiv.org/abs/2310.17571", "description": "Feedforward neural network (FFN) and two specific types of recurrent neural\nnetwork, long short-term memory (LSTM) and gated recurrent unit (GRU), are used\nfor modeling US recessions in the period from 1967 to 2021. The estimated\nmodels are then employed to conduct real-time predictions of the Great\nRecession and the Covid-19 recession in US. Their predictive performances are\ncompared to those of the traditional linear models, the logistic regression\nmodel both with and without the ridge penalty. The out-of-sample performance\nsuggests the application of LSTM and GRU in the area of recession forecasting,\nespecially for the long-term forecasting tasks. They outperform other types of\nmodels across 5 forecasting horizons with respect to different types of\nstatistical performance metrics. Shapley additive explanations (SHAP) method is\napplied to the fitted GRUs across different forecasting horizons to gain\ninsight into the feature importance. The evaluation of predictor importance\ndiffers between the GRU and ridge logistic regression models, as reflected in\nthe variable order determined by SHAP values. When considering the top 5\npredictors, key indicators such as the S\\&amp;P 500 index, real GDP, and private\nresidential fixed investment consistently appear for short-term forecasts (up\nto 3 months). In contrast, for longer-term predictions (6 months or more), the\nterm spread and producer price index become more prominent. These findings are\nsupported by both local interpretable model-agnostic explanations (LIME) and\nmarginal effects."}, "http://arxiv.org/abs/2205.07836": {"title": "2SLS with Multiple Treatments", "link": "http://arxiv.org/abs/2205.07836", "description": "We study what two-stage least squares (2SLS) identifies in models with\nmultiple treatments under treatment effect heterogeneity. Two conditions are\nshown to be necessary and sufficient for the 2SLS to identify positively\nweighted sums of agent-specific effects of each treatment: average conditional\nmonotonicity and no cross effects. Our identification analysis allows for any\nnumber of treatments, any number of continuous or discrete instruments, and the\ninclusion of covariates. We provide testable implications and present\ncharacterizations of choice behavior implied by our identification conditions\nand discuss how the conditions can be tested empirically."}, "http://arxiv.org/abs/2308.12485": {"title": "Optimal Shrinkage Estimation of Fixed Effects in Linear Panel Data Models", "link": "http://arxiv.org/abs/2308.12485", "description": "Shrinkage methods are frequently used to estimate fixed effects to reduce the\nnoisiness of the least squares estimators. However, widely used shrinkage\nestimators guarantee such noise reduction only under strong distributional\nassumptions. I develop an estimator for the fixed effects that obtains the best\npossible mean squared error within a class of shrinkage estimators. This class\nincludes conventional shrinkage estimators and the optimality does not require\ndistributional assumptions. The estimator has an intuitive form and is easy to\nimplement. Moreover, the fixed effects are allowed to vary with time and to be\nserially correlated, and the shrinkage optimally incorporates the underlying\ncorrelation structure in this case. In such a context, I also provide a method\nto forecast fixed effects one period ahead."}, "http://arxiv.org/abs/2310.18504": {"title": "Nonparametric Doubly Robust Identification of Causal Effects of a Continuous Treatment using Discrete Instruments", "link": "http://arxiv.org/abs/2310.18504", "description": "Many empirical applications estimate causal effects of a continuous\nendogenous variable (treatment) using a binary instrument. Estimation is\ntypically done through linear 2SLS. This approach requires a mean treatment\nchange and causal interpretation requires the LATE-type monotonicity in the\nfirst stage. An alternative approach is to explore distributional changes in\nthe treatment, where the first-stage restriction is treatment rank similarity.\nWe propose causal estimands that are doubly robust in that they are valid under\neither of these two restrictions. We apply the doubly robust estimation to\nestimate the impacts of sleep on well-being. Our results corroborate the usual\n2SLS estimates."}, "http://arxiv.org/abs/2310.18563": {"title": "Covariate Balancing and the Equivalence of Weighting and Doubly Robust Estimators of Average Treatment Effects", "link": "http://arxiv.org/abs/2310.18563", "description": "We show that when the propensity score is estimated using a suitable\ncovariate balancing procedure, the commonly used inverse probability weighting\n(IPW) estimator, augmented inverse probability weighting (AIPW) with linear\nconditional mean, and inverse probability weighted regression adjustment\n(IPWRA) with linear conditional mean are all numerically the same for\nestimating the average treatment effect (ATE) or the average treatment effect\non the treated (ATT). Further, suitably chosen covariate balancing weights are\nautomatically normalized, which means that normalized and unnormalized versions\nof IPW and AIPW are identical. For estimating the ATE, the weights that achieve\nthe algebraic equivalence of IPW, AIPW, and IPWRA are based on propensity\nscores estimated using the inverse probability tilting (IPT) method of Graham,\nPinto and Egel (2012). For the ATT, the weights are obtained using the\ncovariate balancing propensity score (CBPS) method developed in Imai and\nRatkovic (2014). These equivalences also make covariate balancing methods\nattractive when the treatment is confounded and one is interested in the local\naverage treatment effect."}, "http://arxiv.org/abs/2310.18836": {"title": "Design of Cluster-Randomized Trials with Cross-Cluster Interference", "link": "http://arxiv.org/abs/2310.18836", "description": "Cluster-randomized trials often involve units that are irregularly\ndistributed in space without well-separated communities. In these settings,\ncluster construction is a critical aspect of the design due to the potential\nfor cross-cluster interference. The existing literature relies on partial\ninterference models, which take clusters as given and assume no cross-cluster\ninterference. We relax this assumption by allowing interference to decay with\ngeographic distance between units. This induces a bias-variance trade-off:\nconstructing fewer, larger clusters reduces bias due to interference but\nincreases variance. We propose new estimators that exclude units most\npotentially impacted by cross-cluster interference and show that this\nsubstantially reduces asymptotic bias relative to conventional\ndifference-in-means estimators. We then study the design of clusters to\noptimize the estimators' rates of convergence. We provide formal justification\nfor a new design that chooses the number of clusters to balance the asymptotic\nbias and variance of our estimators and uses unsupervised learning to automate\ncluster construction."}, "http://arxiv.org/abs/2310.19200": {"title": "Popularity, face and voice: Predicting and interpreting livestreamers' retail performance using machine learning techniques", "link": "http://arxiv.org/abs/2310.19200", "description": "Livestreaming commerce, a hybrid of e-commerce and self-media, has expanded\nthe broad spectrum of traditional sales performance determinants. To\ninvestigate the factors that contribute to the success of livestreaming\ncommerce, we construct a longitudinal firm-level database with 19,175\nobservations, covering an entire livestreaming subsector. By comparing the\nforecasting accuracy of eight machine learning models, we identify a random\nforest model that provides the best prediction of gross merchandise volume\n(GMV). Furthermore, we utilize explainable artificial intelligence to open the\nblack-box of machine learning model, discovering four new facts: 1) variables\nrepresenting the popularity of livestreaming events are crucial features in\npredicting GMV. And voice attributes are more important than appearance; 2)\npopularity is a major determinant of sales for female hosts, while vocal\naesthetics is more decisive for their male counterparts; 3) merits and\ndrawbacks of the voice are not equally valued in the livestreaming market; 4)\nbased on changes of comments, page views and likes, sales growth can be divided\ninto three stages. Finally, we innovatively propose a 3D-SHAP diagram that\ndemonstrates the relationship between predicting feature importance, target\nvariable, and its predictors. This diagram identifies bottlenecks for both\nbeginner and top livestreamers, providing insights into ways to optimize their\nsales performance."}, "http://arxiv.org/abs/2310.19543": {"title": "Spectral identification and estimation of mixed causal-noncausal invertible-noninvertible models", "link": "http://arxiv.org/abs/2310.19543", "description": "This paper introduces new techniques for estimating, identifying and\nsimulating mixed causal-noncausal invertible-noninvertible models. We propose a\nframework that integrates high-order cumulants, merging both the spectrum and\nbispectrum into a single estimation function. The model that most adequately\nrepresents the data under the assumption that the error term is i.i.d. is\nselected. Our Monte Carlo study reveals unbiased parameter estimates and a high\nfrequency with which correct models are identified. We illustrate our strategy\nthrough an empirical analysis of returns from 24 Fama-French emerging market\nstock portfolios. The findings suggest that each portfolio displays noncausal\ndynamics, producing white noise residuals devoid of conditional heteroscedastic\neffects."}, "http://arxiv.org/abs/2310.19557": {"title": "A Bayesian Markov-switching SAR model for time-varying cross-price spillovers", "link": "http://arxiv.org/abs/2310.19557", "description": "The spatial autoregressive (SAR) model is extended by introducing a Markov\nswitching dynamics for the weight matrix and spatial autoregressive parameter.\nThe framework enables the identification of regime-specific connectivity\npatterns and strengths and the study of the spatiotemporal propagation of\nshocks in a system with a time-varying spatial multiplier matrix. The proposed\nmodel is applied to disaggregated CPI data from 15 EU countries to examine\ncross-price dependencies. The analysis identifies distinct connectivity\nstructures and spatial weights across the states, which capture shifts in\nconsumer behaviour, with marked cross-country differences in the spillover from\none price category to another."}, "http://arxiv.org/abs/2310.19747": {"title": "Characteristics of price related fluctuations in Non-Fungible Token (NFT) market", "link": "http://arxiv.org/abs/2310.19747", "description": "Non-fungible token (NFT) market is a new trading invention based on the\nblockchain technology which parallels the cryptocurrency market. In the present\nwork we study capitalization, floor price, the number of transactions, the\ninter-transaction times, and the transaction volume value of a few selected\npopular token collections. The results show that the fluctuations of all these\nquantities are characterized by heavy-tailed probability distribution\nfunctions, in most cases well described by the stretched exponentials, with a\ntrace of power-law scaling at times, long-range memory, and in several cases\neven the fractal organization of fluctuations, mostly restricted to the larger\nfluctuations, however. We conclude that the NFT market - even though young and\ngoverned by a somewhat different mechanisms of trading - shares several\nstatistical properties with the regular financial markets. However, some\ndifferences are visible in the specific quantitative indicators."}, "http://arxiv.org/abs/2310.19788": {"title": "Locally Optimal Best Arm Identification with a Fixed Budget", "link": "http://arxiv.org/abs/2310.19788", "description": "This study investigates the problem of identifying the best treatment arm, a\ntreatment arm with the highest expected outcome. We aim to identify the best\ntreatment arm with a lower probability of misidentification, which has been\nexplored under various names across numerous research fields, including\n\\emph{best arm identification} (BAI) and ordinal optimization. In our\nexperiments, the number of treatment-allocation rounds is fixed. In each round,\na decision-maker allocates a treatment arm to an experimental unit and observes\na corresponding outcome, which follows a Gaussian distribution with a variance\ndifferent among treatment arms. At the end of the experiment, we recommend one\nof the treatment arms as an estimate of the best treatment arm based on the\nobservations. The objective of the decision-maker is to design an experiment\nthat minimizes the probability of misidentifying the best treatment arm. With\nthis objective in mind, we develop lower bounds for the probability of\nmisidentification under the small-gap regime, where the gaps of the expected\noutcomes between the best and suboptimal treatment arms approach zero. Then,\nassuming that the variances are known, we design the\nGeneralized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, which is\nan extension of the Neyman allocation proposed by Neyman (1934) and the\nUniform-EBA strategy proposed by Bubeck et al. (2011). For the GNA-EBA\nstrategy, we show that the strategy is asymptotically optimal because its\nprobability of misidentification aligns with the lower bounds as the sample\nsize approaches infinity under the small-gap regime. We refer to such optimal\nstrategies as locally asymptotic optimal because their performance aligns with\nthe lower bounds within restricted situations characterized by the small-gap\nregime."}, "http://arxiv.org/abs/2009.00553": {"title": "A Vector Monotonicity Assumption for Multiple Instruments", "link": "http://arxiv.org/abs/2009.00553", "description": "When a researcher combines multiple instrumental variables for a single\nbinary treatment, the monotonicity assumption of the local average treatment\neffects (LATE) framework can become restrictive: it requires that all units\nshare a common direction of response even when separate instruments are shifted\nin opposing directions. What I call vector monotonicity, by contrast, simply\nassumes treatment uptake to be monotonic in all instruments, representing a\nspecial case of the partial monotonicity assumption introduced by Mogstad et\nal. (2021). I characterize the class of causal parameters that are point\nidentified under vector monotonicity, when the instruments are binary. This\nclass includes, for example, the average treatment effect among units that are\nin any way responsive to the collection of instruments, or those that are\nresponsive to a given subset of them. The identification results are\nconstructive and yield a simple estimator for the identified treatment effect\nparameters. An empirical application revisits the labor market returns to\ncollege."}, "http://arxiv.org/abs/2109.08109": {"title": "Standard Errors for Calibrated Parameters", "link": "http://arxiv.org/abs/2109.08109", "description": "Calibration, the practice of choosing the parameters of a structural model to\nmatch certain empirical moments, can be viewed as minimum distance estimation.\nExisting standard error formulas for such estimators require a consistent\nestimate of the correlation structure of the empirical moments, which is often\nunavailable in practice. Instead, the variances of the individual empirical\nmoments are usually readily estimable. Using only these variances, we derive\nconservative standard errors and confidence intervals for the structural\nparameters that are valid even under the worst-case correlation structure. In\nthe over-identified case, we show that the moment weighting scheme that\nminimizes the worst-case estimator variance amounts to a moment selection\nproblem with a simple solution. Finally, we develop tests of over-identifying\nor parameter restrictions. We apply our methods empirically to a model of menu\ncost pricing for multi-product firms and to a heterogeneous agent New Keynesian\nmodel."}, "http://arxiv.org/abs/2211.16714": {"title": "Incorporating Prior Knowledge of Latent Group Structure in Panel Data Models", "link": "http://arxiv.org/abs/2211.16714", "description": "The assumption of group heterogeneity has become popular in panel data\nmodels. We develop a constrained Bayesian grouped estimator that exploits\nresearchers' prior beliefs on groups in a form of pairwise constraints,\nindicating whether a pair of units is likely to belong to a same group or\ndifferent groups. We propose a prior to incorporate the pairwise constraints\nwith varying degrees of confidence. The whole framework is built on the\nnonparametric Bayesian method, which implicitly specifies a distribution over\nthe group partitions, and so the posterior analysis takes the uncertainty of\nthe latent group structure into account. Monte Carlo experiments reveal that\nadding prior knowledge yields more accurate estimates of coefficient and scores\npredictive gains over alternative estimators. We apply our method to two\nempirical applications. In a first application to forecasting U.S. CPI\ninflation, we illustrate that prior knowledge of groups improves density\nforecasts when the data is not entirely informative. A second application\nrevisits the relationship between a country's income and its democratic\ntransition; we identify heterogeneous income effects on democracy with five\ndistinct groups over ninety countries."}, "http://arxiv.org/abs/2307.01357": {"title": "Adaptive Principal Component Regression with Applications to Panel Data", "link": "http://arxiv.org/abs/2307.01357", "description": "Principal component regression (PCR) is a popular technique for fixed-design\nerror-in-variables regression, a generalization of the linear regression\nsetting in which the observed covariates are corrupted with random noise. We\nprovide the first time-uniform finite sample guarantees for online\n(regularized) PCR whenever data is collected adaptively. Since the proof\ntechniques for analyzing PCR in the fixed design setting do not readily extend\nto the online setting, our results rely on adapting tools from modern\nmartingale concentration to the error-in-variables setting. As an application\nof our bounds, we provide a framework for experiment design in panel data\nsettings when interventions are assigned adaptively. Our framework may be\nthought of as a generalization of the synthetic control and synthetic\ninterventions frameworks, where data is collected via an adaptive intervention\nassignment policy."}, "http://arxiv.org/abs/2309.06693": {"title": "Stochastic Learning of Semiparametric Monotone Index Models with Large Sample Size", "link": "http://arxiv.org/abs/2309.06693", "description": "I study the estimation of semiparametric monotone index models in the\nscenario where the number of observation points $n$ is extremely large and\nconventional approaches fail to work due to heavy computational burdens.\nMotivated by the mini-batch gradient descent algorithm (MBGD) that is widely\nused as a stochastic optimization tool in the machine learning field, I\nproposes a novel subsample- and iteration-based estimation procedure. In\nparticular, starting from any initial guess of the true parameter, I\nprogressively update the parameter using a sequence of subsamples randomly\ndrawn from the data set whose sample size is much smaller than $n$. The update\nis based on the gradient of some well-chosen loss function, where the\nnonparametric component is replaced with its Nadaraya-Watson kernel estimator\nbased on subsamples. My proposed algorithm essentially generalizes MBGD\nalgorithm to the semiparametric setup. Compared with full-sample-based method,\nthe new method reduces the computational time by roughly $n$ times if the\nsubsample size and the kernel function are chosen properly, so can be easily\napplied when the sample size $n$ is large. Moreover, I show that if I further\nconduct averages across the estimators produced during iterations, the\ndifference between the average estimator and full-sample-based estimator will\nbe $1/\\sqrt{n}$-trivial. Consequently, the average estimator is\n$1/\\sqrt{n}$-consistent and asymptotically normally distributed. In other\nwords, the new estimator substantially improves the computational speed, while\nat the same time maintains the estimation accuracy."}, "http://arxiv.org/abs/2310.16945": {"title": "Causal Q-Aggregation for CATE Model Selection", "link": "http://arxiv.org/abs/2310.16945", "description": "Accurate estimation of conditional average treatment effects (CATE) is at the\ncore of personalized decision making. While there is a plethora of models for\nCATE estimation, model selection is a nontrivial task, due to the fundamental\nproblem of causal inference. Recent empirical work provides evidence in favor\nof proxy loss metrics with double robust properties and in favor of model\nensembling. However, theoretical understanding is lacking. Direct application\nof prior theoretical work leads to suboptimal oracle model selection rates due\nto the non-convexity of the model selection problem. We provide regret rates\nfor the major existing CATE ensembling approaches and propose a new CATE model\nensembling approach based on Q-aggregation using the doubly robust loss. Our\nmain result shows that causal Q-aggregation achieves statistically optimal\noracle model selection regret rates of $\\frac{\\log(M)}{n}$ (with $M$ models and\n$n$ samples), with the addition of higher-order estimation error terms related\nto products of errors in the nuisance functions. Crucially, our regret rate\ndoes not require that any of the candidate CATE models be close to the truth.\nWe validate our new method on many semi-synthetic datasets and also provide\nextensions of our work to CATE model selection with instrumental variables and\nunobserved confounding."}, "http://arxiv.org/abs/2310.19992": {"title": "Robust Estimation of Realized Correlation: New Insight about Intraday Fluctuations in Market Betas", "link": "http://arxiv.org/abs/2310.19992", "description": "Time-varying volatility is an inherent feature of most economic time-series,\nwhich causes standard correlation estimators to be inconsistent. The quadrant\ncorrelation estimator is consistent but very inefficient. We propose a novel\nsubsampled quadrant estimator that improves efficiency while preserving\nconsistency and robustness. This estimator is particularly well-suited for\nhigh-frequency financial data and we apply it to a large panel of US stocks.\nOur empirical analysis sheds new light on intra-day fluctuations in market\nbetas by decomposing them into time-varying correlations and relative\nvolatility changes. Our results show that intraday variation in betas is\nprimarily driven by intraday variation in correlations."}, "http://arxiv.org/abs/2006.07691": {"title": "Synthetic Interventions", "link": "http://arxiv.org/abs/2006.07691", "description": "Consider a setting with $N$ heterogeneous units (e.g., individuals,\nsub-populations) and $D$ interventions (e.g., socio-economic policies). Our\ngoal is to learn the expected potential outcome associated with every\nintervention on every unit, totaling $N \\times D$ causal parameters. Towards\nthis, we present a causal framework, synthetic interventions (SI), to infer\nthese $N \\times D$ causal parameters while only observing each of the $N$ units\nunder at most two interventions, independent of $D$. This can be significant as\nthe number of interventions, i.e., level of personalization, grows. Under a\nnovel tensor factor model across units, outcomes, and interventions, we prove\nan identification result for each of these $N \\times D$ causal parameters,\nestablish finite-sample consistency of our estimator along with asymptotic\nnormality under additional conditions. Importantly, our estimator also allows\nfor latent confounders that determine how interventions are assigned. The\nestimator is further furnished with data-driven tests to examine its\nsuitability. Empirically, we validate our framework through a large-scale A/B\ntest performed on an e-commerce platform. We believe our results could have\nimplications for the design of data-efficient randomized experiments (e.g.,\nrandomized control trials) with heterogeneous units and multiple interventions."}, "http://arxiv.org/abs/2207.04481": {"title": "Detecting Grouped Local Average Treatment Effects and Selecting True Instruments", "link": "http://arxiv.org/abs/2207.04481", "description": "Under an endogenous binary treatment with heterogeneous effects and multiple\ninstruments, we propose a two-step procedure for identifying complier groups\nwith identical local average treatment effects (LATE) despite relying on\ndistinct instruments, even if several instruments violate the identifying\nassumptions. We use the fact that the LATE is homogeneous for instruments which\n(i) satisfy the LATE assumptions (instrument validity and treatment\nmonotonicity in the instrument) and (ii) generate identical complier groups in\nterms of treatment propensities given the respective instruments. We propose a\ntwo-step procedure, where we first cluster the propensity scores in the first\nstep and find groups of IVs with the same reduced form parameters in the second\nstep. Under the plurality assumption that within each set of instruments with\nidentical treatment propensities, instruments truly satisfying the LATE\nassumptions are the largest group, our procedure permits identifying these true\ninstruments in a data driven way. We show that our procedure is consistent and\nprovides consistent and asymptotically normal estimators of underlying LATEs.\nWe also provide a simulation study investigating the finite sample properties\nof our approach and an empirical application investigating the effect of\nincarceration on recidivism in the US with judge assignments serving as\ninstruments."}, "http://arxiv.org/abs/2304.09078": {"title": "Club coefficients in the UEFA Champions League: Time for shift to an Elo-based formula", "link": "http://arxiv.org/abs/2304.09078", "description": "One of the most popular club football tournaments, the UEFA Champions League,\nwill see a fundamental reform from the 2024/25 season: the traditional group\nstage will be replaced by one league where each of the 36 teams plays eight\nmatches. To guarantee that the opponents of the clubs are of the same strength\nin the new design, it is crucial to forecast the performance of the teams\nbefore the tournament as well as possible. This paper investigates whether the\ncurrently used rating of the teams, the UEFA club coefficient, can be improved\nby taking the games played in the national leagues into account. According to\nour logistic regression models, a variant of the Elo method provides a higher\naccuracy in terms of explanatory power in the Champions League matches. The\nUnion of European Football Associations (UEFA) is encouraged to follow the\nexample of the FIFA World Ranking and reform the calculation of the club\ncoefficients in order to avoid unbalanced schedules in the novel tournament\nformat of the Champions League."}, "http://arxiv.org/abs/2308.13564": {"title": "SGMM: Stochastic Approximation to Generalized Method of Moments", "link": "http://arxiv.org/abs/2308.13564", "description": "We introduce a new class of algorithms, Stochastic Generalized Method of\nMoments (SGMM), for estimation and inference on (overidentified) moment\nrestriction models. Our SGMM is a novel stochastic approximation alternative to\nthe popular Hansen (1982) (offline) GMM, and offers fast and scalable\nimplementation with the ability to handle streaming datasets in real time. We\nestablish the almost sure convergence, and the (functional) central limit\ntheorem for the inefficient online 2SLS and the efficient SGMM. Moreover, we\npropose online versions of the Durbin-Wu-Hausman and Sargan-Hansen tests that\ncan be seamlessly integrated within the SGMM framework. Extensive Monte Carlo\nsimulations show that as the sample size increases, the SGMM matches the\nstandard (offline) GMM in terms of estimation accuracy and gains over\ncomputational efficiency, indicating its practical value for both large-scale\nand online datasets. We demonstrate the efficacy of our approach by a proof of\nconcept using two well known empirical examples with large sample sizes."}, "http://arxiv.org/abs/2311.00013": {"title": "Semiparametric Discrete Choice Models for Bundles", "link": "http://arxiv.org/abs/2311.00013", "description": "We propose two approaches to estimate semiparametric discrete choice models\nfor bundles. Our first approach is a kernel-weighted rank estimator based on a\nmatching-based identification strategy. We establish its complete asymptotic\nproperties and prove the validity of the nonparametric bootstrap for inference.\nWe then introduce a new multi-index least absolute deviations (LAD) estimator\nas an alternative, of which the main advantage is its capacity to estimate\npreference parameters on both alternative- and agent-specific regressors. Both\nmethods can account for arbitrary correlation in disturbances across choices,\nwith the former also allowing for interpersonal heteroskedasticity. We also\ndemonstrate that the identification strategy underlying these procedures can be\nextended naturally to panel data settings, producing an analogous localized\nmaximum score estimator and a LAD estimator for estimating bundle choice models\nwith fixed effects. We derive the limiting distribution of the former and\nverify the validity of the numerical bootstrap as an inference tool. All our\nproposed methods can be applied to general multi-index models. Monte Carlo\nexperiments show that they perform well in finite samples."}, "http://arxiv.org/abs/2311.00439": {"title": "Bounds on Treatment Effects under Stochastic Monotonicity Assumption in Sample Selection Models", "link": "http://arxiv.org/abs/2311.00439", "description": "This paper discusses the partial identification of treatment effects in\nsample selection models when the exclusion restriction fails and the\nmonotonicity assumption in the selection effect does not hold exactly, both of\nwhich are key challenges in applying the existing methodologies. Our approach\nbuilds on Lee's (2009) procedure, who considers partial identification under\nthe monotonicity assumption, but we assume only a stochastic (and weaker)\nversion of monotonicity, which depends on a prespecified parameter $\\vartheta$\nthat represents researchers' belief in the plausibility of the monotonicity.\nUnder this assumption, we show that we can still obtain useful bounds even when\nthe monotonic behavioral model does not strictly hold. Our procedure is useful\nwhen empirical researchers anticipate that a small fraction of the population\nwill not behave monotonically in selection; it can also be an effective tool\nfor performing sensitivity analysis or examining the identification power of\nthe monotonicity assumption. Our procedure is easily extendable to other\nrelated settings; we also provide the identification result of the marginal\ntreatment effects setting as an important application. Moreover, we show that\nthe bounds can still be obtained even in the absence of the knowledge of\n$\\vartheta$ under the semiparametric models that nest the classical probit and\nlogit selection models."}, "http://arxiv.org/abs/2311.00577": {"title": "Personalized Assignment to One of Many Treatment Arms via Regularized and Clustered Joint Assignment Forests", "link": "http://arxiv.org/abs/2311.00577", "description": "We consider learning personalized assignments to one of many treatment arms\nfrom a randomized controlled trial. Standard methods that estimate\nheterogeneous treatment effects separately for each arm may perform poorly in\nthis case due to excess variance. We instead propose methods that pool\ninformation across treatment arms: First, we consider a regularized\nforest-based assignment algorithm based on greedy recursive partitioning that\nshrinks effect estimates across arms. Second, we augment our algorithm by a\nclustering scheme that combines treatment arms with consistently similar\noutcomes. In a simulation study, we compare the performance of these approaches\nto predicting arm-wise outcomes separately, and document gains of directly\noptimizing the treatment assignment with regularization and clustering. In a\ntheoretical model, we illustrate how a high number of treatment arms makes\nfinding the best arm hard, while we can achieve sizable utility gains from\npersonalization by regularized optimization."}, "http://arxiv.org/abs/2311.00662": {"title": "On Gaussian Process Priors in Conditional Moment Restriction Models", "link": "http://arxiv.org/abs/2311.00662", "description": "This paper studies quasi-Bayesian estimation and uncertainty quantification\nfor an unknown function that is identified by a nonparametric conditional\nmoment restriction model. We derive contraction rates for a class of Gaussian\nprocess priors and provide conditions under which a Bernstein-von Mises theorem\nholds for the quasi-posterior distribution. As a consequence, we show that\noptimally-weighted quasi-Bayes credible sets have exact asymptotic frequentist\ncoverage. This extends classical result on the frequentist validity of\noptimally weighted quasi-Bayes credible sets for parametric generalized method\nof moments (GMM) models."}, "http://arxiv.org/abs/2209.14502": {"title": "Fast Inference for Quantile Regression with Tens of Millions of Observations", "link": "http://arxiv.org/abs/2209.14502", "description": "Big data analytics has opened new avenues in economic research, but the\nchallenge of analyzing datasets with tens of millions of observations is\nsubstantial. Conventional econometric methods based on extreme estimators\nrequire large amounts of computing resources and memory, which are often not\nreadily available. In this paper, we focus on linear quantile regression\napplied to \"ultra-large\" datasets, such as U.S. decennial censuses. A fast\ninference framework is presented, utilizing stochastic subgradient descent\n(S-subGD) updates. The inference procedure handles cross-sectional data\nsequentially: (i) updating the parameter estimate with each incoming \"new\nobservation\", (ii) aggregating it as a $\\textit{Polyak-Ruppert}$ average, and\n(iii) computing a pivotal statistic for inference using only a solution path.\nThe methodology draws from time-series regression to create an asymptotically\npivotal statistic through random scaling. Our proposed test statistic is\ncalculated in a fully online fashion and critical values are calculated without\nresampling. We conduct extensive numerical studies to showcase the\ncomputational merits of our proposed inference. For inference problems as large\nas $(n, d) \\sim (10^7, 10^3)$, where $n$ is the sample size and $d$ is the\nnumber of regressors, our method generates new insights, surpassing current\ninference methods in computation. Our method specifically reveals trends in the\ngender gap in the U.S. college wage premium using millions of observations,\nwhile controlling over $10^3$ covariates to mitigate confounding effects."}, "http://arxiv.org/abs/2311.00905": {"title": "Data-Driven Fixed-Point Tuning for Truncated Realized Variations", "link": "http://arxiv.org/abs/2311.00905", "description": "Many methods for estimating integrated volatility and related functionals of\nsemimartingales in the presence of jumps require specification of tuning\nparameters for their use. In much of the available theory, tuning parameters\nare assumed to be deterministic, and their values are specified only up to\nasymptotic constraints. However, in empirical work and in simulation studies,\nthey are typically chosen to be random and data-dependent, with explicit\nchoices in practice relying on heuristics alone. In this paper, we consider\nnovel data-driven tuning procedures for the truncated realized variations of a\nsemimartingale with jumps, which are based on a type of stochastic fixed-point\niteration. Being effectively automated, our approach alleviates the need for\ndelicate decision-making regarding tuning parameters, and can be implemented\nusing information regarding sampling frequency alone. We show our methods can\nlead to asymptotically efficient estimation of integrated volatility and\nexhibit superior finite-sample performance compared to popular alternatives in\nthe literature."}, "http://arxiv.org/abs/2311.01217": {"title": "The learning effects of subsidies to bundled goods: a semiparametric approach", "link": "http://arxiv.org/abs/2311.01217", "description": "Can temporary subsidies to bundles induce long-run changes in demand due to\nlearning about the relative quality of one of its constituent goods? This paper\nprovides theoretical and experimental evidence on the role of this mechanism.\nTheoretically, we introduce a model where an agent learns about the quality of\nan innovation on an essential good through consumption. Our results show that\nthe contemporaneous effect of a one-off subsidy to a bundle that contains the\ninnovation may be decomposed into a direct price effect, and an indirect\nlearning motive, whereby an agent leverages the discount to increase the\ninformational bequest left to her future selves. We then assess the predictions\nof our theory in a randomised experiment in a ridesharing platform. The\nexperiment provided two-week discounts for car trips integrating with a train\nor metro station (a bundle). Given the heavy-tailed nature of our data, we\nfollow \\cite{Athey2023} and, motivated by our theory, propose a semiparametric\nmodel for treatment effects that enables the construction of more efficient\nestimators. We introduce a statistically efficient estimator for our model by\nrelying on L-moments, a robust alternative to standard moments. Our estimator\nimmediately yields a specification test for the semiparametric model; moreover,\nin our adopted parametrisation, it can be easily computed through generalized\nleast squares. Our empirical results indicate that a two-week 50\\% discount on\ncar trips integrating with train/metro leads to a contemporaneous increase in\nthe demand for integrated rides, and, consistent with our learning model,\npersistent changes in the mean and dispersion of nonintegrated rides. These\neffects persist for over four months after the discount. A simple calibration\nof our model shows that around 40\\% to 50\\% of the estimated contemporaneous\nincrease in integrated rides may be attributed to a learning motive."}, "http://arxiv.org/abs/2110.10650": {"title": "Attention Overload", "link": "http://arxiv.org/abs/2110.10650", "description": "We introduce an Attention Overload Model that captures the idea that\nalternatives compete for the decision maker's attention, and hence the\nattention that each alternative receives decreases as the choice problem\nbecomes larger. We provide testable implications on the observed choice\nbehavior that can be used to (point or partially) identify the decision maker's\npreference and attention frequency. We then enhance our attention overload\nmodel to accommodate heterogeneous preferences based on the idea of List-based\nAttention Overload, where alternatives are presented to the decision makers as\na list that correlates with both heterogeneous preferences and random\nattention. We show that preference and attention frequencies are (point or\npartially) identifiable under nonparametric assumptions on the list and\nattention formation mechanisms, even when the true underlying list is unknown\nto the researcher. Building on our identification results, we develop\neconometric methods for estimation and inference."}, "http://arxiv.org/abs/2112.13398": {"title": "Long Story Short: Omitted Variable Bias in Causal Machine Learning", "link": "http://arxiv.org/abs/2112.13398", "description": "We derive general, yet simple, sharp bounds on the size of the omitted\nvariable bias for a broad class of causal parameters that can be identified as\nlinear functionals of the conditional expectation function of the outcome. Such\nfunctionals encompass many of the traditional targets of investigation in\ncausal inference studies, such as, for example, (weighted) average of potential\noutcomes, average treatment effects (including subgroup effects, such as the\neffect on the treated), (weighted) average derivatives, and policy effects from\nshifts in covariate distribution -- all for general, nonparametric causal\nmodels. Our construction relies on the Riesz-Frechet representation of the\ntarget functional. Specifically, we show how the bound on the bias depends only\non the additional variation that the latent variables create both in the\noutcome and in the Riesz representer for the parameter of interest. Moreover,\nin many important cases (e.g, average treatment effects and avearage\nderivatives) the bound is shown to depend on easily interpretable quantities\nthat measure the explanatory power of the omitted variables. Therefore, simple\nplausibility judgments on the maximum explanatory power of omitted variables\n(in explaining treatment and outcome variation) are sufficient to place overall\nbounds on the size of the bias. Furthermore, we use debiased machine learning\nto provide flexible and efficient statistical inference on learnable components\nof the bounds. Finally, empirical examples demonstrate the usefulness of the\napproach."}, "http://arxiv.org/abs/2112.03626": {"title": "Phase transitions in nonparametric regressions", "link": "http://arxiv.org/abs/2112.03626", "description": "When the unknown regression function of a single variable is known to have\nderivatives up to the $(\\gamma+1)$th order bounded in absolute values by a\ncommon constant everywhere or a.e. (i.e., $(\\gamma+1)$th degree of smoothness),\nthe minimax optimal rate of the mean integrated squared error (MISE) is stated\nas $\\left(\\frac{1}{n}\\right)^{\\frac{2\\gamma+2}{2\\gamma+3}}$ in the literature.\nThis paper shows that: (i) if $n\\leq\\left(\\gamma+1\\right)^{2\\gamma+3}$, the\nminimax optimal MISE rate is $\\frac{\\log n}{n\\log(\\log n)}$ and the optimal\ndegree of smoothness to exploit is roughly $\\max\\left\\{ \\left\\lfloor \\frac{\\log\nn}{2\\log\\left(\\log n\\right)}\\right\\rfloor ,\\,1\\right\\} $; (ii) if\n$n&gt;\\left(\\gamma+1\\right)^{2\\gamma+3}$, the minimax optimal MISE rate is\n$\\left(\\frac{1}{n}\\right)^{\\frac{2\\gamma+2}{2\\gamma+3}}$ and the optimal degree\nof smoothness to exploit is $\\gamma+1$. The fundamental contribution of this\npaper is a set of metric entropy bounds we develop for smooth function classes.\nSome of our bounds are original, and some of them improve and/or generalize the\nones in the literature (e.g., Kolmogorov and Tikhomirov, 1959). Our metric\nentropy bounds allow us to show phase transitions in the minimax optimal MISE\nrates associated with some commonly seen smoothness classes as well as\nnon-standard smoothness classes, and can also be of independent interest\noutside the nonparametric regression problems."}, "http://arxiv.org/abs/2206.04157": {"title": "Inference for Matched Tuples and Fully Blocked Factorial Designs", "link": "http://arxiv.org/abs/2206.04157", "description": "This paper studies inference in randomized controlled trials with multiple\ntreatments, where treatment status is determined according to a \"matched\ntuples\" design. Here, by a matched tuples design, we mean an experimental\ndesign where units are sampled i.i.d. from the population of interest, grouped\ninto \"homogeneous\" blocks with cardinality equal to the number of treatments,\nand finally, within each block, each treatment is assigned exactly once\nuniformly at random. We first study estimation and inference for matched tuples\ndesigns in the general setting where the parameter of interest is a vector of\nlinear contrasts over the collection of average potential outcomes for each\ntreatment. Parameters of this form include standard average treatment effects\nused to compare one treatment relative to another, but also include parameters\nwhich may be of interest in the analysis of factorial designs. We first\nestablish conditions under which a sample analogue estimator is asymptotically\nnormal and construct a consistent estimator of its corresponding asymptotic\nvariance. Combining these results establishes the asymptotic exactness of tests\nbased on these estimators. In contrast, we show that, for two common testing\nprocedures based on t-tests constructed from linear regressions, one test is\ngenerally conservative while the other generally invalid. We go on to apply our\nresults to study the asymptotic properties of what we call \"fully-blocked\" 2^K\nfactorial designs, which are simply matched tuples designs applied to a full\nfactorial experiment. Leveraging our previous results, we establish that our\nestimator achieves a lower asymptotic variance under the fully-blocked design\nthan that under any stratified factorial design which stratifies the\nexperimental sample into a finite number of \"large\" strata. A simulation study\nand empirical application illustrate the practical relevance of our results."}, "http://arxiv.org/abs/2303.02716": {"title": "Deterministic, quenched and annealed parameter estimation for heterogeneous network models", "link": "http://arxiv.org/abs/2303.02716", "description": "At least two, different approaches to define and solve statistical models for\nthe analysis of economic systems exist: the typical, econometric one,\ninterpreting the Gravity Model specification as the expected link weight of an\narbitrary probability distribution, and the one rooted into statistical\nphysics, constructing maximum-entropy distributions constrained to satisfy\ncertain network properties. In a couple of recent, companion papers they have\nbeen successfully integrated within the framework induced by the constrained\nminimisation of the Kullback-Leibler divergence: specifically, two, broad\nclasses of models have been devised, i.e. the integrated and the conditional\nones, defined by different, probabilistic rules to place links, load them with\nweights and turn them into proper, econometric prescriptions. Still, the\nrecipes adopted by the two approaches to estimate the parameters entering into\nthe definition of each model differ. In econometrics, a likelihood that\ndecouples the binary and weighted parts of a model, treating a network as\ndeterministic, is typically maximised; to restore its random character, two\nalternatives exist: either solving the likelihood maximisation on each\nconfiguration of the ensemble and taking the average of the parameters\nafterwards or taking the average of the likelihood function and maximising the\nlatter one. The difference between these approaches lies in the order in which\nthe operations of averaging and maximisation are taken - a difference that is\nreminiscent of the quenched and annealed ways of averaging out the disorder in\nspin glasses. The results of the present contribution, devoted to comparing\nthese recipes in the case of continuous, conditional network models, indicate\nthat the annealed estimation recipe represents the best alternative to the\ndeterministic one."}, "http://arxiv.org/abs/2307.01284": {"title": "Does regional variation in wage levels identify the effects of a national minimum wage?", "link": "http://arxiv.org/abs/2307.01284", "description": "This paper examines the identification assumptions underlying two types of\nestimators of the causal effects of minimum wages based on regional variation\nin wage levels: the \"effective minimum wage\" and the \"fraction affected/gap\"\ndesigns. For the effective minimum wage design, I show that the identification\nassumptions emphasized by Lee (1999) are crucial for unbiased estimation but\ndifficult to satisfy in empirical applications for reasons arising from\neconomic theory. For the fraction affected design at the region level, I show\nthat economic factors such as a common trend in the dispersion of worker\nproductivity or regional convergence in GDP per capita may lead to violations\nof the \"parallel trends\" identifying assumption. The paper suggests ways to\nincrease the likelihood of detecting those issues when implementing checks for\nparallel pre-trends. I also show that this design may be subject to biases\narising from the misspecification of the treatment intensity variable,\nespecially when the minimum wage strongly affects employment and wages."}, "http://arxiv.org/abs/2311.02196": {"title": "Pooled Bewley Estimator of Long Run Relationships in Dynamic Heterogenous Panels", "link": "http://arxiv.org/abs/2311.02196", "description": "Using a transformation of the autoregressive distributed lag model due to\nBewley, a novel pooled Bewley (PB) estimator of long-run coefficients for\ndynamic panels with heterogeneous short-run dynamics is proposed. The PB\nestimator is directly comparable to the widely used Pooled Mean Group (PMG)\nestimator, and is shown to be consistent and asymptotically normal. Monte Carlo\nsimulations show good small sample performance of PB compared to the existing\nestimators in the literature, namely PMG, panel dynamic OLS (PDOLS), and panel\nfully-modified OLS (FMOLS). Application of two bias-correction methods and a\nbootstrapping of critical values to conduct inference robust to cross-sectional\ndependence of errors are also considered. The utility of the PB estimator is\nillustrated in an empirical application to the aggregate consumption function."}, "http://arxiv.org/abs/2311.02299": {"title": "The Fragility of Sparsity", "link": "http://arxiv.org/abs/2311.02299", "description": "We show, using three empirical applications, that linear regression estimates\nwhich rely on the assumption of sparsity are fragile in two ways. First, we\ndocument that different choices of the regressor matrix that don't impact\nordinary least squares (OLS) estimates, such as the choice of baseline category\nwith categorical controls, can move sparsity-based estimates two standard\nerrors or more. Second, we develop two tests of the sparsity assumption based\non comparing sparsity-based estimators with OLS. The tests tend to reject the\nsparsity assumption in all three applications. Unless the number of regressors\nis comparable to or exceeds the sample size, OLS yields more robust results at\nlittle efficiency cost."}, "http://arxiv.org/abs/2311.02467": {"title": "Individualized Policy Evaluation and Learning under Clustered Network Interference", "link": "http://arxiv.org/abs/2311.02467", "description": "While there now exists a large literature on policy evaluation and learning,\nmuch of prior work assumes that the treatment assignment of one unit does not\naffect the outcome of another unit. Unfortunately, ignoring interference may\nlead to biased policy evaluation and yield ineffective learned policies. For\nexample, treating influential individuals who have many friends can generate\npositive spillover effects, thereby improving the overall performance of an\nindividualized treatment rule (ITR). We consider the problem of evaluating and\nlearning an optimal ITR under clustered network (or partial) interference where\nclusters of units are sampled from a population and units may influence one\nanother within each cluster. Under this model, we propose an estimator that can\nbe used to evaluate the empirical performance of an ITR. We show that this\nestimator is substantially more efficient than the standard inverse probability\nweighting estimator, which does not impose any assumption about spillover\neffects. We derive the finite-sample regret bound for a learned ITR, showing\nthat the use of our efficient evaluation estimator leads to the improved\nperformance of learned policies. Finally, we conduct simulation and empirical\nstudies to illustrate the advantages of the proposed methodology."}, "http://arxiv.org/abs/2311.02789": {"title": "Estimation of Semiparametric Multi-Index Models Using Deep Neural Networks", "link": "http://arxiv.org/abs/2311.02789", "description": "In this paper, we consider estimation and inference for both the multi-index\nparameters and the link function involved in a class of semiparametric\nmulti-index models via deep neural networks (DNNs). We contribute to the design\nof DNN by i) providing more transparency for practical implementation, ii)\ndefining different types of sparsity, iii) showing the differentiability, iv)\npointing out the set of effective parameters, and v) offering a new variant of\nrectified linear activation function (ReLU), etc. Asymptotic properties for the\njoint estimates of both the index parameters and the link functions are\nestablished, and a feasible procedure for the purpose of inference is also\nproposed. We conduct extensive numerical studies to examine the finite-sample\nperformance of the estimation methods, and we also evaluate the empirical\nrelevance and applicability of the proposed models and estimation methods to\nreal data."}, "http://arxiv.org/abs/2201.07880": {"title": "Deep self-consistent learning of local volatility", "link": "http://arxiv.org/abs/2201.07880", "description": "We present an algorithm for the calibration of local volatility from market\noption prices through deep self-consistent learning, by approximating both\nmarket option prices and local volatility using deep neural networks,\nrespectively. Our method uses the initial-boundary value problem of the\nunderlying Dupire's partial differential equation solved by the parameterized\noption prices to bring corrections to the parameterization in a self-consistent\nway. By exploiting the differentiability of the neural networks, we can\nevaluate Dupire's equation locally at each strike-maturity pair; while by\nexploiting their continuity, we sample strike-maturity pairs uniformly from a\ngiven domain, going beyond the discrete points where the options are quoted.\nMoreover, the absence of arbitrage opportunities are imposed by penalizing an\nassociated loss function as a soft constraint. For comparison with existing\napproaches, the proposed method is tested on both synthetic and market option\nprices, which shows an improved performance in terms of reduced interpolation\nand reprice errors, as well as the smoothness of the calibrated local\nvolatility. An ablation study has been performed, asserting the robustness and\nsignificance of the proposed method."}, "http://arxiv.org/abs/2204.12023": {"title": "A One-Covariate-at-a-Time Method for Nonparametric Additive Models", "link": "http://arxiv.org/abs/2204.12023", "description": "This paper proposes a one-covariate-at-a-time multiple testing (OCMT)\napproach to choose significant variables in high-dimensional nonparametric\nadditive regression models. Similarly to Chudik, Kapetanios and Pesaran (2018),\nwe consider the statistical significance of individual nonparametric additive\ncomponents one at a time and take into account the multiple testing nature of\nthe problem. One-stage and multiple-stage procedures are both considered. The\nformer works well in terms of the true positive rate only if the marginal\neffects of all signals are strong enough; the latter helps to pick up hidden\nsignals that have weak marginal effects. Simulations demonstrate the good\nfinite sample performance of the proposed procedures. As an empirical\napplication, we use the OCMT procedure on a dataset we extracted from the\nLongitudinal Survey on Rural Urban Migration in China. We find that our\nprocedure works well in terms of the out-of-sample forecast root mean square\nerrors, compared with competing methods."}, "http://arxiv.org/abs/2209.03259": {"title": "A Ridge-Regularised Jackknifed Anderson-Rubin Test", "link": "http://arxiv.org/abs/2209.03259", "description": "We consider hypothesis testing in instrumental variable regression models\nwith few included exogenous covariates but many instruments -- possibly more\nthan the number of observations. We show that a ridge-regularised version of\nthe jackknifed Anderson Rubin (1949, henceforth AR) test controls asymptotic\nsize in the presence of heteroskedasticity, and when the instruments may be\narbitrarily weak. Asymptotic size control is established under weaker\nassumptions than those imposed for recently proposed jackknifed AR tests in the\nliterature. Furthermore, ridge-regularisation extends the scope of jackknifed\nAR tests to situations in which there are more instruments than observations.\nMonte-Carlo simulations indicate that our method has favourable finite-sample\nsize and power properties compared to recently proposed alternative approaches\nin the literature. An empirical application on the elasticity of substitution\nbetween immigrants and natives in the US illustrates the usefulness of the\nproposed method for practitioners."}, "http://arxiv.org/abs/2212.09193": {"title": "Identification of time-varying counterfactual parameters in nonlinear panel models", "link": "http://arxiv.org/abs/2212.09193", "description": "We develop a general framework for the identification of counterfactual\nparameters in a class of nonlinear semiparametric panel models with fixed\neffects and time effects. Our method applies to models for discrete outcomes\n(e.g., two-way fixed effects binary choice) or continuous outcomes (e.g.,\ncensored regression), with discrete or continuous regressors. Our results do\nnot require parametric assumptions on the error terms or time-homogeneity on\nthe outcome equation. Our main results focus on static models, with a set of\nresults applying to models without any exogeneity conditions. We show that the\nsurvival distribution of counterfactual outcomes is identified (point or\npartial) in this class of models. This parameter is a building block for most\npartial and marginal effects of interest in applied practice that are based on\nthe average structural function as defined by Blundell and Powell (2003, 2004).\nTo the best of our knowledge, ours are the first results on average partial and\nmarginal effects for binary choice and ordered choice models with two-way fixed\neffects and non-logistic errors."}, "http://arxiv.org/abs/2306.04135": {"title": "Semiparametric Discrete Choice Models for Bundles", "link": "http://arxiv.org/abs/2306.04135", "description": "We propose two approaches to estimate semiparametric discrete choice models\nfor bundles. Our first approach is a kernel-weighted rank estimator based on a\nmatching-based identification strategy. We establish its complete asymptotic\nproperties and prove the validity of the nonparametric bootstrap for inference.\nWe then introduce a new multi-index least absolute deviations (LAD) estimator\nas an alternative, of which the main advantage is its capacity to estimate\npreference parameters on both alternative- and agent-specific regressors. Both\nmethods can account for arbitrary correlation in disturbances across choices,\nwith the former also allowing for interpersonal heteroskedasticity. We also\ndemonstrate that the identification strategy underlying these procedures can be\nextended naturally to panel data settings, producing an analogous localized\nmaximum score estimator and a LAD estimator for estimating bundle choice models\nwith fixed effects. We derive the limiting distribution of the former and\nverify the validity of the numerical bootstrap as an inference tool. All our\nproposed methods can be applied to general multi-index models. Monte Carlo\nexperiments show that they perform well in finite samples."}, "http://arxiv.org/abs/2311.03471": {"title": "Optimal Estimation Methodologies for Panel Data Regression Models", "link": "http://arxiv.org/abs/2311.03471", "description": "This survey study discusses main aspects to optimal estimation methodologies\nfor panel data regression models. In particular, we present current\nmethodological developments for modeling stationary panel data as well as\nrobust methods for estimation and inference in nonstationary panel data\nregression models. Some applications from the network econometrics and high\ndimensional statistics literature are also discussed within a stationary time\nseries environment."}, "http://arxiv.org/abs/2311.04073": {"title": "Debiased Fixed Effects Estimation of Binary Logit Models with Three-Dimensional Panel Data", "link": "http://arxiv.org/abs/2311.04073", "description": "Naive maximum likelihood estimation of binary logit models with fixed effects\nleads to unreliable inference due to the incidental parameter problem. We study\nthe case of three-dimensional panel data, where the model includes three sets\nof additive and overlapping unobserved effects. This encompasses models for\nnetwork panel data, where senders and receivers maintain bilateral\nrelationships over time, and fixed effects account for unobserved heterogeneity\nat the sender-time, receiver-time, and sender-receiver levels. In an asymptotic\nframework, where all three panel dimensions grow large at constant relative\nrates, we characterize the leading bias of the naive estimator. The inference\nproblem we identify is particularly severe, as it is not possible to balance\nthe order of the bias and the standard deviation. As a consequence, the naive\nestimator has a degenerating asymptotic distribution, which exacerbates the\ninference problem relative to other fixed effects estimators studied in the\nliterature. To resolve the inference problem, we derive explicit expressions to\ndebias the fixed effects estimator."}, "http://arxiv.org/abs/2207.09246": {"title": "Asymptotic Properties of Endogeneity Corrections Using Nonlinear Transformations", "link": "http://arxiv.org/abs/2207.09246", "description": "This paper considers a linear regression model with an endogenous regressor\nwhich arises from a nonlinear transformation of a latent variable. It is shown\nthat the corresponding coefficient can be consistently estimated without\nexternal instruments by adding a rank-based transformation of the regressor to\nthe model and performing standard OLS estimation. In contrast to other\napproaches, our nonparametric control function approach does not rely on a\nconformably specified copula. Furthermore, the approach allows for the presence\nof additional exogenous regressors which may be (linearly) correlated with the\nendogenous regressor(s). Consistency and asymptotic normality of the estimator\nare proved and the estimator is compared with copula based approaches by means\nof Monte Carlo simulations. An empirical application on wage data of the US\ncurrent population survey demonstrates the usefulness of our method."}, "http://arxiv.org/abs/2301.10643": {"title": "Automatic Locally Robust Estimation with Generated Regressors", "link": "http://arxiv.org/abs/2301.10643", "description": "Many economic and causal parameters of interest depend on generated\nregressors. Examples include structural parameters in models with endogenous\nvariables estimated by control functions and in models with sample selection,\ntreatment effect estimation with propensity score matching, and marginal\ntreatment effects. Inference with generated regressors is complicated by the\nvery complex expression for influence functions and asymptotic variances. To\naddress this problem, we propose Automatic Locally Robust/debiased GMM\nestimators in a general setting with generated regressors. Importantly, we\nallow for the generated regressors to be generated from machine learners, such\nas Random Forest, Neural Nets, Boosting, and many others. We use our results to\nconstruct novel Doubly Robust and Locally Robust estimators for the\nCounterfactual Average Structural Function and Average Partial Effects in\nmodels with endogeneity and sample selection, respectively. We provide\nsufficient conditions for the asymptotic normality of our debiased GMM\nestimators and investigate their finite sample performance through Monte Carlo\nsimulations."}, "http://arxiv.org/abs/2303.11399": {"title": "How Much Should We Trust Instrumental Variable Estimates in Political Science? Practical Advice Based on Over 60 Replicated Studies", "link": "http://arxiv.org/abs/2303.11399", "description": "Instrumental variable (IV) strategies are widely used in political science to\nestablish causal relationships. However, the identifying assumptions required\nby an IV design are demanding, and it remains challenging for researchers to\nassess their validity. In this paper, we replicate 67 papers published in three\ntop journals in political science during 2010-2022 and identify several\ntroubling patterns. First, researchers often overestimate the strength of their\nIVs due to non-i.i.d. errors, such as a clustering structure. Second, the most\ncommonly used t-test for the two-stage-least-squares (2SLS) estimates often\nseverely underestimates uncertainty. Using more robust inferential methods, we\nfind that around 19-30% of the 2SLS estimates in our sample are underpowered.\nThird, in the majority of the replicated studies, the 2SLS estimates are much\nlarger than the ordinary-least-squares estimates, and their ratio is negatively\ncorrelated with the strength of the IVs in studies where the IVs are not\nexperimentally generated, suggesting potential violations of unconfoundedness\nor the exclusion restriction. To help researchers avoid these pitfalls, we\nprovide a checklist for better practice."}, "http://arxiv.org/abs/2208.02028": {"title": "Bootstrap inference in the presence of bias", "link": "http://arxiv.org/abs/2208.02028", "description": "We consider bootstrap inference for estimators which are (asymptotically)\nbiased. We show that, even when the bias term cannot be consistently estimated,\nvalid inference can be obtained by proper implementations of the bootstrap.\nSpecifically, we show that the prepivoting approach of Beran (1987, 1988),\noriginally proposed to deliver higher-order refinements, restores bootstrap\nvalidity by transforming the original bootstrap p-value into an asymptotically\nuniform random variable. We propose two different implementations of\nprepivoting (plug-in and double bootstrap), and provide general high-level\nconditions that imply validity of bootstrap inference. To illustrate the\npractical relevance and implementation of our results, we discuss five\nexamples: (i) inference on a target parameter based on model averaging; (ii)\nridge-type regularized estimators; (iii) nonparametric regression; (iv) a\nlocation model for infinite variance data; and (v) dynamic panel data models."}, "http://arxiv.org/abs/2304.01273": {"title": "Heterogeneity-robust granular instruments", "link": "http://arxiv.org/abs/2304.01273", "description": "Granular instrumental variables (GIV) has experienced sharp growth in\nempirical macro-finance. The methodology's rise showcases granularity's\npotential for identification in a wide set of economic environments, like the\nestimation of spillovers and demand systems. I propose a new estimator--called\nrobust granular instrumental variables (RGIV)--that allows researchers to study\nunit-level heterogeneity in spillovers within GIV's framework. In contrast to\nGIV, RGIV also allows for unknown shock variances and does not require skewness\nof the size distribution of units. I also develop a test of overidentifying\nrestrictions that evaluates RGIV's compatibility with the data, a parameter\nrestriction test that evaluates the appropriateness of the homogeneous\nspillovers assumption, and extend the framework to allow for observable\nexplanatory variables. Applied to the Euro area, I find strong evidence of\ncountry-level heterogeneity in sovereign yield spillovers. In simulations, I\nshow that RGIV produces reliable and informative confidence intervals."}, "http://arxiv.org/abs/2309.11387": {"title": "Identifying Causal Effects in Information Provision Experiments", "link": "http://arxiv.org/abs/2309.11387", "description": "Information provision experiments are a popular way to study causal effects\nof beliefs on behavior. Researchers estimate these effects using TSLS. I show\nthat existing TSLS specifications do not estimate the average partial effect;\nthey have weights proportional to belief updating in the first-stage. If people\nwhose decisions depend on their beliefs gather information before the\nexperiment, the information treatment may shift beliefs more for people with\nweak belief effects. This attenuates TSLS estimates. I propose researchers use\na local-least-squares (LLS) estimator that I show consistently estimates the\naverage partial effect (APE) under Bayesian updating, and apply it to Settele\n(2022)."}, "http://arxiv.org/abs/2107.13737": {"title": "Design-Robust Two-Way-Fixed-Effects Regression For Panel Data", "link": "http://arxiv.org/abs/2107.13737", "description": "We propose a new estimator for average causal effects of a binary treatment\nwith panel data in settings with general treatment patterns. Our approach\naugments the popular two-way-fixed-effects specification with unit-specific\nweights that arise from a model for the assignment mechanism. We show how to\nconstruct these weights in various settings, including the staggered adoption\nsetting, where units opt into the treatment sequentially but permanently. The\nresulting estimator converges to an average (over units and time) treatment\neffect under the correct specification of the assignment model, even if the\nfixed effect model is misspecified. We show that our estimator is more robust\nthan the conventional two-way estimator: it remains consistent if either the\nassignment mechanism or the two-way regression model is correctly specified. In\naddition, the proposed estimator performs better than the two-way-fixed-effect\nestimator if the outcome model and assignment mechanism are locally\nmisspecified. This strong double robustness property underlines and quantifies\nthe benefits of modeling the assignment process and motivates using our\nestimator in practice. We also discuss an extension of our estimator to handle\ndynamic treatment effects."}, "http://arxiv.org/abs/2302.09756": {"title": "Identification-robust inference for the LATE with high-dimensional covariates", "link": "http://arxiv.org/abs/2302.09756", "description": "This paper presents an inference method for the local average treatment\neffect (LATE) in the presence of high-dimensional covariates, irrespective of\nthe strength of identification. We propose a novel high-dimensional conditional\ntest statistic with uniformly correct asymptotic size. We provide an\neasy-to-implement algorithm to infer the high-dimensional LATE by inverting our\ntest statistic and employing the double/debiased machine learning method.\nSimulations indicate that our test is robust against both weak identification\nand high dimensionality concerning size control and power performance,\noutperforming other conventional tests. Applying the proposed method to\nrailroad and population data to study the effect of railroad access on urban\npopulation growth, we observe that our methodology yields confidence intervals\nthat are 49% to 92% shorter than conventional results, depending on\nspecifications."}, "http://arxiv.org/abs/2311.05883": {"title": "Time-Varying Identification of Monetary Policy Shocks", "link": "http://arxiv.org/abs/2311.05883", "description": "We propose a new Bayesian heteroskedastic Markov-switching structural vector\nautoregression with data-driven time-varying identification. The model selects\nalternative exclusion restrictions over time and, as a condition for the\nsearch, allows to verify identification through heteroskedasticity within each\nregime. Based on four alternative monetary policy rules, we show that a monthly\nsix-variable system supports time variation in US monetary policy shock\nidentification. In the sample-dominating first regime, systematic monetary\npolicy follows a Taylor rule extended by the term spread and is effective in\ncurbing inflation. In the second regime, occurring after 2000 and gaining more\npersistence after the global financial and COVID crises, the Fed acts according\nto a money-augmented Taylor rule. This regime's unconventional monetary policy\nprovides economic stimulus, features the liquidity effect, and is complemented\nby a pure term spread shock. Absent the specific monetary policy of the second\nregime, inflation would be over one percentage point higher on average after\n2008."}, "http://arxiv.org/abs/2309.14160": {"title": "Unified Inference for Dynamic Quantile Predictive Regression", "link": "http://arxiv.org/abs/2309.14160", "description": "This paper develops unified asymptotic distribution theory for dynamic\nquantile predictive regressions which is useful when examining quantile\npredictability in stock returns under possible presence of nonstationarity."}, "http://arxiv.org/abs/2311.06256": {"title": "From Deep Filtering to Deep Econometrics", "link": "http://arxiv.org/abs/2311.06256", "description": "Calculating true volatility is an essential task for option pricing and risk\nmanagement. However, it is made difficult by market microstructure noise.\nParticle filtering has been proposed to solve this problem as it favorable\nstatistical properties, but relies on assumptions about underlying market\ndynamics. Machine learning methods have also been proposed but lack\ninterpretability, and often lag in performance. In this paper we implement the\nSV-PF-RNN: a hybrid neural network and particle filter architecture. Our\nSV-PF-RNN is designed specifically with stochastic volatility estimation in\nmind. We then show that it can improve on the performance of a basic particle\nfilter."}, "http://arxiv.org/abs/2311.06831": {"title": "Quasi-Bayes in Latent Variable Models", "link": "http://arxiv.org/abs/2311.06831", "description": "Latent variable models are widely used to account for unobserved determinants\nof economic behavior. Traditional nonparametric methods to estimate latent\nheterogeneity do not scale well into multidimensional settings. Distributional\nrestrictions alleviate tractability concerns but may impart non-trivial\nmisspecification bias. Motivated by these concerns, this paper introduces a\nquasi-Bayes approach to estimate a large class of multidimensional latent\nvariable models. Our approach to quasi-Bayes is novel in that we center it\naround relating the characteristic function of observables to the distribution\nof unobservables. We propose a computationally attractive class of priors that\nare supported on Gaussian mixtures and derive contraction rates for a variety\nof latent variable models."}, "http://arxiv.org/abs/2311.06891": {"title": "Design-based Estimation Theory for Complex Experiments", "link": "http://arxiv.org/abs/2311.06891", "description": "This paper considers the estimation of treatment effects in randomized\nexperiments with complex experimental designs, including cases with\ninterference between units. We develop a design-based estimation theory for\narbitrary experimental designs. Our theory facilitates the analysis of many\ndesign-estimator pairs that researchers commonly employ in practice and provide\nprocedures to consistently estimate asymptotic variance bounds. We propose new\nclasses of estimators with favorable asymptotic properties from a design-based\npoint of view. In addition, we propose a scalar measure of experimental\ncomplexity which can be linked to the design-based variance of the estimators.\nWe demonstrate the performance of our estimators using simulated datasets based\non an actual network experiment studying the effect of social networks on\ninsurance adoptions."}, "http://arxiv.org/abs/2311.07067": {"title": "High Dimensional Binary Choice Model with Unknown Heteroskedasticity or Instrumental Variables", "link": "http://arxiv.org/abs/2311.07067", "description": "This paper proposes a new method for estimating high-dimensional binary\nchoice models. The model we consider is semiparametric, placing no\ndistributional assumptions on the error term, allowing for heteroskedastic\nerrors, and permitting endogenous regressors. Our proposed approaches extend\nthe special regressor estimator originally proposed by Lewbel (2000). This\nestimator becomes impractical in high-dimensional settings due to the curse of\ndimensionality associated with high-dimensional conditional density estimation.\nTo overcome this challenge, we introduce an innovative data-driven dimension\nreduction method for nonparametric kernel estimators, which constitutes the\nmain innovation of this work. The method combines distance covariance-based\nscreening with cross-validation (CV) procedures, rendering the special\nregressor estimation feasible in high dimensions. Using the new feasible\nconditional density estimator, we address the variable and moment (instrumental\nvariable) selection problems for these models. We apply penalized least squares\n(LS) and Generalized Method of Moments (GMM) estimators with a smoothly clipped\nabsolute deviation (SCAD) penalty. A comprehensive analysis of the oracle and\nasymptotic properties of these estimators is provided. Monte Carlo simulations\nare employed to demonstrate the effectiveness of our proposed procedures in\nfinite sample scenarios."}, "http://arxiv.org/abs/2311.07243": {"title": "Optimal Estimation of Large-Dimensional Nonlinear Factor Models", "link": "http://arxiv.org/abs/2311.07243", "description": "This paper studies optimal estimation of large-dimensional nonlinear factor\nmodels. The key challenge is that the observed variables are possibly nonlinear\nfunctions of some latent variables where the functional forms are left\nunspecified. A local principal component analysis method is proposed to\nestimate the factor structure and recover information on latent variables and\nlatent functions, which combines $K$-nearest neighbors matching and principal\ncomponent analysis. Large-sample properties are established, including a sharp\nbound on the matching discrepancy of nearest neighbors, sup-norm error bounds\nfor estimated local factors and factor loadings, and the uniform convergence\nrate of the factor structure estimator. Under mild conditions our estimator of\nthe latent factor structure can achieve the optimal rate of uniform convergence\nfor nonparametric regression. The method is illustrated with a Monte Carlo\nexperiment and an empirical application studying the effect of tax cuts on\neconomic growth."}, "http://arxiv.org/abs/1902.09608": {"title": "On Binscatter", "link": "http://arxiv.org/abs/1902.09608", "description": "Binscatter is a popular method for visualizing bivariate relationships and\nconducting informal specification testing. We study the properties of this\nmethod formally and develop enhanced visualization and econometric binscatter\ntools. These include estimating conditional means with optimal binning and\nquantifying uncertainty. We also highlight a methodological problem related to\ncovariate adjustment that can yield incorrect conclusions. We revisit two\napplications using our methodology and find substantially different results\nrelative to those obtained using prior informal binscatter methods. General\npurpose software in Python, R, and Stata is provided. Our technical work is of\nindependent interest for the nonparametric partition-based estimation\nliterature."}, "http://arxiv.org/abs/2109.09043": {"title": "Composite Likelihood for Stochastic Migration Model with Unobserved Factor", "link": "http://arxiv.org/abs/2109.09043", "description": "We introduce the conditional Maximum Composite Likelihood (MCL) estimation\nmethod for the stochastic factor ordered Probit model of credit rating\ntransitions of firms. This model is recommended for internal credit risk\nassessment procedures in banks and financial institutions under the Basel III\nregulations. Its exact likelihood function involves a high-dimensional\nintegral, which can be approximated numerically before maximization. However,\nthe estimated migration risk and required capital tend to be sensitive to the\nquality of this approximation, potentially leading to statistical regulatory\narbitrage. The proposed conditional MCL estimator circumvents this problem and\nmaximizes the composite log-likelihood of the factor ordered Probit model. We\npresent three conditional MCL estimators of different complexity and examine\ntheir consistency and asymptotic normality when n and T tend to infinity. The\nperformance of these estimators at finite T is examined and compared with a\ngranularity-based approach in a simulation study. The use of the MCL estimator\nis also illustrated in an empirical application."}, "http://arxiv.org/abs/2111.01301": {"title": "Asymptotic in a class of network models with an increasing sub-Gamma degree sequence", "link": "http://arxiv.org/abs/2111.01301", "description": "For the differential privacy under the sub-Gamma noise, we derive the\nasymptotic properties of a class of network models with binary values with a\ngeneral link function. In this paper, we release the degree sequences of the\nbinary networks under a general noisy mechanism with the discrete Laplace\nmechanism as a special case. We establish the asymptotic result including both\nconsistency and asymptotically normality of the parameter estimator when the\nnumber of parameters goes to infinity in a class of network models. Simulations\nand a real data example are provided to illustrate asymptotic results."}, "http://arxiv.org/abs/2309.08982": {"title": "Least squares estimation in nonlinear cohort panels with learning from experience", "link": "http://arxiv.org/abs/2309.08982", "description": "We discuss techniques of estimation and inference for nonlinear cohort panels\nwith learning from experience, showing, inter alia, the consistency and\nasymptotic normality of the nonlinear least squares estimator employed in the\nseminal paper by Malmendier and Nagel (2016, QJE). Potential pitfalls for\nhypothesis testing are identified and solutions proposed. Monte Carlo\nsimulations verify the properties of the estimator and corresponding test\nstatistics in finite samples, while an application to a panel of survey\nexpectations demonstrates the usefulness of the theory developed."}, "http://arxiv.org/abs/2311.08218": {"title": "Estimating Conditional Value-at-Risk with Nonstationary Quantile Predictive Regression Models", "link": "http://arxiv.org/abs/2311.08218", "description": "This paper develops an asymptotic distribution theory for a two-stage\ninstrumentation estimation approach in quantile predictive regressions when\nboth generated covariates and persistent predictors are used. The generated\ncovariates are obtained from an auxiliary quantile regression model and our\nmain interest is the robust estimation and inference of the primary quantile\npredictive regression in which this generated covariate is added to the set of\nnonstationary regressors. We find that the proposed doubly IVX estimator is\nrobust to the abstract degree of persistence regardless of the presence of\ngenerated regressor obtained from the first stage procedure. The asymptotic\nproperties of the two-stage IVX estimator such as mixed Gaussianity are\nestablished while the asymptotic covariance matrix is adjusted to account for\nthe first-step estimation error."}, "http://arxiv.org/abs/2302.00469": {"title": "Regression adjustment in randomized controlled trials with many covariates", "link": "http://arxiv.org/abs/2302.00469", "description": "This paper is concerned with estimation and inference on average treatment\neffects in randomized controlled trials when researchers observe potentially\nmany covariates. By employing Neyman's (1923) finite population perspective, we\npropose a bias-corrected regression adjustment estimator using cross-fitting,\nand show that the proposed estimator has favorable properties over existing\nalternatives. For inference, we derive the first and second order terms in the\nstochastic component of the regression adjustment estimators, study higher\norder properties of the existing inference methods, and propose a\nbias-corrected version of the HC3 standard error. The proposed methods readily\nextend to stratified experiments with large strata. Simulation studies show our\ncross-fitted estimator, combined with the bias-corrected HC3, delivers precise\npoint estimates and robust size controls over a wide range of DGPs. To\nillustrate, the proposed methods are applied to real dataset on randomized\nexperiments of incentives and services for college achievement following\nAngrist, Lang, and Oreopoulos (2009)."}, "http://arxiv.org/abs/2311.08958": {"title": "Locally Asymptotically Minimax Statistical Treatment Rules Under Partial Identification", "link": "http://arxiv.org/abs/2311.08958", "description": "Policymakers often desire a statistical treatment rule (STR) that determines\na treatment assignment rule deployed in a future population from available\ndata. With the true knowledge of the data generating process, the average\ntreatment effect (ATE) is the key quantity characterizing the optimal treatment\nrule. Unfortunately, the ATE is often not point identified but partially\nidentified. Presuming the partial identification of the ATE, this study\nconducts a local asymptotic analysis and develops the locally asymptotically\nminimax (LAM) STR. The analysis does not assume the full differentiability but\nthe directional differentiability of the boundary functions of the\nidentification region of the ATE. Accordingly, the study shows that the LAM STR\ndiffers from the plug-in STR. A simulation study also demonstrates that the LAM\nSTR outperforms the plug-in STR."}, "http://arxiv.org/abs/2311.08963": {"title": "Incorporating Preferences Into Treatment Assignment Problems", "link": "http://arxiv.org/abs/2311.08963", "description": "This study investigates the problem of individualizing treatment allocations\nusing stated preferences for treatments. If individuals know in advance how the\nassignment will be individualized based on their stated preferences, they may\nstate false preferences. We derive an individualized treatment rule (ITR) that\nmaximizes welfare when individuals strategically state their preferences. We\nalso show that the optimal ITR is strategy-proof, that is, individuals do not\nhave a strong incentive to lie even if they know the optimal ITR a priori.\nConstructing the optimal ITR requires information on the distribution of true\npreferences and the average treatment effect conditioned on true preferences.\nIn practice, the information must be identified and estimated from the data. As\ntrue preferences are hidden information, the identification is not\nstraightforward. We discuss two experimental designs that allow the\nidentification: strictly strategy-proof randomized controlled trials and doubly\nrandomized preference trials. Under the presumption that data comes from one of\nthese experiments, we develop data-dependent procedures for determining ITR,\nthat is, statistical treatment rules (STRs). The maximum regret of the proposed\nSTRs converges to zero at a rate of the square root of the sample size. An\nempirical application demonstrates our proposed STRs."}, "http://arxiv.org/abs/2007.04267": {"title": "Difference-in-Differences Estimators of Intertemporal Treatment Effects", "link": "http://arxiv.org/abs/2007.04267", "description": "We study treatment-effect estimation using panel data. The treatment may be\nnon-binary, non-absorbing, and the outcome may be affected by treatment lags.\nWe make a parallel-trends assumption, and propose event-study estimators of the\neffect of being exposed to a weakly higher treatment dose for $\\ell$ periods.\nWe also propose normalized estimators, that estimate a weighted average of the\neffects of the current treatment and its lags. We also analyze commonly-used\ntwo-way-fixed-effects regressions. Unlike our estimators, they can be biased in\nthe presence of heterogeneous treatment effects. A local-projection version of\nthose regressions is biased even with homogeneous effects."}, "http://arxiv.org/abs/2007.10432": {"title": "Treatment Effects with Targeting Instruments", "link": "http://arxiv.org/abs/2007.10432", "description": "Multivalued treatments are commonplace in applications. We explore the use of\ndiscrete-valued instruments to control for selection bias in this setting. Our\ndiscussion revolves around the concept of targeting instruments: which\ninstruments target which treatments. It allows us to establish conditions under\nwhich counterfactual averages and treatment effects are point- or\npartially-identified for composite complier groups. We illustrate the\nusefulness of our framework by applying it to data from the Head Start Impact\nStudy. Under a plausible positive selection assumption, we derive informative\nbounds that suggest less beneficial effects of Head Start expansions than the\nparametric estimates of Kline and Walters (2016)."}, "http://arxiv.org/abs/2310.00786": {"title": "Semidiscrete optimal transport with unknown costs", "link": "http://arxiv.org/abs/2310.00786", "description": "Semidiscrete optimal transport is a challenging generalization of the\nclassical transportation problem in linear programming. The goal is to design a\njoint distribution for two random variables (one continuous, one discrete) with\nfixed marginals, in a way that minimizes expected cost. We formulate a novel\nvariant of this problem in which the cost functions are unknown, but can be\nlearned through noisy observations; however, only one function can be sampled\nat a time. We develop a semi-myopic algorithm that couples online learning with\nstochastic approximation, and prove that it achieves optimal convergence rates,\ndespite the non-smoothness of the stochastic gradient and the lack of strong\nconcavity in the objective function."}, "http://arxiv.org/abs/2311.09435": {"title": "Estimating Functionals of the Joint Distribution of Potential Outcomes with Optimal Transport", "link": "http://arxiv.org/abs/2311.09435", "description": "Many causal parameters depend on a moment of the joint distribution of\npotential outcomes. Such parameters are especially relevant in policy\nevaluation settings, where noncompliance is common and accommodated through the\nmodel of Imbens &amp; Angrist (1994). This paper shows that the sharp identified\nset for these parameters is an interval with endpoints characterized by the\nvalue of optimal transport problems. Sample analogue estimators are proposed\nbased on the dual problem of optimal transport. These estimators are root-n\nconsistent and converge in distribution under mild assumptions. Inference\nprocedures based on the bootstrap are straightforward and computationally\nconvenient. The ideas and estimators are demonstrated in an application\nrevisiting the National Supported Work Demonstration job training program. I\nfind suggestive evidence that workers who would see below average earnings\nwithout treatment tend to see above average benefits from treatment."}, "http://arxiv.org/abs/2311.09972": {"title": "Inference in Auctions with Many Bidders Using Transaction Prices", "link": "http://arxiv.org/abs/2311.09972", "description": "This paper considers inference in first-price and second-price sealed-bid\nauctions with a large number of symmetric bidders having independent private\nvalues. Given the abundance of bidders in each auction, we propose an\nasymptotic framework in which the number of bidders diverges while the number\nof auctions remains fixed. This framework allows us to perform asymptotically\nexact inference on key model features using only transaction price data.\nSpecifically, we examine inference on the expected utility of the auction\nwinner, the expected revenue of the seller, and the tail properties of the\nvaluation distribution. Simulations confirm the accuracy of our inference\nmethods in finite samples. Finally, we also apply them to Hong Kong car license\nauction data."}, "http://arxiv.org/abs/2212.06080": {"title": "Logs with zeros? Some problems and solutions", "link": "http://arxiv.org/abs/2212.06080", "description": "When studying an outcome $Y$ that is weakly-positive but can equal zero (e.g.\nearnings), researchers frequently estimate an average treatment effect (ATE)\nfor a \"log-like\" transformation that behaves like $\\log(Y)$ for large $Y$ but\nis defined at zero (e.g. $\\log(1+Y)$, $\\mathrm{arcsinh}(Y)$). We argue that\nATEs for log-like transformations should not be interpreted as approximating\npercentage effects, since unlike a percentage, they depend on the units of the\noutcome. In fact, we show that if the treatment affects the extensive margin,\none can obtain a treatment effect of any magnitude simply by re-scaling the\nunits of $Y$ before taking the log-like transformation. This arbitrary\nunit-dependence arises because an individual-level percentage effect is not\nwell-defined for individuals whose outcome changes from zero to non-zero when\nreceiving treatment, and the units of the outcome implicitly determine how much\nweight the ATE for a log-like transformation places on the extensive margin. We\nfurther establish a trilemma: when the outcome can equal zero, there is no\ntreatment effect parameter that is an average of individual-level treatment\neffects, unit-invariant, and point-identified. We discuss several alternative\napproaches that may be sensible in settings with an intensive and extensive\nmargin, including (i) expressing the ATE in levels as a percentage (e.g. using\nPoisson regression), (ii) explicitly calibrating the value placed on the\nintensive and extensive margins, and (iii) estimating separate effects for the\ntwo margins (e.g. using Lee bounds). We illustrate these approaches in three\nempirical applications."}, "http://arxiv.org/abs/2308.05486": {"title": "The Distributional Impact of Money Growth and Inflation Disaggregates: A Quantile Sensitivity Analysis", "link": "http://arxiv.org/abs/2308.05486", "description": "We propose an alternative method to construct a quantile dependence system\nfor inflation and money growth. By considering all quantiles, we assess how\nperturbations in one variable's quantile lead to changes in the distribution of\nthe other variable. We demonstrate the construction of this relationship\nthrough a system of linear quantile regressions. The proposed framework is\nexploited to examine the distributional effects of money growth on the\ndistributions of inflation and its disaggregate measures in the United States\nand the Euro area. Our empirical analysis uncovers significant impacts of the\nupper quantile of the money growth distribution on the distribution of\ninflation and its disaggregate measures. Conversely, we find that the lower and\nmedian quantiles of the money growth distribution have a negligible influence\non the distribution of inflation and its disaggregate measures."}, "http://arxiv.org/abs/2311.10685": {"title": "High-Throughput Asset Pricing", "link": "http://arxiv.org/abs/2311.10685", "description": "We use empirical Bayes (EB) to mine for out-of-sample returns among 73,108\nlong-short strategies constructed from accounting ratios, past returns, and\nticker symbols. EB predicts returns are concentrated in accounting and past\nreturn strategies, small stocks, and pre-2004 samples. The cross-section of\nout-of-sample return lines up closely with EB predictions. Data-mined\nportfolios have mean returns comparable with published portfolios, but the\ndata-mined returns are arguably free of data mining bias. In contrast,\ncontrolling for multiple testing following Harvey, Liu, and Zhu (2016) misses\nthe vast majority of returns. This \"high-throughput asset pricing\" provides an\nevidence-based solution for data mining bias."}, "http://arxiv.org/abs/2209.11444": {"title": "Identification of the Marginal Treatment Effect with Multivalued Treatments", "link": "http://arxiv.org/abs/2209.11444", "description": "The multinomial choice model based on utility maximization has been widely\nused to select treatments. In this paper, we establish sufficient conditions\nfor the identification of the marginal treatment effects with multivalued\ntreatments. Our result reveals treatment effects conditioned on the willingness\nto participate in treatments against a specific treatment. Further, our results\ncan identify other parameters such as the marginal distribution of potential\noutcomes."}, "http://arxiv.org/abs/2211.04027": {"title": "Bootstraps for Dynamic Panel Threshold Models", "link": "http://arxiv.org/abs/2211.04027", "description": "This paper develops valid bootstrap inference methods for the dynamic panel\nthreshold regression. For the first-differenced generalized method of moments\n(GMM) estimation for the dynamic short panel, we show that the standard\nnonparametric bootstrap is inconsistent. The inconsistency is due to an\n$n^{1/4}$-consistent non-normal asymptotic distribution for the threshold\nestimate when the parameter resides within the continuity region of the\nparameter space, which stems from the rank deficiency of the approximate\nJacobian of the sample moment conditions on the continuity region. We propose a\ngrid bootstrap to construct confidence sets for the threshold, a residual\nbootstrap to construct confidence intervals for the coefficients, and a\nbootstrap for testing continuity. They are shown to be valid under uncertain\ncontinuity. A set of Monte Carlo experiments demonstrate that the proposed\nbootstraps perform well in the finite samples and improve upon the asymptotic\nnormal approximation even under a large jump at the threshold. An empirical\napplication to firms' investment model illustrates our methods."}, "http://arxiv.org/abs/2304.07480": {"title": "Gini-stable Lorenz curves and their relation to the generalised Pareto distribution", "link": "http://arxiv.org/abs/2304.07480", "description": "We introduce an iterative discrete information production process where we\ncan extend ordered normalised vectors by new elements based on a simple affine\ntransformation, while preserving the predefined level of inequality, G, as\nmeasured by the Gini index.\n\nThen, we derive the family of empirical Lorenz curves of the corresponding\nvectors and prove that it is stochastically ordered with respect to both the\nsample size and G which plays the role of the uncertainty parameter. We prove\nthat asymptotically, we obtain all, and only, Lorenz curves generated by a new,\nintuitive parametrisation of the finite-mean Pickands' Generalised Pareto\nDistribution (GPD) that unifies three other families, namely: the Pareto Type\nII, exponential, and scaled beta ones. The family is not only totally ordered\nwith respect to the parameter G, but also, thanks to our derivations, has a\nnice underlying interpretation. Our result may thus shed a new light on the\ngenesis of this family of distributions.\n\nOur model fits bibliometric, informetric, socioeconomic, and environmental\ndata reasonably well. It is quite user-friendly for it only depends on the\nsample size and its Gini index."}, "http://arxiv.org/abs/2311.11637": {"title": "Modeling economies of scope in joint production: Convex regression of input distance function", "link": "http://arxiv.org/abs/2311.11637", "description": "Modeling of joint production has proved a vexing problem. This paper develops\na radial convex nonparametric least squares (CNLS) approach to estimate the\ninput distance function with multiple outputs. We document the correct input\ndistance function transformation and prove that the necessary orthogonality\nconditions can be satisfied in radial CNLS. A Monte Carlo study is performed to\ncompare the finite sample performance of radial CNLS and other deterministic\nand stochastic frontier approaches in terms of the input distance function\nestimation. We apply our novel approach to the Finnish electricity distribution\nnetwork regulation and empirically confirm that the input isoquants become more\ncurved. In addition, we introduce the weight restriction to radial CNLS to\nmitigate the potential overfitting and increase the out-of-sample performance\nin energy regulation."}, "http://arxiv.org/abs/2311.11858": {"title": "Theory coherent shrinkage of Time-Varying Parameters in VARs", "link": "http://arxiv.org/abs/2311.11858", "description": "Time-Varying Parameters Vector Autoregressive (TVP-VAR) models are frequently\nused in economics to capture evolving relationships among the macroeconomic\nvariables. However, TVP-VARs have the tendency of overfitting the data,\nresulting in inaccurate forecasts and imprecise estimates of typical objects of\ninterests such as the impulse response functions. This paper introduces a\nTheory Coherent Time-Varying Parameters Vector Autoregressive Model\n(TC-TVP-VAR), which leverages on an arbitrary theoretical framework derived by\nan underlying economic theory to form a prior for the time varying parameters.\nThis \"theory coherent\" shrinkage prior significantly improves inference\nprecision and forecast accuracy over the standard TVP-VAR. Furthermore, the\nTC-TVP-VAR can be used to perform indirect posterior inference on the deep\nparameters of the underlying economic theory. The paper reveals that using the\nclassical 3-equation New Keynesian block to form a prior for the TVP- VAR\nsubstantially enhances forecast accuracy of output growth and of the inflation\nrate in a standard model of monetary policy. Additionally, the paper shows that\nthe TC-TVP-VAR can be used to address the inferential challenges during the\nZero Lower Bound period."}, "http://arxiv.org/abs/2007.07842": {"title": "Persistence in Financial Connectedness and Systemic Risk", "link": "http://arxiv.org/abs/2007.07842", "description": "This paper characterises dynamic linkages arising from shocks with\nheterogeneous degrees of persistence. Using frequency domain techniques, we\nintroduce measures that identify smoothly varying links of a transitory and\npersistent nature. Our approach allows us to test for statistical differences\nin such dynamic links. We document substantial differences in transitory and\npersistent linkages among US financial industry volatilities, argue that they\ntrack heterogeneously persistent sources of systemic risk, and thus may serve\nas a useful tool for market participants."}, "http://arxiv.org/abs/2205.11365": {"title": "Graph-Based Methods for Discrete Choice", "link": "http://arxiv.org/abs/2205.11365", "description": "Choices made by individuals have widespread impacts--for instance, people\nchoose between political candidates to vote for, between social media posts to\nshare, and between brands to purchase--moreover, data on these choices are\nincreasingly abundant. Discrete choice models are a key tool for learning\nindividual preferences from such data. Additionally, social factors like\nconformity and contagion influence individual choice. Traditional methods for\nincorporating these factors into choice models do not account for the entire\nsocial network and require hand-crafted features. To overcome these\nlimitations, we use graph learning to study choice in networked contexts. We\nidentify three ways in which graph learning techniques can be used for discrete\nchoice: learning chooser representations, regularizing choice model parameters,\nand directly constructing predictions from a network. We design methods in each\ncategory and test them on real-world choice datasets, including county-level\n2016 US election results and Android app installation and usage data. We show\nthat incorporating social network structure can improve the predictions of the\nstandard econometric choice model, the multinomial logit. We provide evidence\nthat app installations are influenced by social context, but we find no such\neffect on app usage among the same participants, which instead is habit-driven.\nIn the election data, we highlight the additional insights a discrete choice\nframework provides over classification or regression, the typical approaches.\nOn synthetic data, we demonstrate the sample complexity benefit of using social\ninformation in choice models."}, "http://arxiv.org/abs/2306.09287": {"title": "Modelling and Forecasting Macroeconomic Risk with Time Varying Skewness Stochastic Volatility Models", "link": "http://arxiv.org/abs/2306.09287", "description": "Monitoring downside risk and upside risk to the key macroeconomic indicators\nis critical for effective policymaking aimed at maintaining economic stability.\nIn this paper I propose a parametric framework for modelling and forecasting\nmacroeconomic risk based on stochastic volatility models with Skew-Normal and\nSkew-t shocks featuring time varying skewness. Exploiting a mixture stochastic\nrepresentation of the Skew-Normal and Skew-t random variables, in the paper I\ndevelop efficient posterior simulation samplers for Bayesian estimation of both\nunivariate and VAR models of this type. In an application, I use the models to\npredict downside risk to GDP growth in the US and I show that these models\nrepresent a competitive alternative to semi-parametric approaches such as\nquantile regression. Finally, estimating a medium scale VAR on US data I show\nthat time varying skewness is a relevant feature of macroeconomic and financial\nshocks."}, "http://arxiv.org/abs/2309.07476": {"title": "Causal inference in network experiments: regression-based analysis and design-based properties", "link": "http://arxiv.org/abs/2309.07476", "description": "Investigating interference or spillover effects among units is a central task\nin many social science problems. Network experiments are powerful tools for\nthis task, which avoids endogeneity by randomly assigning treatments to units\nover networks. However, it is non-trivial to analyze network experiments\nproperly without imposing strong modeling assumptions. Previously, many\nresearchers have proposed sophisticated point estimators and standard errors\nfor causal effects under network experiments. We further show that\nregression-based point estimators and standard errors can have strong\ntheoretical guarantees if the regression functions and robust standard errors\nare carefully specified to accommodate the interference patterns under network\nexperiments. We first recall a well-known result that the Hajek estimator is\nnumerically identical to the coefficient from the weighted-least-squares fit\nbased on the inverse probability of the exposure mapping. Moreover, we\ndemonstrate that the regression-based approach offers three notable advantages:\nits ease of implementation, the ability to derive standard errors through the\nsame weighted-least-squares fit, and the capacity to integrate covariates into\nthe analysis, thereby enhancing estimation efficiency. Furthermore, we analyze\nthe asymptotic bias of the regression-based network-robust standard errors.\nRecognizing that the covariance estimator can be anti-conservative, we propose\nan adjusted covariance estimator to improve the empirical coverage rates.\nAlthough we focus on regression-based point estimators and standard errors, our\ntheory holds under the design-based framework, which assumes that the\nrandomness comes solely from the design of network experiments and allows for\narbitrary misspecification of the regression models."}, "http://arxiv.org/abs/2311.12267": {"title": "Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity", "link": "http://arxiv.org/abs/2311.12267", "description": "This paper studies causal representation learning, the task of recovering\nhigh-level latent variables and their causal relationships from low-level data\nthat we observe, assuming access to observations generated from multiple\nenvironments. While existing works are able to prove full identifiability of\nthe underlying data generating process, they typically assume access to\nsingle-node, hard interventions which is rather unrealistic in practice. The\nmain contribution of this paper is characterize a notion of identifiability\nwhich is provably the best one can achieve when hard interventions are not\navailable. First, for linear causal models, we provide identifiability\nguarantee for data observed from general environments without assuming any\nsimilarities between them. While the causal graph is shown to be fully\nrecovered, the latent variables are only identified up to an effect-domination\nambiguity (EDA). We then propose an algorithm, LiNGCReL which is guaranteed to\nrecover the ground-truth model up to EDA, and we demonstrate its effectiveness\nvia numerical experiments. Moving on to general non-parametric causal models,\nwe prove the same idenfifiability guarantee assuming access to groups of soft\ninterventions. Finally, we provide counterparts of our identifiability results,\nindicating that EDA is basically inevitable in our setting."}, "http://arxiv.org/abs/2311.12671": {"title": "Predictive Density Combination Using a Tree-Based Synthesis Function", "link": "http://arxiv.org/abs/2311.12671", "description": "Bayesian predictive synthesis (BPS) provides a method for combining multiple\npredictive distributions based on agent/expert opinion analysis theory and\nencompasses a range of existing density forecast pooling methods. The key\ningredient in BPS is a ``synthesis'' function. This is typically specified\nparametrically as a dynamic linear regression. In this paper, we develop a\nnonparametric treatment of the synthesis function using regression trees. We\nshow the advantages of our tree-based approach in two macroeconomic forecasting\napplications. The first uses density forecasts for GDP growth from the euro\narea's Survey of Professional Forecasters. The second combines density\nforecasts of US inflation produced by many regression models involving\ndifferent predictors. Both applications demonstrate the benefits -- in terms of\nimproved forecast accuracy and interpretability -- of modeling the synthesis\nfunction nonparametrically."}, "http://arxiv.org/abs/1810.00283": {"title": "Proxy Controls and Panel Data", "link": "http://arxiv.org/abs/1810.00283", "description": "We provide new results for nonparametric identification, estimation, and\ninference of causal effects using `proxy controls': observables that are noisy\nbut informative proxies for unobserved confounding factors. Our analysis\napplies to cross-sectional settings but is particularly well-suited to panel\nmodels. Our identification results motivate a simple and `well-posed'\nnonparametric estimator. We derive convergence rates for the estimator and\nconstruct uniform confidence bands with asymptotically correct size. In panel\nsettings, our methods provide a novel approach to the difficult problem of\nidentification with non-separable, general heterogeneity and fixed $T$. In\npanels, observations from different periods serve as proxies for unobserved\nheterogeneity and our key identifying assumptions follow from restrictions on\nthe serial dependence structure. We apply our methods to two empirical\nsettings. We estimate consumer demand counterfactuals using panel data and we\nestimate causal effects of grade retention on cognitive performance."}, "http://arxiv.org/abs/2102.08809": {"title": "Testing for Nonlinear Cointegration under Heteroskedasticity", "link": "http://arxiv.org/abs/2102.08809", "description": "This article discusses tests for nonlinear cointegration in the presence of\nvariance breaks. We build on cointegration test approaches under\nheteroskedasticity (Cavaliere and Taylor, 2006, Journal of Time Series\nAnalysis) and for nonlinearity (Choi and Saikkonen, 2010, Econometric Theory)\nto propose a bootstrap test and prove its consistency. A Monte Carlo study\nshows the approach to have good finite sample properties. We provide an\nempirical application to the environmental Kuznets curves (EKC), finding that\nthe cointegration test provides little evidence for the EKC hypothesis.\nAdditionally, we examine the nonlinear relation between the US money and the\ninterest rate, finding that our test does not reject the null of a smooth\ntransition cointegrating relation."}, "http://arxiv.org/abs/2311.12878": {"title": "Adaptive Bayesian Learning with Action and State-Dependent Signal Variance", "link": "http://arxiv.org/abs/2311.12878", "description": "This manuscript presents an advanced framework for Bayesian learning by\nincorporating action and state-dependent signal variances into decision-making\nmodels. This framework is pivotal in understanding complex data-feedback loops\nand decision-making processes in various economic systems. Through a series of\nexamples, we demonstrate the versatility of this approach in different\ncontexts, ranging from simple Bayesian updating in stable environments to\ncomplex models involving social learning and state-dependent uncertainties. The\npaper uniquely contributes to the understanding of the nuanced interplay\nbetween data, actions, outcomes, and the inherent uncertainty in economic\nmodels."}, "http://arxiv.org/abs/2311.13327": {"title": "Regressions under Adverse Conditions", "link": "http://arxiv.org/abs/2311.13327", "description": "We introduce a new regression method that relates the mean of an outcome\nvariable to covariates, given the \"adverse condition\" that a distress variable\nfalls in its tail. This allows to tailor classical mean regressions to adverse\neconomic scenarios, which receive increasing interest in managing macroeconomic\nand financial risks, among many others. In the terminology of the systemic risk\nliterature, our method can be interpreted as a regression for the Marginal\nExpected Shortfall. We propose a two-step procedure to estimate the new models,\nshow consistency and asymptotic normality of the estimator, and propose\nfeasible inference under weak conditions allowing for cross-sectional and time\nseries applications. The accuracy of the asymptotic approximations of the\ntwo-step estimator is verified in simulations. Two empirical applications show\nthat our regressions under adverse conditions are valuable in such diverse\nfields as the study of the relation between systemic risk and asset price\nbubbles, and dissecting macroeconomic growth vulnerabilities into individual\ncomponents."}, "http://arxiv.org/abs/2311.13575": {"title": "Large-Sample Properties of the Synthetic Control Method under Selection on Unobservables", "link": "http://arxiv.org/abs/2311.13575", "description": "We analyze the properties of the synthetic control (SC) method in settings\nwith a large number of units. We assume that the selection into treatment is\nbased on unobserved permanent heterogeneity and pretreatment information, thus\nallowing for both strictly and sequentially exogenous assignment processes.\nExploiting duality, we interpret the solution of the SC optimization problem as\nan estimator for the underlying treatment probabilities. We use this to derive\nthe asymptotic representation for the SC method and characterize sufficient\nconditions for its asymptotic normality. We show that the critical property\nthat determines the behavior of the SC method is the ability of input features\nto approximate the unobserved heterogeneity. Our results imply that the SC\nmethod delivers asymptotically normal estimators for a large class of linear\npanel data models as long as the number of pretreatment periods is large,\nmaking it a natural alternative to conventional methods built on the\nDifference-in-Differences."}, "http://arxiv.org/abs/2108.00723": {"title": "Partial Identification and Inference for Conditional Distributions of Treatment Effects", "link": "http://arxiv.org/abs/2108.00723", "description": "This paper considers identification and inference for the distribution of\ntreatment effects conditional on observable covariates. Since the conditional\ndistribution of treatment effects is not point identified without strong\nassumptions, we obtain bounds on the conditional distribution of treatment\neffects by using the Makarov bounds. We also consider the case where the\ntreatment is endogenous and propose two stochastic dominance assumptions to\ntighten the bounds. We develop a nonparametric framework to estimate the bounds\nand establish the asymptotic theory that is uniformly valid over the support of\ntreatment effects. An empirical example illustrates the usefulness of the\nmethods."}, "http://arxiv.org/abs/2311.13969": {"title": "Was Javert right to be suspicious? Unpacking treatment effect heterogeneity of alternative sentences on time-to-recidivism in Brazil", "link": "http://arxiv.org/abs/2311.13969", "description": "This paper presents new econometric tools to unpack the treatment effect\nheterogeneity of punishing misdemeanor offenses on time-to-recidivism. We show\nhow one can identify, estimate, and make inferences on the distributional,\nquantile, and average marginal treatment effects in setups where the treatment\nselection is endogenous and the outcome of interest, usually a duration\nvariable, is potentially right-censored. We explore our proposed econometric\nmethodology to evaluate the effect of fines and community service sentences as\na form of punishment on time-to-recidivism in the State of S\\~ao Paulo, Brazil,\nbetween 2010 and 2019, leveraging the as-if random assignment of judges to\ncases. Our results highlight substantial treatment effect heterogeneity that\nother tools are not meant to capture. For instance, we find that people whom\nmost judges would punish take longer to recidivate as a consequence of the\npunishment, while people who would be punished only by strict judges recidivate\nat an earlier date than if they were not punished. This result suggests that\ndesigning sentencing guidelines that encourage strict judges to become more\nlenient could reduce recidivism."}, "http://arxiv.org/abs/2311.14032": {"title": "Counterfactual Sensitivity in Equilibrium Models", "link": "http://arxiv.org/abs/2311.14032", "description": "Counterfactuals in equilibrium models are functions of the current state of\nthe world, the exogenous change variables and the model parameters. Current\npractice treats the current state of the world, the observed data, as perfectly\nmeasured, but there is good reason to believe that they are measured with\nerror. The main aim of this paper is to provide tools for quantifying\nuncertainty about counterfactuals, when the current state of the world is\nmeasured with error. I propose two methods, a Bayesian approach and an\nadversarial approach. Both methods are practical and theoretically justified. I\napply the two methods to the application in Adao et al. (2017) and find\nnon-trivial uncertainty about counterfactuals."}, "http://arxiv.org/abs/2311.14204": {"title": "Reproducible Aggregation of Sample-Split Statistics", "link": "http://arxiv.org/abs/2311.14204", "description": "Statistical inference is often simplified by sample-splitting. This\nsimplification comes at the cost of the introduction of randomness that is not\nnative to the data. We propose a simple procedure for sequentially aggregating\nstatistics constructed with multiple splits of the same sample. The user\nspecifies a bound and a nominal error rate. If the procedure is implemented\ntwice on the same data, the nominal error rate approximates the chance that the\nresults differ by more than the bound. We provide a non-asymptotic analysis of\nthe accuracy of the nominal error rate and illustrate the application of the\nprocedure to several widely applied statistical methods."}, "http://arxiv.org/abs/2205.03288": {"title": "Leverage, Influence, and the Jackknife in Clustered Regression Models: Reliable Inference Using summclust", "link": "http://arxiv.org/abs/2205.03288", "description": "We introduce a new Stata package called summclust that summarizes the cluster\nstructure of the dataset for linear regression models with clustered\ndisturbances. The key unit of observation for such a model is the cluster. We\ntherefore propose cluster-level measures of leverage, partial leverage, and\ninfluence and show how to compute them quickly in most cases. The measures of\nleverage and partial leverage can be used as diagnostic tools to identify\ndatasets and regression designs in which cluster-robust inference is likely to\nbe challenging. The measures of influence can provide valuable information\nabout how the results depend on the data in the various clusters. We also show\nhow to calculate two jackknife variance matrix estimators efficiently as a\nbyproduct of our other computations. These estimators, which are already\navailable in Stata, are generally more conservative than conventional variance\nmatrix estimators. The summclust package computes all the quantities that we\ndiscuss."}, "http://arxiv.org/abs/2205.10310": {"title": "Treatment Effects in Bunching Designs: The Impact of Mandatory Overtime Pay on Hours", "link": "http://arxiv.org/abs/2205.10310", "description": "The 1938 Fair Labor Standards Act mandates overtime premium pay for most U.S.\nworkers, but it has proven difficult to assess the policy's impact on the labor\nmarket because the rule applies nationally and has varied little over time. I\nuse the extent to which firms bunch workers at the overtime threshold of 40\nhours in a week to estimate the rule's effect on hours, drawing on data from\nindividual workers' weekly paychecks. To do so I generalize a popular\nidentification strategy that exploits bunching at kink points in a\ndecision-maker's choice set. Making only nonparametric assumptions about\npreferences and heterogeneity, I show that the average causal response among\nbunchers to the policy switch at the kink is partially identified. The bounds\nindicate a relatively small elasticity of demand for weekly hours, suggesting\nthat the overtime mandate has a discernible but limited impact on hours and\nemployment."}, "http://arxiv.org/abs/2305.19484": {"title": "A Simple Method for Predicting Covariance Matrices of Financial Returns", "link": "http://arxiv.org/abs/2305.19484", "description": "We consider the well-studied problem of predicting the time-varying\ncovariance matrix of a vector of financial returns. Popular methods range from\nsimple predictors like rolling window or exponentially weighted moving average\n(EWMA) to more sophisticated predictors such as generalized autoregressive\nconditional heteroscedastic (GARCH) type methods. Building on a specific\ncovariance estimator suggested by Engle in 2002, we propose a relatively simple\nextension that requires little or no tuning or fitting, is interpretable, and\nproduces results at least as good as MGARCH, a popular extension of GARCH that\nhandles multiple assets. To evaluate predictors we introduce a novel approach,\nevaluating the regret of the log-likelihood over a time period such as a\nquarter. This metric allows us to see not only how well a covariance predictor\ndoes over all, but also how quickly it reacts to changes in market conditions.\nOur simple predictor outperforms MGARCH in terms of regret. We also test\ncovariance predictors on downstream applications such as portfolio optimization\nmethods that depend on the covariance matrix. For these applications our simple\ncovariance predictor and MGARCH perform similarly."}, "http://arxiv.org/abs/2311.14698": {"title": "Business Policy Experiments using Fractional Factorial Designs: Consumer Retention on DoorDash", "link": "http://arxiv.org/abs/2311.14698", "description": "This paper investigates an approach to both speed up business decision-making\nand lower the cost of learning through experimentation by factorizing business\npolicies and employing fractional factorial experimental designs for their\nevaluation. We illustrate how this method integrates with advances in the\nestimation of heterogeneous treatment effects, elaborating on its advantages\nand foundational assumptions. We empirically demonstrate the implementation and\nbenefits of our approach and assess its validity in evaluating consumer\npromotion policies at DoorDash, which is one of the largest delivery platforms\nin the US. Our approach discovers a policy with 5% incremental profit at 67%\nlower implementation cost."}, "http://arxiv.org/abs/2311.14813": {"title": "A Review of Cross-Sectional Matrix Exponential Spatial Models", "link": "http://arxiv.org/abs/2311.14813", "description": "The matrix exponential spatial models exhibit similarities to the\nconventional spatial autoregressive model in spatial econometrics but offer\nanalytical, computational, and interpretive advantages. This paper provides a\ncomprehensive review of the literature on the estimation, inference, and model\nselection approaches for the cross-sectional matrix exponential spatial models.\nWe discuss summary measures for the marginal effects of regressors and detail\nthe matrix-vector product method for efficient estimation. Our aim is not only\nto summarize the main findings from the spatial econometric literature but also\nto make them more accessible to applied researchers. Additionally, we\ncontribute to the literature by introducing some new results. We propose an\nM-estimation approach for models with heteroskedastic error terms and\ndemonstrate that the resulting M-estimator is consistent and has an asymptotic\nnormal distribution. We also consider some new results for model selection\nexercises. In a Monte Carlo study, we examine the finite sample properties of\nvarious estimators from the literature alongside the M-estimator."}, "http://arxiv.org/abs/2311.14892": {"title": "An Identification and Dimensionality Robust Test for Instrumental Variables Models", "link": "http://arxiv.org/abs/2311.14892", "description": "I propose a new identification-robust test for the structural parameter in a\nheteroskedastic linear instrumental variables model. The proposed test\nstatistic is similar in spirit to a jackknife version of the K-statistic and\nthe resulting test has exact asymptotic size so long as an auxiliary parameter\ncan be consistently estimated. This is possible under approximate sparsity even\nwhen the number of instruments is much larger than the sample size. As the\nnumber of instruments is allowed, but not required, to be large, the limiting\nbehavior of the test statistic is difficult to examine via existing central\nlimit theorems. Instead, I derive the asymptotic chi-squared distribution of\nthe test statistic using a direct Gaussian approximation technique. To improve\npower against certain alternatives, I propose a simple combination with the\nsup-score statistic of Belloni et al. (2012) based on a thresholding rule. I\ndemonstrate favorable size control and power properties in a simulation study\nand apply the new methods to revisit the effect of social spillovers in movie\nconsumption."}, "http://arxiv.org/abs/2311.15458": {"title": "Causal Models for Longitudinal and Panel Data: A Survey", "link": "http://arxiv.org/abs/2311.15458", "description": "This survey discusses the recent causal panel data literature. This recent\nliterature has focused on credibly estimating causal effects of binary\ninterventions in settings with longitudinal data, with an emphasis on practical\nadvice for empirical researchers. It pays particular attention to heterogeneity\nin the causal effects, often in situations where few units are treated. The\nliterature has extended earlier work on difference-in-differences or\ntwo-way-fixed-effect estimators and more generally incorporated factor models\nor interactive fixed effects. It has also developed novel methods using\nsynthetic control approaches."}, "http://arxiv.org/abs/2311.15829": {"title": "(Frisch-Waugh-Lovell)': On the Estimation of Regression Models by Row", "link": "http://arxiv.org/abs/2311.15829", "description": "We demonstrate that regression models can be estimated by working\nindependently in a row-wise fashion. We document a simple procedure which\nallows for a wide class of econometric estimators to be implemented\ncumulatively, where, in the limit, estimators can be produced without ever\nstoring more than a single line of data in a computer's memory. This result is\nuseful in understanding the mechanics of many common regression models. These\nprocedures can be used to speed up the computation of estimates computed via\nOLS, IV, Ridge regression, LASSO, Elastic Net, and Non-linear models including\nprobit and logit, with all common modes of inference. This has implications for\nestimation and inference with `big data', where memory constraints may imply\nthat working with all data at once is particularly costly. We additionally show\nthat even with moderately sized datasets, this method can reduce computation\ntime compared with traditional estimation routines."}, "http://arxiv.org/abs/2311.15871": {"title": "On Quantile Treatment Effects, Rank Similarity, and Variation of Instrumental Variables", "link": "http://arxiv.org/abs/2311.15871", "description": "This paper investigates how certain relationship between observed and\ncounterfactual distributions serves as an identifying condition for treatment\neffects when the treatment is endogenous, and shows that this condition holds\nin a range of nonparametric models for treatment effects. To this end, we first\nprovide a novel characterization of the prevalent assumption restricting\ntreatment heterogeneity in the literature, namely rank similarity. Our\ncharacterization demonstrates the stringency of this assumption and allows us\nto relax it in an economically meaningful way, resulting in our identifying\ncondition. It also justifies the quest of richer exogenous variations in the\ndata (e.g., multi-valued or multiple instrumental variables) in exchange for\nweaker identifying conditions. The primary goal of this investigation is to\nprovide empirical researchers with tools that are robust and easy to implement\nbut still yield tight policy evaluations."}, "http://arxiv.org/abs/2311.15878": {"title": "Individualized Treatment Allocations with Distributional Welfare", "link": "http://arxiv.org/abs/2311.15878", "description": "In this paper, we explore optimal treatment allocation policies that target\ndistributional welfare. Most literature on treatment choice has considered\nutilitarian welfare based on the conditional average treatment effect (ATE).\nWhile average welfare is intuitive, it may yield undesirable allocations\nespecially when individuals are heterogeneous (e.g., with outliers) - the very\nreason individualized treatments were introduced in the first place. This\nobservation motivates us to propose an optimal policy that allocates the\ntreatment based on the conditional \\emph{quantile of individual treatment\neffects} (QoTE). Depending on the choice of the quantile probability, this\ncriterion can accommodate a policymaker who is either prudent or negligent. The\nchallenge of identifying the QoTE lies in its requirement for knowledge of the\njoint distribution of the counterfactual outcomes, which is generally hard to\nrecover even with experimental data. Therefore, we introduce minimax optimal\npolicies that are robust to model uncertainty. We then propose a range of\nidentifying assumptions under which we can point or partially identify the\nQoTE. We establish the asymptotic bound on the regret of implementing the\nproposed policies. We consider both stochastic and deterministic rules. In\nsimulations and two empirical applications, we compare optimal decisions based\non the QoTE with decisions based on other criteria."}, "http://arxiv.org/abs/2311.15932": {"title": "Valid Wald Inference with Many Weak Instruments", "link": "http://arxiv.org/abs/2311.15932", "description": "This paper proposes three novel test procedures that yield valid inference in\nan environment with many weak instrumental variables (MWIV). It is observed\nthat the t statistic of the jackknife instrumental variable estimator (JIVE)\nhas an asymptotic distribution that is identical to the two-stage-least squares\n(TSLS) t statistic in the just-identified environment. Consequently, test\nprocedures that were valid for TSLS t are also valid for the JIVE t. Two such\nprocedures, i.e., VtF and conditional Wald, are adapted directly. By exploiting\na feature of MWIV environments, a third, more powerful, one-sided VtF-based\ntest procedure can be obtained."}, "http://arxiv.org/abs/2311.15952": {"title": "Robust Conditional Wald Inference for Over-Identified IV", "link": "http://arxiv.org/abs/2311.15952", "description": "For the over-identified linear instrumental variables model, researchers\ncommonly report the 2SLS estimate along with the robust standard error and seek\nto conduct inference with these quantities. If errors are homoskedastic, one\ncan control the degree of inferential distortion using the first-stage F\ncritical values from Stock and Yogo (2005), or use the robust-to-weak\ninstruments Conditional Wald critical values of Moreira (2003). If errors are\nnon-homoskedastic, these methods do not apply. We derive the generalization of\nConditional Wald critical values that is robust to non-homoskedastic errors\n(e.g., heteroskedasticity or clustered variance structures), which can also be\napplied to nonlinear weakly-identified models (e.g. weakly-identified GMM)."}, "http://arxiv.org/abs/1801.00332": {"title": "Confidence set for group membership", "link": "http://arxiv.org/abs/1801.00332", "description": "Our confidence set quantifies the statistical uncertainty from data-driven\ngroup assignments in grouped panel models. It covers the true group memberships\njointly for all units with pre-specified probability and is constructed by\ninverting many simultaneous unit-specific one-sided tests for group membership.\nWe justify our approach under $N, T \\to \\infty$ asymptotics using tools from\nhigh-dimensional statistics, some of which we extend in this paper. We provide\nMonte Carlo evidence that the confidence set has adequate coverage in finite\nsamples.An empirical application illustrates the use of our confidence set."}, "http://arxiv.org/abs/2004.05027": {"title": "Direct and spillover effects of a new tramway line on the commercial vitality of peripheral streets", "link": "http://arxiv.org/abs/2004.05027", "description": "In cities, the creation of public transport infrastructure such as light\nrails can cause changes on a very detailed spatial scale, with different\nstories unfolding next to each other within a same urban neighborhood. We study\nthe direct effect of a light rail line built in Florence (Italy) on the retail\ndensity of the street where it was built and and its spillover effect on other\nstreets in the treated street's neighborhood. To this aim, we investigate the\nuse of the Synthetic Control Group (SCG) methods in panel comparative case\nstudies where interference between the treated and the untreated units is\nplausible, an issue still little researched in the SCG methodological\nliterature. We frame our discussion in the potential outcomes approach. Under a\npartial interference assumption, we formally define relevant direct and\nspillover causal effects. We also consider the ``unrealized'' spillover effect\non the treated street in the hypothetical scenario that another street in the\ntreated unit's neighborhood had been assigned to the intervention."}, "http://arxiv.org/abs/2004.09458": {"title": "Noise-Induced Randomization in Regression Discontinuity Designs", "link": "http://arxiv.org/abs/2004.09458", "description": "Regression discontinuity designs assess causal effects in settings where\ntreatment is determined by whether an observed running variable crosses a\npre-specified threshold. Here we propose a new approach to identification,\nestimation, and inference in regression discontinuity designs that uses\nknowledge about exogenous noise (e.g., measurement error) in the running\nvariable. In our strategy, we weight treated and control units to balance a\nlatent variable of which the running variable is a noisy measure. Our approach\nis explicitly randomization-based and complements standard formal analyses that\nappeal to continuity arguments while ignoring the stochastic nature of the\nassignment mechanism."}, "http://arxiv.org/abs/2111.12258": {"title": "On Recoding Ordered Treatments as Binary Indicators", "link": "http://arxiv.org/abs/2111.12258", "description": "Researchers using instrumental variables to investigate ordered treatments\noften recode treatment into an indicator for any exposure. We investigate this\nestimand under the assumption that the instruments shift compliers from no\ntreatment to some but not from some treatment to more. We show that when there\nare extensive margin compliers only (EMCO) this estimand captures a weighted\naverage of treatment effects that can be partially unbundled into each complier\ngroup's potential outcome means. We also establish an equivalence between EMCO\nand a two-factor selection model and apply our results to study treatment\nheterogeneity in the Oregon Health Insurance Experiment."}, "http://arxiv.org/abs/2311.16260": {"title": "Using Multiple Outcomes to Improve the Synthetic Control Method", "link": "http://arxiv.org/abs/2311.16260", "description": "When there are multiple outcome series of interest, Synthetic Control\nanalyses typically proceed by estimating separate weights for each outcome. In\nthis paper, we instead propose estimating a common set of weights across\noutcomes, by balancing either a vector of all outcomes or an index or average\nof them. Under a low-rank factor model, we show that these approaches lead to\nlower bias bounds than separate weights, and that averaging leads to further\ngains when the number of outcomes grows. We illustrate this via simulation and\nin a re-analysis of the impact of the Flint water crisis on educational\noutcomes."}, "http://arxiv.org/abs/2311.16333": {"title": "From Reactive to Proactive Volatility Modeling with Hemisphere Neural Networks", "link": "http://arxiv.org/abs/2311.16333", "description": "We reinvigorate maximum likelihood estimation (MLE) for macroeconomic density\nforecasting through a novel neural network architecture with dedicated mean and\nvariance hemispheres. Our architecture features several key ingredients making\nMLE work in this context. First, the hemispheres share a common core at the\nentrance of the network which accommodates for various forms of time variation\nin the error variance. Second, we introduce a volatility emphasis constraint\nthat breaks mean/variance indeterminacy in this class of overparametrized\nnonlinear models. Third, we conduct a blocked out-of-bag reality check to curb\noverfitting in both conditional moments. Fourth, the algorithm utilizes\nstandard deep learning software and thus handles large data sets - both\ncomputationally and statistically. Ergo, our Hemisphere Neural Network (HNN)\nprovides proactive volatility forecasts based on leading indicators when it\ncan, and reactive volatility based on the magnitude of previous prediction\nerrors when it must. We evaluate point and density forecasts with an extensive\nout-of-sample experiment and benchmark against a suite of models ranging from\nclassics to more modern machine learning-based offerings. In all cases, HNN\nfares well by consistently providing accurate mean/variance forecasts for all\ntargets and horizons. Studying the resulting volatility paths reveals its\nversatility, while probabilistic forecasting evaluation metrics showcase its\nenviable reliability. Finally, we also demonstrate how this machinery can be\nmerged with other structured deep learning models by revisiting Goulet Coulombe\n(2022)'s Neural Phillips Curve."}, "http://arxiv.org/abs/2311.16440": {"title": "Inference for Low-rank Models without Estimating the Rank", "link": "http://arxiv.org/abs/2311.16440", "description": "This paper studies the inference about linear functionals of high-dimensional\nlow-rank matrices. While most existing inference methods would require\nconsistent estimation of the true rank, our procedure is robust to rank\nmisspecification, making it a promising approach in applications where rank\nestimation can be unreliable. We estimate the low-rank spaces using\npre-specified weighting matrices, known as diversified projections. A novel\nstatistical insight is that, unlike the usual statistical wisdom that\noverfitting mainly introduces additional variances, the over-estimated low-rank\nspace also gives rise to a non-negligible bias due to an implicit ridge-type\nregularization. We develop a new inference procedure and show that the central\nlimit theorem holds as long as the pre-specified rank is no smaller than the\ntrue rank. Empirically, we apply our method to the U.S. federal grants\nallocation data and test the existence of pork-barrel politics."}, "http://arxiv.org/abs/2311.16486": {"title": "On the adaptation of causal forests to manifold data", "link": "http://arxiv.org/abs/2311.16486", "description": "Researchers often hold the belief that random forests are \"the cure to the\nworld's ills\" (Bickel, 2010). But how exactly do they achieve this? Focused on\nthe recently introduced causal forests (Athey and Imbens, 2016; Wager and\nAthey, 2018), this manuscript aims to contribute to an ongoing research trend\ntowards answering this question, proving that causal forests can adapt to the\nunknown covariate manifold structure. In particular, our analysis shows that a\ncausal forest estimator can achieve the optimal rate of convergence for\nestimating the conditional average treatment effect, with the covariate\ndimension automatically replaced by the manifold dimension. These findings\nalign with analogous observations in the realm of deep learning and resonate\nwith the insights presented in Peter Bickel's 2004 Rietz lecture."}, "http://arxiv.org/abs/2311.17021": {"title": "Optimal Categorical Instrumental Variables", "link": "http://arxiv.org/abs/2311.17021", "description": "This paper discusses estimation with a categorical instrumental variable in\nsettings with potentially few observations per category. The proposed\ncategorical instrumental variable estimator (CIV) leverages a regularization\nassumption that implies existence of a latent categorical variable with fixed\nfinite support achieving the same first stage fit as the observed instrument.\nIn asymptotic regimes that allow the number of observations per category to\ngrow at arbitrary small polynomial rate with the sample size, I show that when\nthe cardinality of the support of the optimal instrument is known, CIV is\nroot-n asymptotically normal, achieves the same asymptotic variance as the\noracle IV estimator that presumes knowledge of the optimal instrument, and is\nsemiparametrically efficient under homoskedasticity. Under-specifying the\nnumber of support points reduces efficiency but maintains asymptotic normality."}, "http://arxiv.org/abs/1912.10488": {"title": "Efficient and Convergent Sequential Pseudo-Likelihood Estimation of Dynamic Discrete Games", "link": "http://arxiv.org/abs/1912.10488", "description": "We propose a new sequential Efficient Pseudo-Likelihood (k-EPL) estimator for\ndynamic discrete choice games of incomplete information. k-EPL considers the\njoint behavior of multiple players simultaneously, as opposed to individual\nresponses to other agents' equilibrium play. This, in addition to reframing the\nproblem from conditional choice probability (CCP) space to value function\nspace, yields a computationally tractable, stable, and efficient estimator. We\nshow that each iteration in the k-EPL sequence is consistent and asymptotically\nefficient, so the first-order asymptotic properties do not vary across\niterations. Furthermore, we show the sequence achieves higher-order equivalence\nto the finite-sample maximum likelihood estimator with iteration and that the\nsequence of estimators converges almost surely to the maximum likelihood\nestimator at a nearly-superlinear rate when the data are generated by any\nregular Markov perfect equilibrium, including equilibria that lead to\ninconsistency of other sequential estimators. When utility is linear in\nparameters, k-EPL iterations are computationally simple, only requiring that\nthe researcher solve linear systems of equations to generate pseudo-regressors\nwhich are used in a static logit/probit regression. Monte Carlo simulations\ndemonstrate the theoretical results and show k-EPL's good performance in finite\nsamples in both small- and large-scale games, even when the game admits\nspurious equilibria in addition to one that generated the data. We apply the\nestimator to study the role of competition in the U.S. wholesale club industry."}, "http://arxiv.org/abs/2202.07150": {"title": "Asymptotics of Cointegration Tests for High-Dimensional VAR($k$)", "link": "http://arxiv.org/abs/2202.07150", "description": "The paper studies nonstationary high-dimensional vector autoregressions of\norder $k$, VAR($k$). Additional deterministic terms such as trend or\nseasonality are allowed. The number of time periods, $T$, and the number of\ncoordinates, $N$, are assumed to be large and of the same order. Under this\nregime the first-order asymptotics of the Johansen likelihood ratio (LR),\nPillai-Bartlett, and Hotelling-Lawley tests for cointegration are derived: the\ntest statistics converge to nonrandom integrals. For more refined analysis, the\npaper proposes and analyzes a modification of the Johansen test. The new test\nfor the absence of cointegration converges to the partial sum of the Airy$_1$\npoint process. Supporting Monte Carlo simulations indicate that the same\nbehavior persists universally in many situations beyond those considered in our\ntheorems.\n\nThe paper presents empirical implementations of the approach for the analysis\nof S$\\&amp;$P$100$ stocks and of cryptocurrencies. The latter example has a strong\npresence of multiple cointegrating relationships, while the results for the\nformer are consistent with the null of no cointegration."}, "http://arxiv.org/abs/2311.17575": {"title": "Identifying Causal Effects of Nonbinary, Ordered Treatments using Multiple Instrumental Variables", "link": "http://arxiv.org/abs/2311.17575", "description": "This paper addresses the challenge of identifying causal effects of\nnonbinary, ordered treatments with multiple binary instruments. Next to\npresenting novel insights into the widely-applied two-stage least squares\nestimand, I show that a weighted average of local average treatment effects for\ncombined complier populations is identified under the limited monotonicity\nassumption. This novel causal parameter has an intuitive interpretation,\noffering an appealing alternative to two-stage least squares. I employ recent\nadvances in causal machine learning for estimation. I further demonstrate how\ncausal forests can be used to detect local violations of the underlying limited\nmonotonicity assumption. The methodology is applied to study the impact of\ncommunity nurseries on child health outcomes."}, "http://arxiv.org/abs/2311.17858": {"title": "On the Limits of Regression Adjustment", "link": "http://arxiv.org/abs/2311.17858", "description": "Regression adjustment, sometimes known as Controlled-experiment Using\nPre-Experiment Data (CUPED), is an important technique in internet\nexperimentation. It decreases the variance of effect size estimates, often\ncutting confidence interval widths in half or more while never making them\nworse. It does so by carefully regressing the goal metric against\npre-experiment features to reduce the variance. The tremendous gains of\nregression adjustment begs the question: How much better can we do by\nengineering better features from pre-experiment data, for example by using\nmachine learning techniques or synthetic controls? Could we even reduce the\nvariance in our effect sizes arbitrarily close to zero with the right\npredictors? Unfortunately, our answer is negative. A simple form of regression\nadjustment, which uses just the pre-experiment values of the goal metric,\ncaptures most of the benefit. Specifically, under a mild assumption that\nobservations closer in time are easier to predict that ones further away in\ntime, we upper bound the potential gains of more sophisticated feature\nengineering, with respect to the gains of this simple form of regression\nadjustment. The maximum reduction in variance is $50\\%$ in Theorem 1, or\nequivalently, the confidence interval width can be reduced by at most an\nadditional $29\\%$."}, "http://arxiv.org/abs/2110.04442": {"title": "A Primer on Deep Learning for Causal Inference", "link": "http://arxiv.org/abs/2110.04442", "description": "This review systematizes the emerging literature for causal inference using\ndeep neural networks under the potential outcomes framework. It provides an\nintuitive introduction on how deep learning can be used to estimate/predict\nheterogeneous treatment effects and extend causal inference to settings where\nconfounding is non-linear, time varying, or encoded in text, networks, and\nimages. To maximize accessibility, we also introduce prerequisite concepts from\ncausal inference and deep learning. The survey differs from other treatments of\ndeep learning and causal inference in its sharp focus on observational causal\nestimation, its extended exposition of key algorithms, and its detailed\ntutorials for implementing, training, and selecting among deep estimators in\nTensorflow 2 available at github.com/kochbj/Deep-Learning-for-Causal-Inference."}, "http://arxiv.org/abs/2111.05243": {"title": "Bounding Treatment Effects by Pooling Limited Information across Observations", "link": "http://arxiv.org/abs/2111.05243", "description": "We provide novel bounds on average treatment effects (on the treated) that\nare valid under an unconfoundedness assumption. Our bounds are designed to be\nrobust in challenging situations, for example, when the conditioning variables\ntake on a large number of different values in the observed sample, or when the\noverlap condition is violated. This robustness is achieved by only using\nlimited \"pooling\" of information across observations. Namely, the bounds are\nconstructed as sample averages over functions of the observed outcomes such\nthat the contribution of each outcome only depends on the treatment status of a\nlimited number of observations. No information pooling across observations\nleads to so-called \"Manski bounds\", while unlimited information pooling leads\nto standard inverse propensity score weighting. We explore the intermediate\nrange between these two extremes and provide corresponding inference methods.\nWe show in Monte Carlo experiments and through an empirical application that\nour bounds are indeed robust and informative in practice."}, "http://arxiv.org/abs/2303.01231": {"title": "Robust Hicksian Welfare Analysis under Individual Heterogeneity", "link": "http://arxiv.org/abs/2303.01231", "description": "Welfare effects of price changes are often estimated with cross-sections;\nthese do not identify demand with heterogeneous consumers. We develop a\ntheoretical method addressing this, utilizing uncompensated demand moments to\nconstruct local approximations for compensated demand moments, robust to\nunobserved preference heterogeneity. Our methodological contribution offers\nrobust approximations for average and distributional welfare estimates,\nextending to price indices, taxable income elasticities, and general\nequilibrium welfare. Our methods apply to any cross-section; we demonstrate\nthem via UK household budget survey data. We uncover an insight: simple\nnon-parametric representative agent models might be less biased than complex\nparametric models accounting for heterogeneity."}, "http://arxiv.org/abs/2305.02185": {"title": "Doubly Robust Uniform Confidence Bands for Group-Time Conditional Average Treatment Effects in Difference-in-Differences", "link": "http://arxiv.org/abs/2305.02185", "description": "We consider a panel data analysis to examine the heterogeneity in treatment\neffects with respect to a pre-treatment covariate of interest in the staggered\ndifference-in-differences setting of Callaway and Sant'Anna (2021). Under\nstandard identification conditions, a doubly robust estimand conditional on the\ncovariate identifies the group-time conditional average treatment effect given\nthe covariate. Focusing on the case of a continuous covariate, we propose a\nthree-step estimation procedure based on nonparametric local polynomial\nregressions and parametric estimation methods. Using uniformly valid\ndistributional approximation results for empirical processes and multiplier\nbootstrapping, we develop doubly robust inference methods to construct uniform\nconfidence bands for the group-time conditional average treatment effect\nfunction. The accompanying R package didhetero allows for easy implementation\nof the proposed methods."}, "http://arxiv.org/abs/2311.18136": {"title": "Extrapolating Away from the Cutoff in Regression Discontinuity Designs", "link": "http://arxiv.org/abs/2311.18136", "description": "Canonical RD designs yield credible local estimates of the treatment effect\nat the cutoff under mild continuity assumptions, but they fail to identify\ntreatment effects away from the cutoff without additional assumptions. The\nfundamental challenge of identifying treatment effects away from the cutoff is\nthat the counterfactual outcome under the alternative treatment status is never\nobserved. This paper aims to provide a methodological blueprint to identify\ntreatment effects away from the cutoff in various empirical settings by\noffering a non-exhaustive list of assumptions on the counterfactual outcome.\nInstead of assuming the exact evolution of the counterfactual outcome, this\npaper bounds its variation using the data and sensitivity parameters. The\nproposed assumptions are weaker than those introduced previously in the\nliterature, resulting in partially identified treatment effects that are less\nsusceptible to assumption violations. This approach accommodates both single\ncutoff and multi-cutoff designs. The specific choice of the extrapolation\nassumption depends on the institutional background of each empirical\napplication. Additionally, researchers are recommended to conduct sensitivity\nanalysis on the chosen parameter and assess resulting shifts in conclusions.\nThe paper compares the proposed identification results with results using\nprevious methods via an empirical application and simulated data. It\ndemonstrates that set identification yields a more credible conclusion about\nthe sign of the treatment effect."}, "http://arxiv.org/abs/2311.18555": {"title": "Identification in Endogenous Sequential Treatment Regimes", "link": "http://arxiv.org/abs/2311.18555", "description": "This paper develops a novel nonparametric identification method for treatment\neffects in settings where individuals self-select into treatment sequences. I\npropose an identification strategy which relies on a dynamic version of\nstandard Instrumental Variables (IV) assumptions and builds on a dynamic\nversion of the Marginal Treatment Effects (MTE) as the fundamental building\nblock for treatment effects. The main contribution of the paper is to relax\nassumptions on the support of the observed variables and on unobservable gains\nof treatment that are present in the dynamic treatment effects literature.\nMonte Carlo simulation studies illustrate the desirable finite-sample\nperformance of a sieve estimator for MTEs and Average Treatment Effects (ATEs)\non a close-to-application simulation study."}, "http://arxiv.org/abs/2311.18759": {"title": "Bootstrap Inference on Partially Linear Binary Choice Model", "link": "http://arxiv.org/abs/2311.18759", "description": "The partially linear binary choice model can be used for estimating\nstructural equations where nonlinearity may appear due to diminishing marginal\nreturns, different life cycle regimes, or hectic physical phenomena. The\ninference procedure for this model based on the analytic asymptotic\napproximation could be unreliable in finite samples if the sample size is not\nsufficiently large. This paper proposes a bootstrap inference approach for the\nmodel. Monte Carlo simulations show that the proposed inference method performs\nwell in finite samples compared to the procedure based on the asymptotic\napproximation."}, "http://arxiv.org/abs/2006.01212": {"title": "New Approaches to Robust Inference on Market (Non-)Efficiency, Volatility Clustering and Nonlinear Dependence", "link": "http://arxiv.org/abs/2006.01212", "description": "Many financial and economic variables, including financial returns, exhibit\nnonlinear dependence, heterogeneity and heavy-tailedness. These properties may\nmake problematic the analysis of (non-)efficiency and volatility clustering in\neconomic and financial markets using traditional approaches that appeal to\nasymptotic normality of sample autocorrelation functions of returns and their\nsquares.\n\nThis paper presents new approaches to deal with the above problems. We\nprovide the results that motivate the use of measures of market\n(non-)efficiency and volatility clustering based on (small) powers of absolute\nreturns and their signed versions.\n\nWe further provide new approaches to robust inference on the measures in the\ncase of general time series, including GARCH-type processes. The approaches are\nbased on robust $t-$statistics tests and new results on their applicability are\npresented. In the approaches, parameter estimates (e.g., estimates of measures\nof nonlinear dependence) are computed for groups of data, and the inference is\nbased on $t-$statistics in the resulting group estimates. This results in valid\nrobust inference under heterogeneity and dependence assumptions satisfied in\nreal-world financial markets. Numerical results and empirical applications\nconfirm the advantages and wide applicability of the proposed approaches."}, "http://arxiv.org/abs/2312.00282": {"title": "Stochastic volatility models with skewness selection", "link": "http://arxiv.org/abs/2312.00282", "description": "This paper expands traditional stochastic volatility models by allowing for\ntime-varying skewness without imposing it. While dynamic asymmetry may capture\nthe likely direction of future asset returns, it comes at the risk of leading\nto overparameterization. Our proposed approach mitigates this concern by\nleveraging sparsity-inducing priors to automatically selects the skewness\nparameter as being dynamic, static or zero in a data-driven framework. We\nconsider two empirical applications. First, in a bond yield application,\ndynamic skewness captures interest rate cycles of monetary easing and\ntightening being partially explained by central banks' mandates. In an currency\nmodeling framework, our model indicates no skewness in the carry factor after\naccounting for stochastic volatility which supports the idea of carry crashes\nbeing the result of volatility surges instead of dynamic skewness."}, "http://arxiv.org/abs/2312.00399": {"title": "GMM-lev estimation and individual heterogeneity: Monte Carlo evidence and empirical applications", "link": "http://arxiv.org/abs/2312.00399", "description": "The generalized method of moments (GMM) estimator applied to equations in\nlevels, GMM-lev, has the advantage of being able to estimate the effect of\nmeasurable time-invariant covariates using all available information. This is\nnot possible with GMM-dif, applied to equations in each period transformed into\nfirst differences, while GMM-sys uses little information, as it adds the\nequation in levels for only one period. The GMM-lev, by implying a\ntwo-component error term containing the individual heterogeneity and the shock,\nexposes the explanatory variables to possible double endogeneity. For example,\nthe estimation of true persistence could suffer from bias if instruments were\ncorrelated with the unit-specific error component. We propose to exploit the\n\\citet{Mundlak1978}'s approach together with GMM-lev estimation to capture\ninitial conditions and improve inference. Monte Carlo simulations for different\npanel types and under different double endogeneity assumptions show the\nadvantage of our approach."}, "http://arxiv.org/abs/2312.00590": {"title": "Inference on common trends in functional time series", "link": "http://arxiv.org/abs/2312.00590", "description": "This paper studies statistical inference on unit roots and cointegration for\ntime series in a Hilbert space. We develop statistical inference on the number\nof common stochastic trends that are embedded in the time series, i.e., the\ndimension of the nonstationary subspace. We also consider hypotheses on the\nnonstationary subspace itself. The Hilbert space can be of an arbitrarily large\ndimension, and our methods remain asymptotically valid even when the time\nseries of interest takes values in a subspace of possibly unknown dimension.\nThis has wide applicability in practice; for example, in the case of\ncointegrated vector time series of finite dimension, in a high-dimensional\nfactor model that includes a finite number of nonstationary factors, in the\ncase of cointegrated curve-valued (or function-valued) time series, and\nnonstationary dynamic functional factor models. We include two empirical\nillustrations to the term structure of interest rates and labor market indices,\nrespectively."}, "http://arxiv.org/abs/2305.17083": {"title": "A Policy Gradient Method for Confounded POMDPs", "link": "http://arxiv.org/abs/2305.17083", "description": "In this paper, we propose a policy gradient method for confounded partially\nobservable Markov decision processes (POMDPs) with continuous state and\nobservation spaces in the offline setting. We first establish a novel\nidentification result to non-parametrically estimate any history-dependent\npolicy gradient under POMDPs using the offline data. The identification enables\nus to solve a sequence of conditional moment restrictions and adopt the min-max\nlearning procedure with general function approximation for estimating the\npolicy gradient. We then provide a finite-sample non-asymptotic bound for\nestimating the gradient uniformly over a pre-specified policy class in terms of\nthe sample size, length of horizon, concentratability coefficient and the\nmeasure of ill-posedness in solving the conditional moment restrictions.\nLastly, by deploying the proposed gradient estimation in the gradient ascent\nalgorithm, we show the global convergence of the proposed algorithm in finding\nthe history-dependent optimal policy under some technical conditions. To the\nbest of our knowledge, this is the first work studying the policy gradient\nmethod for POMDPs under the offline setting."}, "http://arxiv.org/abs/2312.00955": {"title": "Identification and Inference for Synthetic Controls with Confounding", "link": "http://arxiv.org/abs/2312.00955", "description": "This paper studies inference on treatment effects in panel data settings with\nunobserved confounding. We model outcome variables through a factor model with\nrandom factors and loadings. Such factors and loadings may act as unobserved\nconfounders: when the treatment is implemented depends on time-varying factors,\nand who receives the treatment depends on unit-level confounders. We study the\nidentification of treatment effects and illustrate the presence of a trade-off\nbetween time and unit-level confounding. We provide asymptotic results for\ninference for several Synthetic Control estimators and show that different\nsources of randomness should be considered for inference, depending on the\nnature of confounding. We conclude with a comparison of Synthetic Control\nestimators with alternatives for factor models."}, "http://arxiv.org/abs/2312.01162": {"title": "Tests for Many Treatment Effects in Regression Discontinuity Panel Data Models", "link": "http://arxiv.org/abs/2312.01162", "description": "Numerous studies use regression discontinuity design (RDD) for panel data by\nassuming that the treatment effects are homogeneous across all\nindividuals/groups and pooling the data together. It is unclear how to test for\nthe significance of treatment effects when the treatments vary across\nindividuals/groups and the error terms may exhibit complicated dependence\nstructures. This paper examines the estimation and inference of multiple\ntreatment effects when the errors are not independent and identically\ndistributed, and the treatment effects vary across individuals/groups. We\nderive a simple analytical expression for approximating the variance-covariance\nstructure of the treatment effect estimators under general dependence\nconditions and propose two test statistics, one is to test for the overall\nsignificance of the treatment effect and the other for the homogeneity of the\ntreatment effects. We find that in the Gaussian approximations to the test\nstatistics, the dependence structures in the data can be safely ignored due to\nthe localized nature of the statistics. This has the important implication that\nthe simulated critical values can be easily obtained. Simulations demonstrate\nour tests have superb size control and reasonable power performance in finite\nsamples regardless of the presence of strong cross-section dependence or/and\nweak serial dependence in the data. We apply our tests to two datasets and find\nsignificant overall treatment effects in each case."}, "http://arxiv.org/abs/2312.01209": {"title": "A Method of Moments Approach to Asymptotically Unbiased Synthetic Controls", "link": "http://arxiv.org/abs/2312.01209", "description": "A common approach to constructing a Synthetic Control unit is to fit on the\noutcome variable and covariates in pre-treatment time periods, but it has been\nshown by Ferman and Pinto (2021) that this approach does not provide asymptotic\nunbiasedness when the fit is imperfect and the number of controls is fixed.\nMany related panel methods have a similar limitation when the number of units\nis fixed. I introduce and evaluate a new method in which the Synthetic Control\nis constructed using a General Method of Moments approach where if the\nSynthetic Control satisfies the moment conditions it must have the same\nloadings on latent factors as the treated unit. I show that a Synthetic Control\nEstimator of this form will be asymptotically unbiased as the number of\npre-treatment time periods goes to infinity, even when pre-treatment fit is\nimperfect and the set of controls is fixed. Furthermore, if both the number of\npre-treatment and post-treatment time periods go to infinity, then averages of\ntreatment effects can be consistently estimated and asymptotically valid\ninference can be conducted using a subsampling method. I conduct simulations\nand an empirical application to compare the performance of this method with\nexisting approaches in the literature."}, "http://arxiv.org/abs/2312.01881": {"title": "Bayesian Nonlinear Regression using Sums of Simple Functions", "link": "http://arxiv.org/abs/2312.01881", "description": "This paper proposes a new Bayesian machine learning model that can be applied\nto large datasets arising in macroeconomics. Our framework sums over many\nsimple two-component location mixtures. The transition between components is\ndetermined by a logistic function that depends on a single threshold variable\nand two hyperparameters. Each of these individual models only accounts for a\nminor portion of the variation in the endogenous variables. But many of them\nare capable of capturing arbitrary nonlinear conditional mean relations.\nConjugate priors enable fast and efficient inference. In simulations, we show\nthat our approach produces accurate point and density forecasts. In a real-data\nexercise, we forecast US macroeconomic aggregates and consider the nonlinear\neffects of financial shocks in a large-scale nonlinear VAR."}, "http://arxiv.org/abs/1905.05237": {"title": "Sustainable Investing and the Cross-Section of Returns and Maximum Drawdown", "link": "http://arxiv.org/abs/1905.05237", "description": "We use supervised learning to identify factors that predict the cross-section\nof returns and maximum drawdown for stocks in the US equity market. Our data\nrun from January 1970 to December 2019 and our analysis includes ordinary least\nsquares, penalized linear regressions, tree-based models, and neural networks.\nWe find that the most important predictors tended to be consistent across\nmodels, and that non-linear models had better predictive power than linear\nmodels. Predictive power was higher in calm periods than in stressed periods.\nEnvironmental, social, and governance indicators marginally impacted the\npredictive power of non-linear models in our data, despite their negative\ncorrelation with maximum drawdown and positive correlation with returns. Upon\nexploring whether ESG variables are captured by some models, we find that ESG\ndata contribute to the prediction nonetheless."}, "http://arxiv.org/abs/2203.08879": {"title": "A Simple and Computationally Trivial Estimator for Grouped Fixed Effects Models", "link": "http://arxiv.org/abs/2203.08879", "description": "This paper introduces a new fixed effects estimator for linear panel data\nmodels with clustered time patterns of unobserved heterogeneity. The method\navoids non-convex and combinatorial optimization by combining a preliminary\nconsistent estimator of the slope coefficient, an agglomerative\npairwise-differencing clustering of cross-sectional units, and a pooled\nordinary least squares regression. Asymptotic guarantees are established in a\nframework where $T$ can grow at any power of $N$, as both $N$ and $T$ approach\ninfinity. Unlike most existing approaches, the proposed estimator is\ncomputationally straightforward and does not require a known upper bound on the\nnumber of groups. As existing approaches, this method leads to a consistent\nestimation of well-separated groups and an estimator of common parameters\nasymptotically equivalent to the infeasible regression controlling for the true\ngroups. An application revisits the statistical association between income and\ndemocracy."}, "http://arxiv.org/abs/2204.02346": {"title": "Finitely Heterogeneous Treatment Effect in Event-study", "link": "http://arxiv.org/abs/2204.02346", "description": "Treatment effect estimation strategies in the event-study setup, namely panel\ndata with variation in treatment timing, often use the parallel trend\nassumption that assumes mean independence of potential outcomes across\ndifferent treatment timings. In this paper, we relax the parallel trend\nassumption by assuming a latent type variable and develop a type-specific\nparallel trend assumption. With a finite support assumption on the latent type\nvariable, we show that an extremum classifier consistently estimates the type\nassignment. Based on the clasfification result, we propose a type-specific\ndiff-in-diff estimator for the type-specific CATT. By estimating the CATT with\nregard to the latent type, we study heterogeneity in treatment effect, in\naddition to heterogeneity in baseline outcomes."}, "http://arxiv.org/abs/2204.07672": {"title": "Abadie's Kappa and Weighting Estimators of the Local Average Treatment Effect", "link": "http://arxiv.org/abs/2204.07672", "description": "In this paper we study the finite sample and asymptotic properties of various\nweighting estimators of the local average treatment effect (LATE), each of\nwhich can be motivated by Abadie's (2003) kappa theorem. Our framework presumes\na binary treatment and a binary instrument, which may only be valid after\nconditioning on additional covariates. We argue that two of the estimators\nunder consideration, which are weight normalized, are generally preferable.\nSeveral other estimators, which are unnormalized, do not satisfy the properties\nof scale invariance with respect to the natural logarithm and translation\ninvariance, thereby exhibiting sensitivity to the units of measurement when\nestimating the LATE in logs and the centering of the outcome variable more\ngenerally. We also demonstrate that, when noncompliance is one sided, certain\nestimators have the advantage of being based on a denominator that is strictly\ngreater than zero by construction. This is the case for only one of the two\nnormalized estimators, and we recommend this estimator for wider use. We\nillustrate our findings with a simulation study and three empirical\napplications. The importance of normalization is particularly apparent in\napplications to real data. The simulations also suggest that covariate\nbalancing estimation of instrument propensity scores may be more robust to\nmisspecification. Software for implementing these methods is available in\nStata."}, "http://arxiv.org/abs/2208.03737": {"title": "(Functional)Characterizations vs (Finite)Tests: Partially Unifying Functional and Inequality-Based Approaches to Testing", "link": "http://arxiv.org/abs/2208.03737", "description": "Historically, testing if decision-makers obey certain choice axioms using\nchoice data takes two distinct approaches. The 'functional' approach observes\nand tests the entire 'demand' or 'choice' function, whereas the 'revealed\npreference(RP)' approach constructs inequalities to test finite choices. I\ndemonstrate that a statistical recasting of the revealed enables uniting both\napproaches. Specifically, I construct a computationally efficient algorithm to\noutput one-sided statistical tests of choice data from functional\ncharacterizations of axiomatic behavior, thus linking statistical and RP\ntesting. An application to weakly separable preferences, where RP\ncharacterizations are provably NP-Hard, demonstrates the approach's merit. I\nalso show that without assuming monotonicity, all restrictions disappear.\nHence, any ability to resolve axiomatic behavior relies on the monotonicity\nassumption."}, "http://arxiv.org/abs/2211.13610": {"title": "Cross-Sectional Dynamics Under Network Structure: Theory and Macroeconomic Applications", "link": "http://arxiv.org/abs/2211.13610", "description": "Many environments in economics feature a cross-section of units linked by\nbilateral ties. I develop a framework for studying dynamics of cross-sectional\nvariables exploiting this network structure. It is a vector autoregression in\nwhich innovations transmit cross-sectionally only via bilateral links and which\ncan accommodate rich patterns of how network effects of higher order accumulate\nover time. The model can be used to estimate dynamic network effects, with the\nnetwork given or inferred from dynamic cross-correlations in the data. It also\noffers a dimensionality-reduction technique for modeling high-dimensional\n(cross-sectional) processes, owing to networks' ability to summarize complex\nrelations among variables (units) by relatively few non-zero bilateral links.\nIn a first application, I estimate how sectoral productivity shocks transmit\nalong supply chain linkages and affect dynamics of sectoral prices in the US\neconomy. The analysis suggests that network positions can rationalize not only\nthe strength of a sector's impact on aggregates, but also its timing. In a\nsecond application, I model industrial production growth across 44 countries by\nassuming global business cycles are driven by bilateral links which I estimate.\nThis reduces out-of-sample mean squared errors by up to 23% relative to a\nprincipal components factor model."}, "http://arxiv.org/abs/2303.00083": {"title": "Transition Probabilities and Moment Restrictions in Dynamic Fixed Effects Logit Models", "link": "http://arxiv.org/abs/2303.00083", "description": "Dynamic logit models are popular tools in economics to measure state\ndependence. This paper introduces a new method to derive moment restrictions in\na large class of such models with strictly exogenous regressors and fixed\neffects. We exploit the common structure of logit-type transition probabilities\nand elementary properties of rational fractions, to formulate a systematic\nprocedure that scales naturally with model complexity (e.g the lag order or the\nnumber of observed time periods). We detail the construction of moment\nrestrictions in binary response models of arbitrary lag order as well as\nfirst-order panel vector autoregressions and dynamic multinomial logit models.\nIdentification of common parameters and average marginal effects is also\ndiscussed for the binary response case. Finally, we illustrate our results by\nstudying the dynamics of drug consumption amongst young people inspired by Deza\n(2015)."}, "http://arxiv.org/abs/2306.13362": {"title": "Sparse plus dense MIDAS regressions and nowcasting during the COVID pandemic", "link": "http://arxiv.org/abs/2306.13362", "description": "The common practice for GDP nowcasting in a data-rich environment is to\nemploy either sparse regression using LASSO-type regularization or a dense\napproach based on factor models or ridge regression, which differ in the way\nthey extract information from high-dimensional datasets. This paper aims to\ninvestigate whether sparse plus dense mixed frequency regression methods can\nimprove the nowcasts of the US GDP growth. We propose two novel MIDAS\nregressions and show that these novel sparse plus dense methods greatly improve\nthe accuracy of nowcasts during the COVID pandemic compared to either only\nsparse or only dense approaches. Using monthly macro and weekly financial\nseries, we further show that the improvement is particularly sharp when the\ndense component is restricted to be macro, while the sparse signal stems from\nboth macro and financial series."}, "http://arxiv.org/abs/2312.02288": {"title": "Almost Dominance: Inference and Application", "link": "http://arxiv.org/abs/2312.02288", "description": "This paper proposes a general framework for inference on three types of\nalmost dominances: Almost Lorenz dominance, almost inverse stochastic\ndominance, and almost stochastic dominance. We first generalize almost Lorenz\ndominance to almost upward and downward Lorenz dominances. We then provide a\nbootstrap inference procedure for the Lorenz dominance coefficients, which\nmeasure the degrees of almost Lorenz dominances. Furthermore, we propose almost\nupward and downward inverse stochastic dominances and provide inference on the\ninverse stochastic dominance coefficients. We also show that our results can\neasily be extended to almost stochastic dominance. Simulation studies\ndemonstrate the finite sample properties of the proposed estimators and the\nbootstrap confidence intervals. We apply our methods to the inequality growth\nin the United Kingdom and find evidence for almost upward inverse stochastic\ndominance."}, "http://arxiv.org/abs/2206.08052": {"title": "Likelihood ratio test for structural changes in factor models", "link": "http://arxiv.org/abs/2206.08052", "description": "A factor model with a break in its factor loadings is observationally\nequivalent to a model without changes in the loadings but a change in the\nvariance of its factors. This effectively transforms a structural change\nproblem of high dimension into a problem of low dimension. This paper considers\nthe likelihood ratio (LR) test for a variance change in the estimated factors.\nThe LR test implicitly explores a special feature of the estimated factors: the\npre-break and post-break variances can be a singular matrix under the\nalternative hypothesis, making the LR test diverging faster and thus more\npowerful than Wald-type tests. The better power property of the LR test is also\nconfirmed by simulations. We also consider mean changes and multiple breaks. We\napply the procedure to the factor modelling and structural change of the US\nemployment using monthly industry-level-data."}, "http://arxiv.org/abs/2207.03035": {"title": "On the instrumental variable estimation with many weak and invalid instruments", "link": "http://arxiv.org/abs/2207.03035", "description": "We discuss the fundamental issue of identification in linear instrumental\nvariable (IV) models with unknown IV validity. With the assumption of the\n\"sparsest rule\", which is equivalent to the plurality rule but becomes\noperational in computation algorithms, we investigate and prove the advantages\nof non-convex penalized approaches over other IV estimators based on two-step\nselections, in terms of selection consistency and accommodation for\nindividually weak IVs. Furthermore, we propose a surrogate sparsest penalty\nthat aligns with the identification condition and provides oracle sparse\nstructure simultaneously. Desirable theoretical properties are derived for the\nproposed estimator with weaker IV strength conditions compared to the previous\nliterature. Finite sample properties are demonstrated using simulations and the\nselection and estimation method is applied to an empirical study concerning the\neffect of BMI on diastolic blood pressure."}, "http://arxiv.org/abs/2301.07855": {"title": "Digital Divide: Empirical Study of CIUS 2020", "link": "http://arxiv.org/abs/2301.07855", "description": "Canada and other major countries are investigating the implementation of\n``digital money'' or Central Bank Digital Currencies, necessitating answers to\nkey questions about how demographic and geographic factors influence the\npopulation's digital literacy. This paper uses the Canadian Internet Use Survey\n(CIUS) 2020 and survey versions of Lasso inference methods to assess the\ndigital divide in Canada and determine the relevant factors that influence it.\nWe find that a significant divide in the use of digital technologies, e.g.,\nonline banking and virtual wallet, continues to exist across different\ndemographic and geographic categories. We also create a digital divide score\nthat measures the survey respondents' digital literacy and provide multiple\ncorrespondence analyses that further corroborate these findings."}, "http://arxiv.org/abs/2312.03165": {"title": "A Theory Guide to Using Control Functions to Instrument Hazard Models", "link": "http://arxiv.org/abs/2312.03165", "description": "I develop the theory around using control functions to instrument hazard\nmodels, allowing the inclusion of endogenous (e.g., mismeasured) regressors.\nSimple discrete-data hazard models can be expressed as binary choice panel data\nmodels, and the widespread Prentice and Gloeckler (1978) discrete-data\nproportional hazards model can specifically be expressed as a complementary\nlog-log model with time fixed effects. This allows me to recast it as GMM\nestimation and its instrumented version as sequential GMM estimation in a\nZ-estimation (non-classical GMM) framework; this framework can then be\nleveraged to establish asymptotic properties and sufficient conditions. Whilst\nthis paper focuses on the Prentice and Gloeckler (1978) model, the methods and\ndiscussion developed here can be applied more generally to other hazard models\nand binary choice models. I also introduce my Stata command for estimating a\ncomplementary log-log model instrumented via control functions (available as\nivcloglog on SSC), which allows practitioners to easily instrument the Prentice\nand Gloeckler (1978) model."}, "http://arxiv.org/abs/2005.05942": {"title": "Moment Conditions for Dynamic Panel Logit Models with Fixed Effects", "link": "http://arxiv.org/abs/2005.05942", "description": "This paper investigates the construction of moment conditions in discrete\nchoice panel data with individual specific fixed effects. We describe how to\nsystematically explore the existence of moment conditions that do not depend on\nthe fixed effects, and we demonstrate how to construct them when they exist.\nOur approach is closely related to the numerical \"functional differencing\"\nconstruction in Bonhomme (2012), but our emphasis is to find explicit analytic\nexpressions for the moment functions. We first explain the construction and\ngive examples of such moment conditions in various models. Then, we focus on\nthe dynamic binary choice logit model and explore the implications of the\nmoment conditions for identification and estimation of the model parameters\nthat are common to all individuals."}, "http://arxiv.org/abs/2104.12909": {"title": "Algorithm as Experiment: Machine Learning, Market Design, and Policy Eligibility Rules", "link": "http://arxiv.org/abs/2104.12909", "description": "Algorithms make a growing portion of policy and business decisions. We\ndevelop a treatment-effect estimator using algorithmic decisions as instruments\nfor a class of stochastic and deterministic algorithms. Our estimator is\nconsistent and asymptotically normal for well-defined causal effects. A special\ncase of our setup is multidimensional regression discontinuity designs with\ncomplex boundaries. We apply our estimator to evaluate the Coronavirus Aid,\nRelief, and Economic Security Act, which allocated many billions of dollars\nworth of relief funding to hospitals via an algorithmic rule. The funding is\nshown to have little effect on COVID-19-related hospital activities. Naive\nestimates exhibit selection bias."}, "http://arxiv.org/abs/2312.03915": {"title": "Alternative models for FX, arbitrage opportunities and efficient pricing of double barrier options in L\\'evy models", "link": "http://arxiv.org/abs/2312.03915", "description": "We analyze the qualitative differences between prices of double barrier\nno-touch options in the Heston model and pure jump KoBoL model calibrated to\nthe same set of the empirical data, and discuss the potential for arbitrage\nopportunities if the correct model is a pure jump model. We explain and\ndemonstrate with numerical examples that accurate and fast calculations of\nprices of double barrier options in jump models are extremely difficult using\nthe numerical methods available in the literature. We develop a new efficient\nmethod (GWR-SINH method) based of the Gaver-Wynn-Rho acceleration applied to\nthe Bromwich integral; the SINH-acceleration and simplified trapezoid rule are\nused to evaluate perpetual double barrier options for each value of the\nspectral parameter in GWR-algorithm. The program in Matlab running on a Mac\nwith moderate characteristics achieves the precision of the order of E-5 and\nbetter in several several dozen of milliseconds; the precision E-07 is\nachievable in about 0.1 sec. We outline the extension of GWR-SINH method to\nregime-switching models and models with stochastic parameters and stochastic\ninterest rates."}, "http://arxiv.org/abs/2312.04428": {"title": "A general framework for the generation of probabilistic socioeconomic scenarios: Quantification of national food security risk with application to the cases of Egypt and Ethiopia", "link": "http://arxiv.org/abs/2312.04428", "description": "In this work a general framework for providing detailed probabilistic\nsocioeconomic scenarios as well as estimates concerning country-level food\nsecurity risk is proposed. Our methodology builds on (a) the Bayesian\nprobabilistic version of the world population model and (b) on the\ninterdependencies of the minimum food requirements and the national food system\ncapacities on key drivers, such as: population, income, natural resources, and\nother socioeconomic and climate indicators. Model uncertainty plays an\nimportant role in such endeavours. In this perspective, the concept of the\nrecently developed convex risk measures which mitigate the model uncertainty\neffects, is employed for the development of a framework for assessment, in the\ncontext of food security. The proposed method provides predictions and\nevaluations for food security risk both within and across probabilistic\nscenarios at country level. Our methodology is illustrated through its\nimplementation for the cases of Egypt and Ethiopia, for the time period\n2019-2050, under the combined context of the Shared Socioeconomic Pathways\n(SSPs) and the Representative Concentration Pathways (RCPs)."}, "http://arxiv.org/abs/2108.02196": {"title": "Synthetic Controls for Experimental Design", "link": "http://arxiv.org/abs/2108.02196", "description": "This article studies experimental design in settings where the experimental\nunits are large aggregate entities (e.g., markets), and only one or a small\nnumber of units can be exposed to the treatment. In such settings,\nrandomization of the treatment may result in treated and control groups with\nvery different characteristics at baseline, inducing biases. We propose a\nvariety of synthetic control designs (Abadie, Diamond and Hainmueller, 2010,\nAbadie and Gardeazabal, 2003) as experimental designs to select treated units\nin non-randomized experiments with large aggregate units, as well as the\nuntreated units to be used as a control group. Average potential outcomes are\nestimated as weighted averages of treated units, for potential outcomes with\ntreatment -- and control units, for potential outcomes without treatment. We\nanalyze the properties of estimators based on synthetic control designs and\npropose new inferential techniques. We show that in experimental settings with\naggregate units, synthetic control designs can substantially reduce estimation\nbiases in comparison to randomization of the treatment."}, "http://arxiv.org/abs/2108.04852": {"title": "Multiway empirical likelihood", "link": "http://arxiv.org/abs/2108.04852", "description": "This paper develops a general methodology to conduct statistical inference\nfor observations indexed by multiple sets of entities. We propose a novel\nmultiway empirical likelihood statistic that converges to a chi-square\ndistribution under the non-degenerate case, where corresponding Hoeffding type\ndecomposition is dominated by linear terms. Our methodology is related to the\nnotion of jackknife empirical likelihood but the leave-out pseudo values are\nconstructed by leaving columns or rows. We further develop a modified version\nof our multiway empirical likelihood statistic, which converges to a chi-square\ndistribution regardless of the degeneracy, and discover its desirable\nhigher-order property compared to the t-ratio by the conventional Eicker-White\ntype variance estimator. The proposed methodology is illustrated by several\nimportant statistical problems, such as bipartite network, generalized\nestimating equations, and three-way observations."}, "http://arxiv.org/abs/2306.03632": {"title": "Uniform Inference for Cointegrated Vector Autoregressive Processes", "link": "http://arxiv.org/abs/2306.03632", "description": "Uniformly valid inference for cointegrated vector autoregressive processes\nhas so far proven difficult due to certain discontinuities arising in the\nasymptotic distribution of the least squares estimator. We extend asymptotic\nresults from the univariate case to multiple dimensions and show how inference\ncan be based on these results. Furthermore, we show that lag augmentation and a\nrecent instrumental variable procedure can also yield uniformly valid tests and\nconfidence regions. We verify the theoretical findings and investigate finite\nsample properties in simulation experiments for two specific examples."}, "http://arxiv.org/abs/2005.04141": {"title": "Critical Values Robust to P-hacking", "link": "http://arxiv.org/abs/2005.04141", "description": "P-hacking is prevalent in reality but absent from classical hypothesis\ntesting theory. As a consequence, significant results are much more common than\nthey are supposed to be when the null hypothesis is in fact true. In this\npaper, we build a model of hypothesis testing with p-hacking. From the model,\nwe construct critical values such that, if the values are used to determine\nsignificance, and if scientists' p-hacking behavior adjusts to the new\nsignificance standards, significant results occur with the desired frequency.\nSuch robust critical values allow for p-hacking so they are larger than\nclassical critical values. To illustrate the amount of correction that\np-hacking might require, we calibrate the model using evidence from the medical\nsciences. In the calibrated model the robust critical value for any test\nstatistic is the classical critical value for the same test statistic with one\nfifth of the significance level."}, "http://arxiv.org/abs/2312.05342": {"title": "Occasionally Misspecified", "link": "http://arxiv.org/abs/2312.05342", "description": "When fitting a particular Economic model on a sample of data, the model may\nturn out to be heavily misspecified for some observations. This can happen\nbecause of unmodelled idiosyncratic events, such as an abrupt but short-lived\nchange in policy. These outliers can significantly alter estimates and\ninferences. A robust estimation is desirable to limit their influence. For\nskewed data, this induces another bias which can also invalidate the estimation\nand inferences. This paper proposes a robust GMM estimator with a simple bias\ncorrection that does not degrade robustness significantly. The paper provides\nfinite-sample robustness bounds, and asymptotic uniform equivalence with an\noracle that discards all outliers. Consistency and asymptotic normality ensue\nfrom that result. An application to the \"Price-Puzzle,\" which finds inflation\nincreases when monetary policy tightens, illustrates the concerns and the\nmethod. The proposed estimator finds the intuitive result: tighter monetary\npolicy leads to a decline in inflation."}, "http://arxiv.org/abs/2312.05373": {"title": "GCov-Based Portmanteau Test", "link": "http://arxiv.org/abs/2312.05373", "description": "We examine finite sample performance of the Generalized Covariance (GCov)\nresidual-based specification test for semiparametric models with i.i.d. errors.\nThe residual-based multivariate portmanteau test statistic follows\nasymptotically a $\\chi^2$ distribution when the model is estimated by the GCov\nestimator. The test is shown to perform well in application to the univariate\nmixed causal-noncausal MAR, double autoregressive (DAR) and multivariate Vector\nAutoregressive (VAR) models. We also introduce a bootstrap procedure that\nprovides the limiting distribution of the test statistic when the specification\ntest is applied to a model estimated by the maximum likelihood, or the\napproximate or quasi-maximum likelihood under a parametric assumption on the\nerror distribution."}, "http://arxiv.org/abs/2312.05593": {"title": "Economic Forecasts Using Many Noises", "link": "http://arxiv.org/abs/2312.05593", "description": "This paper addresses a key question in economic forecasting: does pure noise\ntruly lack predictive power? Economists typically conduct variable selection to\neliminate noises from predictors. Yet, we prove a compelling result that in\nmost economic forecasts, the inclusion of noises in predictions yields greater\nbenefits than its exclusion. Furthermore, if the total number of predictors is\nnot sufficiently large, intentionally adding more noises yields superior\nforecast performance, outperforming benchmark predictors relying on dimension\nreduction. The intuition lies in economic predictive signals being densely\ndistributed among regression coefficients, maintaining modest forecast bias\nwhile diversifying away overall variance, even when a significant proportion of\npredictors constitute pure noises. One of our empirical demonstrations shows\nthat intentionally adding 300~6,000 pure noises to the Welch and Goyal (2008)\ndataset achieves a noteworthy 10% out-of-sample R square accuracy in\nforecasting the annual U.S. equity premium. The performance surpasses the\nmajority of sophisticated machine learning models."}, "http://arxiv.org/abs/2312.05700": {"title": "Influence Analysis with Panel Data", "link": "http://arxiv.org/abs/2312.05700", "description": "The presence of units with extreme values in the dependent and/or independent\nvariables (i.e., vertical outliers, leveraged data) has the potential to\nseverely bias regression coefficients and/or standard errors. This is common\nwith short panel data because the researcher cannot advocate asymptotic theory.\nExample include cross-country studies, cell-group analyses, and field or\nlaboratory experimental studies, where the researcher is forced to use few\ncross-sectional observations repeated over time due to the structure of the\ndata or research design. Available diagnostic tools may fail to properly detect\nthese anomalies, because they are not designed for panel data. In this paper,\nwe formalise statistical measures for panel data models with fixed effects to\nquantify the degree of leverage and outlyingness of units, and the joint and\nconditional influences of pairs of units. We first develop a method to visually\ndetect anomalous units in a panel data set, and identify their type. Second, we\ninvestigate the effect of these units on LS estimates, and on other units'\ninfluence on the estimated parameters. To illustrate and validate the proposed\nmethod, we use a synthetic data set contaminated with different types of\nanomalous units. We also provide an empirical example."}, "http://arxiv.org/abs/2312.05858": {"title": "The Machine Learning Control Method for Counterfactual Forecasting", "link": "http://arxiv.org/abs/2312.05858", "description": "Without a credible control group, the most widespread methodologies for\nestimating causal effects cannot be applied. To fill this gap, we propose the\nMachine Learning Control Method (MLCM), a new approach for causal panel\nanalysis based on counterfactual forecasting with machine learning. The MLCM\nestimates policy-relevant causal parameters in short- and long-panel settings\nwithout relying on untreated units. We formalize identification in the\npotential outcomes framework and then provide estimation based on supervised\nmachine learning algorithms. To illustrate the advantages of our estimator, we\npresent simulation evidence and an empirical application on the impact of the\nCOVID-19 crisis on educational inequality in Italy. We implement the proposed\nmethod in the companion R package MachineControl."}, "http://arxiv.org/abs/2312.05898": {"title": "Dynamic Spatiotemporal ARCH Models: Small and Large Sample Results", "link": "http://arxiv.org/abs/2312.05898", "description": "This paper explores the estimation of a dynamic spatiotemporal autoregressive\nconditional heteroscedasticity (ARCH) model. The log-volatility term in this\nmodel can depend on (i) the spatial lag of the log-squared outcome variable,\n(ii) the time-lag of the log-squared outcome variable, (iii) the spatiotemporal\nlag of the log-squared outcome variable, (iv) exogenous variables, and (v) the\nunobserved heterogeneity across regions and time, i.e., the regional and time\nfixed effects. We examine the small and large sample properties of two\nquasi-maximum likelihood estimators and a generalized method of moments\nestimator for this model. We first summarize the theoretical properties of\nthese estimators and then compare their finite sample properties through Monte\nCarlo simulations."}, "http://arxiv.org/abs/2312.05985": {"title": "Fused Extended Two-Way Fixed Effects for Difference-in-Differences with Staggered Adoptions", "link": "http://arxiv.org/abs/2312.05985", "description": "To address the bias of the canonical two-way fixed effects estimator for\ndifference-in-differences under staggered adoptions, Wooldridge (2021) proposed\nthe extended two-way fixed effects estimator, which adds many parameters.\nHowever, this reduces efficiency. Restricting some of these parameters to be\nequal helps, but ad hoc restrictions may reintroduce bias. We propose a machine\nlearning estimator with a single tuning parameter, fused extended two-way fixed\neffects (FETWFE), that enables automatic data-driven selection of these\nrestrictions. We prove that under an appropriate sparsity assumption FETWFE\nidentifies the correct restrictions with probability tending to one. We also\nprove the consistency, asymptotic normality, and oracle efficiency of FETWFE\nfor two classes of heterogeneous marginal treatment effect estimators under\neither conditional or marginal parallel trends, and we prove consistency for\ntwo classes of conditional average treatment effects under conditional parallel\ntrends. We demonstrate FETWFE in simulation studies and an empirical\napplication."}, "http://arxiv.org/abs/2312.06379": {"title": "Trends in Temperature Data: Micro-foundations of Their Nature", "link": "http://arxiv.org/abs/2312.06379", "description": "Determining whether Global Average Temperature (GAT) is an integrated process\nof order 1, I(1), or is a stationary process around a trend function is crucial\nfor detection, attribution, impact and forecasting studies of climate change.\nIn this paper, we investigate the nature of trends in GAT building on the\nanalysis of individual temperature grids. Our 'micro-founded' evidence suggests\nthat GAT is stationary around a non-linear deterministic trend in the form of a\nlinear function with a one-period structural break. This break can be\nattributed to a combination of individual grid breaks and the standard\naggregation method under acceleration in global warming. We illustrate our\nfindings using simulations."}, "http://arxiv.org/abs/2312.06402": {"title": "Structural Analysis of Vector Autoregressive Models", "link": "http://arxiv.org/abs/2312.06402", "description": "This set of lecture notes discuss key concepts for the structural analysis of\nVector Autoregressive models for the teaching of Applied Macroeconometrics\nmodule."}, "http://arxiv.org/abs/2110.12722": {"title": "Functional instrumental variable regression with an application to estimating the impact of immigration on native wages", "link": "http://arxiv.org/abs/2110.12722", "description": "Functional linear regression gets its popularity as a statistical tool to\nstudy the relationship between function-valued response and exogenous\nexplanatory variables. However, in practice, it is hard to expect that the\nexplanatory variables of interest are perfectly exogenous, due to, for example,\nthe presence of omitted variables and measurement error. Despite its empirical\nrelevance, it was not until recently that this issue of endogeneity was studied\nin the literature on functional regression, and the development in this\ndirection does not seem to sufficiently meet practitioners' needs; for example,\nthis issue has been discussed with paying particular attention on consistent\nestimation and thus distributional properties of the proposed estimators still\nremain to be further explored. To fill this gap, this paper proposes new\nconsistent FPCA-based instrumental variable estimators and develops their\nasymptotic properties in detail. Simulation experiments under a wide range of\nsettings show that the proposed estimators perform considerably well. We apply\nour methodology to estimate the impact of immigration on native wages."}, "http://arxiv.org/abs/2205.01882": {"title": "Approximating Choice Data by Discrete Choice Models", "link": "http://arxiv.org/abs/2205.01882", "description": "We obtain a necessary and sufficient condition under which random-coefficient\ndiscrete choice models, such as mixed-logit models, are rich enough to\napproximate any nonparametric random utility models arbitrarily well across\nchoice sets. The condition turns out to be the affine-independence of the set\nof characteristic vectors. When the condition fails, resulting in some random\nutility models that cannot be closely approximated, we identify preferences and\nsubstitution patterns that are challenging to approximate accurately. We also\npropose algorithms to quantify the magnitude of approximation errors."}, "http://arxiv.org/abs/2305.18114": {"title": "Identifying Dynamic LATEs with a Static Instrument", "link": "http://arxiv.org/abs/2305.18114", "description": "In many situations, researchers are interested in identifying dynamic effects\nof an irreversible treatment with a static binary instrumental variable (IV).\nFor example, in evaluations of dynamic effects of training programs, with a\nsingle lottery determining eligibility. A common approach in these situations\nis to report per-period IV estimates. Under a dynamic extension of standard IV\nassumptions, we show that such IV estimators identify a weighted sum of\ntreatment effects for different latent groups and treatment exposures. However,\nthere is possibility of negative weights. We consider point and partial\nidentification of dynamic treatment effects in this setting under different\nsets of assumptions."}, "http://arxiv.org/abs/2312.07520": {"title": "Estimating Counterfactual Matrix Means with Short Panel Data", "link": "http://arxiv.org/abs/2312.07520", "description": "We develop a more flexible approach for identifying and estimating average\ncounterfactual outcomes when several but not all possible outcomes are observed\nfor each unit in a large cross section. Such settings include event studies and\nstudies of outcomes of \"matches\" between agents of two types, e.g. workers and\nfirms or people and places. When outcomes are generated by a factor model that\nallows for low-dimensional unobserved confounders, our method yields\nconsistent, asymptotically normal estimates of counterfactual outcome means\nunder asymptotics that fix the number of outcomes as the cross section grows\nand general outcome missingness patterns, including those not accommodated by\nexisting methods. Our method is also computationally efficient, requiring only\na single eigendecomposition of a particular aggregation of any factor estimates\nconstructed using subsets of units with the same observed outcomes. In a\nsemi-synthetic simulation study based on matched employer-employee data, our\nmethod performs favorably compared to a Two-Way-Fixed-Effects-model-based\nestimator."}, "http://arxiv.org/abs/2211.16362": {"title": "Score-based calibration testing for multivariate forecast distributions", "link": "http://arxiv.org/abs/2211.16362", "description": "Calibration tests based on the probability integral transform (PIT) are\nroutinely used to assess the quality of univariate distributional forecasts.\nHowever, PIT-based calibration tests for multivariate distributional forecasts\nface various challenges. We propose two new types of tests based on proper\nscoring rules, which overcome these challenges. They arise from a general\nframework for calibration testing in the multivariate case, introduced in this\nwork. The new tests have good size and power properties in simulations and\nsolve various problems of existing tests. We apply the tests to forecast\ndistributions for macroeconomic and financial time series data."}, "http://arxiv.org/abs/2309.04926": {"title": "Testing for Stationary or Persistent Coefficient Randomness in Predictive Regressions", "link": "http://arxiv.org/abs/2309.04926", "description": "This study considers tests for coefficient randomness in predictive\nregressions. Our focus is on how tests for coefficient randomness are\ninfluenced by the persistence of random coefficient. We find that when the\nrandom coefficient is stationary, or I(0), Nyblom's (1989) LM test loses its\noptimality (in terms of power), which is established against the alternative of\nintegrated, or I(1), random coefficient. We demonstrate this by constructing\ntests that are more powerful than the LM test when random coefficient is\nstationary, although these tests are dominated in terms of power by the LM test\nwhen random coefficient is integrated. This implies that the best test for\ncoefficient randomness differs from context to context, and practitioners\nshould take into account the persistence of potentially random coefficient and\nchoose from several tests accordingly. We apply tests for coefficient constancy\nto real data. The results mostly reverse the conclusion of an earlier empirical\nstudy."}, "http://arxiv.org/abs/2312.07683": {"title": "On Rosenbaum's Rank-based Matching Estimator", "link": "http://arxiv.org/abs/2312.07683", "description": "In two influential contributions, Rosenbaum (2005, 2020) advocated for using\nthe distances between component-wise ranks, instead of the original data\nvalues, to measure covariate similarity when constructing matching estimators\nof average treatment effects. While the intuitive benefits of using covariate\nranks for matching estimation are apparent, there is no theoretical\nunderstanding of such procedures in the literature. We fill this gap by\ndemonstrating that Rosenbaum's rank-based matching estimator, when coupled with\na regression adjustment, enjoys the properties of double robustness and\nsemiparametric efficiency without the need to enforce restrictive covariate\nmoment assumptions. Our theoretical findings further emphasize the statistical\nvirtues of employing ranks for estimation and inference, more broadly aligning\nwith the insights put forth by Peter Bickel in his 2004 Rietz lecture (Bickel,\n2004)."}, "http://arxiv.org/abs/2312.07881": {"title": "Efficiency of QMLE for dynamic panel data models with interactive effects", "link": "http://arxiv.org/abs/2312.07881", "description": "This paper derives the efficiency bound for estimating the parameters of\ndynamic panel data models in the presence of an increasing number of incidental\nparameters. We study the efficiency problem by formulating the dynamic panel as\na simultaneous equations system, and show that the quasi-maximum likelihood\nestimator (QMLE) applied to the system achieves the efficiency bound.\nComparison of QMLE with fixed effects estimators is made."}, "http://arxiv.org/abs/2312.08171": {"title": "Individual Updating of Subjective Probability of Homicide Victimization: a \"Natural Experiment'' on Risk Communication", "link": "http://arxiv.org/abs/2312.08171", "description": "We investigate the dynamics of the update of subjective homicide\nvictimization risk after an informational shock by developing two econometric\nmodels able to accommodate both optimal decisions of changing prior\nexpectations which enable us to rationalize skeptical Bayesian agents with\ntheir disregard to new information. We apply our models to a unique household\ndata (N = 4,030) that consists of socioeconomic and victimization expectation\nvariables in Brazil, coupled with an informational ``natural experiment''\nbrought by the sample design methodology, which randomized interviewers to\ninterviewees. The higher priors about their own subjective homicide\nvictimization risk are set, the more likely individuals are to change their\ninitial perceptions. In case of an update, we find that elders and females are\nmore reluctant to change priors and choose the new response level. In addition,\neven though the respondents' level of education is not significant, the\ninterviewers' level of education has a key role in changing and updating\ndecisions. The results show that our econometric approach fits reasonable well\nthe available empirical evidence, stressing the salient role heterogeneity\nrepresented by individual characteristics of interviewees and interviewers have\non belief updating and lack of it, say, skepticism. Furthermore, we can\nrationalize skeptics through an informational quality/credibility argument."}, "http://arxiv.org/abs/2312.08174": {"title": "Double Machine Learning for Static Panel Models with Fixed Effects", "link": "http://arxiv.org/abs/2312.08174", "description": "Machine Learning (ML) algorithms are powerful data-driven tools for\napproximating high-dimensional or non-linear nuisance functions which are\nuseful in practice because the true functional form of the predictors is\nex-ante unknown. In this paper, we develop estimators of policy interventions\nfrom panel data which allow for non-linear effects of the confounding\nregressors, and investigate the performance of these estimators using three\nwell-known ML algorithms, specifically, LASSO, classification and regression\ntrees, and random forests. We use Double Machine Learning (DML) (Chernozhukov\net al., 2018) for the estimation of causal effects of homogeneous treatments\nwith unobserved individual heterogeneity (fixed effects) and no unobserved\nconfounding by extending Robinson (1988)'s partially linear regression model.\nWe develop three alternative approaches for handling unobserved individual\nheterogeneity based on extending the within-group estimator, first-difference\nestimator, and correlated random effect estimator (Mundlak, 1978) for\nnon-linear models. Using Monte Carlo simulations, we find that conventional\nleast squares estimators can perform well even if the data generating process\nis non-linear, but there are substantial performance gains in terms of bias\nreduction under a process where the true effect of the regressors is non-linear\nand discontinuous. However, for the same scenarios, we also find -- despite\nextensive hyperparameter tuning -- inference to be problematic for both\ntree-based learners because these lead to highly non-normal estimator\ndistributions and the estimator variance being severely under-estimated. This\ncontradicts the performance of trees in other circumstances and requires\nfurther investigation. Finally, we provide an illustrative example of DML for\nobservational panel data showing the impact of the introduction of the national\nminimum wage in the UK."}, "http://arxiv.org/abs/2201.11304": {"title": "Standard errors for two-way clustering with serially correlated time effects", "link": "http://arxiv.org/abs/2201.11304", "description": "We propose improved standard errors and an asymptotic distribution theory for\ntwo-way clustered panels. Our proposed estimator and theory allow for arbitrary\nserial dependence in the common time effects, which is excluded by existing\ntwo-way methods, including the popular two-way cluster standard errors of\nCameron, Gelbach, and Miller (2011) and the cluster bootstrap of Menzel (2021).\nOur asymptotic distribution theory is the first which allows for this level of\ninter-dependence among the observations. Under weak regularity conditions, we\ndemonstrate that the least squares estimator is asymptotically normal, our\nproposed variance estimator is consistent, and t-ratios are asymptotically\nstandard normal, permitting conventional inference. We present simulation\nevidence that confidence intervals constructed with our proposed standard\nerrors obtain superior coverage performance relative to existing methods. We\nillustrate the relevance of the proposed method in an empirical application to\na standard Fama-French three-factor regression."}, "http://arxiv.org/abs/2303.04416": {"title": "Inference on Optimal Dynamic Policies via Softmax Approximation", "link": "http://arxiv.org/abs/2303.04416", "description": "Estimating optimal dynamic policies from offline data is a fundamental\nproblem in dynamic decision making. In the context of causal inference, the\nproblem is known as estimating the optimal dynamic treatment regime. Even\nthough there exists a plethora of methods for estimation, constructing\nconfidence intervals for the value of the optimal regime and structural\nparameters associated with it is inherently harder, as it involves non-linear\nand non-differentiable functionals of unknown quantities that need to be\nestimated. Prior work resorted to sub-sample approaches that can deteriorate\nthe quality of the estimate. We show that a simple soft-max approximation to\nthe optimal treatment regime, for an appropriately fast growing temperature\nparameter, can achieve valid inference on the truly optimal regime. We\nillustrate our result for a two-period optimal dynamic regime, though our\napproach should directly extend to the finite horizon case. Our work combines\ntechniques from semi-parametric inference and $g$-estimation, together with an\nappropriate triangular array central limit theorem, as well as a novel analysis\nof the asymptotic influence and asymptotic bias of softmax approximations."}, "http://arxiv.org/abs/1904.00111": {"title": "Simple subvector inference on sharp identified set in affine models", "link": "http://arxiv.org/abs/1904.00111", "description": "This paper studies a regularized support function estimator for bounds on\ncomponents of the parameter vector in the case in which the identified set is a\npolygon. The proposed regularized estimator has three important properties: (i)\nit has a uniform asymptotic Gaussian limit in the presence of flat faces in the\nabsence of redundant (or overidentifying) constraints (or vice versa); (ii) the\nbias from regularization does not enter the first-order limiting\ndistribution;(iii) the estimator remains consistent for sharp identified set\nfor the individual components even in the non-regualar case. These properties\nare used to construct uniformly valid confidence sets for an element\n$\\theta_{1}$ of a parameter vector $\\theta\\in\\mathbb{R}^{d}$ that is partially\nidentified by affine moment equality and inequality conditions. The proposed\nconfidence sets can be computed as a solution to a small number of linear and\nconvex quadratic programs, which leads to a substantial decrease in computation\ntime and guarantees a global optimum. As a result, the method provides\nuniformly valid inference in applications in which the dimension of the\nparameter space, $d$, and the number of inequalities, $k$, were previously\ncomputationally unfeasible ($d,k=100$). The proposed approach can be extended\nto construct confidence sets for intersection bounds, to construct joint\npolygon-shaped confidence sets for multiple components of $\\theta$, and to find\nthe set of solutions to a linear program. Inference for coefficients in the\nlinear IV regression model with an interval outcome is used as an illustrative\nexample."}, "http://arxiv.org/abs/1911.04529": {"title": "Identification in discrete choice models with imperfect information", "link": "http://arxiv.org/abs/1911.04529", "description": "We study identification of preferences in static single-agent discrete choice\nmodels where decision makers may be imperfectly informed about the state of the\nworld. We leverage the notion of one-player Bayes Correlated Equilibrium by\nBergemann and Morris (2016) to provide a tractable characterization of the\nsharp identified set. We develop a procedure to practically construct the sharp\nidentified set following a sieve approach, and provide sharp bounds on\ncounterfactual outcomes of interest. We use our methodology and data on the\n2017 UK general election to estimate a spatial voting model under weak\nassumptions on agents' information about the returns to voting. Counterfactual\nexercises quantify the consequences of imperfect information on the well-being\nof voters and parties."}, "http://arxiv.org/abs/2312.10333": {"title": "Logit-based alternatives to two-stage least squares", "link": "http://arxiv.org/abs/2312.10333", "description": "We propose logit-based IV and augmented logit-based IV estimators that serve\nas alternatives to the traditionally used 2SLS estimator in the model where\nboth the endogenous treatment variable and the corresponding instrument are\nbinary. Our novel estimators are as easy to compute as the 2SLS estimator but\nhave an advantage over the 2SLS estimator in terms of causal interpretability.\nIn particular, in certain cases where the probability limits of both our\nestimators and the 2SLS estimator take the form of weighted-average treatment\neffects, our estimators are guaranteed to yield non-negative weights whereas\nthe 2SLS estimator is not."}, "http://arxiv.org/abs/2312.10487": {"title": "The Dynamic Triple Gamma Prior as a Shrinkage Process Prior for Time-Varying Parameter Models", "link": "http://arxiv.org/abs/2312.10487", "description": "Many current approaches to shrinkage within the time-varying parameter\nframework assume that each state is equipped with only one innovation variance\nfor all time points. Sparsity is then induced by shrinking this variance\ntowards zero. We argue that this is not sufficient if the states display large\njumps or structural changes, something which is often the case in time series\nanalysis. To remedy this, we propose the dynamic triple gamma prior, a\nstochastic process that has a well-known triple gamma marginal form, while\nstill allowing for autocorrelation. Crucially, the triple gamma has many\ninteresting limiting and special cases (including the horseshoe shrinkage\nprior) which can also be chosen as the marginal distribution. Not only is the\nmarginal form well understood, we further derive many interesting properties of\nthe dynamic triple gamma, which showcase its dynamic shrinkage characteristics.\nWe develop an efficient Markov chain Monte Carlo algorithm to sample from the\nposterior and demonstrate the performance through sparse covariance modeling\nand forecasting of the returns of the components of the EURO STOXX 50 index."}, "http://arxiv.org/abs/2312.10558": {"title": "Some Finite-Sample Results on the Hausman Test", "link": "http://arxiv.org/abs/2312.10558", "description": "This paper shows that the endogeneity test using the control function\napproach in linear instrumental variable models is a variant of the Hausman\ntest. Moreover, we find that the test statistics used in these tests can be\nnumerically ordered, indicating their relative power properties in finite\nsamples."}, "http://arxiv.org/abs/2312.10984": {"title": "Predicting Financial Literacy via Semi-supervised Learning", "link": "http://arxiv.org/abs/2312.10984", "description": "Financial literacy (FL) represents a person's ability to turn assets into\nincome, and understanding digital currencies has been added to the modern\ndefinition. FL can be predicted by exploiting unlabelled recorded data in\nfinancial networks via semi-supervised learning (SSL). Measuring and predicting\nFL has not been widely studied, resulting in limited understanding of customer\nfinancial engagement consequences. Previous studies have shown that low FL\nincreases the risk of social harm. Therefore, it is important to accurately\nestimate FL to allocate specific intervention programs to less financially\nliterate groups. This will not only increase company profitability, but will\nalso reduce government spending. Some studies considered predicting FL in\nclassification tasks, whereas others developed FL definitions and impacts. The\ncurrent paper investigated mechanisms to learn customer FL level from their\nfinancial data using sampling by synthetic minority over-sampling techniques\nfor regression with Gaussian noise (SMOGN). We propose the SMOGN-COREG model\nfor semi-supervised regression, applying SMOGN to deal with unbalanced datasets\nand a nonparametric multi-learner co-regression (COREG) algorithm for labeling.\nWe compared the SMOGN-COREG model with six well-known regressors on five\ndatasets to evaluate the proposed models effectiveness on unbalanced and\nunlabelled financial data. Experimental results confirmed that the proposed\nmethod outperformed the comparator models for unbalanced and unlabelled\nfinancial data. Therefore, SMOGN-COREG is a step towards using unlabelled data\nto estimate FL level."}, "http://arxiv.org/abs/2312.11283": {"title": "The 2010 Census Confidentiality Protections Failed, Here's How and Why", "link": "http://arxiv.org/abs/2312.11283", "description": "Using only 34 published tables, we reconstruct five variables (census block,\nsex, age, race, and ethnicity) in the confidential 2010 Census person records.\nUsing the 38-bin age variable tabulated at the census block level, at most\n20.1% of reconstructed records can differ from their confidential source on\neven a single value for these five variables. Using only published data, an\nattacker can verify that all records in 70% of all census blocks (97 million\npeople) are perfectly reconstructed. The tabular publications in Summary File 1\nthus have prohibited disclosure risk similar to the unreleased confidential\nmicrodata. Reidentification studies confirm that an attacker can, within blocks\nwith perfect reconstruction accuracy, correctly infer the actual census\nresponse on race and ethnicity for 3.4 million vulnerable population uniques\n(persons with nonmodal characteristics) with 95% accuracy, the same precision\nas the confidential data achieve and far greater than statistical baselines.\nThe flaw in the 2010 Census framework was the assumption that aggregation\nprevented accurate microdata reconstruction, justifying weaker disclosure\nlimitation methods than were applied to 2010 Census public microdata. The\nframework used for 2020 Census publications defends against attacks that are\nbased on reconstruction, as we also demonstrate here. Finally, we show that\nalternatives to the 2020 Census Disclosure Avoidance System with similar\naccuracy (enhanced swapping) also fail to protect confidentiality, and those\nthat partially defend against reconstruction attacks (incomplete suppression\nimplementations) destroy the primary statutory use case: data for redistricting\nall legislatures in the country in compliance with the 1965 Voting Rights Act."}, "http://arxiv.org/abs/1811.11603": {"title": "Distribution Regression with Sample Selection, with an Application to Wage Decompositions in the UK", "link": "http://arxiv.org/abs/1811.11603", "description": "We develop a distribution regression model under endogenous sample selection.\nThis model is a semi-parametric generalization of the Heckman selection model.\nIt accommodates much richer effects of the covariates on outcome distribution\nand patterns of heterogeneity in the selection process, and allows for drastic\ndepartures from the Gaussian error structure, while maintaining the same level\ntractability as the classical model. The model applies to continuous, discrete\nand mixed outcomes. We provide identification, estimation, and inference\nmethods, and apply them to obtain wage decomposition for the UK. Here we\ndecompose the difference between the male and female wage distributions into\ncomposition, wage structure, selection structure, and selection sorting\neffects. After controlling for endogenous employment selection, we still find\nsubstantial gender wage gap -- ranging from 21% to 40% throughout the (latent)\noffered wage distribution that is not explained by composition. We also uncover\npositive sorting for single men and negative sorting for married women that\naccounts for a substantive fraction of the gender wage gap at the top of the\ndistribution."}, "http://arxiv.org/abs/2204.01884": {"title": "Policy Learning with Competing Agents", "link": "http://arxiv.org/abs/2204.01884", "description": "Decision makers often aim to learn a treatment assignment policy under a\ncapacity constraint on the number of agents that they can treat. When agents\ncan respond strategically to such policies, competition arises, complicating\nestimation of the optimal policy. In this paper, we study capacity-constrained\ntreatment assignment in the presence of such interference. We consider a\ndynamic model where the decision maker allocates treatments at each time step\nand heterogeneous agents myopically best respond to the previous treatment\nassignment policy. When the number of agents is large but finite, we show that\nthe threshold for receiving treatment under a given policy converges to the\npolicy's mean-field equilibrium threshold. Based on this result, we develop a\nconsistent estimator for the policy gradient. In simulations and a\nsemi-synthetic experiment with data from the National Education Longitudinal\nStudy of 1988, we demonstrate that this estimator can be used for learning\ncapacity-constrained policies in the presence of strategic behavior."}, "http://arxiv.org/abs/2204.10359": {"title": "Boundary Adaptive Local Polynomial Conditional Density Estimators", "link": "http://arxiv.org/abs/2204.10359", "description": "We begin by introducing a class of conditional density estimators based on\nlocal polynomial techniques. The estimators are boundary adaptive and easy to\nimplement. We then study the (pointwise and) uniform statistical properties of\nthe estimators, offering characterizations of both probability concentration\nand distributional approximation. In particular, we establish uniform\nconvergence rates in probability and valid Gaussian distributional\napproximations for the Studentized t-statistic process. We also discuss\nimplementation issues such as consistent estimation of the covariance function\nfor the Gaussian approximation, optimal integrated mean squared error bandwidth\nselection, and valid robust bias-corrected inference. We illustrate the\napplicability of our results by constructing valid confidence bands and\nhypothesis tests for both parametric specification and shape constraints,\nexplicitly characterizing their approximation errors. A companion R software\npackage implementing our main results is provided."}, "http://arxiv.org/abs/2312.11710": {"title": "Real-time monitoring with RCA models", "link": "http://arxiv.org/abs/2312.11710", "description": "We propose a family of weighted statistics based on the CUSUM process of the\nWLS residuals for the online detection of changepoints in a Random Coefficient\nAutoregressive model, using both the standard CUSUM and the Page-CUSUM process.\nWe derive the asymptotics under the null of no changepoint for all possible\nweighing schemes, including the case of the standardised CUSUM, for which we\nderive a Darling-Erdos-type limit theorem; our results guarantee the\nprocedure-wise size control under both an open-ended and a closed-ended\nmonitoring. In addition to considering the standard RCA model with no\ncovariates, we also extend our results to the case of exogenous regressors. Our\nresults can be applied irrespective of (and with no prior knowledge required as\nto) whether the observations are stationary or not, and irrespective of whether\nthey change into a stationary or nonstationary regime. Hence, our methodology\nis particularly suited to detect the onset, or the collapse, of a bubble or an\nepidemic. Our simulations show that our procedures, especially when\nstandardising the CUSUM process, can ensure very good size control and short\ndetection delays. We complement our theory by studying the online detection of\nbreaks in epidemiological and housing prices series."}, "http://arxiv.org/abs/2304.05805": {"title": "GDP nowcasting with artificial neural networks: How much does long-term memory matter?", "link": "http://arxiv.org/abs/2304.05805", "description": "In our study, we apply artificial neural networks (ANNs) to nowcast quarterly\nGDP growth for the U.S. economy. Using the monthly FRED-MD database, we compare\nthe nowcasting performance of five different ANN architectures: the multilayer\nperceptron (MLP), the one-dimensional convolutional neural network (1D CNN),\nthe Elman recurrent neural network (RNN), the long short-term memory network\n(LSTM), and the gated recurrent unit (GRU). The empirical analysis presents the\nresults from two distinctively different evaluation periods. The first (2012:Q1\n-- 2019:Q4) is characterized by balanced economic growth, while the second\n(2012:Q1 -- 2022:Q4) also includes periods of the COVID-19 recession. According\nto our results, longer input sequences result in more accurate nowcasts in\nperiods of balanced economic growth. However, this effect ceases above a\nrelatively low threshold value of around six quarters (eighteen months). During\nperiods of economic turbulence (e.g., during the COVID-19 recession), longer\ninput sequences do not help the models' predictive performance; instead, they\nseem to weaken their generalization capability. Combined results from the two\nevaluation periods indicate that architectural features enabling for long-term\nmemory do not result in more accurate nowcasts. On the other hand, the 1D CNN\nhas proved to be a highly suitable model for GDP nowcasting. The network has\nshown good nowcasting performance among the competitors during the first\nevaluation period and achieved the overall best accuracy during the second\nevaluation period. Consequently, first in the literature, we propose the\napplication of the 1D CNN for economic nowcasting."}, "http://arxiv.org/abs/2312.12741": {"title": "Locally Optimal Fixed-Budget Best Arm Identification in Two-Armed Gaussian Bandits with Unknown Variances", "link": "http://arxiv.org/abs/2312.12741", "description": "We address the problem of best arm identification (BAI) with a fixed budget\nfor two-armed Gaussian bandits. In BAI, given multiple arms, we aim to find the\nbest arm, an arm with the highest expected reward, through an adaptive\nexperiment. Kaufmann et al. (2016) develops a lower bound for the probability\nof misidentifying the best arm. They also propose a strategy, assuming that the\nvariances of rewards are known, and show that it is asymptotically optimal in\nthe sense that its probability of misidentification matches the lower bound as\nthe budget approaches infinity. However, an asymptotically optimal strategy is\nunknown when the variances are unknown. For this open issue, we propose a\nstrategy that estimates variances during an adaptive experiment and draws arms\nwith a ratio of the estimated standard deviations. We refer to this strategy as\nthe Neyman Allocation (NA)-Augmented Inverse Probability weighting (AIPW)\nstrategy. We then demonstrate that this strategy is asymptotically optimal by\nshowing that its probability of misidentification matches the lower bound when\nthe budget approaches infinity, and the gap between the expected rewards of two\narms approaches zero (small-gap regime). Our results suggest that under the\nworst-case scenario characterized by the small-gap regime, our strategy, which\nemploys estimated variance, is asymptotically optimal even when the variances\nare unknown."}, "http://arxiv.org/abs/2312.13195": {"title": "Principal Component Copulas for Capital Modelling", "link": "http://arxiv.org/abs/2312.13195", "description": "We introduce a class of copulas that we call Principal Component Copulas.\nThis class intends to combine the strong points of copula-based techniques with\nprincipal component-based models, which results in flexibility when modelling\ntail dependence along the most important directions in multivariate data. The\nproposed techniques have conceptual similarities and technical differences with\nthe increasingly popular class of factor copulas. Such copulas can generate\ncomplex dependence structures and also perform well in high dimensions. We show\nthat Principal Component Copulas give rise to practical and technical\nadvantages compared to other techniques. We perform a simulation study and\napply the copula to multivariate return data. The copula class offers the\npossibility to avoid the curse of dimensionality when estimating very large\ncopula models and it performs particularly well on aggregate measures of tail\nrisk, which is of importance for capital modeling."}, "http://arxiv.org/abs/2103.07066": {"title": "Finding Subgroups with Significant Treatment Effects", "link": "http://arxiv.org/abs/2103.07066", "description": "Researchers often run resource-intensive randomized controlled trials (RCTs)\nto estimate the causal effects of interventions on outcomes of interest. Yet\nthese outcomes are often noisy, and estimated overall effects can be small or\nimprecise. Nevertheless, we may still be able to produce reliable evidence of\nthe efficacy of an intervention by finding subgroups with significant effects.\nIn this paper, we propose a machine-learning method that is specifically\noptimized for finding such subgroups in noisy data. Unlike available methods\nfor personalized treatment assignment, our tool is fundamentally designed to\ntake significance testing into account: it produces a subgroup that is chosen\nto maximize the probability of obtaining a statistically significant positive\ntreatment effect. We provide a computationally efficient implementation using\ndecision trees and demonstrate its gain over selecting subgroups based on\npositive (estimated) treatment effects. Compared to standard tree-based\nregression and classification tools, this approach tends to yield higher power\nin detecting subgroups affected by the treatment."}, "http://arxiv.org/abs/2208.06729": {"title": "Optimal Recovery for Causal Inference", "link": "http://arxiv.org/abs/2208.06729", "description": "Problems in causal inference can be fruitfully addressed using signal\nprocessing techniques. As an example, it is crucial to successfully quantify\nthe causal effects of an intervention to determine whether the intervention\nachieved desired outcomes. We present a new geometric signal processing\napproach to classical synthetic control called ellipsoidal optimal recovery\n(EOpR), for estimating the unobservable outcome of a treatment unit. EOpR\nprovides policy evaluators with both worst-case and typical outcomes to help in\ndecision making. It is an approximation-theoretic technique that relates to the\ntheory of principal components, which recovers unknown observations given a\nlearned signal class and a set of known observations. We show EOpR can improve\npre-treatment fit and mitigate bias of the post-treatment estimate relative to\nother methods in causal inference. Beyond recovery of the unit of interest, an\nadvantage of EOpR is that it produces worst-case limits over the estimates\nproduced. We assess our approach on artificially-generated data, on datasets\ncommonly used in the econometrics literature, and in the context of the\nCOVID-19 pandemic, showing better performance than baseline techniques"}, "http://arxiv.org/abs/2301.01085": {"title": "The Chained Difference-in-Differences", "link": "http://arxiv.org/abs/2301.01085", "description": "This paper studies the identification, estimation, and inference of long-term\n(binary) treatment effect parameters when balanced panel data is not available,\nor consists of only a subset of the available data. We develop a new estimator:\nthe chained difference-in-differences, which leverages the overlapping\nstructure of many unbalanced panel data sets. This approach consists in\naggregating a collection of short-term treatment effects estimated on multiple\nincomplete panels. Our estimator accommodates (1) multiple time periods, (2)\nvariation in treatment timing, (3) treatment effect heterogeneity, (4) general\nmissing data patterns, and (5) sample selection on observables. We establish\nthe asymptotic properties of the proposed estimator and discuss identification\nand efficiency gains in comparison to existing methods. Finally, we illustrate\nits relevance through (i) numerical simulations, and (ii) an application about\nthe effects of an innovation policy in France."}, "http://arxiv.org/abs/2312.13939": {"title": "Binary Endogenous Treatment in Stochastic Frontier Models with an Application to Soil Conservation in El Salvador", "link": "http://arxiv.org/abs/2312.13939", "description": "Improving the productivity of the agricultural sector is part of one of the\nSustainable Development Goals set by the United Nations. To this end, many\ninternational organizations have funded training and technology transfer\nprograms that aim to promote productivity and income growth, fight poverty and\nenhance food security among smallholder farmers in developing countries.\nStochastic production frontier analysis can be a useful tool when evaluating\nthe effectiveness of these programs. However, accounting for treatment\nendogeneity, often intrinsic to these interventions, only recently has received\nany attention in the stochastic frontier literature. In this work, we extend\nthe classical maximum likelihood estimation of stochastic production frontier\nmodels by allowing both the production frontier and inefficiency to depend on a\npotentially endogenous binary treatment. We use instrumental variables to\ndefine an assignment mechanism for the treatment, and we explicitly model the\ndensity of the first and second-stage composite error terms. We provide\nempirical evidence of the importance of controlling for endogeneity in this\nsetting using farm-level data from a soil conservation program in El Salvador."}, "http://arxiv.org/abs/2312.14095": {"title": "RetailSynth: Synthetic Data Generation for Retail AI Systems Evaluation", "link": "http://arxiv.org/abs/2312.14095", "description": "Significant research effort has been devoted in recent years to developing\npersonalized pricing, promotions, and product recommendation algorithms that\ncan leverage rich customer data to learn and earn. Systematic benchmarking and\nevaluation of these causal learning systems remains a critical challenge, due\nto the lack of suitable datasets and simulation environments. In this work, we\npropose a multi-stage model for simulating customer shopping behavior that\ncaptures important sources of heterogeneity, including price sensitivity and\npast experiences. We embedded this model into a working simulation environment\n-- RetailSynth. RetailSynth was carefully calibrated on publicly available\ngrocery data to create realistic synthetic shopping transactions. Multiple\npricing policies were implemented within the simulator and analyzed for impact\non revenue, category penetration, and customer retention. Applied researchers\ncan use RetailSynth to validate causal demand models for multi-category retail\nand to incorporate realistic price sensitivity into emerging benchmarking\nsuites for personalized pricing, promotions, and product recommendations."}, "http://arxiv.org/abs/2201.06898": {"title": "Difference-in-Differences Estimators for Treatments Continuously Distributed at Every Period", "link": "http://arxiv.org/abs/2201.06898", "description": "We propose difference-in-differences estimators for continuous treatments\nwith heterogeneous effects. We assume that between consecutive periods, the\ntreatment of some units, the switchers, changes, while the treatment of other\nunits does not change. We show that under a parallel trends assumption, an\nunweighted and a weighted average of the slopes of switchers' potential\noutcomes can be estimated. While the former parameter may be more intuitive,\nthe latter can be used for cost-benefit analysis, and it can often be estimated\nmore precisely. We generalize our estimators to the instrumental-variable case.\nWe use our results to estimate the price-elasticity of gasoline consumption."}, "http://arxiv.org/abs/2211.14236": {"title": "Strategyproof Decision-Making in Panel Data Settings and Beyond", "link": "http://arxiv.org/abs/2211.14236", "description": "We consider the problem of decision-making using panel data, in which a\ndecision-maker gets noisy, repeated measurements of multiple units (or agents).\nWe consider a setup where there is a pre-intervention period, when the\nprincipal observes the outcomes of each unit, after which the principal uses\nthese observations to assign a treatment to each unit. Unlike this classical\nsetting, we permit the units generating the panel data to be strategic, i.e.\nunits may modify their pre-intervention outcomes in order to receive a more\ndesirable intervention. The principal's goal is to design a strategyproof\nintervention policy, i.e. a policy that assigns units to their\nutility-maximizing interventions despite their potential strategizing. We first\nidentify a necessary and sufficient condition under which a strategyproof\nintervention policy exists, and provide a strategyproof mechanism with a simple\nclosed form when one does exist. Along the way, we prove impossibility results\nfor strategic multiclass classification, which may be of independent interest.\nWhen there are two interventions, we establish that there always exists a\nstrategyproof mechanism, and provide an algorithm for learning such a\nmechanism. For three or more interventions, we provide an algorithm for\nlearning a strategyproof mechanism if there exists a sufficiently large gap in\nthe principal's rewards between different interventions. Finally, we\nempirically evaluate our model using real-world panel data collected from\nproduct sales over 18 months. We find that our methods compare favorably to\nbaselines which do not take strategic interactions into consideration, even in\nthe presence of model misspecification."}, "http://arxiv.org/abs/2312.14191": {"title": "Noisy Measurements Are Important, the Design of Census Products Is Much More Important", "link": "http://arxiv.org/abs/2312.14191", "description": "McCartan et al. (2023) call for \"making differential privacy work for census\ndata users.\" This commentary explains why the 2020 Census Noisy Measurement\nFiles (NMFs) are not the best focus for that plea. The August 2021 letter from\n62 prominent researchers asking for production of the direct output of the\ndifferential privacy system deployed for the 2020 Census signaled the\nengagement of the scholarly community in the design of decennial census data\nproducts. NMFs, the raw statistics produced by the 2020 Census Disclosure\nAvoidance System before any post-processing, are one component of that\ndesign--the query strategy output. The more important component is the query\nworkload output--the statistics released to the public. Optimizing the query\nworkload--the Redistricting Data (P.L. 94-171) Summary File,\nspecifically--could allow the privacy-loss budget to be more effectively\nmanaged. There could be fewer noisy measurements, no post-processing bias, and\ndirect estimates of the uncertainty from disclosure avoidance for each\npublished statistic."}, "http://arxiv.org/abs/2312.14325": {"title": "Exploring Distributions of House Prices and House Price Indices", "link": "http://arxiv.org/abs/2312.14325", "description": "We use house prices (HP) and house price indices (HPI) as a proxy to income\ndistribution. Specifically, we analyze sale prices in the 1970-2010 window of\nover 116,000 single-family homes in Hamilton County, Ohio, including Cincinnati\nmetro area of about 2.2 million people. We also analyze HPI, published by\nFederal Housing Finance Agency (FHFA), for nearly 18,000 US ZIP codes that\ncover a period of over 40 years starting in 1980's. If HP can be viewed as a\nfirst derivative of income, HPI can be viewed as its second derivative. We use\ngeneralized beta (GB) family of functions to fit distributions of HP and HPI\nsince GB naturally arises from the models of economic exchange described by\nstochastic differential equations. Our main finding is that HP and multi-year\nHPI exhibit a negative Dragon King (nDK) behavior, wherein power-law\ndistribution tail gives way to an abrupt decay to a finite upper limit value,\nwhich is similar to our recent findings for realized volatility of S\\&amp;P500\nindex in the US stock market. This type of tail behavior is best fitted by a\nmodified GB (mGB) distribution. Tails of single-year HPI appear to show more\nconsistency with power-law behavior, which is better described by a GB Prime\n(GB2) distribution. We supplement full distribution fits by mGB and GB2 with\ndirect linear fits (LF) of the tails. Our numerical procedure relies on\nevaluation of confidence intervals (CI) of the fits, as well as of p-values\nthat give the likelihood that data come from the fitted distributions."}, "http://arxiv.org/abs/2207.09943": {"title": "Efficient Bias Correction for Cross-section and Panel Data", "link": "http://arxiv.org/abs/2207.09943", "description": "Bias correction can often improve the finite sample performance of\nestimators. We show that the choice of bias correction method has no effect on\nthe higher-order variance of semiparametrically efficient parametric\nestimators, so long as the estimate of the bias is asymptotically linear. It is\nalso shown that bootstrap, jackknife, and analytical bias estimates are\nasymptotically linear for estimators with higher-order expansions of a standard\nform. In particular, we find that for a variety of estimators the\nstraightforward bootstrap bias correction gives the same higher-order variance\nas more complicated analytical or jackknife bias corrections. In contrast, bias\ncorrections that do not estimate the bias at the parametric rate, such as the\nsplit-sample jackknife, result in larger higher-order variances in the i.i.d.\nsetting we focus on. For both a cross-sectional MLE and a panel model with\nindividual fixed effects, we show that the split-sample jackknife has a\nhigher-order variance term that is twice as large as that of the\n`leave-one-out' jackknife."}, "http://arxiv.org/abs/2312.15119": {"title": "Functional CLTs for subordinated stable L\\'evy models in physics, finance, and econometrics", "link": "http://arxiv.org/abs/2312.15119", "description": "We present a simple unifying treatment of a large class of applications from\nstatistical mechanics, econometrics, mathematical finance, and insurance\nmathematics, where stable (possibly subordinated) L\\'evy noise arises as a\nscaling limit of some form of continuous-time random walk (CTRW). For each\napplication, it is natural to rely on weak convergence results for stochastic\nintegrals on Skorokhod space in Skorokhod's J1 or M1 topologies. As compared to\nearlier and entirely separate works, we are able to give a more streamlined\naccount while also allowing for greater generality and providing important new\ninsights. For each application, we first make clear how the fundamental\nconclusions for J1 convergent CTRWs emerge as special cases of the same general\nprinciples, and we then illustrate how the specific settings give rise to\ndifferent results for strictly M1 convergent CTRWs."}, "http://arxiv.org/abs/2312.15494": {"title": "Variable Selection in High Dimensional Linear Regressions with Parameter Instability", "link": "http://arxiv.org/abs/2312.15494", "description": "This paper is concerned with the problem of variable selection in the\npresence of parameter instability when both the marginal effects of signals on\nthe target variable and the correlations of the covariates in the active set\ncould vary over time. We pose the issue of whether one should use weighted or\nunweighted observations at the variable selection stage in the presence of\nparameter instability, particularly when the number of potential covariates is\nlarge. We allow parameter instability to be continuous or discrete, subject to\ncertain regularity conditions. We discuss the pros and cons of Lasso and the\nOne Covariate at a time Multiple Testing (OCMT) method for variable selection\nand argue that OCMT has important advantages under parameter instability. We\nestablish three main theorems on selection, estimation post selection, and\nin-sample fit. These theorems provide justification for using unweighted\nobservations at the selection stage of OCMT and down-weighting of observations\nonly at the forecasting stage. It is shown that OCMT delivers better forecasts,\nin mean squared error sense, as compared to Lasso, Adaptive Lasso and boosting\nboth in Monte Carlo experiments as well as in 3 sets of empirical applications:\nforecasting monthly returns on 28 stocks from Dow Jones , forecasting quarterly\noutput growths across 33 countries, and forecasting euro area output growth\nusing surveys of professional forecasters."}, "http://arxiv.org/abs/2312.15524": {"title": "The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective", "link": "http://arxiv.org/abs/2312.15524", "description": "Large Language Models (LLMs) have demonstrated impressive potential to\nsimulate human behavior. Using a causal inference framework, we empirically and\ntheoretically analyze the challenges of conducting LLM-simulated experiments,\nand explore potential solutions. In the context of demand estimation, we show\nthat variations in the treatment included in the prompt (e.g., price of focal\nproduct) can cause variations in unspecified confounding factors (e.g., price\nof competitors, historical prices, outside temperature), introducing\nendogeneity and yielding implausibly flat demand curves. We propose a\ntheoretical framework suggesting this endogeneity issue generalizes to other\ncontexts and won't be fully resolved by merely improving the training data.\nUnlike real experiments where researchers assign pre-existing units across\nconditions, LLMs simulate units based on the entire prompt, which includes the\ndescription of the treatment. Therefore, due to associations in the training\ndata, the characteristics of individuals and environments simulated by the LLM\ncan be affected by the treatment assignment. We explore two potential\nsolutions. The first specifies all contextual variables that affect both\ntreatment and outcome, which we demonstrate to be challenging for a\ngeneral-purpose LLM. The second explicitly specifies the source of treatment\nvariation in the prompt given to the LLM (e.g., by informing the LLM that the\nstore is running an experiment). While this approach only allows the estimation\nof a conditional average treatment effect that depends on the specific\nexperimental design, it provides valuable directional results for exploratory\nanalysis."}, "http://arxiv.org/abs/2312.15595": {"title": "Zero-Inflated Bandits", "link": "http://arxiv.org/abs/2312.15595", "description": "Many real applications of bandits have sparse non-zero rewards, leading to\nslow learning rates. A careful distribution modeling that utilizes\nproblem-specific structures is known as critical to estimation efficiency in\nthe statistics literature, yet is under-explored in bandits. To fill the gap,\nwe initiate the study of zero-inflated bandits, where the reward is modeled as\na classic semi-parametric distribution called zero-inflated distribution. We\ncarefully design Upper Confidence Bound (UCB) and Thompson Sampling (TS)\nalgorithms for this specific structure. Our algorithms are suitable for a very\ngeneral class of reward distributions, operating under tail assumptions that\nare considerably less stringent than the typical sub-Gaussian requirements.\nTheoretically, we derive the regret bounds for both the UCB and TS algorithms\nfor multi-armed bandit, showing that they can achieve rate-optimal regret when\nthe reward distribution is sub-Gaussian. The superior empirical performance of\nthe proposed methods is shown via extensive numerical studies."}, "http://arxiv.org/abs/2312.15624": {"title": "Negative Controls for Instrumental Variable Designs", "link": "http://arxiv.org/abs/2312.15624", "description": "Studies using instrumental variables (IV) often assess the validity of their\nidentification assumptions using falsification tests. However, these tests are\noften carried out in an ad-hoc manner, without theoretical foundations. In this\npaper, we establish a theoretical framework for negative control tests, the\npredominant category of falsification tests for IV designs. These tests are\nconditional independence tests between negative control variables and either\nthe IV or the outcome (e.g., examining the ``effect'' on the lagged outcome).\nWe introduce a formal definition for threats to IV exogeneity (alternative path\nvariables) and characterize the necessary conditions that proxy variables for\nsuch unobserved threats must meet to serve as negative controls. The theory\nhighlights prevalent errors in the implementation of negative control tests and\nhow they could be corrected. Our theory can also be used to design new\nfalsification tests by identifying appropriate negative control variables,\nincluding currently underutilized types, and suggesting alternative statistical\ntests. The theory shows that all negative control tests assess IV exogeneity.\nHowever, some commonly used tests simultaneously evaluate the 2SLS functional\nform assumptions. Lastly, we show that while negative controls are useful for\ndetecting biases in IV designs, their capacity to correct or quantify such\nbiases requires additional non-trivial assumptions."}, "http://arxiv.org/abs/2312.15999": {"title": "Pricing with Contextual Elasticity and Heteroscedastic Valuation", "link": "http://arxiv.org/abs/2312.15999", "description": "We study an online contextual dynamic pricing problem, where customers decide\nwhether to purchase a product based on its features and price. We introduce a\nnovel approach to modeling a customer's expected demand by incorporating\nfeature-based price elasticity, which can be equivalently represented as a\nvaluation with heteroscedastic noise. To solve the problem, we propose a\ncomputationally efficient algorithm called \"Pricing with Perturbation (PwP)\",\nwhich enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary\nadversarial input context sequences. We also prove a matching lower bound at\n$\\Omega(\\sqrt{dT})$ to show the optimality regarding $d$ and $T$ (up to $\\log\nT$ factors). Our results shed light on the relationship between contextual\nelasticity and heteroscedastic valuation, providing insights for effective and\npractical pricing strategies."}, "http://arxiv.org/abs/2312.16099": {"title": "Direct Multi-Step Forecast based Comparison of Nested Models via an Encompassing Test", "link": "http://arxiv.org/abs/2312.16099", "description": "We introduce a novel approach for comparing out-of-sample multi-step\nforecasts obtained from a pair of nested models that is based on the forecast\nencompassing principle. Our proposed approach relies on an alternative way of\ntesting the population moment restriction implied by the forecast encompassing\nprinciple and that links the forecast errors from the two competing models in a\nparticular way. Its key advantage is that it is able to bypass the variance\ndegeneracy problem afflicting model based forecast comparisons across nested\nmodels. It results in a test statistic whose limiting distribution is standard\nnormal and which is particularly simple to construct and can accommodate both\nsingle period and longer-horizon prediction comparisons. Inferences are also\nshown to be robust to different predictor types, including stationary,\nhighly-persistent and purely deterministic processes. Finally, we illustrate\nthe use of our proposed approach through an empirical application that explores\nthe role of global inflation in enhancing individual country specific inflation\nforecasts."}, "http://arxiv.org/abs/2010.05117": {"title": "Combining Observational and Experimental Data to Improve Efficiency Using Imperfect Instruments", "link": "http://arxiv.org/abs/2010.05117", "description": "Randomized controlled trials generate experimental variation that can\ncredibly identify causal effects, but often suffer from limited scale, while\nobservational datasets are large, but often violate desired identification\nassumptions. To improve estimation efficiency, I propose a method that\nleverages imperfect instruments - pretreatment covariates that satisfy the\nrelevance condition but may violate the exclusion restriction. I show that\nthese imperfect instruments can be used to derive moment restrictions that, in\ncombination with the experimental data, improve estimation efficiency. I\noutline estimators for implementing this strategy, and show that my methods can\nreduce variance by up to 50%; therefore, only half of the experimental sample\nis required to attain the same statistical precision. I apply my method to a\nsearch listing dataset from Expedia that studies the causal effect of search\nrankings on clicks, and show that the method can substantially improve the\nprecision."}, "http://arxiv.org/abs/2105.12891": {"title": "Identification and Estimation of Partial Effects in Nonlinear Semiparametric Panel Models", "link": "http://arxiv.org/abs/2105.12891", "description": "Average partial effects (APEs) are often not point identified in panel models\nwith unrestricted unobserved heterogeneity, such as binary response panel model\nwith fixed effects and logistic errors. This lack of point-identification\noccurs despite the identification of these models' common coefficients. We\nprovide a unified framework to establish the point identification of various\npartial effects in a wide class of nonlinear semiparametric models under an\nindex sufficiency assumption on the unobserved heterogeneity, even when the\nerror distribution is unspecified and non-stationary. This assumption does not\nimpose parametric restrictions on the unobserved heterogeneity and\nidiosyncratic errors. We also present partial identification results when the\nsupport condition fails. We then propose three-step semiparametric estimators\nfor the APE, the average structural function, and average marginal effects, and\nshow their consistency and asymptotic normality. Finally, we illustrate our\napproach in a study of determinants of married women's labor supply."}, "http://arxiv.org/abs/2212.11833": {"title": "Efficient Sampling for Realized Variance Estimation in Time-Changed Diffusion Models", "link": "http://arxiv.org/abs/2212.11833", "description": "This paper analyzes the benefits of sampling intraday returns in intrinsic\ntime for the standard and pre-averaging realized variance (RV) estimators. We\ntheoretically show in finite samples and asymptotically that the RV estimator\nis most efficient under the new concept of realized business time, which\nsamples according to a combination of observed trades and estimated tick\nvariance. Our asymptotic results carry over to the pre-averaging RV estimator\nunder market microstructure noise. The analysis builds on the assumption that\nasset prices follow a diffusion that is time-changed with a jump process that\nseparately models the transaction times. This provides a flexible model that\nseparately captures the empirically varying trading intensity and tick variance\nprocesses, which are particularly relevant for disentangling the driving forces\nof the sampling schemes. Extensive simulations confirm our theoretical results\nand show that realized business time remains superior also under more general\nnoise and process specifications. An application to stock data provides\nempirical evidence for the benefits of using realized business time sampling to\nconstruct more efficient RV estimators as well as for an improved forecasting\nperformance."}, "http://arxiv.org/abs/2301.05580": {"title": "Randomization Test for the Specification of Interference Structure", "link": "http://arxiv.org/abs/2301.05580", "description": "This study considers testing the specification of spillover effects in causal\ninference. We focus on experimental settings in which the treatment assignment\nmechanism is known to researchers. We develop a new randomization test\nutilizing a hierarchical relationship between different exposures. Compared\nwith existing approaches, our approach is essentially applicable to any null\nexposure specifications and produces powerful test statistics without a priori\nknowledge of the true interference structure. As empirical illustrations, we\nrevisit two existing social network experiments: one on farmers' insurance\nadoption and the other on anti-conflict education programs."}, "http://arxiv.org/abs/2312.16214": {"title": "Stochastic Equilibrium the Lucas Critique and Keynesian Economics", "link": "http://arxiv.org/abs/2312.16214", "description": "In this paper, a mathematically rigorous solution overturns existing wisdom\nregarding New Keynesian Dynamic Stochastic General Equilibrium. I develop a\nformal concept of stochastic equilibrium. I prove uniqueness and necessity,\nwhen agents are patient, across a wide class of dynamic stochastic models.\nExistence depends on appropriately specified eigenvalue conditions. Otherwise,\nno solution of any kind exists. I construct the equilibrium for the benchmark\nCalvo New Keynesian. I provide novel comparative statics with the\nnon-stochastic model of independent mathematical interest. I uncover a\nbifurcation between neighbouring stochastic systems and approximations taken\nfrom the Zero Inflation Non-Stochastic Steady State (ZINSS). The correct\nPhillips curve agrees with the zero limit from the trend inflation framework.\nIt contains a large lagged inflation coefficient and a small response to\nexpected inflation. The response to the output gap is always muted and is zero\nat standard parameters. A neutrality result is presented to explain why and to\nalign Calvo with Taylor pricing. Present and lagged demand shocks enter the\nPhillips curve so there is no Divine Coincidence and the system is identified\nfrom structural shocks alone. The lagged inflation slope is increasing in the\ninflation response, embodying substantive policy trade-offs. The Taylor\nprinciple is reversed, inactive settings are necessary for existence, pointing\ntowards inertial policy. The observational equivalence idea of the Lucas\ncritique is disproven. The bifurcation results from the breakdown of the\nconstraints implied by lagged nominal rigidity, associated with cross-equation\ncancellation possible only at ZINSS. There is a dual relationship between\nrestrictions on the econometrician and constraints on repricing firms. Thus if\nthe model is correct, goodness of fit will jump."}, "http://arxiv.org/abs/2312.16307": {"title": "Incentive-Aware Synthetic Control: Accurate Counterfactual Estimation via Incentivized Exploration", "link": "http://arxiv.org/abs/2312.16307", "description": "We consider a panel data setting in which one observes measurements of units\nover time, under different interventions. Our focus is on the canonical family\nof synthetic control methods (SCMs) which, after a pre-intervention time period\nwhen all units are under control, estimate counterfactual outcomes for test\nunits in the post-intervention time period under control by using data from\ndonor units who have remained under control for the entire post-intervention\nperiod. In order for the counterfactual estimate produced by synthetic control\nfor a test unit to be accurate, there must be sufficient overlap between the\noutcomes of the donor units and the outcomes of the test unit. As a result, a\ncanonical assumption in the literature on SCMs is that the outcomes for the\ntest units lie within either the convex hull or the linear span of the outcomes\nfor the donor units. However despite their ubiquity, such overlap assumptions\nmay not always hold, as is the case when, e.g., units select their own\ninterventions and different subpopulations of units prefer different\ninterventions a priori.\n\nWe shed light on this typically overlooked assumption, and we address this\nissue by incentivizing units with different preferences to take interventions\nthey would not normally consider. Specifically, we provide a SCM for\nincentivizing exploration in panel data settings which provides\nincentive-compatible intervention recommendations to units by leveraging tools\nfrom information design and online learning. Using our algorithm, we show how\nto obtain valid counterfactual estimates using SCMs without the need for an\nexplicit overlap assumption on the unit outcomes."}, "http://arxiv.org/abs/2312.16489": {"title": "Best-of-Both-Worlds Linear Contextual Bandits", "link": "http://arxiv.org/abs/2312.16489", "description": "This study investigates the problem of $K$-armed linear contextual bandits,\nan instance of the multi-armed bandit problem, under an adversarial corruption.\nAt each round, a decision-maker observes an independent and identically\ndistributed context and then selects an arm based on the context and past\nobservations. After selecting an arm, the decision-maker incurs a loss\ncorresponding to the selected arm. The decision-maker aims to minimize the\ncumulative loss over the trial. The goal of this study is to develop a strategy\nthat is effective in both stochastic and adversarial environments, with\ntheoretical guarantees. We first formulate the problem by introducing a novel\nsetting of bandits with adversarial corruption, referred to as the contextual\nadversarial regime with a self-bounding constraint. We assume linear models for\nthe relationship between the loss and the context. Then, we propose a strategy\nthat extends the RealLinExp3 by Neu &amp; Olkhovskaya (2020) and the\nFollow-The-Regularized-Leader (FTRL). The regret of our proposed algorithm is\nshown to be upper-bounded by $O\\left(\\min\\left\\{\\frac{(\\log(T))^3}{\\Delta_{*}}\n+ \\sqrt{\\frac{C(\\log(T))^3}{\\Delta_{*}}},\\ \\\n\\sqrt{T}(\\log(T))^2\\right\\}\\right)$, where $T \\in\\mathbb{N}$ is the number of\nrounds, $\\Delta_{*} &gt; 0$ is the constant minimum gap between the best and\nsuboptimal arms for any context, and $C\\in[0, T] $ is an adversarial corruption\nparameter. This regret upper bound implies\n$O\\left(\\frac{(\\log(T))^3}{\\Delta_{*}}\\right)$ in a stochastic environment and\nby $O\\left( \\sqrt{T}(\\log(T))^2\\right)$ in an adversarial environment. We refer\nto our strategy as the Best-of-Both-Worlds (BoBW) RealFTRL, due to its\ntheoretical guarantees in both stochastic and adversarial regimes."}, "http://arxiv.org/abs/2312.16707": {"title": "Modeling Systemic Risk: A Time-Varying Nonparametric Causal Inference Framework", "link": "http://arxiv.org/abs/2312.16707", "description": "We propose a nonparametric and time-varying directed information graph\n(TV-DIG) framework to estimate the evolving causal structure in time series\nnetworks, thereby addressing the limitations of traditional econometric models\nin capturing high-dimensional, nonlinear, and time-varying interconnections\namong series. This framework employs an information-theoretic measure rooted in\na generalized version of Granger-causality, which is applicable to both linear\nand nonlinear dynamics. Our framework offers advancements in measuring systemic\nrisk and establishes meaningful connections with established econometric\nmodels, including vector autoregression and switching models. We evaluate the\nefficacy of our proposed model through simulation experiments and empirical\nanalysis, reporting promising results in recovering simulated time-varying\nnetworks with nonlinear and multivariate structures. We apply this framework to\nidentify and monitor the evolution of interconnectedness and systemic risk\namong major assets and industrial sectors within the financial network. We\nfocus on cryptocurrencies' potential systemic risks to financial stability,\nincluding spillover effects on other sectors during crises like the COVID-19\npandemic and the Federal Reserve's 2020 emergency response. Our findings\nreveals significant, previously underrecognized pre-2020 influences of\ncryptocurrencies on certain financial sectors, highlighting their potential\nsystemic risks and offering a systematic approach in tracking evolving\ncross-sector interactions within financial networks."}, "http://arxiv.org/abs/2312.16927": {"title": "Development of Choice Model for Brand Evaluation", "link": "http://arxiv.org/abs/2312.16927", "description": "Consumer choice modeling takes center stage as we delve into understanding\nhow personal preferences of decision makers (customers) for products influence\ndemand at the level of the individual. The contemporary choice theory is built\nupon the characteristics of the decision maker, alternatives available for the\nchoice of the decision maker, the attributes of the available alternatives and\ndecision rules that the decision maker uses to make a choice. The choice set in\nour research is represented by six major brands (products) of laundry\ndetergents in the Japanese market. We use the panel data of the purchases of 98\nhouseholds to which we apply the hierarchical probit model, facilitated by a\nMarkov Chain Monte Carlo simulation (MCMC) in order to evaluate the brand\nvalues of six brands. The applied model also allows us to evaluate the tangible\nand intangible brand values. These evaluated metrics help us to assess the\nbrands based on their tangible and intangible characteristics. Moreover,\nconsumer choice modeling also provides a framework for assessing the\nenvironmental performance of laundry detergent brands as the model uses the\ninformation on components (physical attributes) of laundry detergents."}, "http://arxiv.org/abs/2312.17061": {"title": "Bayesian Analysis of High Dimensional Vector Error Correction Model", "link": "http://arxiv.org/abs/2312.17061", "description": "Vector Error Correction Model (VECM) is a classic method to analyse\ncointegration relationships amongst multivariate non-stationary time series. In\nthis paper, we focus on high dimensional setting and seek for\nsample-size-efficient methodology to determine the level of cointegration. Our\ninvestigation centres at a Bayesian approach to analyse the cointegration\nmatrix, henceforth determining the cointegration rank. We design two algorithms\nand implement them on simulated examples, yielding promising results\nparticularly when dealing with high number of variables and relatively low\nnumber of observations. Furthermore, we extend this methodology to empirically\ninvestigate the constituents of the S&amp;P 500 index, where low-volatility\nportfolios can be found during both in-sample training and out-of-sample\ntesting periods."}, "http://arxiv.org/abs/1903.08028": {"title": "State-Building through Public Land Disposal? An Application of Matrix Completion for Counterfactual Prediction", "link": "http://arxiv.org/abs/1903.08028", "description": "This paper examines how homestead policies, which opened vast frontier lands\nfor settlement, influenced the development of American frontier states. It uses\na treatment propensity-weighted matrix completion model to estimate the\ncounterfactual size of these states without homesteading. In simulation\nstudies, the method shows lower bias and variance than other estimators,\nparticularly in higher complexity scenarios. The empirical analysis reveals\nthat homestead policies significantly and persistently reduced state government\nexpenditure and revenue. These findings align with continuous\ndifference-in-differences estimates using 1.46 million land patent records.\nThis study's extension of the matrix completion method to include propensity\nscore weighting for causal effect estimation in panel data, especially in\nstaggered treatment contexts, enhances policy evaluation by improving the\nprecision of long-term policy impact assessments."}, "http://arxiv.org/abs/2011.08174": {"title": "Policy design in experiments with unknown interference", "link": "http://arxiv.org/abs/2011.08174", "description": "This paper studies experimental designs for estimation and inference on\npolicies with spillover effects. Units are organized into a finite number of\nlarge clusters and interact in unknown ways within each cluster. First, we\nintroduce a single-wave experiment that, by varying the randomization across\ncluster pairs, estimates the marginal effect of a change in treatment\nprobabilities, taking spillover effects into account. Using the marginal\neffect, we propose a test for policy optimality. Second, we design a\nmultiple-wave experiment to estimate welfare-maximizing treatment rules. We\nprovide strong theoretical guarantees and an implementation in a large-scale\nfield experiment."}, "http://arxiv.org/abs/2104.13367": {"title": "When should you adjust inferences for multiple hypothesis testing?", "link": "http://arxiv.org/abs/2104.13367", "description": "Multiple hypothesis testing practices vary widely, without consensus on which\nare appropriate when. We provide an economic foundation for these practices. In\nstudies of multiple treatments or sub-populations, adjustments may be\nappropriate depending on scale economies in the research production function,\nwith control of classical notions of compound errors emerging in some but not\nall cases. Studies with multiple outcomes motivate testing using a single\nindex, or adjusted tests of several indices when the intended audience is\nheterogeneous. Data on actual research costs suggest both that some adjustment\nis warranted and that standard procedures are overly conservative."}, "http://arxiv.org/abs/2303.11777": {"title": "Quasi Maximum Likelihood Estimation of High-Dimensional Factor Models: A Critical Review", "link": "http://arxiv.org/abs/2303.11777", "description": "We review Quasi Maximum Likelihood estimation of factor models for\nhigh-dimensional panels of time series. We consider two cases: (1) estimation\nwhen no dynamic model for the factors is specified \\citep{baili12,baili16}; (2)\nestimation based on the Kalman smoother and the Expectation Maximization\nalgorithm thus allowing to model explicitly the factor dynamics\n\\citep{DGRqml,BLqml}. Our interest is in approximate factor models, i.e., when\nwe allow for the idiosyncratic components to be mildly cross-sectionally, as\nwell as serially, correlated. Although such setting apparently makes estimation\nharder, we show, in fact, that factor models do not suffer of the {\\it curse of\ndimensionality} problem, but instead they enjoy a {\\it blessing of\ndimensionality} property. In particular, given an approximate factor structure,\nif the cross-sectional dimension of the data, $N$, grows to infinity, we show\nthat: (i) identification of the model is still possible, (ii) the\nmis-specification error due to the use of an exact factor model log-likelihood\nvanishes. Moreover, if we let also the sample size, $T$, grow to infinity, we\ncan also consistently estimate all parameters of the model and make inference.\nThe same is true for estimation of the latent factors which can be carried out\nby weighted least-squares, linear projection, or Kalman filtering/smoothing. We\nalso compare the approaches presented with: Principal Component analysis and\nthe classical, fixed $N$, exact Maximum Likelihood approach. We conclude with a\ndiscussion on efficiency of the considered estimators."}, "http://arxiv.org/abs/2305.08559": {"title": "Designing Discontinuities", "link": "http://arxiv.org/abs/2305.08559", "description": "Discontinuities can be fairly arbitrary but also cause a significant impact\non outcomes in larger systems. Indeed, their arbitrariness is why they have\nbeen used to infer causal relationships among variables in numerous settings.\nRegression discontinuity from econometrics assumes the existence of a\ndiscontinuous variable that splits the population into distinct partitions to\nestimate the causal effects of a given phenomenon. Here we consider the design\nof partitions for a given discontinuous variable to optimize a certain effect\npreviously studied using regression discontinuity. To do so, we propose a\nquantization-theoretic approach to optimize the effect of interest, first\nlearning the causal effect size of a given discontinuous variable and then\napplying dynamic programming for optimal quantization design of discontinuities\nto balance the gain and loss in that effect size. We also develop a\ncomputationally-efficient reinforcement learning algorithm for the dynamic\nprogramming formulation of optimal quantization. We demonstrate our approach by\ndesigning optimal time zone borders for counterfactuals of social capital,\nsocial mobility, and health. This is based on regression discontinuity analyses\nwe perform on novel data, which may be of independent empirical interest."}, "http://arxiv.org/abs/2309.09299": {"title": "Bounds on Average Effects in Discrete Choice Panel Data Models", "link": "http://arxiv.org/abs/2309.09299", "description": "In discrete choice panel data, the estimation of average effects is crucial\nfor quantifying the effect of covariates, and for policy evaluation and\ncounterfactual analysis. This task is challenging in short panels with\nindividual-specific effects due to partial identification and the incidental\nparameter problem. While consistent estimation of the identified set is\npossible, it generally requires very large sample sizes, especially when the\nnumber of support points of the observed covariates is large, such as when the\ncovariates are continuous. In this paper, we propose estimating outer bounds on\nthe identified set of average effects. Our bounds are easy to construct,\nconverge at the parametric rate, and are computationally simple to obtain even\nin moderately large samples, independent of whether the covariates are discrete\nor continuous. We also provide asymptotically valid confidence intervals on the\nidentified set. Simulation studies confirm that our approach works well and is\ninformative in finite samples. We also consider an application to labor force\nparticipation."}, "http://arxiv.org/abs/2312.17623": {"title": "Decision Theory for Treatment Choice Problems with Partial Identification", "link": "http://arxiv.org/abs/2312.17623", "description": "We apply classical statistical decision theory to a large class of treatment\nchoice problems with partial identification, revealing important theoretical\nand practical challenges but also interesting research opportunities. The\nchallenges are: In a general class of problems with Gaussian likelihood, all\ndecision rules are admissible; it is maximin-welfare optimal to ignore all\ndata; and, for severe enough partial identification, there are infinitely many\nminimax-regret optimal decision rules, all of which sometimes randomize the\npolicy recommendation. The opportunities are: We introduce a profiled regret\ncriterion that can reveal important differences between rules and render some\nof them inadmissible; and we uniquely characterize the minimax-regret optimal\nrule that least frequently randomizes. We apply our results to aggregation of\nexperimental estimates for policy adoption, to extrapolation of Local Average\nTreatment Effects, and to policy making in the presence of omitted variable\nbias."}, "http://arxiv.org/abs/2312.17676": {"title": "Robust Inference in Panel Data Models: Some Effects of Heteroskedasticity and Leveraged Data in Small Samples", "link": "http://arxiv.org/abs/2312.17676", "description": "With the violation of the assumption of homoskedasticity, least squares\nestimators of the variance become inefficient and statistical inference\nconducted with invalid standard errors leads to misleading rejection rates.\nDespite a vast cross-sectional literature on the downward bias of robust\nstandard errors, the problem is not extensively covered in the panel data\nframework. We investigate the consequences of the simultaneous presence of\nsmall sample size, heteroskedasticity and data points that exhibit extreme\nvalues in the covariates ('good leverage points') on the statistical inference.\nFocusing on one-way linear panel data models, we examine asymptotic and finite\nsample properties of a battery of heteroskedasticity-consistent estimators\nusing Monte Carlo simulations. We also propose a hybrid estimator of the\nvariance-covariance matrix. Results show that conventional standard errors are\nalways dominated by more conservative estimators of the variance, especially in\nsmall samples. In addition, all types of HC standard errors have excellent\nperformances in terms of size and power tests under homoskedasticity."}, "http://arxiv.org/abs/2401.00249": {"title": "Forecasting CPI inflation under economic policy and geo-political uncertainties", "link": "http://arxiv.org/abs/2401.00249", "description": "Forecasting a key macroeconomic variable, consumer price index (CPI)\ninflation, for BRIC countries using economic policy uncertainty and\ngeopolitical risk is a difficult proposition for policymakers at the central\nbanks. This study proposes a novel filtered ensemble wavelet neural network\n(FEWNet) that can produce reliable long-term forecasts for CPI inflation. The\nproposal applies a maximum overlapping discrete wavelet transform to the CPI\ninflation series to obtain high-frequency and low-frequency signals. All the\nwavelet-transformed series and filtered exogenous variables are fed into\ndownstream autoregressive neural networks to make the final ensemble forecast.\nTheoretically, we show that FEWNet reduces the empirical risk compared to\nsingle, fully connected neural networks. We also demonstrate that the\nrolling-window real-time forecasts obtained from the proposed algorithm are\nsignificantly more accurate than benchmark forecasting methods. Additionally,\nwe use conformal prediction intervals to quantify the uncertainty associated\nwith the forecasts generated by the proposed approach. The excellent\nperformance of FEWNet can be attributed to its capacity to effectively capture\nnon-linearities and long-range dependencies in the data through its adaptable\narchitecture."}, "http://arxiv.org/abs/2401.00264": {"title": "Identification of Dynamic Nonlinear Panel Models under Partial Stationarity", "link": "http://arxiv.org/abs/2401.00264", "description": "This paper studies identification for a wide range of nonlinear panel data\nmodels, including binary choice, ordered repsonse, and other types of limited\ndependent variable models. Our approach accommodates dynamic models with any\nnumber of lagged dependent variables as well as other types of (potentially\ncontemporary) endogeneity. Our identification strategy relies on a partial\nstationarity condition, which not only allows for an unknown distribution of\nerrors but also for temporal dependencies in errors. We derive partial\nidentification results under flexible model specifications and provide\nadditional support conditions for point identification. We demonstrate the\nrobust finite-sample performance of our approach using Monte Carlo simulations,\nwith static and dynamic ordered choice models as illustrative examples."}, "http://arxiv.org/abs/2401.00618": {"title": "Generalized Difference-in-Differences for Ordered Choice Models: Too Many \"False Zeros\"?", "link": "http://arxiv.org/abs/2401.00618", "description": "In this paper, we develop a generalized Difference-in-Differences model for\ndiscrete, ordered outcomes, building upon elements from a continuous\nChanges-in-Changes model. We focus on outcomes derived from self-reported\nsurvey data eliciting socially undesirable, illegal, or stigmatized behaviors\nlike tax evasion, substance abuse, or domestic violence, where too many \"false\nzeros\", or more broadly, underreporting are likely. We provide\ncharacterizations for distributional parallel trends, a concept central to our\napproach, within a general threshold-crossing model framework. In cases where\noutcomes are assumed to be reported correctly, we propose a framework for\nidentifying and estimating treatment effects across the entire distribution.\nThis framework is then extended to modeling underreported outcomes, allowing\nthe reporting decision to depend on treatment status. A simulation study\ndocuments the finite sample performance of the estimators. Applying our\nmethodology, we investigate the impact of recreational marijuana legalization\nfor adults in several U.S. states on the short-term consumption behavior of\n8th-grade high-school students. The results indicate small, but significant\nincreases in consumption probabilities at each level. These effects are further\namplified upon accounting for misreporting."}, "http://arxiv.org/abs/2010.08868": {"title": "A Decomposition Approach to Counterfactual Analysis in Game-Theoretic Models", "link": "http://arxiv.org/abs/2010.08868", "description": "Decomposition methods are often used for producing counterfactual predictions\nin non-strategic settings. When the outcome of interest arises from a\ngame-theoretic setting where agents are better off by deviating from their\nstrategies after a new policy, such predictions, despite their practical\nsimplicity, are hard to justify. We present conditions in Bayesian games under\nwhich the decomposition-based predictions coincide with the equilibrium-based\nones. In many games, such coincidence follows from an invariance condition for\nequilibrium selection rules. To illustrate our message, we revisit an empirical\nanalysis in Ciliberto and Tamer (2009) on firms' entry decisions in the airline\nindustry."}, "http://arxiv.org/abs/2308.10138": {"title": "On the Inconsistency of Cluster-Robust Inference and How Subsampling Can Fix It", "link": "http://arxiv.org/abs/2308.10138", "description": "Conventional methods of cluster-robust inference are inconsistent in the\npresence of unignorably large clusters. We formalize this claim by establishing\na necessary and sufficient condition for the consistency of the conventional\nmethods. We find that this condition for the consistency is rejected for a\nmajority of empirical research papers. In this light, we propose a novel score\nsubsampling method which is robust even under the condition that fails the\nconventional method. Simulation studies support these claims. With real data\nused by an empirical paper, we showcase that the conventional methods conclude\nsignificance while our proposed method concludes insignificance."}, "http://arxiv.org/abs/2401.01064": {"title": "Robust Inference for Multiple Predictive Regressions with an Application on Bond Risk Premia", "link": "http://arxiv.org/abs/2401.01064", "description": "We propose a robust hypothesis testing procedure for the predictability of\nmultiple predictors that could be highly persistent. Our method improves the\npopular extended instrumental variable (IVX) testing (Phillips and Lee, 2013;\nKostakis et al., 2015) in that, besides addressing the two bias effects found\nin Hosseinkouchack and Demetrescu (2021), we find and deal with the\nvariance-enlargement effect. We show that two types of higher-order terms\ninduce these distortion effects in the test statistic, leading to significant\nover-rejection for one-sided tests and tests in multiple predictive\nregressions. Our improved IVX-based test includes three steps to tackle all the\nissues above regarding finite sample bias and variance terms. Thus, the test\nstatistics perform well in size control, while its power performance is\ncomparable with the original IVX. Monte Carlo simulations and an empirical\nstudy on the predictability of bond risk premia are provided to demonstrate the\neffectiveness of the newly proposed approach."}, "http://arxiv.org/abs/2401.01565": {"title": "Classification and Treatment Learning with Constraints via Composite Heaviside Optimization: a Progressive MIP Method", "link": "http://arxiv.org/abs/2401.01565", "description": "This paper proposes a Heaviside composite optimization approach and presents\na progressive (mixed) integer programming (PIP) method for solving multi-class\nclassification and multi-action treatment problems with constraints. A\nHeaviside composite function is a composite of a Heaviside function (i.e., the\nindicator function of either the open $( \\, 0,\\infty )$ or closed $[ \\,\n0,\\infty \\, )$ interval) with a possibly nondifferentiable function.\nModeling-wise, we show how Heaviside composite optimization provides a unified\nformulation for learning the optimal multi-class classification and\nmulti-action treatment rules, subject to rule-dependent constraints stipulating\na variety of domain restrictions. A Heaviside composite function has an\nequivalent discrete formulation %in terms of integer variables, and the\nresulting optimization problem can in principle be solved by integer\nprogramming (IP) methods. Nevertheless, for constrained learning problems with\nlarge data sets, %of modest or large sizes, a straightforward application of\noff-the-shelf IP solvers is usually ineffective in achieving global optimality.\nTo alleviate such a computational burden, our major contribution is the\nproposal of the PIP method by leveraging the effectiveness of state-of-the-art\nIP solvers for problems of modest sizes. We provide the theoretical advantage\nof the PIP method with the connection to continuous optimization and show that\nthe computed solution is locally optimal for a broad class of Heaviside\ncomposite optimization problems. The numerical performance of the PIP method is\ndemonstrated by extensive computational experimentation."}, "http://arxiv.org/abs/2401.01645": {"title": "Model Averaging and Double Machine Learning", "link": "http://arxiv.org/abs/2401.01645", "description": "This paper discusses pairing double/debiased machine learning (DDML) with\nstacking, a model averaging method for combining multiple candidate learners,\nto estimate structural parameters. We introduce two new stacking approaches for\nDDML: short-stacking exploits the cross-fitting step of DDML to substantially\nreduce the computational burden and pooled stacking enforces common stacking\nweights over cross-fitting folds. Using calibrated simulation studies and two\napplications estimating gender gaps in citations and wages, we show that DDML\nwith stacking is more robust to partially unknown functional forms than common\nalternative approaches based on single pre-selected learners. We provide Stata\nand R software implementing our proposals."}, "http://arxiv.org/abs/2401.01804": {"title": "Efficient Computation of Confidence Sets Using Classification on Equidistributed Grids", "link": "http://arxiv.org/abs/2401.01804", "description": "Economic models produce moment inequalities, which can be used to form tests\nof the true parameters. Confidence sets (CS) of the true parameters are derived\nby inverting these tests. However, they often lack analytical expressions,\nnecessitating a grid search to obtain the CS numerically by retaining the grid\npoints that pass the test. When the statistic is not asymptotically pivotal,\nconstructing the critical value for each grid point in the parameter space adds\nto the computational burden. In this paper, we convert the computational issue\ninto a classification problem by using a support vector machine (SVM)\nclassifier. Its decision function provides a faster and more systematic way of\ndividing the parameter space into two regions: inside vs. outside of the\nconfidence set. We label those points in the CS as 1 and those outside as -1.\nResearchers can train the SVM classifier on a grid of manageable size and use\nit to determine whether points on denser grids are in the CS or not. We\nestablish certain conditions for the grid so that there is a tuning that allows\nus to asymptotically reproduce the test in the CS. This means that in the\nlimit, a point is classified as belonging to the confidence set if and only if\nit is labeled as 1 by the SVM."}, "http://arxiv.org/abs/2011.03073": {"title": "Bias correction for quantile regression estimators", "link": "http://arxiv.org/abs/2011.03073", "description": "We study the bias of classical quantile regression and instrumental variable\nquantile regression estimators. While being asymptotically first-order\nunbiased, these estimators can have non-negligible second-order biases. We\nderive a higher-order stochastic expansion of these estimators using empirical\nprocess theory. Based on this expansion, we derive an explicit formula for the\nsecond-order bias and propose a feasible bias correction procedure that uses\nfinite-difference estimators of the bias components. The proposed bias\ncorrection method performs well in simulations. We provide an empirical\nillustration using Engel's classical data on household expenditure."}, "http://arxiv.org/abs/2107.02637": {"title": "Difference-in-Differences with a Continuous Treatment", "link": "http://arxiv.org/abs/2107.02637", "description": "This paper analyzes difference-in-differences setups with a continuous\ntreatment. We show that treatment effect on the treated-type parameters can be\nidentified under a generalized parallel trends assumption that is similar to\nthe binary treatment setup. However, interpreting differences in these\nparameters across different values of the treatment can be particularly\nchallenging due to treatment effect heterogeneity. We discuss alternative,\ntypically stronger, assumptions that alleviate these challenges. We also\nprovide a variety of treatment effect decomposition results, highlighting that\nparameters associated with popular linear two-way fixed-effect (TWFE)\nspecifications can be hard to interpret, \\emph{even} when there are only two\ntime periods. We introduce alternative estimation procedures that do not suffer\nfrom these TWFE drawbacks, and show in an application that they can lead to\ndifferent conclusions."}, "http://arxiv.org/abs/2401.02428": {"title": "Cu\\'anto es demasiada inflaci\\'on? Una clasificaci\\'on de reg\\'imenes inflacionarios", "link": "http://arxiv.org/abs/2401.02428", "description": "The classifications of inflationary regimes proposed in the literature have\nmostly been based on arbitrary characterizations, subject to value judgments by\nresearchers. The objective of this study is to propose a new methodological\napproach that reduces subjectivity and improves accuracy in the construction of\nsuch regimes. The method is built upon a combination of clustering techniques\nand classification trees, which allows for an historical periodization of\nArgentina's inflationary history for the period 1943-2022. Additionally, two\nprocedures are introduced to smooth out the classification over time: a measure\nof temporal contiguity of observations and a rolling method based on the simple\nmajority rule. The obtained regimes are compared against the existing\nliterature on the inflation-relative price variability relationship, revealing\na better performance of the proposed regimes."}, "http://arxiv.org/abs/2401.02819": {"title": "Roughness Signature Functions", "link": "http://arxiv.org/abs/2401.02819", "description": "Inspired by the activity signature introduced by Todorov and Tauchen (2010),\nwhich was used to measure the activity of a semimartingale, this paper\nintroduces the roughness signature function. The paper illustrates how it can\nbe used to determine whether a discretely observed process is generated by a\ncontinuous process that is rougher than a Brownian motion, a pure-jump process,\nor a combination of the two. Further, if a continuous rough process is present,\nthe function gives an estimate of the roughness index. This is done through an\nextensive simulation study, where we find that the roughness signature function\nworks as expected on rough processes. We further derive some asymptotic\nproperties of this new signature function. The function is applied empirically\nto three different volatility measures for the S&amp;P500 index. The three measures\nare realized volatility, the VIX, and the option-extracted volatility estimator\nof Todorov (2019). The realized volatility and option-extracted volatility show\nsigns of roughness, with the option-extracted volatility appearing smoother\nthan the realized volatility, while the VIX appears to be driven by a\ncontinuous martingale with jumps."}, "http://arxiv.org/abs/2401.03293": {"title": "Counterfactuals in factor models", "link": "http://arxiv.org/abs/2401.03293", "description": "We study a new model where the potential outcomes, corresponding to the\nvalues of a (possibly continuous) treatment, are linked through common factors.\nThe factors can be estimated using a panel of regressors. We propose a\nprocedure to estimate time-specific and unit-specific average marginal effects\nin this context. Our approach can be used either with high-dimensional time\nseries or with large panels. It allows for treatment effects heterogenous\nacross time and units and is straightforward to implement since it only relies\non principal components analysis and elementary computations. We derive the\nasymptotic distribution of our estimator of the average marginal effect and\nhighlight its solid finite sample performance through a simulation exercise.\nThe approach can also be used to estimate average counterfactuals or adapted to\nan instrumental variables setting and we discuss these extensions. Finally, we\nillustrate our novel methodology through an empirical application on income\ninequality."}, "http://arxiv.org/abs/2401.03756": {"title": "Contextual Fixed-Budget Best Arm Identification: Adaptive Experimental Design with Policy Learning", "link": "http://arxiv.org/abs/2401.03756", "description": "Individualized treatment recommendation is a crucial task in evidence-based\ndecision-making. In this study, we formulate this task as a fixed-budget best\narm identification (BAI) problem with contextual information. In this setting,\nwe consider an adaptive experiment given multiple treatment arms. At each\nround, a decision-maker observes a context (covariate) that characterizes an\nexperimental unit and assigns the unit to one of the treatment arms. At the end\nof the experiment, the decision-maker recommends a treatment arm estimated to\nyield the highest expected outcome conditioned on a context (best treatment\narm). The effectiveness of this decision is measured in terms of the worst-case\nexpected simple regret (policy regret), which represents the largest difference\nbetween the conditional expected outcomes of the best and recommended treatment\narms given a context. Our initial step is to derive asymptotic lower bounds for\nthe worst-case expected simple regret, which also implies ideal treatment\nassignment rules. Following the lower bounds, we propose the Adaptive Sampling\n(AS)-Policy Learning recommendation (PL) strategy. Under this strategy, we\nrandomly assign a treatment arm with a ratio of a target assignment ratio at\neach round. At the end of the experiment, we train a policy, a function that\nrecommends a treatment arm given a context, by maximizing the counterfactual\nempirical policy value. Our results show that the AS-PL strategy is\nasymptotically minimax optimal, with its leading factor of expected simple\nregret converging with our established worst-case lower bound. This research\nhas broad implications in various domains, and in light of existing literature,\nour method can be perceived as an adaptive experimental design tailored for\npolicy learning, on-policy learning, or adaptive welfare maximization."}, "http://arxiv.org/abs/2401.03990": {"title": "Identification with possibly invalid IVs", "link": "http://arxiv.org/abs/2401.03990", "description": "This paper proposes a novel identification strategy relying on\nquasi-instrumental variables (quasi-IVs). A quasi-IV is a relevant but possibly\ninvalid IV because it is not completely exogenous and/or excluded. We show that\na variety of models with discrete or continuous endogenous treatment, which are\nusually identified with an IV - quantile models with rank invariance additive\nmodels with homogenous treatment effects, and local average treatment effect\nmodels - can be identified under the joint relevance of two complementary\nquasi-IVs instead. To achieve identification we complement one excluded but\npossibly endogenous quasi-IV (e.g., ``relevant proxies'' such as previous\ntreatment choice) with one exogenous (conditional on the excluded quasi-IV) but\npossibly included quasi-IV (e.g., random assignment or exogenous market\nshocks). In practice, our identification strategy should be attractive since\ncomplementary quasi-IVs should be easier to find than standard IVs. Our\napproach also holds if any of the two quasi-IVs turns out to be a valid IV."}, "http://arxiv.org/abs/2401.04050": {"title": "Robust Estimation in Network Vector Autoregression with Nonstationary Regressors", "link": "http://arxiv.org/abs/2401.04050", "description": "This article studies identification and estimation for the network vector\nautoregressive model with nonstationary regressors. In particular, network\ndependence is characterized by a nonstochastic adjacency matrix. The\ninformation set includes a stationary regressand and a node-specific vector of\nnonstationary regressors, both observed at the same equally spaced time\nfrequencies. Our proposed econometric specification correponds to the NVAR\nmodel under time series nonstationarity which relies on the local-to-unity\nparametrization for capturing the unknown form of persistence of these\nnode-specific regressors. Robust econometric estimation is achieved using an\nIVX-type estimator and the asymptotic theory analysis for the augmented vector\nof regressors is studied based on a double asymptotic regime where both the\nnetwork size and the time dimension tend to infinity."}, "http://arxiv.org/abs/2103.12374": {"title": "What Do We Get from Two-Way Fixed Effects Regressions? Implications from Numerical Equivalence", "link": "http://arxiv.org/abs/2103.12374", "description": "In any multiperiod panel, a two-way fixed effects (TWFE) regression is\nnumerically equivalent to a first-difference (FD) regression that pools all\npossible between-period gaps. Building on this observation, this paper develops\nnumerical and causal interpretations of the TWFE coefficient. At the sample\nlevel, the TWFE coefficient is a weighted average of FD coefficients with\ndifferent between-period gaps. This decomposition is useful for assessing the\nsource of identifying variation for the TWFE coefficient. At the population\nlevel, a causal interpretation of the TWFE coefficient requires a common trends\nassumption for any between-period gap, and the assumption has to be conditional\non changes in time-varying covariates. I propose a natural generalization of\nthe TWFE estimator that can relax these requirements."}, "http://arxiv.org/abs/2107.11869": {"title": "Adaptive Estimation and Uniform Confidence Bands for Nonparametric Structural Functions and Elasticities", "link": "http://arxiv.org/abs/2107.11869", "description": "We introduce two data-driven procedures for optimal estimation and inference\nin nonparametric models using instrumental variables. The first is a\ndata-driven choice of sieve dimension for a popular class of sieve two-stage\nleast squares estimators. When implemented with this choice, estimators of both\nthe structural function $h_0$ and its derivatives (such as elasticities)\nconverge at the fastest possible (i.e., minimax) rates in sup-norm. The second\nis for constructing uniform confidence bands (UCBs) for $h_0$ and its\nderivatives. Our UCBs guarantee coverage over a generic class of\ndata-generating processes and contract at the minimax rate, possibly up to a\nlogarithmic factor. As such, our UCBs are asymptotically more efficient than\nUCBs based on the usual approach of undersmoothing. As an application, we\nestimate the elasticity of the intensive margin of firm exports in a\nmonopolistic competition model of international trade. Simulations illustrate\nthe good performance of our procedures in empirically calibrated designs. Our\nresults provide evidence against common parameterizations of the distribution\nof unobserved firm heterogeneity."}, "http://arxiv.org/abs/2301.09397": {"title": "ddml: Double/debiased machine learning in Stata", "link": "http://arxiv.org/abs/2301.09397", "description": "We introduce the package ddml for Double/Debiased Machine Learning (DDML) in\nStata. Estimators of causal parameters for five different econometric models\nare supported, allowing for flexible estimation of causal effects of endogenous\nvariables in settings with unknown functional forms and/or many exogenous\nvariables. ddml is compatible with many existing supervised machine learning\nprograms in Stata. We recommend using DDML in combination with stacking\nestimation which combines multiple machine learners into a final predictor. We\nprovide Monte Carlo evidence to support our recommendation."}, "http://arxiv.org/abs/2302.13857": {"title": "Multi-cell experiments for marginal treatment effect estimation of digital ads", "link": "http://arxiv.org/abs/2302.13857", "description": "Randomized experiments with treatment and control groups are an important\ntool to measure the impacts of interventions. However, in experimental settings\nwith one-sided noncompliance, extant empirical approaches may not produce the\nestimands a decision-maker needs to solve their problem of interest. For\nexample, these experimental designs are common in digital advertising settings,\nbut typical methods do not yield effects that inform the intensive margin --\nhow many consumers should be reached or how much should be spent on a campaign.\nWe propose a solution that combines a novel multi-cell experimental design with\nmodern estimation techniques that enables decision-makers to recover enough\ninformation to solve problems with an intensive margin. Our design is\nstraightforward to implement and does not require any additional budget to be\ncarried out. We illustrate our approach through a series of simulations that\nare calibrated using an advertising experiment at Facebook, finding that our\nmethod outperforms standard techniques in generating better decisions."}, "http://arxiv.org/abs/2401.04200": {"title": "Teacher bias or measurement error?", "link": "http://arxiv.org/abs/2401.04200", "description": "In many countries, teachers' track recommendations are used to allocate\nstudents to secondary school tracks. Previous studies have shown that students\nfrom families with low socioeconomic status (SES) receive lower track\nrecommendations than their peers from high SES families, conditional on\nstandardized test scores. It is often argued this indicates teacher bias.\nHowever, this claim is invalid in the presence of measurement error in test\nscores. We discuss how measurement error in test scores generates a biased\ncoefficient of the conditional SES gap, and consider three empirical strategies\nto address this bias. Using administrative data from the Netherlands, we find\nthat measurement error explains 35 to 43% of the conditional SES gap in track\nrecommendations."}, "http://arxiv.org/abs/2401.04512": {"title": "Robust Bayesian Method for Refutable Models", "link": "http://arxiv.org/abs/2401.04512", "description": "We propose a robust Bayesian method for economic models that can be rejected\nunder some data distributions. The econometrician starts with a structural\nassumption which can be written as the intersection of several assumptions, and\nthe joint assumption is refutable. To avoid the model rejection, the\neconometrician first takes a stance on which assumption $j$ is likely to be\nviolated and considers a measurement of the degree of violation of this\nassumption $j$. She then considers a (marginal) prior belief on the degree of\nviolation $(\\pi_{m_j})$: She considers a class of prior distributions $\\pi_s$\non all economic structures such that all $\\pi_s$ have the same marginal\ndistribution $\\pi_m$. Compared to the standard nonparametric Bayesian method\nthat puts a single prior on all economic structures, the robust Bayesian method\nimposes a single marginal prior distribution on the degree of violation. As a\nresult, the robust Bayesian method allows the econometrician to take a stance\nonly on the likeliness of violation of assumption $j$. Compared to the\nfrequentist approach to relax the refutable assumption, the robust Bayesian\nmethod is transparent on the econometrician's stance of choosing models. We\nalso show that many frequentists' ways to relax the refutable assumption can be\nfound equivalent to particular choices of robust Bayesian prior classes. We use\nthe local average treatment effect (LATE) in the potential outcome framework as\nthe leading illustrating example."}, "http://arxiv.org/abs/2005.10314": {"title": "On the Nuisance of Control Variables in Regression Analysis", "link": "http://arxiv.org/abs/2005.10314", "description": "Control variables are included in regression analyses to estimate the causal\neffect of a treatment on an outcome. In this paper, we argue that the estimated\neffect sizes of controls are unlikely to have a causal interpretation\nthemselves, though. This is because even valid controls are possibly endogenous\nand represent a combination of several different causal mechanisms operating\njointly on the outcome, which is hard to interpret theoretically. Therefore, we\nrecommend refraining from interpreting marginal effects of controls and\nfocusing on the main variables of interest, for which a plausible\nidentification argument can be established. To prevent erroneous managerial or\npolicy implications, coefficients of control variables should be clearly marked\nas not having a causal interpretation or omitted from regression tables\naltogether. Moreover, we advise against using control variable estimates for\nsubsequent theory building and meta-analyses."}, "http://arxiv.org/abs/2401.04803": {"title": "IV Estimation of Panel Data Tobit Models with Normal Errors", "link": "http://arxiv.org/abs/2401.04803", "description": "Amemiya (1973) proposed a ``consistent initial estimator'' for the parameters\nin a censored regression model with normal errors. This paper demonstrates that\na similar approach can be used to construct moment conditions for\nfixed--effects versions of the model considered by Amemiya. This result\nsuggests estimators for models that have not previously been considered."}, "http://arxiv.org/abs/2401.04849": {"title": "A Deep Learning Representation of Spatial Interaction Model for Resilient Spatial Planning of Community Business Clusters", "link": "http://arxiv.org/abs/2401.04849", "description": "Existing Spatial Interaction Models (SIMs) are limited in capturing the\ncomplex and context-aware interactions between business clusters and trade\nareas. To address the limitation, we propose a SIM-GAT model to predict\nspatiotemporal visitation flows between community business clusters and their\ntrade areas. The model innovatively represents the integrated system of\nbusiness clusters, trade areas, and transportation infrastructure within an\nurban region using a connected graph. Then, a graph-based deep learning model,\ni.e., Graph AttenTion network (GAT), is used to capture the complexity and\ninterdependencies of business clusters. We developed this model with data\ncollected from the Miami metropolitan area in Florida. We then demonstrated its\neffectiveness in capturing varying attractiveness of business clusters to\ndifferent residential neighborhoods and across scenarios with an eXplainable AI\napproach. We contribute a novel method supplementing conventional SIMs to\npredict and analyze the dynamics of inter-connected community business\nclusters. The analysis results can inform data-evidenced and place-specific\nplanning strategies helping community business clusters better accommodate\ntheir customers across scenarios, and hence improve the resilience of community\nbusinesses."}, "http://arxiv.org/abs/2306.14653": {"title": "Optimization of the Generalized Covariance Estimator in Noncausal Processes", "link": "http://arxiv.org/abs/2306.14653", "description": "This paper investigates the performance of the Generalized Covariance\nestimator (GCov) in estimating and identifying mixed causal and noncausal\nmodels. The GCov estimator is a semi-parametric method that minimizes an\nobjective function without making any assumptions about the error distribution\nand is based on nonlinear autocovariances to identify the causal and noncausal\norders. When the number and type of nonlinear autocovariances included in the\nobjective function of a GCov estimator is insufficient/inadequate, or the error\ndensity is too close to the Gaussian, identification issues can arise. These\nissues result in local minima in the objective function, which correspond to\nparameter values associated with incorrect causal and noncausal orders. Then,\ndepending on the starting point and the optimization algorithm employed, the\nalgorithm can converge to a local minimum. The paper proposes the use of the\nSimulated Annealing (SA) optimization algorithm as an alternative to\nconventional numerical optimization methods. The results demonstrate that SA\nperforms well when applied to mixed causal and noncausal models, successfully\neliminating the effects of local minima. The proposed approach is illustrated\nby an empirical application involving a bivariate commodity price series."}, "http://arxiv.org/abs/2401.05517": {"title": "On Efficient Inference of Causal Effects with Multiple Mediators", "link": "http://arxiv.org/abs/2401.05517", "description": "This paper provides robust estimators and efficient inference of causal\neffects involving multiple interacting mediators. Most existing works either\nimpose a linear model assumption among the mediators or are restricted to\nhandle conditionally independent mediators given the exposure. To overcome\nthese limitations, we define causal and individual mediation effects in a\ngeneral setting, and employ a semiparametric framework to develop quadruply\nrobust estimators for these causal effects. We further establish the asymptotic\nnormality of the proposed estimators and prove their local semiparametric\nefficiencies. The proposed method is empirically validated via simulated and\nreal datasets concerning psychiatric disorders in trauma survivors."}, "http://arxiv.org/abs/2401.05784": {"title": "Covariance Function Estimation for High-Dimensional Functional Time Series with Dual Factor Structures", "link": "http://arxiv.org/abs/2401.05784", "description": "We propose a flexible dual functional factor model for modelling\nhigh-dimensional functional time series. In this model, a high-dimensional\nfully functional factor parametrisation is imposed on the observed functional\nprocesses, whereas a low-dimensional version (via series approximation) is\nassumed for the latent functional factors. We extend the classic principal\ncomponent analysis technique for the estimation of a low-rank structure to the\nestimation of a large covariance matrix of random functions that satisfies a\nnotion of (approximate) functional \"low-rank plus sparse\" structure; and\ngeneralise the matrix shrinkage method to functional shrinkage in order to\nestimate the sparse structure of functional idiosyncratic components. Under\nappropriate regularity conditions, we derive the large sample theory of the\ndeveloped estimators, including the consistency of the estimated factors and\nfunctional factor loadings and the convergence rates of the estimated matrices\nof covariance functions measured by various (functional) matrix norms.\nConsistent selection of the number of factors and a data-driven rule to choose\nthe shrinkage parameter are discussed. Simulation and empirical studies are\nprovided to demonstrate the finite-sample performance of the developed model\nand estimation methodology."}, "http://arxiv.org/abs/2305.16377": {"title": "Validating a dynamic input-output model for the propagation of supply and demand shocks during the COVID-19 pandemic in Belgium", "link": "http://arxiv.org/abs/2305.16377", "description": "This work validates a dynamic production network model, used to quantify the\nimpact of economic shocks caused by COVID-19 in the UK, using data for Belgium.\nBecause the model was published early during the 2020 COVID-19 pandemic, it\nrelied on several assumptions regarding the magnitude of the observed economic\nshocks, for which more accurate data have become available in the meantime. We\nrefined the propagated shocks to align with observed data collected during the\npandemic and calibrated some less well-informed parameters using 115 economic\ntime series. The refined model effectively captures the evolution of GDP,\nrevenue, and employment during the COVID-19 pandemic in Belgium at both\nindividual economic activity and aggregate levels. However, the reduction in\nbusiness-to-business demand is overestimated, revealing structural shortcomings\nin accounting for businesses' motivations to sustain trade despite the\npandemic's induced shocks. We confirm that the relaxation of the stringent\nLeontief production function by a survey on the criticality of inputs\nsignificantly improved the model's accuracy. However, despite a large dataset,\ndistinguishing between varying degrees of relaxation proved challenging.\nOverall, this work demonstrates the model's validity in assessing the impact of\neconomic shocks caused by an epidemic in Belgium."}, "http://arxiv.org/abs/2309.13251": {"title": "Nonparametric estimation of conditional densities by generalized random forests", "link": "http://arxiv.org/abs/2309.13251", "description": "Considering a continuous random variable Y together with a continuous random\nvector X, I propose a nonparametric estimator f^(.|x) for the conditional\ndensity of Y given X=x. This estimator takes the form of an exponential series\nwhose coefficients T = (T1,...,TJ) are the solution of a system of nonlinear\nequations that depends on an estimator of the conditional expectation\nE[p(Y)|X=x], where p(.) is a J-dimensional vector of basis functions. A key\nfeature is that E[p(Y)|X=x] is estimated by generalized random forest (Athey,\nTibshirani, and Wager, 2019), targeting the heterogeneity of T across x. I show\nthat f^(.|x) is uniformly consistent and asymptotically normal, while allowing\nJ to grow to infinity. I also provide a standard error formula to construct\nasymptotically valid confidence intervals. Results from Monte Carlo experiments\nand an empirical illustration are provided."}, "http://arxiv.org/abs/2401.06264": {"title": "Exposure effects are policy relevant only under strong assumptions about the interference structure", "link": "http://arxiv.org/abs/2401.06264", "description": "Savje (2023) recommends misspecified exposure effects as a way to avoid\nstrong assumptions about interference when analyzing the results of an\nexperiment. In this discussion, we highlight a key limitation of Savje's\nrecommendation. Exposure effects are not generally useful for evaluating social\npolicies without the strong assumptions that Savje seeks to avoid.\n\nOur discussion is organized as follows. Section 2 summarizes our position,\nsection 3 provides a concrete example, and section 4 concludes. Proof of claims\nare in an appendix."}, "http://arxiv.org/abs/2401.06611": {"title": "Robust Analysis of Short Panels", "link": "http://arxiv.org/abs/2401.06611", "description": "Many structural econometric models include latent variables on whose\nprobability distributions one may wish to place minimal restrictions. Leading\nexamples in panel data models are individual-specific variables sometimes\ntreated as \"fixed effects\" and, in dynamic models, initial conditions. This\npaper presents a generally applicable method for characterizing sharp\nidentified sets when models place no restrictions on the probability\ndistribution of certain latent variables and no restrictions on their\ncovariation with other variables. In our analysis latent variables on which\nrestrictions are undesirable are removed, leading to econometric analysis\nrobust to misspecification of restrictions on their distributions which are\ncommonplace in the applied panel data literature. Endogenous explanatory\nvariables are easily accommodated. Examples of application to some static and\ndynamic binary, ordered and multiple discrete choice and censored panel data\nmodels are presented."}, "http://arxiv.org/abs/2308.00202": {"title": "Randomization Inference of Heterogeneous Treatment Effects under Network Interference", "link": "http://arxiv.org/abs/2308.00202", "description": "We design randomization tests of heterogeneous treatment effects when units\ninteract on a single connected network. Our modeling strategy allows network\ninterference into the potential outcomes framework using the concept of\nexposure mapping. We consider several null hypotheses representing different\nnotions of homogeneous treatment effects. However, these hypotheses are not\nsharp due to nuisance parameters and multiple potential outcomes. To address\nthe issue of multiple potential outcomes, we propose a conditional\nrandomization method that expands on existing procedures. Our conditioning\napproach permits the use of treatment assignment as a conditioning variable,\nwidening the range of application of the randomization method of inference. In\naddition, we propose techniques that overcome the nuisance parameter issue. We\nshow that our resulting testing methods based on the conditioning procedure and\nthe strategies for handling nuisance parameters are asymptotically valid. We\ndemonstrate the testing methods using a network data set and also present the\nfindings of a Monte Carlo study."}, "http://arxiv.org/abs/2401.06864": {"title": "Deep Learning With DAGs", "link": "http://arxiv.org/abs/2401.06864", "description": "Social science theories often postulate causal relationships among a set of\nvariables or events. Although directed acyclic graphs (DAGs) are increasingly\nused to represent these theories, their full potential has not yet been\nrealized in practice. As non-parametric causal models, DAGs require no\nassumptions about the functional form of the hypothesized relationships.\nNevertheless, to simplify the task of empirical evaluation, researchers tend to\ninvoke such assumptions anyway, even though they are typically arbitrary and do\nnot reflect any theoretical content or prior knowledge. Moreover, functional\nform assumptions can engender bias, whenever they fail to accurately capture\nthe complexity of the causal system under investigation. In this article, we\nintroduce causal-graphical normalizing flows (cGNFs), a novel approach to\ncausal inference that leverages deep neural networks to empirically evaluate\ntheories represented as DAGs. Unlike conventional approaches, cGNFs model the\nfull joint distribution of the data according to a DAG supplied by the analyst,\nwithout relying on stringent assumptions about functional form. In this way,\nthe method allows for flexible, semi-parametric estimation of any causal\nestimand that can be identified from the DAG, including total effects,\nconditional effects, direct and indirect effects, and path-specific effects. We\nillustrate the method with a reanalysis of Blau and Duncan's (1967) model of\nstatus attainment and Zhou's (2019) model of conditional versus controlled\nmobility. To facilitate adoption, we provide open-source software together with\na series of online tutorials for implementing cGNFs. The article concludes with\na discussion of current limitations and directions for future development."}, "http://arxiv.org/abs/2401.07038": {"title": "A simple stochastic nonlinear AR model with application to bubble", "link": "http://arxiv.org/abs/2401.07038", "description": "Economic and financial time series can feature locally explosive behavior\nwhen a bubble is formed. The economic or financial bubble, especially its\ndynamics, is an intriguing topic that has been attracting longstanding\nattention. To illustrate the dynamics of the local explosion itself, the paper\npresents a novel, simple, yet useful time series model, called the stochastic\nnonlinear autoregressive model, which is always strictly stationary and\ngeometrically ergodic and can create long swings or persistence observed in\nmany macroeconomic variables. When a nonlinear autoregressive coefficient is\noutside of a certain range, the model has periodically explosive behaviors and\ncan then be used to portray the bubble dynamics. Further, the quasi-maximum\nlikelihood estimation (QMLE) of our model is considered, and its strong\nconsistency and asymptotic normality are established under minimal assumptions\non innovation. A new model diagnostic checking statistic is developed for model\nfitting adequacy. In addition two methods for bubble tagging are proposed, one\nfrom the residual perspective and the other from the null-state perspective.\nMonte Carlo simulation studies are conducted to assess the performances of the\nQMLE and the two bubble tagging methods in finite samples. Finally, the\nusefulness of the model is illustrated by an empirical application to the\nmonthly Hang Seng Index."}, "http://arxiv.org/abs/2401.07152": {"title": "Inference for Synthetic Controls via Refined Placebo Tests", "link": "http://arxiv.org/abs/2401.07152", "description": "The synthetic control method is often applied to problems with one treated\nunit and a small number of control units. A common inferential task in this\nsetting is to test null hypotheses regarding the average treatment effect on\nthe treated. Inference procedures that are justified asymptotically are often\nunsatisfactory due to (1) small sample sizes that render large-sample\napproximation fragile and (2) simplification of the estimation procedure that\nis implemented in practice. An alternative is permutation inference, which is\nrelated to a common diagnostic called the placebo test. It has provable Type-I\nerror guarantees in finite samples without simplification of the method, when\nthe treatment is uniformly assigned. Despite this robustness, the placebo test\nsuffers from low resolution since the null distribution is constructed from\nonly $N$ reference estimates, where $N$ is the sample size. This creates a\nbarrier for statistical inference at a common level like $\\alpha = 0.05$,\nespecially when $N$ is small. We propose a novel leave-two-out procedure that\nbypasses this issue, while still maintaining the same finite-sample Type-I\nerror guarantee under uniform assignment for a wide range of $N$. Unlike the\nplacebo test whose Type-I error always equals the theoretical upper bound, our\nprocedure often achieves a lower unconditional Type-I error than theory\nsuggests; this enables useful inference in the challenging regime when $\\alpha\n&lt; 1/N$. Empirically, our procedure achieves a higher power when the effect size\nis reasonably large and a comparable power otherwise. We generalize our\nprocedure to non-uniform assignments and show how to conduct sensitivity\nanalysis. From a methodological perspective, our procedure can be viewed as a\nnew type of randomization inference different from permutation or rank-based\ninference, which is particularly effective in small samples."}, "http://arxiv.org/abs/2401.07176": {"title": "A Note on Uncertainty Quantification for Maximum Likelihood Parameters Estimated with Heuristic Based Optimization Algorithms", "link": "http://arxiv.org/abs/2401.07176", "description": "Gradient-based solvers risk convergence to local optima, leading to incorrect\nresearcher inference. Heuristic-based algorithms are able to ``break free\" of\nthese local optima to eventually converge to the true global optimum. However,\ngiven that they do not provide the gradient/Hessian needed to approximate the\ncovariance matrix and that the significantly longer computational time they\nrequire for convergence likely precludes resampling procedures for inference,\nresearchers often are unable to quantify uncertainty in the estimates they\nderive with these methods. This note presents a simple and relatively fast\ntwo-step procedure to estimate the covariance matrix for parameters estimated\nwith these algorithms. This procedure relies on automatic differentiation, a\ncomputational means of calculating derivatives that is popular in machine\nlearning applications. A brief empirical example demonstrates the advantages of\nthis procedure relative to bootstrapping and shows the similarity in standard\nerror estimates between this procedure and that which would normally accompany\nmaximum likelihood estimation with a gradient-based algorithm."}, "http://arxiv.org/abs/2401.08290": {"title": "Causal Machine Learning for Moderation Effects", "link": "http://arxiv.org/abs/2401.08290", "description": "It is valuable for any decision maker to know the impact of decisions\n(treatments) on average and for subgroups. The causal machine learning\nliterature has recently provided tools for estimating group average treatment\neffects (GATE) to understand treatment heterogeneity better. This paper\naddresses the challenge of interpreting such differences in treatment effects\nbetween groups while accounting for variations in other covariates. We propose\na new parameter, the balanced group average treatment effect (BGATE), which\nmeasures a GATE with a specific distribution of a priori-determined covariates.\nBy taking the difference of two BGATEs, we can analyse heterogeneity more\nmeaningfully than by comparing two GATEs. The estimation strategy for this\nparameter is based on double/debiased machine learning for discrete treatments\nin an unconfoundedness setting, and the estimator is shown to be\n$\\sqrt{N}$-consistent and asymptotically normal under standard conditions.\nAdding additional identifying assumptions allows specific balanced differences\nin treatment effects between groups to be interpreted causally, leading to the\ncausal balanced group average treatment effect. We explore the finite sample\nproperties in a small-scale simulation study and demonstrate the usefulness of\nthese parameters in an empirical example."}, "http://arxiv.org/abs/2401.08442": {"title": "Assessing the impact of forced and voluntary behavioral changes on economic-epidemiological co-dynamics: A comparative case study between Belgium and Sweden during the 2020 COVID-19 pandemic", "link": "http://arxiv.org/abs/2401.08442", "description": "During the COVID-19 pandemic, governments faced the challenge of managing\npopulation behavior to prevent their healthcare systems from collapsing. Sweden\nadopted a strategy centered on voluntary sanitary recommendations while Belgium\nresorted to mandatory measures. Their consequences on pandemic progression and\nassociated economic impacts remain insufficiently understood. This study\nleverages the divergent policies of Belgium and Sweden during the COVID-19\npandemic to relax the unrealistic -- but persistently used -- assumption that\nsocial contacts are not influenced by an epidemic's dynamics. We develop an\nepidemiological-economic co-simulation model where pandemic-induced behavioral\nchanges are a superposition of voluntary actions driven by fear, prosocial\nbehavior or social pressure, and compulsory compliance with government\ndirectives. Our findings emphasize the importance of early responses, which\nreduce the stringency of measures necessary to safeguard healthcare systems and\nminimize ensuing economic damage. Voluntary behavioral changes lead to a\npattern of recurring epidemics, which should be regarded as the natural\nlong-term course of pandemics. Governments should carefully consider prolonging\nlockdown longer than necessary because this leads to higher economic damage and\na potentially higher second surge when measures are released. Our model can aid\npolicymakers in the selection of an appropriate long-term strategy that\nminimizes economic damage."}, "http://arxiv.org/abs/2101.00009": {"title": "Adversarial Estimation of Riesz Representers", "link": "http://arxiv.org/abs/2101.00009", "description": "Many causal and structural parameters are linear functionals of an underlying\nregression. The Riesz representer is a key component in the asymptotic variance\nof a semiparametrically estimated linear functional. We propose an adversarial\nframework to estimate the Riesz representer using general function spaces. We\nprove a nonasymptotic mean square rate in terms of an abstract quantity called\nthe critical radius, then specialize it for neural networks, random forests,\nand reproducing kernel Hilbert spaces as leading cases. Furthermore, we use\ncritical radius theory -- in place of Donsker theory -- to prove asymptotic\nnormality without sample splitting, uncovering a ``complexity-rate robustness''\ncondition. This condition has practical consequences: inference without sample\nsplitting is possible in several machine learning settings, which may improve\nfinite sample performance compared to sample splitting. Our estimators achieve\nnominal coverage in highly nonlinear simulations where previous methods break\ndown. They shed new light on the heterogeneous effects of matching grants."}, "http://arxiv.org/abs/2101.00399": {"title": "The Law of Large Numbers for Large Stable Matchings", "link": "http://arxiv.org/abs/2101.00399", "description": "In many empirical studies of a large two-sided matching market (such as in a\ncollege admissions problem), the researcher performs statistical inference\nunder the assumption that they observe a random sample from a large matching\nmarket. In this paper, we consider a setting in which the researcher observes\neither all or a nontrivial fraction of outcomes from a stable matching. We\nestablish a concentration inequality for empirical matching probabilities\nassuming strong correlation among the colleges' preferences while allowing\nstudents' preferences to be fully heterogeneous. Our concentration inequality\nyields laws of large numbers for the empirical matching probabilities and other\nstatistics commonly used in empirical analyses of a large matching market. To\nillustrate the usefulness of our concentration inequality, we prove consistency\nfor estimators of conditional matching probabilities and measures of positive\nassortative matching."}, "http://arxiv.org/abs/2203.08050": {"title": "Pairwise Valid Instruments", "link": "http://arxiv.org/abs/2203.08050", "description": "Finding valid instruments is difficult. We propose Validity Set Instrumental\nVariable (VSIV) estimation, a method for estimating local average treatment\neffects (LATEs) in heterogeneous causal effect models when the instruments are\npartially invalid. We consider settings with pairwise valid instruments, that\nis, instruments that are valid for a subset of instrument value pairs. VSIV\nestimation exploits testable implications of instrument validity to remove\ninvalid pairs and provides estimates of the LATEs for all remaining pairs,\nwhich can be aggregated into a single parameter of interest using\nresearcher-specified weights. We show that the proposed VSIV estimators are\nasymptotically normal under weak conditions and remove or reduce the asymptotic\nbias relative to standard LATE estimators (that is, LATE estimators that do not\nuse testable implications to remove invalid variation). We evaluate the finite\nsample properties of VSIV estimation in application-based simulations and apply\nour method to estimate the returns to college education using parental\neducation as an instrument."}, "http://arxiv.org/abs/2212.07052": {"title": "On LASSO for High Dimensional Predictive Regression", "link": "http://arxiv.org/abs/2212.07052", "description": "This paper examines LASSO, a widely-used $L_{1}$-penalized regression method,\nin high dimensional linear predictive regressions, particularly when the number\nof potential predictors exceeds the sample size and numerous unit root\nregressors are present. The consistency of LASSO is contingent upon two key\ncomponents: the deviation bound of the cross product of the regressors and the\nerror term, and the restricted eigenvalue of the Gram matrix. We present new\nprobabilistic bounds for these components, suggesting that LASSO's rates of\nconvergence are different from those typically observed in cross-sectional\ncases. When applied to a mixture of stationary, nonstationary, and cointegrated\npredictors, LASSO maintains its asymptotic guarantee if predictors are\nscale-standardized. Leveraging machine learning and macroeconomic domain\nexpertise, LASSO demonstrates strong performance in forecasting the\nunemployment rate, as evidenced by its application to the FRED-MD database."}, "http://arxiv.org/abs/2303.14226": {"title": "Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions", "link": "http://arxiv.org/abs/2303.14226", "description": "Consider a setting where there are $N$ heterogeneous units and $p$\ninterventions. Our goal is to learn unit-specific potential outcomes for any\ncombination of these $p$ interventions, i.e., $N \\times 2^p$ causal parameters.\nChoosing a combination of interventions is a problem that naturally arises in a\nvariety of applications such as factorial design experiments, recommendation\nengines, combination therapies in medicine, conjoint analysis, etc. Running $N\n\\times 2^p$ experiments to estimate the various parameters is likely expensive\nand/or infeasible as $N$ and $p$ grow. Further, with observational data there\nis likely confounding, i.e., whether or not a unit is seen under a combination\nis correlated with its potential outcome under that combination. To address\nthese challenges, we propose a novel latent factor model that imposes structure\nacross units (i.e., the matrix of potential outcomes is approximately rank\n$r$), and combinations of interventions (i.e., the coefficients in the Fourier\nexpansion of the potential outcomes is approximately $s$ sparse). We establish\nidentification for all $N \\times 2^p$ parameters despite unobserved\nconfounding. We propose an estimation procedure, Synthetic Combinations, and\nestablish it is finite-sample consistent and asymptotically normal under\nprecise conditions on the observation pattern. Our results imply consistent\nestimation given $\\text{poly}(r) \\times \\left( N + s^2p\\right)$ observations,\nwhile previous methods have sample complexity scaling as $\\min(N \\times s^2p, \\\n\\ \\text{poly(r)} \\times (N + 2^p))$. We use Synthetic Combinations to propose a\ndata-efficient experimental design. Empirically, Synthetic Combinations\noutperforms competing approaches on a real-world dataset on movie\nrecommendations. Lastly, we extend our analysis to do causal inference where\nthe intervention is a permutation over $p$ items (e.g., rankings)."}, "http://arxiv.org/abs/2308.15062": {"title": "Forecasting with Feedback", "link": "http://arxiv.org/abs/2308.15062", "description": "Systematically biased forecasts are typically interpreted as evidence of\nforecasters' irrationality and/or asymmetric loss. In this paper we propose an\nalternative explanation: when forecasts inform economic policy decisions, and\nthe resulting actions affect the realization of the forecast target itself,\nforecasts may be optimally biased even under quadratic loss. The result arises\nin environments in which the forecaster is uncertain about the decision maker's\nreaction to the forecast, which is presumably the case in most applications. We\nillustrate the empirical relevance of our theory by reviewing some stylized\nproperties of Green Book inflation forecasts and relating them to the\npredictions from our model. Our results point out that the presence of policy\nfeedback poses a challenge to traditional tests of forecast rationality."}, "http://arxiv.org/abs/2309.05639": {"title": "Forecasted Treatment Effects", "link": "http://arxiv.org/abs/2309.05639", "description": "We consider estimation and inference of the effects of a policy in the\nabsence of a control group. We obtain unbiased estimators of individual\n(heterogeneous) treatment effects and a consistent and asymptotically normal\nestimator of the average treatment effect. Our estimator averages over unbiased\nforecasts of individual counterfactuals, based on a (short) time series of\npre-treatment data. The paper emphasizes the importance of focusing on forecast\nunbiasedness rather than accuracy when the end goal is estimation of average\ntreatment effects. We show that simple basis function regressions ensure\nforecast unbiasedness for a broad class of data-generating processes for the\ncounterfactuals, even in short panels. In contrast, model-based forecasting\nrequires stronger assumptions and is prone to misspecification and estimation\nbias. We show that our method can replicate the findings of some previous\nempirical studies, but without using a control group."}, "http://arxiv.org/abs/1812.10820": {"title": "A $t$-test for synthetic controls", "link": "http://arxiv.org/abs/1812.10820", "description": "We propose a practical and robust method for making inferences on average\ntreatment effects estimated by synthetic controls. We develop a $K$-fold\ncross-fitting procedure for bias correction. To avoid the difficult estimation\nof the long-run variance, inference is based on a self-normalized\n$t$-statistic, which has an asymptotically pivotal $t$-distribution. Our\n$t$-test is easy to implement, provably robust against misspecification, and\nvalid with stationary and non-stationary data. It demonstrates an excellent\nsmall sample performance in application-based simulations and performs well\nrelative to alternative methods. We illustrate the usefulness of the $t$-test\nby revisiting the effect of carbon taxes on emissions."}, "http://arxiv.org/abs/2108.12419": {"title": "Revisiting Event Study Designs: Robust and Efficient Estimation", "link": "http://arxiv.org/abs/2108.12419", "description": "We develop a framework for difference-in-differences designs with staggered\ntreatment adoption and heterogeneous causal effects. We show that conventional\nregression-based estimators fail to provide unbiased estimates of relevant\nestimands absent strong restrictions on treatment-effect homogeneity. We then\nderive the efficient estimator addressing this challenge, which takes an\nintuitive \"imputation\" form when treatment-effect heterogeneity is\nunrestricted. We characterize the asymptotic behavior of the estimator, propose\ntools for inference, and develop tests for identifying assumptions. Our method\napplies with time-varying controls, in triple-difference designs, and with\ncertain non-binary treatments. We show the practical relevance of our results\nin a simulation study and an application. Studying the consumption response to\ntax rebates in the United States, we find that the notional marginal propensity\nto consume is between 8 and 11 percent in the first quarter - about half as\nlarge as benchmark estimates used to calibrate macroeconomic models - and\npredominantly occurs in the first month after the rebate."}, "http://arxiv.org/abs/2301.06720": {"title": "Testing Firm Conduct", "link": "http://arxiv.org/abs/2301.06720", "description": "Evaluating policy in imperfectly competitive markets requires understanding\nfirm behavior. While researchers test conduct via model selection and\nassessment, we present advantages of Rivers and Vuong (2002) (RV) model\nselection under misspecification. However, degeneracy of RV invalidates\ninference. With a novel definition of weak instruments for testing, we connect\ndegeneracy to instrument strength, derive weak instrument properties of RV, and\nprovide a diagnostic for weak instruments by extending the framework of Stock\nand Yogo (2005) to model selection. We test vertical conduct (Villas-Boas,\n2007) using common instrument sets. Some are weak, providing no power. Strong\ninstruments support manufacturers setting retail prices."}, "http://arxiv.org/abs/2307.13686": {"title": "Characteristics and Predictive Modeling of Short-term Impacts of Hurricanes on the US Employment", "link": "http://arxiv.org/abs/2307.13686", "description": "This study examines the short-term employment changes in the US after\nhurricane impacts. An analysis of hurricane events during 1990-2021 suggests\nthat county-level employment changes in the initial month are small on average,\nthough large employment losses (&gt;30%) can occur after extreme cyclones. The\noverall small changes partly result from compensation among opposite changes in\nemployment sectors, such as the construction and leisure and hospitality\nsectors. An analysis of these extreme cases highlights concentrated employment\nlosses in the service-providing industries and delayed, robust employment gains\nrelated to reconstruction activities. The overall employment shock is\nnegatively correlated with the metrics of cyclone hazards (e.g., extreme wind\nand precipitation) and geospatial details of impacts (e.g., cyclone-entity\ndistance). Additionally, non-cyclone factors such as county characteristics\nalso strongly affect short-term employment changes. The findings inform\npredictive modeling of short-term employment changes and help deliver promising\nskills for service-providing industries and high-impact cyclones. Specifically,\nthe Random Forests model, which can account for nonlinear relationships,\ngreatly outperforms the multiple linear regression model commonly used by\neconomics studies. Overall, our findings may help improve post-cyclone aid\nprograms and the modeling of hurricanes socioeconomic impacts in a changing\nclimate."}, "http://arxiv.org/abs/2308.08958": {"title": "Linear Regression with Weak Exogeneity", "link": "http://arxiv.org/abs/2308.08958", "description": "This paper studies linear time series regressions with many regressors. Weak\nexogeneity is the most used identifying assumption in time series. Weak\nexogeneity requires the structural error to have zero conditional expectation\ngiven the present and past regressor values, allowing errors to correlate with\nfuture regressor realizations. We show that weak exogeneity in time series\nregressions with many controls may produce substantial biases and even render\nthe least squares (OLS) estimator inconsistent. The bias arises in settings\nwith many regressors because the normalized OLS design matrix remains\nasymptotically random and correlates with the regression error when only weak\n(but not strict) exogeneity holds. This bias's magnitude increases with the\nnumber of regressors and their average autocorrelation. To address this issue,\nwe propose an innovative approach to bias correction that yields a new\nestimator with improved properties relative to OLS. We establish consistency\nand conditional asymptotic Gaussianity of this new estimator and provide a\nmethod for inference."}, "http://arxiv.org/abs/2401.09874": {"title": "A Quantile Nelson-Siegel model", "link": "http://arxiv.org/abs/2401.09874", "description": "A widespread approach to modelling the interaction between macroeconomic\nvariables and the yield curve relies on three latent factors usually\ninterpreted as the level, slope, and curvature (Diebold et al., 2006). This\napproach is inherently focused on the conditional mean of the yields and\npostulates a dynamic linear model where the latent factors smoothly change over\ntime. However, periods of deep crisis, such as the Great Recession and the\nrecent pandemic, have highlighted the importance of statistical models that\naccount for asymmetric shocks and are able to forecast the tails of a\nvariable's distribution. A new version of the dynamic three-factor model is\nproposed to address this issue based on quantile regressions. The novel\napproach leverages the potential of quantile regression to model the entire\n(conditional) distribution of the yields instead of restricting to its mean. An\napplication to US data from the 1970s shows the significant heterogeneity of\nthe interactions between financial and macroeconomic variables across different\nquantiles. Moreover, an out-of-sample forecasting exercise showcases the\nproposed method's advantages in predicting the yield distribution tails\ncompared to the standard conditional mean model. Finally, by inspecting the\nposterior distribution of the three factors during the recent major crises, new\nevidence is found that supports the greater and longer-lasting negative impact\nof the great recession on the yields compared to the COVID-19 pandemic."}, "http://arxiv.org/abs/2401.10054": {"title": "Nowcasting economic activity in European regions using a mixed-frequency dynamic factor model", "link": "http://arxiv.org/abs/2401.10054", "description": "Timely information about the state of regional economies can be essential for\nplanning, implementing and evaluating locally targeted economic policies.\nHowever, European regional accounts for output are published at an annual\nfrequency and with a two-year delay. To obtain robust and more timely measures\nin a computationally efficient manner, we propose a mixed-frequency dynamic\nfactor model that accounts for national information to produce high-frequency\nestimates of the regional gross value added (GVA). We show that our model\nproduces reliable nowcasts of GVA in 162 regions across 12 European countries."}, "http://arxiv.org/abs/2108.13707": {"title": "Wild Bootstrap for Instrumental Variables Regressions with Weak and Few Clusters", "link": "http://arxiv.org/abs/2108.13707", "description": "We study the wild bootstrap inference for instrumental variable regressions\nin the framework of a small number of large clusters in which the number of\nclusters is viewed as fixed and the number of observations for each cluster\ndiverges to infinity. We first show that the wild bootstrap Wald test, with or\nwithout using the cluster-robust covariance estimator, controls size\nasymptotically up to a small error as long as the parameters of endogenous\nvariables are strongly identified in at least one of the clusters. Then, we\nestablish the required number of strong clusters for the test to have power\nagainst local alternatives. We further develop a wild bootstrap Anderson-Rubin\ntest for the full-vector inference and show that it controls size\nasymptotically up to a small error even under weak or partial identification in\nall clusters. We illustrate the good finite sample performance of the new\ninference methods using simulations and provide an empirical application to a\nwell-known dataset about US local labor markets."}, "http://arxiv.org/abs/2303.13598": {"title": "Bootstrap-Assisted Inference for Generalized Grenander-type Estimators", "link": "http://arxiv.org/abs/2303.13598", "description": "Westling and Carone (2020) proposed a framework for studying the large sample\ndistributional properties of generalized Grenander-type estimators, a versatile\nclass of nonparametric estimators of monotone functions. The limiting\ndistribution of those estimators is representable as the left derivative of the\ngreatest convex minorant of a Gaussian process whose covariance kernel can be\ncomplicated and whose monomial mean can be of unknown order (when the degree of\nflatness of the function of interest is unknown). The standard nonparametric\nbootstrap is unable to consistently approximate the large sample distribution\nof the generalized Grenander-type estimators even if the monomial order of the\nmean is known, making statistical inference a challenging endeavour in\napplications. To address this inferential problem, we present a\nbootstrap-assisted inference procedure for generalized Grenander-type\nestimators. The procedure relies on a carefully crafted, yet automatic,\ntransformation of the estimator. Moreover, our proposed method can be made\n``flatness robust'' in the sense that it can be made adaptive to the (possibly\nunknown) degree of flatness of the function of interest. The method requires\nonly the consistent estimation of a single scalar quantity, for which we\npropose an automatic procedure based on numerical derivative estimation and the\ngeneralized jackknife. Under random sampling, our inference method can be\nimplemented using a computationally attractive exchangeable bootstrap\nprocedure. We illustrate our methods with examples and we also provide a small\nsimulation study. The development of formal results is made possible by some\ntechnical results that may be of independent interest."}, "http://arxiv.org/abs/2401.10261": {"title": "How industrial clusters influence the growth of the regional GDP: A spatial-approach", "link": "http://arxiv.org/abs/2401.10261", "description": "In this paper, we employ spatial econometric methods to analyze panel data\nfrom German NUTS 3 regions. Our goal is to gain a deeper understanding of the\nsignificance and interdependence of industry clusters in shaping the dynamics\nof GDP. To achieve a more nuanced spatial differentiation, we introduce\nindicator matrices for each industry sector which allows for extending the\nspatial Durbin model to a new version of it. This approach is essential due to\nboth the economic importance of these sectors and the potential issue of\nomitted variables. Failing to account for industry sectors can lead to omitted\nvariable bias and estimation problems. To assess the effects of the major\nindustry sectors, we incorporate eight distinct branches of industry into our\nanalysis. According to prevailing economic theory, these clusters should have a\npositive impact on the regions they are associated with. Our findings indeed\nreveal highly significant impacts, which can be either positive or negative, of\nspecific sectors on local GDP growth. Spatially, we observe that direct and\nindirect effects can exhibit opposite signs, indicative of heightened\ncompetitiveness within and between industry sectors. Therefore, we recommend\nthat industry sectors should be taken into consideration when conducting\nspatial analysis of GDP. Doing so allows for a more comprehensive understanding\nof the economic dynamics at play."}, "http://arxiv.org/abs/2111.00822": {"title": "Financial-cycle ratios and medium-term predictions of GDP: Evidence from the United States", "link": "http://arxiv.org/abs/2111.00822", "description": "Using a large quarterly macroeconomic dataset for the period 1960-2017, we\ndocument the ability of specific financial ratios from the housing market and\nfirms' aggregate balance sheets to predict GDP over medium-term horizons in the\nUnited States. A cyclically adjusted house price-to-rent ratio and the\nliabilities-to-income ratio of the non-financial non-corporate business sector\nprovide the best in-sample and out-of-sample predictions of GDP growth over\nhorizons of one to five years, based on a wide variety of rankings. Small\nforecasting models that include these indicators outperform popular\nhigh-dimensional models and forecast combinations. The predictive power of the\ntwo ratios appears strong during both recessions and expansions, stable over\ntime, and consistent with well-established macro-finance theory."}}