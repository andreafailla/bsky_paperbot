{"http://arxiv.org/abs/2310.03435": {"title": "Variational Inference for GARCH-family Models", "link": "http://arxiv.org/abs/2310.03435", "description": "The Bayesian estimation of GARCH-family models has been typically addressed\nthrough Monte Carlo sampling. Variational Inference is gaining popularity and\nattention as a robust approach for Bayesian inference in complex machine\nlearning models; however, its adoption in econometrics and finance is limited.\nThis paper discusses the extent to which Variational Inference constitutes a\nreliable and feasible alternative to Monte Carlo sampling for Bayesian\ninference in GARCH-like models. Through a large-scale experiment involving the\nconstituents of the S&amp;P 500 index, several Variational Inference optimizers, a\nvariety of volatility models, and a case study, we show that Variational\nInference is an attractive, remarkably well-calibrated, and competitive method\nfor Bayesian learning."}, "http://arxiv.org/abs/2310.03521": {"title": "Cutting Feedback in Misspecified Copula Models", "link": "http://arxiv.org/abs/2310.03521", "description": "In copula models the marginal distributions and copula function are specified\nseparately. We treat these as two modules in a modular Bayesian inference\nframework, and propose conducting modified Bayesian inference by ``cutting\nfeedback''. Cutting feedback limits the influence of potentially misspecified\nmodules in posterior inference. We consider two types of cuts. The first limits\nthe influence of a misspecified copula on inference for the marginals, which is\na Bayesian analogue of the popular Inference for Margins (IFM) estimator. The\nsecond limits the influence of misspecified marginals on inference for the\ncopula parameters by using a rank likelihood to define the cut model. We\nestablish that if only one of the modules is misspecified, then the appropriate\ncut posterior gives accurate uncertainty quantification asymptotically for the\nparameters in the other module. Computation of the cut posteriors is difficult,\nand new variational inference methods to do so are proposed. The efficacy of\nthe new methodology is demonstrated using both simulated data and a substantive\nmultivariate time series application from macroeconomic forecasting. In the\nlatter, cutting feedback from misspecified marginals to a 1096 dimension copula\nimproves posterior inference and predictive accuracy greatly, compared to\nconventional Bayesian inference."}, "http://arxiv.org/abs/2205.04345": {"title": "Joint diagnostic test of regression discontinuity designs: multiple testing problem", "link": "http://arxiv.org/abs/2205.04345", "description": "Current diagnostic tests for regression discontinuity (RD) design face a\nmultiple testing problem. We find a massive over-rejection of the identifying\nrestriction among empirical RD studies published in top-five economics\njournals. Each test achieves a nominal size of 5%; however, the median number\nof tests per study is 12. Consequently, more than one-third of studies reject\nat least one of these tests and their diagnostic procedures are invalid for\njustifying the identifying assumption. We offer a joint testing procedure to\nresolve the multiple testing problem. Our procedure is based on a new joint\nasymptotic normality of local linear estimates and local polynomial density\nestimates. In simulation studies, our joint testing procedures outperform the\nBonferroni correction. We implement the procedure as an R package, rdtest, with\ntwo empirical examples in its vignettes."}, "http://arxiv.org/abs/2212.04620": {"title": "On the Non-Identification of Revenue Production Functions", "link": "http://arxiv.org/abs/2212.04620", "description": "Production functions are potentially misspecified when revenue is used as a\nproxy for output. I formalize and strengthen this common knowledge by showing\nthat neither the production function nor Hicks-neutral productivity can be\nidentified with such a revenue proxy. This result holds under the standard\nassumptions used in the literature for a large class of production functions,\nincluding all commonly used parametric forms. Among the prevalent approaches to\naddress this issue, only those that impose assumptions on the underlying demand\nsystem can possibly identify the production function."}, "http://arxiv.org/abs/2307.13364": {"title": "Tuning-free testing of factor regression against factor-augmented sparse alternatives", "link": "http://arxiv.org/abs/2307.13364", "description": "This study introduces a bootstrap test of the validity of factor regression\nwithin a high-dimensional factor-augmented sparse regression model that\nintegrates factor and sparse regression techniques. The test provides a means\nto assess the suitability of the classical dense factor regression model\ncompared to a sparse plus dense alternative augmenting factor regression with\nidiosyncratic shocks. Our proposed test does not require tuning parameters,\neliminates the need to estimate covariance matrices, and offers simplicity in\nimplementation. The validity of the test is theoretically established under\ntime-series dependence. Through simulation experiments, we demonstrate the\nfavorable finite sample performance of our procedure. Moreover, using the\nFRED-MD dataset, we apply the test and reject the adequacy of the classical\nfactor regression model when the dependent variable is inflation but not when\nit is industrial production. These findings offer insights into selecting\nappropriate models for high-dimensional datasets."}, "http://arxiv.org/abs/2201.12936": {"title": "Pigeonhole Design: Balancing Sequential Experiments from an Online Matching Perspective", "link": "http://arxiv.org/abs/2201.12936", "description": "Practitioners and academics have long appreciated the benefits of covariate\nbalancing when they conduct randomized experiments. For web-facing firms\nrunning online A/B tests, however, it still remains challenging in balancing\ncovariate information when experimental subjects arrive sequentially. In this\npaper, we study an online experimental design problem, which we refer to as the\n\"Online Blocking Problem.\" In this problem, experimental subjects with\nheterogeneous covariate information arrive sequentially and must be immediately\nassigned into either the control or the treated group. The objective is to\nminimize the total discrepancy, which is defined as the minimum weight perfect\nmatching between the two groups. To solve this problem, we propose a randomized\ndesign of experiment, which we refer to as the \"Pigeonhole Design.\" The\npigeonhole design first partitions the covariate space into smaller spaces,\nwhich we refer to as pigeonholes, and then, when the experimental subjects\narrive at each pigeonhole, balances the number of control and treated subjects\nfor each pigeonhole. We analyze the theoretical performance of the pigeonhole\ndesign and show its effectiveness by comparing against two well-known benchmark\ndesigns: the match-pair design and the completely randomized design. We\nidentify scenarios when the pigeonhole design demonstrates more benefits over\nthe benchmark design. To conclude, we conduct extensive simulations using\nYahoo! data to show a 10.2% reduction in variance if we use the pigeonhole\ndesign to estimate the average treatment effect."}, "http://arxiv.org/abs/2310.04576": {"title": "Finite Sample Performance of a Conduct Parameter Test in Homogenous Goods Markets", "link": "http://arxiv.org/abs/2310.04576", "description": "We assess the finite sample performance of the conduct parameter test in\nhomogeneous goods markets. Statistical power rises with an increase in the\nnumber of markets, a larger conduct parameter, and a stronger demand rotation\ninstrument. However, even with a moderate number of markets and five firms,\nregardless of instrument strength and the utilization of optimal instruments,\nrejecting the null hypothesis of perfect competition remains challenging. Our\nfindings indicate that empirical results that fail to reject perfect\ncompetition are a consequence of the limited number of markets rather than\nmethodological deficiencies."}, "http://arxiv.org/abs/2310.04853": {"title": "On changepoint detection in functional data using empirical energy distance", "link": "http://arxiv.org/abs/2310.04853", "description": "We propose a novel family of test statistics to detect the presence of\nchangepoints in a sequence of dependent, possibly multivariate,\nfunctional-valued observations. Our approach allows to test for a very general\nclass of changepoints, including the \"classical\" case of changes in the mean,\nand even changes in the whole distribution. Our statistics are based on a\ngeneralisation of the empirical energy distance; we propose weighted\nfunctionals of the energy distance process, which are designed in order to\nenhance the ability to detect breaks occurring at sample endpoints. The\nlimiting distribution of the maximally selected version of our statistics\nrequires only the computation of the eigenvalues of the covariance function,\nthus being readily implementable in the most commonly employed packages, e.g.\nR. We show that, under the alternative, our statistics are able to detect\nchangepoints occurring even very close to the beginning/end of the sample. In\nthe presence of multiple changepoints, we propose a binary segmentation\nalgorithm to estimate the number of breaks and the locations thereof.\nSimulations show that our procedures work very well in finite samples. We\ncomplement our theory with applications to financial and temperature data."}, "http://arxiv.org/abs/2310.05311": {"title": "Identification and Estimation in a Class of Potential Outcomes Models", "link": "http://arxiv.org/abs/2310.05311", "description": "This paper develops a class of potential outcomes models characterized by\nthree main features: (i) Unobserved heterogeneity can be represented by a\nvector of potential outcomes and a type describing the manner in which an\ninstrument determines the choice of treatment; (ii) The availability of an\ninstrumental variable that is conditionally independent of unobserved\nheterogeneity; and (iii) The imposition of convex restrictions on the\ndistribution of unobserved heterogeneity. The proposed class of models\nencompasses multiple classical and novel research designs, yet possesses a\ncommon structure that permits a unifying analysis of identification and\nestimation. In particular, we establish that these models share a common\nnecessary and sufficient condition for identifying certain causal parameters.\nOur identification results are constructive in that they yield estimating\nmoment conditions for the parameters of interest. Focusing on a leading special\ncase of our framework, we further show how these estimating moment conditions\nmay be modified to be doubly robust. The corresponding double robust estimators\nare shown to be asymptotically normally distributed, bootstrap based inference\nis shown to be asymptotically valid, and the semi-parametric efficiency bound\nis derived for those parameters that are root-n estimable. We illustrate the\nusefulness of our results for developing, identifying, and estimating causal\nmodels through an empirical evaluation of the role of mental health as a\nmediating variable in the Moving To Opportunity experiment."}, "http://arxiv.org/abs/2310.05761": {"title": "Robust Minimum Distance Inference in Structural Models", "link": "http://arxiv.org/abs/2310.05761", "description": "This paper proposes minimum distance inference for a structural parameter of\ninterest, which is robust to the lack of identification of other structural\nnuisance parameters. Some choices of the weighting matrix lead to asymptotic\nchi-squared distributions with degrees of freedom that can be consistently\nestimated from the data, even under partial identification. In any case,\nknowledge of the level of under-identification is not required. We study the\npower of our robust test. Several examples show the wide applicability of the\nprocedure and a Monte Carlo investigates its finite sample performance. Our\nidentification-robust inference method can be applied to make inferences on\nboth calibrated (fixed) parameters and any other structural parameter of\ninterest. We illustrate the method's usefulness by applying it to a structural\nmodel on the non-neutrality of monetary policy, as in \\cite{nakamura2018high},\nwhere we empirically evaluate the validity of the calibrated parameters and we\ncarry out robust inference on the slope of the Phillips curve and the\ninformation effect."}, "http://arxiv.org/abs/2302.13066": {"title": "Estimating Fiscal Multipliers by Combining Statistical Identification with Potentially Endogenous Proxies", "link": "http://arxiv.org/abs/2302.13066", "description": "Different proxy variables used in fiscal policy SVARs lead to contradicting\nconclusions regarding the size of fiscal multipliers. In this paper, we show\nthat the conflicting results are due to violations of the exogeneity\nassumptions, i.e. the commonly used proxies are endogenously related to the\nstructural shocks. We propose a novel approach to include proxy variables into\na Bayesian non-Gaussian SVAR, tailored to accommodate potentially endogenous\nproxy variables. Using our model, we show that increasing government spending\nis a more effective tool to stimulate the economy than reducing taxes. We\nconstruct new exogenous proxies that can be used in the traditional proxy VAR\napproach resulting in similar estimates compared to our proposed hybrid SVAR\nmodel."}, "http://arxiv.org/abs/2303.01863": {"title": "Constructing High Frequency Economic Indicators by Imputation", "link": "http://arxiv.org/abs/2303.01863", "description": "Monthly and weekly economic indicators are often taken to be the largest\ncommon factor estimated from high and low frequency data, either separately or\njointly. To incorporate mixed frequency information without directly modeling\nthem, we target a low frequency diffusion index that is already available, and\ntreat high frequency values as missing. We impute these values using multiple\nfactors estimated from the high frequency data. In the empirical examples\nconsidered, static matrix completion that does not account for serial\ncorrelation in the idiosyncratic errors yields imprecise estimates of the\nmissing values irrespective of how the factors are estimated. Single equation\nand systems-based dynamic procedures that account for serial correlation yield\nimputed values that are closer to the observed low frequency ones. This is the\ncase in the counterfactual exercise that imputes the monthly values of consumer\nsentiment series before 1978 when the data was released only on a quarterly\nbasis. This is also the case for a weekly version of the CFNAI index of\neconomic activity that is imputed using seasonally unadjusted data. The imputed\nseries reveals episodes of increased variability of weekly economic information\nthat are masked by the monthly data, notably around the 2014-15 collapse in oil\nprices."}, "http://arxiv.org/abs/2310.06242": {"title": "Treatment Choice, Mean Square Regret and Partial Identification", "link": "http://arxiv.org/abs/2310.06242", "description": "We consider a decision maker who faces a binary treatment choice when their\nwelfare is only partially identified from data. We contribute to the literature\nby anchoring our finite-sample analysis on mean square regret, a decision\ncriterion advocated by Kitagawa, Lee, and Qiu (2022). We find that optimal\nrules are always fractional, irrespective of the width of the identified set\nand precision of its estimate. The optimal treatment fraction is a simple\nlogistic transformation of the commonly used t-statistic multiplied by a factor\ncalculated by a simple constrained optimization. This treatment fraction gets\ncloser to 0.5 as the width of the identified set becomes wider, implying the\ndecision maker becomes more cautious against the adversarial Nature."}, "http://arxiv.org/abs/2009.01995": {"title": "Instrument Validity for Heterogeneous Causal Effects", "link": "http://arxiv.org/abs/2009.01995", "description": "This paper provides a general framework for testing instrument validity in\nheterogeneous causal effect models. The generalization includes the cases where\nthe treatment can be multivalued ordered or unordered. Based on a series of\ntestable implications, we propose a nonparametric test which is proved to be\nasymptotically size controlled and consistent. Compared to the tests in the\nliterature, our test can be applied in more general settings and may achieve\npower improvement. Refutation of instrument validity by the test helps detect\ninvalid instruments that may yield implausible results on causal effects.\nEvidence that the test performs well on finite samples is provided via\nsimulations. We revisit the empirical study on return to schooling to\ndemonstrate application of the proposed test in practice. An extended\ncontinuous mapping theorem and an extended delta method, which may be of\nindependent interest, are provided to establish the asymptotic distribution of\nthe test statistic under null."}, "http://arxiv.org/abs/2009.07551": {"title": "Manipulation-Robust Regression Discontinuity Designs", "link": "http://arxiv.org/abs/2009.07551", "description": "We present a new identification condition for regression discontinuity\ndesigns. We replace the local randomization of Lee (2008) with two restrictions\non its threat, namely, the manipulation of the running variable. Furthermore,\nwe provide the first auxiliary assumption of McCrary's (2008) diagnostic test\nto detect manipulation. Based on our auxiliary assumption, we derive a novel\nexpression of moments that immediately implies the worst-case bounds of Gerard,\nRokkanen, and Rothe (2020) and an enhanced interpretation of their target\nparameters. We highlight two issues: an overlooked source of identification\nfailure, and a missing auxiliary assumption to detect manipulation. In the case\nstudies, we illustrate our solution to these issues using institutional details\nand economic theories."}, "http://arxiv.org/abs/2205.02274": {"title": "Reducing Marketplace Interference Bias Via Shadow Prices", "link": "http://arxiv.org/abs/2205.02274", "description": "Marketplace companies rely heavily on experimentation when making changes to\nthe design or operation of their platforms. The workhorse of experimentation is\nthe randomized controlled trial (RCT), or A/B test, in which users are randomly\nassigned to treatment or control groups. However, marketplace interference\ncauses the Stable Unit Treatment Value Assumption (SUTVA) to be violated,\nleading to bias in the standard RCT metric. In this work, we propose techniques\nfor platforms to run standard RCTs and still obtain meaningful estimates\ndespite the presence of marketplace interference. We specifically consider a\ngeneralized matching setting, in which the platform explicitly matches supply\nwith demand via a linear programming algorithm. Our first proposal is for the\nplatform to estimate the value of global treatment and global control via\noptimization. We prove that this approach is unbiased in the fluid limit. Our\nsecond proposal is to compare the average shadow price of the treatment and\ncontrol groups rather than the total value accrued by each group. We prove that\nthis technique corresponds to the correct first-order approximation (in a\nTaylor series sense) of the value function of interest even in a finite-size\nsystem. We then use this result to prove that, under reasonable assumptions,\nour estimator is less biased than the RCT estimator. At the heart of our result\nis the idea that it is relatively easy to model interference in matching-driven\nmarketplaces since, in such markets, the platform intermediates the spillover."}, "http://arxiv.org/abs/2208.09638": {"title": "Optimal Pre-Analysis Plans: Statistical Decisions Subject to Implementability", "link": "http://arxiv.org/abs/2208.09638", "description": "What is the purpose of pre-analysis plans, and how should they be designed?\nWe propose a principal-agent model where a decision-maker relies on selective\nbut truthful reports by an analyst. The analyst has data access, and\nnon-aligned objectives. In this model, the implementation of statistical\ndecision rules (tests, estimators) requires an incentive-compatible mechanism.\nWe first characterize which decision rules can be implemented. We then\ncharacterize optimal statistical decision rules subject to implementability. We\nshow that implementation requires pre-analysis plans. Focussing specifically on\nhypothesis tests, we show that optimal rejection rules pre-register a valid\ntest for the case when all data is reported, and make worst-case assumptions\nabout unreported data. Optimal tests can be found as a solution to a\nlinear-programming problem."}, "http://arxiv.org/abs/2302.11505": {"title": "Decomposition and Interpretation of Treatment Effects in Settings with Delayed Outcomes", "link": "http://arxiv.org/abs/2302.11505", "description": "This paper studies settings where the analyst is interested in identifying\nand estimating the average causal effect of a binary treatment on an outcome.\nWe consider a setup in which the outcome realization does not get immediately\nrealized after the treatment assignment, a feature that is ubiquitous in\nempirical settings. The period between the treatment and the realization of the\noutcome allows other observed actions to occur and affect the outcome. In this\ncontext, we study several regression-based estimands routinely used in\nempirical work to capture the average treatment effect and shed light on\ninterpreting them in terms of ceteris paribus effects, indirect causal effects,\nand selection terms. We obtain three main and related takeaways. First, the\nthree most popular estimands do not generally satisfy what we call \\emph{strong\nsign preservation}, in the sense that these estimands may be negative even when\nthe treatment positively affects the outcome conditional on any possible\ncombination of other actions. Second, the most popular regression that includes\nthe other actions as controls satisfies strong sign preservation \\emph{if and\nonly if} these actions are mutually exclusive binary variables. Finally, we\nshow that a linear regression that fully stratifies the other actions leads to\nestimands that satisfy strong sign preservation."}, "http://arxiv.org/abs/2302.13455": {"title": "Nickell Bias in Panel Local Projection: Financial Crises Are Worse Than You Think", "link": "http://arxiv.org/abs/2302.13455", "description": "Local Projection is widely used for impulse response estimation, with the\nFixed Effect (FE) estimator being the default for panel data. This paper\nhighlights the presence of Nickell bias for all regressors in the FE estimator,\neven if lagged dependent variables are absent in the regression. This bias is\nthe consequence of the inherent panel predictive specification. We recommend\nusing the split-panel jackknife estimator to eliminate the asymptotic bias and\nrestore the standard statistical inference. Revisiting three macro-finance\nstudies on the linkage between financial crises and economic contraction, we\nfind that the FE estimator substantially underestimates the post-crisis\neconomic losses."}, "http://arxiv.org/abs/2310.07151": {"title": "Identification and Estimation of a Semiparametric Logit Model using Network Data", "link": "http://arxiv.org/abs/2310.07151", "description": "This paper studies the identification and estimation of a semiparametric\nbinary network model in which the unobserved social characteristic is\nendogenous, that is, the unobserved individual characteristic influences both\nthe binary outcome of interest and how links are formed within the network. The\nexact functional form of the latent social characteristic is not known. The\nproposed estimators are obtained based on matching pairs of agents whose\nnetwork formation distributions are the same. The consistency and the\nasymptotic distribution of the estimators are proposed. The finite sample\nproperties of the proposed estimators in a Monte-Carlo simulation are assessed.\nWe conclude this study with an empirical application."}, "http://arxiv.org/abs/2310.07558": {"title": "Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning", "link": "http://arxiv.org/abs/2310.07558", "description": "We study the dynamic pricing problem where the demand function is\nnonparametric and H\\\"older smooth, and we focus on adaptivity to the unknown\nH\\\"older smoothness parameter $\\beta$ of the demand function. Traditionally the\noptimal dynamic pricing algorithm heavily relies on the knowledge of $\\beta$ to\nachieve a minimax optimal regret of\n$\\widetilde{O}(T^{\\frac{\\beta+1}{2\\beta+1}})$. However, we highlight the\nchallenge of adaptivity in this dynamic pricing problem by proving that no\npricing policy can adaptively achieve this minimax optimal regret without\nknowledge of $\\beta$. Motivated by the impossibility result, we propose a\nself-similarity condition to enable adaptivity. Importantly, we show that the\nself-similarity condition does not compromise the problem's inherent complexity\nsince it preserves the regret lower bound\n$\\Omega(T^{\\frac{\\beta+1}{2\\beta+1}})$. Furthermore, we develop a\nsmoothness-adaptive dynamic pricing algorithm and theoretically prove that the\nalgorithm achieves this minimax optimal regret bound without the prior\nknowledge $\\beta$."}, "http://arxiv.org/abs/1910.07452": {"title": "Identifying Network Ties from Panel Data: Theory and an Application to Tax Competition", "link": "http://arxiv.org/abs/1910.07452", "description": "Social interactions determine many economic behaviors, but information on\nsocial ties does not exist in most publicly available and widely used datasets.\nWe present results on the identification of social networks from observational\npanel data that contains no information on social ties between agents. In the\ncontext of a canonical social interactions model, we provide sufficient\nconditions under which the social interactions matrix, endogenous and exogenous\nsocial effect parameters are all globally identified. While this result is\nrelevant across different estimation strategies, we then describe how\nhigh-dimensional estimation techniques can be used to estimate the interactions\nmodel based on the Adaptive Elastic Net GMM method. We employ the method to\nstudy tax competition across US states. We find the identified social\ninteractions matrix implies tax competition differs markedly from the common\nassumption of competition between geographically neighboring states, providing\nfurther insights for the long-standing debate on the relative roles of factor\nmobility and yardstick competition in driving tax setting behavior across\nstates. Most broadly, our identification and application show the analysis of\nsocial interactions can be extended to economic realms where no network data\nexists."}, "http://arxiv.org/abs/2308.00913": {"title": "The Bayesian Context Trees State Space Model for time series modelling and forecasting", "link": "http://arxiv.org/abs/2308.00913", "description": "A hierarchical Bayesian framework is introduced for developing rich mixture\nmodels for real-valued time series, partly motivated by important applications\nin financial time series analysis. At the top level, meaningful discrete states\nare identified as appropriately quantised values of some of the most recent\nsamples. These observable states are described as a discrete context-tree\nmodel. At the bottom level, a different, arbitrary model for real-valued time\nseries -- a base model -- is associated with each state. This defines a very\ngeneral framework that can be used in conjunction with any existing model class\nto build flexible and interpretable mixture models. We call this the Bayesian\nContext Trees State Space Model, or the BCT-X framework. Efficient algorithms\nare introduced that allow for effective, exact Bayesian inference and learning\nin this setting; in particular, the maximum a posteriori probability (MAP)\ncontext-tree model can be identified. These algorithms can be updated\nsequentially, facilitating efficient online forecasting. The utility of the\ngeneral framework is illustrated in two particular instances: When\nautoregressive (AR) models are used as base models, resulting in a nonlinear AR\nmixture model, and when conditional heteroscedastic (ARCH) models are used,\nresulting in a mixture model that offers a powerful and systematic way of\nmodelling the well-known volatility asymmetries in financial data. In\nforecasting, the BCT-X methods are found to outperform state-of-the-art\ntechniques on simulated and real-world data, both in terms of accuracy and\ncomputational requirements. In modelling, the BCT-X structure finds natural\nstructure present in the data. In particular, the BCT-ARCH model reveals a\nnovel, important feature of stock market index data, in the form of an enhanced\nleverage effect."}, "http://arxiv.org/abs/2310.07790": {"title": "Integration or fragmentation? A closer look at euro area financial markets", "link": "http://arxiv.org/abs/2310.07790", "description": "This paper examines the degree of integration at euro area financial markets.\nTo that end, we estimate overall and country-specific integration indices based\non a panel vector-autoregression with factor stochastic volatility. Our results\nindicate a more heterogeneous bond market compared to the market for lending\nrates. At both markets, the global financial crisis and the sovereign debt\ncrisis led to a severe decline in financial integration, which fully recovered\nsince then. We furthermore identify countries that deviate from their peers\neither by responding differently to crisis events or by taking on different\nroles in the spillover network. The latter analysis reveals two set of\ncountries, namely a main body of countries that receives and transmits\nspillovers and a second, smaller group of spillover absorbing economies.\nFinally, we demonstrate by estimating an augmented Taylor rule that euro area\nshort-term interest rates are positively linked to the level of integration on\nthe bond market."}, "http://arxiv.org/abs/2310.07839": {"title": "Marital Sorting, Household Inequality and Selection", "link": "http://arxiv.org/abs/2310.07839", "description": "Using CPS data for 1976 to 2022 we explore how wage inequality has evolved\nfor married couples with both spouses working full time full year, and its\nimpact on household income inequality. We also investigate how marriage sorting\npatterns have changed over this period. To determine the factors driving income\ninequality we estimate a model explaining the joint distribution of wages which\naccounts for the spouses' employment decisions. We find that income inequality\nhas increased for these households and increased assortative matching of wages\nhas exacerbated the inequality resulting from individual wage growth. We find\nthat positive sorting partially reflects the correlation across unobservables\ninfluencing both members' of the marriage wages. We decompose the changes in\nsorting patterns over the 47 years comprising our sample into structural,\ncomposition and selection effects and find that the increase in positive\nsorting primarily reflects the increased skill premia for both observed and\nunobserved characteristics."}, "http://arxiv.org/abs/2310.08063": {"title": "Uniform Inference for Nonlinear Endogenous Treatment Effects with High-Dimensional Covariates", "link": "http://arxiv.org/abs/2310.08063", "description": "Nonlinearity and endogeneity are common in causal effect studies with\nobservational data. In this paper, we propose new estimation and inference\nprocedures for nonparametric treatment effect functions with endogeneity and\npotentially high-dimensional covariates. The main innovation of this paper is\nthe double bias correction procedure for the nonparametric instrumental\nvariable (NPIV) model under high dimensions. We provide a useful uniform\nconfidence band of the marginal effect function, defined as the derivative of\nthe nonparametric treatment function. The asymptotic honesty of the confidence\nband is verified in theory. Simulations and an empirical study of air pollution\nand migration demonstrate the validity of our procedures."}, "http://arxiv.org/abs/2310.08115": {"title": "Model-Agnostic Covariate-Assisted Inference on Partially Identified Causal Effects", "link": "http://arxiv.org/abs/2310.08115", "description": "Many causal estimands are only partially identifiable since they depend on\nthe unobservable joint distribution between potential outcomes. Stratification\non pretreatment covariates can yield sharper partial identification bounds;\nhowever, unless the covariates are discrete with relatively small support, this\napproach typically requires consistent estimation of the conditional\ndistributions of the potential outcomes given the covariates. Thus, existing\napproaches may fail under model misspecification or if consistency assumptions\nare violated. In this study, we propose a unified and model-agnostic\ninferential approach for a wide class of partially identified estimands, based\non duality theory for optimal transport problems. In randomized experiments,\nour approach can wrap around any estimates of the conditional distributions and\nprovide uniformly valid inference, even if the initial estimates are\narbitrarily inaccurate. Also, our approach is doubly robust in observational\nstudies. Notably, this property allows analysts to use the multiplier bootstrap\nto select covariates and models without sacrificing validity even if the true\nmodel is not included. Furthermore, if the conditional distributions are\nestimated at semiparametric rates, our approach matches the performance of an\noracle with perfect knowledge of the outcome model. Finally, we propose an\nefficient computational framework, enabling implementation on many practical\nproblems in causal inference."}, "http://arxiv.org/abs/2310.08173": {"title": "Structural Vector Autoregressions and Higher Moments: Challenges and Solutions in Small Samples", "link": "http://arxiv.org/abs/2310.08173", "description": "Generalized method of moments estimators based on higher-order moment\nconditions derived from independent shocks can be used to identify and estimate\nthe simultaneous interaction in structural vector autoregressions. This study\nhighlights two problems that arise when using these estimators in small\nsamples. First, imprecise estimates of the asymptotically efficient weighting\nmatrix and the asymptotic variance lead to volatile estimates and inaccurate\ninference. Second, many moment conditions lead to a small sample scaling bias\ntowards innovations with a variance smaller than the normalizing unit variance\nassumption. To address the first problem, I propose utilizing the assumption of\nindependent structural shocks to estimate the efficient weighting matrix and\nthe variance of the estimator. For the second issue, I propose incorporating a\ncontinuously updated scaling term into the weighting matrix, eliminating the\nscaling bias. To demonstrate the effectiveness of these measures, I conducted a\nMonte Carlo simulation which shows a significant improvement in the performance\nof the estimator."}, "http://arxiv.org/abs/2310.08536": {"title": "Real-time Prediction of the Great Recession and the Covid-19 Recession", "link": "http://arxiv.org/abs/2310.08536", "description": "A series of standard and penalized logistic regression models is employed to\nmodel and forecast the Great Recession and the Covid-19 recession in the US.\nThese two recessions are scrutinized by closely examining the movement of five\nchosen predictors, their regression coefficients, and the predicted\nprobabilities of recession. The empirical analysis explores the predictive\ncontent of numerous macroeconomic and financial indicators with respect to NBER\nrecession indicator. The predictive ability of the underlying models is\nevaluated using a set of statistical evaluation metrics. The results strongly\nsupport the application of penalized logistic regression models in the area of\nrecession prediction. Specifically, the analysis indicates that a mixed usage\nof different penalized logistic regression models over different forecast\nhorizons largely outperform standard logistic regression models in the\nprediction of Great recession in the US, as they achieve higher predictive\naccuracy across 5 different forecast horizons. The Great Recession is largely\npredictable, whereas the Covid-19 recession remains unpredictable, given that\nthe Covid-19 pandemic is a real exogenous event. The results are validated by\nconstructing via principal component analysis (PCA) on a set of selected\nvariables a recession indicator that suffers less from publication lags and\nexhibits a very high correlation with the NBER recession indicator."}, "http://arxiv.org/abs/2210.11355": {"title": "Network Synthetic Interventions: A Causal Framework for Panel Data Under Network Interference", "link": "http://arxiv.org/abs/2210.11355", "description": "We propose a generalization of the synthetic controls and synthetic\ninterventions methodology to incorporate network interference. We consider the\nestimation of unit-specific potential outcomes from panel data in the presence\nof spillover across units and unobserved confounding. Key to our approach is a\nnovel latent factor model that takes into account network interference and\ngeneralizes the factor models typically used in panel data settings. We propose\nan estimator, Network Synthetic Interventions (NSI), and show that it\nconsistently estimates the mean outcomes for a unit under an arbitrary set of\ncounterfactual treatments for the network. We further establish that the\nestimator is asymptotically normal. We furnish two validity tests for whether\nthe NSI estimator reliably generalizes to produce accurate counterfactual\nestimates. We provide a novel graph-based experiment design that guarantees the\nNSI estimator produces accurate counterfactual estimates, and also analyze the\nsample complexity of the proposed design. We conclude with simulations that\ncorroborate our theoretical findings."}, "http://arxiv.org/abs/2310.08672": {"title": "Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal", "link": "http://arxiv.org/abs/2310.08672", "description": "In many settings, interventions may be more effective for some individuals\nthan others, so that targeting interventions may be beneficial. We analyze the\nvalue of targeting in the context of a large-scale field experiment with over\n53,000 college students, where the goal was to use \"nudges\" to encourage\nstudents to renew their financial-aid applications before a non-binding\ndeadline. We begin with baseline approaches to targeting. First, we target\nbased on a causal forest that estimates heterogeneous treatment effects and\nthen assigns students to treatment according to those estimated to have the\nhighest treatment effects. Next, we evaluate two alternative targeting\npolicies, one targeting students with low predicted probability of renewing\nfinancial aid in the absence of the treatment, the other targeting those with\nhigh probability. The predicted baseline outcome is not the ideal criterion for\ntargeting, nor is it a priori clear whether to prioritize low, high, or\nintermediate predicted probability. Nonetheless, targeting on low baseline\noutcomes is common in practice, for example because the relationship between\nindividual characteristics and treatment effects is often difficult or\nimpossible to estimate with historical data. We propose hybrid approaches that\nincorporate the strengths of both predictive approaches (accurate estimation)\nand causal approaches (correct criterion); we show that targeting intermediate\nbaseline outcomes is most effective, while targeting based on low baseline\noutcomes is detrimental. In one year of the experiment, nudging all students\nimproved early filing by an average of 6.4 percentage points over a baseline\naverage of 37% filing, and we estimate that targeting half of the students\nusing our preferred policy attains around 75% of this benefit."}, "http://arxiv.org/abs/2310.09013": {"title": "Smoothed instrumental variables quantile regression", "link": "http://arxiv.org/abs/2310.09013", "description": "In this article, I introduce the sivqr command, which estimates the\ncoefficients of the instrumental variables (IV) quantile regression model\nintroduced by Chernozhukov and Hansen (2005). The sivqr command offers several\nadvantages over the existing ivqreg and ivqreg2 commands for estimating this IV\nquantile regression model, which complements the alternative \"triangular model\"\nbehind cqiv and the \"local quantile treatment effect\" model of ivqte.\nComputationally, sivqr implements the smoothed estimator of Kaplan and Sun\n(2017), who show that smoothing improves both computation time and statistical\naccuracy. Standard errors are computed analytically or by Bayesian bootstrap;\nfor non-iid sampling, sivqr is compatible with bootstrap. I discuss syntax and\nthe underlying methodology, and I compare sivqr with other commands in an\nexample."}, "http://arxiv.org/abs/2310.09105": {"title": "Estimating Individual Responses when Tomorrow Matters", "link": "http://arxiv.org/abs/2310.09105", "description": "We propose a regression-based approach to estimate how individuals'\nexpectations influence their responses to a counterfactual change. We provide\nconditions under which average partial effects based on regression estimates\nrecover structural effects. We propose a practical three-step estimation method\nthat relies on subjective beliefs data. We illustrate our approach in a model\nof consumption and saving, focusing on the impact of an income tax that not\nonly changes current income but also affects beliefs about future income. By\napplying our approach to survey data from Italy, we find that considering\nindividuals' beliefs matter for evaluating the impact of tax policies on\nconsumption decisions."}, "http://arxiv.org/abs/2210.09828": {"title": "Modelling Large Dimensional Datasets with Markov Switching Factor Models", "link": "http://arxiv.org/abs/2210.09828", "description": "We study a novel large dimensional approximate factor model with regime\nchanges in the loadings driven by a latent first order Markov process. By\nexploiting the equivalent linear representation of the model, we first recover\nthe latent factors by means of Principal Component Analysis. We then cast the\nmodel in state-space form, and we estimate loadings and transition\nprobabilities through an EM algorithm based on a modified version of the\nBaum-Lindgren-Hamilton-Kim filter and smoother that makes use of the factors\npreviously estimated. Our approach is appealing as it provides closed form\nexpressions for all estimators. More importantly, it does not require knowledge\nof the true number of factors. We derive the theoretical properties of the\nproposed estimation procedure, and we show their good finite sample performance\nthrough a comprehensive set of Monte Carlo experiments. The empirical\nusefulness of our approach is illustrated through an application to a large\nportfolio of stocks."}, "http://arxiv.org/abs/2210.13562": {"title": "Prediction intervals for economic fixed-event forecasts", "link": "http://arxiv.org/abs/2210.13562", "description": "The fixed-event forecasting setup is common in economic policy. It involves a\nsequence of forecasts of the same (`fixed') predictand, so that the difficulty\nof the forecasting problem decreases over time. Fixed-event point forecasts are\ntypically published without a quantitative measure of uncertainty. To construct\nsuch a measure, we consider forecast postprocessing techniques tailored to the\nfixed-event case. We develop regression methods that impose constraints\nmotivated by the problem at hand, and use these methods to construct prediction\nintervals for gross domestic product (GDP) growth in Germany and the US."}, "http://arxiv.org/abs/2302.02747": {"title": "Testing Quantile Forecast Optimality", "link": "http://arxiv.org/abs/2302.02747", "description": "Quantile forecasts made across multiple horizons have become an important\noutput of many financial institutions, central banks and international\norganisations. This paper proposes misspecification tests for such quantile\nforecasts that assess optimality over a set of multiple forecast horizons\nand/or quantiles. The tests build on multiple Mincer-Zarnowitz quantile\nregressions cast in a moment equality framework. Our main test is for the null\nhypothesis of autocalibration, a concept which assesses optimality with respect\nto the information contained in the forecasts themselves. We provide an\nextension that allows to test for optimality with respect to larger information\nsets and a multivariate extension. Importantly, our tests do not just inform\nabout general violations of optimality, but may also provide useful insights\ninto specific forms of sub-optimality. A simulation study investigates the\nfinite sample performance of our tests, and two empirical applications to\nfinancial returns and U.S. macroeconomic series illustrate that our tests can\nyield interesting insights into quantile forecast sub-optimality and its\ncauses."}, "http://arxiv.org/abs/2305.00700": {"title": "Double and Single Descent in Causal Inference with an Application to High-Dimensional Synthetic Control", "link": "http://arxiv.org/abs/2305.00700", "description": "Motivated by a recent literature on the double-descent phenomenon in machine\nlearning, we consider highly over-parameterized models in causal inference,\nincluding synthetic control with many control units. In such models, there may\nbe so many free parameters that the model fits the training data perfectly. We\nfirst investigate high-dimensional linear regression for imputing wage data and\nestimating average treatment effects, where we find that models with many more\ncovariates than sample size can outperform simple ones. We then document the\nperformance of high-dimensional synthetic control estimators with many control\nunits. We find that adding control units can help improve imputation\nperformance even beyond the point where the pre-treatment fit is perfect. We\nprovide a unified theoretical perspective on the performance of these\nhigh-dimensional models. Specifically, we show that more complex models can be\ninterpreted as model-averaging estimators over simpler ones, which we link to\nan improvement in average performance. This perspective yields concrete\ninsights into the use of synthetic control when control units are many relative\nto the number of pre-treatment periods."}, "http://arxiv.org/abs/2310.09398": {"title": "An In-Depth Examination of Requirements for Disclosure Risk Assessment", "link": "http://arxiv.org/abs/2310.09398", "description": "The use of formal privacy to protect the confidentiality of responses in the\n2020 Decennial Census of Population and Housing has triggered renewed interest\nand debate over how to measure the disclosure risks and societal benefits of\nthe published data products. Following long-established precedent in economics\nand statistics, we argue that any proposal for quantifying disclosure risk\nshould be based on pre-specified, objective criteria. Such criteria should be\nused to compare methodologies to identify those with the most desirable\nproperties. We illustrate this approach, using simple desiderata, to evaluate\nthe absolute disclosure risk framework, the counterfactual framework underlying\ndifferential privacy, and prior-to-posterior comparisons. We conclude that\nsatisfying all the desiderata is impossible, but counterfactual comparisons\nsatisfy the most while absolute disclosure risk satisfies the fewest.\nFurthermore, we explain that many of the criticisms levied against differential\nprivacy would be levied against any technology that is not equivalent to\ndirect, unrestricted access to confidential data. Thus, more research is\nneeded, but in the near-term, the counterfactual approach appears best-suited\nfor privacy-utility analysis."}, "http://arxiv.org/abs/2310.09545": {"title": "A Semiparametric Instrumented Difference-in-Differences Approach to Policy Learning", "link": "http://arxiv.org/abs/2310.09545", "description": "Recently, there has been a surge in methodological development for the\ndifference-in-differences (DiD) approach to evaluate causal effects. Standard\nmethods in the literature rely on the parallel trends assumption to identify\nthe average treatment effect on the treated. However, the parallel trends\nassumption may be violated in the presence of unmeasured confounding, and the\naverage treatment effect on the treated may not be useful in learning a\ntreatment assignment policy for the entire population. In this article, we\npropose a general instrumented DiD approach for learning the optimal treatment\npolicy. Specifically, we establish identification results using a binary\ninstrumental variable (IV) when the parallel trends assumption fails to hold.\nAdditionally, we construct a Wald estimator, novel inverse probability\nweighting (IPW) estimators, and a class of semiparametric efficient and\nmultiply robust estimators, with theoretical guarantees on consistency and\nasymptotic normality, even when relying on flexible machine learning algorithms\nfor nuisance parameters estimation. Furthermore, we extend the instrumented DiD\nto the panel data setting. We evaluate our methods in extensive simulations and\na real data application."}, "http://arxiv.org/abs/2310.09597": {"title": "Adaptive maximization of social welfare", "link": "http://arxiv.org/abs/2310.09597", "description": "We consider the problem of repeatedly choosing policies to maximize social\nwelfare. Welfare is a weighted sum of private utility and public revenue.\nEarlier outcomes inform later policies. Utility is not observed, but indirectly\ninferred. Response functions are learned through experimentation.\n\nWe derive a lower bound on regret, and a matching adversarial upper bound for\na variant of the Exp3 algorithm. Cumulative regret grows at a rate of\n$T^{2/3}$. This implies that (i) welfare maximization is harder than the\nmulti-armed bandit problem (with a rate of $T^{1/2}$ for finite policy sets),\nand (ii) our algorithm achieves the optimal rate. For the stochastic setting,\nif social welfare is concave, we can achieve a rate of $T^{1/2}$ (for\ncontinuous policy sets), using a dyadic search algorithm.\n\nWe analyze an extension to nonlinear income taxation, and sketch an extension\nto commodity taxation. We compare our setting to monopoly pricing (which is\neasier), and price setting for bilateral trade (which is harder)."}, "http://arxiv.org/abs/2008.08387": {"title": "A Novel Approach to Predictive Accuracy Testing in Nested Environments", "link": "http://arxiv.org/abs/2008.08387", "description": "We introduce a new approach for comparing the predictive accuracy of two\nnested models that bypasses the difficulties caused by the degeneracy of the\nasymptotic variance of forecast error loss differentials used in the\nconstruction of commonly used predictive comparison statistics. Our approach\ncontinues to rely on the out of sample MSE loss differentials between the two\ncompeting models, leads to nuisance parameter free Gaussian asymptotics and is\nshown to remain valid under flexible assumptions that can accommodate\nheteroskedasticity and the presence of mixed predictors (e.g. stationary and\nlocal to unit root). A local power analysis also establishes its ability to\ndetect departures from the null in both stationary and persistent settings.\nSimulations calibrated to common economic and financial applications indicate\nthat our methods have strong power with good size control across commonly\nencountered sample sizes."}, "http://arxiv.org/abs/2211.07506": {"title": "Type I Tobit Bayesian Additive Regression Trees for Censored Outcome Regression", "link": "http://arxiv.org/abs/2211.07506", "description": "Censoring occurs when an outcome is unobserved beyond some threshold value.\nMethods that do not account for censoring produce biased predictions of the\nunobserved outcome. This paper introduces Type I Tobit Bayesian Additive\nRegression Tree (TOBART-1) models for censored outcomes. Simulation results and\nreal data applications demonstrate that TOBART-1 produces accurate predictions\nof censored outcomes. TOBART-1 provides posterior intervals for the conditional\nexpectation and other quantities of interest. The error term distribution can\nhave a large impact on the expectation of the censored outcome. Therefore the\nerror is flexibly modeled as a Dirichlet process mixture of normal\ndistributions."}, "http://arxiv.org/abs/2302.02866": {"title": "Out of Sample Predictability in Predictive Regressions with Many Predictor Candidates", "link": "http://arxiv.org/abs/2302.02866", "description": "This paper is concerned with detecting the presence of out of sample\npredictability in linear predictive regressions with a potentially large set of\ncandidate predictors. We propose a procedure based on out of sample MSE\ncomparisons that is implemented in a pairwise manner using one predictor at a\ntime and resulting in an aggregate test statistic that is standard normally\ndistributed under the global null hypothesis of no linear predictability.\nPredictors can be highly persistent, purely stationary or a combination of\nboth. Upon rejection of the null hypothesis we subsequently introduce a\npredictor screening procedure designed to identify the most active predictors.\nAn empirical application to key predictors of US economic activity illustrates\nthe usefulness of our methods and highlights the important forward looking role\nplayed by the series of manufacturing new orders."}, "http://arxiv.org/abs/2305.12883": {"title": "Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors", "link": "http://arxiv.org/abs/2305.12883", "description": "In recent years, there has been a significant growth in research focusing on\nminimum $\\ell_2$ norm (ridgeless) interpolation least squares estimators.\nHowever, the majority of these analyses have been limited to a simple\nregression error structure, assuming independent and identically distributed\nerrors with zero mean and common variance. In this paper, we explore prediction\nrisk as well as estimation risk under more general regression error\nassumptions, highlighting the benefits of overparameterization in a finite\nsample. We find that including a large number of unimportant parameters\nrelative to the sample size can effectively reduce both risks. Notably, we\nestablish that the estimation difficulties associated with the variance\ncomponents of both risks can be summarized through the trace of the\nvariance-covariance matrix of the regression errors."}, "http://arxiv.org/abs/2203.09001": {"title": "Selection and parallel trends", "link": "http://arxiv.org/abs/2203.09001", "description": "We study the role of selection into treatment in difference-in-differences\n(DiD) designs. We derive necessary and sufficient conditions for parallel\ntrends assumptions under general classes of selection mechanisms. These\nconditions characterize the empirical content of parallel trends. For settings\nwhere the necessary conditions are questionable, we propose tools for\nselection-based sensitivity analysis. We also provide templates for justifying\nDiD in applications with and without covariates. A reanalysis of the causal\neffect of NSW training programs demonstrates the usefulness of our\nselection-based approach to sensitivity analysis."}, "http://arxiv.org/abs/2207.07318": {"title": "Flexible global forecast combinations", "link": "http://arxiv.org/abs/2207.07318", "description": "Forecast combination -- the aggregation of individual forecasts from multiple\nexperts or models -- is a proven approach to economic forecasting. To date,\nresearch on economic forecasting has concentrated on local combination methods,\nwhich handle separate but related forecasting tasks in isolation. Yet, it has\nbeen known for over two decades in the machine learning community that global\nmethods, which exploit task-relatedness, can improve on local methods that\nignore it. Motivated by the possibility for improvement, this paper introduces\na framework for globally combining forecasts while being flexible to the level\nof task-relatedness. Through our framework, we develop global versions of\nseveral existing forecast combinations. To evaluate the efficacy of these new\nglobal forecast combinations, we conduct extensive comparisons using synthetic\nand real data. Our real data comparisons, which involve forecasts of core\neconomic indicators in the Eurozone, provide empirical evidence that the\naccuracy of global combinations of economic forecasts can surpass local\ncombinations."}, "http://arxiv.org/abs/2210.01938": {"title": "Probability of Causation with Sample Selection: A Reanalysis of the Impacts of J\\'ovenes en Acci\\'on on Formality", "link": "http://arxiv.org/abs/2210.01938", "description": "This paper identifies the probability of causation when there is sample\nselection. We show that the probability of causation is partially identified\nfor individuals who are always observed regardless of treatment status and\nderive sharp bounds under three increasingly restrictive sets of assumptions.\nThe first set imposes an exogenous treatment and a monotone sample selection\nmechanism. To tighten these bounds, the second set also imposes the monotone\ntreatment response assumption, while the third set additionally imposes a\nstochastic dominance assumption. Finally, we use experimental data from the\nColombian job training program J\\'ovenes en Acci\\'on to empirically illustrate\nour approach's usefulness. We find that, among always-employed women, at least\n18% and at most 24% transitioned to the formal labor market because of the\nprogram."}, "http://arxiv.org/abs/2212.06312": {"title": "Policy learning for many outcomes of interest: Combining optimal policy trees with multi-objective Bayesian optimisation", "link": "http://arxiv.org/abs/2212.06312", "description": "Methods for learning optimal policies use causal machine learning models to\ncreate human-interpretable rules for making choices around the allocation of\ndifferent policy interventions. However, in realistic policy-making contexts,\ndecision-makers often care about trade-offs between outcomes, not just\nsingle-mindedly maximising utility for one outcome. This paper proposes an\napproach termed Multi-Objective Policy Learning (MOPoL) which combines optimal\ndecision trees for policy learning with a multi-objective Bayesian optimisation\napproach to explore the trade-off between multiple outcomes. It does this by\nbuilding a Pareto frontier of non-dominated models for different hyperparameter\nsettings which govern outcome weighting. The key here is that a low-cost greedy\ntree can be an accurate proxy for the very computationally costly optimal tree\nfor the purposes of making decisions which means models can be repeatedly fit\nto learn a Pareto frontier. The method is applied to a real-world case-study of\nnon-price rationing of anti-malarial medication in Kenya."}, "http://arxiv.org/abs/2302.08002": {"title": "Deep Learning Enhanced Realized GARCH", "link": "http://arxiv.org/abs/2302.08002", "description": "We propose a new approach to volatility modeling by combining deep learning\n(LSTM) and realized volatility measures. This LSTM-enhanced realized GARCH\nframework incorporates and distills modeling advances from financial\neconometrics, high frequency trading data and deep learning. Bayesian inference\nvia the Sequential Monte Carlo method is employed for statistical inference and\nforecasting. The new framework can jointly model the returns and realized\nvolatility measures, has an excellent in-sample fit and superior predictive\nperformance compared to several benchmark models, while being able to adapt\nwell to the stylized facts in volatility. The performance of the new framework\nis tested using a wide range of metrics, from marginal likelihood, volatility\nforecasting, to tail risk forecasting and option pricing. We report on a\ncomprehensive empirical study using 31 widely traded stock indices over a time\nperiod that includes COVID-19 pandemic."}, "http://arxiv.org/abs/2303.10019": {"title": "Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices", "link": "http://arxiv.org/abs/2303.10019", "description": "This paper presents a new method for combining (or aggregating or ensembling)\nmultivariate probabilistic forecasts, considering dependencies between\nquantiles and marginals through a smoothing procedure that allows for online\nlearning. We discuss two smoothing methods: dimensionality reduction using\nBasis matrices and penalized smoothing. The new online learning algorithm\ngeneralizes the standard CRPS learning framework into multivariate dimensions.\nIt is based on Bernstein Online Aggregation (BOA) and yields optimal asymptotic\nlearning properties. The procedure uses horizontal aggregation, i.e.,\naggregation across quantiles. We provide an in-depth discussion on possible\nextensions of the algorithm and several nested cases related to the existing\nliterature on online forecast combination. We apply the proposed methodology to\nforecasting day-ahead electricity prices, which are 24-dimensional\ndistributional forecasts. The proposed method yields significant improvements\nover uniform combination in terms of continuous ranked probability score\n(CRPS). We discuss the temporal evolution of the weights and hyperparameters\nand present the results of reduced versions of the preferred model. A fast C++\nimplementation of the proposed algorithm will be made available in connection\nwith this paper as an open-source R-Package on CRAN."}, "http://arxiv.org/abs/2309.02072": {"title": "DeepVol: A Pre-Trained Universal Asset Volatility Model", "link": "http://arxiv.org/abs/2309.02072", "description": "This paper introduces DeepVol, a pre-trained deep learning volatility model\nthat is more general than traditional econometric models. DeepVol leverage the\npower of transfer learning to effectively capture and model the volatility\ndynamics of all financial assets, including previously unseen ones, using a\nsingle universal model. This contrasts to the usual practice in the\neconometrics literature, which trains a separate model for each asset. The\nintroduction of DeepVol opens up new avenues for volatility modeling in the\nfinance industry, potentially transforming the way volatility is predicted."}, "http://arxiv.org/abs/2310.11680": {"title": "Trimmed Mean Group Estimation of Average Treatment Effects in Ultra Short T Panels under Correlated Heterogeneity", "link": "http://arxiv.org/abs/2310.11680", "description": "Under correlated heterogeneity, the commonly used two-way fixed effects\nestimator is biased and can lead to misleading inference. This paper proposes a\nnew trimmed mean group (TMG) estimator which is consistent at the irregular\nrate of n^{1/3} even if the time dimension of the panel is as small as the\nnumber of its regressors. Extensions to panels with time effects are provided,\nand a Hausman-type test of correlated heterogeneity is proposed. Small sample\nproperties of the TMG estimator (with and without time effects) are\ninvestigated by Monte Carlo experiments and shown to be satisfactory and\nperform better than other trimmed estimators proposed in the literature. The\nproposed test of correlated heterogeneity is also shown to have the correct\nsize and satisfactory power. The utility of the TMG approach is illustrated\nwith an empirical application."}, "http://arxiv.org/abs/2310.11962": {"title": "Machine Learning for Staggered Difference-in-Differences and Dynamic Treatment Effect Heterogeneity", "link": "http://arxiv.org/abs/2310.11962", "description": "We combine two recently proposed nonparametric difference-in-differences\nmethods, extending them to enable the examination of treatment effect\nheterogeneity in the staggered adoption setting using machine learning. The\nproposed method, machine learning difference-in-differences (MLDID), allows for\nestimation of time-varying conditional average treatment effects on the\ntreated, which can be used to conduct detailed inference on drivers of\ntreatment effect heterogeneity. We perform simulations to evaluate the\nperformance of MLDID and find that it accurately identifies the true predictors\nof treatment effect heterogeneity. We then use MLDID to evaluate the\nheterogeneous impacts of Brazil's Family Health Program on infant mortality,\nand find those in poverty and urban locations experienced the impact of the\npolicy more quickly than other subgroups."}, "http://arxiv.org/abs/2310.11969": {"title": "Survey calibration for causal inference: a simple method to balance covariate distributions", "link": "http://arxiv.org/abs/2310.11969", "description": "This paper proposes a simple method for balancing distributions of covariates\nfor causal inference based on observational studies. The method makes it\npossible to balance an arbitrary number of quantiles (e.g., medians, quartiles,\nor deciles) together with means if necessary. The proposed approach is based on\nthe theory of calibration estimators (Deville and S\\\"arndal 1992), in\nparticular, calibration estimators for quantiles, proposed by Harms and\nDuchesne (2006). By modifying the entropy balancing method and the covariate\nbalancing propensity score method, it is possible to balance the distributions\nof the treatment and control groups. The method does not require numerical\nintegration, kernel density estimation or assumptions about the distributions;\nvalid estimates can be obtained by drawing on existing asymptotic theory.\nResults of a simulation study indicate that the method efficiently estimates\naverage treatment effects on the treated (ATT), the average treatment effect\n(ATE), the quantile treatment effect on the treated (QTT) and the quantile\ntreatment effect (QTE), especially in the presence of non-linearity and\nmis-specification of the models. The proposed methods are implemented in an\nopen source R package jointCalib."}, "http://arxiv.org/abs/2203.04080": {"title": "On Robust Inference in Time Series Regression", "link": "http://arxiv.org/abs/2203.04080", "description": "Least squares regression with heteroskedasticity and autocorrelation\nconsistent (HAC) standard errors has proved very useful in cross section\nenvironments. However, several major difficulties, which are generally\noverlooked, must be confronted when transferring the HAC estimation technology\nto time series environments. First, in plausible time-series environments\ninvolving failure of strong exogeneity, OLS parameter estimates can be\ninconsistent, so that HAC inference fails even asymptotically. Second, most\neconomic time series have strong autocorrelation, which renders HAC regression\nparameter estimates highly inefficient. Third, strong autocorrelation similarly\nrenders HAC conditional predictions highly inefficient. Finally, The structure\nof popular HAC estimators is ill-suited for capturing the autoregressive\nautocorrelation typically present in economic time series, which produces large\nsize distortions and reduced power in HACbased hypothesis testing, in all but\nthe largest samples. We show that all four problems are largely avoided by the\nuse of a simple dynamic regression procedure, which is easily implemented. We\ndemonstrate the advantages of dynamic regression with detailed simulations\ncovering a range of practical issues."}, "http://arxiv.org/abs/2308.15338": {"title": "Another Look at the Linear Probability Model and Nonlinear Index Models", "link": "http://arxiv.org/abs/2308.15338", "description": "We reassess the use of linear models to approximate response probabilities of\nbinary outcomes, focusing on average partial effects (APE). We confirm that\nlinear projection parameters coincide with APEs in certain scenarios. Through\nsimulations, we identify other cases where OLS does or does not approximate\nAPEs and find that having large fraction of fitted values in [0, 1] is neither\nnecessary nor sufficient. We also show nonlinear least squares estimation of\nthe ramp model is consistent and asymptotically normal and is equivalent to\nusing OLS on an iteratively trimmed sample to reduce bias. Our findings offer\npractical guidance for empirical research."}, "http://arxiv.org/abs/2309.10642": {"title": "Testing and correcting sample selection in academic achievement comparisons", "link": "http://arxiv.org/abs/2309.10642", "description": "Country comparisons using standardized test scores may in some cases be\nmisleading unless we make sure that the potential sample selection bias created\nby drop-outs and non-enrollment patterns does not alter the analysis. In this\npaper, I propose an answer to this issue which consists of identifying the\ncounterfactual distribution of achievement (I mean the distribution of\nachievement if there was hypothetically no selection) from the observed\ndistribution of achievements. International comparison measures like means,\nquantiles, and inequality measures have to be computed using that\ncounterfactual distribution which is statistically closer to the observed one\nfor a low proportion of out-of-school children. I identify the quantiles of\nthat latent distribution by readjusting the percentile levels of the observed\nquantile function of achievement. Because the data on test scores is by nature\ntruncated, I have to rely on auxiliary data to borrow identification power. I\nfinally applied my method to compute selection corrected means using PISA 2018\nand PASEC 2019 and I found that ranking/comparisons can change."}, "http://arxiv.org/abs/2310.01104": {"title": "Multi-period static hedging of European options", "link": "http://arxiv.org/abs/2310.01104", "description": "We consider the hedging of European options when the price of the underlying\nasset follows a single-factor Markovian framework. By working in such a\nsetting, Carr and Wu \\cite{carr2014static} derived a spanning relation between\na given option and a continuum of shorter-term options written on the same\nasset. In this paper, we have extended their approach to simultaneously include\noptions over multiple short maturities. We then show a practical implementation\nof this with a finite set of shorter-term options to determine the hedging\nerror using a Gaussian Quadrature method. We perform a wide range of\nexperiments for both the \\textit{Black-Scholes} and \\textit{Merton Jump\nDiffusion} models, illustrating the comparative performance of the two methods."}, "http://arxiv.org/abs/2310.02414": {"title": "On Optimal Set Estimation for Partially Identified Binary Choice Models", "link": "http://arxiv.org/abs/2310.02414", "description": "In this paper we reconsider the notion of optimality in estimation of\npartially identified models. We illustrate the general problem in the context\nof a semiparametric binary choice model with discrete covariates as an example\nof a model which is partially identified as shown in, e.g. Bierens and Hartog\n(1988). A set estimator for the regression coefficients in the model can be\nconstructed by implementing the Maximum Score procedure proposed by Manski\n(1975). For many designs this procedure converges to the identified set for\nthese parameters, and so in one sense is optimal. But as shown in Komarova\n(2013) for other cases the Maximum Score objective function gives an outer\nregion of the identified set. This motivates alternative methods that are\noptimal in one sense that they converge to the identified region in all\ndesigns, and we propose and compare such procedures. One is a Hodges type\nestimator combining the Maximum Score estimator with existing procedures. A\nsecond is a two step estimator using a Maximum Score type objective function in\nthe second step. Lastly we propose a new random set quantile estimator,\nmotivated by definitions introduced in Molchanov (2006). Extensions of these\nideas for the cross sectional model to static and dynamic discrete panel data\nmodels are also provided."}, "http://arxiv.org/abs/2310.12825": {"title": "Nonparametric Regression with Dyadic Data", "link": "http://arxiv.org/abs/2310.12825", "description": "This paper studies the identification and estimation of a nonparametric\nnonseparable dyadic model where the structural function and the distribution of\nthe unobservable random terms are assumed to be unknown. The identification and\nthe estimation of the distribution of the unobservable random term are also\nproposed. I assume that the structural function is continuous and strictly\nincreasing in the unobservable heterogeneity. I propose suitable normalization\nfor the identification by allowing the structural function to have some\ndesirable properties such as homogeneity of degree one in the unobservable\nrandom term and some of its observables. The consistency and the asymptotic\ndistribution of the estimators are proposed. The finite sample properties of\nthe proposed estimators in a Monte-Carlo simulation are assessed."}, "http://arxiv.org/abs/2310.12863": {"title": "Moment-dependent phase transitions in high-dimensional Gaussian approximations", "link": "http://arxiv.org/abs/2310.12863", "description": "High-dimensional central limit theorems have been intensively studied with\nmost focus being on the case where the data is sub-Gaussian or sub-exponential.\nHowever, heavier tails are omnipresent in practice. In this article, we study\nthe critical growth rates of dimension $d$ below which Gaussian approximations\nare asymptotically valid but beyond which they are not. We are particularly\ninterested in how these thresholds depend on the number of moments $m$ that the\nobservations possess. For every $m\\in(2,\\infty)$, we construct i.i.d. random\nvectors $\\textbf{X}_1,...,\\textbf{X}_n$ in $\\mathbb{R}^d$, the entries of which\nare independent and have a common distribution (independent of $n$ and $d$)\nwith finite $m$th absolute moment, and such that the following holds: if there\nexists an $\\varepsilon\\in(0,\\infty)$ such that $d/n^{m/2-1+\\varepsilon}\\not\\to\n0$, then the Gaussian approximation error (GAE) satisfies $$\n\n\\limsup_{n\\to\\infty}\\sup_{t\\in\\mathbb{R}}\\left[\\mathbb{P}\\left(\\max_{1\\leq\nj\\leq d}\\frac{1}{\\sqrt{n}}\\sum_{i=1}^n\\textbf{X}_{ij}\\leq\nt\\right)-\\mathbb{P}\\left(\\max_{1\\leq j\\leq d}\\textbf{Z}_j\\leq\nt\\right)\\right]=1,$$ where $\\textbf{Z} \\sim\n\\mathsf{N}_d(\\textbf{0}_d,\\mathbf{I}_d)$. On the other hand, a result in\nChernozhukov et al. (2023a) implies that the left-hand side above is zero if\njust $d/n^{m/2-1-\\varepsilon}\\to 0$ for some $\\varepsilon\\in(0,\\infty)$. In\nthis sense, there is a moment-dependent phase transition at the threshold\n$d=n^{m/2-1}$ above which the limiting GAE jumps from zero to one."}, "http://arxiv.org/abs/2209.11840": {"title": "Revisiting the Analysis of Matched-Pair and Stratified Experiments in the Presence of Attrition", "link": "http://arxiv.org/abs/2209.11840", "description": "In this paper we revisit some common recommendations regarding the analysis\nof matched-pair and stratified experimental designs in the presence of\nattrition. Our main objective is to clarify a number of well-known claims about\nthe practice of dropping pairs with an attrited unit when analyzing\nmatched-pair designs. Contradictory advice appears in the literature about\nwhether or not dropping pairs is beneficial or harmful, and stratifying into\nlarger groups has been recommended as a resolution to the issue. To address\nthese claims, we derive the estimands obtained from the difference-in-means\nestimator in a matched-pair design both when the observations from pairs with\nan attrited unit are retained and when they are dropped. We find limited\nevidence to support the claims that dropping pairs helps recover the average\ntreatment effect, but we find that it may potentially help in recovering a\nconvex weighted average of conditional average treatment effects. We report\nsimilar findings for stratified designs when studying the estimands obtained\nfrom a regression of outcomes on treatment with and without strata fixed\neffects."}, "http://arxiv.org/abs/2210.04523": {"title": "An identification and testing strategy for proxy-SVARs with weak proxies", "link": "http://arxiv.org/abs/2210.04523", "description": "When proxies (external instruments) used to identify target structural shocks\nare weak, inference in proxy-SVARs (SVAR-IVs) is nonstandard and the\nconstruction of asymptotically valid confidence sets for the impulse responses\nof interest requires weak-instrument robust methods. In the presence of\nmultiple target shocks, test inversion techniques require extra restrictions on\nthe proxy-SVAR parameters other those implied by the proxies that may be\ndifficult to interpret and test. We show that frequentist asymptotic inference\nin these situations can be conducted through Minimum Distance estimation and\nstandard asymptotic methods if the proxy-SVAR can be identified by using\n`strong' instruments for the non-target shocks; i.e. the shocks which are not\nof primary interest in the analysis. The suggested identification strategy\nhinges on a novel pre-test for the null of instrument relevance based on\nbootstrap resampling which is not subject to pre-testing issues, in the sense\nthat the validity of post-test asymptotic inferences is not affected by the\noutcomes of the test. The test is robust to conditionally heteroskedasticity\nand/or zero-censored proxies, is computationally straightforward and applicable\nregardless of the number of shocks being instrumented. Some illustrative\nexamples show the empirical usefulness of the suggested identification and\ntesting strategy."}, "http://arxiv.org/abs/2301.07241": {"title": "Unconditional Quantile Partial Effects via Conditional Quantile Regression", "link": "http://arxiv.org/abs/2301.07241", "description": "This paper develops a semi-parametric procedure for estimation of\nunconditional quantile partial effects using quantile regression coefficients.\nThe estimator is based on an identification result showing that, for continuous\ncovariates, unconditional quantile effects are a weighted average of\nconditional ones at particular quantile levels that depend on the covariates.\nWe propose a two-step estimator for the unconditional effects where in the\nfirst step one estimates a structural quantile regression model, and in the\nsecond step a nonparametric regression is applied to the first step\ncoefficients. We establish the asymptotic properties of the estimator, say\nconsistency and asymptotic normality. Monte Carlo simulations show numerical\nevidence that the estimator has very good finite sample performance and is\nrobust to the selection of bandwidth and kernel. To illustrate the proposed\nmethod, we study the canonical application of the Engel's curve, i.e. food\nexpenditures as a share of income."}, "http://arxiv.org/abs/2302.04380": {"title": "Covariate Adjustment in Experiments with Matched Pairs", "link": "http://arxiv.org/abs/2302.04380", "description": "This paper studies inference on the average treatment effect in experiments\nin which treatment status is determined according to \"matched pairs\" and it is\nadditionally desired to adjust for observed, baseline covariates to gain\nfurther precision. By a \"matched pairs\" design, we mean that units are sampled\ni.i.d. from the population of interest, paired according to observed, baseline\ncovariates and finally, within each pair, one unit is selected at random for\ntreatment. Importantly, we presume that not all observed, baseline covariates\nare used in determining treatment assignment. We study a broad class of\nestimators based on a \"doubly robust\" moment condition that permits us to study\nestimators with both finite-dimensional and high-dimensional forms of covariate\nadjustment. We find that estimators with finite-dimensional, linear adjustments\nneed not lead to improvements in precision relative to the unadjusted\ndifference-in-means estimator. This phenomenon persists even if the adjustments\nare interacted with treatment; in fact, doing so leads to no changes in\nprecision. However, gains in precision can be ensured by including fixed\neffects for each of the pairs. Indeed, we show that this adjustment is the\n\"optimal\" finite-dimensional, linear adjustment. We additionally study two\nestimators with high-dimensional forms of covariate adjustment based on the\nLASSO. For each such estimator, we show that it leads to improvements in\nprecision relative to the unadjusted difference-in-means estimator and also\nprovide conditions under which it leads to the \"optimal\" nonparametric,\ncovariate adjustment. A simulation study confirms the practical relevance of\nour theoretical analysis, and the methods are employed to reanalyze data from\nan experiment using a \"matched pairs\" design to study the effect of\nmacroinsurance on microenterprise."}, "http://arxiv.org/abs/2305.03134": {"title": "Debiased inference for dynamic nonlinear models with two-way fixed effects", "link": "http://arxiv.org/abs/2305.03134", "description": "Panel data models often use fixed effects to account for unobserved\nheterogeneities. These fixed effects are typically incidental parameters and\ntheir estimators converge slowly relative to the square root of the sample\nsize. In the maximum likelihood context, this induces an asymptotic bias of the\nlikelihood function. Test statistics derived from the asymptotically biased\nlikelihood, therefore, no longer follow their standard limiting distributions.\nThis causes severe distortions in test sizes. We consider a generic class of\ndynamic nonlinear models with two-way fixed effects and propose an analytical\nbias correction method for the likelihood function. We formally show that the\nlikelihood ratio, the Lagrange-multiplier, and the Wald test statistics derived\nfrom the corrected likelihood follow their standard asymptotic distributions. A\nbias-corrected estimator of the structural parameters can also be derived from\nthe corrected likelihood function. We evaluate the performance of our bias\ncorrection procedure through simulations and an empirical example."}, "http://arxiv.org/abs/2310.13240": {"title": "Transparency challenges in policy evaluation with causal machine learning -- improving usability and accountability", "link": "http://arxiv.org/abs/2310.13240", "description": "Causal machine learning tools are beginning to see use in real-world policy\nevaluation tasks to flexibly estimate treatment effects. One issue with these\nmethods is that the machine learning models used are generally black boxes,\ni.e., there is no globally interpretable way to understand how a model makes\nestimates. This is a clear problem in policy evaluation applications,\nparticularly in government, because it is difficult to understand whether such\nmodels are functioning in ways that are fair, based on the correct\ninterpretation of evidence and transparent enough to allow for accountability\nif things go wrong. However, there has been little discussion of transparency\nproblems in the causal machine learning literature and how these might be\novercome. This paper explores why transparency issues are a problem for causal\nmachine learning in public policy evaluation applications and considers ways\nthese problems might be addressed through explainable AI tools and by\nsimplifying models in line with interpretable AI principles. It then applies\nthese ideas to a case-study using a causal forest model to estimate conditional\naverage treatment effects for a hypothetical change in the school leaving age\nin Australia. It shows that existing tools for understanding black-box\npredictive models are poorly suited to causal machine learning and that\nsimplifying the model to make it interpretable leads to an unacceptable\nincrease in error (in this application). It concludes that new tools are needed\nto properly understand causal machine learning models and the algorithms that\nfit them."}, "http://arxiv.org/abs/2308.04276": {"title": "Causal Interpretation of Linear Social Interaction Models with Endogenous Networks", "link": "http://arxiv.org/abs/2308.04276", "description": "This study investigates the causal interpretation of linear social\ninteraction models in the presence of endogeneity in network formation under a\nheterogeneous treatment effects framework. We consider an experimental setting\nin which individuals are randomly assigned to treatments while no interventions\nare made for the network structure. We show that running a linear regression\nignoring network endogeneity is not problematic for estimating the average\ndirect treatment effect. However, it leads to sample selection bias and\nnegative-weights problem for the estimation of the average spillover effect. To\novercome these problems, we propose using potential peer treatment as an\ninstrumental variable (IV), which is automatically a valid IV for actual\nspillover exposure. Using this IV, we examine two IV-based estimands and\ndemonstrate that they have a local average treatment-effect-type causal\ninterpretation for the spillover effect."}, "http://arxiv.org/abs/2309.01889": {"title": "The Local Projection Residual Bootstrap for AR(1) Models", "link": "http://arxiv.org/abs/2309.01889", "description": "This paper proposes a local projection residual bootstrap method to construct\nconfidence intervals for impulse response coefficients of AR(1) models. Our\nbootstrap method is based on the local projection (LP) approach and a residual\nbootstrap procedure. We present theoretical results for our bootstrap method\nand proposed confidence intervals. First, we prove the uniform consistency of\nthe LP-residual bootstrap over a large class of AR(1) models that allow for a\nunit root. Then, we prove the asymptotic validity of our confidence intervals\nover the same class of AR(1) models. Finally, we show that the LP-residual\nbootstrap provides asymptotic refinements for confidence intervals on a\nrestricted class of AR(1) models relative to those required for the uniform\nconsistency of our bootstrap."}, "http://arxiv.org/abs/2310.13785": {"title": "Bayesian Estimation of Panel Models under Potentially Sparse Heterogeneity", "link": "http://arxiv.org/abs/2310.13785", "description": "We incorporate a version of a spike and slab prior, comprising a pointmass at\nzero (\"spike\") and a Normal distribution around zero (\"slab\") into a dynamic\npanel data framework to model coefficient heterogeneity. In addition to\nhomogeneity and full heterogeneity, our specification can also capture sparse\nheterogeneity, that is, there is a core group of units that share common\nparameters and a set of deviators with idiosyncratic parameters. We fit a model\nwith unobserved components to income data from the Panel Study of Income\nDynamics. We find evidence for sparse heterogeneity for balanced panels\ncomposed of individuals with long employment histories."}, "http://arxiv.org/abs/2310.14068": {"title": "Unobserved Grouped Heteroskedasticity and Fixed Effects", "link": "http://arxiv.org/abs/2310.14068", "description": "This paper extends the linear grouped fixed effects (GFE) panel model to\nallow for heteroskedasticity from a discrete latent group variable. Key\nfeatures of GFE are preserved, such as individuals belonging to one of a finite\nnumber of groups and group membership is unrestricted and estimated. Ignoring\ngroup heteroskedasticity may lead to poor classification, which is detrimental\nto finite sample bias and standard errors of estimators. I introduce the\n\"weighted grouped fixed effects\" (WGFE) estimator that minimizes a weighted\naverage of group sum of squared residuals. I establish $\\sqrt{NT}$-consistency\nand normality under a concept of group separation based on second moments. A\ntest of group homoskedasticity is discussed. A fast computation procedure is\nprovided. Simulations show that WGFE outperforms alternatives that exclude\nsecond moment information. I demonstrate this approach by considering the link\nbetween income and democracy and the effect of unionization on earnings."}, "http://arxiv.org/abs/2310.14142": {"title": "On propensity score matching with a diverging number of matches", "link": "http://arxiv.org/abs/2310.14142", "description": "This paper reexamines Abadie and Imbens (2016)'s work on propensity score\nmatching for average treatment effect estimation. We explore the asymptotic\nbehavior of these estimators when the number of nearest neighbors, $M$, grows\nwith the sample size. It is shown, hardly surprising but technically\nnontrivial, that the modified estimators can improve upon the original\nfixed-$M$ estimators in terms of efficiency. Additionally, we demonstrate the\npotential to attain the semiparametric efficiency lower bound when the\npropensity score achieves \"sufficient\" dimension reduction, echoing Hahn\n(1998)'s insight about the role of dimension reduction in propensity\nscore-based causal inference."}, "http://arxiv.org/abs/2310.14438": {"title": "BVARs and Stochastic Volatility", "link": "http://arxiv.org/abs/2310.14438", "description": "Bayesian vector autoregressions (BVARs) are the workhorse in macroeconomic\nforecasting. Research in the last decade has established the importance of\nallowing time-varying volatility to capture both secular and cyclical\nvariations in macroeconomic uncertainty. This recognition, together with the\ngrowing availability of large datasets, has propelled a surge in recent\nresearch in building stochastic volatility models suitable for large BVARs.\nSome of these new models are also equipped with additional features that are\nespecially desirable for large systems, such as order invariance -- i.e.,\nestimates are not dependent on how the variables are ordered in the BVAR -- and\nrobustness against COVID-19 outliers. Estimation of these large, flexible\nmodels is made possible by the recently developed equation-by-equation approach\nthat drastically reduces the computational cost of estimating large systems.\nDespite these recent advances, there remains much ongoing work, such as the\ndevelopment of parsimonious approaches for time-varying coefficients and other\ntypes of nonlinearities in large BVARs."}, "http://arxiv.org/abs/2310.14983": {"title": "Causal clustering: design of cluster experiments under network interference", "link": "http://arxiv.org/abs/2310.14983", "description": "This paper studies the design of cluster experiments to estimate the global\ntreatment effect in the presence of spillovers on a single network. We provide\nan econometric framework to choose the clustering that minimizes the worst-case\nmean-squared error of the estimated global treatment effect. We show that the\noptimal clustering can be approximated as the solution of a novel penalized\nmin-cut optimization problem computed via off-the-shelf semi-definite\nprogramming algorithms. Our analysis also characterizes easy-to-check\nconditions to choose between a cluster or individual-level randomization. We\nillustrate the method's properties using unique network data from the universe\nof Facebook's users and existing network data from a field experiment."}, "http://arxiv.org/abs/2004.08318": {"title": "Causal Inference under Outcome-Based Sampling with Monotonicity Assumptions", "link": "http://arxiv.org/abs/2004.08318", "description": "We study causal inference under case-control and case-population sampling.\nSpecifically, we focus on the binary-outcome and binary-treatment case, where\nthe parameters of interest are causal relative and attributable risks defined\nvia the potential outcome framework. It is shown that strong ignorability is\nnot always as powerful as it is under random sampling and that certain\nmonotonicity assumptions yield comparable results in terms of sharp identified\nintervals. Specifically, the usual odds ratio is shown to be a sharp identified\nupper bound on causal relative risk under the monotone treatment response and\nmonotone treatment selection assumptions. We offer algorithms for inference on\nthe causal parameters that are aggregated over the true population distribution\nof the covariates. We show the usefulness of our approach by studying three\nempirical examples: the benefit of attending private school for entering a\nprestigious university in Pakistan; the relationship between staying in school\nand getting involved with drug-trafficking gangs in Brazil; and the link\nbetween physicians' hours and size of the group practice in the United States."}, "http://arxiv.org/abs/2108.07455": {"title": "Causal Inference with Noncompliance and Unknown Interference", "link": "http://arxiv.org/abs/2108.07455", "description": "We consider a causal inference model in which individuals interact in a\nsocial network and they may not comply with the assigned treatments. In\nparticular, we suppose that the form of network interference is unknown to\nresearchers. To estimate meaningful causal parameters in this situation, we\nintroduce a new concept of exposure mapping, which summarizes potentially\ncomplicated spillover effects into a fixed dimensional statistic of\ninstrumental variables. We investigate identification conditions for the\nintention-to-treat effects and the average treatment effects for compliers,\nwhile explicitly considering the possibility of misspecification of exposure\nmapping. Based on our identification results, we develop nonparametric\nestimation procedures via inverse probability weighting. Their asymptotic\nproperties, including consistency and asymptotic normality, are investigated\nusing an approximate neighborhood interference framework. For an empirical\nillustration, we apply our method to experimental data on the anti-conflict\nintervention school program. The proposed methods are readily available with\nthe companion R package latenetwork."}, "http://arxiv.org/abs/2112.03872": {"title": "Nonparametric Treatment Effect Identification in School Choice", "link": "http://arxiv.org/abs/2112.03872", "description": "This paper studies nonparametric identification and estimation of causal\neffects in centralized school assignment. In many centralized assignment\nsettings, students are subjected to both lottery-driven variation and\nregression discontinuity (RD) driven variation. We characterize the full set of\nidentified atomic treatment effects (aTEs), defined as the conditional average\ntreatment effect between a pair of schools, given student characteristics.\nAtomic treatment effects are the building blocks of more aggregated notions of\ntreatment contrasts, and common approaches estimating aggregations of aTEs can\nmask important heterogeneity. In particular, many aggregations of aTEs put zero\nweight on aTEs driven by RD variation, and estimators of such aggregations put\nasymptotically vanishing weight on the RD-driven aTEs. We develop a diagnostic\ntool for empirically assessing the weight put on aTEs driven by RD variation.\nLastly, we provide estimators and accompanying asymptotic results for inference\non aggregations of RD-driven aTEs."}, "http://arxiv.org/abs/2203.01425": {"title": "A Modern Gauss-Markov Theorem? Really?", "link": "http://arxiv.org/abs/2203.01425", "description": "We show that the theorems in Hansen (2021a) (the version accepted by\nEconometrica), except for one, are not new as they coincide with classical\ntheorems like the good old Gauss-Markov or Aitken Theorem, respectively; the\nexceptional theorem is incorrect. Hansen (2021b) corrects this theorem. As a\nresult, all theorems in the latter version coincide with the above mentioned\nclassical theorems. Furthermore, we also show that the theorems in Hansen\n(2022) (the version published in Econometrica) either coincide with the\nclassical theorems just mentioned, or contain extra assumptions that are alien\nto the Gauss-Markov or Aitken Theorem."}, "http://arxiv.org/abs/2204.12723": {"title": "Information-theoretic limitations of data-based price discrimination", "link": "http://arxiv.org/abs/2204.12723", "description": "This paper studies third-degree price discrimination (3PD) based on a random\nsample of valuation and covariate data, where the covariate is continuous, and\nthe distribution of the data is unknown to the seller. The main results of this\npaper are twofold. The first set of results is pricing strategy independent and\nreveals the fundamental information-theoretic limitation of any data-based\npricing strategy in revenue generation for two cases: 3PD and uniform pricing.\nThe second set of results proposes the $K$-markets empirical revenue\nmaximization (ERM) strategy and shows that the $K$-markets ERM and the uniform\nERM strategies achieve the optimal rate of convergence in revenue to that\ngenerated by their respective true-distribution 3PD and uniform pricing optima.\nOur theoretical and numerical results suggest that the uniform (i.e.,\n$1$-market) ERM strategy generates a larger revenue than the $K$-markets ERM\nstrategy when the sample size is small enough, and vice versa."}, "http://arxiv.org/abs/2304.12698": {"title": "Enhanced multilayer perceptron with feature selection and grid search for travel mode choice prediction", "link": "http://arxiv.org/abs/2304.12698", "description": "Accurate and reliable prediction of individual travel mode choices is crucial\nfor developing multi-mode urban transportation systems, conducting\ntransportation planning and formulating traffic demand management strategies.\nTraditional discrete choice models have dominated the modelling methods for\ndecades yet suffer from strict model assumptions and low prediction accuracy.\nIn recent years, machine learning (ML) models, such as neural networks and\nboosting models, are widely used by researchers for travel mode choice\nprediction and have yielded promising results. However, despite the superior\nprediction performance, a large body of ML methods, especially the branch of\nneural network models, is also limited by overfitting and tedious model\nstructure determination process. To bridge this gap, this study proposes an\nenhanced multilayer perceptron (MLP; a neural network) with two hidden layers\nfor travel mode choice prediction; this MLP is enhanced by XGBoost (a boosting\nmethod) for feature selection and a grid search method for optimal hidden\nneurone determination of each hidden layer. The proposed method was trained and\ntested on a real resident travel diary dataset collected in Chengdu, China."}, "http://arxiv.org/abs/2306.02584": {"title": "Synthetic Regressing Control Method", "link": "http://arxiv.org/abs/2306.02584", "description": "Estimating weights in the synthetic control method, typically resulting in\nsparse weights where only a few control units have non-zero weights, involves\nan optimization procedure that simultaneously selects and aligns control units\nto closely match the treated unit. However, this simultaneous selection and\nalignment of control units may lead to a loss of efficiency. Another concern\narising from the aforementioned procedure is its susceptibility to\nunder-fitting due to imperfect pre-treatment fit. It is not uncommon for the\nlinear combination, using nonnegative weights, of pre-treatment period outcomes\nfor the control units to inadequately approximate the pre-treatment outcomes\nfor the treated unit. To address both of these issues, this paper proposes a\nsimple and effective method called Synthetic Regressing Control (SRC). The SRC\nmethod begins by performing the univariate linear regression to appropriately\nalign the pre-treatment periods of the control units with the treated unit.\nSubsequently, a SRC estimator is obtained by synthesizing (taking a weighted\naverage) the fitted controls. To determine the weights in the synthesis\nprocedure, we propose an approach that utilizes a criterion of unbiased risk\nestimator. Theoretically, we show that the synthesis way is asymptotically\noptimal in the sense of achieving the lowest possible squared error. Extensive\nnumerical experiments highlight the advantages of the SRC method."}, "http://arxiv.org/abs/2308.12470": {"title": "Scalable Estimation of Multinomial Response Models with Uncertain Consideration Sets", "link": "http://arxiv.org/abs/2308.12470", "description": "A standard assumption in the fitting of unordered multinomial response models\nfor $J$ mutually exclusive nominal categories, on cross-sectional or\nlongitudinal data, is that the responses arise from the same set of $J$\ncategories between subjects. However, when responses measure a choice made by\nthe subject, it is more appropriate to assume that the distribution of\nmultinomial responses is conditioned on a subject-specific consideration set,\nwhere this consideration set is drawn from the power set of $\\{1,2,\\ldots,J\\}$.\nBecause the cardinality of this power set is exponential in $J$, estimation is\ninfeasible in general. In this paper, we provide an approach to overcoming this\nproblem. A key step in the approach is a probability model over consideration\nsets, based on a general representation of probability distributions on\ncontingency tables, which results in mixtures of independent consideration\nmodels. Although the support of this distribution is exponentially large, the\nposterior distribution over consideration sets given parameters is typically\nsparse, and is easily sampled in an MCMC scheme. We show posterior consistency\nof the parameters of the conditional response model and the distribution of\nconsideration sets. The effectiveness of the methodology is documented in\nsimulated longitudinal data sets with $J=100$ categories and real data from the\ncereal market with $J=68$ brands."}, "http://arxiv.org/abs/2310.15512": {"title": "Inference for Rank-Rank Regressions", "link": "http://arxiv.org/abs/2310.15512", "description": "Slope coefficients in rank-rank regressions are popular measures of\nintergenerational mobility, for instance in regressions of a child's income\nrank on their parent's income rank. In this paper, we first point out that\ncommonly used variance estimators such as the homoskedastic or robust variance\nestimators do not consistently estimate the asymptotic variance of the OLS\nestimator in a rank-rank regression. We show that the probability limits of\nthese estimators may be too large or too small depending on the shape of the\ncopula of child and parent incomes. Second, we derive a general asymptotic\ntheory for rank-rank regressions and provide a consistent estimator of the OLS\nestimator's asymptotic variance. We then extend the asymptotic theory to other\nregressions involving ranks that have been used in empirical work. Finally, we\napply our new inference methods to three empirical studies. We find that the\nconfidence intervals based on estimators of the correct variance may sometimes\nbe substantially shorter and sometimes substantially longer than those based on\ncommonly used variance estimators. The differences in confidence intervals\nconcern economically meaningful values of mobility and thus lead to different\nconclusions when comparing mobility in U.S. commuting zones with mobility in\nother countries."}, "http://arxiv.org/abs/2310.15796": {"title": "Testing for equivalence of pre-trends in Difference-in-Differences estimation", "link": "http://arxiv.org/abs/2310.15796", "description": "The plausibility of the ``parallel trends assumption'' in\nDifference-in-Differences estimation is usually assessed by a test of the null\nhypothesis that the difference between the average outcomes of both groups is\nconstant over time before the treatment. However, failure to reject the null\nhypothesis does not imply the absence of differences in time trends between\nboth groups. We provide equivalence tests that allow researchers to find\nevidence in favor of the parallel trends assumption and thus increase the\ncredibility of their treatment effect estimates. While we motivate our tests in\nthe standard two-way fixed effects model, we discuss simple extensions to\nsettings in which treatment adoption is staggered over time."}, "http://arxiv.org/abs/1712.04802": {"title": "Fisher-Schultz Lecture: Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments, with an Application to Immunization in India", "link": "http://arxiv.org/abs/1712.04802", "description": "We propose strategies to estimate and make inference on key features of\nheterogeneous effects in randomized experiments. These key features include\nbest linear predictors of the effects using machine learning proxies, average\neffects sorted by impact groups, and average characteristics of most and least\nimpacted units. The approach is valid in high dimensional settings, where the\neffects are proxied (but not necessarily consistently estimated) by predictive\nand causal machine learning methods. We post-process these proxies into\nestimates of the key features. Our approach is generic, it can be used in\nconjunction with penalized methods, neural networks, random forests, boosted\ntrees, and ensemble methods, both predictive and causal. Estimation and\ninference are based on repeated data splitting to avoid overfitting and achieve\nvalidity. We use quantile aggregation of the results across many potential\nsplits, in particular taking medians of p-values and medians and other\nquantiles of confidence intervals. We show that quantile aggregation lowers\nestimation risks over a single split procedure, and establish its principal\ninferential properties. Finally, our analysis reveals ways to build provably\nbetter machine learning proxies through causal learning: we can use the\nobjective functions that we develop to construct the best linear predictors of\nthe effects, to obtain better machine learning proxies in the initial step. We\nillustrate the use of both inferential tools and causal learners with a\nrandomized field experiment that evaluates a combination of nudges to stimulate\ndemand for immunization in India."}, "http://arxiv.org/abs/2301.09016": {"title": "Inference for Two-stage Experiments under Covariate-Adaptive Randomization", "link": "http://arxiv.org/abs/2301.09016", "description": "This paper studies inference in two-stage randomized experiments under\ncovariate-adaptive randomization. In the initial stage of this experimental\ndesign, clusters (e.g., households, schools, or graph partitions) are\nstratified and randomly assigned to control or treatment groups based on\ncluster-level covariates. Subsequently, an independent second-stage design is\ncarried out, wherein units within each treated cluster are further stratified\nand randomly assigned to either control or treatment groups, based on\nindividual-level covariates. Under the homogeneous partial interference\nassumption, I establish conditions under which the proposed\ndifference-in-\"average of averages\" estimators are consistent and\nasymptotically normal for the corresponding average primary and spillover\neffects and develop consistent estimators of their asymptotic variances.\nCombining these results establishes the asymptotic validity of tests based on\nthese estimators. My findings suggest that ignoring covariate information in\nthe design stage can result in efficiency loss, and commonly used inference\nmethods that ignore or improperly use covariate information can lead to either\nconservative or invalid inference. Finally, I apply these results to studying\noptimal use of covariate information under covariate-adaptive randomization in\nlarge samples, and demonstrate that a specific generalized matched-pair design\nachieves minimum asymptotic variance for each proposed estimator. The practical\nrelevance of the theoretical results is illustrated through a simulation study\nand an empirical application."}, "http://arxiv.org/abs/2306.12003": {"title": "Difference-in-Differences with Interference: A Finite Population Perspective", "link": "http://arxiv.org/abs/2306.12003", "description": "In many scenarios, such as the evaluation of place-based policies, potential\noutcomes are not only dependent upon the unit's own treatment but also its\nneighbors' treatment. Despite this, \"difference-in-differences\" (DID) type\nestimators typically ignore such interference among neighbors. I show in this\npaper that the canonical DID estimators generally fail to identify interesting\ncausal effects in the presence of neighborhood interference. To incorporate\ninterference structure into DID estimation, I propose doubly robust estimators\nfor the direct average treatment effect on the treated as well as the average\nspillover effects under a modified parallel trends assumption. When spillover\neffects are of interest, we often sample the entire population. Thus, I adopt a\nfinite population perspective in the sense that the estimands are defined as\npopulation averages and inference is conditional on the attributes of all\npopulation units. The approach in this paper relaxes common restrictions in the\nliterature, such as partial interference and correctly specified spillover\nfunctions. Moreover, robust inference is discussed based on the asymptotic\ndistribution of the proposed estimators."}, "http://arxiv.org/abs/2310.16281": {"title": "Improving Robust Decisions with Data", "link": "http://arxiv.org/abs/2310.16281", "description": "A decision-maker (DM) faces uncertainty governed by a data-generating process\n(DGP), which is only known to belong to a set of sequences of independent but\npossibly non-identical distributions. A robust decision maximizes the DM's\nexpected payoff against the worst possible DGP in this set. This paper studies\nhow such robust decisions can be improved with data, where improvement is\nmeasured by expected payoff under the true DGP. In this paper, I fully\ncharacterize when and how such an improvement can be guaranteed under all\npossible DGPs and develop inference methods to achieve it. These inference\nmethods are needed because, as this paper shows, common inference methods\n(e.g., maximum likelihood or Bayesian) often fail to deliver such an\nimprovement. Importantly, the developed inference methods are given by simple\naugmentations to standard inference procedures, and are thus easy to implement\nin practice."}, "http://arxiv.org/abs/2310.16290": {"title": "Fair Adaptive Experiments", "link": "http://arxiv.org/abs/2310.16290", "description": "Randomized experiments have been the gold standard for assessing the\neffectiveness of a treatment or policy. The classical complete randomization\napproach assigns treatments based on a prespecified probability and may lead to\ninefficient use of data. Adaptive experiments improve upon complete\nrandomization by sequentially learning and updating treatment assignment\nprobabilities. However, their application can also raise fairness and equity\nconcerns, as assignment probabilities may vary drastically across groups of\nparticipants. Furthermore, when treatment is expected to be extremely\nbeneficial to certain groups of participants, it is more appropriate to expose\nmany of these participants to favorable treatment. In response to these\nchallenges, we propose a fair adaptive experiment strategy that simultaneously\nenhances data use efficiency, achieves an envy-free treatment assignment\nguarantee, and improves the overall welfare of participants. An important\nfeature of our proposed strategy is that we do not impose parametric modeling\nassumptions on the outcome variables, making it more versatile and applicable\nto a wider array of applications. Through our theoretical investigation, we\ncharacterize the convergence rate of the estimated treatment effects and the\nassociated standard deviations at the group level and further prove that our\nadaptive treatment assignment algorithm, despite not having a closed-form\nexpression, approaches the optimal allocation rule asymptotically. Our proof\nstrategy takes into account the fact that the allocation decisions in our\ndesign depend on sequentially accumulated data, which poses a significant\nchallenge in characterizing the properties and conducting statistical inference\nof our method. We further provide simulation evidence to showcase the\nperformance of our fair adaptive experiment strategy."}, "http://arxiv.org/abs/2310.16638": {"title": "Covariate Shift Adaptation Robust to Density-Ratio Estimation", "link": "http://arxiv.org/abs/2310.16638", "description": "Consider a scenario where we have access to train data with both covariates\nand outcomes while test data only contains covariates. In this scenario, our\nprimary aim is to predict the missing outcomes of the test data. With this\nobjective in mind, we train parametric regression models under a covariate\nshift, where covariate distributions are different between the train and test\ndata. For this problem, existing studies have proposed covariate shift\nadaptation via importance weighting using the density ratio. This approach\naverages the train data losses, each weighted by an estimated ratio of the\ncovariate densities between the train and test data, to approximate the\ntest-data risk. Although it allows us to obtain a test-data risk minimizer, its\nperformance heavily relies on the accuracy of the density ratio estimation.\nMoreover, even if the density ratio can be consistently estimated, the\nestimation errors of the density ratio also yield bias in the estimators of the\nregression model's parameters of interest. To mitigate these challenges, we\nintroduce a doubly robust estimator for covariate shift adaptation via\nimportance weighting, which incorporates an additional estimator for the\nregression function. Leveraging double machine learning techniques, our\nestimator reduces the bias arising from the density ratio estimation errors. We\ndemonstrate the asymptotic distribution of the regression parameter estimator.\nNotably, our estimator remains consistent if either the density ratio estimator\nor the regression function is consistent, showcasing its robustness against\npotential errors in density ratio estimation. Finally, we confirm the soundness\nof our proposed method via simulation studies."}, "http://arxiv.org/abs/2310.16819": {"title": "CATE Lasso: Conditional Average Treatment Effect Estimation with High-Dimensional Linear Regression", "link": "http://arxiv.org/abs/2310.16819", "description": "In causal inference about two treatments, Conditional Average Treatment\nEffects (CATEs) play an important role as a quantity representing an\nindividualized causal effect, defined as a difference between the expected\noutcomes of the two treatments conditioned on covariates. This study assumes\ntwo linear regression models between a potential outcome and covariates of the\ntwo treatments and defines CATEs as a difference between the linear regression\nmodels. Then, we propose a method for consistently estimating CATEs even under\nhigh-dimensional and non-sparse parameters. In our study, we demonstrate that\ndesirable theoretical properties, such as consistency, remain attainable even\nwithout assuming sparsity explicitly if we assume a weaker assumption called\nimplicit sparsity originating from the definition of CATEs. In this assumption,\nwe suppose that parameters of linear models in potential outcomes can be\ndivided into treatment-specific and common parameters, where the\ntreatment-specific parameters take difference values between each linear\nregression model, while the common parameters remain identical. Thus, in a\ndifference between two linear regression models, the common parameters\ndisappear, leaving only differences in the treatment-specific parameters.\nConsequently, the non-zero parameters in CATEs correspond to the differences in\nthe treatment-specific parameters. Leveraging this assumption, we develop a\nLasso regression method specialized for CATE estimation and present that the\nestimator is consistent. Finally, we confirm the soundness of the proposed\nmethod by simulation studies."}, "http://arxiv.org/abs/2203.06685": {"title": "Encompassing Tests for Nonparametric Regressions", "link": "http://arxiv.org/abs/2203.06685", "description": "We set up a formal framework to characterize encompassing of nonparametric\nmodels through the L2 distance. We contrast it to previous literature on the\ncomparison of nonparametric regression models. We then develop testing\nprocedures for the encompassing hypothesis that are fully nonparametric. Our\ntest statistics depend on kernel regression, raising the issue of bandwidth's\nchoice. We investigate two alternative approaches to obtain a \"small bias\nproperty\" for our test statistics. We show the validity of a wild bootstrap\nmethod. We empirically study the use of a data-driven bandwidth and illustrate\nthe attractive features of our tests for small and moderate samples."}, "http://arxiv.org/abs/2212.11012": {"title": "Partly Linear Instrumental Variables Regressions without Smoothing on the Instruments", "link": "http://arxiv.org/abs/2212.11012", "description": "We consider a semiparametric partly linear model identified by instrumental\nvariables. We propose an estimation method that does not smooth on the\ninstruments and we extend the Landweber-Fridman regularization scheme to the\nestimation of this semiparametric model. We then show the asymptotic normality\nof the parametric estimator and obtain the convergence rate for the\nnonparametric estimator. Our estimator that does not smooth on the instruments\ncoincides with a typical estimator that does smooth on the instruments but\nkeeps the respective bandwidth fixed as the sample size increases. We propose a\ndata driven method for the selection of the regularization parameter, and in a\nsimulation study we show the attractive performance of our estimators."}, "http://arxiv.org/abs/2212.11112": {"title": "A Bootstrap Specification Test for Semiparametric Models with Generated Regressors", "link": "http://arxiv.org/abs/2212.11112", "description": "This paper provides a specification test for semiparametric models with\nnonparametrically generated regressors. Such variables are not observed by the\nresearcher but are nonparametrically identified and estimable. Applications of\nthe test include models with endogenous regressors identified by control\nfunctions, semiparametric sample selection models, or binary games with\nincomplete information. The statistic is built from the residuals of the\nsemiparametric model. A novel wild bootstrap procedure is shown to provide\nvalid critical values. We consider nonparametric estimators with an automatic\nbias correction that makes the test implementable without undersmoothing. In\nsimulations the test exhibits good small sample performances, and an\napplication to women's labor force participation decisions shows its\nimplementation in a real data context."}, "http://arxiv.org/abs/2305.07993": {"title": "The Nonstationary Newsvendor with (and without) Predictions", "link": "http://arxiv.org/abs/2305.07993", "description": "The classic newsvendor model yields an optimal decision for a \"newsvendor\"\nselecting a quantity of inventory, under the assumption that the demand is\ndrawn from a known distribution. Motivated by applications such as cloud\nprovisioning and staffing, we consider a setting in which newsvendor-type\ndecisions must be made sequentially, in the face of demand drawn from a\nstochastic process that is both unknown and nonstationary. All prior work on\nthis problem either (a) assumes that the level of nonstationarity is known, or\n(b) imposes additional statistical assumptions that enable accurate predictions\nof the unknown demand.\n\nWe study the Nonstationary Newsvendor, with and without predictions. We\nfirst, in the setting without predictions, design a policy which we prove (via\nmatching upper and lower bounds) achieves order-optimal regret -- ours is the\nfirst policy to accomplish this without being given the level of\nnonstationarity of the underlying demand. We then, for the first time,\nintroduce a model for generic (i.e. with no statistical assumptions)\npredictions with arbitrary accuracy, and propose a policy that incorporates\nthese predictions without being given their accuracy. We upper bound the regret\nof this policy, and show that it matches the best achievable regret had the\naccuracy of the predictions been known. Finally, we empirically validate our\nnew policy with experiments based on two real-world datasets containing\nthousands of time-series, showing that it succeeds in closing approximately 74%\nof the gap between the best approaches based on nonstationarity and predictions\nalone."}, "http://arxiv.org/abs/2310.16849": {"title": "Correlation structure analysis of the global agricultural futures market", "link": "http://arxiv.org/abs/2310.16849", "description": "This paper adopts the random matrix theory (RMT) to analyze the correlation\nstructure of the global agricultural futures market from 2000 to 2020. It is\nfound that the distribution of correlation coefficients is asymmetric and right\nskewed, and many eigenvalues of the correlation matrix deviate from the RMT\nprediction. The largest eigenvalue reflects a collective market effect common\nto all agricultural futures, the other largest deviating eigenvalues can be\nimplemented to identify futures groups, and there are modular structures based\non regional properties or agricultural commodities among the significant\nparticipants of their corresponding eigenvectors. Except for the smallest\neigenvalue, other smallest deviating eigenvalues represent the agricultural\nfutures pairs with highest correlations. This paper can be of reference and\nsignificance for using agricultural futures to manage risk and optimize asset\nallocation."}, "http://arxiv.org/abs/2310.16850": {"title": "The impact of the Russia-Ukraine conflict on the extreme risk spillovers between agricultural futures and spots", "link": "http://arxiv.org/abs/2310.16850", "description": "The ongoing Russia-Ukraine conflict between two major agricultural powers has\nposed significant threats and challenges to the global food system and world\nfood security. Focusing on the impact of the conflict on the global\nagricultural market, we propose a new analytical framework for tail dependence,\nand combine the Copula-CoVaR method with the ARMA-GARCH-skewed Student-t model\nto examine the tail dependence structure and extreme risk spillover between\nagricultural futures and spots over the pre- and post-outbreak periods. Our\nresults indicate that the tail dependence structures in the futures-spot\nmarkets of soybean, maize, wheat, and rice have all reacted to the\nRussia-Ukraine conflict. Furthermore, the outbreak of the conflict has\nintensified risks of the four agricultural markets in varying degrees, with the\nwheat market being affected the most. Additionally, all the agricultural\nfutures markets exhibit significant downside and upside risk spillovers to\ntheir corresponding spot markets before and after the outbreak of the conflict,\nwhereas the strengths of these extreme risk spillover effects demonstrate\nsignificant asymmetries at the directional (downside versus upside) and\ntemporal (pre-outbreak versus post-outbreak) levels."}, "http://arxiv.org/abs/2310.17278": {"title": "Dynamic Factor Models: a Genealogy", "link": "http://arxiv.org/abs/2310.17278", "description": "Dynamic factor models have been developed out of the need of analyzing and\nforecasting time series in increasingly high dimensions. While mathematical\nstatisticians faced with inference problems in high-dimensional observation\nspaces were focusing on the so-called spiked-model-asymptotics, econometricians\nadopted an entirely and considerably more effective asymptotic approach, rooted\nin the factor models originally considered in psychometrics. The so-called\ndynamic factor model methods, in two decades, has grown into a wide and\nsuccessful body of techniques that are widely used in central banks, financial\ninstitutions, economic and statistical institutes. The objective of this\nchapter is not an extensive survey of the topic but a sketch of its historical\ngrowth, with emphasis on the various assumptions and interpretations, and a\nfamily tree of its main variants."}, "http://arxiv.org/abs/2310.17473": {"title": "Bayesian SAR model with stochastic volatility and multiple time-varying weights", "link": "http://arxiv.org/abs/2310.17473", "description": "A novel spatial autoregressive model for panel data is introduced, which\nincorporates multilayer networks and accounts for time-varying relationships.\nMoreover, the proposed approach allows the structural variance to evolve\nsmoothly over time and enables the analysis of shock propagation in terms of\ntime-varying spillover effects. The framework is applied to analyse the\ndynamics of international relationships among the G7 economies and their impact\non stock market returns and volatilities. The findings underscore the\nsubstantial impact of cooperative interactions and highlight discernible\ndisparities in network exposure across G7 nations, along with nuanced patterns\nin direct and indirect spillover effects."}, "http://arxiv.org/abs/2310.17496": {"title": "Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach", "link": "http://arxiv.org/abs/2310.17496", "description": "In modern recommendation systems, the standard pipeline involves training\nmachine learning models on historical data to predict user behaviors and\nimprove recommendations continuously. However, these data training loops can\nintroduce interference in A/B tests, where data generated by control and\ntreatment algorithms, potentially with different distributions, are combined.\nTo address these challenges, we introduce a novel approach called weighted\ntraining. This approach entails training a model to predict the probability of\neach data point appearing in either the treatment or control data and\nsubsequently applying weighted losses during model training. We demonstrate\nthat this approach achieves the least variance among all estimators without\ncausing shifts in the training distributions. Through simulation studies, we\ndemonstrate the lower bias and variance of our approach compared to other\nmethods."}, "http://arxiv.org/abs/2310.17571": {"title": "Inside the black box: Neural network-based real-time prediction of US recessions", "link": "http://arxiv.org/abs/2310.17571", "description": "Feedforward neural network (FFN) and two specific types of recurrent neural\nnetwork, long short-term memory (LSTM) and gated recurrent unit (GRU), are used\nfor modeling US recessions in the period from 1967 to 2021. The estimated\nmodels are then employed to conduct real-time predictions of the Great\nRecession and the Covid-19 recession in US. Their predictive performances are\ncompared to those of the traditional linear models, the logistic regression\nmodel both with and without the ridge penalty. The out-of-sample performance\nsuggests the application of LSTM and GRU in the area of recession forecasting,\nespecially for the long-term forecasting tasks. They outperform other types of\nmodels across 5 forecasting horizons with respect to different types of\nstatistical performance metrics. Shapley additive explanations (SHAP) method is\napplied to the fitted GRUs across different forecasting horizons to gain\ninsight into the feature importance. The evaluation of predictor importance\ndiffers between the GRU and ridge logistic regression models, as reflected in\nthe variable order determined by SHAP values. When considering the top 5\npredictors, key indicators such as the S\\&amp;P 500 index, real GDP, and private\nresidential fixed investment consistently appear for short-term forecasts (up\nto 3 months). In contrast, for longer-term predictions (6 months or more), the\nterm spread and producer price index become more prominent. These findings are\nsupported by both local interpretable model-agnostic explanations (LIME) and\nmarginal effects."}, "http://arxiv.org/abs/2205.07836": {"title": "2SLS with Multiple Treatments", "link": "http://arxiv.org/abs/2205.07836", "description": "We study what two-stage least squares (2SLS) identifies in models with\nmultiple treatments under treatment effect heterogeneity. Two conditions are\nshown to be necessary and sufficient for the 2SLS to identify positively\nweighted sums of agent-specific effects of each treatment: average conditional\nmonotonicity and no cross effects. Our identification analysis allows for any\nnumber of treatments, any number of continuous or discrete instruments, and the\ninclusion of covariates. We provide testable implications and present\ncharacterizations of choice behavior implied by our identification conditions\nand discuss how the conditions can be tested empirically."}, "http://arxiv.org/abs/2308.12485": {"title": "Optimal Shrinkage Estimation of Fixed Effects in Linear Panel Data Models", "link": "http://arxiv.org/abs/2308.12485", "description": "Shrinkage methods are frequently used to estimate fixed effects to reduce the\nnoisiness of the least squares estimators. However, widely used shrinkage\nestimators guarantee such noise reduction only under strong distributional\nassumptions. I develop an estimator for the fixed effects that obtains the best\npossible mean squared error within a class of shrinkage estimators. This class\nincludes conventional shrinkage estimators and the optimality does not require\ndistributional assumptions. The estimator has an intuitive form and is easy to\nimplement. Moreover, the fixed effects are allowed to vary with time and to be\nserially correlated, and the shrinkage optimally incorporates the underlying\ncorrelation structure in this case. In such a context, I also provide a method\nto forecast fixed effects one period ahead."}, "http://arxiv.org/abs/2310.18504": {"title": "Nonparametric Doubly Robust Identification of Causal Effects of a Continuous Treatment using Discrete Instruments", "link": "http://arxiv.org/abs/2310.18504", "description": "Many empirical applications estimate causal effects of a continuous\nendogenous variable (treatment) using a binary instrument. Estimation is\ntypically done through linear 2SLS. This approach requires a mean treatment\nchange and causal interpretation requires the LATE-type monotonicity in the\nfirst stage. An alternative approach is to explore distributional changes in\nthe treatment, where the first-stage restriction is treatment rank similarity.\nWe propose causal estimands that are doubly robust in that they are valid under\neither of these two restrictions. We apply the doubly robust estimation to\nestimate the impacts of sleep on well-being. Our results corroborate the usual\n2SLS estimates."}, "http://arxiv.org/abs/2310.18563": {"title": "Covariate Balancing and the Equivalence of Weighting and Doubly Robust Estimators of Average Treatment Effects", "link": "http://arxiv.org/abs/2310.18563", "description": "We show that when the propensity score is estimated using a suitable\ncovariate balancing procedure, the commonly used inverse probability weighting\n(IPW) estimator, augmented inverse probability weighting (AIPW) with linear\nconditional mean, and inverse probability weighted regression adjustment\n(IPWRA) with linear conditional mean are all numerically the same for\nestimating the average treatment effect (ATE) or the average treatment effect\non the treated (ATT). Further, suitably chosen covariate balancing weights are\nautomatically normalized, which means that normalized and unnormalized versions\nof IPW and AIPW are identical. For estimating the ATE, the weights that achieve\nthe algebraic equivalence of IPW, AIPW, and IPWRA are based on propensity\nscores estimated using the inverse probability tilting (IPT) method of Graham,\nPinto and Egel (2012). For the ATT, the weights are obtained using the\ncovariate balancing propensity score (CBPS) method developed in Imai and\nRatkovic (2014). These equivalences also make covariate balancing methods\nattractive when the treatment is confounded and one is interested in the local\naverage treatment effect."}, "http://arxiv.org/abs/2310.18836": {"title": "Design of Cluster-Randomized Trials with Cross-Cluster Interference", "link": "http://arxiv.org/abs/2310.18836", "description": "Cluster-randomized trials often involve units that are irregularly\ndistributed in space without well-separated communities. In these settings,\ncluster construction is a critical aspect of the design due to the potential\nfor cross-cluster interference. The existing literature relies on partial\ninterference models, which take clusters as given and assume no cross-cluster\ninterference. We relax this assumption by allowing interference to decay with\ngeographic distance between units. This induces a bias-variance trade-off:\nconstructing fewer, larger clusters reduces bias due to interference but\nincreases variance. We propose new estimators that exclude units most\npotentially impacted by cross-cluster interference and show that this\nsubstantially reduces asymptotic bias relative to conventional\ndifference-in-means estimators. We then study the design of clusters to\noptimize the estimators' rates of convergence. We provide formal justification\nfor a new design that chooses the number of clusters to balance the asymptotic\nbias and variance of our estimators and uses unsupervised learning to automate\ncluster construction."}, "http://arxiv.org/abs/2310.19200": {"title": "Popularity, face and voice: Predicting and interpreting livestreamers' retail performance using machine learning techniques", "link": "http://arxiv.org/abs/2310.19200", "description": "Livestreaming commerce, a hybrid of e-commerce and self-media, has expanded\nthe broad spectrum of traditional sales performance determinants. To\ninvestigate the factors that contribute to the success of livestreaming\ncommerce, we construct a longitudinal firm-level database with 19,175\nobservations, covering an entire livestreaming subsector. By comparing the\nforecasting accuracy of eight machine learning models, we identify a random\nforest model that provides the best prediction of gross merchandise volume\n(GMV). Furthermore, we utilize explainable artificial intelligence to open the\nblack-box of machine learning model, discovering four new facts: 1) variables\nrepresenting the popularity of livestreaming events are crucial features in\npredicting GMV. And voice attributes are more important than appearance; 2)\npopularity is a major determinant of sales for female hosts, while vocal\naesthetics is more decisive for their male counterparts; 3) merits and\ndrawbacks of the voice are not equally valued in the livestreaming market; 4)\nbased on changes of comments, page views and likes, sales growth can be divided\ninto three stages. Finally, we innovatively propose a 3D-SHAP diagram that\ndemonstrates the relationship between predicting feature importance, target\nvariable, and its predictors. This diagram identifies bottlenecks for both\nbeginner and top livestreamers, providing insights into ways to optimize their\nsales performance."}, "http://arxiv.org/abs/2310.19543": {"title": "Spectral identification and estimation of mixed causal-noncausal invertible-noninvertible models", "link": "http://arxiv.org/abs/2310.19543", "description": "This paper introduces new techniques for estimating, identifying and\nsimulating mixed causal-noncausal invertible-noninvertible models. We propose a\nframework that integrates high-order cumulants, merging both the spectrum and\nbispectrum into a single estimation function. The model that most adequately\nrepresents the data under the assumption that the error term is i.i.d. is\nselected. Our Monte Carlo study reveals unbiased parameter estimates and a high\nfrequency with which correct models are identified. We illustrate our strategy\nthrough an empirical analysis of returns from 24 Fama-French emerging market\nstock portfolios. The findings suggest that each portfolio displays noncausal\ndynamics, producing white noise residuals devoid of conditional heteroscedastic\neffects."}, "http://arxiv.org/abs/2310.19557": {"title": "A Bayesian Markov-switching SAR model for time-varying cross-price spillovers", "link": "http://arxiv.org/abs/2310.19557", "description": "The spatial autoregressive (SAR) model is extended by introducing a Markov\nswitching dynamics for the weight matrix and spatial autoregressive parameter.\nThe framework enables the identification of regime-specific connectivity\npatterns and strengths and the study of the spatiotemporal propagation of\nshocks in a system with a time-varying spatial multiplier matrix. The proposed\nmodel is applied to disaggregated CPI data from 15 EU countries to examine\ncross-price dependencies. The analysis identifies distinct connectivity\nstructures and spatial weights across the states, which capture shifts in\nconsumer behaviour, with marked cross-country differences in the spillover from\none price category to another."}, "http://arxiv.org/abs/2310.19747": {"title": "Characteristics of price related fluctuations in Non-Fungible Token (NFT) market", "link": "http://arxiv.org/abs/2310.19747", "description": "Non-fungible token (NFT) market is a new trading invention based on the\nblockchain technology which parallels the cryptocurrency market. In the present\nwork we study capitalization, floor price, the number of transactions, the\ninter-transaction times, and the transaction volume value of a few selected\npopular token collections. The results show that the fluctuations of all these\nquantities are characterized by heavy-tailed probability distribution\nfunctions, in most cases well described by the stretched exponentials, with a\ntrace of power-law scaling at times, long-range memory, and in several cases\neven the fractal organization of fluctuations, mostly restricted to the larger\nfluctuations, however. We conclude that the NFT market - even though young and\ngoverned by a somewhat different mechanisms of trading - shares several\nstatistical properties with the regular financial markets. However, some\ndifferences are visible in the specific quantitative indicators."}, "http://arxiv.org/abs/2310.19788": {"title": "Locally Optimal Best Arm Identification with a Fixed Budget", "link": "http://arxiv.org/abs/2310.19788", "description": "This study investigates the problem of identifying the best treatment arm, a\ntreatment arm with the highest expected outcome. We aim to identify the best\ntreatment arm with a lower probability of misidentification, which has been\nexplored under various names across numerous research fields, including\n\\emph{best arm identification} (BAI) and ordinal optimization. In our\nexperiments, the number of treatment-allocation rounds is fixed. In each round,\na decision-maker allocates a treatment arm to an experimental unit and observes\na corresponding outcome, which follows a Gaussian distribution with a variance\ndifferent among treatment arms. At the end of the experiment, we recommend one\nof the treatment arms as an estimate of the best treatment arm based on the\nobservations. The objective of the decision-maker is to design an experiment\nthat minimizes the probability of misidentifying the best treatment arm. With\nthis objective in mind, we develop lower bounds for the probability of\nmisidentification under the small-gap regime, where the gaps of the expected\noutcomes between the best and suboptimal treatment arms approach zero. Then,\nassuming that the variances are known, we design the\nGeneralized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, which is\nan extension of the Neyman allocation proposed by Neyman (1934) and the\nUniform-EBA strategy proposed by Bubeck et al. (2011). For the GNA-EBA\nstrategy, we show that the strategy is asymptotically optimal because its\nprobability of misidentification aligns with the lower bounds as the sample\nsize approaches infinity under the small-gap regime. We refer to such optimal\nstrategies as locally asymptotic optimal because their performance aligns with\nthe lower bounds within restricted situations characterized by the small-gap\nregime."}, "http://arxiv.org/abs/2009.00553": {"title": "A Vector Monotonicity Assumption for Multiple Instruments", "link": "http://arxiv.org/abs/2009.00553", "description": "When a researcher combines multiple instrumental variables for a single\nbinary treatment, the monotonicity assumption of the local average treatment\neffects (LATE) framework can become restrictive: it requires that all units\nshare a common direction of response even when separate instruments are shifted\nin opposing directions. What I call vector monotonicity, by contrast, simply\nassumes treatment uptake to be monotonic in all instruments, representing a\nspecial case of the partial monotonicity assumption introduced by Mogstad et\nal. (2021). I characterize the class of causal parameters that are point\nidentified under vector monotonicity, when the instruments are binary. This\nclass includes, for example, the average treatment effect among units that are\nin any way responsive to the collection of instruments, or those that are\nresponsive to a given subset of them. The identification results are\nconstructive and yield a simple estimator for the identified treatment effect\nparameters. An empirical application revisits the labor market returns to\ncollege."}, "http://arxiv.org/abs/2109.08109": {"title": "Standard Errors for Calibrated Parameters", "link": "http://arxiv.org/abs/2109.08109", "description": "Calibration, the practice of choosing the parameters of a structural model to\nmatch certain empirical moments, can be viewed as minimum distance estimation.\nExisting standard error formulas for such estimators require a consistent\nestimate of the correlation structure of the empirical moments, which is often\nunavailable in practice. Instead, the variances of the individual empirical\nmoments are usually readily estimable. Using only these variances, we derive\nconservative standard errors and confidence intervals for the structural\nparameters that are valid even under the worst-case correlation structure. In\nthe over-identified case, we show that the moment weighting scheme that\nminimizes the worst-case estimator variance amounts to a moment selection\nproblem with a simple solution. Finally, we develop tests of over-identifying\nor parameter restrictions. We apply our methods empirically to a model of menu\ncost pricing for multi-product firms and to a heterogeneous agent New Keynesian\nmodel."}, "http://arxiv.org/abs/2211.16714": {"title": "Incorporating Prior Knowledge of Latent Group Structure in Panel Data Models", "link": "http://arxiv.org/abs/2211.16714", "description": "The assumption of group heterogeneity has become popular in panel data\nmodels. We develop a constrained Bayesian grouped estimator that exploits\nresearchers' prior beliefs on groups in a form of pairwise constraints,\nindicating whether a pair of units is likely to belong to a same group or\ndifferent groups. We propose a prior to incorporate the pairwise constraints\nwith varying degrees of confidence. The whole framework is built on the\nnonparametric Bayesian method, which implicitly specifies a distribution over\nthe group partitions, and so the posterior analysis takes the uncertainty of\nthe latent group structure into account. Monte Carlo experiments reveal that\nadding prior knowledge yields more accurate estimates of coefficient and scores\npredictive gains over alternative estimators. We apply our method to two\nempirical applications. In a first application to forecasting U.S. CPI\ninflation, we illustrate that prior knowledge of groups improves density\nforecasts when the data is not entirely informative. A second application\nrevisits the relationship between a country's income and its democratic\ntransition; we identify heterogeneous income effects on democracy with five\ndistinct groups over ninety countries."}, "http://arxiv.org/abs/2307.01357": {"title": "Adaptive Principal Component Regression with Applications to Panel Data", "link": "http://arxiv.org/abs/2307.01357", "description": "Principal component regression (PCR) is a popular technique for fixed-design\nerror-in-variables regression, a generalization of the linear regression\nsetting in which the observed covariates are corrupted with random noise. We\nprovide the first time-uniform finite sample guarantees for online\n(regularized) PCR whenever data is collected adaptively. Since the proof\ntechniques for analyzing PCR in the fixed design setting do not readily extend\nto the online setting, our results rely on adapting tools from modern\nmartingale concentration to the error-in-variables setting. As an application\nof our bounds, we provide a framework for experiment design in panel data\nsettings when interventions are assigned adaptively. Our framework may be\nthought of as a generalization of the synthetic control and synthetic\ninterventions frameworks, where data is collected via an adaptive intervention\nassignment policy."}, "http://arxiv.org/abs/2309.06693": {"title": "Stochastic Learning of Semiparametric Monotone Index Models with Large Sample Size", "link": "http://arxiv.org/abs/2309.06693", "description": "I study the estimation of semiparametric monotone index models in the\nscenario where the number of observation points $n$ is extremely large and\nconventional approaches fail to work due to heavy computational burdens.\nMotivated by the mini-batch gradient descent algorithm (MBGD) that is widely\nused as a stochastic optimization tool in the machine learning field, I\nproposes a novel subsample- and iteration-based estimation procedure. In\nparticular, starting from any initial guess of the true parameter, I\nprogressively update the parameter using a sequence of subsamples randomly\ndrawn from the data set whose sample size is much smaller than $n$. The update\nis based on the gradient of some well-chosen loss function, where the\nnonparametric component is replaced with its Nadaraya-Watson kernel estimator\nbased on subsamples. My proposed algorithm essentially generalizes MBGD\nalgorithm to the semiparametric setup. Compared with full-sample-based method,\nthe new method reduces the computational time by roughly $n$ times if the\nsubsample size and the kernel function are chosen properly, so can be easily\napplied when the sample size $n$ is large. Moreover, I show that if I further\nconduct averages across the estimators produced during iterations, the\ndifference between the average estimator and full-sample-based estimator will\nbe $1/\\sqrt{n}$-trivial. Consequently, the average estimator is\n$1/\\sqrt{n}$-consistent and asymptotically normally distributed. In other\nwords, the new estimator substantially improves the computational speed, while\nat the same time maintains the estimation accuracy."}, "http://arxiv.org/abs/2310.16945": {"title": "Causal Q-Aggregation for CATE Model Selection", "link": "http://arxiv.org/abs/2310.16945", "description": "Accurate estimation of conditional average treatment effects (CATE) is at the\ncore of personalized decision making. While there is a plethora of models for\nCATE estimation, model selection is a nontrivial task, due to the fundamental\nproblem of causal inference. Recent empirical work provides evidence in favor\nof proxy loss metrics with double robust properties and in favor of model\nensembling. However, theoretical understanding is lacking. Direct application\nof prior theoretical work leads to suboptimal oracle model selection rates due\nto the non-convexity of the model selection problem. We provide regret rates\nfor the major existing CATE ensembling approaches and propose a new CATE model\nensembling approach based on Q-aggregation using the doubly robust loss. Our\nmain result shows that causal Q-aggregation achieves statistically optimal\noracle model selection regret rates of $\\frac{\\log(M)}{n}$ (with $M$ models and\n$n$ samples), with the addition of higher-order estimation error terms related\nto products of errors in the nuisance functions. Crucially, our regret rate\ndoes not require that any of the candidate CATE models be close to the truth.\nWe validate our new method on many semi-synthetic datasets and also provide\nextensions of our work to CATE model selection with instrumental variables and\nunobserved confounding."}, "http://arxiv.org/abs/2310.19992": {"title": "Robust Estimation of Realized Correlation: New Insight about Intraday Fluctuations in Market Betas", "link": "http://arxiv.org/abs/2310.19992", "description": "Time-varying volatility is an inherent feature of most economic time-series,\nwhich causes standard correlation estimators to be inconsistent. The quadrant\ncorrelation estimator is consistent but very inefficient. We propose a novel\nsubsampled quadrant estimator that improves efficiency while preserving\nconsistency and robustness. This estimator is particularly well-suited for\nhigh-frequency financial data and we apply it to a large panel of US stocks.\nOur empirical analysis sheds new light on intra-day fluctuations in market\nbetas by decomposing them into time-varying correlations and relative\nvolatility changes. Our results show that intraday variation in betas is\nprimarily driven by intraday variation in correlations."}, "http://arxiv.org/abs/2006.07691": {"title": "Synthetic Interventions", "link": "http://arxiv.org/abs/2006.07691", "description": "Consider a setting with $N$ heterogeneous units (e.g., individuals,\nsub-populations) and $D$ interventions (e.g., socio-economic policies). Our\ngoal is to learn the expected potential outcome associated with every\nintervention on every unit, totaling $N \\times D$ causal parameters. Towards\nthis, we present a causal framework, synthetic interventions (SI), to infer\nthese $N \\times D$ causal parameters while only observing each of the $N$ units\nunder at most two interventions, independent of $D$. This can be significant as\nthe number of interventions, i.e., level of personalization, grows. Under a\nnovel tensor factor model across units, outcomes, and interventions, we prove\nan identification result for each of these $N \\times D$ causal parameters,\nestablish finite-sample consistency of our estimator along with asymptotic\nnormality under additional conditions. Importantly, our estimator also allows\nfor latent confounders that determine how interventions are assigned. The\nestimator is further furnished with data-driven tests to examine its\nsuitability. Empirically, we validate our framework through a large-scale A/B\ntest performed on an e-commerce platform. We believe our results could have\nimplications for the design of data-efficient randomized experiments (e.g.,\nrandomized control trials) with heterogeneous units and multiple interventions."}, "http://arxiv.org/abs/2207.04481": {"title": "Detecting Grouped Local Average Treatment Effects and Selecting True Instruments", "link": "http://arxiv.org/abs/2207.04481", "description": "Under an endogenous binary treatment with heterogeneous effects and multiple\ninstruments, we propose a two-step procedure for identifying complier groups\nwith identical local average treatment effects (LATE) despite relying on\ndistinct instruments, even if several instruments violate the identifying\nassumptions. We use the fact that the LATE is homogeneous for instruments which\n(i) satisfy the LATE assumptions (instrument validity and treatment\nmonotonicity in the instrument) and (ii) generate identical complier groups in\nterms of treatment propensities given the respective instruments. We propose a\ntwo-step procedure, where we first cluster the propensity scores in the first\nstep and find groups of IVs with the same reduced form parameters in the second\nstep. Under the plurality assumption that within each set of instruments with\nidentical treatment propensities, instruments truly satisfying the LATE\nassumptions are the largest group, our procedure permits identifying these true\ninstruments in a data driven way. We show that our procedure is consistent and\nprovides consistent and asymptotically normal estimators of underlying LATEs.\nWe also provide a simulation study investigating the finite sample properties\nof our approach and an empirical application investigating the effect of\nincarceration on recidivism in the US with judge assignments serving as\ninstruments."}, "http://arxiv.org/abs/2304.09078": {"title": "Club coefficients in the UEFA Champions League: Time for shift to an Elo-based formula", "link": "http://arxiv.org/abs/2304.09078", "description": "One of the most popular club football tournaments, the UEFA Champions League,\nwill see a fundamental reform from the 2024/25 season: the traditional group\nstage will be replaced by one league where each of the 36 teams plays eight\nmatches. To guarantee that the opponents of the clubs are of the same strength\nin the new design, it is crucial to forecast the performance of the teams\nbefore the tournament as well as possible. This paper investigates whether the\ncurrently used rating of the teams, the UEFA club coefficient, can be improved\nby taking the games played in the national leagues into account. According to\nour logistic regression models, a variant of the Elo method provides a higher\naccuracy in terms of explanatory power in the Champions League matches. The\nUnion of European Football Associations (UEFA) is encouraged to follow the\nexample of the FIFA World Ranking and reform the calculation of the club\ncoefficients in order to avoid unbalanced schedules in the novel tournament\nformat of the Champions League."}, "http://arxiv.org/abs/2308.13564": {"title": "SGMM: Stochastic Approximation to Generalized Method of Moments", "link": "http://arxiv.org/abs/2308.13564", "description": "We introduce a new class of algorithms, Stochastic Generalized Method of\nMoments (SGMM), for estimation and inference on (overidentified) moment\nrestriction models. Our SGMM is a novel stochastic approximation alternative to\nthe popular Hansen (1982) (offline) GMM, and offers fast and scalable\nimplementation with the ability to handle streaming datasets in real time. We\nestablish the almost sure convergence, and the (functional) central limit\ntheorem for the inefficient online 2SLS and the efficient SGMM. Moreover, we\npropose online versions of the Durbin-Wu-Hausman and Sargan-Hansen tests that\ncan be seamlessly integrated within the SGMM framework. Extensive Monte Carlo\nsimulations show that as the sample size increases, the SGMM matches the\nstandard (offline) GMM in terms of estimation accuracy and gains over\ncomputational efficiency, indicating its practical value for both large-scale\nand online datasets. We demonstrate the efficacy of our approach by a proof of\nconcept using two well known empirical examples with large sample sizes."}, "http://arxiv.org/abs/2311.00013": {"title": "Semiparametric Discrete Choice Models for Bundles", "link": "http://arxiv.org/abs/2311.00013", "description": "We propose two approaches to estimate semiparametric discrete choice models\nfor bundles. Our first approach is a kernel-weighted rank estimator based on a\nmatching-based identification strategy. We establish its complete asymptotic\nproperties and prove the validity of the nonparametric bootstrap for inference.\nWe then introduce a new multi-index least absolute deviations (LAD) estimator\nas an alternative, of which the main advantage is its capacity to estimate\npreference parameters on both alternative- and agent-specific regressors. Both\nmethods can account for arbitrary correlation in disturbances across choices,\nwith the former also allowing for interpersonal heteroskedasticity. We also\ndemonstrate that the identification strategy underlying these procedures can be\nextended naturally to panel data settings, producing an analogous localized\nmaximum score estimator and a LAD estimator for estimating bundle choice models\nwith fixed effects. We derive the limiting distribution of the former and\nverify the validity of the numerical bootstrap as an inference tool. All our\nproposed methods can be applied to general multi-index models. Monte Carlo\nexperiments show that they perform well in finite samples."}, "http://arxiv.org/abs/2311.00439": {"title": "Bounds on Treatment Effects under Stochastic Monotonicity Assumption in Sample Selection Models", "link": "http://arxiv.org/abs/2311.00439", "description": "This paper discusses the partial identification of treatment effects in\nsample selection models when the exclusion restriction fails and the\nmonotonicity assumption in the selection effect does not hold exactly, both of\nwhich are key challenges in applying the existing methodologies. Our approach\nbuilds on Lee's (2009) procedure, who considers partial identification under\nthe monotonicity assumption, but we assume only a stochastic (and weaker)\nversion of monotonicity, which depends on a prespecified parameter $\\vartheta$\nthat represents researchers' belief in the plausibility of the monotonicity.\nUnder this assumption, we show that we can still obtain useful bounds even when\nthe monotonic behavioral model does not strictly hold. Our procedure is useful\nwhen empirical researchers anticipate that a small fraction of the population\nwill not behave monotonically in selection; it can also be an effective tool\nfor performing sensitivity analysis or examining the identification power of\nthe monotonicity assumption. Our procedure is easily extendable to other\nrelated settings; we also provide the identification result of the marginal\ntreatment effects setting as an important application. Moreover, we show that\nthe bounds can still be obtained even in the absence of the knowledge of\n$\\vartheta$ under the semiparametric models that nest the classical probit and\nlogit selection models."}, "http://arxiv.org/abs/2311.00577": {"title": "Personalized Assignment to One of Many Treatment Arms via Regularized and Clustered Joint Assignment Forests", "link": "http://arxiv.org/abs/2311.00577", "description": "We consider learning personalized assignments to one of many treatment arms\nfrom a randomized controlled trial. Standard methods that estimate\nheterogeneous treatment effects separately for each arm may perform poorly in\nthis case due to excess variance. We instead propose methods that pool\ninformation across treatment arms: First, we consider a regularized\nforest-based assignment algorithm based on greedy recursive partitioning that\nshrinks effect estimates across arms. Second, we augment our algorithm by a\nclustering scheme that combines treatment arms with consistently similar\noutcomes. In a simulation study, we compare the performance of these approaches\nto predicting arm-wise outcomes separately, and document gains of directly\noptimizing the treatment assignment with regularization and clustering. In a\ntheoretical model, we illustrate how a high number of treatment arms makes\nfinding the best arm hard, while we can achieve sizable utility gains from\npersonalization by regularized optimization."}, "http://arxiv.org/abs/2311.00662": {"title": "On Gaussian Process Priors in Conditional Moment Restriction Models", "link": "http://arxiv.org/abs/2311.00662", "description": "This paper studies quasi-Bayesian estimation and uncertainty quantification\nfor an unknown function that is identified by a nonparametric conditional\nmoment restriction model. We derive contraction rates for a class of Gaussian\nprocess priors and provide conditions under which a Bernstein-von Mises theorem\nholds for the quasi-posterior distribution. As a consequence, we show that\noptimally-weighted quasi-Bayes credible sets have exact asymptotic frequentist\ncoverage. This extends classical result on the frequentist validity of\noptimally weighted quasi-Bayes credible sets for parametric generalized method\nof moments (GMM) models."}, "http://arxiv.org/abs/2209.14502": {"title": "Fast Inference for Quantile Regression with Tens of Millions of Observations", "link": "http://arxiv.org/abs/2209.14502", "description": "Big data analytics has opened new avenues in economic research, but the\nchallenge of analyzing datasets with tens of millions of observations is\nsubstantial. Conventional econometric methods based on extreme estimators\nrequire large amounts of computing resources and memory, which are often not\nreadily available. In this paper, we focus on linear quantile regression\napplied to \"ultra-large\" datasets, such as U.S. decennial censuses. A fast\ninference framework is presented, utilizing stochastic subgradient descent\n(S-subGD) updates. The inference procedure handles cross-sectional data\nsequentially: (i) updating the parameter estimate with each incoming \"new\nobservation\", (ii) aggregating it as a $\\textit{Polyak-Ruppert}$ average, and\n(iii) computing a pivotal statistic for inference using only a solution path.\nThe methodology draws from time-series regression to create an asymptotically\npivotal statistic through random scaling. Our proposed test statistic is\ncalculated in a fully online fashion and critical values are calculated without\nresampling. We conduct extensive numerical studies to showcase the\ncomputational merits of our proposed inference. For inference problems as large\nas $(n, d) \\sim (10^7, 10^3)$, where $n$ is the sample size and $d$ is the\nnumber of regressors, our method generates new insights, surpassing current\ninference methods in computation. Our method specifically reveals trends in the\ngender gap in the U.S. college wage premium using millions of observations,\nwhile controlling over $10^3$ covariates to mitigate confounding effects."}, "http://arxiv.org/abs/2311.00905": {"title": "Data-Driven Fixed-Point Tuning for Truncated Realized Variations", "link": "http://arxiv.org/abs/2311.00905", "description": "Many methods for estimating integrated volatility and related functionals of\nsemimartingales in the presence of jumps require specification of tuning\nparameters for their use. In much of the available theory, tuning parameters\nare assumed to be deterministic, and their values are specified only up to\nasymptotic constraints. However, in empirical work and in simulation studies,\nthey are typically chosen to be random and data-dependent, with explicit\nchoices in practice relying on heuristics alone. In this paper, we consider\nnovel data-driven tuning procedures for the truncated realized variations of a\nsemimartingale with jumps, which are based on a type of stochastic fixed-point\niteration. Being effectively automated, our approach alleviates the need for\ndelicate decision-making regarding tuning parameters, and can be implemented\nusing information regarding sampling frequency alone. We show our methods can\nlead to asymptotically efficient estimation of integrated volatility and\nexhibit superior finite-sample performance compared to popular alternatives in\nthe literature."}, "http://arxiv.org/abs/2311.01217": {"title": "The learning effects of subsidies to bundled goods: a semiparametric approach", "link": "http://arxiv.org/abs/2311.01217", "description": "Can temporary subsidies to bundles induce long-run changes in demand due to\nlearning about the relative quality of one of its constituent goods? This paper\nprovides theoretical and experimental evidence on the role of this mechanism.\nTheoretically, we introduce a model where an agent learns about the quality of\nan innovation on an essential good through consumption. Our results show that\nthe contemporaneous effect of a one-off subsidy to a bundle that contains the\ninnovation may be decomposed into a direct price effect, and an indirect\nlearning motive, whereby an agent leverages the discount to increase the\ninformational bequest left to her future selves. We then assess the predictions\nof our theory in a randomised experiment in a ridesharing platform. The\nexperiment provided two-week discounts for car trips integrating with a train\nor metro station (a bundle). Given the heavy-tailed nature of our data, we\nfollow \\cite{Athey2023} and, motivated by our theory, propose a semiparametric\nmodel for treatment effects that enables the construction of more efficient\nestimators. We introduce a statistically efficient estimator for our model by\nrelying on L-moments, a robust alternative to standard moments. Our estimator\nimmediately yields a specification test for the semiparametric model; moreover,\nin our adopted parametrisation, it can be easily computed through generalized\nleast squares. Our empirical results indicate that a two-week 50\\% discount on\ncar trips integrating with train/metro leads to a contemporaneous increase in\nthe demand for integrated rides, and, consistent with our learning model,\npersistent changes in the mean and dispersion of nonintegrated rides. These\neffects persist for over four months after the discount. A simple calibration\nof our model shows that around 40\\% to 50\\% of the estimated contemporaneous\nincrease in integrated rides may be attributed to a learning motive."}, "http://arxiv.org/abs/2110.10650": {"title": "Attention Overload", "link": "http://arxiv.org/abs/2110.10650", "description": "We introduce an Attention Overload Model that captures the idea that\nalternatives compete for the decision maker's attention, and hence the\nattention that each alternative receives decreases as the choice problem\nbecomes larger. We provide testable implications on the observed choice\nbehavior that can be used to (point or partially) identify the decision maker's\npreference and attention frequency. We then enhance our attention overload\nmodel to accommodate heterogeneous preferences based on the idea of List-based\nAttention Overload, where alternatives are presented to the decision makers as\na list that correlates with both heterogeneous preferences and random\nattention. We show that preference and attention frequencies are (point or\npartially) identifiable under nonparametric assumptions on the list and\nattention formation mechanisms, even when the true underlying list is unknown\nto the researcher. Building on our identification results, we develop\neconometric methods for estimation and inference."}, "http://arxiv.org/abs/2112.13398": {"title": "Long Story Short: Omitted Variable Bias in Causal Machine Learning", "link": "http://arxiv.org/abs/2112.13398", "description": "We derive general, yet simple, sharp bounds on the size of the omitted\nvariable bias for a broad class of causal parameters that can be identified as\nlinear functionals of the conditional expectation function of the outcome. Such\nfunctionals encompass many of the traditional targets of investigation in\ncausal inference studies, such as, for example, (weighted) average of potential\noutcomes, average treatment effects (including subgroup effects, such as the\neffect on the treated), (weighted) average derivatives, and policy effects from\nshifts in covariate distribution -- all for general, nonparametric causal\nmodels. Our construction relies on the Riesz-Frechet representation of the\ntarget functional. Specifically, we show how the bound on the bias depends only\non the additional variation that the latent variables create both in the\noutcome and in the Riesz representer for the parameter of interest. Moreover,\nin many important cases (e.g, average treatment effects and avearage\nderivatives) the bound is shown to depend on easily interpretable quantities\nthat measure the explanatory power of the omitted variables. Therefore, simple\nplausibility judgments on the maximum explanatory power of omitted variables\n(in explaining treatment and outcome variation) are sufficient to place overall\nbounds on the size of the bias. Furthermore, we use debiased machine learning\nto provide flexible and efficient statistical inference on learnable components\nof the bounds. Finally, empirical examples demonstrate the usefulness of the\napproach."}, "http://arxiv.org/abs/2112.03626": {"title": "Phase transitions in nonparametric regressions", "link": "http://arxiv.org/abs/2112.03626", "description": "When the unknown regression function of a single variable is known to have\nderivatives up to the $(\\gamma+1)$th order bounded in absolute values by a\ncommon constant everywhere or a.e. (i.e., $(\\gamma+1)$th degree of smoothness),\nthe minimax optimal rate of the mean integrated squared error (MISE) is stated\nas $\\left(\\frac{1}{n}\\right)^{\\frac{2\\gamma+2}{2\\gamma+3}}$ in the literature.\nThis paper shows that: (i) if $n\\leq\\left(\\gamma+1\\right)^{2\\gamma+3}$, the\nminimax optimal MISE rate is $\\frac{\\log n}{n\\log(\\log n)}$ and the optimal\ndegree of smoothness to exploit is roughly $\\max\\left\\{ \\left\\lfloor \\frac{\\log\nn}{2\\log\\left(\\log n\\right)}\\right\\rfloor ,\\,1\\right\\} $; (ii) if\n$n&gt;\\left(\\gamma+1\\right)^{2\\gamma+3}$, the minimax optimal MISE rate is\n$\\left(\\frac{1}{n}\\right)^{\\frac{2\\gamma+2}{2\\gamma+3}}$ and the optimal degree\nof smoothness to exploit is $\\gamma+1$. The fundamental contribution of this\npaper is a set of metric entropy bounds we develop for smooth function classes.\nSome of our bounds are original, and some of them improve and/or generalize the\nones in the literature (e.g., Kolmogorov and Tikhomirov, 1959). Our metric\nentropy bounds allow us to show phase transitions in the minimax optimal MISE\nrates associated with some commonly seen smoothness classes as well as\nnon-standard smoothness classes, and can also be of independent interest\noutside the nonparametric regression problems."}, "http://arxiv.org/abs/2206.04157": {"title": "Inference for Matched Tuples and Fully Blocked Factorial Designs", "link": "http://arxiv.org/abs/2206.04157", "description": "This paper studies inference in randomized controlled trials with multiple\ntreatments, where treatment status is determined according to a \"matched\ntuples\" design. Here, by a matched tuples design, we mean an experimental\ndesign where units are sampled i.i.d. from the population of interest, grouped\ninto \"homogeneous\" blocks with cardinality equal to the number of treatments,\nand finally, within each block, each treatment is assigned exactly once\nuniformly at random. We first study estimation and inference for matched tuples\ndesigns in the general setting where the parameter of interest is a vector of\nlinear contrasts over the collection of average potential outcomes for each\ntreatment. Parameters of this form include standard average treatment effects\nused to compare one treatment relative to another, but also include parameters\nwhich may be of interest in the analysis of factorial designs. We first\nestablish conditions under which a sample analogue estimator is asymptotically\nnormal and construct a consistent estimator of its corresponding asymptotic\nvariance. Combining these results establishes the asymptotic exactness of tests\nbased on these estimators. In contrast, we show that, for two common testing\nprocedures based on t-tests constructed from linear regressions, one test is\ngenerally conservative while the other generally invalid. We go on to apply our\nresults to study the asymptotic properties of what we call \"fully-blocked\" 2^K\nfactorial designs, which are simply matched tuples designs applied to a full\nfactorial experiment. Leveraging our previous results, we establish that our\nestimator achieves a lower asymptotic variance under the fully-blocked design\nthan that under any stratified factorial design which stratifies the\nexperimental sample into a finite number of \"large\" strata. A simulation study\nand empirical application illustrate the practical relevance of our results."}, "http://arxiv.org/abs/2303.02716": {"title": "Deterministic, quenched and annealed parameter estimation for heterogeneous network models", "link": "http://arxiv.org/abs/2303.02716", "description": "At least two, different approaches to define and solve statistical models for\nthe analysis of economic systems exist: the typical, econometric one,\ninterpreting the Gravity Model specification as the expected link weight of an\narbitrary probability distribution, and the one rooted into statistical\nphysics, constructing maximum-entropy distributions constrained to satisfy\ncertain network properties. In a couple of recent, companion papers they have\nbeen successfully integrated within the framework induced by the constrained\nminimisation of the Kullback-Leibler divergence: specifically, two, broad\nclasses of models have been devised, i.e. the integrated and the conditional\nones, defined by different, probabilistic rules to place links, load them with\nweights and turn them into proper, econometric prescriptions. Still, the\nrecipes adopted by the two approaches to estimate the parameters entering into\nthe definition of each model differ. In econometrics, a likelihood that\ndecouples the binary and weighted parts of a model, treating a network as\ndeterministic, is typically maximised; to restore its random character, two\nalternatives exist: either solving the likelihood maximisation on each\nconfiguration of the ensemble and taking the average of the parameters\nafterwards or taking the average of the likelihood function and maximising the\nlatter one. The difference between these approaches lies in the order in which\nthe operations of averaging and maximisation are taken - a difference that is\nreminiscent of the quenched and annealed ways of averaging out the disorder in\nspin glasses. The results of the present contribution, devoted to comparing\nthese recipes in the case of continuous, conditional network models, indicate\nthat the annealed estimation recipe represents the best alternative to the\ndeterministic one."}, "http://arxiv.org/abs/2307.01284": {"title": "Does regional variation in wage levels identify the effects of a national minimum wage?", "link": "http://arxiv.org/abs/2307.01284", "description": "This paper examines the identification assumptions underlying two types of\nestimators of the causal effects of minimum wages based on regional variation\nin wage levels: the \"effective minimum wage\" and the \"fraction affected/gap\"\ndesigns. For the effective minimum wage design, I show that the identification\nassumptions emphasized by Lee (1999) are crucial for unbiased estimation but\ndifficult to satisfy in empirical applications for reasons arising from\neconomic theory. For the fraction affected design at the region level, I show\nthat economic factors such as a common trend in the dispersion of worker\nproductivity or regional convergence in GDP per capita may lead to violations\nof the \"parallel trends\" identifying assumption. The paper suggests ways to\nincrease the likelihood of detecting those issues when implementing checks for\nparallel pre-trends. I also show that this design may be subject to biases\narising from the misspecification of the treatment intensity variable,\nespecially when the minimum wage strongly affects employment and wages."}, "http://arxiv.org/abs/2311.02196": {"title": "Pooled Bewley Estimator of Long Run Relationships in Dynamic Heterogenous Panels", "link": "http://arxiv.org/abs/2311.02196", "description": "Using a transformation of the autoregressive distributed lag model due to\nBewley, a novel pooled Bewley (PB) estimator of long-run coefficients for\ndynamic panels with heterogeneous short-run dynamics is proposed. The PB\nestimator is directly comparable to the widely used Pooled Mean Group (PMG)\nestimator, and is shown to be consistent and asymptotically normal. Monte Carlo\nsimulations show good small sample performance of PB compared to the existing\nestimators in the literature, namely PMG, panel dynamic OLS (PDOLS), and panel\nfully-modified OLS (FMOLS). Application of two bias-correction methods and a\nbootstrapping of critical values to conduct inference robust to cross-sectional\ndependence of errors are also considered. The utility of the PB estimator is\nillustrated in an empirical application to the aggregate consumption function."}, "http://arxiv.org/abs/2311.02299": {"title": "The Fragility of Sparsity", "link": "http://arxiv.org/abs/2311.02299", "description": "We show, using three empirical applications, that linear regression estimates\nwhich rely on the assumption of sparsity are fragile in two ways. First, we\ndocument that different choices of the regressor matrix that don't impact\nordinary least squares (OLS) estimates, such as the choice of baseline category\nwith categorical controls, can move sparsity-based estimates two standard\nerrors or more. Second, we develop two tests of the sparsity assumption based\non comparing sparsity-based estimators with OLS. The tests tend to reject the\nsparsity assumption in all three applications. Unless the number of regressors\nis comparable to or exceeds the sample size, OLS yields more robust results at\nlittle efficiency cost."}, "http://arxiv.org/abs/2311.02467": {"title": "Individualized Policy Evaluation and Learning under Clustered Network Interference", "link": "http://arxiv.org/abs/2311.02467", "description": "While there now exists a large literature on policy evaluation and learning,\nmuch of prior work assumes that the treatment assignment of one unit does not\naffect the outcome of another unit. Unfortunately, ignoring interference may\nlead to biased policy evaluation and yield ineffective learned policies. For\nexample, treating influential individuals who have many friends can generate\npositive spillover effects, thereby improving the overall performance of an\nindividualized treatment rule (ITR). We consider the problem of evaluating and\nlearning an optimal ITR under clustered network (or partial) interference where\nclusters of units are sampled from a population and units may influence one\nanother within each cluster. Under this model, we propose an estimator that can\nbe used to evaluate the empirical performance of an ITR. We show that this\nestimator is substantially more efficient than the standard inverse probability\nweighting estimator, which does not impose any assumption about spillover\neffects. We derive the finite-sample regret bound for a learned ITR, showing\nthat the use of our efficient evaluation estimator leads to the improved\nperformance of learned policies. Finally, we conduct simulation and empirical\nstudies to illustrate the advantages of the proposed methodology."}, "http://arxiv.org/abs/2311.02789": {"title": "Estimation of Semiparametric Multi-Index Models Using Deep Neural Networks", "link": "http://arxiv.org/abs/2311.02789", "description": "In this paper, we consider estimation and inference for both the multi-index\nparameters and the link function involved in a class of semiparametric\nmulti-index models via deep neural networks (DNNs). We contribute to the design\nof DNN by i) providing more transparency for practical implementation, ii)\ndefining different types of sparsity, iii) showing the differentiability, iv)\npointing out the set of effective parameters, and v) offering a new variant of\nrectified linear activation function (ReLU), etc. Asymptotic properties for the\njoint estimates of both the index parameters and the link functions are\nestablished, and a feasible procedure for the purpose of inference is also\nproposed. We conduct extensive numerical studies to examine the finite-sample\nperformance of the estimation methods, and we also evaluate the empirical\nrelevance and applicability of the proposed models and estimation methods to\nreal data."}, "http://arxiv.org/abs/2201.07880": {"title": "Deep self-consistent learning of local volatility", "link": "http://arxiv.org/abs/2201.07880", "description": "We present an algorithm for the calibration of local volatility from market\noption prices through deep self-consistent learning, by approximating both\nmarket option prices and local volatility using deep neural networks,\nrespectively. Our method uses the initial-boundary value problem of the\nunderlying Dupire's partial differential equation solved by the parameterized\noption prices to bring corrections to the parameterization in a self-consistent\nway. By exploiting the differentiability of the neural networks, we can\nevaluate Dupire's equation locally at each strike-maturity pair; while by\nexploiting their continuity, we sample strike-maturity pairs uniformly from a\ngiven domain, going beyond the discrete points where the options are quoted.\nMoreover, the absence of arbitrage opportunities are imposed by penalizing an\nassociated loss function as a soft constraint. For comparison with existing\napproaches, the proposed method is tested on both synthetic and market option\nprices, which shows an improved performance in terms of reduced interpolation\nand reprice errors, as well as the smoothness of the calibrated local\nvolatility. An ablation study has been performed, asserting the robustness and\nsignificance of the proposed method."}, "http://arxiv.org/abs/2204.12023": {"title": "A One-Covariate-at-a-Time Method for Nonparametric Additive Models", "link": "http://arxiv.org/abs/2204.12023", "description": "This paper proposes a one-covariate-at-a-time multiple testing (OCMT)\napproach to choose significant variables in high-dimensional nonparametric\nadditive regression models. Similarly to Chudik, Kapetanios and Pesaran (2018),\nwe consider the statistical significance of individual nonparametric additive\ncomponents one at a time and take into account the multiple testing nature of\nthe problem. One-stage and multiple-stage procedures are both considered. The\nformer works well in terms of the true positive rate only if the marginal\neffects of all signals are strong enough; the latter helps to pick up hidden\nsignals that have weak marginal effects. Simulations demonstrate the good\nfinite sample performance of the proposed procedures. As an empirical\napplication, we use the OCMT procedure on a dataset we extracted from the\nLongitudinal Survey on Rural Urban Migration in China. We find that our\nprocedure works well in terms of the out-of-sample forecast root mean square\nerrors, compared with competing methods."}, "http://arxiv.org/abs/2209.03259": {"title": "A Ridge-Regularised Jackknifed Anderson-Rubin Test", "link": "http://arxiv.org/abs/2209.03259", "description": "We consider hypothesis testing in instrumental variable regression models\nwith few included exogenous covariates but many instruments -- possibly more\nthan the number of observations. We show that a ridge-regularised version of\nthe jackknifed Anderson Rubin (1949, henceforth AR) test controls asymptotic\nsize in the presence of heteroskedasticity, and when the instruments may be\narbitrarily weak. Asymptotic size control is established under weaker\nassumptions than those imposed for recently proposed jackknifed AR tests in the\nliterature. Furthermore, ridge-regularisation extends the scope of jackknifed\nAR tests to situations in which there are more instruments than observations.\nMonte-Carlo simulations indicate that our method has favourable finite-sample\nsize and power properties compared to recently proposed alternative approaches\nin the literature. An empirical application on the elasticity of substitution\nbetween immigrants and natives in the US illustrates the usefulness of the\nproposed method for practitioners."}, "http://arxiv.org/abs/2212.09193": {"title": "Identification of time-varying counterfactual parameters in nonlinear panel models", "link": "http://arxiv.org/abs/2212.09193", "description": "We develop a general framework for the identification of counterfactual\nparameters in a class of nonlinear semiparametric panel models with fixed\neffects and time effects. Our method applies to models for discrete outcomes\n(e.g., two-way fixed effects binary choice) or continuous outcomes (e.g.,\ncensored regression), with discrete or continuous regressors. Our results do\nnot require parametric assumptions on the error terms or time-homogeneity on\nthe outcome equation. Our main results focus on static models, with a set of\nresults applying to models without any exogeneity conditions. We show that the\nsurvival distribution of counterfactual outcomes is identified (point or\npartial) in this class of models. This parameter is a building block for most\npartial and marginal effects of interest in applied practice that are based on\nthe average structural function as defined by Blundell and Powell (2003, 2004).\nTo the best of our knowledge, ours are the first results on average partial and\nmarginal effects for binary choice and ordered choice models with two-way fixed\neffects and non-logistic errors."}, "http://arxiv.org/abs/2306.04135": {"title": "Semiparametric Discrete Choice Models for Bundles", "link": "http://arxiv.org/abs/2306.04135", "description": "We propose two approaches to estimate semiparametric discrete choice models\nfor bundles. Our first approach is a kernel-weighted rank estimator based on a\nmatching-based identification strategy. We establish its complete asymptotic\nproperties and prove the validity of the nonparametric bootstrap for inference.\nWe then introduce a new multi-index least absolute deviations (LAD) estimator\nas an alternative, of which the main advantage is its capacity to estimate\npreference parameters on both alternative- and agent-specific regressors. Both\nmethods can account for arbitrary correlation in disturbances across choices,\nwith the former also allowing for interpersonal heteroskedasticity. We also\ndemonstrate that the identification strategy underlying these procedures can be\nextended naturally to panel data settings, producing an analogous localized\nmaximum score estimator and a LAD estimator for estimating bundle choice models\nwith fixed effects. We derive the limiting distribution of the former and\nverify the validity of the numerical bootstrap as an inference tool. All our\nproposed methods can be applied to general multi-index models. Monte Carlo\nexperiments show that they perform well in finite samples."}, "http://arxiv.org/abs/2311.03471": {"title": "Optimal Estimation Methodologies for Panel Data Regression Models", "link": "http://arxiv.org/abs/2311.03471", "description": "This survey study discusses main aspects to optimal estimation methodologies\nfor panel data regression models. In particular, we present current\nmethodological developments for modeling stationary panel data as well as\nrobust methods for estimation and inference in nonstationary panel data\nregression models. Some applications from the network econometrics and high\ndimensional statistics literature are also discussed within a stationary time\nseries environment."}, "http://arxiv.org/abs/2311.04073": {"title": "Debiased Fixed Effects Estimation of Binary Logit Models with Three-Dimensional Panel Data", "link": "http://arxiv.org/abs/2311.04073", "description": "Naive maximum likelihood estimation of binary logit models with fixed effects\nleads to unreliable inference due to the incidental parameter problem. We study\nthe case of three-dimensional panel data, where the model includes three sets\nof additive and overlapping unobserved effects. This encompasses models for\nnetwork panel data, where senders and receivers maintain bilateral\nrelationships over time, and fixed effects account for unobserved heterogeneity\nat the sender-time, receiver-time, and sender-receiver levels. In an asymptotic\nframework, where all three panel dimensions grow large at constant relative\nrates, we characterize the leading bias of the naive estimator. The inference\nproblem we identify is particularly severe, as it is not possible to balance\nthe order of the bias and the standard deviation. As a consequence, the naive\nestimator has a degenerating asymptotic distribution, which exacerbates the\ninference problem relative to other fixed effects estimators studied in the\nliterature. To resolve the inference problem, we derive explicit expressions to\ndebias the fixed effects estimator."}, "http://arxiv.org/abs/2207.09246": {"title": "Asymptotic Properties of Endogeneity Corrections Using Nonlinear Transformations", "link": "http://arxiv.org/abs/2207.09246", "description": "This paper considers a linear regression model with an endogenous regressor\nwhich arises from a nonlinear transformation of a latent variable. It is shown\nthat the corresponding coefficient can be consistently estimated without\nexternal instruments by adding a rank-based transformation of the regressor to\nthe model and performing standard OLS estimation. In contrast to other\napproaches, our nonparametric control function approach does not rely on a\nconformably specified copula. Furthermore, the approach allows for the presence\nof additional exogenous regressors which may be (linearly) correlated with the\nendogenous regressor(s). Consistency and asymptotic normality of the estimator\nare proved and the estimator is compared with copula based approaches by means\nof Monte Carlo simulations. An empirical application on wage data of the US\ncurrent population survey demonstrates the usefulness of our method."}, "http://arxiv.org/abs/2301.10643": {"title": "Automatic Locally Robust Estimation with Generated Regressors", "link": "http://arxiv.org/abs/2301.10643", "description": "Many economic and causal parameters of interest depend on generated\nregressors. Examples include structural parameters in models with endogenous\nvariables estimated by control functions and in models with sample selection,\ntreatment effect estimation with propensity score matching, and marginal\ntreatment effects. Inference with generated regressors is complicated by the\nvery complex expression for influence functions and asymptotic variances. To\naddress this problem, we propose Automatic Locally Robust/debiased GMM\nestimators in a general setting with generated regressors. Importantly, we\nallow for the generated regressors to be generated from machine learners, such\nas Random Forest, Neural Nets, Boosting, and many others. We use our results to\nconstruct novel Doubly Robust and Locally Robust estimators for the\nCounterfactual Average Structural Function and Average Partial Effects in\nmodels with endogeneity and sample selection, respectively. We provide\nsufficient conditions for the asymptotic normality of our debiased GMM\nestimators and investigate their finite sample performance through Monte Carlo\nsimulations."}, "http://arxiv.org/abs/2303.11399": {"title": "How Much Should We Trust Instrumental Variable Estimates in Political Science? Practical Advice Based on Over 60 Replicated Studies", "link": "http://arxiv.org/abs/2303.11399", "description": "Instrumental variable (IV) strategies are widely used in political science to\nestablish causal relationships. However, the identifying assumptions required\nby an IV design are demanding, and it remains challenging for researchers to\nassess their validity. In this paper, we replicate 67 papers published in three\ntop journals in political science during 2010-2022 and identify several\ntroubling patterns. First, researchers often overestimate the strength of their\nIVs due to non-i.i.d. errors, such as a clustering structure. Second, the most\ncommonly used t-test for the two-stage-least-squares (2SLS) estimates often\nseverely underestimates uncertainty. Using more robust inferential methods, we\nfind that around 19-30% of the 2SLS estimates in our sample are underpowered.\nThird, in the majority of the replicated studies, the 2SLS estimates are much\nlarger than the ordinary-least-squares estimates, and their ratio is negatively\ncorrelated with the strength of the IVs in studies where the IVs are not\nexperimentally generated, suggesting potential violations of unconfoundedness\nor the exclusion restriction. To help researchers avoid these pitfalls, we\nprovide a checklist for better practice."}, "http://arxiv.org/abs/2208.02028": {"title": "Bootstrap inference in the presence of bias", "link": "http://arxiv.org/abs/2208.02028", "description": "We consider bootstrap inference for estimators which are (asymptotically)\nbiased. We show that, even when the bias term cannot be consistently estimated,\nvalid inference can be obtained by proper implementations of the bootstrap.\nSpecifically, we show that the prepivoting approach of Beran (1987, 1988),\noriginally proposed to deliver higher-order refinements, restores bootstrap\nvalidity by transforming the original bootstrap p-value into an asymptotically\nuniform random variable. We propose two different implementations of\nprepivoting (plug-in and double bootstrap), and provide general high-level\nconditions that imply validity of bootstrap inference. To illustrate the\npractical relevance and implementation of our results, we discuss five\nexamples: (i) inference on a target parameter based on model averaging; (ii)\nridge-type regularized estimators; (iii) nonparametric regression; (iv) a\nlocation model for infinite variance data; and (v) dynamic panel data models."}, "http://arxiv.org/abs/2304.01273": {"title": "Heterogeneity-robust granular instruments", "link": "http://arxiv.org/abs/2304.01273", "description": "Granular instrumental variables (GIV) has experienced sharp growth in\nempirical macro-finance. The methodology's rise showcases granularity's\npotential for identification in a wide set of economic environments, like the\nestimation of spillovers and demand systems. I propose a new estimator--called\nrobust granular instrumental variables (RGIV)--that allows researchers to study\nunit-level heterogeneity in spillovers within GIV's framework. In contrast to\nGIV, RGIV also allows for unknown shock variances and does not require skewness\nof the size distribution of units. I also develop a test of overidentifying\nrestrictions that evaluates RGIV's compatibility with the data, a parameter\nrestriction test that evaluates the appropriateness of the homogeneous\nspillovers assumption, and extend the framework to allow for observable\nexplanatory variables. Applied to the Euro area, I find strong evidence of\ncountry-level heterogeneity in sovereign yield spillovers. In simulations, I\nshow that RGIV produces reliable and informative confidence intervals."}, "http://arxiv.org/abs/2309.11387": {"title": "Identifying Causal Effects in Information Provision Experiments", "link": "http://arxiv.org/abs/2309.11387", "description": "Information provision experiments are a popular way to study causal effects\nof beliefs on behavior. Researchers estimate these effects using TSLS. I show\nthat existing TSLS specifications do not estimate the average partial effect;\nthey have weights proportional to belief updating in the first-stage. If people\nwhose decisions depend on their beliefs gather information before the\nexperiment, the information treatment may shift beliefs more for people with\nweak belief effects. This attenuates TSLS estimates. I propose researchers use\na local-least-squares (LLS) estimator that I show consistently estimates the\naverage partial effect (APE) under Bayesian updating, and apply it to Settele\n(2022)."}, "http://arxiv.org/abs/2107.13737": {"title": "Design-Robust Two-Way-Fixed-Effects Regression For Panel Data", "link": "http://arxiv.org/abs/2107.13737", "description": "We propose a new estimator for average causal effects of a binary treatment\nwith panel data in settings with general treatment patterns. Our approach\naugments the popular two-way-fixed-effects specification with unit-specific\nweights that arise from a model for the assignment mechanism. We show how to\nconstruct these weights in various settings, including the staggered adoption\nsetting, where units opt into the treatment sequentially but permanently. The\nresulting estimator converges to an average (over units and time) treatment\neffect under the correct specification of the assignment model, even if the\nfixed effect model is misspecified. We show that our estimator is more robust\nthan the conventional two-way estimator: it remains consistent if either the\nassignment mechanism or the two-way regression model is correctly specified. In\naddition, the proposed estimator performs better than the two-way-fixed-effect\nestimator if the outcome model and assignment mechanism are locally\nmisspecified. This strong double robustness property underlines and quantifies\nthe benefits of modeling the assignment process and motivates using our\nestimator in practice. We also discuss an extension of our estimator to handle\ndynamic treatment effects."}, "http://arxiv.org/abs/2302.09756": {"title": "Identification-robust inference for the LATE with high-dimensional covariates", "link": "http://arxiv.org/abs/2302.09756", "description": "This paper presents an inference method for the local average treatment\neffect (LATE) in the presence of high-dimensional covariates, irrespective of\nthe strength of identification. We propose a novel high-dimensional conditional\ntest statistic with uniformly correct asymptotic size. We provide an\neasy-to-implement algorithm to infer the high-dimensional LATE by inverting our\ntest statistic and employing the double/debiased machine learning method.\nSimulations indicate that our test is robust against both weak identification\nand high dimensionality concerning size control and power performance,\noutperforming other conventional tests. Applying the proposed method to\nrailroad and population data to study the effect of railroad access on urban\npopulation growth, we observe that our methodology yields confidence intervals\nthat are 49% to 92% shorter than conventional results, depending on\nspecifications."}, "http://arxiv.org/abs/2311.05883": {"title": "Time-Varying Identification of Monetary Policy Shocks", "link": "http://arxiv.org/abs/2311.05883", "description": "We propose a new Bayesian heteroskedastic Markov-switching structural vector\nautoregression with data-driven time-varying identification. The model selects\nalternative exclusion restrictions over time and, as a condition for the\nsearch, allows to verify identification through heteroskedasticity within each\nregime. Based on four alternative monetary policy rules, we show that a monthly\nsix-variable system supports time variation in US monetary policy shock\nidentification. In the sample-dominating first regime, systematic monetary\npolicy follows a Taylor rule extended by the term spread and is effective in\ncurbing inflation. In the second regime, occurring after 2000 and gaining more\npersistence after the global financial and COVID crises, the Fed acts according\nto a money-augmented Taylor rule. This regime's unconventional monetary policy\nprovides economic stimulus, features the liquidity effect, and is complemented\nby a pure term spread shock. Absent the specific monetary policy of the second\nregime, inflation would be over one percentage point higher on average after\n2008."}, "http://arxiv.org/abs/2309.14160": {"title": "Unified Inference for Dynamic Quantile Predictive Regression", "link": "http://arxiv.org/abs/2309.14160", "description": "This paper develops unified asymptotic distribution theory for dynamic\nquantile predictive regressions which is useful when examining quantile\npredictability in stock returns under possible presence of nonstationarity."}, "http://arxiv.org/abs/2311.06256": {"title": "From Deep Filtering to Deep Econometrics", "link": "http://arxiv.org/abs/2311.06256", "description": "Calculating true volatility is an essential task for option pricing and risk\nmanagement. However, it is made difficult by market microstructure noise.\nParticle filtering has been proposed to solve this problem as it favorable\nstatistical properties, but relies on assumptions about underlying market\ndynamics. Machine learning methods have also been proposed but lack\ninterpretability, and often lag in performance. In this paper we implement the\nSV-PF-RNN: a hybrid neural network and particle filter architecture. Our\nSV-PF-RNN is designed specifically with stochastic volatility estimation in\nmind. We then show that it can improve on the performance of a basic particle\nfilter."}, "http://arxiv.org/abs/2311.06831": {"title": "Quasi-Bayes in Latent Variable Models", "link": "http://arxiv.org/abs/2311.06831", "description": "Latent variable models are widely used to account for unobserved determinants\nof economic behavior. Traditional nonparametric methods to estimate latent\nheterogeneity do not scale well into multidimensional settings. Distributional\nrestrictions alleviate tractability concerns but may impart non-trivial\nmisspecification bias. Motivated by these concerns, this paper introduces a\nquasi-Bayes approach to estimate a large class of multidimensional latent\nvariable models. Our approach to quasi-Bayes is novel in that we center it\naround relating the characteristic function of observables to the distribution\nof unobservables. We propose a computationally attractive class of priors that\nare supported on Gaussian mixtures and derive contraction rates for a variety\nof latent variable models."}, "http://arxiv.org/abs/2311.06891": {"title": "Design-based Estimation Theory for Complex Experiments", "link": "http://arxiv.org/abs/2311.06891", "description": "This paper considers the estimation of treatment effects in randomized\nexperiments with complex experimental designs, including cases with\ninterference between units. We develop a design-based estimation theory for\narbitrary experimental designs. Our theory facilitates the analysis of many\ndesign-estimator pairs that researchers commonly employ in practice and provide\nprocedures to consistently estimate asymptotic variance bounds. We propose new\nclasses of estimators with favorable asymptotic properties from a design-based\npoint of view. In addition, we propose a scalar measure of experimental\ncomplexity which can be linked to the design-based variance of the estimators.\nWe demonstrate the performance of our estimators using simulated datasets based\non an actual network experiment studying the effect of social networks on\ninsurance adoptions."}, "http://arxiv.org/abs/2311.07067": {"title": "High Dimensional Binary Choice Model with Unknown Heteroskedasticity or Instrumental Variables", "link": "http://arxiv.org/abs/2311.07067", "description": "This paper proposes a new method for estimating high-dimensional binary\nchoice models. The model we consider is semiparametric, placing no\ndistributional assumptions on the error term, allowing for heteroskedastic\nerrors, and permitting endogenous regressors. Our proposed approaches extend\nthe special regressor estimator originally proposed by Lewbel (2000). This\nestimator becomes impractical in high-dimensional settings due to the curse of\ndimensionality associated with high-dimensional conditional density estimation.\nTo overcome this challenge, we introduce an innovative data-driven dimension\nreduction method for nonparametric kernel estimators, which constitutes the\nmain innovation of this work. The method combines distance covariance-based\nscreening with cross-validation (CV) procedures, rendering the special\nregressor estimation feasible in high dimensions. Using the new feasible\nconditional density estimator, we address the variable and moment (instrumental\nvariable) selection problems for these models. We apply penalized least squares\n(LS) and Generalized Method of Moments (GMM) estimators with a smoothly clipped\nabsolute deviation (SCAD) penalty. A comprehensive analysis of the oracle and\nasymptotic properties of these estimators is provided. Monte Carlo simulations\nare employed to demonstrate the effectiveness of our proposed procedures in\nfinite sample scenarios."}, "http://arxiv.org/abs/2311.07243": {"title": "Optimal Estimation of Large-Dimensional Nonlinear Factor Models", "link": "http://arxiv.org/abs/2311.07243", "description": "This paper studies optimal estimation of large-dimensional nonlinear factor\nmodels. The key challenge is that the observed variables are possibly nonlinear\nfunctions of some latent variables where the functional forms are left\nunspecified. A local principal component analysis method is proposed to\nestimate the factor structure and recover information on latent variables and\nlatent functions, which combines $K$-nearest neighbors matching and principal\ncomponent analysis. Large-sample properties are established, including a sharp\nbound on the matching discrepancy of nearest neighbors, sup-norm error bounds\nfor estimated local factors and factor loadings, and the uniform convergence\nrate of the factor structure estimator. Under mild conditions our estimator of\nthe latent factor structure can achieve the optimal rate of uniform convergence\nfor nonparametric regression. The method is illustrated with a Monte Carlo\nexperiment and an empirical application studying the effect of tax cuts on\neconomic growth."}, "http://arxiv.org/abs/1902.09608": {"title": "On Binscatter", "link": "http://arxiv.org/abs/1902.09608", "description": "Binscatter is a popular method for visualizing bivariate relationships and\nconducting informal specification testing. We study the properties of this\nmethod formally and develop enhanced visualization and econometric binscatter\ntools. These include estimating conditional means with optimal binning and\nquantifying uncertainty. We also highlight a methodological problem related to\ncovariate adjustment that can yield incorrect conclusions. We revisit two\napplications using our methodology and find substantially different results\nrelative to those obtained using prior informal binscatter methods. General\npurpose software in Python, R, and Stata is provided. Our technical work is of\nindependent interest for the nonparametric partition-based estimation\nliterature."}, "http://arxiv.org/abs/2109.09043": {"title": "Composite Likelihood for Stochastic Migration Model with Unobserved Factor", "link": "http://arxiv.org/abs/2109.09043", "description": "We introduce the conditional Maximum Composite Likelihood (MCL) estimation\nmethod for the stochastic factor ordered Probit model of credit rating\ntransitions of firms. This model is recommended for internal credit risk\nassessment procedures in banks and financial institutions under the Basel III\nregulations. Its exact likelihood function involves a high-dimensional\nintegral, which can be approximated numerically before maximization. However,\nthe estimated migration risk and required capital tend to be sensitive to the\nquality of this approximation, potentially leading to statistical regulatory\narbitrage. The proposed conditional MCL estimator circumvents this problem and\nmaximizes the composite log-likelihood of the factor ordered Probit model. We\npresent three conditional MCL estimators of different complexity and examine\ntheir consistency and asymptotic normality when n and T tend to infinity. The\nperformance of these estimators at finite T is examined and compared with a\ngranularity-based approach in a simulation study. The use of the MCL estimator\nis also illustrated in an empirical application."}, "http://arxiv.org/abs/2111.01301": {"title": "Asymptotic in a class of network models with an increasing sub-Gamma degree sequence", "link": "http://arxiv.org/abs/2111.01301", "description": "For the differential privacy under the sub-Gamma noise, we derive the\nasymptotic properties of a class of network models with binary values with a\ngeneral link function. In this paper, we release the degree sequences of the\nbinary networks under a general noisy mechanism with the discrete Laplace\nmechanism as a special case. We establish the asymptotic result including both\nconsistency and asymptotically normality of the parameter estimator when the\nnumber of parameters goes to infinity in a class of network models. Simulations\nand a real data example are provided to illustrate asymptotic results."}, "http://arxiv.org/abs/2309.08982": {"title": "Least squares estimation in nonlinear cohort panels with learning from experience", "link": "http://arxiv.org/abs/2309.08982", "description": "We discuss techniques of estimation and inference for nonlinear cohort panels\nwith learning from experience, showing, inter alia, the consistency and\nasymptotic normality of the nonlinear least squares estimator employed in the\nseminal paper by Malmendier and Nagel (2016, QJE). Potential pitfalls for\nhypothesis testing are identified and solutions proposed. Monte Carlo\nsimulations verify the properties of the estimator and corresponding test\nstatistics in finite samples, while an application to a panel of survey\nexpectations demonstrates the usefulness of the theory developed."}, "http://arxiv.org/abs/2311.08218": {"title": "Estimating Conditional Value-at-Risk with Nonstationary Quantile Predictive Regression Models", "link": "http://arxiv.org/abs/2311.08218", "description": "This paper develops an asymptotic distribution theory for a two-stage\ninstrumentation estimation approach in quantile predictive regressions when\nboth generated covariates and persistent predictors are used. The generated\ncovariates are obtained from an auxiliary quantile regression model and our\nmain interest is the robust estimation and inference of the primary quantile\npredictive regression in which this generated covariate is added to the set of\nnonstationary regressors. We find that the proposed doubly IVX estimator is\nrobust to the abstract degree of persistence regardless of the presence of\ngenerated regressor obtained from the first stage procedure. The asymptotic\nproperties of the two-stage IVX estimator such as mixed Gaussianity are\nestablished while the asymptotic covariance matrix is adjusted to account for\nthe first-step estimation error."}, "http://arxiv.org/abs/2302.00469": {"title": "Regression adjustment in randomized controlled trials with many covariates", "link": "http://arxiv.org/abs/2302.00469", "description": "This paper is concerned with estimation and inference on average treatment\neffects in randomized controlled trials when researchers observe potentially\nmany covariates. By employing Neyman's (1923) finite population perspective, we\npropose a bias-corrected regression adjustment estimator using cross-fitting,\nand show that the proposed estimator has favorable properties over existing\nalternatives. For inference, we derive the first and second order terms in the\nstochastic component of the regression adjustment estimators, study higher\norder properties of the existing inference methods, and propose a\nbias-corrected version of the HC3 standard error. The proposed methods readily\nextend to stratified experiments with large strata. Simulation studies show our\ncross-fitted estimator, combined with the bias-corrected HC3, delivers precise\npoint estimates and robust size controls over a wide range of DGPs. To\nillustrate, the proposed methods are applied to real dataset on randomized\nexperiments of incentives and services for college achievement following\nAngrist, Lang, and Oreopoulos (2009)."}, "http://arxiv.org/abs/2311.08958": {"title": "Locally Asymptotically Minimax Statistical Treatment Rules Under Partial Identification", "link": "http://arxiv.org/abs/2311.08958", "description": "Policymakers often desire a statistical treatment rule (STR) that determines\na treatment assignment rule deployed in a future population from available\ndata. With the true knowledge of the data generating process, the average\ntreatment effect (ATE) is the key quantity characterizing the optimal treatment\nrule. Unfortunately, the ATE is often not point identified but partially\nidentified. Presuming the partial identification of the ATE, this study\nconducts a local asymptotic analysis and develops the locally asymptotically\nminimax (LAM) STR. The analysis does not assume the full differentiability but\nthe directional differentiability of the boundary functions of the\nidentification region of the ATE. Accordingly, the study shows that the LAM STR\ndiffers from the plug-in STR. A simulation study also demonstrates that the LAM\nSTR outperforms the plug-in STR."}, "http://arxiv.org/abs/2311.08963": {"title": "Incorporating Preferences Into Treatment Assignment Problems", "link": "http://arxiv.org/abs/2311.08963", "description": "This study investigates the problem of individualizing treatment allocations\nusing stated preferences for treatments. If individuals know in advance how the\nassignment will be individualized based on their stated preferences, they may\nstate false preferences. We derive an individualized treatment rule (ITR) that\nmaximizes welfare when individuals strategically state their preferences. We\nalso show that the optimal ITR is strategy-proof, that is, individuals do not\nhave a strong incentive to lie even if they know the optimal ITR a priori.\nConstructing the optimal ITR requires information on the distribution of true\npreferences and the average treatment effect conditioned on true preferences.\nIn practice, the information must be identified and estimated from the data. As\ntrue preferences are hidden information, the identification is not\nstraightforward. We discuss two experimental designs that allow the\nidentification: strictly strategy-proof randomized controlled trials and doubly\nrandomized preference trials. Under the presumption that data comes from one of\nthese experiments, we develop data-dependent procedures for determining ITR,\nthat is, statistical treatment rules (STRs). The maximum regret of the proposed\nSTRs converges to zero at a rate of the square root of the sample size. An\nempirical application demonstrates our proposed STRs."}, "http://arxiv.org/abs/2007.04267": {"title": "Difference-in-Differences Estimators of Intertemporal Treatment Effects", "link": "http://arxiv.org/abs/2007.04267", "description": "We study treatment-effect estimation using panel data. The treatment may be\nnon-binary, non-absorbing, and the outcome may be affected by treatment lags.\nWe make a parallel-trends assumption, and propose event-study estimators of the\neffect of being exposed to a weakly higher treatment dose for $\\ell$ periods.\nWe also propose normalized estimators, that estimate a weighted average of the\neffects of the current treatment and its lags. We also analyze commonly-used\ntwo-way-fixed-effects regressions. Unlike our estimators, they can be biased in\nthe presence of heterogeneous treatment effects. A local-projection version of\nthose regressions is biased even with homogeneous effects."}, "http://arxiv.org/abs/2007.10432": {"title": "Treatment Effects with Targeting Instruments", "link": "http://arxiv.org/abs/2007.10432", "description": "Multivalued treatments are commonplace in applications. We explore the use of\ndiscrete-valued instruments to control for selection bias in this setting. Our\ndiscussion revolves around the concept of targeting instruments: which\ninstruments target which treatments. It allows us to establish conditions under\nwhich counterfactual averages and treatment effects are point- or\npartially-identified for composite complier groups. We illustrate the\nusefulness of our framework by applying it to data from the Head Start Impact\nStudy. Under a plausible positive selection assumption, we derive informative\nbounds that suggest less beneficial effects of Head Start expansions than the\nparametric estimates of Kline and Walters (2016)."}, "http://arxiv.org/abs/2310.00786": {"title": "Semidiscrete optimal transport with unknown costs", "link": "http://arxiv.org/abs/2310.00786", "description": "Semidiscrete optimal transport is a challenging generalization of the\nclassical transportation problem in linear programming. The goal is to design a\njoint distribution for two random variables (one continuous, one discrete) with\nfixed marginals, in a way that minimizes expected cost. We formulate a novel\nvariant of this problem in which the cost functions are unknown, but can be\nlearned through noisy observations; however, only one function can be sampled\nat a time. We develop a semi-myopic algorithm that couples online learning with\nstochastic approximation, and prove that it achieves optimal convergence rates,\ndespite the non-smoothness of the stochastic gradient and the lack of strong\nconcavity in the objective function."}, "http://arxiv.org/abs/2311.09435": {"title": "Estimating Functionals of the Joint Distribution of Potential Outcomes with Optimal Transport", "link": "http://arxiv.org/abs/2311.09435", "description": "Many causal parameters depend on a moment of the joint distribution of\npotential outcomes. Such parameters are especially relevant in policy\nevaluation settings, where noncompliance is common and accommodated through the\nmodel of Imbens &amp; Angrist (1994). This paper shows that the sharp identified\nset for these parameters is an interval with endpoints characterized by the\nvalue of optimal transport problems. Sample analogue estimators are proposed\nbased on the dual problem of optimal transport. These estimators are root-n\nconsistent and converge in distribution under mild assumptions. Inference\nprocedures based on the bootstrap are straightforward and computationally\nconvenient. The ideas and estimators are demonstrated in an application\nrevisiting the National Supported Work Demonstration job training program. I\nfind suggestive evidence that workers who would see below average earnings\nwithout treatment tend to see above average benefits from treatment."}, "http://arxiv.org/abs/2311.09972": {"title": "Inference in Auctions with Many Bidders Using Transaction Prices", "link": "http://arxiv.org/abs/2311.09972", "description": "This paper considers inference in first-price and second-price sealed-bid\nauctions with a large number of symmetric bidders having independent private\nvalues. Given the abundance of bidders in each auction, we propose an\nasymptotic framework in which the number of bidders diverges while the number\nof auctions remains fixed. This framework allows us to perform asymptotically\nexact inference on key model features using only transaction price data.\nSpecifically, we examine inference on the expected utility of the auction\nwinner, the expected revenue of the seller, and the tail properties of the\nvaluation distribution. Simulations confirm the accuracy of our inference\nmethods in finite samples. Finally, we also apply them to Hong Kong car license\nauction data."}, "http://arxiv.org/abs/2212.06080": {"title": "Logs with zeros? Some problems and solutions", "link": "http://arxiv.org/abs/2212.06080", "description": "When studying an outcome $Y$ that is weakly-positive but can equal zero (e.g.\nearnings), researchers frequently estimate an average treatment effect (ATE)\nfor a \"log-like\" transformation that behaves like $\\log(Y)$ for large $Y$ but\nis defined at zero (e.g. $\\log(1+Y)$, $\\mathrm{arcsinh}(Y)$). We argue that\nATEs for log-like transformations should not be interpreted as approximating\npercentage effects, since unlike a percentage, they depend on the units of the\noutcome. In fact, we show that if the treatment affects the extensive margin,\none can obtain a treatment effect of any magnitude simply by re-scaling the\nunits of $Y$ before taking the log-like transformation. This arbitrary\nunit-dependence arises because an individual-level percentage effect is not\nwell-defined for individuals whose outcome changes from zero to non-zero when\nreceiving treatment, and the units of the outcome implicitly determine how much\nweight the ATE for a log-like transformation places on the extensive margin. We\nfurther establish a trilemma: when the outcome can equal zero, there is no\ntreatment effect parameter that is an average of individual-level treatment\neffects, unit-invariant, and point-identified. We discuss several alternative\napproaches that may be sensible in settings with an intensive and extensive\nmargin, including (i) expressing the ATE in levels as a percentage (e.g. using\nPoisson regression), (ii) explicitly calibrating the value placed on the\nintensive and extensive margins, and (iii) estimating separate effects for the\ntwo margins (e.g. using Lee bounds). We illustrate these approaches in three\nempirical applications."}, "http://arxiv.org/abs/2308.05486": {"title": "The Distributional Impact of Money Growth and Inflation Disaggregates: A Quantile Sensitivity Analysis", "link": "http://arxiv.org/abs/2308.05486", "description": "We propose an alternative method to construct a quantile dependence system\nfor inflation and money growth. By considering all quantiles, we assess how\nperturbations in one variable's quantile lead to changes in the distribution of\nthe other variable. We demonstrate the construction of this relationship\nthrough a system of linear quantile regressions. The proposed framework is\nexploited to examine the distributional effects of money growth on the\ndistributions of inflation and its disaggregate measures in the United States\nand the Euro area. Our empirical analysis uncovers significant impacts of the\nupper quantile of the money growth distribution on the distribution of\ninflation and its disaggregate measures. Conversely, we find that the lower and\nmedian quantiles of the money growth distribution have a negligible influence\non the distribution of inflation and its disaggregate measures."}, "http://arxiv.org/abs/2311.10685": {"title": "High-Throughput Asset Pricing", "link": "http://arxiv.org/abs/2311.10685", "description": "We use empirical Bayes (EB) to mine for out-of-sample returns among 73,108\nlong-short strategies constructed from accounting ratios, past returns, and\nticker symbols. EB predicts returns are concentrated in accounting and past\nreturn strategies, small stocks, and pre-2004 samples. The cross-section of\nout-of-sample return lines up closely with EB predictions. Data-mined\nportfolios have mean returns comparable with published portfolios, but the\ndata-mined returns are arguably free of data mining bias. In contrast,\ncontrolling for multiple testing following Harvey, Liu, and Zhu (2016) misses\nthe vast majority of returns. This \"high-throughput asset pricing\" provides an\nevidence-based solution for data mining bias."}, "http://arxiv.org/abs/2209.11444": {"title": "Identification of the Marginal Treatment Effect with Multivalued Treatments", "link": "http://arxiv.org/abs/2209.11444", "description": "The multinomial choice model based on utility maximization has been widely\nused to select treatments. In this paper, we establish sufficient conditions\nfor the identification of the marginal treatment effects with multivalued\ntreatments. Our result reveals treatment effects conditioned on the willingness\nto participate in treatments against a specific treatment. Further, our results\ncan identify other parameters such as the marginal distribution of potential\noutcomes."}, "http://arxiv.org/abs/2211.04027": {"title": "Bootstraps for Dynamic Panel Threshold Models", "link": "http://arxiv.org/abs/2211.04027", "description": "This paper develops valid bootstrap inference methods for the dynamic panel\nthreshold regression. For the first-differenced generalized method of moments\n(GMM) estimation for the dynamic short panel, we show that the standard\nnonparametric bootstrap is inconsistent. The inconsistency is due to an\n$n^{1/4}$-consistent non-normal asymptotic distribution for the threshold\nestimate when the parameter resides within the continuity region of the\nparameter space, which stems from the rank deficiency of the approximate\nJacobian of the sample moment conditions on the continuity region. We propose a\ngrid bootstrap to construct confidence sets for the threshold, a residual\nbootstrap to construct confidence intervals for the coefficients, and a\nbootstrap for testing continuity. They are shown to be valid under uncertain\ncontinuity. A set of Monte Carlo experiments demonstrate that the proposed\nbootstraps perform well in the finite samples and improve upon the asymptotic\nnormal approximation even under a large jump at the threshold. An empirical\napplication to firms' investment model illustrates our methods."}, "http://arxiv.org/abs/2304.07480": {"title": "Gini-stable Lorenz curves and their relation to the generalised Pareto distribution", "link": "http://arxiv.org/abs/2304.07480", "description": "We introduce an iterative discrete information production process where we\ncan extend ordered normalised vectors by new elements based on a simple affine\ntransformation, while preserving the predefined level of inequality, G, as\nmeasured by the Gini index.\n\nThen, we derive the family of empirical Lorenz curves of the corresponding\nvectors and prove that it is stochastically ordered with respect to both the\nsample size and G which plays the role of the uncertainty parameter. We prove\nthat asymptotically, we obtain all, and only, Lorenz curves generated by a new,\nintuitive parametrisation of the finite-mean Pickands' Generalised Pareto\nDistribution (GPD) that unifies three other families, namely: the Pareto Type\nII, exponential, and scaled beta ones. The family is not only totally ordered\nwith respect to the parameter G, but also, thanks to our derivations, has a\nnice underlying interpretation. Our result may thus shed a new light on the\ngenesis of this family of distributions.\n\nOur model fits bibliometric, informetric, socioeconomic, and environmental\ndata reasonably well. It is quite user-friendly for it only depends on the\nsample size and its Gini index."}, "http://arxiv.org/abs/2311.11637": {"title": "Modeling economies of scope in joint production: Convex regression of input distance function", "link": "http://arxiv.org/abs/2311.11637", "description": "Modeling of joint production has proved a vexing problem. This paper develops\na radial convex nonparametric least squares (CNLS) approach to estimate the\ninput distance function with multiple outputs. We document the correct input\ndistance function transformation and prove that the necessary orthogonality\nconditions can be satisfied in radial CNLS. A Monte Carlo study is performed to\ncompare the finite sample performance of radial CNLS and other deterministic\nand stochastic frontier approaches in terms of the input distance function\nestimation. We apply our novel approach to the Finnish electricity distribution\nnetwork regulation and empirically confirm that the input isoquants become more\ncurved. In addition, we introduce the weight restriction to radial CNLS to\nmitigate the potential overfitting and increase the out-of-sample performance\nin energy regulation."}, "http://arxiv.org/abs/2311.11858": {"title": "Theory coherent shrinkage of Time-Varying Parameters in VARs", "link": "http://arxiv.org/abs/2311.11858", "description": "Time-Varying Parameters Vector Autoregressive (TVP-VAR) models are frequently\nused in economics to capture evolving relationships among the macroeconomic\nvariables. However, TVP-VARs have the tendency of overfitting the data,\nresulting in inaccurate forecasts and imprecise estimates of typical objects of\ninterests such as the impulse response functions. This paper introduces a\nTheory Coherent Time-Varying Parameters Vector Autoregressive Model\n(TC-TVP-VAR), which leverages on an arbitrary theoretical framework derived by\nan underlying economic theory to form a prior for the time varying parameters.\nThis \"theory coherent\" shrinkage prior significantly improves inference\nprecision and forecast accuracy over the standard TVP-VAR. Furthermore, the\nTC-TVP-VAR can be used to perform indirect posterior inference on the deep\nparameters of the underlying economic theory. The paper reveals that using the\nclassical 3-equation New Keynesian block to form a prior for the TVP- VAR\nsubstantially enhances forecast accuracy of output growth and of the inflation\nrate in a standard model of monetary policy. Additionally, the paper shows that\nthe TC-TVP-VAR can be used to address the inferential challenges during the\nZero Lower Bound period."}, "http://arxiv.org/abs/2007.07842": {"title": "Persistence in Financial Connectedness and Systemic Risk", "link": "http://arxiv.org/abs/2007.07842", "description": "This paper characterises dynamic linkages arising from shocks with\nheterogeneous degrees of persistence. Using frequency domain techniques, we\nintroduce measures that identify smoothly varying links of a transitory and\npersistent nature. Our approach allows us to test for statistical differences\nin such dynamic links. We document substantial differences in transitory and\npersistent linkages among US financial industry volatilities, argue that they\ntrack heterogeneously persistent sources of systemic risk, and thus may serve\nas a useful tool for market participants."}, "http://arxiv.org/abs/2205.11365": {"title": "Graph-Based Methods for Discrete Choice", "link": "http://arxiv.org/abs/2205.11365", "description": "Choices made by individuals have widespread impacts--for instance, people\nchoose between political candidates to vote for, between social media posts to\nshare, and between brands to purchase--moreover, data on these choices are\nincreasingly abundant. Discrete choice models are a key tool for learning\nindividual preferences from such data. Additionally, social factors like\nconformity and contagion influence individual choice. Traditional methods for\nincorporating these factors into choice models do not account for the entire\nsocial network and require hand-crafted features. To overcome these\nlimitations, we use graph learning to study choice in networked contexts. We\nidentify three ways in which graph learning techniques can be used for discrete\nchoice: learning chooser representations, regularizing choice model parameters,\nand directly constructing predictions from a network. We design methods in each\ncategory and test them on real-world choice datasets, including county-level\n2016 US election results and Android app installation and usage data. We show\nthat incorporating social network structure can improve the predictions of the\nstandard econometric choice model, the multinomial logit. We provide evidence\nthat app installations are influenced by social context, but we find no such\neffect on app usage among the same participants, which instead is habit-driven.\nIn the election data, we highlight the additional insights a discrete choice\nframework provides over classification or regression, the typical approaches.\nOn synthetic data, we demonstrate the sample complexity benefit of using social\ninformation in choice models."}, "http://arxiv.org/abs/2306.09287": {"title": "Modelling and Forecasting Macroeconomic Risk with Time Varying Skewness Stochastic Volatility Models", "link": "http://arxiv.org/abs/2306.09287", "description": "Monitoring downside risk and upside risk to the key macroeconomic indicators\nis critical for effective policymaking aimed at maintaining economic stability.\nIn this paper I propose a parametric framework for modelling and forecasting\nmacroeconomic risk based on stochastic volatility models with Skew-Normal and\nSkew-t shocks featuring time varying skewness. Exploiting a mixture stochastic\nrepresentation of the Skew-Normal and Skew-t random variables, in the paper I\ndevelop efficient posterior simulation samplers for Bayesian estimation of both\nunivariate and VAR models of this type. In an application, I use the models to\npredict downside risk to GDP growth in the US and I show that these models\nrepresent a competitive alternative to semi-parametric approaches such as\nquantile regression. Finally, estimating a medium scale VAR on US data I show\nthat time varying skewness is a relevant feature of macroeconomic and financial\nshocks."}, "http://arxiv.org/abs/2309.07476": {"title": "Causal inference in network experiments: regression-based analysis and design-based properties", "link": "http://arxiv.org/abs/2309.07476", "description": "Investigating interference or spillover effects among units is a central task\nin many social science problems. Network experiments are powerful tools for\nthis task, which avoids endogeneity by randomly assigning treatments to units\nover networks. However, it is non-trivial to analyze network experiments\nproperly without imposing strong modeling assumptions. Previously, many\nresearchers have proposed sophisticated point estimators and standard errors\nfor causal effects under network experiments. We further show that\nregression-based point estimators and standard errors can have strong\ntheoretical guarantees if the regression functions and robust standard errors\nare carefully specified to accommodate the interference patterns under network\nexperiments. We first recall a well-known result that the Hajek estimator is\nnumerically identical to the coefficient from the weighted-least-squares fit\nbased on the inverse probability of the exposure mapping. Moreover, we\ndemonstrate that the regression-based approach offers three notable advantages:\nits ease of implementation, the ability to derive standard errors through the\nsame weighted-least-squares fit, and the capacity to integrate covariates into\nthe analysis, thereby enhancing estimation efficiency. Furthermore, we analyze\nthe asymptotic bias of the regression-based network-robust standard errors.\nRecognizing that the covariance estimator can be anti-conservative, we propose\nan adjusted covariance estimator to improve the empirical coverage rates.\nAlthough we focus on regression-based point estimators and standard errors, our\ntheory holds under the design-based framework, which assumes that the\nrandomness comes solely from the design of network experiments and allows for\narbitrary misspecification of the regression models."}, "http://arxiv.org/abs/2311.12267": {"title": "Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity", "link": "http://arxiv.org/abs/2311.12267", "description": "This paper studies causal representation learning, the task of recovering\nhigh-level latent variables and their causal relationships from low-level data\nthat we observe, assuming access to observations generated from multiple\nenvironments. While existing works are able to prove full identifiability of\nthe underlying data generating process, they typically assume access to\nsingle-node, hard interventions which is rather unrealistic in practice. The\nmain contribution of this paper is characterize a notion of identifiability\nwhich is provably the best one can achieve when hard interventions are not\navailable. First, for linear causal models, we provide identifiability\nguarantee for data observed from general environments without assuming any\nsimilarities between them. While the causal graph is shown to be fully\nrecovered, the latent variables are only identified up to an effect-domination\nambiguity (EDA). We then propose an algorithm, LiNGCReL which is guaranteed to\nrecover the ground-truth model up to EDA, and we demonstrate its effectiveness\nvia numerical experiments. Moving on to general non-parametric causal models,\nwe prove the same idenfifiability guarantee assuming access to groups of soft\ninterventions. Finally, we provide counterparts of our identifiability results,\nindicating that EDA is basically inevitable in our setting."}, "http://arxiv.org/abs/2311.12671": {"title": "Predictive Density Combination Using a Tree-Based Synthesis Function", "link": "http://arxiv.org/abs/2311.12671", "description": "Bayesian predictive synthesis (BPS) provides a method for combining multiple\npredictive distributions based on agent/expert opinion analysis theory and\nencompasses a range of existing density forecast pooling methods. The key\ningredient in BPS is a ``synthesis'' function. This is typically specified\nparametrically as a dynamic linear regression. In this paper, we develop a\nnonparametric treatment of the synthesis function using regression trees. We\nshow the advantages of our tree-based approach in two macroeconomic forecasting\napplications. The first uses density forecasts for GDP growth from the euro\narea's Survey of Professional Forecasters. The second combines density\nforecasts of US inflation produced by many regression models involving\ndifferent predictors. Both applications demonstrate the benefits -- in terms of\nimproved forecast accuracy and interpretability -- of modeling the synthesis\nfunction nonparametrically."}, "http://arxiv.org/abs/1810.00283": {"title": "Proxy Controls and Panel Data", "link": "http://arxiv.org/abs/1810.00283", "description": "We provide new results for nonparametric identification, estimation, and\ninference of causal effects using `proxy controls': observables that are noisy\nbut informative proxies for unobserved confounding factors. Our analysis\napplies to cross-sectional settings but is particularly well-suited to panel\nmodels. Our identification results motivate a simple and `well-posed'\nnonparametric estimator. We derive convergence rates for the estimator and\nconstruct uniform confidence bands with asymptotically correct size. In panel\nsettings, our methods provide a novel approach to the difficult problem of\nidentification with non-separable, general heterogeneity and fixed $T$. In\npanels, observations from different periods serve as proxies for unobserved\nheterogeneity and our key identifying assumptions follow from restrictions on\nthe serial dependence structure. We apply our methods to two empirical\nsettings. We estimate consumer demand counterfactuals using panel data and we\nestimate causal effects of grade retention on cognitive performance."}, "http://arxiv.org/abs/2102.08809": {"title": "Testing for Nonlinear Cointegration under Heteroskedasticity", "link": "http://arxiv.org/abs/2102.08809", "description": "This article discusses tests for nonlinear cointegration in the presence of\nvariance breaks. We build on cointegration test approaches under\nheteroskedasticity (Cavaliere and Taylor, 2006, Journal of Time Series\nAnalysis) and for nonlinearity (Choi and Saikkonen, 2010, Econometric Theory)\nto propose a bootstrap test and prove its consistency. A Monte Carlo study\nshows the approach to have good finite sample properties. We provide an\nempirical application to the environmental Kuznets curves (EKC), finding that\nthe cointegration test provides little evidence for the EKC hypothesis.\nAdditionally, we examine the nonlinear relation between the US money and the\ninterest rate, finding that our test does not reject the null of a smooth\ntransition cointegrating relation."}, "http://arxiv.org/abs/2311.12878": {"title": "Adaptive Bayesian Learning with Action and State-Dependent Signal Variance", "link": "http://arxiv.org/abs/2311.12878", "description": "This manuscript presents an advanced framework for Bayesian learning by\nincorporating action and state-dependent signal variances into decision-making\nmodels. This framework is pivotal in understanding complex data-feedback loops\nand decision-making processes in various economic systems. Through a series of\nexamples, we demonstrate the versatility of this approach in different\ncontexts, ranging from simple Bayesian updating in stable environments to\ncomplex models involving social learning and state-dependent uncertainties. The\npaper uniquely contributes to the understanding of the nuanced interplay\nbetween data, actions, outcomes, and the inherent uncertainty in economic\nmodels."}, "http://arxiv.org/abs/2311.13327": {"title": "Regressions under Adverse Conditions", "link": "http://arxiv.org/abs/2311.13327", "description": "We introduce a new regression method that relates the mean of an outcome\nvariable to covariates, given the \"adverse condition\" that a distress variable\nfalls in its tail. This allows to tailor classical mean regressions to adverse\neconomic scenarios, which receive increasing interest in managing macroeconomic\nand financial risks, among many others. In the terminology of the systemic risk\nliterature, our method can be interpreted as a regression for the Marginal\nExpected Shortfall. We propose a two-step procedure to estimate the new models,\nshow consistency and asymptotic normality of the estimator, and propose\nfeasible inference under weak conditions allowing for cross-sectional and time\nseries applications. The accuracy of the asymptotic approximations of the\ntwo-step estimator is verified in simulations. Two empirical applications show\nthat our regressions under adverse conditions are valuable in such diverse\nfields as the study of the relation between systemic risk and asset price\nbubbles, and dissecting macroeconomic growth vulnerabilities into individual\ncomponents."}, "http://arxiv.org/abs/2311.13575": {"title": "Large-Sample Properties of the Synthetic Control Method under Selection on Unobservables", "link": "http://arxiv.org/abs/2311.13575", "description": "We analyze the properties of the synthetic control (SC) method in settings\nwith a large number of units. We assume that the selection into treatment is\nbased on unobserved permanent heterogeneity and pretreatment information, thus\nallowing for both strictly and sequentially exogenous assignment processes.\nExploiting duality, we interpret the solution of the SC optimization problem as\nan estimator for the underlying treatment probabilities. We use this to derive\nthe asymptotic representation for the SC method and characterize sufficient\nconditions for its asymptotic normality. We show that the critical property\nthat determines the behavior of the SC method is the ability of input features\nto approximate the unobserved heterogeneity. Our results imply that the SC\nmethod delivers asymptotically normal estimators for a large class of linear\npanel data models as long as the number of pretreatment periods is large,\nmaking it a natural alternative to conventional methods built on the\nDifference-in-Differences."}, "http://arxiv.org/abs/2108.00723": {"title": "Partial Identification and Inference for Conditional Distributions of Treatment Effects", "link": "http://arxiv.org/abs/2108.00723", "description": "This paper considers identification and inference for the distribution of\ntreatment effects conditional on observable covariates. Since the conditional\ndistribution of treatment effects is not point identified without strong\nassumptions, we obtain bounds on the conditional distribution of treatment\neffects by using the Makarov bounds. We also consider the case where the\ntreatment is endogenous and propose two stochastic dominance assumptions to\ntighten the bounds. We develop a nonparametric framework to estimate the bounds\nand establish the asymptotic theory that is uniformly valid over the support of\ntreatment effects. An empirical example illustrates the usefulness of the\nmethods."}, "http://arxiv.org/abs/2311.13969": {"title": "Was Javert right to be suspicious? Unpacking treatment effect heterogeneity of alternative sentences on time-to-recidivism in Brazil", "link": "http://arxiv.org/abs/2311.13969", "description": "This paper presents new econometric tools to unpack the treatment effect\nheterogeneity of punishing misdemeanor offenses on time-to-recidivism. We show\nhow one can identify, estimate, and make inferences on the distributional,\nquantile, and average marginal treatment effects in setups where the treatment\nselection is endogenous and the outcome of interest, usually a duration\nvariable, is potentially right-censored. We explore our proposed econometric\nmethodology to evaluate the effect of fines and community service sentences as\na form of punishment on time-to-recidivism in the State of S\\~ao Paulo, Brazil,\nbetween 2010 and 2019, leveraging the as-if random assignment of judges to\ncases. Our results highlight substantial treatment effect heterogeneity that\nother tools are not meant to capture. For instance, we find that people whom\nmost judges would punish take longer to recidivate as a consequence of the\npunishment, while people who would be punished only by strict judges recidivate\nat an earlier date than if they were not punished. This result suggests that\ndesigning sentencing guidelines that encourage strict judges to become more\nlenient could reduce recidivism."}, "http://arxiv.org/abs/2311.14032": {"title": "Counterfactual Sensitivity in Equilibrium Models", "link": "http://arxiv.org/abs/2311.14032", "description": "Counterfactuals in equilibrium models are functions of the current state of\nthe world, the exogenous change variables and the model parameters. Current\npractice treats the current state of the world, the observed data, as perfectly\nmeasured, but there is good reason to believe that they are measured with\nerror. The main aim of this paper is to provide tools for quantifying\nuncertainty about counterfactuals, when the current state of the world is\nmeasured with error. I propose two methods, a Bayesian approach and an\nadversarial approach. Both methods are practical and theoretically justified. I\napply the two methods to the application in Adao et al. (2017) and find\nnon-trivial uncertainty about counterfactuals."}, "http://arxiv.org/abs/2311.14204": {"title": "Reproducible Aggregation of Sample-Split Statistics", "link": "http://arxiv.org/abs/2311.14204", "description": "Statistical inference is often simplified by sample-splitting. This\nsimplification comes at the cost of the introduction of randomness that is not\nnative to the data. We propose a simple procedure for sequentially aggregating\nstatistics constructed with multiple splits of the same sample. The user\nspecifies a bound and a nominal error rate. If the procedure is implemented\ntwice on the same data, the nominal error rate approximates the chance that the\nresults differ by more than the bound. We provide a non-asymptotic analysis of\nthe accuracy of the nominal error rate and illustrate the application of the\nprocedure to several widely applied statistical methods."}, "http://arxiv.org/abs/2205.03288": {"title": "Leverage, Influence, and the Jackknife in Clustered Regression Models: Reliable Inference Using summclust", "link": "http://arxiv.org/abs/2205.03288", "description": "We introduce a new Stata package called summclust that summarizes the cluster\nstructure of the dataset for linear regression models with clustered\ndisturbances. The key unit of observation for such a model is the cluster. We\ntherefore propose cluster-level measures of leverage, partial leverage, and\ninfluence and show how to compute them quickly in most cases. The measures of\nleverage and partial leverage can be used as diagnostic tools to identify\ndatasets and regression designs in which cluster-robust inference is likely to\nbe challenging. The measures of influence can provide valuable information\nabout how the results depend on the data in the various clusters. We also show\nhow to calculate two jackknife variance matrix estimators efficiently as a\nbyproduct of our other computations. These estimators, which are already\navailable in Stata, are generally more conservative than conventional variance\nmatrix estimators. The summclust package computes all the quantities that we\ndiscuss."}, "http://arxiv.org/abs/2205.10310": {"title": "Treatment Effects in Bunching Designs: The Impact of Mandatory Overtime Pay on Hours", "link": "http://arxiv.org/abs/2205.10310", "description": "The 1938 Fair Labor Standards Act mandates overtime premium pay for most U.S.\nworkers, but it has proven difficult to assess the policy's impact on the labor\nmarket because the rule applies nationally and has varied little over time. I\nuse the extent to which firms bunch workers at the overtime threshold of 40\nhours in a week to estimate the rule's effect on hours, drawing on data from\nindividual workers' weekly paychecks. To do so I generalize a popular\nidentification strategy that exploits bunching at kink points in a\ndecision-maker's choice set. Making only nonparametric assumptions about\npreferences and heterogeneity, I show that the average causal response among\nbunchers to the policy switch at the kink is partially identified. The bounds\nindicate a relatively small elasticity of demand for weekly hours, suggesting\nthat the overtime mandate has a discernible but limited impact on hours and\nemployment."}, "http://arxiv.org/abs/2305.19484": {"title": "A Simple Method for Predicting Covariance Matrices of Financial Returns", "link": "http://arxiv.org/abs/2305.19484", "description": "We consider the well-studied problem of predicting the time-varying\ncovariance matrix of a vector of financial returns. Popular methods range from\nsimple predictors like rolling window or exponentially weighted moving average\n(EWMA) to more sophisticated predictors such as generalized autoregressive\nconditional heteroscedastic (GARCH) type methods. Building on a specific\ncovariance estimator suggested by Engle in 2002, we propose a relatively simple\nextension that requires little or no tuning or fitting, is interpretable, and\nproduces results at least as good as MGARCH, a popular extension of GARCH that\nhandles multiple assets. To evaluate predictors we introduce a novel approach,\nevaluating the regret of the log-likelihood over a time period such as a\nquarter. This metric allows us to see not only how well a covariance predictor\ndoes over all, but also how quickly it reacts to changes in market conditions.\nOur simple predictor outperforms MGARCH in terms of regret. We also test\ncovariance predictors on downstream applications such as portfolio optimization\nmethods that depend on the covariance matrix. For these applications our simple\ncovariance predictor and MGARCH perform similarly."}, "http://arxiv.org/abs/2311.14698": {"title": "Business Policy Experiments using Fractional Factorial Designs: Consumer Retention on DoorDash", "link": "http://arxiv.org/abs/2311.14698", "description": "This paper investigates an approach to both speed up business decision-making\nand lower the cost of learning through experimentation by factorizing business\npolicies and employing fractional factorial experimental designs for their\nevaluation. We illustrate how this method integrates with advances in the\nestimation of heterogeneous treatment effects, elaborating on its advantages\nand foundational assumptions. We empirically demonstrate the implementation and\nbenefits of our approach and assess its validity in evaluating consumer\npromotion policies at DoorDash, which is one of the largest delivery platforms\nin the US. Our approach discovers a policy with 5% incremental profit at 67%\nlower implementation cost."}, "http://arxiv.org/abs/2311.14813": {"title": "A Review of Cross-Sectional Matrix Exponential Spatial Models", "link": "http://arxiv.org/abs/2311.14813", "description": "The matrix exponential spatial models exhibit similarities to the\nconventional spatial autoregressive model in spatial econometrics but offer\nanalytical, computational, and interpretive advantages. This paper provides a\ncomprehensive review of the literature on the estimation, inference, and model\nselection approaches for the cross-sectional matrix exponential spatial models.\nWe discuss summary measures for the marginal effects of regressors and detail\nthe matrix-vector product method for efficient estimation. Our aim is not only\nto summarize the main findings from the spatial econometric literature but also\nto make them more accessible to applied researchers. Additionally, we\ncontribute to the literature by introducing some new results. We propose an\nM-estimation approach for models with heteroskedastic error terms and\ndemonstrate that the resulting M-estimator is consistent and has an asymptotic\nnormal distribution. We also consider some new results for model selection\nexercises. In a Monte Carlo study, we examine the finite sample properties of\nvarious estimators from the literature alongside the M-estimator."}, "http://arxiv.org/abs/2311.14892": {"title": "An Identification and Dimensionality Robust Test for Instrumental Variables Models", "link": "http://arxiv.org/abs/2311.14892", "description": "I propose a new identification-robust test for the structural parameter in a\nheteroskedastic linear instrumental variables model. The proposed test\nstatistic is similar in spirit to a jackknife version of the K-statistic and\nthe resulting test has exact asymptotic size so long as an auxiliary parameter\ncan be consistently estimated. This is possible under approximate sparsity even\nwhen the number of instruments is much larger than the sample size. As the\nnumber of instruments is allowed, but not required, to be large, the limiting\nbehavior of the test statistic is difficult to examine via existing central\nlimit theorems. Instead, I derive the asymptotic chi-squared distribution of\nthe test statistic using a direct Gaussian approximation technique. To improve\npower against certain alternatives, I propose a simple combination with the\nsup-score statistic of Belloni et al. (2012) based on a thresholding rule. I\ndemonstrate favorable size control and power properties in a simulation study\nand apply the new methods to revisit the effect of social spillovers in movie\nconsumption."}, "http://arxiv.org/abs/2311.15458": {"title": "Causal Models for Longitudinal and Panel Data: A Survey", "link": "http://arxiv.org/abs/2311.15458", "description": "This survey discusses the recent causal panel data literature. This recent\nliterature has focused on credibly estimating causal effects of binary\ninterventions in settings with longitudinal data, with an emphasis on practical\nadvice for empirical researchers. It pays particular attention to heterogeneity\nin the causal effects, often in situations where few units are treated. The\nliterature has extended earlier work on difference-in-differences or\ntwo-way-fixed-effect estimators and more generally incorporated factor models\nor interactive fixed effects. It has also developed novel methods using\nsynthetic control approaches."}, "http://arxiv.org/abs/2311.15829": {"title": "(Frisch-Waugh-Lovell)': On the Estimation of Regression Models by Row", "link": "http://arxiv.org/abs/2311.15829", "description": "We demonstrate that regression models can be estimated by working\nindependently in a row-wise fashion. We document a simple procedure which\nallows for a wide class of econometric estimators to be implemented\ncumulatively, where, in the limit, estimators can be produced without ever\nstoring more than a single line of data in a computer's memory. This result is\nuseful in understanding the mechanics of many common regression models. These\nprocedures can be used to speed up the computation of estimates computed via\nOLS, IV, Ridge regression, LASSO, Elastic Net, and Non-linear models including\nprobit and logit, with all common modes of inference. This has implications for\nestimation and inference with `big data', where memory constraints may imply\nthat working with all data at once is particularly costly. We additionally show\nthat even with moderately sized datasets, this method can reduce computation\ntime compared with traditional estimation routines."}, "http://arxiv.org/abs/2311.15871": {"title": "On Quantile Treatment Effects, Rank Similarity, and Variation of Instrumental Variables", "link": "http://arxiv.org/abs/2311.15871", "description": "This paper investigates how certain relationship between observed and\ncounterfactual distributions serves as an identifying condition for treatment\neffects when the treatment is endogenous, and shows that this condition holds\nin a range of nonparametric models for treatment effects. To this end, we first\nprovide a novel characterization of the prevalent assumption restricting\ntreatment heterogeneity in the literature, namely rank similarity. Our\ncharacterization demonstrates the stringency of this assumption and allows us\nto relax it in an economically meaningful way, resulting in our identifying\ncondition. It also justifies the quest of richer exogenous variations in the\ndata (e.g., multi-valued or multiple instrumental variables) in exchange for\nweaker identifying conditions. The primary goal of this investigation is to\nprovide empirical researchers with tools that are robust and easy to implement\nbut still yield tight policy evaluations."}, "http://arxiv.org/abs/2311.15878": {"title": "Individualized Treatment Allocations with Distributional Welfare", "link": "http://arxiv.org/abs/2311.15878", "description": "In this paper, we explore optimal treatment allocation policies that target\ndistributional welfare. Most literature on treatment choice has considered\nutilitarian welfare based on the conditional average treatment effect (ATE).\nWhile average welfare is intuitive, it may yield undesirable allocations\nespecially when individuals are heterogeneous (e.g., with outliers) - the very\nreason individualized treatments were introduced in the first place. This\nobservation motivates us to propose an optimal policy that allocates the\ntreatment based on the conditional \\emph{quantile of individual treatment\neffects} (QoTE). Depending on the choice of the quantile probability, this\ncriterion can accommodate a policymaker who is either prudent or negligent. The\nchallenge of identifying the QoTE lies in its requirement for knowledge of the\njoint distribution of the counterfactual outcomes, which is generally hard to\nrecover even with experimental data. Therefore, we introduce minimax optimal\npolicies that are robust to model uncertainty. We then propose a range of\nidentifying assumptions under which we can point or partially identify the\nQoTE. We establish the asymptotic bound on the regret of implementing the\nproposed policies. We consider both stochastic and deterministic rules. In\nsimulations and two empirical applications, we compare optimal decisions based\non the QoTE with decisions based on other criteria."}, "http://arxiv.org/abs/2311.15932": {"title": "Valid Wald Inference with Many Weak Instruments", "link": "http://arxiv.org/abs/2311.15932", "description": "This paper proposes three novel test procedures that yield valid inference in\nan environment with many weak instrumental variables (MWIV). It is observed\nthat the t statistic of the jackknife instrumental variable estimator (JIVE)\nhas an asymptotic distribution that is identical to the two-stage-least squares\n(TSLS) t statistic in the just-identified environment. Consequently, test\nprocedures that were valid for TSLS t are also valid for the JIVE t. Two such\nprocedures, i.e., VtF and conditional Wald, are adapted directly. By exploiting\na feature of MWIV environments, a third, more powerful, one-sided VtF-based\ntest procedure can be obtained."}, "http://arxiv.org/abs/2311.15952": {"title": "Robust Conditional Wald Inference for Over-Identified IV", "link": "http://arxiv.org/abs/2311.15952", "description": "For the over-identified linear instrumental variables model, researchers\ncommonly report the 2SLS estimate along with the robust standard error and seek\nto conduct inference with these quantities. If errors are homoskedastic, one\ncan control the degree of inferential distortion using the first-stage F\ncritical values from Stock and Yogo (2005), or use the robust-to-weak\ninstruments Conditional Wald critical values of Moreira (2003). If errors are\nnon-homoskedastic, these methods do not apply. We derive the generalization of\nConditional Wald critical values that is robust to non-homoskedastic errors\n(e.g., heteroskedasticity or clustered variance structures), which can also be\napplied to nonlinear weakly-identified models (e.g. weakly-identified GMM)."}, "http://arxiv.org/abs/1801.00332": {"title": "Confidence set for group membership", "link": "http://arxiv.org/abs/1801.00332", "description": "Our confidence set quantifies the statistical uncertainty from data-driven\ngroup assignments in grouped panel models. It covers the true group memberships\njointly for all units with pre-specified probability and is constructed by\ninverting many simultaneous unit-specific one-sided tests for group membership.\nWe justify our approach under $N, T \\to \\infty$ asymptotics using tools from\nhigh-dimensional statistics, some of which we extend in this paper. We provide\nMonte Carlo evidence that the confidence set has adequate coverage in finite\nsamples.An empirical application illustrates the use of our confidence set."}, "http://arxiv.org/abs/2004.05027": {"title": "Direct and spillover effects of a new tramway line on the commercial vitality of peripheral streets", "link": "http://arxiv.org/abs/2004.05027", "description": "In cities, the creation of public transport infrastructure such as light\nrails can cause changes on a very detailed spatial scale, with different\nstories unfolding next to each other within a same urban neighborhood. We study\nthe direct effect of a light rail line built in Florence (Italy) on the retail\ndensity of the street where it was built and and its spillover effect on other\nstreets in the treated street's neighborhood. To this aim, we investigate the\nuse of the Synthetic Control Group (SCG) methods in panel comparative case\nstudies where interference between the treated and the untreated units is\nplausible, an issue still little researched in the SCG methodological\nliterature. We frame our discussion in the potential outcomes approach. Under a\npartial interference assumption, we formally define relevant direct and\nspillover causal effects. We also consider the ``unrealized'' spillover effect\non the treated street in the hypothetical scenario that another street in the\ntreated unit's neighborhood had been assigned to the intervention."}, "http://arxiv.org/abs/2004.09458": {"title": "Noise-Induced Randomization in Regression Discontinuity Designs", "link": "http://arxiv.org/abs/2004.09458", "description": "Regression discontinuity designs assess causal effects in settings where\ntreatment is determined by whether an observed running variable crosses a\npre-specified threshold. Here we propose a new approach to identification,\nestimation, and inference in regression discontinuity designs that uses\nknowledge about exogenous noise (e.g., measurement error) in the running\nvariable. In our strategy, we weight treated and control units to balance a\nlatent variable of which the running variable is a noisy measure. Our approach\nis explicitly randomization-based and complements standard formal analyses that\nappeal to continuity arguments while ignoring the stochastic nature of the\nassignment mechanism."}, "http://arxiv.org/abs/2111.12258": {"title": "On Recoding Ordered Treatments as Binary Indicators", "link": "http://arxiv.org/abs/2111.12258", "description": "Researchers using instrumental variables to investigate ordered treatments\noften recode treatment into an indicator for any exposure. We investigate this\nestimand under the assumption that the instruments shift compliers from no\ntreatment to some but not from some treatment to more. We show that when there\nare extensive margin compliers only (EMCO) this estimand captures a weighted\naverage of treatment effects that can be partially unbundled into each complier\ngroup's potential outcome means. We also establish an equivalence between EMCO\nand a two-factor selection model and apply our results to study treatment\nheterogeneity in the Oregon Health Insurance Experiment."}}